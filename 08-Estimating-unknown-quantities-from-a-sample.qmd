# 運用樣本估計未知量數 {#sec-Estimating-unknown-quantities-from-a-sample} 

```{r}
#| include: FALSE
source("header.R")

# add symbol picture into table
#addLink <- function(picfile) {
#  paste0("<a href=\"", link_location, "\">", as.character(fa(picfile)), "</a>")
#}
```

在前一章開頭，我們曾區辨描述統計與*推論統計*的差異。 @sec-Descriptive-statistics 曾經討論過，描述統計的功能是清晰地總結已知的資料。對照到推論統計的目標，就是"由已知資料裡獲得未知的資訊"。經過再學習機率理論基本概念，這一章就要好好理解進行統計推論會遇到的各種問題。我們能從資料獲得的什麼樣資訊？我們要如何獲得？這兩項問題是推論統計的核心，統計學家習慣用兩個”關鍵詞“稱呼：估計(estimation)與假設檢定(hypothesis testing)。這一章的學習目標是認識有關估計的理論，不過首先會討論抽樣理論，因為要認識抽樣，才能充分理解估計的理論。因此這一章的編排分成兩個部分，前半部8.1到8.3是討論抽樣理論，後半部8.4與8.5是示範如何應用抽樣理論進行點估計與區間估計。

<!---At the start of the last chapter I highlighted the critical distinction between descriptive statistics and *inferential statistics*. As discussed in @sec-Descriptive-statistics, the role of descriptive statistics is to concisely summarise what we *do* know. In contrast, the purpose of inferential statistics is to "learn what we do not know from what we do". Now that we have a foundation in probability theory we are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two "big ideas": estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I'm going to witter on about sampling theory first because estimation theory doesn't make sense until you understand sampling. As a consequence, this chapter divides naturally into two parts, the first three sections are focused on sampling theory, and the last two sections make use of sampling theory to discuss how statisticians think about estimation. --->

## 樣本、母群、抽樣{#sec-sample-population-sampling}

[中場報告](Prelude-Part-IV.html)提到一則歸納之謎，讓同學從中學習如何建構能判斷故事結局的假設。如果現在你能接受這樣的思考方式，應該能同意任何統計實務都是從接受原始資料的通用假設開始，這就是我們要學習**抽樣理論**的原因。如果說統計理論都是要建立在機率理論的地基之上，抽樣理論則是組建統計理論的材料。有效統計推論所需要的條件都是來自抽樣理論，也就是要達到統計學者們認可的“推論過程”，我們必須清楚說明推論*來自*什麼樣本，推論適用於什麼樣的母群。

身為研究者，在任何要使用統計的狀況，最感興趣的都是資料代表的**樣本**。像是參加實驗的受試者反應，民調公司打電話詢問民眾投票意向等等。透過這類方式收集的資料，通常是有限且不完整的。因為我們不可能請全世界的人來參加實驗，民調公司也沒有時間和經費詢問所有選民。在 @sec-Descriptive-statistics 學習的描述統計，目標只有處理手上使用的資料。那一章只有學到如何描述、總結和視覺化樣本的特點。現在我們要再往前一步了。

<!---In the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where **sampling theory** comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about "making inferences" the way statisticians think about it we need to be a bit more explicit about what it is that we're drawing inferences *from* (the sample) and what it is that we're drawing inferences about (the population).

In almost every situation of interest what we have available to us as researchers is a **sample** of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can't possibly get every person in the world to do our experiment, for example a polling company doesn't have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in @sec-Descriptive-statistics this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.  --->

### 何謂母群{#sec-what-is-population}

樣本是實在的，打看一份資料檔案，儲存的資料就是你能處理的樣本。然而，**母群**就抽象多了。這個詞是指所有可以用來推導結論的人類行為紀錄，可以觀測到的數值，而且比你能處理的樣本**大上許多**。最理想的狀態是研究者清楚曉得想研究的母群是什麼模樣，因為設計實驗和分析資料檢測假設的程序，都是根據研究者對於母群的基本認識。

只有很少的狀況，我們能清楚了解想研究的母群是什麼模樣。例如前面提到的民調公司，要研究的母群是有資格投票的選民，能調查的樣本就是這群選民之中的1000人。多數研究要面對的母群都是模糊的。典型的心理學實驗設定的母群模樣相當複雜。假如我做了一個探討人類如何思考的認知實驗，找了一百位大學生參加實驗。那麼，這個實驗的母群是以下那一種呢？

- 澳洲阿得雷德大學心理系大學部所有學生?
- 世界上所有大學心理系的大學生?
- 所有住在台灣的台灣人?
- 和原作者/譯者年紀相仿的國民?
- 任何世界上活生生的人類?
- 過去、現在、和未來的人類?
- 任何能在行星環境生存，有足夠智能的生物個體?
- 任何有智能的實體?

上述每一條都是有智慧的個體所組成的群體，不過到底是那一個群體才是原作者設計的認知實驗目標母群呢？另一個很難定義清楚母群條件的狀況，就是[中場故事](/Prelude-Part-IV.html)提到的瓶子裡裝什麼球的例子[^08-translation-01]。
例子提到抽出的12個球都是白球，並沒有彩球。我們要如何設定這個例子的母群呢？似乎以下每一條設定都有道理。

- 瓶子裡的球都被抽完？
- 負責抽球的人抽到不想抽為止？
- 用機器人抽球，抽到機器人無法運作為止？
- 找奇異博士去無限多的平行宇宙查看，看看每個宇宙抽出12個球的結果？


[^08-translation-01]: 譯註~原文的例子已換成較通用的例子，置換理由請見[中場故事](/Prelude-Part-IV.html)的翻譯說明。

<!---A sample is a concrete thing. You can open up a data file and there's the data from your sample. A **population**, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally *much bigger* than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.

Sometimes it's easy to state the population of interest. For instance, in the "polling company" example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as "the population":

- All of the undergraduate psychology students at the University of Adelaide?
- Undergraduate psychology students in general, anywhere in the world?
- Australians currently living?
- Australians of similar ages to my sample?
- Anyone currently alive?
- Any human being, past, present or future?
- Any biological organism with a sufficient degree of intelligence operating in a terrestrial environment?
- Any intelligent being?

Each of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it's not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it's not obvious what the population is.

- All outcomes until Wellesley and Croker arrived at their destination?
- All outcomes if Wellesley and Croker had played the game for the rest of their lives?
- All outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?
- All outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe? --->

### 簡單隨機樣本

不論要採用那種母群的定義，重點是我們要找出代表母群一部分的樣本，還有運用從樣本學到的知識推測母群的性質。樣本和母群的關係建立在選取樣本的的程序。這樣的程序就是**抽樣方法(sampling method)**，理解抽樣方法是正確有效運用推論統計的關鍵。

我們用一個裝有十顆球的的袋子舉例說明抽樣方法。每顆球各印有一個字母區別，並且被塗上黑色或白色。這十顆球構成的母群能繪製成像 @fig-fig8-1 的示意圖一覽無遺。十顆球裡有四顆黑球和六顆白球，但是請假裝還沒打開袋子之前，我們完全不知道每顆球長什麼樣子。接著我們做一次「想像實驗」：先把袋子拿起來搖幾下，矇起眼睛從袋子裡依序一次一顆，抽出其中四顆球，將他們排在眼前。第一輪我們拿出的是字母a黑球，字母c白球，字母j白球，還有字母b黑球。紀錄完畢再將四顆球放回袋子裡，我們可以重覆同樣的抽球程序數輪，每次紀錄累積就如 @fig-fig8-1 右邊的樣子。如此不斷重覆、結果卻每次不同的抽球程序是一種*隨機程序*。[^08-estimating-unknown-quantities-from-a-sample-1] 我們能認可這是隨機程序的主要理由是每次抽球之前，都要先搖一搖袋子，讓每顆球被抽出的機會是相等的。能讓母群裡的每一份子以相等機會抽出，形成的樣本就是**簡單隨機樣本**。每取出一顆球都不會放回袋子，能確保每一次抽樣得到的樣本不會有兩個同樣的球，這種取樣限制稱為**不放回抽樣**。


[^08-estimating-unknown-quantities-from-a-sample-1]: 隨機的數學推導相當繁複，並且已經超出本書的學習範圍。本書不依靠數學公式解說隨機程序，讀者只要理解取樣程序可以不斷重覆，每次取樣結果都不一樣就是一種隨機程序。

<!---Irrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a **sampling method** and it is important to understand why it matters.

To keep things simple, let's imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of @fig-fig8-1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn't know that unless we looked in the bag. Now imagine you run the following "experiment": you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of @fig-fig8-1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a *random process*.[^08-estimating-unknown-quantities-from-a-sample-1] However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a **simple random sample**. The fact that we did not put the chips back in the bag after pulling them out means that you can't observe the same thing twice, and in such cases the observations are said to have been sampled **without replacement**. --->

```{r}
#| label: fig-fig8-1
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 自有限母群不放回抽樣的可能樣本。
#Simple random sampling without replacement from a finite population
knitr::include_graphics("images/fig8-1.png")
```

為了讓同學更了解抽樣方法如何影響樣本的組合，我們來想像另一種取樣狀況。想像有個五歲小朋友跑進來，擅自打開袋子拿出全部黑球，沒放回去又跑開了。旁邊做紀錄的同學沒注意到還當成是一次實驗結果，如果發生好幾次，就會看到像 @fig-fig8-2 展示的偏誤樣本。請想想如果做實驗的同學每次依照*隨機抽樣規則*抽球，看到四顆球的樣本可能性有多高？抽樣規則確實會影響樣本組成。如果我們了解全是黑球是有偏誤的抽樣方法造成的，那麼這樣的**偏誤樣本**無法有效推測母群的性質！這是為什麼統計學者特別重視資料檔案裡的紀錄是不是來自簡單隨機樣本，隨機樣本的資料進行分析不大需要太多處理。

<!--- [^08-estimating-unknown-quantities-from-a-sample-1]: The proper mathematical definition of randomness is extraordinarily technical, and way beyond the scope of this book. We'll be non-technical here and say that a process has an element of randomness to it whenever it is possible to repeat the process and get different answers each time. 

To help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in @fig-fig8-2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn't tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis *much* easier. --->

```{r}
#| label: fig-fig8-2
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 自有限母群偏誤抽樣的樣本。
#Biased sampling without replacement from a finite population
knitr::include_graphics("images/fig8-2.png")
```

我們再來想像第三種抽樣狀況。這次每輪抽出一顆球之前，先搖一搖袋子，取出一顆球做好紀錄，再放回袋子重新搖一搖袋子，再取出一顆球紀錄，如此重覆直到完成四次紀錄。如此程序取得的樣本也是簡單隨機樣本，因為每次取球都要放回袋子，因此這樣的方法稱為**放回抽樣**。放回抽樣與不放回抽樣的主要差別在於，放回抽樣得到的樣本有可能看到同一顆球在樣本裡出現兩次，如果 @fig-fig8-3 的展示。

<!---A third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample **with replacement**. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in @fig-fig8-3. --->

```{r}
#| label: fig-fig8-3
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 自有限母群*放回抽樣*的可能樣本。
#Simple random sampling *with* replacement from a finite population
knitr::include_graphics("images/fig8-3.png")
```

大多數心理學實驗取得的樣本是*不放回抽樣*的結果，因為同一個人不大能參加同一項實驗兩次。不過，大多數統計理論是建立在**放回抽樣**形成的簡單隨機樣本。現實與理論的差異在多數研究實務並不會有太多影響。如果研究對象的母群組成份子多到一個程度，放回抽樣和不放回抽樣的隨機樣本幾乎沒有差別。另一方面，簡單隨機樣本與偏誤樣本之間的差別，在很多實務狀況裡很難看得出來。

<!---In my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample **with replacement**. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss. --->

### 你知道的樣本並不是簡單隨機樣本

就像前面說明的範例所展示的，我們幾乎不可能從想研究的母群取得真正的簡單隨機樣本。許多在大學實驗室執行的心理學實驗，都是直接找該校大學部同學來參加，設計實驗的教授和研究生都要**假裝**他們的參與者都是隨機樣本的一部分。取樣方法還有很多種，而且其實是超出這門課程的學習範圍，在此還是做點介紹，給好奇心強的同學一些進階學習的指引。

- *分層取樣* 

- *滾雪球取樣* 

- *方便取樣* 


<!---As you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I'd consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what's out there I'll list a few of the more important ones.--->

-   *Stratified sampling*. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you're running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two [^08-estimating-unknown-quantities-from-a-sample-2] strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups
-   *Snowball sampling* is a technique that is especially useful when sampling from a "hidden" or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren't careful you might end up outing people who don't want to be outed (very, very bad form), and even if you don't make that mistake it can still be intrusive to use people's social networks to study them. It's certainly very hard to get people's informed consent before contacting them, yet in many cases the simple act of contacting them and saying "hey we want to study you" can be hurtful. Social networks are complex things, and just because you can use them to get data doesn't always mean you should.
-   *Convenience sampling* is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.

[^08-estimating-unknown-quantities-from-a-sample-2]: Nothing in life is that simple. There's not an obvious division of people into binary categories like "schizophrenic" and "not schizophrenic". But this isn't a clinical psychology text so please forgive me a few simplifications here and there.

### 不是簡單隨機樣本該怎麼辦？

好吧，現實世界收集的資料經常不是簡單隨機樣本。這有關係嗎？萬一資料不是簡單隨機樣本，稍微思考一下就會明白，這可能會造成糟糕的分析。想想 @fig-fig8-1 和 @fig-fig8-2 兩種取樣程序的差異就能明白。然而，事實沒有聽起來那麼糟。某些類型的偏誤樣本是不會影響分析結果的。例如，使用分層取樣時，我們要知道什麼條件 會造成偏誤，因為我們是有意識地製造樣本。這通常是為了增加研究的有效性，而且有統計技術可以用來調整引入資料的偏差（本課程不談這部分！）。因此，在某些情況下，偏誤樣本並不是一個問題。

在一般情況的重要關鍵是，隨機取樣只是達成目標的手段，而不是目的本身。假設你採用的是方便取樣，因此可以假定樣本具有偏誤性。只有取樣方法未控制偏誤，才會導致錯誤的結論，這才是問題所在。從這個角度來看，我認為我們並不需要在每個條件都使用隨機化樣本，我們只需要針對感興趣的心理現象進行隨機取樣即可。假定我正在進行一項探討工作記憶容量研究。在第一個研究中，我能從全世界活生生的人群裡隨機取樣，只有一個例外：我只能取樣星期一出生的人。在第二個研究中，我能夠從所有澳洲人中隨機取樣，然後將結果類推到所有人類。哪一個研究結果比較好呢？答案顯然是第一個研究。為什麼？因為我們沒有理由認為“出生在星期一”與工作記憶容量有任何有意思的關聯。相比之下，我可以想到幾個原因，認為“澳洲人”可能是重要的偏誤因素。澳洲是一個富裕、工業化的國家，擁有非常發達的教育系統。在這個國家成長的人們會有很多與設計測試方法的人相似的生活經驗。這種共同經驗可能很容易轉化為相似的測試方法、心理實驗假設等等。這些考慮可能真的很重要。例如，澳洲參與者可能已經習慣專注“測試方法”彙整的相對抽象的測試材料，比起不是在類似環境中成長的人有更多“應付”經驗，而這可能導致我對工作記憶容量的錯誤推論。

這個小節的討論隱含兩個觀點。首先是身為研究的設計者立場，重要的是要考慮你所關心的母群條件，儘可能以最適合的方式進行取樣。實務上，我們通常只能使用“方便樣本”（例如，心理學教師找修心理學的學生收集資料，因為這是收集資料最便宜的方法，而我們的經費通常很有限）。如果必須使用的話，設計者應該要好好思考這種取樣方法可能存在的偏誤。另一方面是評論他人研究的立場，如果他們不得不使用方便樣本，而不是從整個人類母群進行隨機抽樣，那麼我們要有禮貌地提供一個具體的理論，解釋他們的取樣方法可能如何扭曲結果。

<!---Okay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between @fig-fig8-1 and @fig-fig8-2. However, it's not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to *increase* the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you've introduced (not covered in this book!). So in those situations it's not a problem. 

More generally though, it's important to remember that random sampling is a means to an end, and not the end in itself. Let's assume you've relied on a convenience sample, and as such you can assume it's biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I'd argue that we don't need the sample to be randomly generated in *every* respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I'm doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being "born on a Monday" has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why "being Australian" might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to "take a test", a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, "test taking" style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven't grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.

There are two points hidden in this discussion. First, when designing your own studies, it's important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you're usually forced to put up with a "sample of convenience" (e.g., psychology lecturers sample psychology students because that's the least expensive way to collect data, and our coffers aren't exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you're going to criticise someone else's study because they've used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.--->

### 母群參數與樣本統計

好的。撇開獲取隨機樣本的棘手方法論問題，讓我們考慮一個略微不同的問題。到目前為止，我們一直是以科學家的觀點討論母群。對於心理學家來說，母群可能是一群人；對於生態學家來說，母群可能是一群熊。在大多數情況，各類科學家關心的母群是現實世界中實際存在的具體事物。然而，統計學家有點與眾不同。他們像其他科學家一樣對現實世界的資料和科學感興趣，也像數學家一樣探討抽象符號的操作。因此，統計學理論定義母群的方式通常有些抽象。就像心理學研究人員想用具體的測量方法將抽象理論轉換成操作型定義(參考 @sec-Introduction-to-psychological-measurement)，統計學家將“母群”的抽象概念轉換為可操作的數學符號。同學們已經在前一章(@sec-Introduction-to-probability)學習了這些知識。它們被稱為機率分布。

在此簡單示範一下。假定我們的研究對象是一群人的智力分數。對於心理學家而言，這裡的母群是一群有智力測驗成績的真實人類。而統計學家會透過 @fig-fig8-4 (a) 展示的機率分佈定義母群，來「簡化」這個問題。智力測驗成續的平均智商是100，標準差是15，而且分佈是常態分佈。這些數值被稱為**母群參數**，因為它們代表整個母群的特徵。也就是說，我們會定義這個母群的平均值$\mu$是100，母群標準差 $\sigma$ 是15。

<!---Okay. Setting aside the thorny methodological issues associated with obtaining a random sample, let's consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (@sec-Introduction-to-psychological-measurement), statisticians operationalise the concept of a "population" in terms of mathematical objects that they know how to work with. You've already come across these objects in @sec-Introduction-to-probability. They're called probability distributions.

The idea is quite simple. Let's say we're talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician "simplifies" this by operationally defining the population as the probability distribution depicted in @fig-fig8-4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the **population parameters** because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.--->

```{r}
#| label: fig-fig8-4
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 智力測驗分數的母群分佈（圖a）以及從中取得的兩個隨機樣本。圖（b）是一個由100個觀察值組成的樣本，圖（c）是一個由10,000個觀察值組成的樣本。
#The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations
knitr::include_graphics("images/fig8-4.png")
```

假設現在我做了一個實驗。我隨機選出100個人，請他們進行智力測驗，這樣我就得到了母群的一個簡單隨機樣本。我的樣本裡有以下一系列數字：

<!---Now suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:--->

106 101 98 80 74 ... 107 72 100

每個分數都是從一個平均值為100、標準差為15的常態分佈隨機取樣得到的。如果我繪製樣本的直方圖，就會得到像 @fig-fig8-4 (b)的結果。你可以看到，直方圖的形狀與常態分佈大致相似，但只是粗略近似真實母群（如 @fig-fig8-4 (a)）的分佈。以這群樣本計算樣本平均值，我得到了一個與母群平均值100相當接近但不完全相同的數字。在這個例子中，我得到的樣本平均智商是98.5，樣本標準差是15.9。這些來自樣本的統計量數是呈現資料的特徵，雖然它們接近真實母群的值，卻並不相同。通常，樣本統計量數是從資料集計算出來的，而母群參數是你想要了解的。在本章稍後，我將談到如何使用樣本統計量來[估計母群參數](08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters)，以及[估計置信區間](08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval)，但在那之前，你需要了解更多有關取樣理論的概念。

<!---Each of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in @fig-fig8-4 (b). As you can see, the histogram is roughly the right shape but it's a very crude approximation to the true population distribution shown in @fig-fig8-4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These **sample statistics** are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I'll talk about [Estimating population parameters] using your sample statistics and also [Estimating a confidence interval] but before we get to that there's a few more ideas in sampling theory that you need to know about--->

## 大數法則

在前一節中，我們展示了一個樣本數本數是 N = 100 的虛構智力測驗實驗結果。這個結果有些令人振奮，因為真實母群的平均值是 100，而樣本平均值 98.5 是一個相當合理的近似值。在許多科學研究中，這種精確度是可以接受的，但在其他情況下，我們希望更加精確。如果我們希望樣本統計量數更接近母群參數，可以怎麼做呢？顯然需要收集更多的資料。假定我們進行了一個更大型的實驗，這次測量了 10,000 人的智商。同學們可以使用 jamovi 模擬這個實驗的結果。在示範檔案庫裡的IQsim.omv ，我們生成了 10,000 個從平均值為 100、標準差為 15 的常態分佈中隨機抽樣的數字。這是通過使用計算變項函式 = NORM(100, 15) 生成的。參見圖 @fig-fig8-5 的直方圖和密度圖，模擬結果顯示這個更大的樣本比較小的樣本更近似真實母群分佈。這也反映在樣本統計量數。更大樣本的平均智力分數是 99.68，標準差為 14.90。這些值現在非常接近真實母群。

<!---In the previous section I showed you the results of one fictitious IQ experiment with a sample size of N = 100. The results were somewhat encouraging as the true population mean is 100 and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it? The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using jamovi. The IQsim.omv file is a jamovi data file. In this file I have generated 10,000 random numbers sampled from a normal distribution for a population with mean = 100 and sd = 15. This was done by computing a new variable using the = NORM(100,15) function. A histogram and density plot shows that this larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard deviation is 14.90. These values are now very close to the true population. See @fig-fig8-5.--->

```{r}
#| label: fig-fig8-5
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 使用jamovi產生由符合常態分佈的母群隨機取樣之結果。
#A random sample drawn from a normal distribution using jamovi
knitr::include_graphics("images/fig8-5.png")
```

希望同學能從這個示範得到一點啟示，雖然有點不好意思，因為大樣本能提供更有品質的訊息，是顯而易見的，似乎不需要特別說明。其實這個觀點非常直觀，以至於機率理論的創始人之一雅各布·伯努利在1713年發表大數法則的論文時，曾經用尖酸刻薄的語氣描述這個人人都有的直覺：

> *即使是最蠢的人，也憑藉本能的直覺，不靠他人教導(相當了不起)就能獨自明白：觀察的次數越多，結果就越不容易偏離目標。* [@Stigler1986, 第65頁]。

嗯，這段話聽起來有點自大（而且還有點性別歧視），但他的主要觀點是正確的。事實上，更多數據確實會產生更好的結果。問題是，為什麼會這樣呢？不出所料，所有人類都會直覺地認為的這樣的看法是正確的，統計學家稱之為**大數法則**。大數法則是一條適用於許多不同樣本統計量的數學法則，但最簡單的想法就是關於平均數的法則。樣本平均值是一個最明顯的例子，因為它是算述平均的產物（因為算述平均就是一個平均值）。大數法則應用於算述平均的意思是，隨著樣本數增加，算述平均的結果越趨近於真實的母群平均值。或者更精確一點地說，當樣本數“趨近”於無窮大（寫為$N \longrightarrow \infty$）時，樣本平均值趨近於母群平均值（$\bar{X} \longrightarrow \mu$）[^08-estimating-unknown-quantities-from-a-sample-3]。

[^08-estimating-unknown-quantities-from-a-sample-3]: 技術上來說，大數法則適用於任何一種形式如同算術平均值的樣本統計量數，樣本平均值當然適用。但是，其他樣本統計量數也可以寫成像平均值的形式。例如，樣本的變異數可以改寫為一種算術平均值，因此也受到大數法則的影響。然而，樣本的最小值無法寫成任何平均值的形式，因此不受大數法則的支配。

我並不打算向同學示範如何證明大數法則，不過它是統計理論中最重要的工具之一。大數法則可以用來證明，收集越多的資料，最終將接近真相的數學工具。對於任何特定的資料集，所計算出來的樣本統計量數都可能是錯誤的，但是大數法則告訴我們，只要繼續收集更多的數據，這些樣本統計量數將趨近於真實的母群參數。

<!---I feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it's so bloody obvious that it shouldn't need to be said. In fact, it's such an obvious point that when Jacob Bernoulli, one of the founders of probability theory, formalised this idea back in 1713 he was kind of a jerk about it. Here's how he described the fact that we all share this intuition:

> *For even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one's goal* [@Stigler1986, p. 65].

Okay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct. It really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the **law of large numbers**. The law of large numbers is a mathematical law that applies to many different sample statistics but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that's what the mean is... an average), so let's look at that. When applied to the sample mean what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size "approaches" infinity (written as $N \longrightarrow \infty$), the sample mean approaches the population mean $\bar{X} \longrightarrow \mu$)[^08-estimating-unknown-quantities-from-a-sample-3]

[^08-estimating-unknown-quantities-from-a-sample-3]: Technically, the law of large numbers pertains to any sample statistic that can be described as an average of independent quantities. That's certainly true for the sample mean. However, it's also possible to write many other sample statistics as averages of one form or another. The variance of a sample, for instance, can be rewritten as a kind of average and so is subject to the law of large numbers. The minimum value of a sample, however, cannot be written as an average of anything and is therefore not governed by the law of large numbers

I don't intend to subject you to a proof that the law of large numbers is true, but it's one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters.--->

## 樣本分佈與中央極限定理

大數法則是非常強大的工具，但它並不能回答所有現實生活的問題。它給我們的只是一個“長期保證”。長期而言，如果我們能夠收集無限量的資料，那麼大數法則保證我們的樣本統計數據是正確的。但是正如經濟學大師凱因斯的著名論點，長期保證在我們的現實生活中幾乎沒有用處。

> *長期保證對當前問題的指引是一個誤導，(因為問題解決時)，我們(可能)都已經死去。如果經濟學家只能告訴我們，當暴風雨過去後，海洋會再次變平靜，那麼他們的任務就太容易也太無用了，特別是在風雨飄搖的時刻。* [@Keynes1923, p. 80]

就像經濟學一樣，心理學和統計學也是如此。僅僅知道最終計算出的樣本平均值會趨近於母體平均值是不夠的。一個無限大的資料集終將揭露實際的母群平均值，只是精神上的慰藉，但是如果實際的資料樣本數只有 $N = 100$ ，那麼這樣的知識在現實生活中是不夠用的。在心理學的實際研究場景，我們通常要從一個樣本數更小的資料計算樣本平均值，描述這群樣本的行為！


<!---The law of large numbers is a very powerful tool but it's not going to be good enough to answer all our questions. Among other things, all it gives us is a "long run guarantee". In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life.

> *[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again.* [@Keynes1923, p. 80].

As in economics, so too in psychology and statistics. It is not enough to know that we will eventually arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of $N = 100$. In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set! --->

### 平均值的樣本分佈 {#sec-Sampling-distribution-of-the-mean}

考慮到現實情況，讓我們不得不放棄這樣的想法，也就是不要期待我們的研究會收集到一萬人，改成設計一個規模非常小的實驗。這次只找 $N=5$ 個人，測量他們的智力分數。和前面的示範一樣，我們可以在 jamovi 中使用 NORM(100,15) 函式模擬這個實驗，但是這一次我只需要5個參與者，而不是10,000個。以下是 jamovi 隨機生成的五個數值(應該和你的操作結果不同)：

<!---With this in mind, let's abandon the idea that our studies will have sample sizes of 10,000 and consider instead a very modest experiment indeed. This time around we'll sample $N = 5$ people and measure their IQ scores. As before, I can simulate this experiment in jamovi = NORM(100,15) function, but I only need 5 participant IDs this time, not 10,000. These are the five numbers that jamovi generated: --->

90 82 94 99 110

這批樣本的平均智力分數恰好是95。不出所料，比起前面示範的實驗結果不準確得多。現在想像一下，我決定**重現**(replicate)這項實驗。也就是說，我會盡可能地重做原來的實驗程序，隨機選擇另外5個人並測量他們的智力分數。同樣地，Jamovi可以模擬這個過程的結果，生成以下5個數字(也應該和你的操作結果不同)：

<!---The mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to **replicate** the experiment. That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates these five numbers: --->

78 88 111 111 117

這次樣本的平均智力分數是101。如果我重做這個實驗10次，可能得到的結果如 表8-1 ，可以看到每次實驗的樣本平均值都不一樣。

<!---This time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in @tbl-tab8-1, and as you can see the sample mean varies from one replication to the next. #huxtabs[[8]][[1]] 
#| label: tbl-tab8-1
#| tbl-cap: 重現智力測驗實驗十次的結果，每次樣本數都是 $( N = 5 )$ 。
#Ten replications of the IQ experiment, each with a sample size of $( N = 5 )$
--->



表8-1：重現智力測驗實驗十次的結果，每次樣本數都是 $( N = 5 )$ 。

||  參與者 1  |  參與者 2  |   參與者 3  | 參與者 4 |  參與者 5  |樣本平均值 |
|----------|------------|------------|-------------|----------|------------|----------------|
|  實驗批次 1  | 90         |  82        |   94        |  99      |  110       |  95.0          |
|  實驗批次 2  |  78        |  88        |   111       |    111   |    117     |  101.0         |
| 實驗批次 3   |  111       |   122      |   91        |  98      |  86        |  101.6         |
| 實驗批次 4   |  98        |  96        |    119      | 99       |  107       |  103.8         |
| 實驗批次 5   |  105       |   113      |   103       | 103      |  98        |  104.4         |
| 實驗批次 6   |  81        |  89        |   93        |  85      |  114       |  92.4          |
| 實驗批次 7   |  100       |  93        |   108       | 98       |  133       |  106.4         |
| 實驗批次 8   |  107       |  100       |    105      | 117      |  85        |  102.8         |
| 實驗批次 9   |  86        |   119      |   108       | 73       |  116       |  100.4         |
| 實驗批次 10  |  95        |  126       |   112       | 120      |  76        |  105.8         |

假如現在我決定繼續以這種方式，繼續重做這個「五個智力分數」的實驗。每次完成實驗，我都會記錄下樣本平均值。隨著實驗的進行，我會累積一個新的資料集，在這個資料集中，每次實驗都有產生一個資料點。我的資料集的前10個觀察值就是 表8-1 列出的樣本平均值，因此我的資料集前幾個數值是這樣的：


<!---Now suppose that I decided to keep going in this fashion, replicating this "five IQ scores" experiment over and over again. Every time I replicate the experiment I write down the sample mean. Over time, I'd be amassing a new data set, in which every experiment generates a single data point. The first 10 observations from my data set are the sample means listed in @tbl-tab8-1, so my data set starts out like this: --->

95.0 101.0 101.6 103.8 104.4 ...

假如我繼續重做實驗10,000次，然後用每次實驗的樣本平均值繪製直方圖，那會怎麼樣呢？這正是你在@fig-fig8-6看到的結果。如這張圖所示，5個智力分數的平均值大部分落在90到110之間。更重要的是：如果我們一遍又一遍地重做一個實驗，我們最終得到的是一個**樣本平均值的分佈**！（見表8-1）。統計學的正式名稱叫做**樣本平均值的取樣分佈**(sampling distribution of the mean)。

<!---What if I continued like this for 10,000 replications, and then drew a histogram. Well that's exactly what I did, and you can see the results in @fig-fig8-6. As this picture illustrates, the average of 5 IQ scores is usually between 90 and 110. But more importantly, what it highlights is that if we replicate an experiment over and over again, what we end up with is a distribution of sample means! (@tbl-tab8-1)) This distribution has a special name in statistics, it's called the **sampling distribution of the mean**.--->

```{r}
#| label: fig-fig8-6
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 「五個智力分數實驗」的樣本平均值分配。假如你隨機找五個人並計算他們的平均智力測驗分數，幾乎會得到一個介於80和120之間的數字，即使很多人測得的智力分數是高於120或低於80。為了比較，黑線表示智力分數的母群分佈。
#The sampling distribution of the mean for the 'five IQ scores experiment'. If you sample 5 people at random and calculate their average IQ you'll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores
knitr::include_graphics("images/fig8-6.png")
```

取樣分佈是另一個重要的統計理論概念，對於理解小樣本的行為非常關鍵。例如，當我進行第一個“五個智力分數”的實驗時，樣本平均值是95。但是 @fig-fig8-6 的取樣分佈告訴我們，“五個智力分數”的實驗不是很準確。如果我重複進行實驗，取樣分佈告訴我，我可以預期大部分的樣本平均值會是80到120之間。

<!---Sampling distributions are another important theoretical idea in statistics, and they're crucial for understanding the behaviour of small samples. For instance, when I ran the very first "five IQ scores" experiment, the sample mean turned out to be 95. What the sampling distribution in @fig-fig8-6 tells us, though, is that the "five IQ scores" experiment is not very accurate. If I repeat the experiment, the sampling distribution tells me that I can expect to see a sample mean anywhere between 80 and 120. --->

### 任何數值皆有樣本分佈！

針對取樣分佈的概念，需要注意的一點是，任何你可能想要計算的樣本統計量數都有其對應的取樣分佈。例如，假設每次我重複進行 "五個智力分數 " 的實驗時，我都記錄其中最高的智力分數。這會產生一個資料集，一開始的數值可能如下：

<!---One thing to keep in mind when thinking about sampling distributions is that any sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time I replicated the "five IQ scores" experiment I wrote down the largest IQ score in the experiment. This would give me a data set that started out like this: --->

110 117 122 119 113 ...

反覆進行這個實驗過程會給我一個非常不同的取樣分佈，也就是最大值的取樣分佈。五個智力分數的最大值的取樣分佈顯示在@fig-fig8-7。不出所料，如果你隨機選擇五個人，然後找出智力分數最高的人，他們的智力分數很有可能會高於平均水準。大多數情況下，你會得到智力分數在100到140之間的結果。

<!---Doing this over and over again would give me a very different sampling distribution, namely the sampling distribution of the maximum. The sampling distribution of the maximum of 5 IQ scores is shown in @fig-fig8-7. Not surprisingly, if you pick 5 people at random and then find the person with the highest IQ score, they're going to have an above average IQ. Most of the time you'll end up with someone whose IQ is measured in the 100 to 140 range. --->

```{r}
#| label: fig-fig8-7
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 「五個智力分數實驗」最大值的取樣分佈。如果您隨機選取5個人，然後紀錄最高的智力分數，您可能會看到多數智力分數落在100到140之間。
#The sampling distribution of the maximum for the 'five IQ scores experiment'. If you sample 5 people at random and select the one with the highest IQ score you'll probably see someone with an IQ between 100 and 140
knitr::include_graphics("images/fig8-7.png")
```


### 中央極限定理 {#sec-The-central-limit-theorem}

```{r}
#| label: fig-fig8-8
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 這是一個說明樣本數如何影響樣本平均數的取樣分佈的例子。每張圖的直方圖是由10,000組智力分數樣本平值構成。圖中的直方圖顯示了這些平均值的分佈（即平均值的取樣分佈）。每個單獨的智力分數都是從平均值為100，標準差為15的常態分佈選機選取的，這在圖中以實線表示。圖(a)的每個資料集僅包含一個觀察值，因此每個樣本的平均值就是一個人的智力分數。因此，平均數的取樣分佈當然與智力分數的母群分佈相同。然而，當我們增加樣本數到2，任何一個樣本的平均值都比任何一個人的智力分數更接近母體平均值，因此直方圖（即取樣分佈）比母群分佈更窄。當樣本數提高到10（圖(c)），可以看到樣本平均值的分佈會緊密地聚集在真實的母體平均值周圍。
#An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel (a), each data set contained only a single observation, so the mean of each sample is just one person's IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than any one person's IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel (c)), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean
knitr::include_graphics("images/fig8-8.png")
```

至此希望同學對取樣分佈有充分的理解，特別是有關平均值的取樣分佈。在這一節，我想談談平均值的取樣分佈如何隨著樣本數而改變。以直觀來說，你應該已經知道一部分的答案。如果你只有幾個觀察值，樣本平均值可能相當不準確。假如你持續重做一個小樣本實驗並計算平均值，你會得到一個非常不同的答案。換句話說，取樣分佈的變異範圍非常寬。如果你持續重做一個大樣本實驗並計算樣本平均值，你有可能會得到和上次實驗一樣的結果，因此取樣分佈的變異範圍會非常窄。你可以在 @fig-fig8-8 看到這個直觀敘述的效果，由左到右的圖表顯示樣本數越大，取樣分佈越窄。我們可以通過計算取樣分佈的標準差來量化這種變化，統計學名詞為**標準誤差**(standard error)。統計學報告裡的標準誤差通常寫成SE，由於最常報告的標準誤差是樣本平均值的，因此我們通常使用縮寫SEM(standard error of the sample mean)。從 @fig-fig8-8 可以看出，隨著樣本數 $N$ 的增加，SEM會減少。

好的，走到這一節，我有一個到目前為止一直省略的部分。至此示範的模擬實驗都是基於"智力測驗分數"，是因為智力測驗分數大致呈現常態分佈，所以我假設母群的分佈也是常態。如果母群的分佈不是常態分佈，那麼樣本平均數的取樣分佈會變成什麼樣子？令初次學習的同學驚訝的是，無論母群分佈是什麼形狀，當樣本數 $N$ 增加時，樣本平均數的取樣分佈會越來越像是常態分佈。為了讓同學了解，我進行了一些模擬。我們首先從 @fig-fig8-9 (a)，像“斜坡”的母群分佈開始。比較看起來像三角形的直方圖和黑色的鐘形曲線，同學可以看出母群分佈看起來根本不像常態分佈。接下來，我做了大量模擬的實驗。在每次實驗，我從母群隨機選取 $N=2$ 個樣本，然後計算樣本平均值。@fig-fig8-9 (b) 繪製了這些樣本平均值的直方圖（即 $N=2$ 時的樣本平均數的抽樣分佈）。累積的直方圖近似 $\chi^2$ 分佈。雖然不是常態分佈，但是比起 @fig-fig8-9（a）的母群分佈更接近黑線。當我將樣本大小增加到 $N=4$ 時，樣本平均數的取樣分佈就非常接近常態分佈（@fig-fig8-9（c）），當樣本數達到 $N=8$ 時，它幾乎完全等於常態分佈。換句話說，只要你的樣本數不是太小，無論你的母群分佈長什麼樣子，樣本平均值的取樣分佈都會近似於常態分佈！

<!---At this point I hope you have a pretty good sense of what sampling distributions are, and in particular what the sampling distribution of the mean is. In this section I want to talk about how the sampling distribution of the mean changes as a function of sample size. Intuitively, you already know part of the answer. If you only have a few observations, the sample mean is likely to be quite inaccurate. If you replicate a small experiment and recalculate the mean you'll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you'll probably get the same answer you got last time, so the sampling distribution will be very narrow. You can see this visually in @fig-fig8-8, showing that the bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the **standard error**. The standard error of a statistic is often denoted SE, and since we're usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the picture, as the sample size $N$ increases, the SEM decreases. 

Okay, so that's one part of the story. However, there's something I've been glossing over so far. All my examples up to this point have been based on the "IQ scores" experiments, and because IQ scores are roughly normally distributed I've assumed that the population distribution is normal. What if it isn't normal? What happens to the sampling distribution of the mean? The remarkable thing is this, no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this I ran some simulations. To do this, I started with the "ramped" distribution shown in the histogram in @fig-fig8-9. As you can see by comparing the triangular shaped histogram to the bell curve plotted by the black line, the population distribution doesn't look very much like a normal distribution at all. Next, I simulated the results of a large number of experiments. In each experiment I took $N = 2$ samples from this distribution, and then calculated the sample mean. @fig-fig8-9 (b) plots the histogram of these sample means (i.e., the sampling distribution of the mean for $N = 2$). This time, the histogram produces a $\chi^2$-shaped distribution. It's still not normal, but it's a lot closer to the black line than the population distribution in @fig-fig8-9 (a). When I increase the sample size to $N = 4$, the sampling distribution of the mean is very close to normal (@fig-fig8-9 (c)), and by the time we reach a sample size of N = 8 it's almost perfectly normal. In other words, as long as your sample size isn't tiny, the sampling distribution of the mean will be approximately normal no matter what your population distribution looks like!--->

```{r}
#| label: fig-fig8-9
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 中央極限定理的視覺化示範。圖(a)是一個非常態分佈的母群分佈，而圖 (b) - (d) 展示從圖 (a) 取得樣本數分別為 2、4 和 8 的樣本平均值的取樣分佈。正如您所看到的，即使原始母群分佈不是常態分佈，隨著樣本數增加，樣本平均值的取樣分佈會趨近常態分佈。
#A demonstration of the central limit theorem. In panel (a), we have a non-normal population distribution, and panels (b)-(d) show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel (a). As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations
knitr::include_graphics("images/fig8-9.png")
```

根據以上視覺化展示，關於樣本平均值的取樣分佈，我們可以得出以下結論：

- 取樣分佈的平均值與母群的平均值相同。
- 隨著樣本數的增加，取樣分佈的標準差（即標準誤）越來越小。
- 隨著樣本數的增加，取樣分佈的形狀變得越來越接近常態分佈。

中央極限定理（Central Limit Theorem）是統計學中的一個著名定理，除了證明上述所有說法都是正確的。中央極限定理也告訴我們，假設母群的平均值為$\mu$，標準差為$\sigma$，那麼樣本平均值的取樣分佈的平均值也是$\mu$，而平均值的標準誤則是：

$$SEM=\frac{\sigma}{\sqrt{N}}$$

因為是母群標準差$\sigma$除以樣本大小 N 的平方根，因此SEM會隨著樣本數的增加而變小。中央極限定理還告訴我們，樣本平均值的取樣分佈形狀會變成常態分佈。[^08-estimating-unknown-quantities-from-a-sample-4]

[^08-estimating-unknown-quantities-from-a-sample-4]: 本書正文寫的描述都是有點簡化的。中心極限定理的應用範圍比這一節展示更普遍。像大多數介紹統計學的教科書一樣，本書只討論了一種中心極限定理成立的情況：就是從相同的分佈中取出很多獨立事件的平均值。然而，中心極限定理可適用更廣泛的情況。例如，有一種“U統計量數”，這類統計量都滿足中心極限定理的條件 ，因此在處理大樣本數的狀況，U統計量數的取樣分佈都會是常態分佈。平均值是一種符合定理的統計量數，但它不是唯一的。

中央極限定理在處理各種問題都很有用。它告訴我們為什麼大樣本實驗比小樣本實驗更可靠，而且因為有標準誤的明確公式，所以它還告訴我們大樣本實驗的可靠性有多高。它也解釋了為什麼常態分佈是正常的。在真正的實驗中，我們想要測量的許多事物實際上是綜合各種不同數值指標的平均值（例如，智力測驗所測量的“普遍”智力是很多“具體”技能和能力的平均值），遇到這種情況時，各種指標的平均值應該遵循常態分佈。因為中央極限定理，常態分佈在各種真實數據隨處可見。

<!---On the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean.

- The mean of the sampling distribution is the same as the mean of the population
- The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases
- The shape of the sampling distribution becomes normal as the sample size increases 


As it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the **central limit theorem**. Among other things, the central limit theorem tells us that if the population distribution has mean µ and standard deviation σ, then the sampling distribution of the mean also has mean µ and the standard error of the mean is


Because we divide the population standard deviation σ by the square root of the sample size N, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.[^08-estimating-unknown-quantities-from-a-sample-4]

[^08-estimating-unknown-quantities-from-a-sample-4]: As usual, I'm being a bit sloppy here. The central limit theorem is a bit more general than this section implies. Like most introductory stats texts I've discussed one situation where the central limit theorem holds: when you're taking an average across lots of independent events drawn from the same distribution. However, the central limit theorem is much broader than this. There's a whole class of things called "U-statistics" for instance, all of which satisfy the central limit theorem and therefore become normally distributed for large sample sizes. The mean is one such statistic, but it's not the only one.

This result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, "general" intelligence as measured by IQ is an average of a large number of "specific" skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data.--->

## 母群參數的點估計 {#sec-Estimating-point-of-parameters}

在前面的章節有關智力測驗分數的所有例子中，我們已經知道了母群的參數。就像在每一位大學生的心理測驗學第一堂課中所學的，智力測驗分數的平均值被定義為100，標準差為15。但這其實是有點取巧的。我們怎麼知道智力測驗分數的真實母群平均值是100？這是因為測驗的設計者對大樣本進行了測試，然後在測驗規則做點“手腳”，使得樣本平均值為100。當然，這不是什麼壞事，這是心理測學重要的一部分。但是，需要牢記的是，這個理論上的平均值100僅適用於測試設計者用來設計測驗的母群。優秀的測驗設計師實際上會花費一些心思來提供可以應用於許多不同母群（例如不同的年齡組、不同國籍等）的“測試常模”。

這樣設計很方便，但是幾乎每個有趣的研究都要能適用於研究不同於建立測試常模的其他人群。例如，假設您想要測量南澳大利亞州一個工業鎮Port Pirie中低水平的鉛中毒對認知功能的影響。也許您會想要將Port Pirie中的智力測驗分數與南澳另一個工業城市Whyalla的樣本進行比較。[^08-estimating-unknown-quantities-from-a-sample-5]不論您考慮的是哪個城鎮，僅僅假設真實的母群平均智商是100並不合理。在我所知道的範圍內，沒有人提出合理的測量數據，能適用於南澳大利亞的所有工業城鎮。因此，我們不得不通過樣本資料估計母群參數。

[^08-estimating-unknown-quantities-from-a-sample-5]: 請注意，如果你真的對想研究這個問題，你必須比我這裡描述的研究方式更加謹慎。不能只比較Whyalla和Port Pirie的智力測驗分數，並假設任何差異都是由鉛中毒引起的。即使這兩個城鎮之間的唯一差異確實是不一樣的精煉廠（實際情況不只如此），也需要考慮到人們已經相信鉛污染會導致認知偏誤。 復習一下@sec-A-brief-introduction-to-research-design 談到的動機效應，Port Pirie和Whyalla兩地的樣本很 可能存在不同的需求效應。換句話說，您可能會在數據中得出虛假的群體差異，因為人們認為存在真實差異。我認為這種想法相當不可信，如果一群穿著實驗袍的研究人員帶著智力測驗來到Port Pirie，當地人也許不會在乎你的真正目的，甚至會表現強烈的抗拒感，不會配合研究者進行測試。另一群Port Pirie的居民可能會有表現良好的動機，因為他們不希望他們的家鄉看起來很糟糕。Whyalla的動機效應可能會更弱，因為人們沒有任何關於“鐵礦污染”的概念，就像他們沒有“鉛污染”的概念一樣。心理學真的不是簡單的學問。

<!---In all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then "rigged" the scoring rules so that their sample has mean 100. That's not a bad thing of course, it's an important part of designing a psychological measurement. However, it's important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide "test norms" that can apply to lots of different populations (e.g., different age groups, nationalities etc).

This is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.[^08-estimating-unknown-quantities-from-a-sample-5] Regardless of which town you're thinking about, it doesn't make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We're going to have to **estimate** the population parameters from a sample of data. So how do we do this?

[^08-estimating-unknown-quantities-from-a-sample-5]: Please note that if you were actually interested in this question you would need to be a lot more careful than I'm being here. You can't just compare IQ scores in Whyalla to Port Pirie and assume that any differences are due to lead poisoning. Even if it were true that the only differences between the two towns corresponded to the different refineries (and it isn't, not by a long shot), you need to account for the fact that people already believe that lead pollution causes cognitive deficits. If you recall back to @sec-A-brief-introduction-to-research-design, this means that there are different demand effects for the Port Pirie sample than for the Whyalla sample. In other words, you might end up with an illusory group difference in your data, caused by the fact that people think that there is a real difference. I find it pretty implausible to think that the locals wouldn't be well aware of what you were trying to do if a bunch of researchers turned up in Port Pirie with lab coats and IQ tests, and even less plausible to think that a lot of people would be pretty resentful of you for doing it. Those people won't be as co-operative in the tests. Other people in Port Pirie might be more motivated to do well because they don't want their home town to look bad. The motivational effects that would apply in Whyalla are likely to be weaker, because people don't have any concept of "iron ore poisoning" in the same way that they have a concept for "lead poisoning". Psychology is hard. --->

### 母群平均值

假設我們前往 Port Pirie，邀請100位當地居民參加智力測驗。這些人的平均智力測驗分數為 $\bar{X}=98.5$。那麼整個 Port Pirie 的母群的智力測驗平均分數是多少呢？我們顯然無法知道。可能是 97.2，也可能是 103.5。由於我們的樣本並不是全部的母群，因此無法給出確定的答案。不過，如果非要我給一個“最好的猜測”，我會說是 98.5。這就是統計估計的精髓：給出最佳的猜測。

在這個例子中，未知的母群參數估計值是簡單明瞭的。直接用樣本平均值當成母群平均值的估計值。在下一節中，我將解釋這個類似直覺的答案的統計學理由。但是，現在同學們要確實了解**樣本統計量**和**母群參數估計值**是不同的概念。樣本統計量是您的數據描述，而估計值是對母群的猜測。考慮兩者的差異，統計學家通常使用不同的符號來指代它們。例如，真實母群平均值的符號為 $\mu$，母群平均值的估計值符號則是 $\hat{\mu}$ 。另一方面，樣本平均值符號為 $\bar{X}$，也可以寫成 m。不過，簡單隨機樣本的母群平均值之估計值與樣本平均值相同。如果我觀察到樣本平均值為 $\bar{X}=98.5$，則我對母群平均值的估計也是 $\hat{\mu}=98.5$。為了保持符號清晰，我整理成以下表格 (@tbl-tab8-1) ：

<!---Suppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be $\bar{X}=98.5$. So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don't know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn't exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a "best guess" I'd have to say 98.5. That's the essence of statistical estimation: giving a best guess.

In this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my **estimate of the population mean**. It's pretty simple, and in the next section I'll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted $\mu$, then we would use $\hat{mu}$ to refer to our estimate of the population mean. In contrast, the sample mean is denoted $\bar{X}$ or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of $\bar{X}=98.5$ then my estimate of the population mean is also $\hat{\mu}=98.5$. To help keep the notation clear, here's a handy table (@tbl-tab8-2):--->

```{r}
#| label: tbl-tab8-1
#| tbl-cap: 各種平均值符號釋義
#Notation for the mean
#huxtabs[[8]][[2]]  
data.frame(
    `符號` = "",
    `統計學名詞` = c("樣本平均值", "母群平均值", "母群平均值之估計值"),
    `從那裡來的？` = c("從樣本資料計算出來的", "完全無法知道的", "使用簡單隨機樣本就會等於樣本平均值")
) %>%
  kbl(booktabs=TRUE,escape = F, align = "c") %>%
  kable_paper(full_width = F) %>%
  column_spec(1, image = spec_image(c("images/tbl-8-2-xbar.png", "images/tbl-8-2-mu.png","images/tbl-8-2-muhat.png"),50,50))
```

### 母群標準差

到這裡，估計母群參數看起來相當簡單，但同學可能疑 為什麼要先理解取樣理論。當母群參數是平均值，估計值（$\hat{\mu}$）與相應的樣本統計量（$\bar{X}$）恰好相同。但這種關係不是通用的。為了說明這一點，讓我們思考如何估計母群標準差，數學符號是 $\hat{\sigma}$。我們該使用什麼作為母群標準差的估計值呢？第一個可能的想法是，我們可以像估計平均值那樣，使用樣本統計量作為估計值。這樣做幾乎是正確的，但是不完全正確。

以下是我的說明。假如我有一個只有一個觀察值的樣本，這個例子能讓
對母群參數真實數值一無所知的情況，比較容易理解。在此我們使用一些完全虛構的資料：假設這個觀察值是我的鞋子的“光滑度”，經過測量，我的鞋子“光滑度”是$20$。以下是這個樣本的描述：

這是一個完全合格的樣本，即使它只有$N=1$個觀察值。它的樣本平均值為$20$，並且每個觀察值都等於樣本平均值（這是多說的！），所以它的樣本標準差為0。作為對樣本的描述，這似乎是正確的，因為樣本只有了一個觀察值，因此在樣本內觀察不到任何變異。此例報告樣本標準差$s=0$是正確的。但是，作為母群標準差的估計值，這樣做完全不合理，對吧？即使同學們和我都對“光滑度”一無所知，但我們對於資料處理都有一些了解。我們看不到樣本內任何變異性的唯一原因是樣本太小而無法顯示任何變異性！因此，如果樣本大小為$N=1$，那麼關於母群標準差我們只能回答“根本不知道”。

留意一下，稍早談樣本平均值和母群平均值時，你並沒有察覺不對勁。如果一定要猜測母群平均值，猜母群平均值是 $20$ 並不感覺完全不合理。當然，同學可能對這樣的猜測自信程度比不上我，因為你看到樣本只有一個觀察值，但這依然是你能做出的最好猜測。

讓我們擴大一下這個樣本，假如現在有了第二個觀察值。我的資料集現在有 $N=2$ 個鞋子的"光滑度"觀察值，完整的樣本看起來像這樣：


<!---So far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. $\hat{\mu}$) turned out to identical to the corresponding sample statistic (i.e. $\bar{X}$). However, that's not always true. To see this, let's have a think about how to construct an **estimate of the population standard deviation**, which we'll denote $\hat{\sigma}$. What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That's almost the right thing to do, but not quite.

Here's why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let's use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of $20$. So here's my sample:

This is a perfectly legitimate sample, even if it does have a sample size of $N = 1$. It has a sample mean of $20$ and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the *sample* this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of $s = 0$ is the right answer here. But as an estimate of the *population* standard deviation it feels completely insane, right? Admittedly, you and I don't know anything at all about what "cromulence" is, but we know something about data. The only reason that we don't see any variability in the *sample* is that the sample is too small to display any variation! So, if you have a sample size of $N = 1$ it feels like the right answer is just to say "no idea at all".

Notice that you *don't* have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn't feel completely insane to guess that the population mean is $20$. Sure, you probably wouldn't feel very confident in that guess because you have only the one observation to work with, but it's still the best guess you can make.

Let's extend this example a little. Suppose I now make a second observation. My data set now has $N = 2$ observations of the cromulence of shoes, and the complete sample now looks like this:--->

$$20, 22$$

這一次的樣本大小剛好足夠大到能觀察一些變異性：兩個觀察值是能觀察到任何資料變異性的最低條件！以新資料集計算的樣本平均值是 $\bar{X} = 21$，樣本標準差是 $s=1$。那麼我們能如何猜測母群參數？同樣地，母群平均值的最佳猜測就是樣本平均值，也就是說“光滑度”的母群平均值為 $21$。那麼標準差呢？這就有點複雜了。樣本標準差只是基於兩個觀察值，如果同學學得夠認真，可能會覺得僅僅只有兩個觀察值，是不足以“充分展現” 真正的母群變異性。這不僅僅是估計值是否正確的問題，畢竟，只有兩個觀察值的估計值，我們能合理懷疑在某些程度是錯誤的。更要擔憂的問題是誤差是系統性的。具體而言，我們懷疑樣本標準差可能比真正的母群標準差小。

如果同學有這樣的警覺非常不錯，能夠證明這一點的話就更好。其實一些數學家已經證明了系統性誤差的直覺是對的，只是除非你有一定程度的數學知識，否則了解這些證明對我們學習統計沒有太大幫助。另一方面，我們可以做一些模擬實驗來展示系統性誤差。讓我們回到智力測驗分數研究：假設真正的母群平均智力測驗分數為100，標準差為15。第一份模擬實驗先測量2個智力測驗分數，然後計算樣本標準差。如果我做好幾遍模擬，然後繪製這些樣本標準差的直方圖，得到的就是標準差的取樣分配。 @fig-fig8-10 展示樣本標準差取樣分配的直方圖。儘管真實的母群標準差為15，樣本標準差的平均值只有8.5。請注意，這與我們在 @fig-fig8-8 (b)中繪製的平均值取樣分配的完全不同，我們設定母群平均值為100，所有樣本平均值的平均值也是100。


<!---This time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is $\bar{X} = 21$, and the sample standard deviation is $s = 1$. What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we'd probably guess that the population mean cromulence is $21$. What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you're at all like me you probably have the intuition that, with only two observations we haven't given the population "enough of a chance" to reveal its true variability to us. It's not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.

This intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don't help very much. Instead, what I'll do is simulate the results of some experiments. With that in mind, let's return to our IQ studies. Suppose the true population mean IQ is $100$ and the standard deviation is $15$. First I'll conduct an experiment in which I measure $N = 2$ IQ scores and I'll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I've plotted this distribution in @fig-fig8-10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in @fig-fig8-8 (b) when we plotted the sampling distribution of the mean, where the population mean is $100$ and the average of the sample means is also $100$.--->

```{r}
#| label: fig-fig8-10
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 此為「兩個智力測驗分數」模擬實驗的樣本標準差取樣分佈。虛線標示設定的母群標準差為15，但是從直方圖可以看出，大多數實驗所得的樣本標準差遠小於母群標準差。平均而言，這個實驗所產生的樣本標準差只有8.5，明顯低於母群標準差！換句話說，樣本標準差是母群標準差的偏誤估計值。
#The sampling distribution of the sample standard deviation for a 'two IQ scores' experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation
knitr::include_graphics("images/fig8-10.png")
```

現在讓我們擴展模擬實驗規模。不再只做於N = 2的情況，使用樣本大小從1到10，將這個模擬實驗重複進行。如果分別將各種樣本大小(橫軸)情況取得的樣本平均值和樣本標準差之平均值(縱軸)，繪製折線圖，就可以得到 @fig-fig8-11 的成品。其中圖（a）是樣本平均值的總體平均，圖（b）是樣本標準差的變化曲線。兩幅圖的意義非常不同：通常不論樣本大小，樣本平均值等於母群平均值，代表這是一種不偏的估計值，這就是為什麼母群平均值的最佳估計值是樣本平均值的原因 。[^08-estimating-unknown-quantities-from-a-sample-6] 圖（b）則顯示另一回事：多數情況的樣本標準差$s$總是小於母群標準差$\sigma$ ，代表這是一種有偏誤的估計值。也就是說，如果要為母群標準差$\hat{\sigma}$做出“最佳猜测” $\hat{\sigma}$，最好用比樣本標準差$s$大一點的數值。

[^08-estimating-unknown-quantities-from-a-sample-6]: 這裡有些細節我並未明確解釋。不偏性(unbiasedness)對於估計值來說是一個理想的特徵，但除了這個特徵之外，還有其他重要因素也要考量。然而，這方面的細節已經超出本書討論的範圍。我只是想提醒同學注意這裡存在一些尚未明說的複雜條件 。

<!---Now let's extend the simulation. Instead of restricting ourselves to the situation where $N=2$, let's repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in @fig-fig8-11. On the left hand side (panel (a)) I've plotted the average sample mean and on the right hand side (panel (b)) I've plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an **unbiased estimator**, which is essentially the reason why your best estimate for the population mean is the sample mean.[^08-estimating-unknown-quantities-from-a-sample-6] The plot on the right is quite different: on average, the sample standard deviation $s$ is smaller than the population standard deviation $\sigma$. It is a **biased estimator**. In other words, if we want to make a "best guess" $\hat{\sigma}$ about the value of the population standard deviation $\hat{\sigma}$ we should make sure our guess is a little bit larger than the sample standard deviation $s$. 

[^08-estimating-unknown-quantities-from-a-sample-6]: I should note that I'm hiding something here. Unbiasedness is a desirable characteristic for an estimator, but there are other things that matter besides bias. However, it's beyond the scope of this book to discuss this in any detail. I just want to draw your attention to the fact that there's some hidden complexity here.--->

```{r}
#| label: fig-fig8-11
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 模擬實驗結果證實樣本平均值是母群平均值的不偏估計值（圖a），但是樣本標準差是母群標準差的有偏誤估計值（圖b）。兩幅圖的每個點都是來自10,000筆模擬資料，首先每筆模擬資料都有1個觀察值，再生成10,000組2個觀察值、以此類推直到10個觀察值為止。每個資料都來自虛構的智力測驗分數母群參數設定，即符合常態分佈且母群平均值為100、 標準差為15。平均來看，不論樣本大小如何（圖a），取樣的樣本平均值幾乎 等於100。然而，取樣的的樣本標準差通常小於母群標準差，小樣本情況特别明顯（圖b）。
#An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated $10,000$ simulated data sets with 1 observation each, $10,000$ more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes
knitr::include_graphics("images/fig8-11.png")
```

解決這個系統性偏誤的方法其實非常簡單，以下說明如何解決。細談標準差之前，先回顧一下變異數。回顧一下[4.2.4 變異數](04-Descriptive-statistics.html#variance)所提到的，變異數是所有資料偏離平均值的平方和之平均值。也就是以下公式：

$$s^2=\frac{1}{N} \sum_{i=1}^{N}(X_i-\bar{X})^2$$

樣本變異數 $s^2$ 是母群變異數 $\sigma^2$ 的有偏誤估計量。不過在統計實務，我們只需要做個微小的調整，就可以將其轉換為不偏估計值。方法就是改成除以 $N-1$ 而不是除以 $N$。

(08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters)

修改後的公式就是母群變異數 $\sigma$ 的不偏估計量。這也回答了我們在[4.2.4 變異數](04-Descriptive-statistics.html#variance)遇到的問題。為什麼 jamovi 給我們的變異數稍有不同？那是因為 jamovi 計算的是 $\hat{\sigma}^2$ 而不是 $s^2$，標準差也是一樣。如果我們除以 $N-1$ 而不是 $N$，就能得到母群標準差的不偏估計值。請記住使用 jamovi 內建的標準差函數時，計算的是 $\hat{\sigma}$ 而不是 $s$。[^08-estimating-unknown-quantities-from-a-sample-7]

[^08-estimating-unknown-quantities-from-a-sample-7]: 除以 $N-1$ 可以得到一個不偏的母群變異數估計值：$$\hat{\sigma}^2=\frac{1}{N-1}\sum_{i=1}^{N}(X_i - \bar{X})^2$$ 以及不偏的母群標準差估計值：$$\hat{\sigma}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(X_i-\bar{X})^2}$$ 好啦，這裡我少講了另外一些東西。 經過一些怪異且違反直覺的推導，我們會認為取平方根是沒問題的，自然認為既然 $\hat{\sigma}^2$ 是 $\sigma^2$ 的不偏估計值，那麼$\hat{\sigma}$ 也是$\sigma$的不偏估計值。對吧？但奇怪的是並非如此。實際上，$\hat{\sigma}$ 存在一種微妙而極小的偏差。 這實在是太奇怪了：$\hat{\sigma}^2$ 是母群變異數 $\sigma^2$ 的不偏估計值，但是開根號後，$\hat{\sigma}$ 卻是母群標準差 $\sigma$ 的有偏誤估計值。很奇怪吧？那麼為什麼 $\hat{\sigma}$  存在偏誤呢？用數學術語來說，原因是「非線性轉換（例如平方根）不相容於期望值」，但是對於沒有修過數理統計課程的同學來說，這聽起來就像外星語言。幸運地是，在實際情況，了不了解這一點並不重要。因為偏誤非常微小，在大多數統計實務使用 $\hat{\sigma}$ 做為母群標準差估計值不會有太多問題。有時候試著洞察數學原理只是令人惱火罷了。

最後一點，很多人進行統計實務都習慣稱呼 $\hat{\sigma}$（分母是 $N-1$ 的標準差公式）為樣本標準差。嚴格來說這並不正確。樣本標準差應該是$s$（分母是$N$ 的標準差公式）。兩者在概念和數值的意義都不相同。前者是一種母群標準差的估計值，後者是一種樣本性質。不過，幾乎所有現實世界的統計問題都是要找出母群參數的合理估計值，因此多數報告都是呈現$\hat{\sigma}$，而非$s$。雖然這是正確的報告方式，只是多數人在撰寫報告時，對於術語往往不夠精確，因為「樣本標準差」寫起來比「估計母體標準差」字比較少。這並不是嚴重的問題，實際上我也和其他人一樣這樣使用詞彙。儘管如此，我認為了解兩者在概念上的差異非常重要。混淆「已知樣本的已知屬性」和「透過樣本對母群的猜測」永遠都不是一件好事。當你開始認為 $s$ 和 $\hat{\sigma}$ 是相同的東西時，你就要好好拿出這本書複習了。

最後，我將這一節的重要符號與概念整理一下(@tbl-tab8-2 與 @tbl-tab8-3)。

<!---The fix to this systematic bias turns out to be very simple. Here's how it works. Before tackling the standard deviation let's look at the variance. If you recall from the section on [Estimating population parameters], the sample variance is defined to be the average of the squared deviations from the sample mean. That is: $$s^2=\frac{1}{N} \sum_{i=1}^{N}(X_i-\bar{X})^2$$ The sample variance $s^2$ is a biased estimator of the population variance $\sigma^2$. But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by $N-1$ rather than by $N$.
 
This is an unbiased estimator of the population variance $\sigma$. Moreover, this finally answers the question we raised in [Estimating population parameters]. Why did jamovi give us slightly different answers for variance? It's because jamovi calculates $\hat{\sigma}^2 \text{ not } s^2$, that's why. A similar story applies for the standard deviation. If we divide by $N - 1$ rather than $N$ our estimate of the population standard deviation is unbiased, and when we use jamovi's built in standard deviation function, what it's doing is calculating $\hat{\sigma}$ not $s$.[^08-estimating-unknown-quantities-from-a-sample-7] 

[^08-estimating-unknown-quantities-from-a-sample-7]: 
Dividing by $N-1$ gives us an unbiased estimate of the population variance: $$\hat{\sigma}^2=\frac{1}{N-1}\sum_{i=1}^{N}(X_i - \bar{X})^2$$, and similarly for standard deviation: 
$$\hat{\sigma}=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(X_i-\bar{X})^2}$$. Okay, I'm hiding something else here. In a bizarre and counter-intuitive twist, since $\hat{\sigma}^2$ is an unbiased estimator of $\sigma^2$, you'd assume that taking the square root would be fine and $\hat{\sigma}$ would be an unbiased estimator of σ. Right? Weirdly, it's not. There's actually a subtle, tiny bias in $\hat{\sigma}$. This is just bizarre: $\hat{\sigma}^2$ is an unbiased estimate of the population variance $\sigma^2$ , but when you take the square root, it turns out that $\hat{\sigma}$ is a biased estimator of the population standard deviation σ. Weird, weird, weird, right? So, why is $\hat{\sigma}$ biased? The technical answer is "because non-linear transformations (e.g., the square root) don't commute with expectation", but that just sounds like gibberish to everyone who hasn't taken a course in mathematical statistics. Fortunately, it doesn't matter for practical purposes. The bias is small, and in real life everyone uses $\hat{\sigma}$ and it works just fine. Sometimes mathematics is just annoying.

One final point. In practice, a lot of people tend to refer to $\hat{\sigma}$ (i.e., the formula where we divide by $N - 1$) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren't the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report $\hat{\sigma}$ rather than s. This is the right number to report, of course. It's just that people tend to get a little bit imprecise about terminology when they write it up, because "sample standard deviation" is shorter than "estimated population standard deviation". It's no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it's important to keep the two concepts separate. It's never a good idea to confuse "known properties of your sample" with "guesses about the population from which it came". The moment you start thinking that $s$ and $\hat{\sigma}$ are the same thing, you start doing exactly that.

To finish this section off, here's another couple of tables to help keep things clear (@tbl-tab8-3 and @tbl-tab8-4).--->

```{r}
#| label: tbl-tab8-2
#| tbl-cap: 各種標準差符號釋義
#Notation for standard deviation
#huxtabs[[8]][[3]] 
data.frame(
    `符號` = "",
    `統計學名詞` = c("樣本標準差", "母群標準差", "母群標準差之估計值"),
    `從那裡來的？` = c("從樣本資料計算出來的", "完全無法知道的", "可使用簡單隨機樣本計算，但是不會等於樣本標準差")
) %>%
  kbl(booktabs=TRUE,escape = F, align = "c") %>%
  kable_paper(full_width = F) %>%
  column_spec(1, image = spec_image(c("images/tbl-8-2-s.png", "images/tbl-8-2-sigma.png","images/tbl-8-2-sigmahat.png"),50,50))
```

```{r}
#| label: tbl-tab8-3
#| tbl-cap: 各種變異數符號釋義
#Notation for variance
#huxtabs[[8]][[4]] 
data.frame(
    `符號` = "",
    `統計學名詞` = c("樣本變異數", "母群變異數", "母群變異數之估計值"),
    `從那裡來的？` = c("從樣本資料計算出來的", "完全無法知道的", "可使用簡單隨機樣本計算，但是不會等於樣本變異數")
) %>%
  kbl(booktabs=TRUE,escape = F, align = "c") %>%
  kable_paper(full_width = F) %>%
  column_spec(1, image = spec_image(c("images/tbl-8-2-ssquare.png", "images/tbl-8-2-sigmasquare.png","images/tbl-8-2-sigmasquarehat.png"),50,50))
```

## 母群參數的區間估計 {#sec-Estimating-a-confidence-interval}

> *學習統計的最大收穫是永遠不再說「我肯定這就是答案」*\
> -- 來源不明[^08-estimating-unknown-quantities-from-a-sample-8]

[^08-estimating-unknown-quantities-from-a-sample-8]: 此箴言的原文散見於許多網站及客製T恤，但是原作者無法找到原始來源。

這一章到目前為止，已經概述了統計學家常取用的基本取樣理論，用來根樣本資料猜測母群參數。正如己經解說的各節主題，我們需要取樣理論的原因之一是每個資料集都會有一些不確定性，因此估計值永遠不會百分之百準確。這一節我們要學的是量化估計不確定程度的方法，因為只報告像是大學心理學專業學生平均智商為115這要的點估計值並不足夠，我們還要能用數值表達猜測程度。例如：有95％ 的可能性，真實平均值介於109和121之間。這種表達方式被稱做平均值的信賴區間。

充分理解取樣分佈，計算平均值的信賴區間其實相當簡單。這裡解說計算方法：假設母群平均值為 $\mu$，標準差為 $\sigma$。我剛完成了一項有 N 名參與者的研究，這些參與者的智力測驗分數的平均值是 $\bar{X}$。根據[中央極限定理](08-Estimating-unknown-quantities-from-a-sample.html#sec-The-central-limit-theorem)，我們可以知道平均值的取樣分佈接近常態分佈。經由 @sec-The-normal-distribution 關於常態分佈的學習，我們也知道一個服從常態分佈的取樣分佈，在平均值周圍相距約兩個標準差之內，有95％左右的量數落在此範圍內。

更精確的正確答案是：以一個符合常態分佈的取樣分佈估計母群平均值，估計值有95% 的機率落在真正母群平均值的1.96個標準差內。接下來，請記住取樣分配的標準差正式名稱為標準誤(standar error)，因此平均值的標準誤差可簡寫為SEM(Standard Error of Measurement, 測量標準誤)。當我們合成這些數字時，就能得知有95%的可能性確定由樣本資料計算得到的樣本平均值 $\bar{X}$ ，與母群平均值差距在1.96倍的標準誤差之內。

當然，1.96並不是什麼神奇數字。如果你想要計算一個95％的信賴區間，這就是經常需要使用的乘數。若是我想要70％的置信區間，我會使用1.04作為神奇數字而不是1.96。

[更多技術細節[^08-estimating-unknown-quantities-from-a-sample-9]]

[^08-estimating-unknown-quantities-from-a-sample-9]: 信賴區間的數學公式是：$$\mu-(1.96 \times SEM ) \leq \bar{X} \leq \mu + (1.96 \times SEM)$$ 其中SEM等於$\frac{\sigma}{\sqrt{N}}$，而且我們有95％的信心這段數值範圍是正確的。然而，這並沒有回答我們真正感興趣的問題。以上公式告訴我們，如果我們想知道母群參數是多少，那麼應該計算所樣樣本平均值的期望值。但是，假如要用信賴區間回答這個問題，也就是想知道，計算特定樣本資料後，應該要多相信母群參數是多少，這並不難做到。使用一些高中學過的代數知識就可以巧妙地以上公式推 導如下：$$\bar{X}-(1.96 \times SEM ) \leq \mu \leq \bar{X}+(1.96\times SEM )$$ 這公式透露出這段數值範圍有95％的機率包含母群平均值$\mu$。我們稱此範圍為95％信賴區間，簡寫為$CI_{95}$。簡而言之，只要N足夠大（足夠讓我們相信平均值的取樣分佈符合常態分佈），那麼我們可以將95％信賴區間的公式變得更精簡：$$CI_{95}=\bar{X} \pm (1.96 \times \frac{\sigma}{\sqrt{N}})$$


<!---
> *Statistics means never having to say you're certain*\
> -- Unknown origin [^08-estimating-unknown-quantities-from-a-sample-8]

[^08-estimating-unknown-quantities-from-a-sample-8]: This quote appears on a great many t-shirts and websites, and even gets a mention in a few academic papers (e.g., <a href="http://www.amstat.org/publications/jse/v10n3/friedman.html"
    target="_blank">http://www.amstat.org/publications/jse/v10n3/friedman.html</a> , but I've never found the original source. 

Up to this point in this chapter, I've outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with a some of uncertainty, so our estimates are never going to be perfectly accurate. The thing that has been missing from this discussion is an attempt to quantify the amount of uncertainty that attaches to our estimate. It's not enough to be able guess that, say, the mean IQ of undergraduate psychology students is $115$ (yes, I just made that number up). We also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a $95\%$ chance that the true mean lies between $109$ and $121$. The name for this is a **confidence interval** for the mean.

Armed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Here's how it works. Suppose the true population mean is $\mu$ and the standard deviation is $\sigma$. I've just finished running my study that has N participants, and the mean IQ among those participants is $\bar{X}$. We know from our discussion of [The central limit theorem] that the sampling distribution of the mean is approximately normal. We also know from our discussion of the normal distribution in @sec-The-normal-distribution that there is a $95\%$ chance that a normally-distributed quantity will fall within about two standard deviations of the true mean.

To be more precise, the more correct answer is that there is a $95\%$ chance that a normally distributed quantity will fall within $1.96$ standard deviations of the true mean. Next, recall that the standard deviation of the sampling distribution is referred to as the standard error, and the standard error of the mean is written as SEM. When we put all these pieces together, we learn that there is a 95% probability that the sample mean $\bar{X}$ that we have actually observed lies within $1.96$ standard errors of the population mean.

Of course, there's nothing special about the number $1.96$. It just happens to be the multiplier you need to use if you want a $95\%$ confidence interval. If I'd wanted a $70\%$ confidence interval, I would have used $1.04$ as the magic number rather than $1.96$.

[Additional technical detail [^08-estimating-unknown-quantities-from-a-sample-9]]

[^08-estimating-unknown-quantities-from-a-sample-9]: Mathematically, we write this as: $$\mu-(1.96 \times SEM ) \leq \bar{X} \leq \mu + (1.96 \times SEM)$$ where the SEM is equal to $\frac{\sigma}{\sqrt{N}}$ N and we can be 95% confident that this is true. However, that's not answering the question that we're actually interested in. The equation above tells us what we should expect about the sample mean given that we know what the population parameters are. What we want is to have this work the other way around. We want to know what we should believe about the population parameters, given that we have observed a particular sample. However, it's not too difficult to do this. Using a little high school algebra, a sneaky way to rewrite our equation is like this: $$\bar{X}-(1.96 \times SEM ) \leq \mu \leq \bar{X}+(1.96 \times SEM )$$ What this is telling is is that the range of values has a 95% probability of containing the population mean µ. We refer to this range as a **95% confidence interval**, denoted $CI_{95}$. In short, as long as N is sufficiently large (large enough for us to believe that the sampling distribution of the mean is normal), then we can write this as our formula for the 95% confidence interval: $$CI_{95}=\bar{X} \pm (1.96 \times \frac{\sigma}{\sqrt{N}})$$

Of course, there's nothing special about the number 1.96. It just happens to be the multiplier you need to use if you want a 95% confidence interval. If I'd wanted a 70% confidence interval, I would have used 1.04 as the magic number rather than 1.96.--->

### 解讀信賴區間



學習信賴區間最困難的地方是理解其意義。很多學生第一次接觸信賴區間時，總是憑直覺說“真實平均值有95％的機率落在信賴區間內”。這種說法很簡單，似乎捕捉到“我有95％的自信”的常識想法。不幸的是，這並不完全正確。直覺定義非常依賴個人對母群平均值的個人價值觀。我之所以會說我有95％的自信，是因為這樣的說法是基於我的個人信仰。在日常生活這樣講沒什麼問題，但是回顧一下 **[如何解讀機率？](07-Introduction-to-probability.html#sec-What-does-probability-mean)**你會注意到談論個人信念和自信心的統計觀點是**貝氏統計**的中心思想。然而，信賴區間不是基於貝氏觀點開發出來的工具。就像這一章各節介紹的其他估計方法，信賴區間是**次數主義學派**開發的工具；如果要使用次數主義學派的方法，用貝氏觀點解釋就不對盤了！

好吧，如果這樣子不是正確回答，那麼要怎麼回答呢？記得討論次數主義如何解讀機率的時候，唯一“陳述機率”的合法方式是列舉一系列可觀察的機率事件，再計算各種事件的出現次數。由此看來，解釋95％信賴區間必須根據**次數**。具體而言，如果我們反覆執行同樣的實驗程序，並計算每次實驗結果的95％信賴區間，大約會有95％的實驗結果計算的區間會包含真正的平均值。更進一步的說法是，使用這樣的實驗程序得到的所有資料樣本信賴區間，應該會有95％包括真正的母群平均值 。@fig-fig8-12 用模擬實驗結果的視覺化說明這樣的概念，兩幅圖各顯示50個信賴區間，分別表示“10位受測者的智力測驗分數”（圖a）和“25位受測者的智力測驗分數”（圖b）。有點幸運的是，在這100次模擬實驗結果裡，恰好有95次包含了真實平均值。

這與貝氏觀點的關鍵差異在於，貝氏認為我們能用一個機率值，表達我們看不到的母群平均值有多麼不確定，但是次數主義學派不允許這樣解讀機率，因為沒有人能“復刻”母群！次數主義學派主張母群平均值是固定的，不能母群做任何機率式的陳述。不過，資料樣本的信賴區間是可重覆取得及計算的，所以我們可以不斷重做實驗。

因此，*次數主義學派*允許使用信賴區間（一種隨機變數），表達包括真實平均值的機率，但不允許估計真實母群平均值（不可重覆觀察的事件）落在置信區間內的概率。我知道這似乎有點拘泥於細節，但這確實很重要。它之所以重要是因為解釋方式的差異導致了數學方法的差異。貝氏統計有一種替代信賴區間的方法，稱為*可信區間*(Credible intervals)。在大多數情況下，可信區間與信賴區間的數值範圍非常接近，但在其他情況下可能會截然不同。在本書@sec-Bayesian-statistics，我們將學習更多貝氏統計觀點。


<!---The hardest thing about confidence intervals is understanding what they mean. Whenever people first encounter confidence intervals, the first instinct is almost always to say that "there is a 95% probability that the true mean lies inside the confidence interval". It's simple and it seems to capture the common sense idea of what it means to say that I am "95% confident". Unfortunately, it's not quite right. The intuitive definition relies very heavily on your own personal beliefs about the value of the population mean. I say that I am 95% confident because those are my beliefs. In everyday life that's perfectly okay, but if you remember back to the the section [What does probability mean?], you'll notice that talking about personal belief and confidence is a Bayesian idea. However, confidence intervals are not Bayesian tools. Like everything else in this chapter, confidence intervals are frequentist tools, and if you are going to use frequentist methods then it's not appropriate to attach a Bayesian interpretation to them. If you use frequentist methods, you must adopt frequentist interpretations! Okay, so if that's not the right answer, what is? Remember what we said about frequentist probability. The only way we are allowed to make "probability statements" is to talk about a sequence of events, and to count up the frequencies of different kinds of events. From that perspective, the interpretation of a 95% confidence interval must have something to do with replication. Specifically, if we replicated the experiment over and over again and computed a 95% confidence interval for each replication, then 95% of those intervals would contain the true mean. More generally, 95% of all confidence intervals constructed using this procedure should contain the true population mean. This idea is illustrated in @fig-fig8-12, which shows 50 confidence intervals constructed for a "measure 10 IQ scores" experiment (top panel) and another 50 confidence intervals for a "measure 25 IQ scores" experiment (bottom panel). A bit fortuitously, across the 100 replications that I simulated, it turned out that exactly 95 of them contained the true mean. The critical difference here is that the Bayesian claim makes a probability statement about the population mean (i.e., it refers to our uncertainty about the population mean), which is not allowed under the frequentist interpretation of probability because you can't "replicate" a population! In the frequentist claim, the population mean is fixed and no probabilistic claims can be made about it. Confidence intervals, however, are repeatable so we can replicate experiments. Therefore a *frequentist* is allowed to talk about the probability that the *confidence interval* (a random variable) contains the true mean, but is not allowed to talk about the probability that the *true population mean* (not a repeatable event) falls within the confidence interval I know that this seems a little pedantic, but it does matter. It matters because the difference in interpretation leads to a difference in the mathematics. There is a Bayesian alternative to confidence intervals, known as *credible intervals*. In most situations credible intervals are quite similar to confidence intervals, but in other cases they are drastically different. As promised, though, I'll talk more about the Bayesian perspective in @sec-Bayesian-statistics. --->

```{r}
#| label: fig-fig8-12
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 95％信賴區間視覺化。圖（a）顯示50次模擬實驗的結果，每次實驗測量10人的智力測驗分數。一個點表示一次實驗的樣本平均值，一個線條表示95％信賴區間。其中有47個信賴區間包含期望的平均值（即100），標示星號的三個區間則沒有包含。圖（b）展示另一項類似的模擬實驗，每次實驗測量25人的智力測驗分數。
#95% confidence intervals. The top (panel a) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean and the line shows the 95% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel b) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people
knitr::include_graphics("images/fig8-12.png")
```

### 計算信賴區間

jamovi 的**描述統計**模組內建計算信賴區間的簡單設定。開啟'Descriptives'面板的'Statistics'子選單，最下面有兩個勾選框，分別是'Std. Error of Mean'（平均值標準誤差）和'Confidence interval for the mean'（平均值信賴區間），同學可以使用這些功能計算每個平均值的95％信賴區間（預設值）。例如，載入 IQsim.omv 檔案並勾選'Confidence interval for the mean'，就可以得到與智力測驗模擬實驗平均分數的信賴區間：下限95％CI = 99.39 和上限95％CI = 99.97。也就是大樣本數據（N=10,000）的模擬結果顯示，平均智商得分為99.68，95％ CI 為99.39至99.97。

假如要使用 jamovi 繪製信賴區間的統計圖，可以指定平均值作為箱形圖選項之一。此外，在學習特定統計方法時（例如 @sec-Comparing-several-means-one-way-ANOVA ），我們還可以將信賴區間作為分析報表的一部分。這很酷，稍後我們會學習如何做到。

<!---jamovi include a simple way to calculate confidence intervals for the mean as part of the 'Descriptives' functionality. Under 'Descriptives'-'Statistics' there is a check box for both the 'Std. error of Mean' and 'Confidence interval for the mean', so you can use this to find out the 95% confidence interval (which is the default). So, for example, if I load the IQsim.omv file, check 'Confidence interval for the mean', I can see the confidence interval associated with the simulated mean IQ: Lower 95% CI = 99.39 and Upper 95% CI = 99.97 So, in our simulated large sample data with N=10,000, the mean IQ score is 99.68 with a 95% CI from 99.39 to 99.97. 

When it comes to plotting confidence intervals in jamovi, you can specify that the mean is included as an option in a box plot. Moreover, when we get onto learning about specific statistical tests, for example in @sec-Comparing-several-means-one-way-ANOVA, we will see that we can also plot confidence intervals as part of the data analysis. That's pretty cool, so we'll show you how to do that later on. --->

## 本章小結

這一章涵蓋兩大主題。前半部談取樣理論，後半部談運用取樣理論估計母群參數的方法。兩大主題分為幾個小主題：

- 有關[樣本、母群、取樣](08-Estimating-unknown-quantities-from-a-sample.html#sec-sample-population-sampling)的基本概念
- 取樣的機率理論：[大數法則](08-Estimating-unknown-quantities-from-a-sample.html#%E5%A4%A7%E6%95%B8%E6%B3%95%E5%89%87)、[樣本分佈與中央極限定理](08-Estimating-unknown-quantities-from-a-sample.html#%E6%A8%A3%E6%9C%AC%E5%88%86%E4%BD%88%E8%88%87%E4%B8%AD%E5%A4%AE%E6%A5%B5%E9%99%90%E5%AE%9A%E7%90%86)
- [母群參數的點估計](08-Estimating-unknown-quantities-from-a-sample.html#%E6%AF%8D%E7%BE%A4%E5%8F%83%E6%95%B8%E7%9A%84%E9%BB%9E%E4%BC%B0%E8%A8%88)討論平均值及標準差的機率意義。
- [母群參數的區間估計](08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval)

還有許多取樣與估計的主題尚未談到，本章對於心理學及統計的初學者應該是能夠理解且能吸收的。而且應用取向的心理學者不大需要深入了解統計理論。唯一你可能需要了解但本章未探討的問題，是分析的資料不是來自隨機樣本的狀況。其實已經有許多處理非隨機樣本的統計理論，不過已經超出本書範圍了。

<!---
In this chapter I've covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:

- Basic ideas about [Samples, populations and sampling]
- Statistical theory of sampling: [The law of large numbers] and [Sampling distributions and the central limit theorem]
- [Estimating population parameters]. Means and standard deviations
- [Estimating a confidence interval]

As always, there's a lot of topics related to sampling and estimation that aren't covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won't need much more theory than this. One big question that I haven't touched on in this chapter is what you do when you don't have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it's well beyond the scope of this book.
--->
