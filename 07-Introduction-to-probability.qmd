# 機率入門 {#sec-Introduction-to-probability}

```{r}
#| include: FALSE
source("header.R")
```

> [導讀簡報](slides7.html){target="_blank"}


<!---

> *[God] has afforded us only the twilight ... of Probability.*\
> -- John Locke


Up to this point in the book we've discussed some of the key ideas in experimental design, and we've talked a little about how you can summarise a data set. To a lot of people this is all there is to statistics: collecting all the numbers, calculating averages, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics and one of the least powerful. The bigger and more useful part of statistics is that it provides information that lets you make inferences about data.

Once you start thinking about statistics in these terms, that statistics is there to help us draw inferences from data, you start seeing examples of it everywhere. For instance, here's a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):

> "I have a tough job," the Premier said in response to a poll which found her government is now the most unpopular Labor administration in polling history, with a primary vote of just 23 per cent.

This kind of remark is entirely unremarkable in the papers or in everyday life, but let's have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I'm too lazy to track down the original survey so let's just imagine that they called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those claimed that they intended to vote for the Australian Labor Party (ALP). For the 2010 Federal election the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?

The answer to the question is pretty obvious. If I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the only 230 people out of the entire voting public who actually intend to vote ALP. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it's a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.

**Inferential statistics** provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of **probability theory**. And it is to probability theory that we must now turn. This discussion of probability theory is basically background detail. There's not a lot of statistics per se in this chapter, and you don't need to understand this material in as much depth as the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it's worth covering some of the basics.
--->


> _(上帝)只允許我們置身於...機率的微光之中。_

> -- 約翰·洛克

到目前為止,本書已介紹過一些實驗設計的關鍵原則,並且稍微介紹如何製作一個資料集總結報告。對於許多人來說,統計學就是這些工作的總和:收集所有數字,計算平均值,繪製統計圖,再將這些素材整理在一份報告中裡。這些工作有點像是做數字的拼貼畫，然而，統計學的作用遠多於這些工作。事實上,描述統計學只是統計學最小的一部分,也是功能最不強大的一部分。統計學更有用的一部分是提供以資料進行推論的訊息。

一旦你能從推論的角度來看待統計學,認為統計學是幫助我們從資料中歸納出結論,就會在許多資訊媒介上看到各式各樣能用統計學處理的例子。例如,以下是來自《雪梨晨鋒報》(2010年10月30日)的簡短報導摘錄:  

> “我現在的工作很艱困,”面對一項表明她的政府現在是歷史上最不受歡迎的工黨政府的民調結果,總理如是說。預估初選執政黨能獲得23%的選票。  

這種報導在報紙或日常生活中並不特別吸引人,但讓我們思考裡面所提示的資訊。民調機構做的民意調查,通常是相當大規模的,因為這些機構有資金。由於原作者太懶了,不想查找原始資訊,所以讓我們假裝這家機構隨機調查了1000位住在新南威爾斯(NSW)的選民,其中230人(23%)聲稱他們打算投票給澳大利亞工黨(ALP)。根據澳大利亞選舉委員會的報告,2010年聯邦大選登記在新南威爾斯投票的選民有4,610,795位,所以其餘4,609,795名選民(約99.98%的選民)的意見對我們來說是一片混沌。即使就當沒有人在民調中說謊,我們可以100%確定的是,ALP真正的初選得票數就是在230/4610795(約0.005%)和4610025/4610795(約99.83%)之間。那麼看了民調結果的民調機構、報社和讀者得出ALP的初選得票大約只有23%的結論是依據什麼?   

這個問題的答案很明顯。如果我隨機打電話給1000人,其中230人表示打算投ALP，這些人很可能並不是全體選民裡真正要投給ALP的唯一230人。換句話說,我們假設民調機構收集的資料能代表所有有投票權的選民。但是究竟有多少代表性呢?如果開票後，ALP得到的初選票實際上是24%? 甚至是29%或 37%? 這時候只憑直覺的想法會開始有點動搖。沒有人會對24%感到驚訝,但是會對37%感到驚訝,然而很難說29%會讓人相信民調結果。在這個狀況，我們需要比查看數字和直覺猜測更強大的工具。  

**推論統計學**提供我們回答這些問題所需要的工具,而且這類問題就是科學研究工作的核心,每門簡介統計學和研究方法課程的一大部分單元都是在學推論統計。然而,推論統計的理論基礎是建立在**機率論**，因此這個單元要討論機率論，內容是了解機率論必須掌握的背景知識。本單元沒有太多統計學,讀者也不用像其他單元一樣，需要深入理解這個單元的所有內容。儘管如此,由於機率論是推論統計的重要基礎，認識一些基本的機率論知識是值得的。


## 機率和統計有什麼不一樣？

<!--- teaching note:
要用數字回答的問題，需不需要用「機率模型」解決？必須用「機率模型」解決的數字問題，一定是機率問題。 
要用數字回答的問題，只要找到合理方法排列數字就能解決，只是（描述）統計問題。例如1854年倫敦蘇活區霍亂疫情。
各式各樣的機率分佈，就是「機率模型」。
--->

<!---
Before we start talking about probability theory, it's helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they're not identical. Probability theory is "the doctrine of chances". It's a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:


- What are the chances of a fair coin coming up heads 10 times in a row?
- If I roll a six sided dice twice, how likely is it that I'll roll two sixes?
- How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?
- What are the chances that I'll win the lottery?

Notice that all of these questions have something in common. In each case the "truth of the world" is known and my question relates to the "what kind of events" will happen. In the first question I know that the coin is fair so there's a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known **model** of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:

$$P(head)=0.5$$

which you can read as "the probability of heads is 0.5". As we'll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don't actually know exactly what's going to happen. Maybe I'll get 10 heads, like the question says. But maybe I'll get three heads. That's the key thing. In probability theory the model is known but the data are not.

So that's probability. What about statistics? Statistical questions work the other way around. In statistics we <u>do not</u> know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:

-   If my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?
-   If five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?
-   If the lottery commissioner's spouse wins the lottery how likely is it that the lottery was rigged?

This time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:

H H H H H H H H H H H

and what I'm trying to do is work out which "model of the world" I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as $P(heads)\ne{0.5}$. In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn't the same as the probability question, but they're deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works. --->

開始談論機率論之前,先花點時間思考機率與統計學之間的關係是很有幫助的。這兩門學科關係密切,但是兩者是不一樣的。機率論是“隨機事件的理論”，是數學的一個分支,用來探討不同種類的事件發生的次數/頻率。例如,使用機率論可以回答以下這些問題:

- 一枚公平的硬幣連續擲出正面10次的機率是多少?  

- 如果我擲兩次6面骰子,擲出兩次都是6點的可能性有多大?

- 從完美洗牌的撲克牌組中抽出5張全部都是紅心的牌可能性有多大?  

- 這一期我買的彩劵中獎的機率是多少?  

這些問題都有一些共同點。在所有可預料會發生的結果,“事實宇宙”是可知的,問題的答案與“什麼樣的事件”會發生有關。 第一個問題,我知道硬幣是公平的,所以每次硬幣正面朝上都有50%的機率。第二個問題,我知道一枚骰子擲到6的機率為1/6。第三個問題,我知道洗牌後的撲克牌，抽到那一張的機率都是1/52。第四個問題,我知道彩劵中獎遵循發行商訂下的規則。共同關鍵是每個機率問題都是來自已知的世界**模型**,使用模型做點計算，我們就可以知道答案。 

機率模型可以非常簡單。以擲硬幣的問題來說,模型可以寫成這樣:  

$$P(\text{正面})=0.5$$  

我們可以用白話講“擲出正面的機率是0.5”。 正如稍後會看到的,就像表達百分比只有用0%到100%之間的任何一個數字,機率只用從0到1之間的任何數字。 使用這個機率模型回答第一個問題時,其實我並不知道會發生什麼事。也許會像問題所問的,我真的會擲出10次正面，但也許我會擲出3次正面。這就是關鍵所在：在機率論的世界,模型是已知的,但是資料是未知的。  

那麼統計學呢?統計學解決問題的原則剛好相反。在統計學世界裡,我們並不知道世界的真實樣貌。唯一能掌握的只有資料,我們想要從資料中獲得世界的真實樣貌。用統計學問同樣的四道問題，會像是這樣:  

- 如果我的朋友在我的面前擲了10次硬幣全是正面,這是在耍我嗎?

- 如果莊家今天用的6面骰子，連續兩次都是6點，這顆骰子有機關的可能性有多大?

- 如果撲克牌組最前面的五張牌都是紅心,洗牌不夠徹底的可能性有多大?  

- 如果彩劵行的老闆娘中了頭獎，這期開獎有人作假的可能性有多大?

在這裡我們唯一擁有的就只有資料。我能知道的是,我看到我的朋友擲硬幣10次,每次都是正面。我能做推論的是我應不應該相信眼前看到的是使用公平硬幣而出現的結果，還是應該懷疑我的朋友在耍我。這個場景我得到的資料如下:  

正面 正面 正面 正面 正面 正面 正面 正面 正面 正面  

我要努力弄清楚我應該相信哪種“世界模型”。 如果硬幣是公平的,那麼我應該相信的模型是正面機率為0.5,即 $P(\text{正面}) = 0.5$。 如果硬幣是有機關的,那麼我應該判斷出現正面的機率不是0.5,可以寫成 $P(\text{正面})\neq0.5$。 換句話說,使用推論統計的目標是要弄清楚那一種機率模型是正確的。統計問題與機率問題顯然不是一樣的,但是兩者之間有深刻的關聯。因此,要充分學懂統計理論，就要從認識什麼是機率及運算機率的定理開始。  



## 如何解讀機率？ {#sec-What-does-probability-mean}


<!--- teaching note:
探討「歸納之謎」，如何估計未知的事件發生機率，你認為W君與C君誰的想法合理？
C君 = 次數主義觀點 ~ 只有長期觀察的結果才能掌握發生機率。標示相對發生次數的統計量是解答的元素。
W君 = 貝氏主義觀點 ~ 掌握每次試驗結果，更新事件發生機率的信心。事件發生的機率是解答的元素。
--->

<!--- Let's start with the first of these questions. What is "probability"? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there's much less of a consensus on what the word really means. It seems weird because we're all very comfortable using words like "chance", "likely", "possible" and "probable", and it doesn't seem like it should be a very difficult question to answer. But if you've ever had that experience in real life you might walk away from the conversation feeling like you didn't quite get it right, and that (like many everyday concepts) it turns out that you don't really know what it's all about.

So I'll have a go at it. Let's suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:

- They're robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.
- For any given game, I would agree that betting on this game is only "fair" if a \$1 bet on C Milan gives a \$5 payoff (i.e. I get my \$1 back plus a \$4 reward for being correct), as would a \$4 bet on Arduino Arsenal (i.e., my \$4 bet plus a \$1 reward).
- My subjective "belief" or "confidence" in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.

Each of these seems sensible. However, they're not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they're the two big ones. --->

讓我們先談談如何回答第一個問題：什麼是“機率”?統計學家和大多數的數學家同意機率的運算規則是,但是很少人知道兩方對於如何解釋這個問題沒有太多共識。這似乎很奇怪,因為很多人都非常習慣使用 “可能性”(chance)、“很有可能”(likely)、“有可能”(possibe) 和 “很可能”(probable) 這類詞彙,回答這個問題似乎不應該很困難。但是,你我都可能有過這樣經驗,聽不懂兩方人馬的對話究竟在講什麼，你我一樣的平凡人會默默走開，而且之後完全記不得兩方的對話。

讓我用這個例子來試著解釋什麼是機率。假如有一場機器人足球賽，上場的球隊是阿杜伊諾阿森納和米蘭，我想賭其中一隊會贏球。經過深思熟慮後,我判斷阿杜伊諾阿森納隊勝出的機率為80%。我這麼說是什麼意思呢?其中有三種可能意義:

- 雙方是機器人球隊,他們可以比賽無數次,如此一來,以所有比賽結果平均來說,阿杜伊諾阿森納會贏得每10場比賽中的8場。

- 看一看任何一場比賽的下注賠率,我花1美元押米蘭隊贏球，他們贏我就能獲得5美元(拿回我押的1美元再加上4美元的贏球獎勵)。相對地，我花4美元押阿杜伊諾阿森納隊贏球，他們贏我也能獲得5美元(拿回我押的4美元再加上1美元的贏球獎勵)。

- 我對阿杜伊諾阿森納隊獲勝的主觀“信念”或“信心”，比米蘭隊獲勝的信念高出四倍。

這些解釋似乎都有道理。然而,每種解釋的意義是不一樣的,並且不是每位統計學家都會認同所有解釋。原因是現代統計學存在壁壘分明的門派(沒錯,真的有!),並且會因為你追隨其中一派學習久了,你會認為另一派的解釋毫無意義。在這一節,我簡單介紹文兩大主要門派的方法論。不過請讀者記得，這兩種主要方法論，沒有那一方是統計學的唯一方法論。

### 次數主義觀點

<!---The first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the **次數主義觀點(frequentist view)** and it defines probability as a **長期累積的相對次數(long-run frequency)**. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has $P(H) = 0.5$. What might we observe? One possibility is that the first 20 flips might look like this:
                                                                                                                                                   
T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H

In this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I'd been keeping a running tally of the number of heads (which I'll call $N_H$) that I've seen, across the first N flips, and calculate the proportion of heads $\frac{N_H}{N}$ every time. @tbl-tab7-1 shows what I'd get (I did literally flip coins to produce this!): --->

兩大觀點中佔現代統計學主導地位的是次數主義觀點(frequentist view)[^translation-01],這個觀點將機率定義為長期累積次數。假如我們投擲一枚公平硬幣無數次，按照次數主義的定義,這一枚硬幣的機率模型是$P(H)= 0.5$。那麼可能的觀察結果是什麼?某次連續投擲20次的結果可能像以下紀錄:

[^translation-01]: 譯註~許多機率與統計的教科書或科普書，將frequentist翻成“頻率主義”。由於frequency在英文指涉事件發生的空間與時間，配合動詞時態，能在語境裡理解frequency是指空間或時間的事件變動。然而中文動詞無時態，英文原句的frequency如果沒有指涉時間的變化，直翻為"頻率"不但不夠精確，在許多語境還可能造成誤解。由於在行為科學的統計學應用場景，「時間」是較少被提及的因素，因此本書中文版以“次數主義”稱呼。在各單元的中文翻譯，會根據語境調整用詞。

反,正,正,正,正,反,反,正,正,正,正,反,正,正,反,反,反,反,反,正

在這20次裡有11次為正面(55%)。 假如一直記錄下去，以$N_H$表示擲出正面的次數，$N$代表累積至紀錄當下的總次數，每次紀錄計算擲出正面的比例就是 $\frac{N_H}{N}$。 @tbl-tab7-1 整理我得到的結果(原作者的親自實驗結果!):

```{r}
#| label: tbl-tab7-1
#| tbl-cap: 擲硬幣和正面的比例
#huxtabs[[7]][[1]] |>
#  merge_cells(1, 1:2) |>
#  merge_cells(2, 1:2) |>
#  merge_cells(3, 1:2) |>
#  merge_cells(4, 1:2) |>
#  merge_cells(5, 1:2) |>
#  merge_cells(6, 1:2) |>
#  set_top_border(4, 1:12) |>
#  set_bottom_border(4, 1:12) |>
#  set_bold(4, 1:12) 
huxthattibble(tibble(`投擲硬幣次數` = c(1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20),
                    `硬幣正面朝上次數` = c( 0 , 1 , 2 , 3 , 4 , 4 , 4 , 5 , 6 , 7,  8  ,  8 ,  9 , 10 , 10 , 10 , 10 , 10 , 10 , 11 ),
                    `正面朝上的次數比例` = c( 0.00 , .50 , .67 , .75 , .80 , .67 , .57 , .63 , .67 , .70, .73 , .67 , .69 , .71 , .67 ,  .63 , .59 , .56 , .53 , .55))) |>
  set_number_format(,3,"%.2f") |>
  set_top_padding(3:21, everywhere, value=0 ) |>
   set_bottom_padding(2:20, everywhere, value=0 )
```

<!--- Notice that at the start of the sequence the *proportion* of heads fluctuates wildly, starting at $.00$ and rising as high as $.80$. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the "right" answer of $.50$. This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted $N \rightarrow \infty$ ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that's how the frequentists define probability. Unfortunately, I don't have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion $\frac{N_H}{N}$ as $N$ increases. Actually, I did it four times just to make sure it wasn't a fluke. The results are shown in @fig-fig7-1.[^translation-5] As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.

[^translation-5]: 譯註~建議搭配[seeing theory期望值互動網頁](https://seeing-theory.brown.edu/basic-probability/index.html#section2)實際操作。

The frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is *necessarily* grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.[^07-introduction-to-probability-1] Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.

[^07-introduction-to-probability-1]: This doesn't mean that frequentists can't make hypothetical statements, of course. It's just that if you want to make a statement about probability then it must be possible to redescribe that statement in terms of a sequence of potentially observable events, together with the relative frequencies of different outcomes that appear within that sequence.

However, it also has undesirable characteristics. First, infinite sequences don't exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an "infinite" sequence of coin flips is even a meaningful concept, or an objective one. We can't say that an "infinite sequence" of events is a real thing in the physical universe, because the physical universe doesn't allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says "the probability of rain in Adelaide on 2 November 2048 is 60%" we humans are happy to accept this. But it's not clear how to define this in frequentist terms. There's only one city of Adelaide, and only one 2 November 2048. There's no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely *forbids* us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no "probability" that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like "There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain". It's very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in @sec-Estimating-a-confidence-interval). --->

留意一下這次實驗一開始,擲出正面的比例波動很大,一開始是$.00$,後來陡升至$.80$。到後來,累積的紀錄越来越多，比例值會接近多數人印象中的“正确”答案 $.50$。這就是次數主義觀點對於「機率」的核心定義：一次又一次地擲出一枚公平的硬幣,當 N 越大(接近無窮大,數學表示為 $N \rightarrow \infty$ ),正面的比例將收斂至 50%。數學家還會關注一些其他技術細節,但是以質性的角度來說,這就是次數主義論者定義機率的方式。可惜的是,我没有辦法拿到無限數量的硬幣，還有反覆做無限次地投擲硬幣所需要的耐心。不過只要有一台電腦,就可以讓電腦做不必動腦的重複性作業。所以我用電腦模擬投擲硬幣1000次,然後畫出隨著$N$增加， $\frac{N_H}{N}$ 的比例如何變化的趨勢圖。為了確保這不是偶然發生的現象,我做了四次獨立的模擬計算。模擬結果就像 @fig-fig7-1 。如圖所示,擲出正面的比例值最後會停止波動並趨向中間的數值，趨勢線最後停留的數值就是擲出正面的真實機率值。

次數主義觀點的機率定義有一些理想化的條件。首先,觀察必須是客觀的。事件發生的機率必然取決於事實，機率模型的數學表述式，只有定義其中的符號代指物理世界所發生的一系列事件才有意義。[^07-introduction-to-probability-1] 還有,每個機率事件是可被明確觀察的。任何兩個人觀察同一種事件的所紀錄的發生序列,都能用來計算特定事件的機率,計算的結果必定會相同。

[^07-introduction-to-probability-1]: 這當然並不是說次數主義觀點的方法論不能做假設性陳述。只是如果您想使用機率模型表達假設,可能必須將假設的內容重新定義為潛在可觀測事件的次數序列，以及序列的各種結果的相對次數。

然而次數主義方法論要成立的條件很難達成。首先,物理世界並不存在無窮序列。就拿你錢包裡的硬幣來說，拿出來做擲硬幣實驗，每次落地都會撞到地面，每撞擊一次都會磨損硬幣表面一點，最终這枚硬幣會被磨光。有些同學聽了可能會問，討論能“無限次”擲硬幣的假設是有意義的概念，還是客觀的概念，對於了解機率是什麼有何幫助？物理世界不會存在任何無限的事物，所以“無限序列”的機率事件不可能是真實的。更嚴重的是,次數主義定義容許的範圍非常狹窄。人類的語言能力可以給很多看得到的事物發生用機率表達，但是就理論來說，也無法將機率對應到假想的事件序列。例如,如果澳洲電視台的氣象預報員說“2018年11月2日阿德雷德降雨的機率是60%”,所有人都很樂意接受這種說法，但是很難用次數主義的條件來定義報導中所提的機率。 全世界只有一個地方叫阿德雷德,2018年11月2日也只是一個時間點。所以不存在無限序列，只是一時一地的單一事件。然而，次數主義的機率觀點不允許我們對單一事件以機率描述。從次數主義的角度來想,明天要麼下雨,要麼不下雨，不可重複事件不可能有“機率”。 
其實次數主義的追隨者可以使用一些非常聰明的技巧來解決這道難題，一種方案就像有的氣象預報員會用的說辭：「以我所使用的預測方法，只要出現某些氣象條件就能預測60%的降雨機率。只要明日的氣象具備這些條件，預測有60%的降雨機率是合理的。」這樣的說法好像繞了好幾圈又違反直覺,不過之後的單元裡，我們會看到這套次數主義方法論如何形塑現代統計方法(例如 @sec-Estimating-a-confidence-interval)。

```{r}
#| label: fig-fig7-1
#| fig-cap: 次數主義機率論的模擬實驗範例。 如果您投擲一枚公平的硬幣無數次,最後會看到正面的比例值會趨近真實機率 0.5。每個趨勢圖呈現四個不同的模擬實驗， 每次實驗都是模擬投擲硬幣 $1000$ 次,並且逐次紀錄擲出正面的比例。 儘管没有一個實驗結果的最後紀錄停在 $.5$ ,如是能將模擬次數增加到無限次則必定會達到。
#knitr::include_graphics("images/fig7-1.png")

coinflips <- list()
coinflips <- lapply(1:4, function(i) {
    ggplot(data=tibble(flips = 1:1000, heads = cumsum(as.numeric( runif(1000) > .5 )) / (1:1000)),
           aes(x=flips, y=heads)) +
    geom_line(color=blueshade, linewidth=1) +
    ylim(0.3, 0.7) +
    xlim(0, 1000) +
    geom_hline(yintercept=0.5, color="grey") +
    xlab("投擲硬幣次數") +
    ylab("硬幣正面朝上的比例") +
    theme_classic()
})
gridExtra::grid.arrange(grobs = coinflips)
```

### 貝氏觀點


<!---
**貝氏觀點(The Bayesian view)** of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what "the" Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the **事件發生的信念程度(degree of belief)** that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don't exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.[^translation-4]

[^translation-4]: 譯註~數學家版的貝氏定理$$Pr(A|B) = \frac{Pr(B|A)*Pr(A)}{Pr(B)}$$ 統計學家版的貝氏定理$$Pr(H_i|Data) = \frac{Pr(Data|H_i)*Pr(H_i)}{\sum_{j=1}^n Pr(Data|H_j)*Pr(H_j)}$$ 兩套公式取自旗標出版[AI 必須！從做中學貝氏統計](https://www.flag.com.tw/books/product/F2308)。

However, in order for this approach to work we need some way of operationalising "degree of belief". One way that you can do this is to formalise it in terms of "rational gambling", though there are many other ways. Suppose that I believe that there's a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win \$5, but if it doesn't rain I lose \$5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it's a bad bet to take. So we can operationalise the notion of a "subjective probability" in terms of what bets I'm willing to accept.

What are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don't need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can't be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don't have to, even though we're both rational. The frequentist view doesn't allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers). --->

貝氏的機率觀通常也稱為主觀觀點,雖然這個觀點在統計學領域中一直是少數派,但是影響力在過去幾十年持續穩定增長。 貝氏一門還有很多分支,以至於難用三言兩語說明什麼是“貝氏”觀點。解釋主觀機率的最常見說法是將事件的機率定義為有智慧與理性能力的現代人對於事件發生的信念程度(degree of belief)。從這個角度進一步說,機率並不存在於現實世界,而是存在於人類及其他智能生物的思維和想像。[^translation-02]

[^translation-02]: 譯註~數學家版的貝氏定理$$Pr(A|B) = \frac{Pr(B|A)*Pr(A)}{Pr(B)}$$ 統計學家版的貝氏定理$$Pr(H_i|Data) = \frac{Pr(Data|H_i)*Pr(H_i)}{\sum_{j=1}^n Pr(Data|H_j)*P r(H_j)}$$ 兩套公式取自旗標出版[AI 必須！從做中學貝氏統計](https://www.flag.com.tw/books/product/F2308)。

然而,為了實際應用主觀機率發展出來的方法，我們需要先把“信念程度”轉換為可操作的形式。今天已經發展出許多方法，在此介紹的形式化方法是“理性博弈”(rational gamling)： 假如我相信明天下雨的機率為 60%。若有人讓我下注,賭明天下雨的話,我會贏5美元,如果沒下雨,我就輸5美元。從我的角度來看,這顯然是一個相當不錯的賭注。 換句話說,如果我看到明天降雨機率只有 40%, 那麼接受這個賭注就是不明智的。 因此,我們可以通過我願意接受的賭注大小操作“主觀機率”。

貝氏的方法論有什麼優缺點? 主要優點是你能指定想要評估的任何事件是否發生的機率，而且不必考慮那些事件會不會重複。對許多人來說，最大的缺點是你不能完全客觀看待機率事件。指定一個事件機率需要一個對應信念程度的實體，這個實體可能是一個人類、外星人、機器人或者甚至一個統計學家，無論如何必須設定存在一位持有某種信念的智能代理者。對許多計較客觀的學者來說,這是令人不舒服的想法,似乎會讓機率運算變得武斷。儘管貝氏方法論強調信念的代理者是理性的(遵守機率規則),但是每個人都可以有屬於自己，不一定和他人相同的信念。以前面的擲硬幣問題來說，觀察者A可以相信硬幣是公平的,而觀察者B不需要,不過兩者的看法都是理性的。次數主義觀點則不允許任兩位觀察者認為同一事件的機率有不同看法。如果有這種情況,至少其中一種看法是錯的。 然而貝氏方法論觀點容許歧異看法並存的情形， 不同背景知識的兩造觀察者，(在貝氏的機率擂台)可以合法地對同一事件持有不同信念。簡而言之,次數主義觀點有時被認為範圍太狹窄(禁止觀察者對許多事件指定機率),貝氏觀點有時則被認為範圍太廣泛(允許觀察者保留彼此看法的差異)。

### 觀點之間的差異是什麼？何者正確？

<!--- Now that you've seen each of these two views independently it's useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you've understood the two perspectives you should have some sense of how to answer those questions.

Okay, assuming you understand the difference then you might be wondering which of them is *right*? Honestly, I don't know that there is a right answer. As far as I can tell there's nothing mathematically incorrect about the way frequentists think about sequences of events, and there's nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.

For the most part, I'm a pragmatist so I'll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I'll explain towards the end of the book. But I'm not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as "an impenetrable jungle [that] arrests progress towards precision of statistical concepts" [@Fisher1922b, p. 311]. Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into "a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring" [@Meehl1967 p. 114]. The history of statistics, as you might gather, is not devoid of entertainment.

In any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you'll need a good grasp of frequentist methods. I promise you that this isn't wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the "orthodox" frequentist view. Besides, I won't completely ignore the Bayesian perspective. Every now and then I'll add some commentary from a Bayesian point of view, and I'll revisit the topic in more depth in @sec-Bayesian-statistics.--->


既然你已經看懂了這兩種觀點的核心想法,比較兩方觀點的差異有助你回答什麼是機率。回頭來談那場機器人足球比賽結果的三種解釋，你認為次數主義和貝氏兩種觀點會如何看待那種是最正確的機率定義?次數主義的追隨者會說那個解釋最符合他們心目中的機率觀?貝氏的追隨者會贊成那一個?那個解釋對於兩方來說毫無意義?如果您有充分理解兩派觀點,應該多少能回答以上的問題。  

好啦,假如你真的理解觀點之間的區別,你應該想知道**那一種觀點是正確的**?老實說,我不知道有沒有正確的答案。據我所知,次數主義追隨者思考事件序列的方式，以數學來看沒有什麼不正確的,貝氏觀點設定一個持有信念的理性代理者，這一點也有數學基礎。其實，如果讀者能研究得更深入,會發現兩個觀點在很多事情的意見是一致的。許多基於次數主義方法所做出的決定，也是貝氏觀點追隨者認為一個理性代理者也有相同結論；許多貝氏主義方法具有相當明顯的次數主義特性。  

我(原作者)自認是務實主義者,所以我會使用任何可相信的統計方法。工作多年的經驗,使我更喜歡貝氏方法,理由我將在本書的最後單元解釋。但是我並不會不再使用次數主義方法，而且不是每個人都能在兩種觀點之間自在轉換。像是 羅納德·費雪爵士(Sir Ronald Fisher),20世紀統計學巨擘之一,就是徹底反對貝氏觀點，立場最極端的反對者。他在討論統計學數學基礎的著作裡提到主觀機率觀點是 “不可突破的叢林，(放任發展)會阻止精確統計學概念的進步” [ @Fisher1922b, 第 311 頁]。反方代表如心理學家保羅·米爾(Paul Meehl),他認為依靠次數主義方法會讓科學家變成 “能生產知識但無法提出真正洞見的書呆子，,與次數主義相伴的快樂學術路上留下了一整列受害者,其中沒有任何人輸出的科學知識能有效地開枝散葉”[@Meehl1967, 第 114 頁]。讀者有心挖掘爭論觀點的文獻會發現,現代統計學的歷史和暢銷小說一樣精采。  

無論如何,雖然我(原作者)比較喜歡貝氏主義的觀點,但大多數統計分析都是建立在次數主義方法論，有用的推論方法都有實用價值。本書介紹的統計方法與多數心理學系統計課內容大致相同,如果讀者想了解大多數心理學家使用的統計工具,需要好好掌握次數主義方法。我能保證,學習次數主義的統計方法不會浪費時間。就算之後有一天想要跳到貝氏的陣營,讀者也應該至少要讀通一本談論 “正統” 次數主義觀點的書。此外,我不會完全忽略貝氏觀點。之後每個章節,我會在適當的段落添加一些貝氏觀點, 最後的 @sec-Bayesian-statistics 會詳細介紹貝氏觀點的統計方法。  


## 基本機率論 {#sec-Basic-probability-theory}

<!---Ideological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won't go into a lot of detail, but I'll try to give you a bit of a sense of how it works. And in order to do so I'm going to have to talk about my trousers. --->

### 機率分佈入門 {#sec-introducing-probability-distributions}

<!--- teaching note
使用 seeing theory展示機率分佈
https://seeing-theory.brown.edu/
--->

<!-- One of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I've given them names: I call them $X_1$, $X_2$, $X_3$, $X_4$ and $X_5$. I really have, that's why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I'm so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each $X$) as an elementary event. The key characteristic of **elementary events** is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a **sample space**. Granted, some people would call it a "wardrobe", but that's because they're refusing to think about my trousers in probabilistic terms. Sad.

Okay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a **probability** of one of these elementary events. For an event $X$, the probability of that event $P(X)$ is a number that lies between 0 and 1. The bigger the value of $P(X)$, the more likely the event is to occur. So, for example, if $P(X) = 0$ it means the event $X$ is impossible (i.e., I never wear those trousers). On the other hand, if $P(X) = 1$ it means that event $X$ is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if $P(X) = 0.5$ it means that I wear those trousers half of the time.

At this point, we're almost done. The last thing we need to recognise is that "something always happens". Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the **law of total probability**, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a **probability distribution**. For example, @tbl-tab7-2 shows an example of a probability distribution. --->

我(原作者)個人最難以啟齒的一件事情是,我的衣櫃裡只有5條褲子。三條牛仔褲,一件西裝褲和一條運動褲。更讓我的朋友感到無聊的是,每條褲子都有個代號:$X_1$、$X_2$、$X_3$、$X_4$ 和 $X_5$。這是真的，這是為什麼我的朋友都叫我「內心小劇場男子」。每個要外出的日子，我都會想好要穿那一條。即使我充滿想像力，也不會想要穿兩條褲子出門，而且都活了這麼久，我不會不穿褲子就出門。如果要用機率論的語言來描述我今天穿那條褲子出門,每條褲子的代號( $X$ )合稱為基本事件。**基本事件**的關鍵條件是每紀錄一次觀察，紀錄結果可用其中一個代號代表，像是每天我穿了其中一條褲子就紀下這條的代號。因為現在我出門只會穿一條褲子，每一天穿那條褲子出門的紀錄剛好滿足基本事件的條件。同樣的,所有可能發生的基本事件集合稱為**樣本空間**。當然,我的朋友或有些學生會說“衣櫃”,那是因為他們不想用機率的詞彙來想像我的褲子。哭哭。

好的,現在有一個樣本空間(衣櫃),包括許多可能的基本事件(今天穿出門的褲子),現在要做的就是為每一個基本事件分派一個機率。對於其中一個事件$X$,該事件的機率$P(X)$是介於0和1之間的數字。$P(X)$的值越大,事件發生的可能性就越大。例如$P(X)=0$ 表示事件$X$是不可能發生的(我從不穿那一條褲子出門)。另一方面,$P(X)=1$ 表示事件$X$必定會發生(我總是穿那一條褲子出門)。至於介於兩者之間的機率值,代表我有時候會穿那條褲子。例如 $P(X)=0.5$代表我會出門的某幾天，有一半都會穿這條褲子。

到這裡,我們快要建好一套機率分佈模型了。最後一件要知道的事懂是“某個事件總是會發生”。每一天我穿上褲子,我一定會穿好穿滿(你會想這是廢話對吧?)。這幾句陳腔濫調在機率的語彙裡，代表所有基本事件的機率加起來必定等於1。儘管多數人都不太在意，數學家給這個事實命名為**總機率律(law of total probability)**。更重要的是,只要符合總機率律的條件，就自然會得到了一個**機率分佈(probability distribution)**。@tbl-tab7-2 就是有關原作者本人會穿那條褲子出門的機率分佈。

```{r}
#| label: tbl-tab7-2
#| tbl-cap: 今天穿那條褲子出門的機率分佈
#huxtabs[[7]][[2]]
kable(data.frame(
    `褲子樣式` = c("藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲"),
    `事件代號` = c("X_1", "X_2", "X_3", "X_4", "X_5"),
    `事件機率` = c("P(X_1)=.5", "P(X_2) = .3", "P(X_3) = .1", "P(X_4) = .0", "P(X_5) = .1" )
),format = "html", aligh = "c", escape = FALSE)

```

<!--- Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see @sec-Bar-graphs) to visualise this distribution, as shown in @fig-fig7-2. And, at this point, we've all achieved something. You've learned what a probability distribution is, and I've finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about **non elementary events** as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it's perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the "Dani wears jeans" event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case "blue jeans", "black jeans" or "grey jeans". In mathematical terms we defined the "jeans" event $E$ to correspond to the set of elementary events $(X1, X2, X3)$. If any of these elementary events occurs then $E$ is also said to have occurred. Having decided to write down the definition of the E this way, it's pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are $.5$, $.3$ and $.1$, the probability that I wear jeans is equal to $.9$. is: we just add everything up. In this particular case $$P(E)=P(X_1)+P(X_2)+P(X_3)$$ At this point you might be thinking that this is all terribly obvious and simple and you'd be right. All we've really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it's possible to construct some extremely powerful mathematical tools. I'm definitely not going to go into the details in this book, but what I will do is list, in @tbl-tab7-3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I've outlined above, but since we don't actually use these rules for anything in this book I won't do so here. 

A visual depiction of the 'trousers' probability distribution. There are five 'elementary events', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1

Some rules that probabilities satisfy
--->

每個基本事件都有一個介於 0 和 1 之間的機率值,只要加總所有事件的機率,會發現總和為 1。太棒了！我們還可以畫出一張漂亮的長條圖將這套機率分佈視覺化(複習一下 @sec-Bar-graphs),成果就像 @fig-fig7-2 。到這裡的學習如果沒有意外，代表讀者已經熟悉之後的單元討論機率分佈的方式了，我(原作者)也能用一張圖解釋為何我總是穿牛仔褲出門了! 還需要強調的事情是,機率論容許**非基本事件**以及基本事件並存。舉個例子來說明，同樣也是用原作者會穿那條褲子出門的機率分佈，要如何用樣本空間裡的基本事件，描述穿任何一條牛仔褲出門的機率？也就是今天出門穿的是“藍色牛仔褲”、“黑色牛仔褲”或“灰色牛仔褲”，都是符合這個狀況的事件。以數學的詞彙來說，我們可以將事件 $E$ 定義為對應符合條件的基本事件集合 $(X_1,X_2,X_3)$。只要其中任何一個基本事件發生,就是代表事件 $E$ 發生。$E$ 的定義確立後, $P(E)$ 的機率是多少很容易了，就是全部加起來：因為藍色、灰色和黑色牛仔褲的機率分別是 $.5$、$.3$ 和 $.1$ ，所以我穿牛仔褲出門的機率等於 $.9$。用數學式子表達的話，就是 $$P(E)=P(X_1)+P(X_2)+P(X_3)$$ 

也許您會認為,這些都太好懂，計算也沒什麼困難,這樣的感覺很多學生都會有。 現在討論的內容只是將日常生活的直觀用一些基本數學算則包裝而已。 不過，從簡單的案例開始,我們能構建一些非常強大的數學工具。 雖然這本書不會討論更深的細節, @tbl-tab7-3 列出滿足總機率律的樣本空間，可用的其他運算規則。 這些規則可以從前面描述的基本條件推導,由於本書不會用到這些規則,所以不會展示如何推導。

```{r}
#| label: fig-fig7-2
#| fig-cap: “褲子”機率分佈的視覺化呈現。 五個“基本事件”各自對應五條褲子。 每個事件都有發生的機率 - 這個值是 0 到 1 之間的數字。 所有事件的機率總和為 1。
#knitr::include_graphics("images/fig7-2.png")
pantsdat <- tibble(probabilities = c( .5, .3, .1, 0, .1),
                    eventNames = c( "藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲" ))
pantsdat$eventNames = factor(pantsdat$eventNames, levels = c( "藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲" ))

ggplot(data=pantsdat, aes(x=eventNames, y=probabilities)) +
  geom_bar(stat='identity', col="black", fill=blueshade, alpha=0.6) +
  theme_classic() +
  ylab("各事件的機率") +
  xlab("事件")
```


<!--- teaching note
使用 seeing theory展示集合論
https://seeing-theory.brown.edu/compound-probability/index.html#section1
--->


```{r}
#| label: tbl-tab7-3
#| tbl-cap: 滿足總機率律的一些運算規則
huxtabs[[7]][[3]] 
```

## 二項分佈 {#sec-The-binomial-distribution}

<!---As you might imagine, probability distributions vary enormously and there's an enormous range of distributions out there. However, they aren't all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the t distribution, the $\chi^2$ ("chi-square") distribution and the F distribution. Given this, what I'll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I'll start with the binomial distribution since it's the simplest of the five. --->

可想而知現在用於統計分析的機率分佈非常多,還有很 多分佈是只有純數學家才會研究的，不過不是所有機率分佈都必須要認識。本書介紹的統計方法主要依靠五種機率分佈運作:二項分佈(binomial distribution)、常態分佈(normal distribution)、 t 分佈(t distribution)、卡方分佈( $\chi^2$ distribution )以及 F 分佈( F distribution )。所以這個單元後半部就是簡要介紹這五種機率分佈,二項分佈和常態分佈的篇幅會比較多。我們先從二項分佈開始,因為這是五種之中最簡單的機率分佈。  

### 二項分佈入門

<!--- The theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the **binomial distribution** should involve a discussion of rolling dice and flipping coins. Let's imagine a simple "experiment". In my hot little hand I'm holding 20 identical six-sided dice. On one face of each die there's a picture of a skull, the other five faces are all blank. If I proceed to roll all 20 dice, what's the probability that I'll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6. To say this another way, the skull probability for a single die is approximately .167. This is enough information to answer our question, so let's have a look at how it's done.

As usual, we'll want to introduce some names and some notation. We'll let $N$ denote the number of dice rolls in our experiment, which is often referred to as the **size parameter** of our binomial distribution. We'll also use $\theta$ to refer to the the probability that a single die comes up skulls, a quantity that is usually called the **success probability** of the binomial.[^07-introduction-to-probability-2] Finally, we'll use $X$ to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of $X$ is due to chance we refer to it as a **random variable**. In any case, now that we have all this terminology and notation we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that $X = 4$ given that we know that $\theta = .167$ and $N = 20$. The general "form" of the thing I'm interested in calculating could be written as

[^07-introduction-to-probability-2]: Note that the term "success" is pretty arbitrary and doesn't actually imply that the outcome is something to be desired. If $\theta$ referred to the probability that any one passenger gets injured in a bus crash I'd still call it the success probability, but that doesn't mean I want people to get hurt in bus crashes!

$$P(X|\theta,N)$$

and we're interested in the special case where $X = 4$, $\theta = .167$ and $N = 20$.

[Additional technical detail [^07-introduction-to-probability-3]]

[^07-introduction-to-probability-3]: For those readers who know a little calculus, I'll give a slightly more precise explanation. In the same way that probabilities are non-negative numbers that must sum to 1, probability densities are non-negative numbers that must integrate to 1 (where the integral is taken across all possible values of X). To calculate the probability that X falls between a and b we calculate the definite integral of the density function over the corresponding range, $\int\_{a}^{b} p(x) dx$. If you don't remember or never learned calculus, don't worry about this. It's not needed for this book.

Yeah, yeah. I know what you're thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. I've included the formula for the binomial distribution in @tbl-tab7-2, since some readers may want to play with it themselves, but since most people probably don't care that much and because we don't need the formula in this book, I won't talk about it in any detail. Instead, I just want to show you what the binomial distribution looks like.

To that end, @fig-fig7-3 plots the binomial probabilities for all possible values of $X$ for our dice rolling experiment, from $X = 0$ (no skulls) all the way up to $X = 20$ (all skulls). Note that this is basically a bar chart, and is no different to the "trousers probability" plot I drew in @fig-fig7-2. On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling $4$ skulls out of $20$ is about $0.20$ (the actual answer is $0.2022036$, as we'll see in a moment). In other words, you'd expect that to happen about 20% of the times you repeated this experiment.

To give you a feel for how the binomial distribution changes when we alter the values of $theta$ and $N$, let's suppose that instead of rolling dice I'm actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly and the outcome that I'm interested in is the number of heads that I observe. In this scenario, the success probability is now $\theta = \frac{1}{2}$. Suppose I were to flip the coin $N = 20$ times. In this example, I've changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as @fig-fig7-4 shows, the main effect of this is to shift the whole distribution, as you'd expect. Okay, what if we flipped a coin $N = 100$ times? Well, in that case we get @fig-fig7-4 (b). The distribution stays roughly in the middle but there's a bit more variability in the possible outcomes. --->


機率論的源起是分析博奕遊戲的過程，所以擲骰子或擲幣的實驗經常用做認識**二項分佈**的教材。現在想像一個簡單的“實驗”：你面前的莊家手裡拿著20個相同的六面骰子，每個骰子的其中一面是骷髏圖案,其他五面都是空白的。如果莊家接著擲出所有骰子,你會看到其中4個骰子的骷髏圖案那面朝上的機率是多少?假如骰子是公平的,我們能假定任何一個骰子擲出骷髏的機率是 1/6。也就是說,擲出一個骷髏機率約為.167。 這些資訊足以回答請你想像的問題,讓我們用機率分佈計算看看。  

如同說明本人會穿什麼褲子出門的機率分佈一樣,這裡先要介紹一些名詞和符號。在這個例子裡，我們用$N$表示骰子擲出次數,通常被稱為二項分佈的**大小參數(size parameter?)**；還有用$\theta$代表一個骰子擲出骷髏的機率,這個參數通常稱為二項式的**成功機率**[^07-introduction-to-probability-2]。

我們將使用$N$表示我們實驗中的骰子擲出次數,這通常被稱為我們二項分佈的**大小參數**。 我們也將使用$\theta$引用單個骰子出現骷髏的機率,這個量通常被稱為二項式的**成功機率**[^07-introduction-to-probability-2]。 最後一個參數 $X$ 代表這次實驗擲出的幾個骷髏朝上的骰子。因為$X$的數值多少是由機率決定的,有個通用名稱**隨機變數**。有了這些專有名詞和符號,現在能準確地陳述要計算的問題。 我們已經知道 $\theta = .167$ 還有 $N = 20$，要計算 $X = 4$ 的機率。只要知道要用什麼機率分佈計算，任何事件發生機率的通用“數學式”一律可以寫為  

$$P(X|\theta,N)$$  

表示我們要算出的是 $X = 4, \theta = .167$ 和 $N = 20$ 的機率值。  

[^07-introduction-to-probability-2]: 請注意,這裡的“成功”是武斷的命名,並不是說出現這樣的結果是我所希望的。 如果 $\theta$ 指的是某位機車騎士因為交通事故受傷的機率,我還是會用成功機率稱呼,但這並不是說我希望有人因為交通事故受傷!  

[附加技術細節 [^07-introduction-to-probability-3]]  

[^07-introduction-to-probability-3]: 如果讀者知道一點微積分,我會用一種稍微精確方式解釋。如同離散機率函數涵蓋所有事件的機率，其值為必為正數且總和為1，連續機率函數涵蓋範圍內非零的機率密度且總和也是1。要計算代表事件a與b之間所有數值的發生機率，只要以密度函數計算兩者之間機率密度所佔面積的積分$\int_{a}^{b} p(x) dx$。若是讀者從未學過或已經忘記微積分,請不用擔心這個，本書的材料不需要懂微積分也能學習。


我知道很多讀者走到這裡會想：怎麼又是一堆符號，能不用知道那麼多嗎？這本書是為不想學太多數學符號的讀者而設計的，所以接下來會直接談如何運用二項分佈計算機率。不過有些比較好學的讀者可能想知道細節，所以我將詳細公式說明寫在腳注[^07-introduction-to-probability-3_1]。因為多數讀者沒興趣知道如何用公式計算機率，本書也不要求讀者要了解公式才能學如何計算，只要看看符合實驗條件 的二項分佈長什麼樣子。  


[^07-introduction-to-probability-3_1]: 二項分佈公式的 $X!$ 代表階乘(將1到$X$的所有整數相乘):$$P(X | \theta, N) = \displaystyle\frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$ 如果這個公式對您來說没有太大意義,請不用擔心。 使用本書學習統計不需要知道這個公式。

請看 @fig-fig7-3 。這個柱狀圖展示這個擲骰實驗所有可能結果 $X$ 的出現機率，包括$X = 0$(没有一個骷髏) 到$X = 20$(全部都是骷髏)。讀者只要知道，解讀這個柱狀圖的方式與 @fig-fig7-2 一樣，水平軸的值代表所有可能的實驗結果,垂直軸的值代表看到各種實驗結果的機率。所以,擲出 $4$ 個骷髏的機率是約 $0.20$(實際答案是 $0.2022036$,我們很快會學到怎麼算)。也就是說，若你試著重做這個時驗，看到相同結果的機率大約是20%。  


```{r}
#| label: fig-fig7-3
#| fig-cap: 大小參數為 $N = 20$,基礎成功機率為 $\theta = \frac{1}{6}$ 的二項分佈。 每個柱子對應一個特定结果(即X的一個可能值)的機率。 因為這是一個機率分佈,所以每個事件的機率值必須在 0 到 1 之間,所有柱子的高度之和必須為 1  
#knitr::include_graphics("images/fig7-3.png")

binom1 <- data.frame(heads = 0:20, pmf = dbinom(x = 0:20, size = 20, prob = 1/6)+0.0001)

ggplot(data=binom1, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "\n擲出骷髏的次數",
       y = "機率\n") +
  scale_x_discrete(breaks=seq(0,20,5)) +
  theme_classic()
```

<!--- I were to flip the coin $N = 20$ times. In this example, I've changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as @fig-fig7-4 (a) shows, the main effect of this is to shift the whole distribution, as you'd expect. Okay, what if we flipped a coin $N = 100$ times? Well, in that case we get @fig-fig7-4 (a). The distribution stays roughly in the middle but there's a bit more variability in the possible outcomes. --->


因為二項分佈柱狀圖的形狀會隨$\theta$ 和 $N$改變，為了讓讀者好好感受，現在我們把實驗材料改成一面是骷髏頭像硬幣。這次的實驗是持續投擲一枚公平的硬幣，我會紀錄會有幾次骷髏的那面朝上，所以設定這個實驗每次結果的成功機率$\theta = \frac{1}{2}$，做完實驗一共擲硬幣 $N = 20$ 次。只有成功機率改變的話，二項分佈有會有什麼變化？你有稍微研究一下公式的話，會預期如 @fig-fig7-4 (a) 展示的柱狀圖一樣，分佈的中心點位置移動到水平軸中央了。那麼連續擲$N = 100$ 次呢? 如同 @fig-fig7-4(b) ，分佈依然位於中間，但是可能的實驗結果(柱子)數量增加了。


```{r}
#| label: fig-fig7-4
#| fig-width: 5
#| fig-height: 7
#| fig-cap: 兩個擲硬幣實驗的二項分佈,假如是公平硬幣,基礎成功機率是 $\theta = \frac{1}{2}$。 圖(a)是連續擲 $N = 20$ 次硬幣的所有可能結果。 圖(b)是連續擲  $N = 100$ 次硬幣的所有可能結果。
#knitr::include_graphics("images/fig7-4.png")

binom2 <- data.frame(heads = 0:20, pmf = dbinom(x = 0:20, size = 20, prob = 1/2)+0.0001)
bd1 <- ggplot(data=binom2, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "擲出骷髏的次數",
       y = "機率") +
  scale_x_discrete(breaks=seq(0,20,5)) +
  labs(tag = "(a)") +
  theme(plot.tag.position = c(0.3, 0.70)) +
  theme_classic()

binom3 <- data.frame(heads = 0:100, pmf = dbinom(x = 0:100, size = 100, prob = 1/2)+0.0001)
bd2 <- ggplot(data=binom3, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "\n擲出骷髏的次數",
       y = "機率\n") +
  scale_x_discrete(breaks=seq(0,100,5)) +
  labs(tag = "(b)") +
  theme(plot.tag.position = c(0.3, 0.70)) +
  theme_classic()

gridExtra::grid.arrange(bd1,bd2,nrow=2)
```


## 常態分佈 {#sec-The-normal-distribution}

<!---While the binomial distribution is conceptually the simplest distribution to understand, it's not the most important one. That particular honour goes to the normal distribution, also referred to as "the bell curve" or a "Gaussian distribution". A **normal distribution** is described using two parameters: the mean of the distribution µ and the standard deviation of the distribution $\sigma$.

[Additional technical detail [^07-introduction-to-probability-3_1]]

[^07-introduction-to-probability-3_1]: The notation that we sometimes use to say that a variable $X$ is normally distributed is as follows: $$X \sim Normal(\mu,\sigma)$$ Of course, that's just notation. It doesn't tell us anything interesting about the normal distribution itself. As was the case with the binomial distribution, I have included the formula for the normal distribution in this book, because I think it's important enough that everyone who learns statistics should at least look at it, but since this is an introductory text I don't want to focus on it, so I've tucked it away in this footnote.

Let's try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at @fig-fig7-5 which plots a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. You can see where the name "bell curve" comes from; it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in @fig-fig7-5 shows a smooth curve instead of "histogram-like" bars. This isn't an arbitrary choice, the normal distribution is continuous whereas the binomial is discrete. For instance, in the die rolling example from the last section it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I drew in the previous section reflected this fact. In @fig-fig7-3, for instance, there's a bar located at $X = 3$ and another one at $X = 4$ but there's nothing in between. Continuous quantities don't have this constraint. For instance, suppose we're talking about the weather. The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, 23.9 degrees, or anything in between since temperature is a continuous variable. And so a normal distribution might be quite appropriate for describing Spring temperatures[^07-introduction-to-probability-5]

[^07-introduction-to-probability-5]: In practice, the normal distribution is so handy that people tend to use it even when the variable isn't actually continuous. As long as there are enough categories (e.g., Likert scale responses to a questionnaire), it's pretty standard practice to use the normal distribution as an approximation. This works out much better in practice than you'd think. 

@fig-fig7-5 The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled *Probability Density* and not *Probability*. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly - the height of the curve here isn't actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see [Probability density] section for all the annoying details)

@fig-fig7-6 An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu = 4$. The dashed line shows a normal distribution with a mean of $\mu = 7$. In both cases, the standard deviation is $\sigma = 1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right

@fig-fig7-7 An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma = 1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are 'centred' on the same spot, but the dashed line is wider than the solid one --->

雖然二項分佈是最好懂的機率分佈，不過並不是統計學裡最重要的分佈，運作各種統計方法要依賴的其實是常態分佈,又稱“鐘形曲線”或“高斯分佈”。**常態分佈**用兩個參數描述分佈的形狀：平均值 μ 和標準差$\sigma$。用隨機變數符號 $X$ 來表現的話就是:  

$$X \sim \text{Normal}(\mu,\sigma)$$  


[附加技術細節 [^07-introduction-to-probability-4]]  

[^07-introduction-to-probability-4]: 如同二項分佈,在此列出常態分佈的隨機變數公式,是因為這非常重要,每個學習統計學的同學都應該至少看一眼。由于這是一本簡介教材,為了不做深入介紹,完整公式隱藏於這個脚注:  $$p(X|\mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(X-\mu)^2}{2\sigma^2}}$$  

接著來談談為何常態分佈的視覺化長得如同 @fig-fig7-5 的樣子。圖中的常態分佈參數是平均值 $\mu = 0$ 、標準差 $\sigma = 1$ 。從外形就能了解又稱為“鐘形曲線”的原因;真得看起來有點像中世紀時期的鐘。那麼常態分佈和二項分佈有何不同？首先 @fig-fig7-5 的常態分佈呈現光滑的曲線，而不是“柱狀圖”。這不是隨便畫的,因為常態分佈是連續隨機變數,而二項分佈是離散隨機變數。用上一節的擲骰子範例來解釋的話,每次會獲得3個骷髏或4個骷髏,但是不可能會獲得3.9個骷髏。如同 @fig-fig7-3 呈現的事實：$X = 3$ 有一個長條, $X = 4$ 處也有一個長條,但是兩者之間没有。連續隨機變數没有這樣的限制。以天氣溫度來說，某個春天的日子氣溫可能是23度、24度、23.9度、或介於任兩個數值之間的任何數字,因為溫度是連續變數。這也是為什麼，常態分佈很適合描述春季的氣溫[^07-introduction-to-probability-5]。  

[^07-introduction-to-probability-5]: 常態分佈在統計分析實務非常好用,使得就算變數實際上並不是連續時，分析人員也會使用。只要構成結果變項的選項够多(像是很多問卷常用的李克特量表),以常態分佈作為近似的資料樣本分佈是相當標準的做法。這可能讓分析實務變得簡單。  


```{r}
#| label: fig-fig7-5
#| fig-cap: 平均值 $\mu = 0$ 和標準差 $\sigma = 1$ 的常態分佈。 x 軸對應某個隨機變數的值,y 軸的值代表測得某數值的可能性有多大。請留意 y 軸標籤是**機率密度(probability density)**而不是機率。 連續分佈具有一些微妙且不太好懂的特徵,使得 y 軸的數值不能直觀理解—這條曲條的高度並不是觀測到特定 x 值的機率，曲線的高度只是顯示那些 x 值特別容易觀測到(最高點之下的那一小塊!)。 這些需要花多一點時間學習的細節,請見[機率密度]。 
#knitr::include_graphics("images/fig7-5.png")

ggplot(data = data.frame(x = seq(-3, 3, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```


```{r}
#| label: fig-fig7-6
#| fig-cap: 展示更改常態分佈的平均值後會改變什麼。 實線是平均值為 $\mu = 4$ 的常態分佈。 虛線是平均值為 $\mu = 7$ 的常態分佈。 兩個分佈的標準差都是 $\sigma = 1$。 兩個分佈的形狀相同並不讓人意外,只是虛線部分站得更右邊。
#knitr::include_graphics("images/fig7-6.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 4, sd = 1), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 7, sd = 1), col=blueshade, linetype=2, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```

<!--- With this in mind, let's see if we can't get an intuition for how the normal distribution works. First, let's have a look at what happens when we play around with the parameters of the distribution. To that end, @fig-fig7-6 plots normal distributions that have different means but have the same standard deviation. As you might expect, all of these distributions have the same "width". The only difference between them is that they've been shifted to the left or to the right. In every other respect they're identical. In contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place but the distribution gets wider, as you can see in @fig-fig7-7. Notice, though, that when we widen the distribution the height of the peak shrinks. This has to happen, in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to sum to 1, the total area under the curve for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, $68.3\%$ of the area falls within 1 standard deviation of the mean. Similarly, $95.4\%$ of the distribution falls within 2 standard deviations of the mean, and $(99.7\%)$ of the distribution is within 3 standard deviations. This idea is illustrated in @fig-fig7-8; see also @fig-fig7-9.


@fig-fig7-8 The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu = 0$ and standard deviation $\sigma = 1$. The shaded areas illustrate 'areas under the curve' for two important cases. In panel (a), we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel (b), we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean

@fig-fig7-9 Two more examples of the 'area under the curve idea'. There is a 15.9% chance that an observation is one standard deviation below the mean or smaller (panel (a)), and a 34.1% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel (b)). Notice that if you add these two numbers together you get 15.9% + 34.1% = 50%. For normally distributed data, there is a 50% chance that an observation falls below the mean. And of course that also implies that there is a 50% chance that it falls above the mean
--->


讓我們試著用直觀的方式瞭解常態分佈為何是連續隨機變數，首先看一下改變分佈的參數會改變什麼。@fig-fig7-6 並列平均值不同，但是標準差相同的常態分佈。讀者可以預期兩個分佈都具有相同的“寬度”，唯一區別是其中一個的最高點在左邊、另一個在右邊。兩者的其他特徵都是相同的。如果保持平均值不變、增加標準差的話,新的分佈最高點還是在同樣的位置,不過分佈曲線變得更寬,如同 @fig-fig7-7 的展示。留意一下分佈寬度變大時,最高點的高度會跟著降低。這是必然的,如同離散二項分佈的所有柱子，高度相加起來必須為1,常態分佈曲線覆蓋的面積也必須等於1。 除此之外，常態分佈還有一個重要特徵需要讀者知道：無論實際平均值和標準差是多少,從平均值向左右延伸1個標準差覆蓋了 $68.3\%$ 的面積；從平均值向左右延伸2個標準差覆蓋了 $95.4\%$ ；從平均值向左右延伸3個標準差覆蓋了 $99.7\%$ 。 請參考@fig-fig7-8和 @fig-fig7-9 的說明。


```{r}
#| label: fig-fig7-7
#| fig-cap: 說明變更常態分佈的標準差會發生什麼事。 圖中的兩個分佈平均值相同 $\mu = 5$,只有標準差不一樣。 實線是標準差為 $\sigma = 1$ 的分佈,虛線顯示了一個標準差為 $\sigma = 2$ 的分佈。 因此,這兩個分佈均以相同的位置“置中”,只是虛線比實線寬。
#knitr::include_graphics("images/fig7-7.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 5, sd = 1), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 5, sd = 2), col=blueshade, linetype=2, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度ty\n") +
  xlab("\n觀察值") +
  theme_classic()

```

```{r}
#| label: fig-fig7-8
#| fig-width: 8
#| fig-height: 4
#| fig-cap: 曲線下的陰影面積表示觀察值落在特定範圍内的機率。 實線是根據平均值 $\mu = 0$ 和標準差 $\sigma = 1$ 的常態分佈繪製。 灰色區域呈現了指定標準差的“曲線下陰影面積”。 圖(a)顯示觀察值落在距離平均值一個標準差内的機率為 68.3%。 圖(b),顯示觀察值落在距離平均值兩個標準差内的機率為 95.4%。
#knitr::include_graphics("images/fig7-8.png")

p1 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-1, 1)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 68.3%") +
  xlab("\n(a)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

p2 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-2, 2)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 95.4%") +
  xlab("\n(b)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

gridExtra::grid.arrange(p1,p2, ncol=2)
```

```{r}
#| label: fig-fig7-9
#| fig-width: 8
#| fig-height: 4
#| fig-cap: 另外兩個說明“曲線下面積”的例子。圖(a)展示 觀察值小於或等於距離平均值一個標準差之外的機率為 15.9%；圖(b)展示觀察值落在距離平均值一個標準差和平均值之間的機率為 34.1%。 留意一下，把這兩個圖的面積加在一起,就是 15.9% + 34.1% = 50%。也就是說，對於符合常態分佈的資料,觀察值低於平均值的機率為 50%。 這也表示高於平均值的觀察值出現機率為 50%。
#knitr::include_graphics("images/fig7-9.png")

p3 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-4, -1)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 15.9%") +
  xlab("\n(a)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

p4 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-1, 0)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 34.1%") +
  xlab("\n(b)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

gridExtra::grid.arrange(p3,p4, ncol=2)
```

### 機率密度

<!--- There's something I've been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so. This "thing" that I'm hiding is weird and counter-intuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it's not something that you need to understand at a deep level in order to do basic statistics. Rather, it's something that starts to become important later on when you move beyond the basics. So, if it doesn't make complete sense, don't worry too much, but try to make sure that you follow the gist of it.

Throughout my discussion of the normal distribution there's been one or two things that don't quite make sense. Perhaps you noticed that the y-axis in these figures is labelled "Probability Density" rather than density. Maybe you noticed that I used $P(X)$ instead of $P(X)$ when giving the formula for the normal distribution.

As it turns out, what is presented here isn't actually a probability, it's something else. To understand what that something is you have to spend a little time thinking about what it really means to say that $X$ is a continuous variable. Let's say we're talking about the temperature outside. The thermometer tells me it's $23$ degrees, but I know that's not really true. It's not exactly $23$ degrees. Maybe it's $23.1$ degrees, I think to myself. But I know that that's not really true either because it might actually be $23.09$ degrees. But I know that... well, you get the idea. The tricky thing with genuinely continuous quantities is that you never really know exactly what they are.

Now think about what this implies when we talk about probabilities. Suppose that tomorrow's maximum temperature is sampled from a normal distribution with mean $23$ and standard deviation 1. What's the probability that the temperature will be exactly $23$ degrees? The answer is "zero", or possibly "a number so close to zero that it might as well be zero". Why is this? It's like trying to throw a dart at an infinitely small dart board. No matter how good your aim, you'll never hit it. In real life you'll never get a value of exactly $23$. It'll always be something like $23.1$ or $22.99998$ or suchlike. In other words, it's completely meaningless to talk about the probability that the temperature is exactly $23$ degrees. However, in everyday language if I told you that it was $23$ degrees outside and it turned out to be $22.9998$ degrees you probably wouldn't call me a liar. Because in everyday language "$23$ degrees" usually means something like "somewhere between $22.5$ and $23.5$ degrees". And while it doesn't feel very meaningful to ask about the probability that the temperature is exactly $23$ degrees, it does seem sensible to ask about the probability that the temperature lies between $22.5$ and $23.5$, or between $20$ and $30$, or any other range of temperatures.

The point of this discussion is to make clear that when we're talking about continuous distributions it's not meaningful to talk about the probability of a specific value. However, what we can talk about is the probability that the value lies within a particular range of values. To find out the probability associated with a particular range what you need to do is calculate the "area under the curve". We've seen this concept already, in @fig-fig7-8 the shaded areas shown depict genuine probabilities (e.g., in @fig-fig7-8) it shows the probability of observing a value that falls within 1 standard deviation of the mean).

Okay, so that explains part of the story. I've explained a little bit about how continuous probability distributions should be interpreted (i.e., area under the curve is the key thing). But what does the formula for ppxq that I described earlier actually mean? Obviously, $P(x)$ doesn't describe a probability, but what is it? The name for this quantity $P(x)$ is a **probability density**, and in terms of the plots we've been drawing it corresponds to the height of the curve. The densities themselves aren't meaningful in and of themselves, but they're "rigged" to ensure that the area under the curve is always interpretable as genuine probabilities. To be honest, that's about as much as you really need to know for now.[^07-introduction-to-probability-6]

[^07-introduction-to-probability-6]: For those readers who know a little calculus, I'll give a slightly more precise explanation. In the same way that probabilities are non-negative numbers that must sum to 1, probability densities are non-negative numbers that must integrate to 1 (where the integral is taken across all possible values of X). To calculate the probability that X falls between a and b we calculate the definite integral of the density function over the corresponding range, $\int\_{a}^{b} p(x) dx$. If you don't remember or never learned calculus, don't worry about this. It's not needed for this book. 

[^07-introduction-to-probability-6]: 若是讀者知道一點微積分,可參考這個稍微更精確的解釋。與機率是必須相加為 1 的非負數的相同方式,機率密度是必須積分為 1 的非負數(其中積分是對 X 的所有可能值進行的)。要計算 X 落在 a 和 b 之間的機率,我們計算密度函數在相應範圍內的定積分,$\int_{a}^{b} p(x) dx$。如果您不記得或者從未學過微積分,請不要太擔心這個。這本書不需要這個。   --->

以上常態分佈的介紹之中，其實一直避而不談機率密度，許多基礎統計教科書也會省略不提。不談機率密度也是是對的，因為這概念的本質既奇怪又違反多數人的直覺，雖然現代統計方法的許多使用標準是採用機率密度設定的。幸好如果謮者只是想要學完基礎統計的所有內容，有沒有弄懂機率密度並非必要，只有之後要學習更進階的統計課程才有必要。如果讀完這一節還是弄不懂的話也不必灰心，就算只記得一部分內容，也能讓你更能掌握如何學習使用各種統計方法

在介紹常態分佈的視覺化統計圖裡,讀者可能已經注意到這些圖中的 y 軸標記為“機率密度”而不是密度。還有若是有仔細看常態分佈公式的話,應該會注意到符號是 $p(X)$ 而不是 $P(X)$。  

這是因為圖中呈現數值的並不是實際的機率,而是某種代理指標。要理解那些y軸的數值是什麼意思,需要花一些時間了解 $X$ 是連續隨機變數是什麼意思。比如我們正在聊現在戶外氣溫有幾度。溫度計顯示 $23$ 度,但我知道這並不是真的。因為不是剛好 $23$ 度，我心裡想也可能是 $23.1$ 度。但我知道這也並不是真的,因為它實際上可能是 $23.09$ 度。但是我知道......好啦,我講這麼一大串，是讓你了解連續隨機變數的真正棘手之處是，沒有工具能測出準確的數值。  

現在來想一想如何計算連續隨機變數的機率。假設要預測明天的最高溫度，就從從平均值為 $23$ 度、標準差為 1 的常態分佈中隨機選出一個數值。溫度正好是 $23$ 度的機率是多少?答案是“零”,或者可能是“一個如此接近零以至於可以視為零的數字”。為什麼會這樣?這就像往一個無限小的飛鏢靶心丟飛鏢。無論你多會瞄準,你永遠不會命中(除非你是某個娛樂作品的主角)。在現實生活中,永遠不會出現剛好是 $23$ 度的值，真實數值總是會像 $23.1$ 或 $22.99998$等一類的數值。換句話說,想要知道溫度正好是 $23$ 度的機率是多少是完全没有意義的。然而,日常的溝通我跟你說戶外是 $23$ 度,就算實際上是 $22.9998$ 度,你也不會認為我在騙你。因為在日常溝通的情境,$23$ 度通常涵蓋“ $22.5$ 度及 $23.5$ 度之間的某個位數值”。雖然計較溫度正好是 $23$ 度的機率沒有不太意義,但是評估溫度在 $22.5$ 度和 $23.5$ 度之間，或者 $20$ 度和 $30$ 度之間，或者任何兩個數值範圍之間的機率相當合理。   

這段討論的重點是,當我們使用連續機率分佈時,計算某個特定值的機率並没有意義。然而,我們可以評估某個數值落在那段數值範圍內的機率。為了找出某個數值範圍的機率,要做的就是計算“曲線下的面積”。我們已經在 @fig-fig7-8 看到這個概念的視覺化,其中顯示的陰影區域代表真實機率，像是其中的圖(a)顯示了觀察到的值落在離平均值 1 個標準差範圍內的機率。   

到這裡已經解釋了一部分被省略的情節，以上說明了為何知道一段數值範圍覆蓋的曲線下面積，是解讀連續機率分佈的面積關鍵。那麼這一節開始提到的 $p(x)$ 在 公式裡是什麼角色?很 顯然$p(x)$ 不是代表一個機率值,那究竟是什麼?這個符號 $p(x)$ 的正式名稱是**機率密度**。在這個單元展示的曲線圖，就是y軸的數值，對應曲線的高度。密度本身並没有任何意義,但是經過“調整”，就可確保曲線下的部分面積可以代表真實機率。老實說,這就是現在你需要學習有關**機率密度**的全部知識。[^07-introduction-to-probability-3]   


## 其他常見機率分佈 {#sec-Other-useful-distributions}

<!--- The normal distribution is the distribution that statistics makes most use of (for reasons to be discussed shortly), and the binomial distribution is a very useful one for lots of purposes. But the world of statistics is filled with probability distributions, some of which we'll run into in passing. In particular, the three that will appear in this book are the t distribution, the $\chi^2$ distribution and the F distribution. I won't give formulas for any of these, or talk about them in too much detail, but I will show you some pictures: @fig-fig7-10, @fig-fig7-11 and @fig-fig7-12. 

@fig-fig7-10 A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line.

@fig-fig7-11 $\chi^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square  distribution.

@fig-fig7-12 An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general.
--->

常態分佈是統計學最常使用的機率分佈(原因很快就會討論),二項分佈在很多狀況也會用到。 因為使用統計都是遇到各式各樣機率分佈,我們將在後面的單元會遇到其中一些。 特別是另外三個還沒介紹的t 分佈、$\chi^2$ 分佈和 F 分佈。 我也不會跟你解釋這些分佈的公式,也不會多寫幾段話討論這些分佈,讀者只要透過這些視覺化圖像感受他們:@fig-fig7-10、@fig-fig7-11 和 @fig-fig7-12。  



```{r}
#| label: fig-fig7-10
#| fig-cap: 自由度為 3 的 t 分佈(實線)。 看起來很像常態分佈,但完全是另位一種機率分佈。 請比較虛線繪製的標準常態分佈。 
#knitr::include_graphics("images/fig7-10.png")


ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dt, n = 1000, args = list(df = 3), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", linetype=5, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```


- t 分佈是一種與常態分佈非常相似的連續分佈,請見 @fig-fig7-10。 仔細看t 分佈的“尾巴”比常態分佈的尾巴更“重”(尾巴曲線稍微高一些)。這是兩者之間的重要區別，當你認為資料的分佈符合常態分佈但並不知道平均值或標準差，其實應該是符合t分佈。 @sec-Comparing-two-means 將學習使用t分佈的統計分析方法。  

```{r}
#| label: fig-fig7-11
#| fig-cap: 自由度為 3 的 $\chi^2$ 分佈。 請注意,所有觀察值都是大於零,並且該分佈形狀相當歪斜。 這些是 $\chi^2$ 分佈的關鍵特徵
#knitr::include_graphics("images/fig7-11.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dchisq, n = 1000, args = list(df = 3), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```



- $\chi^2$ 分佈也是經常在許多文獻報告裡看到的機率分佈。 最常使用的場景是 @sec-Categorical-data-analysis 介紹的分類資料分析,實際上這個分佈無處不在。如果讀者有興趣深入研究機率分佈的數學原理(如果你上完這個課程開竅的話?),會發現 $\chi^2$ 分佈隨處可見的主要原因是,如果要處理不只一個符合常態分佈的變項,計算出平方和之總和後(稱為計算“平方和”),形成的隨機變數就是 $\chi^2$ 分佈。知道這個原理會提昇你運用統計的能力。 現在先感受一下 $\chi^2$ 分佈的視覺特徵: @fig-fig7-11。  


```{r}
#| label: fig-fig7-12
#| fig-cap: 自由度為 3 及 5 的 F 分佈。 以關鍵特徵來看,非常像 $\chi^2$ 分佈,但他們完全是兩種不同的機率分佈。
#knitr::include_graphics("images/fig7-12.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = df, n = 1000, args = list(df1 = 3, df2 = 5), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()
```

<!---
-   The $t$ distribution is a continuous distribution that looks very similar to a normal distribution, see @fig-fig7-10. Note that the "tails" of the t distribution are "heavier" (i.e., extend further outwards) than the tails of the normal distribution). That's the important difference between the two. This distribution tends to arise in situations where you think that the data actually follow a normal distribution, but you don't know the mean or standard deviation. We'll run into this distribution again in @sec-Comparing-two-means.
-   The $\chi^2$ distribution is another distribution that turns up in lots of different places. The situation in which we'll see it is when doing categorical data analysis in @sec-Categorical-data-analysis, but it's one of those things that actually pops up all over the place. When you dig into the maths (and who doesn't love doing that?), it turns out that the main reason why the $\chi^2$ distribution turns up all over the place is that if you have a bunch of variables that are normally distributed, square their values and then add them up (a procedure referred to as taking a "sum of squares"), this sum has a $\chi^2$ distribution. You'd be amazed how often this fact turns out to be useful. Anyway, here's what a $\chi^2$ distribution looks like: @fig-fig7-11.
-   The $F$ distribution looks a bit like a $\chi^2$ distribution, and it arises whenever you need to compare two $\chi^2$ distributions to one another. Admittedly, this doesn't exactly sound like something that any sane person would want to do, but it turns out to be very important in real world data analysis. Remember when I said that $\chi^2$ turns out to be the key distribution when we're taking a "sum of squares"? Well, what that means is if you want to compare two different "sums of squares", you're probably talking about something that has an F distribution. Of course, as yet I still haven't given you an example of anything that involves a sum of squares, but I will in @sec-Comparing-several-means-one-way-ANOVA. And that's where we'll run into the F distribution. Oh, and there's a picture in @fig-fig7-12.

Okay, time to wrap this section up. We've seen three new distributions: $\chi^2$), $t$ and $F$. They're all continuous distributions, and they're all closely related to the normal distribution. The main thing for our purposes is that you grasp the basic idea that these distributions are all deeply related to one another, and to the normal distribution. Later on in this book we're going to run into data that are normally distributed, or at least assumed to be normally distributed. What I want you to understand right now is that, if you make the assumption that your data are normally distributed, you shouldn't be surprised to see $\chi^2$, $t$ and $F$ distributions popping up all over the place when you start trying to do your data analysis. --->


- F 分佈看起來有點像 $\chi^2$ 分佈,並且需要比較個 $\chi^2$ 分佈就會遇到。 雖然任何有頭腦的人都不會想挖這個坑跳進去,但這種比較在資料分析實務非常重要。 還記得 $\chi^2$ 分佈是處理“平方和”會用到的機率分佈嗎?這也就是說如果要處理的統計問題是比較兩個“平方和”，就要使用F分佈。當然，我們目前還沒遇到有平方和的範例，這是 @sec-Comparing-several-means-one-way-ANOVA 的學習課題，我們將在那個單元好好認識F分佈。現在請看 @fig-fig7-12 感受F分佈的特徵。


這個單元終於要結束了。讀者們又看到三種機率分佈:$\chi^2$、t 和 F。 這些都是連續分佈,並且都與常態分佈有密切關聯。不想深究數學原理的讀者，只要知道這些基本事實就足夠了。在後面的單元，我們將遭遇遵循常態分佈或至少假定遵循常態分佈的資料。讀者現在只要記住，如果假定要處理的資料遵循常態分佈，其實應該使用$\chi^2$、t 或 F 分佈的話，不必感到驚慌了。  


## 本章小結

<!--- In this chapter we've talked about probability. We've talked about what probability means and why statisticians can't agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:

- Probability theory versus statistics: [How are probability and statistics different?]
- [The frequentist view] versus [The Bayesian view] of probability
- [Basic probability theory]
- [The binomial distribution], [The normal distribution], and [Other useful distributions]

As you'd expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the "simpler" task of documenting standard probability distributions is a big topic. I've described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called "Statistical Distributions" [@Evans2011] that lists a lot more than that. Fortunately for you, very little of this is necessary. You're unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won't need them for this book, but it never hurts to know that there's other possibilities out there.

Picking up on that last point, there's a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often "forget" to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it's important to understand these things before moving onto the applications. For example, there are a lot of rules about what you're "allowed" to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in @sec-Comparing-two-means we're going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.--->

在本章中,我們討論了機率。我們討論了機率的意義以及為什麼統計學家無法就其意義達成共識。我們討論了機率必須遵守的規則。並且我們介紹了機率分佈的概念,並在本章中花了很大篇幅討論統計學家使用的一些更重要的機率分佈。逐節概述如下:  

- 機率論與統計學之間的區別:[機率和統計有什麼不一樣？]  

- 機率的[次數主義觀點]與[貝氏觀點]  

- [基本機率論]  

- [二項分佈]、[常態分佈]和[其他常見機率分佈]  

如您所料,我的內容絕不詳盡。機率論本身就是數學的一個巨大分支,與其在統計和資料分析中的應用完全分開。因此,這個主題上已經寫了上千本書,大學通常提供多門完全致力於機率論的課程。即使是記錄標準機率分佈這樣“更簡單”的任務也是一個大課題。我在本章中描述了五種標準機率分佈,但在我的書架上有一本45章的書,名為“統計分佈” [@Evans2011],其中列出了很多更多分佈。幸運的是,您需要的很少。當您走出去做現實世界的資料分析時,您不太可能需要知道幾十種統計分佈,在本書中您肯定不需要它們,但是瞭解還有其他可能性永遠不會有害。  

關於最後一點,在某種意義上,整章可以說是額外的內容。許多本科心理學統計課程會很快減少這部分內容(我知道我的課程就是這樣),即使是更高級的課程也經常會“忘記”重新訪問該領域的基礎。大多數學術心理學家不會區分機率和密度,直到最近,很少有人意識到貝式和次數主義機率之間的區別。然而,我認為在繼續應用之前理解這些很重要。例如,在進行統計推論時,有很多關於您“允許”說什麼的規則,其中許多看起來很武斷和奇怪。然而,如果您瞭解貝式與次數主義的區別,它們就會變得有意義。同樣,在 @sec-Comparing-two-means 中,我們將談論所謂的 t 檢定,如果您真的想完全理解 t 檢定的機制,那麼瞭解 t 分佈的實際情況真的會有幫助。我希望您理解我的意思。  
