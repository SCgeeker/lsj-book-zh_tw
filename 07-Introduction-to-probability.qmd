# 機率入門 {#sec-Introduction-to-probability}

(以下為AI初翻，尚待校稿) = 

```{r}
#| include: FALSE
source("header.R")
```

<!---

> *[God] has afforded us only the twilight ... of Probability.*\
> -- John Locke


Up to this point in the book we've discussed some of the key ideas in experimental design, and we've talked a little about how you can summarise a data set. To a lot of people this is all there is to statistics: collecting all the numbers, calculating averages, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics and one of the least powerful. The bigger and more useful part of statistics is that it provides information that lets you make inferences about data.

Once you start thinking about statistics in these terms, that statistics is there to help us draw inferences from data, you start seeing examples of it everywhere. For instance, here's a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):

> "I have a tough job," the Premier said in response to a poll which found her government is now the most unpopular Labor administration in polling history, with a primary vote of just 23 per cent.

This kind of remark is entirely unremarkable in the papers or in everyday life, but let's have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I'm too lazy to track down the original survey so let's just imagine that they called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those claimed that they intended to vote for the Australian Labor Party (ALP). For the 2010 Federal election the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?

The answer to the question is pretty obvious. If I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the only 230 people out of the entire voting public who actually intend to vote ALP. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it's a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.

**Inferential statistics** provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of **probability theory**. And it is to probability theory that we must now turn. This discussion of probability theory is basically background detail. There's not a lot of statistics per se in this chapter, and you don't need to understand this material in as much depth as the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it's worth covering some of the basics.
--->


> _(上帝)只允許我們置身于...機率的微光之中。_

> -- 約翰·洛克

在本書到目前為止的部分,我們討論了實驗設計的一些關鍵思想,並且我們稍微討論了如何總結一個資料集。對許多人來說,這就是統計學的全部:收集所有數字,計算平均值,繪製圖片,並在某個報告中將它們全部放在一起。有點像數字版的集郵。然而,統計學涵蓋的範圍遠不止於此。事實上,描述性統計學是統計學最小的一部分,也是最不強大的一部分。統計學更大和更有用的一部分在於它提供的信息使您可以對資料進行推論。

一旦你以這種方式來思考統計學,認為統計學是在那裡幫助我們從資料中得出結論,你就會開始看到它無所不在的例子。例如,這是一個來自《雪梨晨鋒報》(2010年10月30日)的簡短摘錄:  

> “我的工作很艱鉅,”面對一項民調表明她的政府現在是歷史上最不受歡迎的工黨政府的調查結果,總理如是說,初選票只有23%。  

這種評論在報紙或日常生活中並不引人注目,但讓我們來思考它所暗示的內容。民調機構進行了一次調查,通常是相當大規模的,因為他們有資金。我太懶了,不想查找原始調查,所以讓我們假Imaginer他們隨機抽取了1000位新南威爾斯(NSW)選民,其中230人(23%)聲稱他們打算投票給澳大利亞工黨(ALP)。根據澳大利亞選舉委員會的報告,2010年聯邦大選在新南威爾斯有4,610,795登記選民,所以其餘4,609,795名選民(約99.98%的選民)的意見對我們來說仍然不明。即使假設沒有人在民調中說謊,我們可以100%確定的是,真正的ALP初選票數在230/4610795(約0.005%)和4610025/4610795(約99.83%)之間。那麼,民調機構、報社和讀者得出ALP初選票只有大約23%的結論的依據是什麼?   

這個問題的答案相當明顯。如果我隨機打電話給1000人,其中230人表示打算投ALP,那麼這些人很可能並不是整個選民中實際打算投ALP的唯一230人。換句話說,我們假設民調機構收集的資料與整個人群比較代表性。但是有多少代表性呢?如果我們發現ALP的真正初選票實際上是24%? 29%? 37%?這時候直觀開始有點動搖。沒有人會對24%感到驚訝,每個人都會對37%感到驚訝,但是很難說29%是否令人信服。我們需要比查看數字和猜測更強大的工具。  

**推論統計學**提供了我們需要的工具來回答這些問題,而由於這些問題位於科學工作的核心,它們佔據了每門簡介統計學和研究方法課程的絕大部分內容。然而,統計推論理論是建立在**機率論**的基礎上的。我們現在必須轉向機率論。這個對機率論的討論基本上是背景詳情。本章沒有太多統計內容,您也不需要像理解本書這部分的其他章節那樣深入地理解這些材料。儘管如此,由於機率論確實支撐了統計學的如此之大的一部分,所以涵蓋一些基礎知識是值得的。


## 機率和統計有什麼不一樣？

<!--- teaching note:
要用數字回答的問題，需不需要用「機率模型」解決？必須用「機率模型」解決的數字問題，一定是機率問題。 
要用數字回答的問題，只要找到合理方法排列數字就能解決，只是（描述）統計問題。例如1854年倫敦蘇活區霍亂疫情。
各式各樣的機率分佈，就是「機率模型」。
--->

<!---
Before we start talking about probability theory, it's helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they're not identical. Probability theory is "the doctrine of chances". It's a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:


- What are the chances of a fair coin coming up heads 10 times in a row?
- If I roll a six sided dice twice, how likely is it that I'll roll two sixes?
- How likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?
- What are the chances that I'll win the lottery?

Notice that all of these questions have something in common. In each case the "truth of the world" is known and my question relates to the "what kind of events" will happen. In the first question I know that the coin is fair so there's a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known **model** of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:

$$P(head)=0.5$$

which you can read as "the probability of heads is 0.5". As we'll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don't actually know exactly what's going to happen. Maybe I'll get 10 heads, like the question says. But maybe I'll get three heads. That's the key thing. In probability theory the model is known but the data are not.

So that's probability. What about statistics? Statistical questions work the other way around. In statistics we <u>do not</u> know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:

-   If my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?
-   If five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?
-   If the lottery commissioner's spouse wins the lottery how likely is it that the lottery was rigged?

This time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:

H H H H H H H H H H H

and what I'm trying to do is work out which "model of the world" I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as $P(heads)\ne{0.5}$. In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn't the same as the probability question, but they're deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works. --->

在我們開始談論機率論之前,花點時間思考機率與統計學之間的關係是很有幫助的。這兩個學科關係密切,但它們不是一樣的。機率論是“隨機事件的理論”。它是數學的一個分支,告訴你不同种类事件发生的频率。例如,使用機率論可以回答所有這些問題:

- 一枚公平的硬幣連續擲出正面10次的機率是多少?  

- 如果我擲兩次6面骰子,擲出兩次6的可能性有多大?

- 從洗混完美的牌組中抽出5張牌全部都是紅心的可能性有多大?  

- 我贏得彩票的機率是多少?  

請注意,所有這些問題都有一些共同點。在每種情况下,“世界事實”都是已知的,我的問題與“什麼樣的事件”會發生有關。 在第一個問題中,我知道硬幣是公平的,所以每次擲幣正面朝上都有50%的機率。 在第二個问题中,我知道擲一枚骰子擲到6的機率為1/6。 在第三個問題中,我知道牌組洗混得当。 在第四個問題中,我知道彩票遵循特定規則。 您懂得。關鍵點是機率問題是從已知的世界**模型**出發的,我們使用該模型進行一些計算。 相關模型可以非常簡單。例如,在擲幣示例中,我們可以這樣寫下模型:  

$$P(\text{正面})=0.5$$  

您可以把它讀作“擲到正面的機率為0.5”。 正如我們稍後會看到的,與百分比是從0%到100%的數字一樣,機率只是從0到1的數字。 在使用這個機率模型回答第一個問題時,我实际上並不知道會發生什麼。也許像問題所說,我會得到10次正面。但也許我會得到3次正面。 這就是關鍵。 在機率論中,模型是已知的,但數據是未知的。  

這就是機率。那統計學呢?統計問題的工作原理相反。在統計學中,我們並不知道世界的真相。我們所擁有的唯一是數據,我們希望從數據中了解世界的真相。統計問題趨向於更像這些:  

- 如果我的朋友擲幣10次全是正面,他們是在愚弄我嗎?

- 如果牌組頂部的五張牌都是紅心,牌組是否洗混的可能性有多大?  

- 如果彩票委員的配偶贏得了彩票,彩票是否有人操作的可能性有多大?

這次我們擁有的唯一東西就是數據。 我知道的是,我看到我的朋友擲幣10次,每次都是正面。 我想推斷的是我是否應該得出我剛才看到的只是10次連續擲出的公平硬幣,或者我是否應該懷疑我的朋友在愚弄我。 我擁有的數據如下:  

正面 正面 正面 正面 正面 正面 正面 正面 正面 正面  

我正在嘗試去弄清楚我應該相信哪種“世界模型”。 如果硬幣是公平的,那麼我應該采用的模型是正面機率為0.5,即 $P(\text{正面}) = 0.5$。 如果硬幣不公平,那麼我應該得出正面的機率不為0.5,我們將其寫為 $P(\text{正面})\neq0.5$。 換句話說,統計推論的問題是弄清楚這些機率模型中哪個是正確的。 顯然,統計問題與機率問題不是一樣的,但它們之間有深刻的聯繫。 因此,統計理論的良好介紹將從討論機率的定義及其運作原理開始。  



## 如何解讀機率？ {#sec-What-does-probability-mean}

<!--- teaching note:
探討「歸納之謎」，如何估計未知的事件發生機率，你認為W君與C君誰的想法合理？
C君 = 次數主義觀點 ~ 只有長期觀察的結果才能掌握發生機率。標示相對發生次數的統計量是解答的元素。
W君 = 貝氏主義觀點 ~ 掌握每次試驗結果，更新事件發生機率的信心。事件發生的機率是解答的元素。
--->

<!--- Let's start with the first of these questions. What is "probability"? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there's much less of a consensus on what the word really means. It seems weird because we're all very comfortable using words like "chance", "likely", "possible" and "probable", and it doesn't seem like it should be a very difficult question to answer. But if you've ever had that experience in real life you might walk away from the conversation feeling like you didn't quite get it right, and that (like many everyday concepts) it turns out that you don't really know what it's all about.

So I'll have a go at it. Let's suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:

- They're robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.
- For any given game, I would agree that betting on this game is only "fair" if a \$1 bet on C Milan gives a \$5 payoff (i.e. I get my \$1 back plus a \$4 reward for being correct), as would a \$4 bet on Arduino Arsenal (i.e., my \$4 bet plus a \$1 reward).
- My subjective "belief" or "confidence" in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.

Each of these seems sensible. However, they're not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they're the two big ones. --->

讓我們從第一個問題開始。“機率”是什麼?您可能會感到驚訝,儘管統計學家和數學家(大多數)同意機率的規則是什麼,但對該詞的真正意義卻沒有太多共識。 這似乎很奇怪,因為我們都非常習慣使用像 “可能性”、“很有可能”、“有可能” 和 “很可能” 這樣的詞,回答這個問題似乎不應該很困難。 但是,如果你生活中曾有過這種經歷,你可能會覺得你没有完全搞懂,並且(像許多日常概念一樣)事實證明你並不真正知道這是怎麼回事。

那么我來試試。假設我想打賭一場两個机器人球队的足球賽,阿杜伊諾阿森納和米蘭。經過深思熟慮后,我決定阿杜伊諾阿森納贏得的機率為80%。我這麼說是什麼意思呢?這裡有三种可能性:

- 它們是機器人球隊,所以我可以讓它們一次又一次地比賽,如果這樣的話,平均而言,阿杜伊諾阿森納會贏得10場比賽中的8場。

- 對於任何一場比賽,我會同意押注這場比賽只有當押注米蘭1美元能獲得5美元的賠率時(即我獲得我的1美元再加上4美元的正確獎勵),相應地,押注阿杜伊諾阿森納4美元也是一樣(即我的4美元加上1美元的獎勵)。

- 我對阿杜伊諾阿森納獲勝的主觀“信念”或“信心”是對米蘭獲勝的信念的四倍。

這些似乎都有道理。然而,它們不是一樣的,並且不是每個統計學家都會認可其中所有的陳述。原因是存在不同的統計學流派(沒錯,真的有!),並且取決於您追隨哪一派,您可能會說其中一些陳述毫無意義或不相干。在本節中,我簡要介紹了文獻中存在的两大主要方法。這些绝不是唯一的方法,但它們是两大主要方法。

### 次數主義觀點

<!---The first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the **次數主義觀點(frequentist view)** and it defines probability as a **長期累積的相對次數(long-run frequency)**. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has $P(H) = 0.5$. What might we observe? One possibility is that the first 20 flips might look like this:
                                                                                                                                                   
T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H

In this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I'd been keeping a running tally of the number of heads (which I'll call $N_H$) that I've seen, across the first N flips, and calculate the proportion of heads $\frac{N_H}{N}$ every time. @tbl-tab7-1 shows what I'd get (I did literally flip coins to produce this!): --->


第一個也是統計學中更占主導地位的两大機率觀點稱為次數主義觀點,它將機率定義為長期頻率。假设我們試圖一次又一次擲掷一枚公平的硬幣。按定義,這是一枚擁有$P(H)= 0.5$的硬幣。我們可能會觀察到什麼?一种可能性是前20次投掷如下:

尾,正,正,正,正,尾,尾,正,正,正,正,尾,正,正,尾,尾,尾,尾,尾,正

在這20次投掷中有11次(55%)為正面。 现在假设我一直記錄着我見過的頭數 $N_H$,跨越前 N 次投掷,並在每次計算比例 $\frac{N_H}{N}$。 @tbl-tab7-1 顯示了我得到的結果(我確實是這樣產生的!):

```{r}
#| label: tbl-tab7-1
#| tbl-cap: 擲硬幣和正面的比例
#huxtabs[[7]][[1]] |>
#  merge_cells(1, 1:2) |>
#  merge_cells(2, 1:2) |>
#  merge_cells(3, 1:2) |>
#  merge_cells(4, 1:2) |>
#  merge_cells(5, 1:2) |>
#  merge_cells(6, 1:2) |>
#  set_top_border(4, 1:12) |>
#  set_bottom_border(4, 1:12) |>
#  set_bold(4, 1:12) 
huxthattibble(tibble(`投擲硬幣次數` = c(1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10, 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20),
                    `硬幣正面朝上次數` = c( 0 , 1 , 2 , 3 , 4 , 4 , 4 , 5 , 6 , 7,  8  ,  8 ,  9 , 10 , 10 , 10 , 10 , 10 , 10 , 11 ),
                    `正面朝上的次數比例` = c( 0.00 , .50 , .67 , .75 , .80 , .67 , .57 , .63 , .67 , .70, .73 , .67 , .69 , .71 , .67 ,  .63 , .59 , .56 , .53 , .55))) |>
  set_number_format(,3,"%.2f") |>
  set_top_padding(3:21, everywhere, value=0 ) |>
   set_bottom_padding(2:20, everywhere, value=0 )
```

<!--- Notice that at the start of the sequence the *proportion* of heads fluctuates wildly, starting at $.00$ and rising as high as $.80$. Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the "right" answer of $.50$. This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted $N \rightarrow \infty$ ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that's how the frequentists define probability. Unfortunately, I don't have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion $\frac{N_H}{N}$ as $N$ increases. Actually, I did it four times just to make sure it wasn't a fluke. The results are shown in @fig-fig7-1.[^translation-5] As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.

[^translation-5]: 譯註~建議搭配[seeing theory期望值互動網頁](https://seeing-theory.brown.edu/basic-probability/index.html#section2)實際操作。

The frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is *necessarily* grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.[^07-introduction-to-probability-1] Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.

[^07-introduction-to-probability-1]: This doesn't mean that frequentists can't make hypothetical statements, of course. It's just that if you want to make a statement about probability then it must be possible to redescribe that statement in terms of a sequence of potentially observable events, together with the relative frequencies of different outcomes that appear within that sequence.

However, it also has undesirable characteristics. First, infinite sequences don't exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an "infinite" sequence of coin flips is even a meaningful concept, or an objective one. We can't say that an "infinite sequence" of events is a real thing in the physical universe, because the physical universe doesn't allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says "the probability of rain in Adelaide on 2 November 2048 is 60%" we humans are happy to accept this. But it's not clear how to define this in frequentist terms. There's only one city of Adelaide, and only one 2 November 2048. There's no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely *forbids* us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no "probability" that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like "There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain". It's very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in @sec-Estimating-a-confidence-interval). --->

請注意,在序列開頭,頭的比例波動很大,一開始為$.00$,后來升至高達$.80$。后來,人們會得到一種印象,即越来越多的值实际上非常接近“正确”答案 $.50$。 这就是次數主義機率的核心定義。一次又一次地擲掷一枚公平的硬幣,並且當 N 變大(接近無窮大,表示為 $N \rightarrow \infty$ )時,正面的比例將收斂至 50%。數學家們在技術上還關注一些其他細节,但是從定性的角度来说,這就是次數主義者定義機率的方式。不幸的是,我没有無限数量的硬幣或反复無限次投掷硬幣所需的無限耐心。然而,我確實有一台電腦,電腦擅長不假思索的重複任務。 所以我讓我的計算機模擬投掷硬幣1000次,然后繪製了當 N 增加时 $\frac{N_H}{N}$ 的比例發生什麼的圖片。事實上,為了確保這不是偶然的,我做了四次。 結果如 @fig-fig7-1 所示。 如您所見,觀察到的正面比例最终停止波动並穩定下来。 當它穩定时,它最终停留的数字就是正面機率的真實值。


次數主義機率定義具有一些理想的特征。首先,它是客觀的。事件的機率必然取決於事實。機率陳述只有在它們指代(一系列)物理世界中發生的事件時才有意義。[^07-introduction-to-probability-1] 其次,它是明確的。觀察同一序列事件展開的任何两人,企圖计算某個事件的機率,必然计算出相同的答案。

[^07-introduction-to-probability-1]: 這當然並不是說次數主義者不能做出假設性陳述。只是如果您想做出關於機率的陳述,則必須有可能將該陳述重新描述為潛在可觀測事件序列及該序列中出現的不同結果的相對頻率。

然而,它也有不利的特点。首先,物理世界中没有存在無窮序列。假設您从口袋里拿出一枚硬幣並開始擲掷。 每次它落地時都會撞到地面。 每次撞擊都會使硬幣磨損一點。 最终硬幣會被摧毀。 因此,有人可能会问,假装“无限”次投掷硬幣序列甚至是有意义的概念,或者是客觀的概念,这真的有意义吗? 我们不能说“无限序列”事件在物理宇宙中是真实的东西,因為物理宇宙不允许无限的任何东西。 更严重的是,次數主義定義的範圍很窄。 人類在日常語言中很樂意將機率指派给很多事物,但是在理論上也無法將其映射到一個假想的事件序列上。例如,如果氣象預報員上電視说“2018年11月2日阿德雷德降雨的機率為60%”,我們人類很樂意接受這一點。 但是很難用次數主義术語來定義這一點。 阿德雷德只有一座城市,2048年11月2日也只有一天。这里没有无限序列,只是一個一次性事件。 次數主義機率確實禁止我們對單個事件進行機率陳述。從次數主義角度來看,明天要么下雨,要么不下雨。 沒有可以指派给不可重複事件的“機率”。 当然,應該指出次數主義者可以使用一些非常聰明的技巧來解決这個问题。 一种可能性是氣象預報員的意思有點像:“有一類我預測有 60% 機率降雨的日子,如果我們只看我做出這個預測的那些日子,那么在 60% 的那些日子裡,降雨是實際发生的”。 這樣想有點奇怪和反直覺,但您會看到次數主義者有時會這樣做。 这将在本书的后面出现(例如在 @sec-Estimating-a-confidence-interval)。

```{r}
#| label: fig-fig7-1
#| fig-cap: 次數主義機率工作方式的示例。 如果您一次又一次地投擲一枚公平的硬幣,那么您看到的正面比例最终會穩定在真實機率 0.5。 每個面板顯示了四個不同的模擬實驗。 在每種情况下,我們假裝我們投掷了 $1000$ 次硬幣,並在過程中記錄了正面的比例。 儘管這些序列中没有一個確切地以 $.5$ 結束,但是如果我們將實驗擴展到無限次投掷硬幣,它們會的。
#knitr::include_graphics("images/fig7-1.png")

coinflips <- list()
coinflips <- lapply(1:4, function(i) {
    ggplot(data=tibble(flips = 1:1000, heads = cumsum(as.numeric( runif(1000) > .5 )) / (1:1000)),
           aes(x=flips, y=heads)) +
    geom_line(color=blueshade, linewidth=1) +
    ylim(0.3, 0.7) +
    xlim(0, 1000) +
    geom_hline(yintercept=0.5, color="grey") +
    xlab("投擲硬幣次數") +
    ylab("硬幣正面朝上的比例") +
    theme_classic()
})
gridExtra::grid.arrange(grobs = coinflips)
```

### 貝氏觀點


<!---
**貝氏觀點(The Bayesian view)** of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what "the" Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the **事件發生的信念程度(degree of belief)** that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don't exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.[^translation-4]

[^translation-4]: 譯註~數學家版的貝氏定理$$Pr(A|B) = \frac{Pr(B|A)*Pr(A)}{Pr(B)}$$ 統計學家版的貝氏定理$$Pr(H_i|Data) = \frac{Pr(Data|H_i)*Pr(H_i)}{\sum_{j=1}^n Pr(Data|H_j)*Pr(H_j)}$$ 兩套公式取自旗標出版[AI 必須！從做中學貝氏統計](https://www.flag.com.tw/books/product/F2308)。

However, in order for this approach to work we need some way of operationalising "degree of belief". One way that you can do this is to formalise it in terms of "rational gambling", though there are many other ways. Suppose that I believe that there's a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win \$5, but if it doesn't rain I lose \$5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it's a bad bet to take. So we can operationalise the notion of a "subjective probability" in terms of what bets I'm willing to accept.

What are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don't need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can't be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don't have to, even though we're both rational. The frequentist view doesn't allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers). --->

貝氏觀點的機率通常被稱為主觀機率,雖然在統計學家中一直是少數派觀點,但在過去几十年中一直在穩步獲得影響力。 貝氏主義有很多流派,很難確切說明“貝氏”觀點是什麼。 最常見的主觀機率思維方式是將事件的機率定義為智能合理的代理人對該事件事實的信念程度。 從這個角度來看,機率並不存在于世界中,而是存在于人類和其他智能生物的思想和假設中。[^translation-1]

[^translation-1]: 譯註~數學家版的貝氏定理$$Pr(A|B) = \frac{Pr(B|A)*Pr(A)}{Pr(B)}$$ 統計學家版的貝氏定理$$Pr(H_i|Data) = \frac{Pr(Data|H_i)*Pr(H_i)}{\sum_{j=1}^n Pr(Data|H_j)*Pr(H_j)}$$ 兩套公式取自旗標出版[AI 必須！從做中學貝氏統計](https://www.flag.com.tw/books/product/F2308)。

然而,為了使這種方法奏效,我們需要一些方法來操作化“信念程度”。 一種可以做到這一點的方法是用“理性博弈”的方式將其形式化,儘管還有許多其他方法。 假设我相信明天下雨的機率為 60%。 如果有人提供我一個賭注,如果明天下雨,我贏5美元,如果不下雨,我輸5美元。 從我的角度來看,這顯然是一個相當好的賭注。 另一方面,如果我認為降雨的機率只有 40%,那麼接受這個賭注就是不明智的。 因此,我們可以通過我願意接受的賭注操作化“主觀機率”的概念。

貝氏方法的優缺點是什麼? 主要優點是它允許您指派想要的任何事件的機率。 您不需要局限于那些可重複的事件。 主要缺點(對許多人來說)是我們不能完全客觀。 指定一個機率要求我們指定一個具有相應信念程度的實體。 這個實體可能是一個人類、外星人、機器人或者甚至一個統計學家。 但是必須有一個智能代理存在並有某些信念。 對許多人來說,這是令人不舒服的,似乎會使機率變得武斷。 儘管貝氏方法要求相關代理人是理性的(即遵守機率規則),但它確實允許每個人都有自己的信念。 在硬幣公平的問題上,我可以相信硬幣是公平的,而您不需要,即使我們都是理性的。 次數主義觀點不允許任何两個觀察者為同一事件指派不同的機率。 當這種情况發生時,至少其中一人必須是錯的。 貝氏觀點並没有阻止这种情况发生。 兩個具有不同背景知識的觀察者可以合法地對同一事件持有不同信念。 简而言之,次數主義觀點有時被認為範圍太窄(禁止很多我們想要指派機率的事物),而貝氏觀點有時則被認為範圍太廣(允許觀察者之間太多差異)。

### 觀點之間的差異是什麼？何者正確？

<!--- Now that you've seen each of these two views independently it's useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you've understood the two perspectives you should have some sense of how to answer those questions.

Okay, assuming you understand the difference then you might be wondering which of them is *right*? Honestly, I don't know that there is a right answer. As far as I can tell there's nothing mathematically incorrect about the way frequentists think about sequences of events, and there's nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.

For the most part, I'm a pragmatist so I'll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I'll explain towards the end of the book. But I'm not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as "an impenetrable jungle [that] arrests progress towards precision of statistical concepts" [@Fisher1922b, p. 311]. Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into "a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring" [@Meehl1967 p. 114]. The history of statistics, as you might gather, is not devoid of entertainment.

In any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you'll need a good grasp of frequentist methods. I promise you that this isn't wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the "orthodox" frequentist view. Besides, I won't completely ignore the Bayesian perspective. Every now and then I'll add some commentary from a Bayesian point of view, and I'll revisit the topic in more depth in @sec-Bayesian-statistics.--->


既然您已經看到了這兩種觀點中的每一種,確保您能夠對比兩者是很有用的。回到本節開頭的假想機器人足球比賽。您認為次數主義者和貝氏主義者會如何看待這三個陳述?哪個陳述次數主義者會說是機率的正確定義?貝氏主義者會選擇哪一個?這些陳述中有些對次數主義者或貝氏主義者來說是否毫無意義?如果您理解了這兩種角度,您應該在某種程度上能夠回答這些問題。  

好的,假設您理解其中的區別,那麼您可能想知道**哪一種是正確的**?老實說,我不知道是否有正確的答案。據我所知,次數主義者思考事件序列的方式在數學上沒有什麼不正確的,貝氏主義者定義一個理性代理人的信念的方式在數學上也沒有什麼不正確的。事實上,當您深入研究細節時,貝氏主義者和次數主義者實際上在很多事情上意見一致。許多次數主義方法會導致貝氏主義者認為一個理性代理人會做出的決定。許多貝氏主義方法具有非常好的次數主義特性。  

在大多數情况下,我是務實主義者,所以我會使用任何我信任的統計方法。事實證明,這使我更喜歡貝氏主義方法,原因我將在書的最後解釋。但是我並不從根本上反對次數主義方法。並非每個人都如此輕鬆自在。例如,考慮 @Fisher1922b 的羅納德·費雪爵士,20世紀統計學的偉大巨人之一,反對一切與貝氏主義有關的事物的激烈反對者,他在統計學數學基礎文章中將貝氏機率稱為 “不可突破的叢林*會* 阻止統計概念的精準進步”(第 311 頁)。或者 @Meehl1967 的心理學家保羅·米爾,他認為依靠次數主義方法會使您變成 “一個有生育能力但不育的知識分子荒腔走板,在他快樂的道路上留下了一長串受害者,但沒有任何有效的科學後代”(第 114 頁)。您可能會發現,統計學的歷史並不乏味。  

無論如何,雖然我個人更喜歡貝氏主義的觀點,但大多數統計分析都是基於次數主義方法的。我的推論很務實。這本書的目的是涵蓋與典型本科心理統計課大致相同的內容,如果您想了解大多數心理學家使用的統計工具,您將需要很好地掌握次數主義方法。我向您保證,這並不是浪費精力。即使最終您想切換到貝氏主義角度,您也真的應該至少通讀一本關於 “正統” 次數主義觀點的書。此外,我不會完全忽略貝氏主義角度。每隔一段時間,我會從貝氏主義角度添加一些評論,並將在 @sec-Bayesian-statistics 更深入地重新訪問這個話題。  


## 基本機率理論 {#sec-Basic-probability-theory}

<!---Ideological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won't go into a lot of detail, but I'll try to give you a bit of a sense of how it works. And in order to do so I'm going to have to talk about my trousers. --->

### 機率分佈入門 {#sec-introducing-probability-distributions}

<!--- teaching note
使用 seeing theory展示機率分佈
https://seeing-theory.brown.edu/
--->

<!-- One of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I've given them names: I call them $X_1$, $X_2$, $X_3$, $X_4$ and $X_5$. I really have, that's why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I'm so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each $X$) as an elementary event. The key characteristic of **elementary events** is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a **sample space**. Granted, some people would call it a "wardrobe", but that's because they're refusing to think about my trousers in probabilistic terms. Sad.

Okay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a **probability** of one of these elementary events. For an event $X$, the probability of that event $P(X)$ is a number that lies between 0 and 1. The bigger the value of $P(X)$, the more likely the event is to occur. So, for example, if $P(X) = 0$ it means the event $X$ is impossible (i.e., I never wear those trousers). On the other hand, if $P(X) = 1$ it means that event $X$ is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if $P(X) = 0.5$ it means that I wear those trousers half of the time.

At this point, we're almost done. The last thing we need to recognise is that "something always happens". Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the **law of total probability**, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a **probability distribution**. For example, @tbl-tab7-2 shows an example of a probability distribution. --->


我生活中令人不安的真相之一是,我只有5條褲子。三條牛仔褲,一套西裝的下半身和一條運動褲。更令人難過的是,我給它們起了名字:我把它們稱為$X_1$、$X_2$、$X_3$、$X_4$ 和 $X_5$。我真的有,這就是為什麼他們稱我為Mister想像力。現在,在任何給定的一天,我會挑選穿上正好一條褲子。即使是我,也不會愚蠢到嘗試穿兩條褲子,而且由於多年的訓練,我再也不會不穿褲子就外出了。如果我要用機率論的語言來描述這種情况,我會將每條褲子(即每個 $X$)稱為基本事件。基本事件的關鍵特徵在於,每次我們進行觀察(例如,每次我穿上一條褲子時)的結果將是且只是這些事件中的一個。就像我說的,近年來我總是穿著正好一條褲子,所以我的褲子滿足這個約束條件。同樣,所有可能事件的集合稱為樣本空間。當然,有些人會稱它為“衣櫃”,但那是因為他們拒絕以機率術語來思考我的褲子。太遺憾了。

好的,現在我們有了一個樣本空間(衣櫃),它是由許多可能的基本事件(褲子)構建而成,我們要做的就是為這些基本事件中某個分配一個機率。對於事件$X$,該事件的機率$P(X)$是介於0和1之間的數字。$P(X)$的值越大,事件發生的可能性就越大。因此,例如,如果$P(X)=0$,則表示事件$X$是不可能發生的(即,我從不穿那條褲子)。另一方面,如果$P(X)=1$,則表示事件$X$必定會發生(即,我總是穿那條褲子)。對於介於兩者之間的機率值,意味著我有時會穿那條褲子。例如,如果$P(X)=0.5$,那麼意味著我一半的時間穿這條褲子。

至此,我們幾乎完成了。我們需要認識到的最後一件事是“某件事總會發生”。每次我穿褲子時,我確實最終都會穿上褲子(瘋狂,對吧?)。這個有點陳腔濫調的語句在機率術語中的意思是,基本事件的機率需要加起來等於1。這被稱為機率總律,儘管我們都不太在意。更重要的是,如果滿足這些要求,那麼我們就得到了一個機率分佈。例如, @tbl-tab7-2 顯示了機率分佈的一個示例。

```{r}
#| label: tbl-tab7-2
#| tbl-cap: 穿褲子的機率分佈
#huxtabs[[7]][[2]]
kable(data.frame(
    `褲子樣式` = c("藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲"),
    `事件代號` = c("X_1", "X_2", "X_3", "X_4", "X_5"),
    `事件機率` = c("P(X_1)=.5", "P(X_2) = .3", "P(X_3) = .1", "P(X_4) = .0", "P(X_5) = .1" )
),format = "html", aligh = "c", escape = FALSE)

```

<!--- Each of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see @sec-Bar-graphs) to visualise this distribution, as shown in @fig-fig7-2. And, at this point, we've all achieved something. You've learned what a probability distribution is, and I've finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about **non elementary events** as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it's perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the "Dani wears jeans" event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case "blue jeans", "black jeans" or "grey jeans". In mathematical terms we defined the "jeans" event $E$ to correspond to the set of elementary events $(X1, X2, X3)$. If any of these elementary events occurs then $E$ is also said to have occurred. Having decided to write down the definition of the E this way, it's pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are $.5$, $.3$ and $.1$, the probability that I wear jeans is equal to $.9$. is: we just add everything up. In this particular case $$P(E)=P(X_1)+P(X_2)+P(X_3)$$ At this point you might be thinking that this is all terribly obvious and simple and you'd be right. All we've really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it's possible to construct some extremely powerful mathematical tools. I'm definitely not going to go into the details in this book, but what I will do is list, in @tbl-tab7-3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I've outlined above, but since we don't actually use these rules for anything in this book I won't do so here. 

A visual depiction of the 'trousers' probability distribution. There are five 'elementary events', corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1

Some rules that probabilities satisfy
--->

每個事件都有一個介于 0 和 1 之間的機率,如果我們添加所有事件的機率,它們的和為 1。太棒了。我們甚至可以繪製一個漂亮的長條圖(見 @sec-Bar-graphs)來可視化此分佈,如 @fig-fig7-2 所示。 在這一點上,我們都取得了一些成果。 您已經了解了什麼是機率分佈,而我也終于找到了一種方法來創建一個完全集中在我的褲子上的圖表。 每個人都是贏家! 我需要指出的唯一其他事項是,機率理論允許您談論非基本事件以及基本事件。 最簡單的說明該概念的方法是舉個例子。 在褲子示例中,合法地提及我穿牛仔褲的機率。 在這種情况下,“丹妮穿牛仔褲”事件若所發生的基本事件是適當的事件之一,則說已發生。 在這種情况下為“藍色牛仔褲”、“黑色牛仔褲”或“灰色牛仔褲”。 在數學術語中,我們將事件 $E$ 定義為對應于基本事件集 $(X1,X2,X3)$。 如果發生任何這些基本事件,則也說事件 $E$ 已經發生。 決定以這種方式寫下 $E$ 的定義后,很容易說明 $P(E)$ 的機率是多少,並且由于藍色、灰色和黑色牛仔褲的機率分別為 $.5$、$.3$ 和 $.1$,我穿牛仔褲的機率等于 $.9$。就是:我們只需全部加起來。 在這種特定情况下,$$P(E)=P(X_1)+P(X_2)+P(X_3)$$ 這時您可能會認為,這一切都太明顯和簡單了,您是對的。 我們所做的实际上只是在一些常識直觀周圍裹上一些基本數學。 然而,從這些簡單的開端出發,可以構建一些非常強大的數學工具。 我絕不會在這本書中深入細節,但是我會在 @tbl-tab7-3 中列出機率滿足的其他一些規則。 這些規則可以從我前面概述的簡單假設推導出來,但是由於我們在本書中實際上並没有使用這些規則,所以我不會在这里這樣做。

```{r}
#| label: fig-fig7-2
#| fig-cap: “褲子”機率分佈的視覺描繪。 有五個“基本事件”,對應於我擁有的五條褲子。 每個事件都有發生的某些機率 - 這個機率是 0 到 1 之間的數字。 所有事件的機率總和為 1
#knitr::include_graphics("images/fig7-2.png")
pantsdat <- tibble(probabilities = c( .5, .3, .1, 0, .1),
                    eventNames = c( "藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲" ))
pantsdat$eventNames = factor(pantsdat$eventNames, levels = c( "藍色牛仔褲", "灰色牛仔褲", "黑色牛仔褲","黑色西裝褲","藍色運動褲" ))

ggplot(data=pantsdat, aes(x=eventNames, y=probabilities)) +
  geom_bar(stat='identity', col="black", fill=blueshade, alpha=0.6) +
  theme_classic() +
  ylab("各事件的機率") +
  xlab("事件")
```


<!--- teaching note
使用 seeing theory展示集合論
https://seeing-theory.brown.edu/compound-probability/index.html#section1
--->


```{r}
#| label: tbl-tab7-3
#| tbl-cap: 機率滿足的一些規則
huxtabs[[7]][[3]] 
```

## 二項分佈 {#sec-The-binomial-distribution}

<!---As you might imagine, probability distributions vary enormously and there's an enormous range of distributions out there. However, they aren't all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the t distribution, the $\chi^2$ ("chi-square") distribution and the F distribution. Given this, what I'll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I'll start with the binomial distribution since it's the simplest of the five. --->

您可以想像,機率分佈會有很大差異,並且有大量不同的分佈。但是,它們並非都同等重要。事實上,本書絕大部分內容依賴於五種分佈中的一種:二項分佈、常態分佈、 t 分佈、$\chi^2$(“卡方”)分佈和 F 分佈。鑑於此,我在接下來的幾節中要做的就是簡要介紹這五種分佈,並特別關注二項分佈和常態分佈。我將從二項分佈開始,因為它是五種分佈中最簡單的。  


### 二項分佈入門

<!--- The theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the **binomial distribution** should involve a discussion of rolling dice and flipping coins. Let's imagine a simple "experiment". In my hot little hand I'm holding 20 identical six-sided dice. On one face of each die there's a picture of a skull, the other five faces are all blank. If I proceed to roll all 20 dice, what's the probability that I'll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6. To say this another way, the skull probability for a single die is approximately .167. This is enough information to answer our question, so let's have a look at how it's done.

As usual, we'll want to introduce some names and some notation. We'll let $N$ denote the number of dice rolls in our experiment, which is often referred to as the **size parameter** of our binomial distribution. We'll also use $\theta$ to refer to the the probability that a single die comes up skulls, a quantity that is usually called the **success probability** of the binomial.[^07-introduction-to-probability-2] Finally, we'll use $X$ to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of $X$ is due to chance we refer to it as a **random variable**. In any case, now that we have all this terminology and notation we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that $X = 4$ given that we know that $\theta = .167$ and $N = 20$. The general "form" of the thing I'm interested in calculating could be written as

[^07-introduction-to-probability-2]: Note that the term "success" is pretty arbitrary and doesn't actually imply that the outcome is something to be desired. If $\theta$ referred to the probability that any one passenger gets injured in a bus crash I'd still call it the success probability, but that doesn't mean I want people to get hurt in bus crashes!

$$P(X|\theta,N)$$

and we're interested in the special case where $X = 4$, $\theta = .167$ and $N = 20$.

[Additional technical detail [^07-introduction-to-probability-3]]

[^07-introduction-to-probability-3]: For those readers who know a little calculus, I'll give a slightly more precise explanation. In the same way that probabilities are non-negative numbers that must sum to 1, probability densities are non-negative numbers that must integrate to 1 (where the integral is taken across all possible values of X). To calculate the probability that X falls between a and b we calculate the definite integral of the density function over the corresponding range, $\int\_{a}^{b} p(x) dx$. If you don't remember or never learned calculus, don't worry about this. It's not needed for this book.

Yeah, yeah. I know what you're thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. I've included the formula for the binomial distribution in @tbl-tab7-2, since some readers may want to play with it themselves, but since most people probably don't care that much and because we don't need the formula in this book, I won't talk about it in any detail. Instead, I just want to show you what the binomial distribution looks like.

To that end, @fig-fig7-3 plots the binomial probabilities for all possible values of $X$ for our dice rolling experiment, from $X = 0$ (no skulls) all the way up to $X = 20$ (all skulls). Note that this is basically a bar chart, and is no different to the "trousers probability" plot I drew in @fig-fig7-2. On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling $4$ skulls out of $20$ is about $0.20$ (the actual answer is $0.2022036$, as we'll see in a moment). In other words, you'd expect that to happen about 20% of the times you repeated this experiment.

To give you a feel for how the binomial distribution changes when we alter the values of $theta$ and $N$, let's suppose that instead of rolling dice I'm actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly and the outcome that I'm interested in is the number of heads that I observe. In this scenario, the success probability is now $\theta = \frac{1}{2}$. Suppose I were to flip the coin $N = 20$ times. In this example, I've changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as @fig-fig7-4 shows, the main effect of this is to shift the whole distribution, as you'd expect. Okay, what if we flipped a coin $N = 100$ times? Well, in that case we get @fig-fig7-4 (b). The distribution stays roughly in the middle but there's a bit more variability in the possible outcomes. --->


機率論的起源是試圖描述運氣遊戲的工作原理,所以我們討論**二項分佈**時涉及擲骰子和擲幣似乎很合適。讓我們想象一個簡單的“實驗”。我滾燙的小手中拿著20個相同的六面骰子。 每個骰子的一個面上有一個骷髏圖案,其他五個面都是空白的。 如果我繼續擲出所有20個骰子,我會得到正好4個骷髏的機率是多少? 假設骰子是公平的,我們知道任何一個骰子出現骷髏的機率是 1/6。 用另一種方式說,單個骰子的骷髏機率約為.167。 這些信息足以回答我們的問題,所以讓我們看看如何完成。  

和往常一樣,我們將引入一些名稱和一些符號。我們將使用$N$表示我們實驗中的骰子擲出次數,這通常被稱為我們二項分佈的**大小參數**。 我們也將使用$\theta$引用單個骰子出現骷髏的機率,這個量通常被稱為二項式的**成功機率**[^07-introduction-to-probability-2]。 最後,我們將使用 $X$ 引用我們實驗的結果,即我擲骰子時得到的骷髏數。 由於 $X$ 的實際值是由機率決定的,我們將其稱為**隨機變數**。無論如何,現在我們有了所有這些術語和符號,我們可以用它們來更準確地陳述問題。 我們要計算的量是在知道 $\theta = .167$ 和 $N = 20$ 時 $X = 4$ 的 機率。 我有興趣計算的事物的一般“形式”可以寫為  

$$P(X|\theta,N)$$  

我們感興趣的是 $X = 4, \theta = .167$ 和 $N = 20$ 的特殊情况。  

[^07-introduction-to-probability-2]: 請注意,“成功”一詞是任意的,並不實際意味著結果是令人渴望的。 如果 $\theta$ 指的是任何一位乘客在公交車事故中受傷的機率,我仍然會稱它為成功機率,但這並不意味著我希望公交車事故中有人受傷!  

[附加技術細節 [^07-introduction-to-probability-3]]  

[^07-introduction-to-probability-3]: 對於那些知道一點微積分的讀者,我會給出一個稍微更精確的解釋。 與機率是必須相加為1的非負數的相同方式,機率密度是必須積分為1的非負數(其中積分是對X的所有可能值進行的)。 要計算X落在a和b之間的機率,我們計算密度函數在相應範圍內的定積分,$\int_{a}^{b} p(x) dx$。 如果您不記得或者從未學過微積分,請不要太擔心這個。 這本書不需要這個。  

是的,是的。 我知道你在想什麼:符號,符號,符號。 真的,誰在乎? 這本書的很少讀者是為了符號而來的,所以我可能應該繼續前進,談論如何使用二項分佈。 我已在腳注中[^07-introduction-to-probability-3_1]包含了二項分佈的公式,因為一些讀者可能想要自己使用它,但是由於大多數人可能不太在意,並且因為我們在本書中並不需要該公式,所以我不會詳細談論它。 相反,我只想讓你看看二項分佈的樣子。  

[^07-introduction-to-probability-3_1]: 在二項式方程中,$X!$ 是階乘函數(即,將從1乘到$X$的所有整數相乘):$$P(X | \theta, N) = \displaystyle\frac{N!}{X!(N-X)!}\theta^X(1-\theta)^{N-X}$$ 如果這個方程對您來說没有太大意义,请不要太擔心。 这本书不需要它。

為此,@fig-fig7-3 繪製了我們的擲骰實驗中所有可能的 $X$ 值的二項式機率,從 $X = 0$(没有骷髏)一直到 $X = 20$(全部骷髏)。 請注意,這基本上是一個長條圖,与我在 @fig-fig7-2 中繪製的“褲子機率”圖沒有區別。 在水平軸上,我們有所有可能的事件,在垂直軸上,我們可以讀取每個事件的機率。 所以,滾出 $4$ 個骷髏的機率是約 $0.20$(實際答案是 $0.2022036$,我們很快會看到)。 換句話說,如果您重複這個實驗,預計這种情况会发生大约 20% 的次数。  


```{r}
#| label: fig-fig7-3
#| fig-cap: 大小參數為 $N = 20$,基礎成功機率為 $\theta = \frac{1}{6}$ 的二項分佈。 每個柱子對應一個特定结果(即X的一個可能值)的機率。 因為这是一個機率分佈,所以每個機率必須是 0 到 1 之間的数,而且所有柱子的高度之和也必須為 1  
#knitr::include_graphics("images/fig7-3.png")

binom1 <- data.frame(heads = 0:20, pmf = dbinom(x = 0:20, size = 20, prob = 1/6)+0.0001)

ggplot(data=binom1, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "\n擲出骷髏的次數",
       y = "機率\n") +
  scale_x_discrete(breaks=seq(0,20,5)) +
  theme_classic()
```

<!--- I were to flip the coin $N = 20$ times. In this example, I've changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as @fig-fig7-4 (a) shows, the main effect of this is to shift the whole distribution, as you'd expect. Okay, what if we flipped a coin $N = 100$ times? Well, in that case we get @fig-fig7-4 (a). The distribution stays roughly in the middle but there's a bit more variability in the possible outcomes. --->


為了讓您感受一下改變 $\theta$ 和 $N$ 的值时二項分佈會发生什么变化,讓我們假設我实际上是在擲硬幣而不是擲骰子。 這次,我的實驗涉及反复擲掷一枚公平的硬幣,我感興趣的結果是我觀察到的頭數。 在這種情况下,成功機率現在是 $\theta = \frac{1}{2}$。 假設我擲硬幣 $N = 20$ 次。 在這個示例中,我改變了成功機率但保持了實驗的規模。 這對我們的二項分佈有什么影響? 好吧,正如 @fig-fig7-4 所示,這的主要影響是移動整個分佈,正如您所料。 好吧,如果我擲 $N = 100$ 次硬幣呢? 那麼,在這種情况下,我們得到 @fig-fig7-4(b)。 該分佈仍保持在中間,但可能的結果有更多變化。  


```{r}
#| label: fig-fig7-4
#| fig-width: 5
#| fig-height: 7
#| fig-cap: 兩個二項分佈,涉及我擲公平硬幣的情况,所以基礎成功機率是 $\theta = \frac{1}{2}$。 在(a)面板中,我擲了 $N = 20$ 次硬幣。 在(b)面板中,硬幣被擲了 $N = 100$ 次 
#knitr::include_graphics("images/fig7-4.png")

binom2 <- data.frame(heads = 0:20, pmf = dbinom(x = 0:20, size = 20, prob = 1/2)+0.0001)
bd1 <- ggplot(data=binom2, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "擲出骷髏的次數",
       y = "機率") +
  scale_x_discrete(breaks=seq(0,20,5)) +
  labs(tag = "(a)") +
  theme(plot.tag.position = c(0.3, 0.70)) +
  theme_classic()

binom3 <- data.frame(heads = 0:100, pmf = dbinom(x = 0:100, size = 100, prob = 1/2)+0.0001)
bd2 <- ggplot(data=binom3, aes(x = factor(heads), y = pmf)) +
  geom_col(col=blueshade, fill=blueshade, alpha=0.6, width=.1) +
  labs(x = "\n擲出骷髏的次數",
       y = "機率\n") +
  scale_x_discrete(breaks=seq(0,100,5)) +
  labs(tag = "(b)") +
  theme(plot.tag.position = c(0.3, 0.70)) +
  theme_classic()

gridExtra::grid.arrange(bd1,bd2,nrow=2)
```


## 常態分佈 {#sec-The-normal-distribution}

<!---While the binomial distribution is conceptually the simplest distribution to understand, it's not the most important one. That particular honour goes to the normal distribution, also referred to as "the bell curve" or a "Gaussian distribution". A **normal distribution** is described using two parameters: the mean of the distribution µ and the standard deviation of the distribution $\sigma$.

[Additional technical detail [^07-introduction-to-probability-3_1]]

[^07-introduction-to-probability-3_1]: The notation that we sometimes use to say that a variable $X$ is normally distributed is as follows: $$X \sim Normal(\mu,\sigma)$$ Of course, that's just notation. It doesn't tell us anything interesting about the normal distribution itself. As was the case with the binomial distribution, I have included the formula for the normal distribution in this book, because I think it's important enough that everyone who learns statistics should at least look at it, but since this is an introductory text I don't want to focus on it, so I've tucked it away in this footnote.

Let's try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at @fig-fig7-5 which plots a normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. You can see where the name "bell curve" comes from; it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in @fig-fig7-5 shows a smooth curve instead of "histogram-like" bars. This isn't an arbitrary choice, the normal distribution is continuous whereas the binomial is discrete. For instance, in the die rolling example from the last section it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I drew in the previous section reflected this fact. In @fig-fig7-3, for instance, there's a bar located at $X = 3$ and another one at $X = 4$ but there's nothing in between. Continuous quantities don't have this constraint. For instance, suppose we're talking about the weather. The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, 23.9 degrees, or anything in between since temperature is a continuous variable. And so a normal distribution might be quite appropriate for describing Spring temperatures[^07-introduction-to-probability-5]

[^07-introduction-to-probability-5]: In practice, the normal distribution is so handy that people tend to use it even when the variable isn't actually continuous. As long as there are enough categories (e.g., Likert scale responses to a questionnaire), it's pretty standard practice to use the normal distribution as an approximation. This works out much better in practice than you'd think. 

@fig-fig7-5 The normal distribution with mean $\mu = 0$ and standard deviation $\sigma = 1$. The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled *Probability Density* and not *Probability*. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the $y$ axis behave a bit oddly - the height of the curve here isn't actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which $x$ values are more likely (the higher ones!). (see [Probability density] section for all the annoying details)

@fig-fig7-6 An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of $\mu = 4$. The dashed line shows a normal distribution with a mean of $\mu = 7$. In both cases, the standard deviation is $\sigma = 1$. Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right

@fig-fig7-7 An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of $\mu = 5$, but they have different standard deviations. The solid line plots a distribution with standard deviation $\sigma = 1$, and the dashed line shows a distribution with standard deviation $\sigma = 2$. As a consequence, both distributions are 'centred' on the same spot, but the dashed line is wider than the solid one --->


雖然概念上二項分佈是最簡單的分佈,但它並不是最重要的分佈。這項特殊榮譽屬於常態分佈,也稱為“鐘形曲線”或“高斯分佈”。**常態分佈**使用兩個參數來描述:分佈的均值 μ 和分佈的標准差 $\sigma$。我們有時使用的符號來說明一個變數 $X$ 是常態分佈的如下:  

$$X \sim \text{Normal}(\mu,\sigma)$$  

[附加技術細節 [^07-introduction-to-probability-4]]  

[^07-introduction-to-probability-4]: 和二項分佈一樣,我在這本書中包含了常態分佈的公式,因為我認為每個學習統計學的人都應該至少看一看它很重要,但是由于這是一本簡介教材,所以我不想專注于此,因此我將其隱藏在此脚注中:  

$$p(X|\mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(X-\mu)^2}{2\sigma^2}}$$  

讓我們試圖瞭解變數遵循常態分佈意味著什麼。為此,请看 @fig-fig7-5,其中繪製了平均值 $\mu = 0$ 和標准差 $\sigma = 1$ 的常態分佈。您可以看到它被稱為“鐘形曲線”的原因;它看起來有點像鐘。 請注意,與我繪製的用於說明二項式分佈的圖不同,@fig-fig7-5 中常態分佈的圖片顯示了光滑的曲線而不是“直方圖狀”的長條。這不是任意選擇的,常態分佈是連續的,而二項式是離散的。例如,在上一節中的擲骰子示例中,可以獲得3個骷髏或4個骷髏,但不可能獲得3.9個骷髏。我在上一節中繪製的圖反映了這一事實。例如,在 @fig-fig7-3 中,在 $X = 3$ 處有一個長條,在 $X = 4$ 處有另一個長條,但之間没有。 連續量没有這種限制。例如,假設我們在談論天氣。 悠閒的春天氣溫可能是23度、24度、23.9度或介于兩者之間的任何位置,因為溫度是連續變數。 因此,常態分佈可能非常適合用于描述春季氣溫[^07-introduction-to-probability-5]。  

[^07-introduction-to-probability-5]: 在實踐中,常態分佈非常方便,以至於人們傾向於在變數實際上並不是連續時也使用它。只要類別足够多(例如,問卷的李克特量表響應),使用常態分佈作為近似是相當標準的做法。這在實踐中比你想象的工作得更好。  




```{r}
#| label: fig-fig7-5
#| fig-cap: 平均值 $\mu = 0$ 和標准差 $\sigma = 1$ 的常態分佈。 x 軸對應于某個變數的值,y 軸告訴我們觀察到该值的可能性有多大。 但是,請注意 y 軸標籤為機率密度而不是機率。 連續分佈具有一些微妙且有些令人沮喪的特征,使得 y 軸的行為有點奇怪——这里曲线的高度实际上並不是观测到特定 x 值的機率。 另一方面,曲線的高度确实告訴您哪些 x 值更有可能(更高的那些!)。 有關所有令人討厭的細節,請參見[機率密度]部分。 
#knitr::include_graphics("images/fig7-5.png")

ggplot(data = data.frame(x = seq(-3, 3, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-3, 3, 1)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```


```{r}
#| label: fig-fig7-6
#| fig-cap: 說明當您更改常態分佈的均值時會發生什麼。 實線描繪了一個均值為 $\mu = 4$ 的常態分佈。 虛線顯示了一個均值為 $\mu = 7$ 的常態分佈。 在這兩種情况下,標准差都是 $\sigma = 1$。 不令人驚訝,这兩個分佈具有相同的形狀,但虛線向右移動。
#knitr::include_graphics("images/fig7-6.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 4, sd = 1), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 7, sd = 1), col=blueshade, linetype=2, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```

<!--- With this in mind, let's see if we can't get an intuition for how the normal distribution works. First, let's have a look at what happens when we play around with the parameters of the distribution. To that end, @fig-fig7-6 plots normal distributions that have different means but have the same standard deviation. As you might expect, all of these distributions have the same "width". The only difference between them is that they've been shifted to the left or to the right. In every other respect they're identical. In contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place but the distribution gets wider, as you can see in @fig-fig7-7. Notice, though, that when we widen the distribution the height of the peak shrinks. This has to happen, in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to sum to 1, the total area under the curve for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, $68.3\%$ of the area falls within 1 standard deviation of the mean. Similarly, $95.4\%$ of the distribution falls within 2 standard deviations of the mean, and $(99.7\%)$ of the distribution is within 3 standard deviations. This idea is illustrated in @fig-fig7-8; see also @fig-fig7-9.


@fig-fig7-8 The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean $\mu = 0$ and standard deviation $\sigma = 1$. The shaded areas illustrate 'areas under the curve' for two important cases. In panel (a), we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel (b), we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean

@fig-fig7-9 Two more examples of the 'area under the curve idea'. There is a 15.9% chance that an observation is one standard deviation below the mean or smaller (panel (a)), and a 34.1% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel (b)). Notice that if you add these two numbers together you get 15.9% + 34.1% = 50%. For normally distributed data, there is a 50% chance that an observation falls below the mean. And of course that also implies that there is a 50% chance that it falls above the mean
--->



考慮到這一點,讓我們看看我們是否可以直觀地瞭解常態分佈的工作原理。首先,讓我們看一下改變分佈參數時會發生什麼。為此, @fig-fig7-6 繪製了具有不同均值但具有相同標准差的常態分佈。您可能預期的是,所有這些分佈都具有相同的“寬度”。它們之間的唯一區別在于它們已向左或向右移動。在其他所有方面,它們都是相同的。相比之下,如果我們在保持均值不變的情况下增加標准差,則分佈的峰值保持在同一位置,但分佈變得更寬,如您在 @fig-fig7-7 中所見。但是,請注意,當我們擴大分佈時,峰值的高度會下降。這必須發生,就像我們用於繪製離散二項式分佈的長條的高度必須相加為1,常態分佈下的總面積也必須等于1。在繼續讀下去之前,我想指出常態分佈的一個重要特性。無論實際平均值和標準差是多少,$68.3\%$ 的面積落在平均值的1個標準差內。同樣,$95.4\%$ 的分佈在均值的2個標準差内,$99.7\%$ 的分佈在3個標準差内。 @fig-fig7-8 和 @fig-fig7-9 說明了這個想法;另請參見 @fig-fig7-9 。


```{r}
#| label: fig-fig7-7
#| fig-cap: 說明更改常態分佈的標准差時會發生什麼。 本圖中繪製的兩個分佈均具有均值 $\mu = 5$,但標准差不同。 實線繪製了一個標准差為 $\sigma = 1$ 的分佈,虛線顯示了一個標准差為 $\sigma = 2$ 的分佈。 因此,這兩個分佈均以相同的位置“居中”,但虛線比實線寬。
#knitr::include_graphics("images/fig7-7.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 5, sd = 1), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 5, sd = 2), col=blueshade, linetype=2, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度ty\n") +
  xlab("\n觀察值") +
  theme_classic()

```

```{r}
#| label: fig-fig7-8
#| fig-width: 8
#| fig-height: 4
#| fig-cap: 曲線下的陰影面積告訴您觀察值落在特定範圍内的機率。 實線繪製了均值 $\mu = 0$ 和標准差 $\sigma = 1$ 的常態分佈。 著色區域說明了兩種重要情況下的“曲線下陰影面積”。 在圖(a),我們可以看到觀察值落在平均值一個標準差内的機率為 68.3%。 在圖(b),我們可以看到觀察值落在均值兩個標准差内的機率為 95.4%。
#knitr::include_graphics("images/fig7-8.png")

p1 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-1, 1)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 68.3%") +
  xlab("\n(a)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

p2 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-2, 2)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 95.4%") +
  xlab("\n(b)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

gridExtra::grid.arrange(p1,p2, ncol=2)
```

```{r}
#| label: fig-fig7-9
#| fig-width: 8
#| fig-height: 4
#| fig-cap: “曲線下面積”思想的另外兩個例子。 觀察值小於或等於均值一個標准差的機率為 15.9%(a 面板),並且觀察值落在均值一個標准差和均值之間的機率為 34.1%(b 面板)。 請注意,如果您把這兩個數字加在一起,您會得到 15.9% + 34.1% = 50%。 對於常態分佈的数据,觀察值低於均值的機率為 50%。 當然,這也意味著它高於均值的機率為 50%。
#knitr::include_graphics("images/fig7-9.png")

p3 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-4, -1)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 15.9%") +
  xlab("\n(a)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

p4 <- ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", geom = "area",
                fill = "grey", alpha=0.8, xlim = c(-1, 0)) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 1)) +
  ggtitle("陰影面積 = 34.1%") +
  xlab("\n(b)") +
  theme_classic() +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.title = element_text(hjust = 0.5))

gridExtra::grid.arrange(p3,p4, ncol=2)
```

### 機率密度

<!--- There's something I've been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so. This "thing" that I'm hiding is weird and counter-intuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it's not something that you need to understand at a deep level in order to do basic statistics. Rather, it's something that starts to become important later on when you move beyond the basics. So, if it doesn't make complete sense, don't worry too much, but try to make sure that you follow the gist of it.

Throughout my discussion of the normal distribution there's been one or two things that don't quite make sense. Perhaps you noticed that the y-axis in these figures is labelled "Probability Density" rather than density. Maybe you noticed that I used $P(X)$ instead of $P(X)$ when giving the formula for the normal distribution.

As it turns out, what is presented here isn't actually a probability, it's something else. To understand what that something is you have to spend a little time thinking about what it really means to say that $X$ is a continuous variable. Let's say we're talking about the temperature outside. The thermometer tells me it's $23$ degrees, but I know that's not really true. It's not exactly $23$ degrees. Maybe it's $23.1$ degrees, I think to myself. But I know that that's not really true either because it might actually be $23.09$ degrees. But I know that... well, you get the idea. The tricky thing with genuinely continuous quantities is that you never really know exactly what they are.

Now think about what this implies when we talk about probabilities. Suppose that tomorrow's maximum temperature is sampled from a normal distribution with mean $23$ and standard deviation 1. What's the probability that the temperature will be exactly $23$ degrees? The answer is "zero", or possibly "a number so close to zero that it might as well be zero". Why is this? It's like trying to throw a dart at an infinitely small dart board. No matter how good your aim, you'll never hit it. In real life you'll never get a value of exactly $23$. It'll always be something like $23.1$ or $22.99998$ or suchlike. In other words, it's completely meaningless to talk about the probability that the temperature is exactly $23$ degrees. However, in everyday language if I told you that it was $23$ degrees outside and it turned out to be $22.9998$ degrees you probably wouldn't call me a liar. Because in everyday language "$23$ degrees" usually means something like "somewhere between $22.5$ and $23.5$ degrees". And while it doesn't feel very meaningful to ask about the probability that the temperature is exactly $23$ degrees, it does seem sensible to ask about the probability that the temperature lies between $22.5$ and $23.5$, or between $20$ and $30$, or any other range of temperatures.

The point of this discussion is to make clear that when we're talking about continuous distributions it's not meaningful to talk about the probability of a specific value. However, what we can talk about is the probability that the value lies within a particular range of values. To find out the probability associated with a particular range what you need to do is calculate the "area under the curve". We've seen this concept already, in @fig-fig7-8 the shaded areas shown depict genuine probabilities (e.g., in @fig-fig7-8) it shows the probability of observing a value that falls within 1 standard deviation of the mean).

Okay, so that explains part of the story. I've explained a little bit about how continuous probability distributions should be interpreted (i.e., area under the curve is the key thing). But what does the formula for ppxq that I described earlier actually mean? Obviously, $P(x)$ doesn't describe a probability, but what is it? The name for this quantity $P(x)$ is a **probability density**, and in terms of the plots we've been drawing it corresponds to the height of the curve. The densities themselves aren't meaningful in and of themselves, but they're "rigged" to ensure that the area under the curve is always interpretable as genuine probabilities. To be honest, that's about as much as you really need to know for now.[^07-introduction-to-probability-6]

[^07-introduction-to-probability-6]: For those readers who know a little calculus, I'll give a slightly more precise explanation. In the same way that probabilities are non-negative numbers that must sum to 1, probability densities are non-negative numbers that must integrate to 1 (where the integral is taken across all possible values of X). To calculate the probability that X falls between a and b we calculate the definite integral of the density function over the corresponding range, $\int\_{a}^{b} p(x) dx$. If you don't remember or never learned calculus, don't worry about this. It's not needed for this book. --->


在我討論常態分佈的過程中,有一兩件事情不太合理。您可能已經注意到這些圖中的 y 軸標記為“機率密度”而不是密度。您可能已經注意到,在給出常態分佈公式時,我使用了 $p(X)$ 而不是 $P(X)$。  

事實證明,這裡呈現的並不是實際的機率,而是其他東西。要理解那是什麼,您需要花一些時間思考什麼是說 $X$ 是連續變數的真正意思。假設我們正在談論室外的溫度。溫度計告訴我是 $23$ 度,但我知道這並不是真的。它並不是正好 $23$ 度。也許是 $23.1$ 度,我心裡想。但我知道這也並不是真的,因為它實際上可能是 $23.09$ 度。但是我知道......好吧,你懂的。真正連續量的棘手之處在於您永遠不會真正知道它們的準確值。  

現在想一想這對我們談論機率意味著什麼。假設明天的最高溫度從平均值為 $23$ 度、標准差為 1 的常態分佈中抽樣。溫度正好是 $23$ 度的機率是多少?答案是“零”,或者可能是“一個如此接近零以至於可以視為零的數字”。這是為什麼?這就像試圖向一個無限小的飛鏢靶心投一支鏢。無論您的瞄準多好,您都永遠不會命中。在現實生活中,您永遠不會得到正好 $23$ 度的值。它總會是像 $23.1$ 或 $22.99998$ 或者類似的值。換句話說,談論溫度正好是 $23$ 度的機率是完全没有意義的。然而,在日常語言中,如果我告訴你室外是 $23$ 度,而實際上是 $22.9998$ 度,你可能不會稱我為騙子。因為在日常語言中,$23$ 度通常意味著“在 $22.5$ 度和 $23.5$ 度之間的某個位置”。雖然問溫度正好是 $23$ 度的機率並不太有意義,但是問溫度在 $22.5$ 度和 $23.5$ 度之間或者在 $20$ 度和 $30$ 度之間或者任何其他溫度範圍之間的機率確實很合理。   

這段討論的重點是明確指出,當我們談論連續分佈時,談論某個特定值的機率並没有意義。然而,我們可以談論的值落在某個值範圍內的機率。為了找出某個範圍相關的機率,您需要做的就是計算“曲線下的面積”。我們已經看過這個概念了,在 @fig-fig7-8 中顯示的陰影區域描述了真實的機率(例如,在 @fig-fig7-8 中顯示了觀察到的值落在均值 1 個標準差內的機率)。   

好的,這就解釋了部分故事。我已經解釋了一點關於如何解讀連續機率分佈(即曲線下的面積是關鍵)。但是我早些時候描述的 $p(x)$ 公式实际上意味著什麼?顯然,$P(x)$ 並不描述一個機率,但它是什麼?這個量 $P(x)$ 的名字是**機率密度**,就我們繪製的圖而言,它對應於曲線的高度。密度本身並没有意義,但它們被“調整”以確保曲線下的面積始終可以解釋為真實機率。老實說,這就是您現在真正需要知道的全部內容。[^07-introduction-to-probability-6]   

[^07-introduction-to-probability-6]: 對於那些知道一點微積分的讀者,我會給出一個稍微更精確的解釋。與機率是必須相加為 1 的非負數的相同方式,機率密度是必須積分為 1 的非負數(其中積分是對 X 的所有可能值進行的)。要計算 X 落在 a 和 b 之間的機率,我們計算密度函數在相應範圍內的定積分,$\int_{a}^{b} p(x) dx$。如果您不記得或者從未學過微積分,請不要太擔心這個。這本書不需要這個。   



## 其他常見機率分佈 {#sec-Other-useful-distributions}

<!--- The normal distribution is the distribution that statistics makes most use of (for reasons to be discussed shortly), and the binomial distribution is a very useful one for lots of purposes. But the world of statistics is filled with probability distributions, some of which we'll run into in passing. In particular, the three that will appear in this book are the t distribution, the $\chi^2$ distribution and the F distribution. I won't give formulas for any of these, or talk about them in too much detail, but I will show you some pictures: @fig-fig7-10, @fig-fig7-11 and @fig-fig7-12. 

@fig-fig7-10 A $t$ distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it's not quite the same. For comparison purposes I've plotted a standard normal distribution as the dashed line.

@fig-fig7-11 $\chi^2$ distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square  distribution.

@fig-fig7-12 An $F$ distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they're not quite the same in general.
--->

常態分佈是統計學最常使用的分佈(原因很快會討論),二項分佈在很多目的下也非常有用。 但統計學的世界充滿了機率分佈,我們將在傳遞中遇到其中一些。 特別是,將在本書中出現的三個分佈是 t 分佈、$\chi^2$ 分佈和 F 分佈。 我不會給出這些的公式,也不會過多詳細討論它們,但我會給您顯示一些圖片:@fig-fig7-10、@fig-fig7-11 和 @fig-fig7-12。  


```{r}
#| label: fig-fig7-10
#| fig-cap: 自由度為 3 的 t 分佈(實線)。 它看起來與常態分佈相似,但並不完全相同。 為了比較,我使用虛線繪製了標准常態分佈 
#knitr::include_graphics("images/fig7-10.png")


ggplot(data = data.frame(x = seq(-4, 4, 1)), aes(x)) +
  stat_function(fun = dt, n = 1000, args = list(df = 3), col=blueshade, linewidth=1) +
  stat_function(fun = dnorm, n = 1000, args = list(mean = 0, sd = 1), col="grey", linetype=5, linewidth=1) +
  scale_x_continuous(breaks = seq(-4, 4, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```

```{r}
#| label: fig-fig7-11
#| fig-cap: 自由度為 3 的 $\chi^2$ 分佈。 請注意,觀察到的值必須始終大於零,並且該分佈相當歪斜。 這些是 $\chi^2$ 分佈的關鍵特徵
#knitr::include_graphics("images/fig7-11.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = dchisq, n = 1000, args = list(df = 3), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()

```

```{r}
#| label: fig-fig7-12
#| fig-cap: 自由度分別為 3 和 5 的 F 分佈。 從定性上講,它看起來與 $\chi^2$ 分佈非常相似,但一般情况下它們並不完全相同
#knitr::include_graphics("images/fig7-12.png")

ggplot(data = data.frame(x = seq(0, 10, 1)), aes(x)) +
  stat_function(fun = df, n = 1000, args = list(df1 = 3, df2 = 5), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 10, 2)) +
  ylab("機率密度\n") +
  xlab("\n觀察值") +
  theme_classic()
```

<!---
-   The $t$ distribution is a continuous distribution that looks very similar to a normal distribution, see @fig-fig7-10. Note that the "tails" of the t distribution are "heavier" (i.e., extend further outwards) than the tails of the normal distribution). That's the important difference between the two. This distribution tends to arise in situations where you think that the data actually follow a normal distribution, but you don't know the mean or standard deviation. We'll run into this distribution again in @sec-Comparing-two-means.
-   The $\chi^2$ distribution is another distribution that turns up in lots of different places. The situation in which we'll see it is when doing categorical data analysis in @sec-Categorical-data-analysis, but it's one of those things that actually pops up all over the place. When you dig into the maths (and who doesn't love doing that?), it turns out that the main reason why the $\chi^2$ distribution turns up all over the place is that if you have a bunch of variables that are normally distributed, square their values and then add them up (a procedure referred to as taking a "sum of squares"), this sum has a $\chi^2$ distribution. You'd be amazed how often this fact turns out to be useful. Anyway, here's what a $\chi^2$ distribution looks like: @fig-fig7-11.
-   The $F$ distribution looks a bit like a $\chi^2$ distribution, and it arises whenever you need to compare two $\chi^2$ distributions to one another. Admittedly, this doesn't exactly sound like something that any sane person would want to do, but it turns out to be very important in real world data analysis. Remember when I said that $\chi^2$ turns out to be the key distribution when we're taking a "sum of squares"? Well, what that means is if you want to compare two different "sums of squares", you're probably talking about something that has an F distribution. Of course, as yet I still haven't given you an example of anything that involves a sum of squares, but I will in @sec-Comparing-several-means-one-way-ANOVA. And that's where we'll run into the F distribution. Oh, and there's a picture in @fig-fig7-12.

Okay, time to wrap this section up. We've seen three new distributions: $\chi^2$), $t$ and $F$. They're all continuous distributions, and they're all closely related to the normal distribution. The main thing for our purposes is that you grasp the basic idea that these distributions are all deeply related to one another, and to the normal distribution. Later on in this book we're going to run into data that are normally distributed, or at least assumed to be normally distributed. What I want you to understand right now is that, if you make the assumption that your data are normally distributed, you shouldn't be surprised to see $\chi^2$, $t$ and $F$ distributions popping up all over the place when you start trying to do your data analysis. --->


- t 分佈是一種與常態分佈非常相似的連續分佈,請參見 @fig-fig7-10。 請注意,t 分佈的“尾部”比常態分佈的尾部更“重”(即向外擴展得更遠)。這是两者之間的重要區別。當您認為數據实際上遵循常態分佈但並不知道均值或標准差時,就會出现這種分佈。我們将在 @sec-Comparing-two-means 再次遇到此分佈。  

- $\chi^2$ 分佈是在很多不同地方出現的另一種分佈。 我們將看到它的情况是在 @sec-Categorical-data-analysis 中進行分類數據分析,但它实际上無處不在。 當您深入研究數學時(而誰不喜歡這樣做呢?),結果發現 $\chi^2$ 分佈無處不在的主要原因是,如果您有很多遵循常態分佈的變數,則計算它們的平方和加和(稱為計算“平方和”)后,此和具有 $\chi^2$ 分佈。 你會驚訝地發現這個事實經常有用。 無論如何,以下是 $\chi^2$ 分佈的樣子:@fig-fig7-11。  

- F 分佈看起來有點像 $\chi^2$ 分佈,並且在您需要將两個 $\chi^2$ 分佈相互比較時就會出现。 雖然聽起來這並不像任何理智的人會想要做的事情,但它在現實世界的數據分析中非常重要。 還記得我說過 $\chi^2$ 分佈在我們進行“平方和”時是關鍵分佈嗎? 那么,這意味著如果您想比較两個不同的“平方和”,您可能正在談論一個遵循 F 分佈的東西。當然,到目前為止,我仍没有給出任何涉及平方和的示例,但是我將在 @sec-Comparing-several-means-one-way-ANOVA 中給出。 这也是我们将遇到 F 分佈的地方。 哦,還有 @fig-fig7-12 中的一個圖片。  

好的,是時候總結本節了。 我們已經看到三种新的分佈:$\chi^2$、t 和 F。 它們都是連續分佈,並且它們都與常態分佈密切相關。 對我們而言,主要是您需要理解的基本思想是,這些分佈之間以及與常態分佈之間有深刻的聯系。 在本書的后面,我們將遇到遵循常態分佈或至少假定遵循常態分佈的數據。 我現在想讓您理解的是,如果您假定數據遵循常態分佈,那么当您開始進行數據分析時,在各個地方出現 $\chi^2$、t 和 F 分佈時,您不應感到驚訝。  


## 本章小結

<!--- In this chapter we've talked about probability. We've talked about what probability means and why statisticians can't agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:

- Probability theory versus statistics: [How are probability and statistics different?]
- [The frequentist view] versus [The Bayesian view] of probability
- [Basic probability theory]
- [The binomial distribution], [The normal distribution], and [Other useful distributions]

As you'd expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the "simpler" task of documenting standard probability distributions is a big topic. I've described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called "Statistical Distributions" [@Evans2011] that lists a lot more than that. Fortunately for you, very little of this is necessary. You're unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won't need them for this book, but it never hurts to know that there's other possibilities out there.

Picking up on that last point, there's a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often "forget" to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it's important to understand these things before moving onto the applications. For example, there are a lot of rules about what you're "allowed" to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in @sec-Comparing-two-means we're going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.--->

在本章中,我們討論了機率。我們討論了機率的意義以及為什麼統計學家無法就其意義達成共識。我們討論了機率必須遵守的規則。並且我們介紹了機率分佈的概念,並在本章中花了很大篇幅討論統計學家使用的一些更重要的機率分佈。逐節概述如下:  

- 機率論與統計學之間的區別:[機率和統計有什麼不一樣？]  

- 機率的[次數主義觀點]與[貝氏觀點]  

- [基本機率理論]  

- [二項分佈]、[常態分佈]和[其他常見機率分佈]  

如您所料,我的內容絕不詳盡。機率論本身就是數學的一個巨大分支,與其在統計和數據分析中的應用完全分開。因此,這個主題上已經寫了上千本書,大學通常提供多門完全致力於機率論的課程。即使是記錄標準機率分佈這樣“更簡單”的任務也是一個大課題。我在本章中描述了五種標準機率分佈,但在我的書架上有一本45章的書,名為“統計分佈” [@Evans2011],其中列出了很多更多分佈。幸運的是,您需要的很少。當您走出去做現實世界的數據分析時,您不太可能需要知道幾十種統計分佈,在本書中您肯定不需要它們,但是瞭解還有其他可能性永遠不會有害。  

關於最後一點,在某種意義上,整章可以說是額外的內容。許多本科心理學統計課程會很快減少這部分內容(我知道我的課程就是這樣),即使是更高級的課程也經常會“忘記”重新訪問該領域的基礎。大多數學術心理學家不會區分機率和密度,直到最近,很少有人意識到貝式和次數主義機率之間的區別。然而,我認為在繼續應用之前理解這些很重要。例如,在進行統計推論時,有很多關於您“允許”說什麼的規則,其中許多看起來很武斷和奇怪。然而,如果您瞭解貝式與次數主義的區別,它們就會變得有意義。同樣,在 @sec-Comparing-two-means 中,我們將談論所謂的 t 檢定,如果您真的想完全理解 t 檢定的機制,那麼瞭解 t 分佈的實際情況真的會有幫助。我希望您理解我的意思。  
