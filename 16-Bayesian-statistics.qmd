# 貝氏統計 {#sec-Bayesian-statistics}

```{r}
#| include: FALSE
source("header.R")
```

> **譯者註** 20240108初步以Claude-2.1完成翻譯，內容待編修。

<!---
> *"In our reasonings concerning matter of fact, there are all imaginable degrees of assurance, from the highest certainty to the lowest species of moral evidence. A wise man, therefore, proportions his belief to the evidence."*\
> -- David Hume [^bayesian-statistics-1]

[^bayesian-statistics-1]: <a href="http://en.wikiquote.org/wiki/David%20Hume."
    target="_blank"><http://en.wikiquote.org/wiki/David_Hume>.</a>

The ideas I've presented to you in this book describe inferential statistics from the frequentist perspective. I'm not alone in doing this. In fact, almost every textbook given to undergraduate psychology students presents the opinions of the frequentist statistician as *the* theory of inferential statistics, the one true way to do things. I have taught this way for practical reasons. The frequentist view of statistics dominated the academic field of statistics for most of the 20th century, and this dominance is even more extreme among applied scientists. It was and is current practice among psychologists to use frequentist methods. Because frequentist methods are ubiquitous in scientific papers, every student of statistics needs to understand those methods, otherwise they will be unable to make sense of what those papers are saying! Unfortunately, in my opinion at least, the current practice in psychology is often misguided and the reliance on frequentist methods is partly to blame. In this chapter I explain why I think this and provide an introduction to Bayesian statistics, an approach that I think is generally superior to the orthodox approach.

This chapter comes in two parts. In the first three sections I talk about what Bayesian statistics are all about, covering the basic mathematical rules for how it works as well as an explanation for why I think the Bayesian approach is so useful. Afterwards, I provide a brief overview of how you can do [Bayesian t-tests]. --->


> *"在我們推理事實的過程中,確信度有各種程度,從最高的確定性到最低的道德證據。因此,一個明智的人會根據證據的比例判斷他的信念。"*

> -- 大衛·休謨 [^bayesian-statistics-1]

[^bayesian-statistics-1]: <a href="http://en.wikiquote.org/wiki/David%20Hume."
    target="_blank"><http://en.wikiquote.org/wiki/David_Hume>.</a>

我在這本書中向您介紹的想法描述了次數主義的推論統計學。在這一點上,我並不孤單。事實上,幾乎每一本提供給本科心理學生的教科書都將次數統計學家的意見呈現為推論統計學的*理論*,唯一正確的方法。出於實際原因,我以這種方式進行了教學。在20世紀的大部分時間裡,次數主義的統計學觀點主導了統計學這一學術領域,而在應用科學家中這種主導地位甚至更加極端。過去和現在,心理學家使用次數方法是常規做法。由於次數方法在科學論文中無處不在,每一個統計學學習者都需要理解這些方法,否則他們將無法理解這些論文在說什麼!不幸的是,至少在我看來,心理學中的當前做法通常是誤導的,而過度依賴次數方法也應承擔部分責任。在本章中,我解釋了我的這些看法,並概述了貝氏統計學,我認為這種方法總體上優於正統方法。  

本章分為兩個部分。在前三節中,我談論了貝氏統計學的全部內容,涵蓋了它的基本數學規則以及我認為貝氏方法如此有用的解釋。之後,我概述了如何進行[貝氏t檢定]。  


## 理性者的機率推論

<!---From a Bayesian perspective statistical inference is all about *belief revision*. I start out with a set of candidate hypotheses h about the world. I don't know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That's it! At the end of this section I'll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.

> *I'm carrying an umbrella. Do you think it will rain?*

In this problem I have presented you with a single piece of data (d = I'm carrying the umbrella), and I'm asking you to tell me your belief or hypothesis about whether it's raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem? --->

以下是翻譯初稿:

從貝氏的角度來看,統計推論都是關於*信念修正*. 我從一組有關世界的候選假設 h 出發。我不知道這些假設中哪一個是真的,但我確實對哪些假設是合理的和哪些不是有一些信念。當我觀察到數據 d 時,我必須修正那些信念。如果數據與某個假設一致,那麼我對該假設的信心就會增強。如果數據與假設不一致,那麼我對該假設的信心就會減弱。就這樣! 在本節的末尾,我將給出貝葉斯推理如何運作的精確描述,但首先我想通過一個簡單的示例來介紹關鍵思想。考慮以下推理問題。

> *我帶著雨傘。你認為會下雨嗎?*

在這個問題中,我向您提供了一個數據(d = 我帶著雨傘),並要求您告訴我您對是否下雨的信念或假設。您有兩個選擇,h:今天要麼下雨,要麼不下雨。你應該如何解決這個問題?

在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。

### 事前機率：你一開始的信念

<!--- The first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I'm writing this in January and so you can assume it's the middle of summer. In fact, you might have decided to take a quick look on Wikipedia[^bayesian-statistics-2] and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see @tbl-tab16-1). If this is really what you believe about Adelaide rainfall (and now that I've told it to you I'm betting that this really is what you believe) then what I have written here is your **prior distribution**, written $P(h)$.

[^bayesian-statistics-2]: <a href="http://en.wikipedia.org/wiki/Climate_of_Adelaide"
    target="_blank">http://en.wikipedia.org/wiki/Climate_of_Adelaide</a>

How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall. --->

以下是翻譯初稿:

你需要做的第一件事是忽略我告訴你的有關雨傘的信息,並記下你預先存在的關於雨水的信念。這很重要。如果您想誠實地說明您的信念在新的證據(數據)出現後是如何修正的,那麼您必須說明您在那些數據出現之前相信什麼! 那麼,您可能相信今天是否會下雨呢?您可能知道我住在澳大利亞,澳大利亞的大部分地區都很熱,也很乾燥。我所居住的阿德萊德市擁有地中海氣候,非常類似於南加利福尼亞州、南歐或北非。我是在1月寫這篇文章的,所以您可以假設這是夏季中期。事實上,您可能決定在維基百科[^bayesian-statistics-2]上快速查找,並發現阿德萊德在1月的31天裡平均有4.4天降雨。在不知道任何其他信息的情況下,您可能得出的結論是,阿德萊德1月降雨的概率約為15%,乾燥日的概率為85%(見 @tbl-tab16-1 )。如果這真的是您對阿德萊德降水量的信念(既然我告訴了您,我敢打賭這真的是您的信念),那麼我在這裡寫的就是您的**事前分布**,表示為$P(h)$。  


[^bayesian-statistics-2]: <a href="http://en.wikipedia.org/wiki/Climate_of_Adelaide"
    target="_blank">http://en.wikipedia.org/wiki/Climate_of_Adelaide</a>


```{r}
#| label: tbl-tab16-1
#| tbl-cap: 阿德萊德降雨的可能性 - 基於1月平均降水量知識的預先存在的信念。
huxtabs[[16]][[1]]  
```

### 似然值: 對手上資料的理論

<!---To solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I'm not a complete idiot,[^bayesian-statistics-3] and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn't be all that surprised to know that I'm pretty forgetful about this sort of thing. Let's suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let's say that on dry days I'm only about 5% likely to be carrying an umbrella. So you might write this out as in @tbl-tab16-2.

[^bayesian-statistics-3]: It's a leap of faith, I know, but let's run with it okay?

How likely am I to be carrying an umbrella on rainy and dry days? --->

以下是翻譯初稿:

為了解決這個推理問題,您需要一個關於我行為的理論。丹帶雨傘是在什麼時候?您可能猜測我並非完全白痴[^bayesian-statistics-3],我只會在雨天帶雨傘。另一方面,您也知道我有小孩,您並不會太驚訝地知道在這類事情上我相當健忘。假設在雨天我記得帶雨傘的概率約為30%(我在這方面真的很糟糕)。但在乾燥的日子裡,我帶雨傘的可能性只有大約5%。所以您可能會像@tbl-tab16-2中這樣寫出來。 

[^bayesian-statistics-3]: 我知道,這需要一定的信任,但讓我們繼續吧,好嗎?





```{r}
#| label: tbl-tab16-2
#| tbl-cap: 在雨天和晴天我帶雨傘的可能性有多大? 
huxtabs[[16]][[2]] 
```

<!---It's important to remember that each cell in this table describes your beliefs about what data d will be observed, *given* the truth of a particular hypothesis $h$. This "conditional probability" is written $P(d|h)$, which you can read as "the probability of $d$ given $h$". In Bayesian statistics, this is referred to as the **likelihood** of the data $d$ given the hypothesis $h$.[^bayesian-statistics-4]

[^bayesian-statistics-4]: Um. I hate to bring this up, but some statisticians would object to me using the word "likelihood" here. The problem is that the word "likelihood" has a very specific meaning in frequentist statistics, and it's not quite the same as what it means in Bayesian statistics. As far as I can tell Bayesians didn't originally have any agreed upon name for the likelihood, and so it became common practice for people to use the frequentist terminology. This wouldn't have been a problem except for the fact that the way that Bayesians use the word turns out to be quite different to the way frequentists do. This isn't the place for yet another lengthy history lesson but, to put it crudely, when a Bayesian says "a likelihood function" they're usually referring one of the rows of the table. When a frequentist says the same thing, they're referring to the same table, but to them "a likelihood function" almost always refers to one of the columns. This distinction matters in some contexts, but it's not important for our purposes. --->

重要的是要記住,此表中的每個單元都描述了在特定假設 $h$ 為真的情況下將觀察到的數據 $d$ 的信念。這種“條件概率”寫為 $P(d|h)$,您可以讀作“在條件 $h$ 下 $d$ 的概率”。在貝氏統計中,這被稱為在假設 $h$ 條件下數據 $d$ 的**似然性**。[^bayesian-statistics-4]  

[^bayesian-statistics-4]: 噢。我很不願意提起這一點,但一些統計學家會反對我在這裡使用“似然性”一詞。問題在於,“似然性”在次數主義統計中有非常具體的含義,而且與它在貝氏統計中的含義不完全相同。據我所知,貝葉斯最初沒有任何公認的名稱來表示似然性,所以人們使用次數主義術語成為常規做法。這本不會成為問題,除非事實證明貝葉斯使用這個詞的方式與次數主義者的方式相當不同。這裡不是再來一堂冗長的歷史課的地方,但粗略地說,當貝葉斯說“一個似然函數”時,他們通常是指表中的一行。當次數主義者說同樣的事情時,他們指的是同一個表,但對他們來說,“一個似然函數”幾乎總是指的是其中一列。這種區別在某些語境中很重要,但對我們的目的來說並不重要。

在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。

### 資料與理論的聯合機率

<!---At this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there's a very simple equation that we can use here, but it's important that you understand why we use it so I'm going to try to build it up from more basic ideas.

Let's start out with one of the rules of probability theory. I listed it way back in @tbl-tab7-1, but I didn't make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I'm carrying an umbrella (i.e., data $d$ is observed). The **joint probability** of the hypothesis and the data is written $P(d,h)$, and you can calculate it by multiplying the prior $P(h)$ by the likelihood $P(d|h)$. Mathematically, we say that

$$P(d,h)=P(d|h)P(h)$$

So, what is the probability that today is a rainy day *and* I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is $30\%$. So the probability that both of these things are true is calculated by multiplying the two

$$
\begin{split} 
P(rainy, umbrella) & = P(umbrella|rainy) \times P(rainy) \\
& = 0.30 \times 0.15 \\ 
& = 0.045 
\end{split}
$$

In other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let's repeat the exercise for all four. If we do that, we end up with @tbl-tab16-3.

Four possibilities combining rain (or not) and umbrella carrying (or not). --->


至此,所有的要素都已到位。寫下了先驗分布和似然性后,您擁有了進行貝氏推理所需的所有信息。那麼現在的問題是,我們如何使用這些信息?事實證明,我們在這裡可以使用一個非常簡單的方程式,但重要的是您要理解我們為什麼要使用它,所以我將嘗試從更基本的思想建立它。  

讓我們從概率理論的一個規則開始。我在很早以前的 @tbl-tab7-1 中列出了它,但當時我並沒有把它當回事,您可能也忽略了它。問題涉及到的規則是關於兩件事同時發生的概率。在我們的示例中,您可能想計算今天降雨(即假設 $h$ 為真)和我帶雨傘(即觀測到數據 $d$)的概率。假設和數據的**聯合概率**寫為 $P(d,h)$,可以通過將先驗概率 $P(h)$ 與似然性 $P(d|h)$ 相乘來計算。在數學上,我們說  

$$P(d,h)=P(d|h)P(h)$$  

那麼,今天是雨天*和*我記得帶雨傘的概率是多少? 如我們前面討論的,先驗概率告訴我們雨天的概率為15%,似然性告訴我們我在雨天記得帶雨傘的概率為$30\\%$。 所以這兩件事同時發生的概率是將兩個相乘計算得到的  

\$$ \\begin{split} P(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\\ & = 0.30 \\times 0.15 \\\\\\ & = 0.045 \\end{split} \$$  

換句話說,在被告知實際發生的任何事情之前,您認為今天是雨天且我會記得帶雨傘的概率為4.5%。當然,可能發生的事情有四種,對嗎? 那麼讓我們對所有四種情況重複這個運算。如果我們這樣做,我們會得到 @tbl-tab16-3。  



```{r}
#| label: tbl-tab16-3
#| tbl-cap: 合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性
huxtabs[[16]][[3]] 
```

<!--- This table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us @tbl-tab16-4.

Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals. --->

這個表格捕捉了有關四種可能性中的哪種可能性更大的所有信息。不過,為了真正全面理解,添加行總和和列總和會有幫助。這給了我們 @tbl-tab16-4 。  


```{r}
#| label: tbl-tab16-4
#| tbl-cap: 合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性,以及行和列總和。
huxtabs[[16]][[4]] 
```

<!--- This is a very useful table, so it's worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren't telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That's not surprising, of course, as that's our prior.[^bayesian-statistics-5] The important thing isn't the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven't explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it's a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.

[^bayesian-statistics-5]: Just to be clear, "prior" information is pre-existing knowledge or beliefs, before we collect or use any data to improve that information.

Now, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (@tbl-tab16-5):

Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities. --->


這是一個非常有用的表格,所以值得花點時間思考這些數字都在告訴我們什麼。首先,請注意,行總和並沒有告訴我們任何新信息。例如,第一行告訴我們,如果我們忽略所有與雨傘有關的事,今天將是雨天的機率為15%。當然,這一點都不奇怪,因為這就是我們的先驗分布。[^bayesian-statistics-5] 重要的不是數字本身。相反,重要的是它給了我們一些信心,即我們的計算是合理的! 現在看看列總和,並請注意,它們告訴了我們一些我們還沒有明確說明過的事情。與行總和告訴我們降雨的概率一樣,列總和告訴我們我帶傘的概率。具體來說,第一列告訴我們,平均而言(即,不管是雨天還是晴天),我帶雨傘的概率為8.75%。 最後,請注意,當我們在所有四種邏輯上可能的事件中求和時,所有內容的總和為1。換句話說,我們寫下的是在所有可能的數據和假設組合上定義的適當的概率分布。  

[^bayesian-statistics-5]:為明確起見,“先驗”信息是預先存在的知識或信念,在我們收集或使用任何數據來改善該信息之前。  

現在,由於這個表非常有用,我想確保您了解所有元素對應的是什麼以及它們是如何寫的(@tbl-tab16-5):  

合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性,表示為條件概率。  



```{r}
#| label: tbl-tab16-5
#| tbl-cap: Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities
huxtabs[[16]][[5]] 
```

<!--- Finally, let's use "proper" statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we'll let $d_1$ refer to the possibility that you observe me carrying an umbrella, and $d_2$ refers to you observing me not carrying one. Similarly, $h_1$ is your hypothesis that today is rainy, and $h_2$ is the hypothesis that it is not. Using this notation, the table looks like @tbl-tab16-6.

Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities. --->


最后,讓我們使用“適當的”統計符號。在下雨的問題中,數據對應於我是否帶雨傘的觀察。因此,我們將讓$d_1$ 表示您觀察到我帶雨傘的可能性, $d_2$ 是您觀察到我沒有帶雨傘。 類似地,$h_1$ 是您認為今天是雨天的假設, $h_2$ 是今天不是雨天的假設。 使用這種符號,表格如 @tbl-tab16-6 所示。  



```{r}
#| label: tbl-tab16-6
#| tbl-cap: 以條件概率的形式表示的合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性的假設條件。
huxtabs[[16]][[6]] 
```

### 透過貝氏法則更新信念

<!--- The table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It's now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren't expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now *know* that I have an umbrella.[^bayesian-statistics-6] To reflect this new knowledge, our *revised* table must have the following numbers. (see @tbl-tab16-7).

[^bayesian-statistics-6]: If we were being a bit more sophisticated, we could extend the example to accommodate the possibility that I'm lying about the umbrella. But let's keep things simple, shall we? 

Revising beliefs given new data about umbrella carrying.--->

以下是第一段翻譯初稿:  

在上一節中我們列出的表格是解決下雨天問題的一個非常有力的工具,因為它考慮了所有四種邏輯可能性,並準確地說明了在獲得任何數據之前,您對每一種可能性的信心程度。現在是考慮我們在實際獲得數據時我們的信念會發生什麼時候了。 在下雨天的問題中,您被告知我確實在帶雨傘。這有些令人驚訝。根據我們的表格,我帶雨傘的概率只有8.75%。但這很有意義,對嗎?一個女人在炎熱乾燥的城市的夏天帶著雨傘是相當不尋常的,所以您並不真的期望這樣。儘管如此,數據告訴您,這是真的。無論您認為它有多不可能,您現在必须調整您的信念以容納您現在*知道*我有雨傘這一事實。[^bayesian-statistics-6] 為了反映這些新知識,我們的*修訂*表必須有以下數字。 (見 @tbl-tab16-7 )。  

[^bayesian-statistics-6]: 如果我們更複雜一些,我們可以擴展示例以容納我撒謊關於雨傘的可能性。但讓我們保持簡單,可以嗎?


```{r}
#| label: tbl-tab16-7
#| tbl-cap: 根據有關帶雨傘的新數據修訂信念。 
huxtabs[[16]][[7]] 
```

<!---In other words, the facts have eliminated any possibility of "no umbrella", so we have to put zeros into any cell in the table that implies that I'm not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that $P(umbrella) = 1$.

What two numbers should we put in the empty cells? Again, let's not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of "rain and umbrella" was 4.5%, and the joint probability of "dry and umbrella" was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you'd have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you'd agree that it's still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that "rain and umbrella" is *slightly* more plausible than "dry and umbrella", while still ensuring that numbers in the table add up. Something like @tbl-tab16-8, perhaps?

Revising probabilities given new data about umbrella carrying. --->

換句話說,事實已經排除了“沒有雨傘”的任何可能性,所以我們必須將表中表示我沒有帶雨傘的任何單元數據設置為零。此外,您確實知道我正在帶雨傘,所以左側的列總和必須為1,以正確描述 $P(umbrella) = 1$ 這一事實。  

我們應該在空白單元中填入什麼兩個數字呢?同樣,我們不擔心數學,而是考慮我們的直覺。當我們第一次編寫表格時,結果是這兩個單元中幾乎有相同的數字,對嗎? 我們計算出“雨和傘”的聯合概率為4.5%,“旱和傘”的聯合概率為4.25%。 換句話說,在我告訴您我實際上在帶雨傘之前,您會說這兩個事件的概率幾乎相同,是嗎? 但請注意,這兩種可能性與我確實在帶雨傘這一事實是一致的。 從這兩種可能性來看,幾乎沒什麼改變。我希望您同意,“雨和雨傘”比“旱和雨傘”更合理一點仍然是正確的。所以我們預期在最終表格中看到的數字是保持“雨和雨傘”比“旱和雨傘”較合理這一事實的數字,同時仍確保表格中的數字加起來。也許像 @tbl-tab16-8 ?  



```{r}
#| label: tbl-tab16-8
#| tbl-cap: 根據有關帶雨傘的新數據修訂概率。 
huxtabs[[16]][[8]] 
```

<!--- What this table is telling you is that, after being told that I'm carrying an umbrella, you believe that there's a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won't. That's the answer to our problem! The **posterior probability** of rain $P(h\|d)$ given that I am carrying an umbrella is 51.4%

How did I calculate these numbers? You can probably guess. To work out that there was a $0.514$ probability of "rain", all I did was take the $0.045$ probability of "rain and umbrella" and divide it by the $0.0875$ chance of "umbrella". This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I've done here is divide the joint probability of the hypothesis and the data $P(d, h)$ by the **marginal probability** of the data $P(d)$, and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: [^bayesian-statistics-7]

[^bayesian-statistics-7]: You might notice that this equation is actually a restatement of the same basic rule I listed at the start of the last section. If you multiply both sides of the equation by $P(d)$, then you get $P(d)P(h|d) = P(d, h)$, which is the rule for how joint probabilities are calculated. So I'm not actually introducing any "new" rules here, I'm just using the same rule in a different way. 

$$P(h|d)=\frac{P(h|d)}{P(d)}$$

However, remember what I said at the start of the last section, namely that the joint probability $P(d, h)$ is calculated by multiplying the prior Pphq by the likelihood $P(d|h)$. In real life, the things we actually know how to write down are the priors and the likelihood, so let's substitute those back into the equation. This gives us the following formula for the posterior probability:

$$P(h|d)=\frac{P(d|h)P(h)}{P(d)}$$

And this formula, folks, is known as **Bayes' rule**. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule. --->

以下是第二段翻譯初稿:

這個表格告訴您的信息是,在被告知我正在帶雨傘后,您相信今天降雨的可能性為51.4%,不降雨的可能性為48.6%。這就是我們問題的答案! 在我帶雨傘的情況下降雨的**後驗概率** $P(h\\|d)$ 為51.4%。

我是如何計算這些數字的呢?您可能已經猜到了。為了算出“雨”的概率為$0.514$,我所做的就是取“雨和雨傘”的概率 $0.045$,並將其除以“雨傘”的機率 $0.0875$。這會產生一個滿足我們的要求的表,即所有內容的總和為1,並且不會干擾與實際數據一致的兩個事件之間的相對合理性。 用花俏的統計術語來說,我在這裡所做的就是將假設和數據的聯合概率 $P(d,h)$ 除以數據的**邊際概率** $P(d)$,這就是給我們觀察到的數據條件下假設的後驗概率。 用方程式寫出來是:[^bayesian-statistics-7]  

[^bayesian-statistics-7]: 您可能會注意到,這個方程式實際上是我在上一節開頭列出的同一個基本規則的重述。 如果您將等式兩邊都乘以 $P(d)$,那麼您得到 $P(d)P(h|d)=P(d,h)$,這是聯合概率的計算規則。 所以我在這裡實際上並沒有引入任何“新的”規則,我只是以不同的方式使用了相同的規則。  

$$P(h|d)=\\frac{P(h|d)}{P(d)}$$  

然而,記住我在上一節開頭所說的,即聯合概率 $P(d,h)$ 是通過將先驗概率 Pphq 乘以似然性 $P(d|h)$ 計算出來的。 在現實生活中,我們實際上知道如何計算的是先驗概率和似然性,所以讓我們將它們代回方程式。 這給了我們後驗概率的以下公式:  

$$P(h|d)=\\frac{P(d|h)P(h)}{P(d)}$$  

女士們先生們,這個公式被稱為**貝葉斯規則**。它描述了一個學習者如何從對不同假設的合理性的先驗信念出發,並告訴您當面對數據時應如何修正這些信念。 在貝氏範例中,所有的統計推論都源自這個簡單的規則。  

在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。

## 貝氏假設檢定

<!--- In @sec-Hypothesis-testing I described the orthodox approach to hypothesis testing. It took an entire chapter to describe, because null hypothesis testing is a very elaborate contraption that people find very hard to make sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let's pick a setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to compare, a null hypothesis $h_0$ and an alternative hypothesis $h_1$. Prior to running the experiment we have some beliefs $P(h)$ about which hypotheses are true. We run an experiment and obtain data d. Unlike frequentist statistics, Bayesian statistics does allow us to talk about the probability that the null hypothesis is true. Better yet, it allows us to calculate the **posterior probability of the null hypothesis**, using Bayes' rule:

$$P(h_0|d)=\frac{P(d|h_0)P(h_0)}{P(d)}$$

This formula tells us exactly how much belief we should have in the null hypothesis after having observed the data d. Similarly, we can work out how much belief to place in the alternative hypothesis using essentially the same equation. All we do is change the subscript

$$P(h_1|d)=\frac{P(d|h_1)P(h_1)}{P(d)}$$

It's all so simple that I feel like an idiot even bothering to write these equations down, since all I'm doing is copying Bayes rule from the previous section.[^bayesian-statistics-8]

[^bayesian-statistics-8]: Obviously, this is a highly simplified story. All the complexity of real life Bayesian hypothesis testing comes down to how you calculate the likelihood $P(d\|h)$ when the hypothesis h is a complex and vague thing. I'm not going to talk about those complexities in this book, but I do want to highlight that although this simple story is true as far as it goes, real life is messier than I'm able to cover in an introductory stats textbook. --->

在 @sec-Hypothesis-testing 中,我描述了正統的假設檢定方法。 這需要整整一章來描述,因為虛無假設檢定是一個非常精密的設備,人們覺得很難理解。 相反,貝氏假設檢定的方法簡直簡單得令人難以置信。 讓我們選擇一種與正統場景緊密類似的設置。 有兩個要比較的假設,虛無假設 $h_0$ 和替代假設 $h_1$。 在運行實驗之前,我們對哪些假設是真實的有一些信念 $P(h)$。我們運行一項實驗並獲得數據 d。 與次數主義統計不同,貝氏統計允許我們討論虛無假設為真的概率。 更妙的是,它允許我們使用貝葉斯規則計算**虛無假設的後驗概率**:

$$P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}$$

該公式準確地告訴我們在觀察到數據 d 後,我們對虛無假設應該有多大的信念。 類似地,我們可以使用基本相同的方程式確定對替代假設的信任程度。 我們所要做的只是更改下標:  

$$P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}$$

這非常簡單,以至於我覺得我連把這些方程寫下來都很傻,因為我所做的只是從前一節複製了貝氏規則。[^bayesian-statistics-8]  

[^bayesian-statistics-8]: 顯然,這是一個高度簡化的故事。 真實世界中貝氏假設檢定的所有複雜性都歸結為當假設 h 是一个複雜且模糊的事物時如何計算似然性 $P(d\|h)$。 我不打算在這本書中討論這些複雜性,但我確實想要強調,儘管這個簡單的故事在目前是正確的,但現實生活比我能夠在入門統計教科書中涵蓋的要複雜得多。

在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。

### 貝氏因子

<!--- In practice, most Bayesian data analysts tend not to talk in terms of the raw posterior probabilities $P(h_0|d)$ and $P(h_1|d)$. Instead, we tend to talk in terms of the **posterior odds** ratio. Think of it like betting. Suppose, for instance, the posterior probability of the null hypothesis is 25%, and the posterior probability of the alternative is 75%. The alternative hypothesis is three times as probable as the null, so we say that the odds are 3:1 in favour of the alternative. Mathematically, all we have to do to calculate the posterior odds is divide one posterior probability by the other

$$\frac{P(h_1|d)}{P(h_0|d)}=\frac{0.75}{0.25}=3$$

Or, to write the same thing in terms of the equations above

$$\frac{P(h_1|d)}{P(h_0|d)}=\frac{d|h_1}{d|h_0} \times \frac{h_1}{h_0}$$

Actually, this equation is worth expanding on. There are three different terms here that you should know. On the left hand side, we have the posterior odds, which tells you what you believe about the relative plausibilty of the null hypothesis and the alternative hypothesis after seeing the data. On the right hand side, we have the **prior odds**, which indicates what you thought before seeing the data. In the middle, we have the **Bayes factor**, which describes the amount of evidence provided by the data. (@tbl-tab16-9).

Posterior odds given the Bsyes factor and prior odds. --->

在實踐中,大多數貝氏數據分析師傾向於不以原始後驗概率 $P(h_0|d)$ 和 $P(h_1|d)$ 的形式談論。相反,我們傾向於以**後驗似然比**的形式談論。可以想像成下注。例如,假設虛無假設的後驗概率為25%,替代假設的後驗概率為75%。替代假設的概率是虛無假設的三倍,所以我們說似然比為3:1,有利于替代假設。數學上,我們計算後驗似然比所要做的就是將一個後驗概率除以另一個

$$\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3$$  

或者,用上面的方程式書寫相同的內容  

$$\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}$$

實際上,這個等式值得擴展。 這裡有三個不同的術語您應該知道。 在左側,我們有後驗似然比,它告訴您在看到數據后對虛無假設和替代假設的相對合理性的信念。 在右側,我們有**先驗似然比**,它指示您在看到數據之前的想法。在中間,我們有**貝氏因子**,它描述了數據提供的證據量。 (@tbl-tab16-9)。  





```{r}
#| label: tbl-tab16-9
#| tbl-cap: 基於貝氏因子和先驗似然比的後驗似然比
huxtabs[[16]][[9]] 
```

<!--- The Bayes factor (sometimes abbreviated as BF) has a special place in Bayesian hypothesis testing, because it serves a similar role to the p-value in orthodox hypothesis testing. The Bayes factor quantifies the strength of evidence provided by the data, and as such it is the Bayes factor that people tend to report when running a Bayesian hypothesis test. The reason for reporting Bayes factors rather than posterior odds is that different researchers will have different priors. Some people might have a strong bias to believe the null hypothesis is true, others might have a strong bias to believe it is false. Because of this, the polite thing for an applied researcher to do is report the Bayes factor. That way, anyone reading the paper can multiply the Bayes factor by their own personal prior odds, and they can work out for themselves what the posterior odds would be. In any case, by convention we like to pretend that we give equal consideration to both the null hypothesis and the alternative, in which case the prior odds equals 1, and the posterior odds becomes the same as the Bayes factor. --->

貝氏因子(有時縮寫為 BF)在貝氏假設檢定中佔有特殊地位,因為它在正統假設檢定中的 p 值起著類似作用。 貝氏因子量化了數據提供的證據力度,因此報告貝氏因子是人們在運行貝氏假設檢定時傾向於報告的內容。 報告貝氏因子而不是後驗似然比的原因是不同的研究人員會有不同的先驗分布。有些人可能傾向於相信虛無假設為真,而其他人可能傾向於相信它為假。因此,禮貌的應用研究者應做的是報告貝氏因子。這樣,閱讀論文的任何人都可以將貝氏因子與自己的個人先驗似然比相乘,並且他們可以自己算出後驗似然比。無論如何,按照慣例,我們喜歡假裝我們給予虛無假設和替代假設同等考慮,在這種情況下,先驗似然比等於1,後驗似然比就等於貝氏因子。


### 解讀貝氏因子

<!--- One of the really nice things about the Bayes factor is the numbers are inherently meaningful. If you run an experiment and you compute a Bayes factor of 4, it means that the evidence provided by your data corresponds to betting odds of 4:1 in favour of the alternative. However, there have been some attempts to quantify the standards of evidence that would be considered meaningful in a scientific context. The two most widely used are from @Jeffreys1961 and @Kass1995. Of the two, I tend to prefer the @Kass1995 table because it's a bit more conservative. So here it is (@tbl-tab16-10).

Bayes factors and strength of evidence.--->

貝氏因子的一個真正好處是這些數字本身就很有意義。如果您運行一項實驗並計算出貝氏因子為4,這意味著您的數據提供的證據對應於有利於替代假設的4:1的賭注比例。 然而,已經有一些嘗試量化在科學環境下被認為有意義的證據標准。最廣泛使用的兩個是來自 @Jeffreys1961 和 @Kass1995 。在這兩者中,我傾向於更喜歡 @Kass1995 表,因為它更加保守。 所以在這裡它是(@tbl-tab16-10)。  



```{r}
#| label: tbl-tab16-10
#| tbl-cap: 貝氏因子和證據力度  
huxtabs[[16]][[10]] 
```

<!--- And to be perfectly honest, I think that even the @Kass1995 standards are being a bit charitable. If it were up to me, I'd have called the "positive evidence" category "weak evidence". To me, anything in the range 3:1 to 20:1 is "weak" or "modest" evidence at best. But there are no hard and fast rules here. What counts as strong or weak evidence depends entirely on how conservative you are and upon the standards that your community insists upon before it is willing to label a finding as "true".

In any case, note that all the numbers listed above make sense if the Bayes factor is greater than 1 (i.e., the evidence favours the alternative hypothesis). However, one big practical advantage of the Bayesian approach relative to the orthodox approach is that it also allows you to quantify evidence for the null. When that happens, the Bayes factor will be less than 1. You can choose to report a Bayes factor less than 1, but to be honest I find it confusing. For example, suppose that the likelihood of the data under the null hypothesis $P(d|h_0)$ is equal to 0.2, and the corresponding likelihood $P(d|h_1)$ under the alternative hypothesis is 0.1. Using the equations given above, Bayes factor here would be

$$BF=\frac{P(d|h_1)}{P(d|h_0)}=\frac{0.1}{0.2}=0.5$$

Read literally, this result tells is that the evidence in favour of the alternative is 0.5 to 1. I find this hard to understand. To me, it makes a lot more sense to turn the equation "upside down", and report the amount op evidence in favour of the null. In other words, what we calculate is this

$$BF^{'}=\frac{P(d|h_0)}{P(d|h_1)}=\frac{0.2}{0.1}=2$$

And what we would report is a Bayes factor of 2:1 in favour of the null. Much easier to understand, and you can interpret this using the table above. --->




而且要老實說,我認為即使是 @Kass1995 的標準也有點太仁慈了。如果由我決定,我會將“積極證據”類別稱為“弱證據”。 對我來說,3:1 到 20:1 的任何範圍都只是“弱”或“溫和”的證據而已。 但這裡沒有硬性規定。什麼算作強或弱證據完全取決於您的保守程度以及您的社區在願意將一項發現標籤為“真”之前堅持的標準。  

無論如何,請注意,如果貝氏因子大於1(即證據有利於替代假設),那麼上面列出的所有數字都是有意義的。 然而,與正統方法相比,貝氏方法的一個大的實際優勢是它還允許您量化虛無假設的證據。 當這種情況發生時,貝氏因子將小於1。 您可以選擇報告小於1的貝氏因子,但說實話,我覺得這很容易造成困惑。 例如,假設在虛無假設 $h_0$ 下數據的似然性 $P(d|h_0)$ 等於0.2,而在替代假設 $h_1$ 下對應的似然性 $P(d|h_1)$ 為 0.1。 使用上面給出的方程式,這裡的貝氏因子為  

$$BF=\\frac{P(d|h_1)}{P(d|h_0)}=\\frac{0.1}{0.2}=0.5$$  

如果逐字理解,這個結果告訴我們,支持替代假設的證據為 0.5:1。 我覺得這很難理解。 對我來說,把等式“上下翻轉”更有意義,並報告支持虛無假設的證據量。 換句話說,我們計算的是  

$$BF' = \\frac{P(d|h_0)}{P(d|h_1)}=\\frac{0.2}{0.1}=2$$  

而我們要報告的是 2:1 的貝氏因子支持虛無假設。更容易理解,並且可以使用上面的表來解釋。


## 為何需要貝氏統計

<!--- Up to this point I've focused exclusively on the logic underpinning Bayesian statistics. We've talked about the idea of "probability as a degree of belief", and what it implies about how a rational agent should reason about the world. The question that you have to answer for yourself is this: how do you want to do your statistics? Do you want to be an orthodox statistician, relying on sampling distributions and p-values to guide your decisions? Or do you want to be a Bayesian, relying on things like prior beliefs, Bayes factors and the rules for rational belief revision? And to be perfectly honest, I can't answer this question for you. Ultimately it depends on what you think is right. It's your call and your call alone. That being said, I can talk a little about why I prefer the Bayesian approach. --->

以下是翻譯初稿:

到目前為止,我一直專注於支撐貝氏統計的邏輯基礎。我們討論了“概率作為信念程度”的觀念以及它隱含的關於理性代理人應如何推理世界的內容。您必須自行回答的問題是:您希望如何進行統計學? 您想作為正統統計學家依靠抽樣分布和 p 值來指導您的決策? 還是您想作為一個貝氏主義者,依靠先驗信念、貝氏因子和有理性信念修正規則等? 老實說,我不能為您回答這個問題。 最終,這取決於您認為什麼是正確的。 這是您的決定,僅您一人的決定。 雖然如此,我可以談談我為什麼更喜歡貝氏方法。


### 統計學呈現你所相信的世界面貌

<!--- 
> *You keep using that word. I do not think it means what you think it means*\
> -- Inigo Montoya, The Princess Bride [^bayesian-statistics-9]

[^bayesian-statistics-9]: <a href="http://www.imdb.com/title/tt0093779/quotes"
    target="_blank">http://www.imdb.com/title/tt0093779/quotes</a> . I should note in passing that I'm not the first person to use this quote to complain about frequentist methods. Rich Morey and colleagues had the idea first. I'm shamelessly stealing it because it's such an awesome pull quote to use in this context and I refuse to miss any opportunity to quote *The Princess Bride*.

To me, one of the biggest advantages to the Bayesian approach is that it answers the right questions. Within the Bayesian framework, it is perfectly sensible and allowable to refer to "the probability that a hypothesis is true". You can even try to calculate this probability. Ultimately, isn't that what you want your statistical tests to tell you? To an actual human being, this would seem to be the whole point of doing statistics, i.e., to determine what is true and what isn't. Any time that you aren't exactly sure about what the truth is, you should use the language of probability theory to say things like "there is an 80% chance that Theory A is true, but a 20% chance that Theory B is true instead".

This seems so obvious to a human, yet it is explicitly forbidden within the orthodox framework. To a frequentist, such statements are a nonsense because "the theory is true" is not a repeatable event. A theory is true or it is not, and no probabilistic statements are allowed, no matter how much you might want to make them. There's a reason why, back in Section 9.5, I repeatedly warned you not to interpret the p-value as the probability that the null hypothesis is true. There's a reason why almost every textbook on statstics is forced to repeat that warning. It's because people desperately want that to be the correct interpretation. Frequentist dogma notwithstanding, a lifetime of experience of teaching undergraduates and of doing data analysis on a daily basis suggests to me that most actual humans think that "the probability that the hypothesis is true" is not only meaningful, it's the thing we care most about. It's such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a p-value this way. For example, here is a quote from an official Newspoll report in 2013, explaining how to interpret their (frequentist) data analysis:[^bayesian-statistics-10]

[^bayesian-statistics-10]: <a href="http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/%0A" target="_blank">http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/</a> 

> *Throughout the report, where relevant, statistically significant changes have been noted. All significance tests have been based on the 95 percent level of confidence. **This means that if a change is noted as being statistically significant, there is a 95 percent probability that a real change has occurred**, and is not simply due to chance variation. (emphasis added)*

Nope! That's not what p < .05 means. That's not what 95% confidence means to a frequentist statistician. The bolded section is just plain wrong. Orthodox methods cannot tell you that "there is a 95% chance that a real change has occurred", because this is not the kind of event to which frequentist probabilities may be assigned. To an ideological frequentist, this sentence should be meaningless. Even if you're a more pragmatic frequentist, it's still the wrong definition of a p-value. It is simply not an allowed or correct thing to say if you want to rely on orthodox statistical tools.

On the other hand, let's suppose you are a Bayesian. Although the bolded passage is the wrong definition of a p-value, it's pretty much exactly what a Bayesian means when they say that the posterior probability of the alternative hypothesis is greater than 95%. And here's the thing. If the Bayesian posterior is actually the thing you want to report, why are you even trying to use orthodox methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.

Speaking for myself, I found this to be the most liberating thing about switching to the Bayesian view. Once you've made the jump, you no longer have to wrap your head around counter-intuitive definitions of p-values. You don't have to bother remembering why you can't say that you're 95% confident that the true mean lies within some interval. All you have to do is be honest about what you believed before you ran the study and then report what you learned from doing it. Sounds nice, doesn't it? To me, this is the big promise of the Bayesian approach. You do the analysis you really want to do, and express what you really believe the data are telling you.
--->


> _"您一直在使用那個詞。我不認為它意味著您認為的意思"_  
> -- 伊尼戈·蒙托亞,《公主新娘》[^bayesian-statistics-9]  

[^bayesian-statistics-9]: <a href="http://www.imdb.com/title/tt0093779/quotes"
    target="_blank">http://www.imdb.com/title/tt0093779/quotes</a> 。我應該指出,我不是第一個使用這句話來抱怨次數學派方法的人。Rich Morey和他的同事們首先有了這個想法。我厚顏無恥地偷了它,因為在這種情況下使用它是個非常棒的引文,而且我絕不會錯過任何引用《公主新娘》的機會。  

對我來說,貝氏方法的最大優勢之一是它能夠回答正確的問題。 在貝氏框架內,提到“假設為真的概率”是完全合理和允許的。 您甚至可以嘗試計算這個概率。 最終,這難道不是您希望統計檢驗告訴您的嗎? 對於真正的人類而言,這似乎是做統計的全部重點,即確定什麼是真的什麼不是真的。 每當您不完全確定真相時,都應使用概率論的語言來說些“理論 A 為真的概率為 80%,但理論 B 為真的概率為 20%”之類的話。

對人類而言,這似乎非常明顯,但在正統框架中這明確被禁止了。 對次數主義者來說,這種陳述是無意義的,因為“理論是真實的”不是一個可重複的事件。一個理論要麼是真的,要麼是假的,不管您多麼希望做出這樣的概率陳述,都是不允許的。 這就是為什麼在 @sec-The-p-value-of-a-test 中我一再警告您不要將 p 值解釋為虛無假設為真的概率。這就是為什麼幾乎每本統計學教科書都不得不重複這一警告。 那是因為人們迫切希望這是正確的解釋。 儘管有次數主義教條,但終生教授本科生和每天進行數據分析的經驗告訴我,大多數實際的人類認為“假設為真的概率”不僅有意義,而且是我們最關心的事情。 這是一個非常吸引人的想法,甚至受過訓練的統計學家也會誤解試圖以這種方式解釋 p 值。例如,以下是 2013 年 Newspoll 官方報告中的一段話,解釋如何解釋他們的(次數主義)數據分析:[^bayesian-statistics-10]

[^bayesian-statistics-10]: <a href="http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/%0A" target="_blank">http://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/</a> 


> _在整個報告中,在相關的地方,已注意到統計上顯著的變化。所有顯著性測試都是基於 95% 的置信水平進行的。_**這意味著如果注意到一項變化在統計上顯著,那麼實際變化發生的概率為 95%,並且僅僅是由於機會變異引起的**。*(強調添加)*

不!這不是 p < .05 的意思。對次數主義統計學家來說,95% 的置信水平也不是這個意思。加粗部分就是完全錯誤的。正統方法不能告訴您“實際變化發生的概率為 95%”,因為這不是次數主義概率可以分配給的事件類型。對於意識形態次數主義者來說,這句話應該是沒有意義的。即使您是更務實的次數主義者,它仍然是 p 值的錯誤定義。如果您想依賴正統統計工具,那麼這句話就根本不被允許或正確。  

另一方面,假設您是貝氏主義者。儘管加粗通過是 p 值的錯誤定義,但當貝氏主義者說替代假設的後驗概率大於 95% 時,這幾乎正是他們的意思。 而這裡有一個問題。如果您實際上想報告的是貝氏後驗,那麼您為什麼還要嘗試使用正統方法呢? 如果您想做出貝氏主義者的認定,您所要做的就是成為貝氏主義者並使用貝氏工具。  

就我個人而言,我發現切換到貝氏觀點最自由的就是這一點。 一旦您轉換了思維方式,您就不再需要圍繞 p 值的反直覺定義困惑您的大腦。 您不必費心記住為什麼您不能說您 95% 確定真值落在某個區間內。 您所要做的只是誠實地說出您在運行該研究之前的信念,然後報告您從中學到了什麼。 聽起來不錯,不是嗎? 對我來說,這就是貝氏方法的大前提。 您可以真正地做出您想要的分析,並表達您真正相信數據正在告訴您的內容。


### 你能相信的證據標準

<!--- 
> *If* $p$ is below .02 it is strongly indicated that the $null$ hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that smaller values of $p$ indicate a real discrepancy.\
> -- Sir Ronald Fisher [@Fisher1925]

Consider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of p-values, it's Fisher. In this passage, taken from his classic guide Statistical Methods for Research Workers, he's pretty clear about what it means to reject a null hypothesis at p < .05. In his opinion, if we take p < .05 to mean there is "a real effect", then "we shall not often be astray". This view is hardly unusual. In my experience, most practitioners express views very similar to Fisher's. In essence, the p < .05 convention is assumed to represent a fairly stringent evidential standard.

Well, how true is that? One way to approach this question is to try to convert p-values to Bayes factors, and see how the two compare. It's not an easy thing to do because a p-value is a fundamentally different kind of calculation to a Bayes factor, and they don't measure the same thing. However, there have been some attempts to work out the relationship between the two, and it's somewhat surprising. For example, @Johnson2013 presents a pretty compelling case that (for t-tests at least) the p < .05 threshold corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If that's right, then Fisher's claim is a bit of a stretch. Let's suppose that the null hypothesis is true about half the time (i.e., the prior probability of $H_0$ is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at p < .05. Using the data from @Johnson2013, we see that if you reject the null at p ă .05, you'll be correct about 80% of the time. I don't know about you but, in my opinion, an evidential standard that ensures you'll be wrong on 20% of your decisions isn't good enough. The fact remains that, quite contrary to Fisher's claim, if you reject at p < .05 you shall quite often go astray. It's not a very stringent evidential threshold at all. --->


> _如果 $p$低於.02,則強烈表明 $null$ 假設無法解釋全部事實。如果我們在 .05 畫一條常規線,並認為更小的 $p$值表示真實的差異,我們通常不會犯太大的錯誤。_  

> -- 羅納德·費雪爵士 [@Fisher1925]  

考慮上述羅納德·費雪爵士的引文,他是當今正統統計方法的奠基人之一。 如果有人有資格表達對 p 值預期功能的意見,那就是費雪了。 在摘自他的經典指南《研究工作者的統計方法》的這段話中,他非常清楚地說明了什麼是在 p < .05 時拒絕虛無假設。 在他看來,如果我們認為 p < .05 意味著“真實效應”,那麼“我們通常不會犯太大的錯誤”。 這種觀點並不罕見。 根據我的經驗,大多數從業人員的觀點與費雪的觀點非常相似。 從本質上講,假定 p < .05 建議代表了相當嚴格的證據標准。  

這是真的嗎?解決這個問題的一種方法是嘗試將 p 值轉換為貝氏因子,並查看兩者的比較情況。 這並不容易,因為 p 值與貝氏因子是一種基本不同的計算,它們不測量同樣的事情。 然而,已經有一些嘗試找出兩者之間的關係,而且有些令人吃驚。 例如,@Johnson2013 提出了一個相當有說服力的論點,即(至少對於 t 檢驗)p < .05 的閾值大致對應於大約 3:1 到 5:1 之間的貝氏因子支持替代假設。 如果這是正確的,那麼費雪的說法有些牽強。 假設虛無假設一半時間為真(即 $H_0$ 的先驗概率為 0.5),並使用這些數字計算在 p < .05 拒絕虛無假設時虛無假設的後驗概率。 使用 @Johnson2013 的數據,我們看到如果您在 p ă .05 時拒絕虛無假設,那麼大約 80% 的時候您是正確的。 我不知道您怎麼看,但在我看來,一個確保您在 20% 的決策中會錯誤的證據標准還不夠好。 事實仍然是,與費雪的說法完全相反,如果您在 p < .05 時拒絕虛無假設,您確實會經常犯錯。 這根本不是一個非常嚴格的證據閾值。  


### p值只是幻象

<!---

> *The cake is a lie.*\
> *The cake is a lie.*\
> *The cake is a lie.*\
> *The cake is a lie.*\
> -- Portal[^bayesian-statistics-11]

--->

> ![](images/The_cake_is_a_lie.jpg)
> -- 出自電玩遊戲「傳送門」的迷因梗[^bayesian-statistics-11]

[^bayesian-statistics-11]: <a href="http://knowyourmeme.com/memes/the-cake-is-a-lie"
    target="_blank">http://knowyourmeme.com/memes/the-cake-is-a-lie</a> .

<!---
Okay, at this point you might be thinking that the real problem is not with orthodox statistics, just the p \< .05 standard. In one sense, that's true. The recommendation that @Johnson2013 gives is not that "everyone must be a Bayesian now". Instead, the suggestion is that it would be wiser to shift the conventional standard to something like a p < .01 level. That's not an unreasonable view to take, but in my view the problem is a little more severe than that. In my opinion, there's a fairly big problem built into the way most (but not all) orthodox hypothesis tests are constructed. They are grossly naive about how humans actually do research, and because of this most p-values are wrong.

Sounds like an absurd claim, right? Well, consider the following scenario. You've come up with a really exciting research hypothesis and you design a study to test it. You're very diligent, so you run a power analysis to work out what your sample size should be, and you run the study. You run your hypothesis test and out pops a p-value of 0.072. Really bloody annoying, right?

What should you do? Here are some possibilities:

1.  You conclude that there is no effect and try to publish it as a null result
2.  You guess that there might be an effect and try to publish it as a "borderline significant" result
3.  You give up and try a new study
4.  You collect some more data to see if the p value goes up or (preferably!) drops below the "magic" criterion of p < .05

Which would you choose? Before reading any further, I urge you to take some time to think about it. Be honest with yourself. But don't stress about it too much, because you're screwed no matter what you choose. Based on my own experiences as an author, reviewer and editor, as well as stories I've heard from others, here's what will happen in each case:

- Let's start with option 1. If you try to publish it as a null result, the paper will struggle to be published. Some reviewers will think that p = .072 is not really a null result. They'll argue it's borderline significant. Other reviewers will agree it's a null result but will claim that even though some null results are publishable, yours isn't. One or two reviewers might even be on your side, but you'll be fighting an uphill battle to get it through.

- Okay, let's think about option number 2. Suppose you try to publish it as a borderline significant result. Some reviewers will claim that it's a null result and should not be published. Others will claim that the evidence is ambiguous, and that you should collect more data until you get a clear significant result. Again, the publication process does not favour you.

- Given the difficulties in publishing an "ambiguous" result like p = .072, option number 3 might seem tempting: give up and do something else. But that's a recipe for career suicide. If you give up and try a new project every time you find yourself faced with ambiguity, your work will never be published. And if you're in academia without a publication record you can lose your job. So that option is out.

- It looks like you're stuck with option 4. You don't have conclusive results, so you decide to collect some more data and re-run the analysis. Seems sensible, but unfortunately for you, if you do this all of your p-values are now incorrect. All of them. Not just the p-values that you calculated for this study. All of them. All the p-values you calculated in the past and all the p-values you will calculate in the future. Fortunately, no-one will notice. You'll get published, and you'll have lied.

Wait, what? How can that last part be true? I mean, it sounds like a perfectly reasonable strategy doesn't it? You collected some data, the results weren't conclusive, so now what you want to do is collect more data until the the results are conclusive. What's wrong with that?

Honestly, there's nothing wrong with it. It's a reasonable, sensible and rational thing to do. In real life, this is exactly what every researcher does. Unfortunately, the theory of null hypothesis testing as I described it in @sec-Hypothesis-testing forbids you from doing this.[^bayesian-statistics-12] The reason is that the theory assumes that the experiment is finished and all the data are in. And because it assumes the experiment is over, it only considers two possible decisions. If you're using the conventional p < .05 threshold, those decisions are shown in @tbl-tab16-11.

[^bayesian-statistics-12]: In the interests of being completely honest, I should acknowledge that not all orthodox statistical tests rely on this silly assumption. There are a number of sequential analysis tools that are sometimes used in clinical trials and the like. These methods are built on the assumption that data are analysed as they arrive, and these tests aren't horribly broken in the way I'm complaining about here. However, sequential analysis methods are constructed in a very different fashion to the "standard" version of null hypothesis testing. They don't make it into any introductory textbooks, and they're not very widely used in the psychological literature. The concern I'm raising here is valid for every single orthodox test I've presented so far and for almost every test I've seen reported in the papers I read.

Conventional Null hypothesis signicance testing (NHST) with p < .05).
--->


在這一點上,您可能會認為真正的問題不在於正統統計學,只在於 p < .05 的標準。在某種意義上,這是正確的。 @Johnson2013 的建議並不是“現在每個人都必須成為貝氏主義者”。相反,建議是更明智的做法是將常規標準轉移到類似 p < .01 的水平。這不是一個不合理的觀點,但在我看來,問題比這更嚴重一些。在我看來,大多數(但不是全部)正統假設檢定所構建的方式中存在一個相當大的問題。它們在研究人員如何進行研究方面極為天真,因此大多數 p 值都是錯誤的。

聽起來像一個荒謬的說法,對嗎?好吧,思考一下以下情景。您提出了一個非常激動人心的研究假設,並設計了一項研究來測試它。您非常勤奮,所以您運行了功效分析以確定樣本量應該是多少,並進行了研究。您運行了您的假設檢定,跳出了一個 p 值為 0.072。真他媽的令人惱火,不是嗎? 

您應該怎麼做?以下是一些可能性:

1. 您得出結論認為沒有效應並嘗試將其作為無效果結果發表  

2. 您猜測可能存在效應並嘗試將其作為“邊緣顯著”結果發表  

3. 您放棄並嘗試新的研究  

4. 您收集更多數據以查看 p 值是否上升或(更好的是!)下降至“魔力”標准 p < .05 以下

你會選哪一個? 在繼續閱讀之前,我強烈建議您花些時間思考一下。 跟自己誠實。 但不要為此費太多心思,因為無論您選擇什麼,您都是搞砸了。 基於我作為作者、評審員和編輯的親身經歷,以及我從其他人那裡聽到的故事,以下是每種情況下會發生的事情:

- 讓我們從選項 1 開始。如果您嘗試將其作為無效果結果發表,論文的發表將會困難重重。一些評審員會認為 p = .072 並不是真的無效果。他們會認為這是邊緣顯著。其他評審員會同意這是無效果,但會聲稱儘管某些無效果結果是可以發表的,但您的研究不是。一兩個評審員甚至可能站在您這邊,但您在讓論文獲得接受方面將面臨一場苦戰。

- 好的,讓我們考慮選項 2。假設您嘗試將其作為邊緣顯著結果發表。一些評審員會聲稱這是無效果並不應該發表。其他人會聲稱證據不明確,並且您應該收集更多數據直到獲得明確的顯著結果。同樣,發表過程並不青睞您。

- 鑑於發表像 p = .072 這樣的“模糊”結果的困難,選項 3 似乎很誘人:放棄並做點其他的事情。但這是職業自殺的計劃。如果您每次面對模糊時都放棄並嘗試新的項目,那麼您的工作永遠不會被發表。如果您在學術領域沒有發表記錄,您可能會失去工作。因此該選項被排除。

- 看起來您只剩下選項 4 了。您沒有決定性的結果,所以您決定收集更多數據並重新運行分析。看起來很合理,但不幸的是,如果您這樣做,那麼您所有的 p 值現在都是不正確的。全部都是。不僅僅是您為這項研究計算的 p 值。全部都是。您過去計算的所有 p 值以及您將來計算的所有 p 值。幸運的是,沒有人會注意到。您將得到發表,而且您撒謊了。

等等,什麼?最后一部分怎麼可能是真的?我的意思是,這聽起來像一個非常合理的策略,不是嗎?您收集了一些數據,結果並不確定,所以現在您想要做的就是收集更多數據直到結果確定。這有什麼問題嗎?

誠實地說,這沒有任何問題。這是一個合理的、明智的和理性的做法。在現實生活中,這正是每個研究人員所做的。不幸的是,如我在 @sec-Hypothesis-testing 中描述的虛無假設檢定理論禁止您這樣做。[^bayesian-statistics-12] 原因是理論假設實驗已經結束並且所有的數據已經收集完畢。並且因為它假設實驗已經結束,它只考慮兩種可能的決策。如果您使用常規的 p < .05 標準,這些決策如 @tbl-tab16-11 所示。  

[^bayesian-statistics-12]: 為了完全誠實,我應該承認並非所有正統統計檢驗都依賴這個愚蠢的假設。有時在臨床試驗等中會使用一些序列分析工具。這些方法建立在假設數據在到達時即進行分析的基礎上,並且這些檢驗在本章中指出的方式中沒有嚴重失誤。然而,序列分析方法的構建與“標準”版虛無假設檢定截然不同。它們沒有進入任何介紹教科書,在心理學文獻中也很少使用。我在此提出的問題對我目前介紹的每一種正統檢驗和我在文獻中看到的幾乎每一種檢驗都是有效的。  




```{r}
#| label: tbl-tab16-11
#| tbl-cap: 常規虛無假設顯著性檢定 (NHST),p < .05)。
huxtabs[[16]][[11]]
```

<!---
What you're doing is adding a third possible action to the decision making problem. Specifically, what you're doing is using the p-value itself as a reason to justify continuing the experiment. And as a consequence you've transformed the decision-making procedure into one that looks more like @tbl-tab16-12.

Carrying on data collecting based on p-values obtained in preliminary testing.
--->

您正在做的是向決策問題添加第三種可能的行動。具體來說,您正在使用 p 值本身作為繼續該實驗的理由。因此,您將決策過程轉變為更類似于 @tbl-tab16-12 的過程。  

基于初步測試中獲得的 p 值繼續收集數據。  

```{r}
#| label: tbl-tab16-12
#| tbl-cap: Carrying on data collecting based on p-values obtained in preliminary testing
huxtabs[[16]][[12]] 
```

<!---
The "basic" theory of null hypothesis testing isn't built to handle this sort of thing, not in the form I described in @sec-Hypothesis-testing. If you're the kind of person who would choose to "collect more data" in real life, it implies that you are not making decisions in accordance with the rules of null hypothesis testing. Even if you happen to arrive at the same decision as the hypothesis test, you aren't following the decision process it implies, and it's this failure to follow the process that is causing the problem.[^bayesian-statistics-13] Your p-values are a lie.

[^bayesian-statistics-13]: A related problem: <a href="http://xkcd.com/1478/"
    target="_blank">http://xkcd.com/1478/</a> .

Worse yet, they're a lie in a dangerous way, because they're all *too small*. To give you a sense of just how bad it can be, consider the following (worst case) scenario. Imagine you're a really super-enthusiastic researcher on a tight budget who didn't pay any attention to my warnings above. You design a study comparing two groups. You desperately want to see a significant result at the $p < .05$ level, but you really don't want to collect any more data than you have to (because it's expensive). In order to cut costs you start collecting data but every time a new observation arrives you run a t-test on your data. If the t-tests says $p < .05$ then you stop the experiment and report a significant result. If not, you keep collecting data. You keep doing this until you reach your pre-defined spending limit for this experiment. Let's say that limit kicks in at $N = 1000$ observations. As it turns out, the truth of the matter is that there is no real effect to be found: the null hypothesis is true. So, what's the chance that you'll make it to the end of the experiment and (correctly) conclude that there is no effect? In an ideal world, the answer here should be 95%. After all, the whole point of the $p < .05$ criterion is to control the Type I error rate at 5%, so what we'd hope is that there's only a 5% chance of falsely rejecting the null hypothesis in this situation. However, there's no guarantee that will be true. You're breaking the rules. Because you're running tests repeatedly, "peeking" at your data to see if you've gotten a significant result, all bets are off.

So how bad is it? The answer is shown as the solid black line in @fig-fig16-1, and it's astoundingly bad. If you peek at your data after every single observation, there is a 49% chance that you will make a Type I error. That's, um, quite a bit bigger than the 5% that it's supposed to be. By way of comparison, imagine that you had used the following strategy. Start collecting data. Every single time an observation arrives, run [Bayesian t-tests] and look at the Bayes factor. I'll assume that @Johnson2013 is right, and I'll treat a Bayes factor of 3:1 as roughly equivalent to a p-value of .05.[^bayesian-statistics-14] This time around, our trigger happy researcher uses the following procedure. If the Bayes factor is 3:1 or more in favour of the null, stop the experiment and retain the null. If it is 3:1 or more in favour of the alternative, stop the experiment and reject the null. Otherwise continue testing. Now, just like last time, let's assume that the null hypothesis is true. What happens? As it happens, I ran the simulations for this scenario too, and the results are shown as the dashed line in @fig-fig16-1. It turns out that the Type I error rate is much much lower than the 49% rate that we were getting by using the orthodox t-test.

[^bayesian-statistics-14]: Some readers might wonder why I picked 3:1 rather than 5:1, given that @Johnson2013 suggests that $p = .05$ lies somewhere in that range. I did so in order to be charitable to the p-value. If I'd chosen a 5:1 Bayes factor instead, the results would look even better for the Bayesian approach.

How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is *very wrong*. --->


我在 @sec-Hypothesis-testing 中描述的虛無假設檢定的“基本”理論並沒有建立來處理這類事情。如果您是在現實生活中會選擇“收集更多數據”的那類人,這意味著您并沒有按照虛無假設檢定的規則進行決策。即使您碰巧與假設檢定得出相同的決定,您也沒有遵循其所暗示的決策過程,這种過程違規正是導致問題的原因。[^bayesian-statistics-13] 您的 p 值只是幻覺。  

[^bayesian-statistics-13]: 一張漫畫解釋這裡談的問題: <a href="http://xkcd.com/1478/"
    target="_blank">http://xkcd.com/1478/</a> .

更糟糕的是,它們以一種危險的方式撒謊,因為它們都_太小_了。為了讓您感受一下這有多嚴重,請考慮以下(最壞)情況。假設您是一位預算十分緊張的超級熱心的研究員,完全沒有注意我上面的警告。您設計了一項比較兩組的研究。您迫切希望在 $p < .05$ 水平上看到顯著結果,但您真的不想收集比需要的更多數據(因為這很昂貴)。為了節省成本,您開始收集數據,但每次有新的觀察值到來時,您都會對數據運行 t 檢驗。如果 t 檢驗說 $p < .05$,那麼您停止實驗並報告顯著結果。如果不是,您會繼續收集數據。您會一直這樣做,直到達到實驗的預定花費限額。假設該限制在 $N = 1000$ 的觀察上觸發。事實證明,真相是沒有真正的效應:虛無假設為真。所以,您達到實驗結束並(正確地)得出沒有影響的結論的機率是多少?在理想的世界中,答案在此應該是 95%。畢竟,$p < .05$ 標准的全部要點是將型I錯誤率控制在 5% 左右,所以我們希望的就是在這種情況下錯誤拒絕虛無假設的概率只有 5%。然而,沒有保證這將是真的。您是在破壞規則。因為您反复運行測試,“偷看”數據以查看是否獲得了顯著結果,所以所有賭注都打消了。

那麼情況究竟有多糟糕呢?答案如 @fig-fig16-1 中實線所示,這是驚人的糟糕。如果您在每個單獨的觀察后查看數據,您將有 49% 的可能性會產生型I錯誤。也就是說,嗯,這比應該的 5% 大得多。相比之下,設想您使用了以下策略。開始收集數據。每次觀察到達時,運行 [貝氏t檢定] 並查看貝氏因子。我假設 @Johnson2013 是正確的,並且我會將 3:1 的貝氏因子大致視為等同於 p 值為 .05。[^bayesian-statistics-14] 這次,我們的扣發狂研究人員使用以下過程。如果貝氏因子大於或等於 3:1 支持虛無假設,則停止實驗並保留虛無假設。如果它大於或等於 3:1 支持替代假設,則停止實驗並拒絕虛無假設。否則繼續測試。現在,就像上次一樣,假設虛無假設為真。會發生什麼?實際上,我也為此情況運行了模擬,結果如圖 @fig-fig16-1 中的虛線所示。結果報表明,I 型錯誤率遠遠低於使用正統 t 檢驗獲得的 49%。  

[^bayesian-statistics-14]: 有些讀者可能會質疑為什麼我選擇 3:1 而不是 5:1,因為 @Johnson2013 建議 $p = .05$ 落在這個範圍內。我這樣做是為了給 p 值一個好處。如果我選擇 5:1 的貝氏因子,結果對於貝氏方法來說會看起來更好。  



```{r}
## 未找到備份檔案，此段程式碼不執行
## this include takes a long time to run
# include("peeking_type_I_error.R")

## so load saved data instead
#p_bf <- readRDS("data_and_tables/p_bf.rds")
#library(scales)
```


```{r}
#| label: fig-fig16-1
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 如果每次有新數據到達時您重新運行測試,事情會變得多糟?如果您是次數主義者,答案是*非常糟*。
knitr::include_graphics("images/fig16-1.png")
```

<!---
In some ways, this is remarkable. The entire *point* of orthodox null hypothesis testing is to control the Type I error rate. Bayesian methods aren't actually designed to do this at all. Yet, as it turns out, when faced with a "trigger happy" researcher who keeps running hypothesis tests as the data come in, the Bayesian approach is much more effective. Even the 3:1 standard, which most Bayesians would consider unacceptably lax, is much safer than the p < .05 rule. --->


在某種意義上,這很令人驚訝。正統虛無假設檢定的*全部*重點是控制型I錯誤率。貝氏方法實際上根本沒有設計來做這件事。然而,事實證明,當面對一個不斷運行假設檢定的“扳機快”研究人員時,貝氏方法更加有效。即使是大多數貝氏主義者認為可以接受的寬鬆 3:1 標準,也比 p < .05 規則更安全。



### 顯著性檢定真的有那麼糟嗎？

<!--- The example I gave in the previous section is a pretty extreme situation. In real life, people don't run hypothesis tests every time a new observation arrives. So it's not fair to say that the p < .05 threshold "really" corresponds to a 49% Type I error rate (i.e., $p = .49$). But the fact remains that if you want your p-values to be honest then you either have to switch to a completely different way of doing hypothesis tests or enforce a strict rule of no peeking. You are not allowed to use the data to decide when to terminate the experiment. You are not allowed to look at a "borderline" p-value and decide to collect more data. You aren't even allowed to change your data analyis strategy after looking at data. You are strictly required to follow these rules, otherwise the p-values you calculate will be nonsense.

And yes, these rules are surprisingly strict. As a class exercise a couple of years back, I asked students to think about this scenario. Suppose you started running your study with the intention of collecting $N = 80$ people. When the study starts out you follow the rules, refusing to look at the data or run any tests. But when you reach $N = 50$ your willpower gives in... and you take a peek. Guess what? You've got a significant result! Now, sure, you know you said that you'd keep running the study out to a sample size of $N = 80$, but it seems sort of pointless now, right? The result is significant with a sample size of $N = 50$, so wouldn't it be wasteful and inefficient to keep collecting data? Aren't you tempted to stop? Just a little? Well, keep in mind that if you do, your Type I error rate at $p < .05$ just ballooned out to 8%. When you report $p < .05$ in your paper, what you're really saying is $p < .08$. That's how bad the consequences of "just one peek" can be.

Now consider this. The scientific literature is filled with t-tests, ANOVAs, regressions and chi-square tests. When I wrote this book I didn't pick these tests arbitrarily. The reason why these four tools appear in most introductory statistics texts is that these are the bread and butter tools of science. None of these tools include a correction to deal with "data peeking": they all assume that you're not doing it. But how realistic is that assumption? In real life, how many people do you think have "peeked" at their data before the experiment was finished and adapted their subsequent behaviour after seeing what the data looked like? Except when the sampling procedure is fixed by an external constraint, I'm guessing the answer is "most people have done it". If that has happened, you can infer that the reported p-values are wrong. Worse yet, because we don't know what decision process they actually followed, we have no way to know what the p-values should have been. You can't compute a p-value when you don't know the decision making procedure that the researcher used. And so the reported p-value remains a lie.

Given all of the above, what is the take home message? It's not that Bayesian methods are foolproof. If a researcher is determined to cheat, they can always do so. Bayes' rule cannot stop people from lying, nor can it stop them from rigging an experiment. That's not my point here. My point is the same one I made at the very beginning of the book in Section 1.1: the reason why we run statistical tests is to protect us from ourselves. And the reason why "data peeking" is such a concern is that it's so tempting, even for honest researchers. A theory for statistical inference has to acknowledge this. Yes, you might try to defend p-values by saying that it's the fault of the researcher for not using them properly, but to my mind that misses the point. A theory of statistical inference that is so completely naive about humans that it doesn't even consider the possibility that the researcher might look at their own data isn't a theory worth having. In essence, my point is this:

> *Good laws have their origins in bad morals.*\
> -- Ambrosius Macrobius [^bayesian-statistics-15]

[^bayesian-statistics-15]: <a href="http://www.quotationspage.com/quotes/Ambrosius%20Macrobius/"
    target="_blank"><http://www.quotationspage.com/quotes/Ambrosius> Macrobius/</a>

Good rules for statistical testing have to acknowledge human frailty. None of us are without sin. None of us are beyond temptation. A good system for statistical inference should still work even when it is used by actual humans. Orthodox null hypothesis testing does not.[^bayesian-statistics-16]

[^bayesian-statistics-16]: Okay, I just know that some knowledgeable frequentists will read this and start complaining about this section. Look, I'm not dumb. I absolutely know that if you adopt a sequential analysis perspective you can avoid these errors within the orthodox framework. I also know that you can explictly design studies with interim analyses in mind. So yes, in one sense I'm attacking a "straw man" version of orthodox methods. However, the straw man that I'm attacking is the one that *is used by almost every single practitioner*. If it ever reaches the point where sequential methods become the norm among experimental psychologists and I'm no longer forced to read 20 extremely dubious ANOVAs a day, I promise I'll rewrite this section and dial down the vitriol. But until that day arrives, I stand by my claim that default Bayes factor methods are much more robust in the face of data analysis practices as they exist in the real world. *Default* orthodox methods suck, and we all know it.--->

在上一節中給出的例子是一種相當極端的情況。在現實生活中,人們不會在每次有新觀察值到達時運行假設檢定。所以說 p < .05 標準“實際上”對應 49% 的型I錯誤率(即 $p = .49$)並不公平。但事實仍然是,如果您想要您的 p 值是誠實的,那麼您要麼需要切換到一種完全不同的假設檢定方法,要麼執行嚴格的不偷看規則。您不允許使用數據來決定何時終止實驗。您不允許查看“邊緣” p 值並決定收集更多數據。您甚至不允許在查看數據后更改數據分析策略。您嚴格要求遵循這些規則,否則您計算的 p 值將是無意義的。   

而且是的,這些規則出人意料地嚴格。幾年前的一次課堂練習中,我要求學生考慮這種情況。假設您開始運行研究,意圖收集 $N = 80$ 人。 當研究開始時,您遵循規則,拒絕查看數據或運行任何測試。 但是當您達到 $N = 50$ 時,您的意志力屈服了......您偷看了一眼。 猜猜怎麼著? 您得到了顯著結果! 現在,當然,您知道您說過您會繼續運行研究,樣本量為 $N = 80$,但現在這似乎有點毫無意義,不是嗎? 當樣本量為 $N = 50$ 時結果就是顯著的,所以繼續收集數據豈不是浪費和低效嗎? 您不會感到誘惑停下來嗎?就是一點點? 請記住,如果您這樣做了,您在 $p < .05$ 水平上的型I錯誤率剛剛膨脹到了 8%。 當您在論文中報告 $p < .05$ 時,您真正在說的是 $p < .08$。 這就是“只偷看一眼”的後果可能會有多糟。   

現在考慮一下。 科學文獻中充滿了 t 檢驗、方差分析、回歸和卡方檢驗。 當我寫這本書的時候,我並沒有隨意選擇這些測試。 出現在大多數入門統計教科書中的這四種工具的原因是它們是科學的主要工具。 這些工具中沒有一個包括處理“查看數據”的校正:它們都假定您不這樣做。 但這個假設有多現實呢? 在現實生活中,您認為在實驗結束前有多少人“查看”過數據並在看到數據的樣子后調整了后續行為呢? 除非採樣過程受外在約束的限制,否則我猜答案是“大多數人都這樣做過”。如果發生這種情況,您可以推斷報告的 p 值是錯誤的。 更糟糕的是,因為我們不知道他們實際遵循的決策過程,我們無從知道 p 值本該是麼。 如果不知道研究人員使用的決策制定過程,則無法計算 p 值。 所以報告的 p 值仍然在撒謊。   

鑑於上述所有內容,結論是麼呢? 並不是貝氏方法天衣無縫。如果一個研究者決心欺騙,他們總是可以這樣做的。 貝葉斯規則無法阻止人們撒謊,也無法阻止他們操縱實驗。 這不是我在此要論述的要點。 這個論點與本書一開始就強調的一樣:我們運行統計檢驗的原因是為了防止我們遭受自身的傷害。(見 @sec-why-do-we-learn-statistics-begin ) 而“查看數據”之所以這麼令人擔憂的是因為它非常誘人,即使對誠實的研究人員也是如此。 推論理論必須承認這一點。 是的,您可能會通過說研究人員沒有正確使用它們來為 p 值辯護,但在我看來,這遺漏了重點。 一種統計推論理論如果對人類的了解是如此天真,甚至不考慮研究人員可能會查看自己的數據的可能性,那麼這種理論就不值得擁有。 從本質上講,我的論點是:  

> *好的法律起源於不良道德。*  
> -- 安布羅西烏斯·馬克羅比烏斯 [^bayesian-statistics-15]  


[^bayesian-statistics-15]: <a href="http://www.quotationspage.com/quotes/Ambrosius%20Macrobius/"
    target="_blank"><http://www.quotationspage.com/quotes/Ambrosius> Macrobius/</a>


良好的統計檢驗規則必須承認人性軟弱。 我們所有人都有罪。我們所有人都面臨誘惑。 一個良好的統計推論系統應該仍然有效,即使它是由真實的人使用的。 正統虛無假設檢定並未做到這一點。 [^bayesian-statistics-16]  

[^bayesian-statistics-16]: 好吧,我知道一些知識豐富的次數主義者會讀這個部分並開始抱怨。 看,我又不傻。 我絕對知道,如果您採用序列分析的角度,您可以在正統框架內避免這些錯誤。 我也知道您可以明確設計考慮過期中分析的研究。 所以是的,從某種意義上說,我正在攻擊正統方法的“稻草人”版本。 然而,我所攻擊的稻草人是*幾乎每一個從業者*都在使用的版本。 如果有朝一日序列方法在實驗心理學家中成為規範,當我不再被迫每天閱讀 20 個極其可疑的方差分析時,我保證我會重寫這一節並減少苦澀。 但是在那一天到來之前,我堅持我的說法,即默認的貝氏因子方法在面對現實世界中存在的數據分析實踐中,具有更強的穩健性。 *默認的*正統方法很糟糕,我們都知道。



## 貝氏t檢定

<!---
An important type of statistical inference problem discussed in this book is comparing two means, discussed in some detail in @sec-Comparing-two-means on t-tests. If you can remember back that far, you'll recall that there are several versions of the t-test. I'll talk a little about Bayesian versions of the independent samples t-tests and the paired samples t-test in this section.
--->

本書中討論的一種重要的統計推論問題是比較兩個均值,在 @sec-Comparing-two-means t檢定部分有詳細討論。如果您還記得那麼遠的內容,您會回憶起有幾種t檢定的版本。在本節中,我將稍微談論獨立樣本t檢定和配對樣本t檢定的貝氏版本。


### 獨立樣本t檢定

<!---
The most common type of t-test is the independent samples t-test, and it arises when you have data as in the harpo.csv data set that we used in @sec-Comparing-two-means on t-tests. In this data set, we have two groups of students, those who received lessons from Anastasia and those who took their classes with Bernadette. The question we want to answer is whether there's any difference in the grades received by these two groups of students. Back in @sec-Comparing-two-means I suggested you could analyse this kind of data using the Independent Samples t-test in jamovi, which gave us the results in @fig-fig16-2. As we obtain a p-value less than 0.05, we reject the null hypothesis.

Independent Samples t-test result in jamovi.
--->

最常見的t檢定是獨立樣本t檢定,當您擁有類似於我們在 @sec-Comparing-two-means t檢定部分使用的harpo.csv數據集的數據時就會出現。在這個數據集中,我們有兩組學生,那些接受Anastasia授課的學生和那些跟Bernadette上課的學生。我們要回答的問題是這兩組學生獲得的成績是否有任何區別。 在 @sec-Comparing-two-means ,我建議您可以使用jamovi中的獨立樣本t檢定分析這種數據,它給我們 @fig-fig16-2 的結果。由於我們獲得了小於0.05的p值,我們拒絕了虛無假設。 


```{r}
#| label: fig-fig16-2
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: jamovi的獨立樣本t檢定結果報表
knitr::include_graphics("images/fig11-11.png")
```

<!---
What does the Bayesian version of the t-test look like? We can get the Bayes factor analysis by selecting the 'Bayes factor' checkbox under the 'Tests' option, and accepting the suggested default value for the 'Prior'. This gives the results shown in the table in @fig-fig16-3. What we get in this table is a Bayes factor statistic of 1.75, meaning that the evidence provided by these data are about 1.8:1 in favour of the alternative hypothesis.

Before moving on, it's worth highlighting the difference between the orthodox test results and the Bayesian one. According to the orthodox test, we obtained a significant result, though only barely. Nevertheless, many people would happily accept p = .043 as reasonably strong evidence for an effect. In contrast, notice that the Bayesian test doesn't even reach 2:1 odds in favour of an effect, and would be considered very weak evidence at best. In my experience that's a pretty typical outcome. Bayesian methods usually require more evidence before rejecting the null.

Bayes factors analysis alongside Independent Samples t-Test.
--->

那麼貝氏t檢定看起來是什麼樣子呢?我們可以通過在“測試”選項下選擇“貝氏因子”複選框並接受對“先驗”的建議默認值來獲得貝氏因子分析。這給出了 @fig-fig16-3 表格中顯示的結果。我們在這個表中得到的貝氏因子統計量為1.75,這意味著這些數據提供的證據大約以1.8:1的比率支持替代假設。

在繼續之前,有必要強調正統檢驗結果和貝氏檢驗結果之間的區別。根據正統檢驗,我們獲得了顯著結果,儘管只是勉强顯著。儘管如此,很多人會很高興地接受p = .043作為效應的合理有力的證據。相比之下,請注意,貝氏檢驗甚至沒有達到2:1的比率支持效應,最多被認為是非常弱的證據。根據我的經驗,這是一個非常典型的結果。在拒絕虛無假設之前,貝氏方法通常需要更多的證據。 


```{r}
#| label: fig-fig16-3
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 在獨立樣本t檢定報表並列貝氏因子
knitr::include_graphics("images/fig16-3.png")
```

### 相依樣本t檢定

<!---
Back in Section 11.5 I discussed the chico.csv data set in which student grades were measured on two tests, and we were interested in finding out whether grades went up from test 1 to test 2. Because every student did both tests, the tool we used to analyse the data was a paired samples t-test. @fig-fig16-4 shows the jamovi results table for the conventional paired t-test alongside the Bayes factor analysis. At this point, I hope you can read this output without any difficulty. The data provide evidence of about 6000:1 in favour of the alternative. We could probably reject the null with some confidence!

Paired samples T-Test and Bayes Factor result in jamovi.
--->


在 @sec-the-independent-samples-t-test-welch-test ,我討論了chico.csv數據集,其中測量了學生在兩個測試中的成績,我們有興趣了解從測試1到測試2的成績是否有所提高。因為每個學生都參加了兩次測試,所以我們使用的數據分析工具是相依樣本t檢定。 @fig-fig16-4 顯示了jamovi結果報表,傳統的成對t檢定結果與貝氏因子分析並列。在這一點上,我希望您可以毫不費力地讀取這個輸出。數據提供了大約6000:1的證據支持替代假設。我們可能可以非常自信地拒絕虛無假設!


```{r}
#| label: fig-fig16-4
#| fig-align: left
#| fig-width: 0.9
#| fig-cap: 在相依樣本t檢定報表並列貝氏因子
knitr::include_graphics("images/fig16-4.png")
```

## 本章小結

本章前半部主要討論貝氏統計學的理論基礎。[理性者的機率推論]這一節介紹貝氏推論的數學推導，接著概述[貝氏假設檢定]。然後我在[為何需要貝氏統計]說明希望同學學習貝氏統計的理由。

後半部是實例說明及演練[貝氏t檢定]。如果同學想知道更多貝氏統計，市面上已經有許多好書問世，等著你去研讀學習。 @Kruschke2011 撰寫的教科書*實作貝氏資料分析(Doing Bayesian Data Analysis)*結合理與實作範例，是不錯的第一選擇[^translation-1]。這本書並非以本章介紹的"貝氏因子"介紹具氏統計，所以你不必充分了解本章內容也能閱讀學習。如果是對認知心理學有興趣的同學，應該要找 @Lee2014 的教科書做為進階學習。原作者推薦以上兩本是因為作者的專長領域相近，還有許多不同領域學者所撰寫的貝氏統計教科書或教程，有心學習的同學請好好留意。


[^translation-1]: 台灣同學可以找旗標出版社出版的〈AI必須! 從做中學貝氏統計〉做為自學材料。這本書援引John Kruschke的說明方式，介紹各種貝氏統計方法理論細節及運算方法。

<!---The first half of this chapter was focused primarily on the theoretical underpinnings of Bayesian statistics. I introduced the mathematics for how Bayesian inference works in the section on [Probabilistic reasoning by rational agents], and gave a very basic overview of Bayesian hypothesis tests]. Finally, I devoted some space to talking about why I think [Bayesian methods are worth using](Why be a Bayesian).

Then I gave a practical example, with [Bayesian t-tests]. If you're interested in learning more about the Bayesian approach, there are many good books you could look into. John Kruschke's book *Doing Bayesian Data Analysis* is a pretty good place to start [@Kruschke2011] and is a nice mix of theory and practice. His approach is a little different to the "Bayes factor" approach that I've discussed here, so you won't be covering the same ground. If you're a cognitive psychologist, you might want to check out @Lee2014. I picked these two because I think they're especially useful for people in my discipline, but there's a lot of good books out there, so look around!--->
