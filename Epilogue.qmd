# 後記 {.unnumbered}


> “從頭開始,”國王非常嚴肅地說,“一直到結尾;然後停止”  -- [路易斯·卡羅](https://zh.wikipedia.org/zh-tw/%E8%B7%AF%E6%98%93%E6%96%AF%C2%B7%E5%8D%A1%E7%BE%85)

我(原作者)寫這個單元時感覺很奇怪,甚至有點不恰當。後記是一本書的完結,但是這本書還沒有真正完成。仍然有很多東西沒有寫進這本書，甚至都沒有提到，並且有*不少*地方沒有引用出處。而且沒有“自我練習”的環節。總之，我覺得這本書的排版、架構和內容都有很多要改進之處。有鑑於此,我並不想寫出“正式的”後記。既然還沒有完成實質內容的寫作,把所有內容聯繫起來是沒有意義的。不過這個版本將上提供學生學習,有需要的話也可以購買紙本版,所以我希望至少讓這本書看起來完整。就請使用這本書的讀者多多指教吧。

## 尚未提到的統計學課題 

首先,我將談論一些我希望擠進本書版本中的內容,這樣您就可以對世界上還有什么其他統計思想有所了解。即使這本書快要最終完成,我也認為這一點很重要。學生們經常沒有意識到,他們的入門統計課只是個入門。如果您想走向廣闊的世界並進行真正的數據分析,您必須學會大量擴展本科課程內容的新工具。不要假定某些事情無法完成,只因為本科並未涵蓋。也不要假定某些事情就是正確的,只因為它在本科課上被涵蓋了。為了防止您陷入這個陷阱,我認為有必要概述一些其他的思路。  

### 各主題單元的遺珠  

即使在我在書中涵蓋的主題中,也有很多遺漏我希望能在未來的版本中補充。僅就純統計學而言(而不是與 jamovi 相關的內容),以下是一份代表性但並不詳盡的清單,我希望能在某個時候擴展這些內容:  

- **其他類型的相關性**。在 @sec-Correlation-and-linear-regression 中,我談到了兩種相關性:皮爾遜和斯皮爾曼。當您有兩個連續變量並希望評估它們之間的關係時,這兩種相關性評估方法都是適用的。如果您的變量都是名義尺度的,那麼情況又如何呢? 或者一個是名義尺度,另一個是連續的呢? 實際上,在這些情況下也存在計算相關性的方法(例如 polychoric 相關),將它們包括進來會很好。  

- **更多關於效應量的詳細信息**。 總的來說,我認為全文中對效應量的處理有點過于簡單。 在幾乎每一種情況下,我都傾向於僅選擇一種效應量測度(通常是最流行的),並描述它。 然而,對於幾乎所有的測試和模型,都存在多種思考效應量的方法,我希望在未來能更詳細地討論這一點。  

- **處理被違反的假設**。 在本書的若干部分中,我談到了在發現您的測試(或模型)的假設被違反時可以採取的一些措施,但我認為在這方面我應該說得更多。特別是,我認為討論更詳細的變量轉換以解決問題的內容會很好。我在 @sec-Pragmatic-matters 中稍微談到了這一點,但我認為討論還不夠詳細。

- **回歸的交互項**。在 @sec-Factorial-ANOVA 中,我談到過方差分析中可以有交互項,我也指出方差分析可以被解釋為某種線性回歸模型。 然而,在 @sec-Correlation-and-linear-regression 中談及回歸時,我完全沒有提及交互。 然而,沒有什么能阻止您在回歸模型中包含交互項。 當您在談論兩個連續預測變量的交互時,弄清“交互”實際上意味著什么會稍微複雜一些,而且可以有多種方法。儘管如此,我本想稍微談論一下這個話題。

- **計劃比較法**。 正如我在 @sec-Factorial-ANOVA 中所提到的,在進行方差分析時,使用像 Tukey HSD 這樣的事後校正並不總是合適的,特別是當您事前就有非常明確(和有限)的比較關注點時。我希望將來能更多地談論這一點。

- **多重比較方法**。 即使在討論事後檢驗和多重比較的背景下,我也希望能更詳細地討論這些方法,並談論除了我提到的幾種選擇之外還存在哪些方法。  

### 尚未提到的統計模型  

統計學是一個巨大的領域。我在這本書中描述的核心工具(卡方檢驗、t檢驗、回歸和方差分析)是廣泛使用的基本數據分析工具,它們構成了大多數入門統計書的核心。然而,還有很多其他工具。有如此多的數據分析情境這些工具無法涵蓋,給您一種感覺,這其中還有很多值得了解的,例如:  

- **非線性回歸**。在 @sec-Correlation-and-linear-regression 中討論回歸時,我們看到回歸假定預測變量與結果變量之間的關係是線性的。另一方面,當我們在 @sec-Descriptive-statistics 中討論了更簡單的相關性問題時,我們看到確實存在能夠評估變量之間非線性關係的工具(例如斯皮爾曼相關)。統計中有許多工具可以用於進行非線性回歸。例如,一些非線性回歸模型假定預測變量與結果變量之間的關係是單調的(例如等分回歸),而其他則假定它是平滑但不一定是單調的(例如 Lowess 回歸)另一些則假定關係的形式是已知的非線性形式(例如多項式回歸)。  

- **邏輯回歸**。當結果變量是二元的但預測變量是連續的時,回歸的另一種變體。例如,假設您正在研究社交媒體,並希望了解是否可以根據收入、年齡等變量預測某人是否在 Twitter 上。這基本上是一種回歸模型,但您無法使用常規的線性回歸,因為結果變量是二元的(您是否在 Twitter 上)。由於結果變量是二元的,殘差不可能正常分佈。統計學家可以將這種情況應用許多工具,其中最突出的是邏輯回歸。  

- **廣義線性模型(GLM)**:GLM 實際上是包含邏輯回歸、線性回歸、(某些)非線性回歸、方差分析和許多其他模型的模型家族。 GLM 中的基本思想與支撐線性模型的思想基本相同,但是它允許您的數據可能不是正常分佈的,並允許預測變量與結果變量之間的非線性關係。有很多非常方便的分析屬於 GLM,所以了解它非常有用。  

- **存活分析**。在 @sec-A-brief-introduction-to-research-design 中,我談到了“差異減員”,即人們以非隨機方式退出研究的趨勢。當時,我是將其作為一種潛在的方法論問題而談論的,但在很多情況下,差異減員實際上就是您感興趣的事情。例如,假設您有興趣了解人們在一次遊戲中玩不同類型的遊戲的時間有多長。人們會否傾向於連續玩實時戰略遊戲的時間長於第一人稱射擊遊戲?您可能會這樣設計您的研究。人們進入實驗室,他們可以玩盡可能長或短的時間。 一旦他們玩完了,您就記錄他們玩的時間。 然而,由于伦理限制,假設您不能讓他們玩超過兩小時。 很多人在兩小時限制之前就會停止遊戲,所以您會正確知道他們玩了多久。 但有些人會遇到兩小時的限制,所以如果允許研究繼續進行,您不知道他們會玩多久。 因此,您的數據會受到系統性刪減:您遺漏了所有非常長的時間。 您如何明智地分析這些數據? 這就是存活分析要解決的問題。 它是專門設計來處理這種情況的,當研究結束時,您會系統性地遺漏某些“一邊”的數據。 它在健康研究中應用非常廣泛,並且在這種情況下,通常被字面意義上用於分析存活。例如,您可能正在跟踪某種癌症的病人,其中一些人接受了治療 A,其他人接受了治療 B,但您只有資金跟踪他們 5 年。 在研究結束時,一些人還活著,其他人已經死了。 在這種情況下,存活分析可用於確定治療的有效性,並告知您他們隨時間面臨的死亡風險。  

- **混合模型**:重複測量方差分析通常用於觀察值聚集在實驗單位中的情況。一個很好的例子是當您在多個時間點跟蹤個人時。假設您正在跟踪兩個人的快樂情緒隨時間的變化。亞倫的快樂指數最初為 10 分,然後下降到 8 分,然後下降到 6 分。貝琳達的快樂指數最初為 6 分,然後上升到 8 分,然后上升到 10 分。這兩個人的整體快樂水平相同(三個時間點的平均值均為 8 分),因此重複測量方差分析會以相同方式對待亞倫和貝琳達。但這明顯是錯誤的。亞倫的快樂正在降低,而貝琳達的快樂正在增加。 如果您想要優化分析實驗數據,其中人們可以隨時間改變,那麼您需要比重複測量方差分析更強大的工具。 人們用來解決這個問題的工具稱為“混合”模型,因為它們旨在了解個體實驗單位的信息(例如個人的快樂隨時間變化),以及總體影響(例如金錢對快樂隨時間推移的影響)。  重複測量方差分析也許是最簡單的混合模型例子,但使用混合模型可以做很多重複測量方差分析無法做到的事情。  

- **多維尺度**。因子分析是“無監督學習”模型的一個例子。這意味著,與我提到的大多數“有監督學習”工具不同,您無法將變量分為預測變量和結果變量。回歸是有監督學習,而因子分析是無監督學習。這並不是唯一一種無監督學習模型。例如,在因子分析中,研究人員關心變量之間的相關分析。然而,在很多情況下,您實際上有興趣分析對象、項目或人之間的相似性或差異。在這種情況下,您可以使用多種工具,最知名的就是多維尺度法(MDS)。 在 MDS 中,思路是為您的項目找到一種“幾何”表示。每個項目被“繪製”為某個空間中的一點,兩點之間的距離是這兩個項目差異的一種測量。  

- **聚類**:無監督學習模型的另一個例子是聚類(也稱為分類),在這種情況下,您希望將所有項目組織成有意義的組,使得相似的項目被分配到同一組中。大量聚類屬於無監督類型,這意味著您不知道組是什麼,您只能猜測。還有其他“有監督聚類”情況,您需要基於其他變量預測組成員資格,並且這些組成員資格實際上是可觀察到的。邏輯回歸是以這種方式工作的工具的一個很好的例子。然而,當您實際上不知道組成員資格時,您必須使用不同的工具(例如 k 均值聚類)。甚至還有一些情況下您想要執行所謂的“半監督聚類”,在這種情況下,您知道某些項目的組成員資格但並非所有。您可以想像,聚類是一個相當大的主題,也是一項相當有用的技能。  

當然,即使這個列表也不完整。我沒有提到時間序列分析、項目反應理論、市場籃分析、分類與回歸樹或其他大量主題。然而,我上面給出的列表基本上是我對這本書的期望清單。當然,它會使書的長度增加一倍,但這意味著範圍已經廣泛到足以涵蓋心理學應用研究者需要使用的大多數內容。  

### 其他統計推論方法

本書的另一個不完整之處在於它比較嚴重地專注於如何進行推論統計的一種非常狹隘且過時的觀點。在 @sec-Estimating-unknown-quantities-from-a-sample 中,我稍微談到了無偏估計、抽樣分布等概念。 在 @sec-Hypothesis-testing 中,我更詳細地談到了空假設顯著性檢驗和 p 值的理論。這些想法可以追溯到 20 世紀初,而我在書中談到的工具高度依賴那個時代的理論思想。我覺得有義務堅持這些主題,因為科學中絕大多數數據分析也依賴這些思想。然而,統計理論的研究並不仅限於這些主題,儘管由於其實際重要性,每個人都應該了解它們,但在許多方面,這些思想並不能代表當代數據分析的最佳實踐。我特別高興的是,我已經能夠有所超越。 @sec-Bayesian-statistics 現在以合理的細節呈現了貝氏觀點,但整體而言,書中仍然相當偏向次數主義正統思想。此外,還有許多其他值得一提的推論方法:  

- **自助法**:在介紹每個假設檢驗時,我都有強烈傾向於簡單地做出類似“BLAH 的抽樣分布是 t 分布”之類的斷言。在某些情況下,我實際上嘗試證明這一斷言。例如,在 @sec-Categorical-data-analysis 中談到 $chi^2$檢驗時,我引用了正態分布與 $chi^2$ 分布之間的已知關係(參見 @sec-Introduction-to-probability ),以解釋我們如何最終假設適配度統計量的抽樣分布是 $chi^2$。 然而,也有很多這些抽樣分布是,嗯,錯誤的。 $chi^2$ 檢驗就是一個很好的例子。它基於對您的数据的分布的假設,而該假設被知道在小樣本量下是錯誤的! 在 20 世紀初,面對這種情況您幾乎無能為力。統計學家得出了數學結果,即“在有關數據的 BLAH 假設下,抽樣分布大致為 BLAH”,這已經是最好的了。在很多情况下,他們甚至沒有這個。有很多數據分析情況沒有人找到所需的抽樣分布的數學解。 所以,直到 20 世紀后期,相應的檢驗要么不存在要么無法工作。 然而,計算機現在已經改變了這一切。 您現在可以使用各種高端技巧和一些不那么高端的技巧來解決這個問題。 最簡單的方法是自助法,最簡單的形式非常簡單。 您要做的就是模擬實驗結果很多很多次,同時假定 (a) 空假設為真和 (b) 未知的人群分布實際上看起來非常類似於您的原始數據。 換句話說,與其假設數據(例如)是正態分布的,不如假设数据实际上和您的样本一样,然后使用计算机模拟检验统计量的抽样分布,如果该假设成立的话。尽管依赖一些有点可疑的假设(即人群分布与样本相同!),但自助法是一个在大量数据分析问题上实际应用中奇迹般有效的快速简单的方法。  

- **交叉驗證**:在我的統計課上偶爾會出現一個問題,通常是學生試圖挑釁時提出的,那就是“我們為什么要關心推論統計?為什么不只描述樣本呢?” 這個問題的答案通常如下:“因為作為科學家,我們的真正興趣不在於我們_過去_已經觀察到的特定樣本,我們想要對未來可能觀察到的數據做出預測”。 統計推論中很多問題的產生都是因為我們總是認為未來會與過去類似但有些不同。 或者,更一般地說,新數據不會和舊數據完全相同。 在很多情況下,我們嘗試推導出數學規則,這些規則可以幫助我們得出對於新數據最有可能正確的推論,而不是選擇最能描述舊數據的語句。例如,給定模型 A 和 B 以及今天您收集的數據集 X,嘗試選擇明天您要收集的新數據集 Y 的最佳模型。有時模擬這個過程很方便,這就是交叉驗證要做的事情。 您要做的就是將數據集劃分為兩個子集 X1 和 X2。 使用子集 X1 訓練模型(例如,估計回歸系數),但 then 在另一個子集 X2 上評估模型性能。 這為您提供了模型從舊數據延伸到新數據的能力的一種測量,這通常是比只使用完整數據集 X 擬合模型所得到的模型好壞測量更好的測量。  

- **穩健統計**:生活很複雜,沒有什么能按應有的方式運作。對於統計來說也是如此,在嘗試分析數據時,我們經常會遇到各種問題,數據的混亂程度比應該的要高。 變量本應正常分布但實際上並非正常分布,關係本應線性但實際上並非線性,您的數據集中的一些觀察結果幾乎肯定是错误的(即,并未測量應測量的內容)。 在本書的大部分統計理論中都忽略了所有这种混乱。 然而,忽略问题并不总是能解决问题。有時,忽略混亂确实可以,因为某些类型的统计工具是“穩健的”,即使数据并不满足您的理论假设,它们仍能很好地工作。 其他类型的统计工具则不是稳健的,即使偏离理论假设很小也会导致它们失效。 稳健统计是统计学的一个分支,它研究这个问题,并谈论统计量的“破坏点”。 也就是说,您的数据必须混亂到什么程度统计量才不再可信?我在一些地方提到了这一点。均值不是變量中心趨勢的稳健估计量,但中位數是稳健的。例如,假设我告訴您我五个最好朋友的年齡分別為 34、39、31、43 和 4003 歲。您認為他們的平均年齡是多少?也就是說,這里的真實人群均值是多少?如果您使用樣本均值作為人群均值的估計量,那麼您得到的答案是 830 歲。 如果您使用樣本中位數作為人群均值的估計量,那麼您得到的答案是 39 歲。 請注意,即使在第二種情況下您在“技術上”做錯了事(使用中位數估計均值!),但您實際獲得了更好的答案。這里的问题是其中一個觀察結果顯然、明顯地是錯的。 我沒有一個 4003 歲的朋友。這可能是打字錯誤,我可能是想打 43。 但是如果我敲錯了,敲成了 53 而不是 43 呢? 您能肯定這是錯字還是不是? 有時數據中的錯誤很隱蔽,所以您無法通過目測樣本來檢測它們,但它們仍然會污染您的数据,並且仍然會影響您的結論。 稳健统计关注的是,即使面临您不知道的污染,您如何能够进行安全推論。 這是相當酷的東西。  

### 難以歸類的重要雜項

- 假設您正在進行一項調查,並對運動和體重感興趣。 您向四個人發送了數據。 亞當說他經常運動,並且沒有超重。 布莉歐妮說她經常運動,並且沒有超重。 卡羅爾說她不運動,並超重。 蒂姆說他不運動,並拒絕回答他的體重問題。 艾琳沒有返回調查。 您現在有一個遺失數據的問題。有一整個調查遺失了,另一個遺失了一個問題,您對此該怎么辦? 忽略缺失數據通常不是一種安全的事情。 讓我們考慮蒂姆的調查。 首先,請注意,根據他的其他回答,他似乎與卡羅爾(我們都不運動)更相似,而不是與亞當或布莉歐妮更相似。 所以,如果您被迫猜測他的體重,您會猜測他比他們更接近她。 也許您會做一些校正,考慮到亞當和蒂姆是男性,而布莉歐妮和卡羅爾是女性。 這種類型猜測的統計名稱是“插補”。 安全地進行插補是很困難的,但它很重要,特別是當遺失的数据以有系統的方式遺失時。 由于社會壓力迫使超重的人常感到自己的體重很差(通常是由于公共衛生運動的緣故),所以我們實際上有理由懷疑不回應的人比回應調查的人更有可能超重。 給蒂姆插補體重意味着如果我們忽略蒂姆,樣本中超重人數將從 3 分之 1 增加到 4 分之 2(如果我們給蒂姆插補體重)。 顯然,這很重要。但是明智地這樣做比它聽起來更複雜。 早些時候,我建議您应将蒂姆视为卡羅爾,因为他們對運動問題給出了相同的答案。 但這并不完全正确。 他們之间存在有系統的差异。 她回答了問題,蒂姆沒有。 鑑于超重人群面临的社会壓力,蒂姆不是_比_卡羅爾更超重嗎? 當然,這仍然忽略了一個事實,即將_单个_體重插補給蒂姆是不明智的,就好像您實際上知道他的體重一樣。 相反,您需要做的是插補一系列合理的猜測(稱為多重插補),以捕捉您對蒂姆體重的不確定性大於對卡羅爾體重的不確定性这一事实。 我們還沒有開始討論艾琳沒有發回調查所帶來的問題。 您可以想像,處理遺失數據日益成為一個重要話題。 事實上,有人告訴我,如果未遵循某種明智的多重插補計劃,某些領域的很多期刊將不會接受具有遺失數據的研究。  

- **考驗力分析**:在@sec-Hypothesis-testing中,我討論了考驗力的概念(即,如果效應實際存在,您有多大可能性能檢測到它)並提到了功效分析,這是一組用於評估您的研究功效的有用工具。功效分析在規劃研究(例如,確定您可能需要的樣本量有多大)很有用,但在分析您已經收集的數據時也起著有用的作用。例如,假設您得到了顯著結果,並且有效應量的估計。您可以使用這些信息來估計您的研究實際具有多大的功效。這還是有點有用的,特別是如果您的效應量不大。例如,假設您在 $p< .05$ 水平上拒絕了空假設,但您使用功效分析得出您的估計功效只有 .08。顯著結果意味著,如果空假設實際上為真,得到這樣的数据的機率為 5%。 但是低功效意味著,即使空假設為假,效應量實際上和它看起來一樣小,得到您得到的數據的機率也只有 8%。這表明您需要相當謹慎,因為運氣似乎在您的結果中起了很大作用,不管是哪一種方式!  

- **使用理論驅動模型進行數據分析**。在本書的一些地方,我提到了反應時間(RT)數據,您記錄某人完成某件事所需的時間(例如,做出簡單決定)。我提到 RT 數據幾乎總是非正態的,並且正偏。此外,還有所謂的速度/精度權衡:如果您嘗試過快做出決定(RT 較低),那麼您可能會做出較差的決定(精度較低)。因此,如果您測量了參與者決策的精度和他們的 RT,您可能會發現速度和精度之間存在關係。當然還有更多內容,因為與反應速度無關,一些人的決策優於其他人。此外,速度取決于認知過程(即思考花費的時間)和生理過程(例如,您能夠移動肌肉的速度有多快)。 分析這些數據的過程聽起來會很複雜。的確,當您深入研究心理學文獻時,您會發現已經存在數學模型(稱為“序列採樣模型”),這些模型描述了人們如何做出簡單的決定,並且這些模型考慮到了我上面提到的許多因素。 您在標準統計教科書中找不到任何這些理論驅動的模型。 標准統計教科書描述了標准工具,這些工具可以有意義地應用于許多不同的學科,不僅僅是心理學。 方差分析就是一個標准工具的例子,它適用于心理學和藥理學。 順序採樣模型不是,它們或多或少是專門針對心理學的。 這並没有使它們成為更加強大的工具。 事實上,如果您正在分析人們必須快速做出選擇的數據,您應該真正使用序列採樣模型來分析數據。 使用方差分析或回歸或任何其他工具的效果都不會那麼好,因為支撐它們的理論假設与您的数据不太匹配。 相反,序列採樣模型是明確設計來分析這種特定類型數據的,它們的理論假設與數據非常吻合。  

## 運用 jamovi 中學習基礎知識

好吧,這是一個很長的列表。 即使只是列出一些內容也是大大不完整的。 統計學中真的有很多我在這本書中没有涵蓋的大思想。 當您讀完一本幾乎 500 頁的教科書后被告知這只是開始,這可能看起來相當沮喪,特別是當您開始懷疑所學的一半東西都是錯的時。例如,有很多人會強烈主張不要使用經典的方差分析模型,然而我卻花了整整兩章的篇幅討論它! 標準方差分析可以從貝葉斯角度、穩健統計角度甚至“它就是錯誤”的角度來攻擊。我花了如此多時間討論機率論基礎。我更詳細地討論了估計和假設檢驗的理論,而不僅僅是我需要的程度。我為什麼要這麼做呢?回頭看,你可能會問,我是否真的需要花那麼多時間談論機率分布是什麼,甚至為什麼會有機率密度部分。如果這本書的目標是教你如何運行 t 檢驗或方差分析,這真的有必要嗎?這難道不是對每一個人時間的巨大浪費嗎???  

我希望您會同意答案是否定的。入門統計的目標不是教授方差分析。它的目標不是教授 t 檢驗、迴歸、直方圖或 p 值。目標是讓您走上成為熟練數據分析師的道路。為了使您成為熟練的數據分析師,您不僅需要掌握方差分析、t 檢驗、迴歸和直方圖等基礎知識。您需要正確地思考數據。您需要能夠學習我在上一節中談到的更高級的統計模型,並理解它們所基於的理論。您需要能夠使用讓您能使用這些高級工具的軟體。在我看來,這就是我在基礎知識上花費額外時間的回報。如果您理解機率論,從次數主義分析切換到貝氏分析對您來說會容易得多。  


簡而言之,我認為透過這種方式學習統計的最大收益在於可擴展性。對於一本只涵蓋數據分析的基礎知識來說,這本書在學習概率論等方面花費了大量精力。這本書強迫您學習的內容遠不止所涵蓋的特定分析。因此,如果您的目標是以最短的時間學習如何執行方差分析,那麼這本書並不是一個好選擇。然而正如我所說,我不認為這是您的目標。我認為您想要學習如何進行數據分析。如果這真的是您的目標,您會想要確保您在入門統計課上學到的技能能夠自然、順暢地擴展到更複雜的實際世界數據分析中所需的模型。您要確保您學習使用真正的數據分析師使用的相同工具,以便您可以學會他們在做什麼。所以,是的,您現在是一個初學者(或者當您開始這本書時是初學者),但是這並不意味著您應該得到一個簡單化的故事,一個我不告訴您概率密度是什麼的故事,或者一個不告訴您失衡設計的主成分方差分析有多麽可怕的故事。且並不意味著您應該得到玩具而不是適當的數據分析工具。初學者并不愚笨,他們只是缺乏知識。您需要的不是從真實世界的複雜數據分析中隱藏複雜性。您需要的是讓您能夠在當它們在現實世界中不可避免地突襲您時處理這些複雜性的技能和工具。


我希望這本書,或者這本書最終會轉變成的完整版本,能夠在這方面提供幫助。

作者註:我以前提過,但我將快速再次提及。這本書的參考文獻列表非常不完整。請不要認為這些是我所依據的唯一來源。最終版本的這本書將包含更多引用。如果您在這本書中看到任何看起來很聰明的內容似乎沒有引用,我絕對保證這些想法是其他人的。這是一本入門教材:沒有一個想法是原創的。我將為所有的錯誤負責,但我不能因為任何好的東西而得到讚賞。這本書中所有聰明的地方都來自于其他人,他們都應該得到適當的歸屬以表彰他們優秀的工作。我只是還沒有機會這樣做。


<!---

> *"Begin at the beginning", the King said, very gravely, "and go on till you come to the end: then stop"* -- Lewis Carroll

It feels somewhat strange to be writing this chapter, and more than a little inappropriate. An epilogue is what you write when a book is finished, and this book really isn't finished. There are a lot of things still missing from this book. It doesn't have an index yet. A *lot* of references are missing. There are no "do it yourself" exercises. And in general, I feel that there a lot of things that are wrong with the presentation, organisation and content of this book. Given all that, I don't want to try to write a "proper" epilogue. I haven't finished writing the substantive content yet, so it doesn't make sense to try to bring it all together. But this version of the book is going to go online for students to use, and you will may be to purchase a hard copy too, so I want to give it at least a veneer of closure. So let's give it a go, shall we?

## The undiscovered statistics

First, I'm going to talk a bit about some of the content that I wish I'd had the chance to cram into this version of the book, just so that you can get a sense of what other ideas are out there in the world of statistics. I think this would be important even if this book were getting close to a final product. One thing that students often fail to realise is that their introductory statistics classes are just that, an introduction. If you want to go out into the wider world and do real data analysis, you have to learn a whole lot of new tools that extend the content of your undergraduate lectures in all sorts of different ways. Don't assume that something can't be done just because it wasn't covered in undergrad. Don't assume that something is the right thing to do just because it was covered in an undergrad class. To stop you from falling victim to that trap, I think it's useful to give a bit of an overview of some of the other ideas out there

### Omissions within the topics covered

Even within the topics that I have covered in the book, there are a lot of omissions that I'd like to redress in future version of the book. Just sticking to things that are purely about statistics (rather than things associated with jamovi), the following is a representative but not exhaustive list of topics that I'd like to expand on at some time:

-   **Other types of correlations.** In @sec-Correlation-and-linear-regression I talked about two types of correlation: Pearson and Spearman. Both of these methods of assessing correlation are applicable to the case where you have two continuous variables and want to assess the relationship between them. What about the case where your variables are both nominal scale? Or when one is nominal scale and the other is continuous? There are actually methods for computing correlations in such cases (e.g., polychoric correlation), and it would be good to see these included.

-   **More detail on effect sizes.** In general, I think the treatment of effect sizes throughout the book is a little more cursory than it should be. In almost every instance, I've tended just to pick one measure of effect size (usually the most popular one) and describe that. However, for almost all tests and models there are multiple ways of thinking about effect size, and I'd like to go into more detail in the future.

-   **Dealing with violated assumptions.** In a number of places in the book I've talked about some things you can do when you find that the assumptions of your test (or model) are violated, but I think that I ought to say more about this. In particular, I think it would have been nice to talk in a lot more detail about how you can tranform variables to fix problems. I talked a bit about this in @sec-Pragmatic-matters, but the discussion isn't detailed enough I think.

-   **Interaction terms for regression.** In @sec-Factorial-ANOVA I talked about the fact that you can have interaction terms in an ANOVA, and I also pointed out that ANOVA can be interpreted as a kind of linear regression model. Yet, when talking about regression in @sec-Correlation-and-linear-regression I made no mention of interactions at all. However, there's nothing stopping you from including interaction terms in a regression model. It's just a little more complicated to figure out what an "interaction" actually means when you're talking about the interaction between two continuous predictors, and it can be done in more than one way. Even so, I would have liked to talk a little about this.

-   **Method of planned comparison.** As I mentioned this in @sec-Factorial-ANOVA, it's not always appropriate to be using a post hoc correction like Tukey's HSD when doing an ANOVA, especially when you had a very clear (and limited) set of comparisons that you cared about ahead of time. I would like to talk more about this in the future.

-   **Multiple comparison methods.** Even within the context of talking about post hoc tests and multiple comparisons, I would have liked to talk about the methods in more detail, and talk about what other methods exist besides the few options I mentioned.

### Statistical models missing from the book

Statistics is a huge field. The core tools that I've described in this book (chi-square tests, t-tests, regression and ANOVA) are basic tools that are widely used in everyday data analysis, and they form the core of most introductory stats books. However, there are a lot of other tools out there. There are so very many data analysis situations that these tools don't cover, and it would be great to give you a sense of just how much more there is, for example:

-   **Nonlinear regression.** When discussing regression in @sec-Correlation-and-linear-regression, we saw that regression assumes that the relationship between predictors and outcomes is linear. On the other hand, when we talked about the simpler problem of correlation in @sec-Descriptive-statistics, we saw that there exist tools (e.g., Spearman correlations) that are able to assess non-linear relationships between variables. There are a number of tools in statistics that can be used to do non-linear regression. For instance, some non-linear regression models assume that the relationship between predictors and outcomes is monotonic (e.g., isotonic regression), while others assume that it is smooth but not necessarily monotonic (e.g., Lowess regression), while others assume that the relationship is of a known form that happens to be nonlinear (e.g., polynomial regression).

-   **Logistic regression.** Yet another variation on regression occurs when the outcome variable is binary, but the predictors are continuous. For instance, suppose you're investigating social media, and you want to know if it's possible to predict whether or not someone is on Twitter as a function of their income, their age, and a range of other variables. This is basically a regression model, but you can't use regular linear regression because the outcome variable is binary (you're either on Twitter or you're not). Because the outcome variable is binary, there's no way that the residuals could possibly be normally distributed. There are a number of tools that statisticians can apply to this situation, the most prominent of which is logistic regression.

-   **The General Linear Model (GLM).** The GLM is actually a family of models that includes logistic regression, linear regression, (some) nonlinear regression, ANOVA and many others. The basic idea in the GLM is essentially the same idea that underpins linear models, but it allows for the idea that your data might not be normally distributed, and allows for nonlinear relationships between predictors and outcomes. There are a lot of very handy analyses that you can run that fall within the GLM, so it's a very useful thing to know about.

-   **Survival analysis.** In @sec-A-brief-introduction-to-research-design I talked about "differential attrition", the tendency for people to leave the study in a non-random fashion. Back then, I was talking about it as a potential methodological concern, but there are a lot of situations in which differential attrition is actually the thing you're interested in. Suppose, for instance, you're interested in finding out how long people play different kinds of computer games in a single session. Do people tend to play RTS (real time strategy) games for longer stretches than FPS (first person shooter) games? You might design your study like this. People come into the lab, and they can play for as long or as little as they like. Once they're finished, you record the time they spent playing. However, due to ethical restrictions, let's suppose that you cannot allow them to keep playing longer than two hours. A lot of people will stop playing before the two hour limit, so you know exactly how long they played. But some people will run into the two hour limit, and so you don't know how long they would have kept playing if you'd been able to continue the study. As a consequence, your data are systematically censored: you're missing all of the very long times. How do you analyse this data sensibly? This is the problem that survival analysis solves. It is specifically designed to handle this situation, where you're systematically missing one "side" of the data because the study ended. It's very widely used in health research, and in that context it is often literally used to analyse survival. For instance, you may be tracking people with a particular type of cancer, some who have received treatment A and others who have received treatment B, but you only have funding to track them for 5 years. At the end of the study period some people are alive, others are not. In this context, survival analysis is useful for determining which treatment is more effective, and telling you about the risk of death that people face over time.

-   **Mixed models.** Repeated measures ANOVA is often used in situations where you have observations clustered within experimental units. A good example of this is when you track individual people across multiple time points. Let's say you're tracking happiness over time, for two people. Aaron's happiness starts at 10, then drops to 8, and then to 6. Belinda's happiness starts at 6, then rises to 8 and then to 10. Both of these two people have the same "overall" level of happiness (the average across the three time points is 8), so a repeated measures ANOVA analysis would treat Aaron and Belinda the same way. But that's clearly wrong. Aaron's happiness is decreasing, whereas Belinda's is increasing. If you want to optimally analyse data from an experiment where people can change over time, then you need a more powerful tool than repeated measures ANOVA. The tools that people use to solve this problem are called "mixed" models, because they are designed to learn about individual experimental units (e.g. happiness of individual people over time) as well as overall effects (e.g. the effect of money on happiness over time). Repeated measures ANOVA is perhaps the simplest example of a mixed model, but there's a lot you can do with mixed models that you can't do with repeated measures ANOVA.

-   **Multidimensional scaling.** Factor analysis is an example of an "unsupervised learning" model. What this means is that, unlike most of the "supervised learning" tools I've mentioned, you can't divide up your variables into predictors and outcomes. Regression is supervised learning whereas factor analysis is unsupervised learning. It's not the only type of unsupervised learning model however. For example, in factor analysis one is concerned with the analysis of correlations between variables. However, there are many situations where you're actually interested in analysing similarities or dissimilarities between objects, items or people. There are a number of tools that you can use in this situation, the best known of which is multidimensional scaling (MDS). In MDS, the idea is to find a "geometric" representation of your items. Each item is "plotted" as a point in some space, and the distance between two points is a measure of how dissimilar those items are.

-   **Clustering.** Another example of an unsupervised learning model is clustering (also referred to as classification), in which you want to organise all of your items into meaningful groups, such that similar items are assigned to the same groups. A lot of clustering is unsupervised, meaning that you don't know anything about what the groups are, you just have to guess. There are other "supervised clustering" situations where you need to predict group memberships on the basis of other variables, and those group memberships are actually observables. Logistic regression is a good example of a tool that works this way. However, when you don't actually know the group memberships, you have to use different tools (e.g., k-means clustering). There are even situations where you want to do something called "semi-supervised clustering", in which you know the group memberships for some items but not others. As you can probably guess, clustering is a pretty big topic, and a pretty useful thing to know about.

-   **Causal models.** One thing that I haven't talked about much in this book is how you can use statistical modelling to learn about the causal relationships between variables. For instance, consider the following three variables which might be of interest when thinking about how someone died in a firing squad. We might want to measure whether or not an execution order was given (variable A), whether or not a marksman fired their gun (variable B), and whether or not the person got hit with a bullet (variable C). These three variables are all correlated with one another (e.g., there is a correlation between guns being fired and people getting hit with bullets), but we actually want to make stronger statements about them than merely talking about correlations. We want to talk about causation. We want to be able to say that the execution order (A) causes the marksman to fire (B) which causes someone to get shot (C). We can express this by a directed arrow notation: we write it as $A rightarrow B rightarrow C$. This "causal chain" is a fundamentally different explanation for events than one in which the marksman fires first, which causes the shooting $B rightarrow C$, and then causes the executioner to "retroactively" issue the execution order, $B rightarrow A$. This "common effect" model says that A and C are both caused by B. You can see why these are different. In the first causal model, if we had managed to stop the executioner from issuing the order (intervening to change A), then no shooting would have happened. In the second model, the shooting would have happened any way because the marksman was not following the execution order. There is a big literature in statistics on trying to understand the causal relationships between variables, and a number of different tools exist to help you test different causal stories about your data. The most widely used of these tools (in psychology at least) is structural equations modelling (SEM), and at some point I'd like to extend the book to talk about it.

Of course, even this listing is incomplete. I haven't mentioned time series analysis, item response theory, market basket analysis, classification and regression trees, or any of a huge range of other topics. However, the list that I've given above is essentially my wish list for this book. Sure, it would double the length of the book, but it would mean that the scope has become broad enough to cover most things that applied researchers in psychology would need to use.

### Other ways of doing inference

A different sense in which this book is incomplete is that it focuses pretty heavily on a very narrow and old-fashioned view of how inferential statistics should be done. In @sec-Estimating-unknown-quantities-from-a-sample I talked a little bit about the idea of unbiased estimators, sampling distributions and so on. In @sec-Hypothesis-testing I talked about the theory of null hypothesis significance testing and p-values. These ideas have been around since the early 20th century, and the tools that I've talked about in the book rely very heavily on the theoretical ideas from that time. I've felt obligated to stick to those topics because the vast majority of data analysis in science is also reliant on those ideas. However, the theory of statistics is not restricted to those topics and, whilst everyone should know about them because of their practical importance, in many respects those ideas do not represent best practice for contemporary data analysis. One of the things that I'm especially happy with is that I've been able to go a little beyond this. @sec-Bayesian-statistics now presents the Bayesian perspective in a reasonable amount of detail, but the book overall is still pretty heavily weighted towards the frequentist orthodoxy. Additionally, there are a number of other approaches to inference that are worth mentioning:

-   Bootstrapping. Throughout the book, whenever I've introduced a hypothesis test, I've had a strong tendency just to make assertions like "the sampling distribution for BLAH is a t-distribution" or something like that. In some cases, I've actually attempted to justify this assertion. For example, when talking about $chi^2$ tests in @sec-Categorical-data-analysis I made reference to the known relationship between normal distributions and $chi^2$ distributions (see @sec-Introduction-to-probability) to explain how we end up assuming that the sampling distribution of the goodness-of-fit statistic is $chi^2$ . However, it's also the case that a lot of these sampling distributions are, well, wrong. The $chi^2$ test is a good example. It is based on an assumption about the distribution of your data, an assumption which is known to be wrong for small sample sizes! Back in the early 20th century, there wasn't much you could do about this situation. Statisticians had developed mathematical results that said that "under assumptions BLAH about the data, the sampling distribution is approximately BLAH", and that was about the best you could do. A lot of times they didn't even have that. There are lots of data analysis situations for which no-one has found a mathematical solution for the sampling distributions that you need. And so up until the late 20th century, the corresponding tests didn't exist or didn't work. However, computers have changed all that now. There are lots of fancy tricks, and some not-so-fancy, that you can use to get around it. The simplest of these is bootstrapping, and in it's simplest form it's incredibly simple. What you do is simulate the results of your experiment lots and lots of times, under the twin assumptions that (a) the null hypothesis is true and (b) the unknown population distribution actually looks pretty similar to your raw data. In other words, instead of assuming that the data are (for instance) normally distributed, just assume that the population looks the same as your sample, and then use computers to simulate the sampling distribution for your test statistic if that assumption holds. Despite relying on a somewhat dubious assumption (i.e., the population distribution is the same as the sample!) bootstrapping is quick and easy method that works remarkably well in practice for lots of data analysis problems.

-   Cross validation. One question that pops up in my stats classes every now and then, usually by a student trying to be provocative, is "Why do we care about inferential statistics at all? Why not just describe your sample?" The answer to the question is usually something like this, "Because our true interest as scientists is not the specific sample that we have observed in the <u>past</u>, we want to make predictions about data we might observe in the future". A lot of the issues in statistical inference arise because of the fact that we always expect the future to be similar to but a bit different from the past. Or, more generally, new data won't be quite the same as old data. What we do, in a lot of situations, is try to derive mathematical rules that help us to draw the inferences that are most likely to be correct for new data, rather than to pick the statements that best describe old data. For instance, given two models A and B, and a data set $X$ you collected today, try to pick the model that will best describe a new data set $Y$ that you're going to collect tomorrow. Sometimes it's convenient to simulate the process, and that's what cross-validation does. What you do is divide your data set into two subsets, $X1$ and $X2$. Use the subset $X1$ to train the model (e.g., estimate regression coefficients, let's say), but then assess the model performance on the other one $X2$. This gives you a measure of how well the model generalises from an old data set to a new one, and is often a better measure of how good your model is than if you just fit it to the full data set $X$.

-   Robust statistics. Life is messy, and nothing really works the way it's supposed to. This is just as true for statistics as it is for anything else, and when trying to analyse data we're often stuck with all sorts of problems in which the data are just messier than they're supposed to be. Variables that are supposed to be normally distributed are not actually normally distributed, relationships that are supposed to be linear are not actually linear, and some of the observations in your data set are almost certainly junk (i.e., not measuring what they're supposed to). All of this messiness is ignored in most of the statistical theory I developed in this book. However, ignoring a problem doesn't always solve it. Sometimes, it's actually okay to ignore the mess, because some types of statistical tools are "robust", i.e., if the data don't satisfy your theoretical assumptions they nevertheless still work pretty well. Other types of statistical tools are not robust, and even minor deviations from the theoretical assumptions cause them to break. Robust statistics is a branch of stats concerned with this question, and they talk about things like the "breakdown point" of a statistic. That is, how messy does your data have to be before the statistic cannot be trusted? I touched on this in places. The mean is not a robust estimator of the central tendency of a variable, but the median is. For instance, suppose I told you that the ages of my five best friends are 34, 39, 31, 43 and 4003 years. How old do you think they are on average? That is, what is the true population mean here? If you use the sample mean as your estimator of the population mean, you get an answer of 830 years. If you use the sample median as the estimator of the population mean, you get an answer of 39 years. Notice that, even though you're "technically" doing the wrong thing in the second case (using the median to estimate the mean!) you're actually getting a better answer. The problem here is that one of the observations is clearly, obviously, a lie. I don't have a friend aged 4003 years. It's probably a typo, I probably meant to type 43. But what if I had typed 53 instead of 43, or 34 instead of 43? Could you be sure if this was a typo or not? Sometimes the errors in the data are subtle, so you can't detect them just by eyeballing the sample, but they're still errors that contaminate your data, and they still affect your conclusions. Robust statistics is concerned with how you can make safe inferences even when faced with contamination that you don't know about. It's pretty cool stuff.

### Miscellaneous topics

-   Suppose you're doing a survey, and you're interested in exercise and weight. You send data to four people. Adam says he exercises a lot and is not overweight. Briony says she exercises a lot and is not overweight. Carol says she does not exercise and is overweight. Tim says he does not exercise and refuses to answer the question about his weight. Elaine does not return the survey. You now have a missing data problem. There is one entire survey missing, and one question missing from another one, What do you do about it? Ignoring missing data is not, in general, a safe thing to do. Let's think about Tim's survey here. Firstly, notice that, on the basis of his other responses, he appear to be more similar to Carol (neither of us exercise) than to Adam or Briony. So if you were forced to guess his weight, you'd guess that he is closer to her than to them. Maybe you'd make some correction for the fact that Adam and Tim are males and Briony and Carol are females. The statistical name for this kind of guessing is "imputation". Doing imputation safely is hard, but it's important, especially when the missing data are missing in a systematic way. Because of the fact that people who are overweight are often pressured to feel poorly about their weight (often thanks to public health campaigns), we actually have reason to suspect that the people who are not responding are more likely to be overweight than the people who do respond. Imputing a weight to Tim means that the number of overweight people in the sample will probably rise from 1 out of 3 (if we ignore Tim), to 2 out of 4 (if we impute Tim's weight). Clearly this matters. But doing it sensibly is more complicated than it sounds. Earlier, I suggested you should treat Tim like Carol, since they gave the same answer to the exercise question. But that's not quite right. There is a systematic difference between them. She answered the question, and Tim didn't. Given the social pressures faced by overweight people, isn't it likely that Tim is *more* overweight than Carol? And of course this is still ignoring the fact that it's not sensible to impute a *single* weight to Tim, as if you actually knew his weight. Instead, what you need to do it is impute a range of plausible guesses (referred to as multiple imputation), in order to capture the fact that you're more uncertain about Tim's weight than you are about Carol's. And let's not get started on the problem posed by the fact that Elaine didn't send in the survey. As you can probably guess, dealing with missing data is an increasingly important topic. In fact, I've been told that a lot of journals in some fields will not accept studies that have missing data unless some kind of sensible multiple imputation scheme is followed.

-   Power analysis. In @sec-Hypothesis-testing I discussed the concept of power (i.e., how likely are you to be able to detect an effect if it actually exists) and referred to power analysis, a collection of tools that are useful for assessing how much power your study has. Power analysis can be useful for planning a study (e.g., figuring out how large a sample you're likely to need), but it also serves a useful role in analysing data that you already collected. For instance, suppose you get a significant result, and you have an estimate of your effect size. You can use this information to estimate how much power your study actually had. This is kind of useful, especially if your effect size is not large. For instance, suppose you reject the null hypothesis at $p< .05$, but you use power analysis to figure out that your estimated power was only .08. The significant result means that, if the null hypothesis was in fact true, there was a 5% chance of getting data like this. But the low power means that, even if the null hypothesis is false and the effect size was really as small as it looks, there was only an 8% chance of getting data like you did. This suggests that you need to be pretty cautious, because luck seems to have played a big part in your results, one way or the other!

-   Data analysis using theory-inspired models. In a few places in this book I've mentioned response time (RT) data, where you record how long it takes someone to do something (e.g., make a simple decision). I've mentioned that RT data are almost invariably non-normal, and positively skewed. Additionally, there's a thing known as the speed / accuracy trade-off: if you try to make decisions too quickly (low RT) then you're likely to make poorer decisions (lower accuracy). So if you measure both the accuracy of a participant's decisions and their RT, you'll probably find that speed and accuracy are related. There's more to the story than this, of course, because some people make better decisions than others regardless of how fast they're going. Moreover, speed depends on both cognitive processes (i.e., time spent thinking) but also physiological ones (e.g., how fast can you move your muscles). It's starting to sound like analysing this data will be a complicated process. And indeed it is, but one of the things that you find when you dig into the psychological literature is that there already exist mathematical models (called "sequential sampling models") that describe how people make simple decisions, and these models take into account a lot of the factors I mentioned above. You won't find any of these theoretically-inspired models in a standard statistics textbook. Standard stats textbooks describe standard tools, tools that could meaningfully be applied in lots of different disciplines, not just psychology. ANOVA is an example of a standard tool that is just as applicable to psychology as to pharmacology. Sequential sampling models are not, they are psychology-specific, more or less. This doesn't make them less powerful tools. In fact, if you're analysing data where people have to make choices quickly you should really be using sequential sampling models to analyse the data. Using ANOVA or regression or whatever won't work as well, because the theoretical assumptions that underpin them are not well-matched to your data. In contrast, sequential sampling models were explicitly designed to analyse this specific type of data, and their theoretical assumptions are extremely well-matched to the data.

## Learning the basics, and learning them in jamovi

Okay, that was a long list. And even that listing is massively incomplete. There really are a lot of big ideas in statistics that I haven't covered in this book. It can seem pretty depressing to finish an almost 500-page textbook only to be told that this is only the beginning, especially when you start to suspect that half of the stuff you've been taught is wrong. For instance, there are a lot of people in the field who would strongly argue against the use of the classical ANOVA model, yet I've devoted two whole chapters to it! Standard ANOVA can be attacked from a Bayesian perspective, or from a robust statistics perspective, or even from a "it's just plain wrong" perspective (people very frequently use ANOVA when they should actually be using mixed models). So why learn it at all?

As I see it, there are two key arguments. Firstly, there's the pure pragmatism argument. Rightly or wrongly, ANOVA is widely used. If you want to understand the scientific literature, you need to understand ANOVA. And secondly, there's the "incremental knowledge" argument. In the same way that it was handy to have seen one-way ANOVA before trying to learn factorial ANOVA, understanding ANOVA is helpful for understanding more advanced tools, because a lot of those tools extend on or modify the basic ANOVA setup in some way. For instance, although mixed models are way more useful than ANOVA and regression, I've never heard of anyone learning how mixed models work without first having worked through ANOVA and regression. You have to learn to crawl before you can climb a mountain.

Actually, I want to push this point a bit further. One thing that I've done a lot of in this book is talk about fundamentals. I spent a lot of time on probability theory. I talked about the theory of estimation and hypothesis tests in more detail than I needed to. Why did I do all this? Looking back, you might ask whether I really needed to spend all that time talking about what a probability distribution is, or why there was even a section on probability density. If the goal of the book was to teach you how to run a t-test or an ANOVA, was all that really necessary? Was this all just a huge waste of everyone's time???

The answer, I hope you'll agree, is no. The goal of an introductory stats is not to teach ANOVA. It's not to teach t-tests, or regressions, or histograms, or p-values. The goal is to start you on the path towards becoming a skilled data analyst. And in order for you to become a skilled data analyst, you need to be able to do more than ANOVA, more than t-tests, regressions and histograms. You need to be able to think properly about data. You need to be able to learn the more advanced statistical models that I talked about in the last section, and to understand the theory upon which they are based. And you need to have access to software that will let you use those advanced tools. And this is where, in my opinion at least, all that extra time I've spent on the fundamentals pays off. If you understand probability theory, you'll find it much easier to switch from frequentist analyses to Bayesian ones.

In short, I think that the big payoff for learning statistics this way is extensibility. For a book that only covers the very basics of data analysis, this book has a massive overhead in terms of learning probability theory and so on. There's a whole lot of other things that it pushes you to learn besides the specific analyses that the book covers. So if your goal had been to learn how to run an ANOVA in the minimum possible time, well, this book wasn't a good choice. But as I say, I don't think that is your goal. I think you want to learn how to do data analysis. And if that really is your goal, you want to make sure that the skills you learn in your introductory stats class are naturally and cleanly extensible to the more complicated models that you need in real world data analysis. You want to make sure that you learn to use the same tools that real data analysts use, so that you can learn to do what they do. And so yeah, okay, you're a beginner right now (or you were when you started this book), but that doesn't mean you should be given a dumbed-down story, a story in which I don't tell you about probability density, or a story where I don't tell you about the nightmare that is factorial ANOVA with unbalanced designs. And it doesn't mean that you should be given baby toys instead of proper data analysis tools. Beginners aren't dumb, they just lack knowledge. What you need is not to have the complexities of real world data analysis hidden from from you. What you need are the skills and tools that will let you handle those complexities when they inevitably ambush you in the real world.

And what I hope is that this book, or the finished book that this will one day turn into, is able to help you with that.

Author's note -- I've mentioned it before, but I'll quickly mention it again. The book's reference list is appallingly incomplete. Please don't assume that these are the only sources I've relied upon. The final version of this book will have a lot more references. And if you see anything clever sounding in this book that doesn't seem to have a reference, I can absolutely promise you that the idea was someone else's. This is an introductory textbook: none of the ideas are original. I'll take responsibility for all the errors, but I can't take credit for any of the good stuff. Everything smart in this book came from someone else, and they all deserve proper attribution for their excellent work. I just haven't had the chance to give it to them yet.


--->
