<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.427">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>用jamovi上手統計學 - 12&nbsp; 相關與線性迴歸</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./13-Comparing-several-means-one-way-ANOVA.html" rel="next">
<link href="./11-Comparing-two-means.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "沒有結果",
    "search-matching-documents-text": "符合的文件",
    "search-copy-link-title": "複製搜尋連結",
    "search-hide-matches-text": "隱藏其他符合的結果",
    "search-more-match-text": "更多符合結果",
    "search-more-matches-text": "更多符合結果",
    "search-clear-button-title": "清除",
    "search-detached-cancel-button-title": "取消",
    "search-submit-button-title": "送出",
    "search-label": "搜尋"
  }
}</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['S$','S$'], ["\\[","\\]"] ]
      processEscapes: true
    }
  });
</script>
    

<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="用jamovi上手統計學 - 12&nbsp; 相關與線性迴歸">
<meta property="og:description" content="">
<meta property="og:image" content="https://scgeeker.github.io/lsj-book-zh_tw/images/fig12-1.png">
<meta property="og:site-name" content="用jamovi上手統計學">
<meta property="og:image:height" content="224">
<meta property="og:image:width" content="639">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="切換側邊欄導航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./10-Categorical-data-analysis.html">統計方法</a></li><li class="breadcrumb-item"><a href="./12-Correlation-and-linear-regression.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">相關與線性迴歸</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="切換側邊欄導航" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">用jamovi上手統計學</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/SCgeeker/lsj-book-zh_tw/" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=|url|">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=|url|">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
      </ul>
    </div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="切換深色模式"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="切換閱讀器模式">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="搜尋"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">&nbsp;</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Preface.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">前言</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">新手須知</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-Why-do-we-learn-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">為什麼要學習統計</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-A-brief-introduction-to-research-design.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">研究設計入門</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">jamovi初體驗</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-Getting-started-with-jamovi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">與jamovi的第一次接觸</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">資料處理</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-Descriptive-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">描述統計</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-Drawing-graphs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">繪製統計圖</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-Pragmatic-matters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">實務課題</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false">
 <span class="menu-text">統計理論</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="false" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Prelude-Part-IV.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">中場故事</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-Introduction-to-probability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">機率入門</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-Estimating-unknown-quantities-from-a-sample.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">運用樣本估計未知量數</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-Hypothesis-testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">假設檢定</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">統計方法</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-Categorical-data-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">類別資料分析</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-Comparing-two-means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">比較單一與兩組平均值</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-Correlation-and-linear-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">相關與線性迴歸</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-Comparing-several-means-one-way-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">比較多組平均值(單因子變異數分析)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-Factorial-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">多因子變異數分析</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-Factor-Analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">因素分析</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">貝氏統計</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="切換區段">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-Bayesian-statistics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">貝氏統計</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Epilogue.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">後記</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./References.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">參考資料</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目錄</h2>
   
  <ul>
  <li><a href="#sec-what-is-correlation" id="toc-sec-what-is-correlation" class="nav-link active" data-scroll-target="#sec-what-is-correlation"><span class="header-section-number">12.1</span> 相關</a>
  <ul class="collapse">
  <li><a href="#示範資料" id="toc-示範資料" class="nav-link" data-scroll-target="#示範資料"><span class="header-section-number">12.1.1</span> 示範資料</a></li>
  <li><a href="#相關的強度與方向" id="toc-相關的強度與方向" class="nav-link" data-scroll-target="#相關的強度與方向"><span class="header-section-number">12.1.2</span> 相關的強度與方向</a></li>
  <li><a href="#相關係數" id="toc-相關係數" class="nav-link" data-scroll-target="#相關係數"><span class="header-section-number">12.1.3</span> 相關係數</a></li>
  <li><a href="#相關係數計算實務" id="toc-相關係數計算實務" class="nav-link" data-scroll-target="#相關係數計算實務"><span class="header-section-number">12.1.4</span> 相關係數計算實務</a></li>
  <li><a href="#sec-interpreting-a-correlation" id="toc-sec-interpreting-a-correlation" class="nav-link" data-scroll-target="#sec-interpreting-a-correlation"><span class="header-section-number">12.1.5</span> 解讀相關係數</a></li>
  <li><a href="#斯皮爾曼等級相關" id="toc-斯皮爾曼等級相關" class="nav-link" data-scroll-target="#斯皮爾曼等級相關"><span class="header-section-number">12.1.6</span> 斯皮爾曼等級相關</a></li>
  </ul></li>
  <li><a href="#sec-the-scatter-plot" id="toc-sec-the-scatter-plot" class="nav-link" data-scroll-target="#sec-the-scatter-plot"><span class="header-section-number">12.2</span> 散佈圖</a>
  <ul class="collapse">
  <li><a href="#更多解讀散佈圖的方法" id="toc-更多解讀散佈圖的方法" class="nav-link" data-scroll-target="#更多解讀散佈圖的方法"><span class="header-section-number">12.2.1</span> 更多解讀散佈圖的方法</a></li>
  </ul></li>
  <li><a href="#sec-what-is-linear-model" id="toc-sec-what-is-linear-model" class="nav-link" data-scroll-target="#sec-what-is-linear-model"><span class="header-section-number">12.3</span> 認識線性迴歸模型</a></li>
  <li><a href="#sec-the-regression-coefficients-parameters" id="toc-sec-the-regression-coefficients-parameters" class="nav-link" data-scroll-target="#sec-the-regression-coefficients-parameters"><span class="header-section-number">12.4</span> 線性迴歸模型的參數估計</a>
  <ul class="collapse">
  <li><a href="#實作線性迴歸模型" id="toc-實作線性迴歸模型" class="nav-link" data-scroll-target="#實作線性迴歸模型"><span class="header-section-number">12.4.1</span> 實作線性迴歸模型</a></li>
  <li><a href="#解讀線性迴歸模型參數估計" id="toc-解讀線性迴歸模型參數估計" class="nav-link" data-scroll-target="#解讀線性迴歸模型參數估計"><span class="header-section-number">12.4.2</span> 解讀線性迴歸模型參數估計</a></li>
  </ul></li>
  <li><a href="#sec-multiple-regression" id="toc-sec-multiple-regression" class="nav-link" data-scroll-target="#sec-multiple-regression"><span class="header-section-number">12.5</span> 多元線性迴歸</a>
  <ul class="collapse">
  <li><a href="#jamovi實作示範" id="toc-jamovi實作示範" class="nav-link" data-scroll-target="#jamovi實作示範"><span class="header-section-number">12.5.1</span> jamovi實作示範</a></li>
  </ul></li>
  <li><a href="#sec-regression-model-fittedness" id="toc-sec-regression-model-fittedness" class="nav-link" data-scroll-target="#sec-regression-model-fittedness"><span class="header-section-number">12.6</span> 迴歸模型的適合度</a>
  <ul class="collapse">
  <li><a href="#sec-The-R2-value" id="toc-sec-The-R2-value" class="nav-link" data-scroll-target="#sec-The-R2-value"><span class="header-section-number">12.6.1</span> <span class="math inline">\(R^2\)</span></a></li>
  <li><a href="#迴歸與相關的關聯" id="toc-迴歸與相關的關聯" class="nav-link" data-scroll-target="#迴歸與相關的關聯"><span class="header-section-number">12.6.2</span> 迴歸與相關的關聯</a></li>
  <li><a href="#校正後-r2" id="toc-校正後-r2" class="nav-link" data-scroll-target="#校正後-r2"><span class="header-section-number">12.6.3</span> 校正後 <span class="math inline">\(R^2\)</span></a></li>
  </ul></li>
  <li><a href="#sec-regression-hypothesis-testing" id="toc-sec-regression-hypothesis-testing" class="nav-link" data-scroll-target="#sec-regression-hypothesis-testing"><span class="header-section-number">12.7</span> 迴歸模型的假設檢定</a>
  <ul class="collapse">
  <li><a href="#檢定所有預測變項的模型" id="toc-檢定所有預測變項的模型" class="nav-link" data-scroll-target="#檢定所有預測變項的模型"><span class="header-section-number">12.7.1</span> 檢定所有預測變項的模型</a></li>
  <li><a href="#單一迴歸係數的檢定" id="toc-單一迴歸係數的檢定" class="nav-link" data-scroll-target="#單一迴歸係數的檢定"><span class="header-section-number">12.7.2</span> 單一迴歸係數的檢定</a></li>
  <li><a href="#sec-jamovi-regression-hypothesis-testing" id="toc-sec-jamovi-regression-hypothesis-testing" class="nav-link" data-scroll-target="#sec-jamovi-regression-hypothesis-testing"><span class="header-section-number">12.7.3</span> 用 jamovi 執行假設檢定</a></li>
  </ul></li>
  <li><a href="#sec-regression-estimations" id="toc-sec-regression-estimations" class="nav-link" data-scroll-target="#sec-regression-estimations"><span class="header-section-number">12.8</span> 迴歸係數的估計值</a>
  <ul class="collapse">
  <li><a href="#迴歸係數的信賴區間" id="toc-迴歸係數的信賴區間" class="nav-link" data-scroll-target="#迴歸係數的信賴區間"><span class="header-section-number">12.8.1</span> 迴歸係數的信賴區間</a></li>
  <li><a href="#標準化迴歸係數的計算方法" id="toc-標準化迴歸係數的計算方法" class="nav-link" data-scroll-target="#標準化迴歸係數的計算方法"><span class="header-section-number">12.8.2</span> 標準化迴歸係數的計算方法</a></li>
  </ul></li>
  <li><a href="#sec-regression-assumptions" id="toc-sec-regression-assumptions" class="nav-link" data-scroll-target="#sec-regression-assumptions"><span class="header-section-number">12.9</span> 迴歸模型的適用條件</a></li>
  <li><a href="#sec-regression-Model-diagnosis" id="toc-sec-regression-Model-diagnosis" class="nav-link" data-scroll-target="#sec-regression-Model-diagnosis"><span class="header-section-number">12.10</span> 診斷迴歸模型的適用條件</a>
  <ul class="collapse">
  <li><a href="#三種殘差" id="toc-三種殘差" class="nav-link" data-scroll-target="#三種殘差"><span class="header-section-number">12.10.1</span> 三種殘差</a></li>
  <li><a href="#sec-three-kinds-of-anomalous-data" id="toc-sec-three-kinds-of-anomalous-data" class="nav-link" data-scroll-target="#sec-three-kinds-of-anomalous-data"><span class="header-section-number">12.10.2</span> 三種異常資料</a></li>
  <li><a href="#檢測殘差常態性" id="toc-檢測殘差常態性" class="nav-link" data-scroll-target="#檢測殘差常態性"><span class="header-section-number">12.10.3</span> 檢測殘差常態性</a></li>
  <li><a href="#sec-check-regression-collinearity" id="toc-sec-check-regression-collinearity" class="nav-link" data-scroll-target="#sec-check-regression-collinearity"><span class="header-section-number">12.10.4</span> 檢測共線性</a></li>
  </ul></li>
  <li><a href="#sec-multiple-variables-combinaion" id="toc-sec-multiple-variables-combinaion" class="nav-link" data-scroll-target="#sec-multiple-variables-combinaion"><span class="header-section-number">12.11</span> 決定線性模型的變項組合</a>
  <ul class="collapse">
  <li><a href="#逐步排除法" id="toc-逐步排除法" class="nav-link" data-scroll-target="#逐步排除法"><span class="header-section-number">12.11.1</span> 逐步排除法</a></li>
  <li><a href="#逐步納入法" id="toc-逐步納入法" class="nav-link" data-scroll-target="#逐步納入法"><span class="header-section-number">12.11.2</span> 逐步納入法</a></li>
  <li><a href="#使用警告" id="toc-使用警告" class="nav-link" data-scroll-target="#使用警告"><span class="header-section-number">12.11.3</span> 使用警告</a></li>
  <li><a href="#比較迴歸模型" id="toc-比較迴歸模型" class="nav-link" data-scroll-target="#比較迴歸模型"><span class="header-section-number">12.11.4</span> 比較迴歸模型</a></li>
  </ul></li>
  <li><a href="#本章小結" id="toc-本章小結" class="nav-link" data-scroll-target="#本章小結"><span class="header-section-number">12.12</span> 本章小結</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/SCgeeker/lsj-book-zh_tw/edit/main/12-Correlation-and-linear-regression.qmd" class="toc-action">編輯本頁面</a></p><p><a href="https://github.com/SCgeeker/lsj-book-zh_tw/issues/new" class="toc-action">回報問題</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header">
<h1 class="title display-7"><span id="sec-Correlation-and-linear-regression" class="quarto-section-identifier"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">相關與線性迴歸</span></span></h1>

</header>

<p>這個單元的學習主題是<strong>相關</strong>與<strong>線性迴歸</strong>。這些是用來分析預測變項及應變項關係的標準統計學工具。<!---[^10-translation-01]

The goal in this chapter is to introduce **correlation** and **linear regression**. These are the standard tools that statisticians rely on when analysing the relationship between continuous predictors and continuous outcomes. 
[^10-translation-01]: 本章在原書為第12章，繁體中文版為了整合 **[基礎統計方法的線性模型學習取向](Prelude-Part-V.html)**，改為第10章。---></p>
<section id="sec-what-is-correlation" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="sec-what-is-correlation"><span class="header-section-number">12.1</span> 相關</h2>
<p>這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的<strong>相關</strong>。首先，讓我們看一下列在 <a href="#tbl-tab10-1">表&nbsp;<span>12.1</span></a> 的本章示範資料描述統計。</p>
<!---In this section we'll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the **correlation** between variables. But first, we need some data (@tbl-tab10-1). --->
<section id="示範資料" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="示範資料"><span class="header-section-number">12.1.1</span> 示範資料</h3>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab10-1" class="anchored">
<table class="lightable-paper table table-sm table-striped small" data-quarto-postprocess="true">
<caption>表&nbsp;12.1: 相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。</caption>
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">變項</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">最小值</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">最大值</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">平均值</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">中位數</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">標準差</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">四分位數間距</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">老爸的沮喪程度</td>
<td style="text-align: center;">41.00</td>
<td style="text-align: center;">91.00</td>
<td style="text-align: center;">63.71</td>
<td style="text-align: center;">62.00</td>
<td style="text-align: center;">10.05</td>
<td style="text-align: center;">14.00</td>
</tr>
<tr class="even">
<td style="text-align: center;">老爸睡眠小時數</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">6.97</td>
<td style="text-align: center;">7.03</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">1.45</td>
</tr>
<tr class="odd">
<td style="text-align: center;">小嬰兒睡眠小時數</td>
<td style="text-align: center;">3.25</td>
<td style="text-align: center;">12.07</td>
<td style="text-align: center;">8.05</td>
<td style="text-align: center;">7.95</td>
<td style="text-align: center;">2.07</td>
<td style="text-align: center;">3.21</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<p>讓我們從一個所有新生兒父母都會煩惱的問題談起：睡眠。這裡使用的資料集是虛構的，但是來自原作者本人的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有自主測量的沮喪分數、我個人的睡眠時間和兒子的睡眠時間等紀錄持續100天。身為一位數位時代的書呆子，資料都保存在一個名為parenthood.csv的檔案，讀者可由本書資料庫匯入。在jamovi開啟後，可以看到四個變項：<code>dani.sleep</code>，<code>baby.sleep</code>，<code>dani.grump</code>和<code>day</code>。請注意，第一次打開這份檔案，jamovi可能無法正確設定每個變項的資料類型，請讀者自行修正，四個變項都可以被設定為連續變項，<code>ID</code>是一個名義尺度且為整數的變項。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>接著我會先看一些基本的描述統計報表，包括三個我有興趣的變項統計圖，也就是 <a href="#fig-fig10-1">圖&nbsp;<span>12.1</span></a> 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計資訊，你就要在報表顯示所有資訊。如果我要以此結果撰寫報告，我會挑出那些我自己以及讀者最感興趣的統計資訊，然後將它們放入像 <a href="#tbl-tab10-1">表&nbsp;<span>12.1</span></a> 這樣的簡潔的表格裡。<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> 需要注意的是，當我將資訊放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。</p>

<!---Let's turn to a topic close to every parent's heart: sleep. The data set we'll use is fictitious, but based on real events. Suppose I'm curious to find out how much my infant son's sleeping habits affect my mood. Let's say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to $100$ (grumpy as a very, very grumpy old man or woman). And lets also assume that I've been measuring my grumpiness, my sleeping patterns and my son's sleeping patterns for quite some time now. Let's say, for $100$ days. And, being a nerd, I've saved the data as a file called parenthood.csv. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.[^correlation-and-linear-regression-1] 

[^correlation-and-linear-regression-1]: I've noticed that in jamovi you can also specify an 'ID' variable type, but for our purposes it does not matter how we specify the ID variable as we won't be including it in any analyses.

Next, I'll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, @fig-fig10-1 plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn't mean you should report all of them. If I were writing this up for a report, I'd probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.[^correlation-and-linear-regression-2] Notice that when I put it into a table, I gave everything "human readable" names. This is always good practice. Notice also that I'm not getting enough sleep. This isn't good practice, but other parents tell me that it's pretty standard.

[^correlation-and-linear-regression-2]: Actually, even that table is more than I'd bother with. In practice most people pick one measure of central tendency, and one measure of variability only.--->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-1" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-1.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="相關的強度與方向" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="相關的強度與方向"><span class="header-section-number">12.1.2</span> 相關的強度與方向</h3>
<p>我們還可以繪製散佈圖，來俯瞰兩個變項之間的相關性。雖然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較<code>dani.sleep</code>和<code>dani.grump</code>之間的關係（ <a href="#fig-fig10-2">圖&nbsp;<span>12.2</span></a> ，左）與<code>baby.sleep</code>和<code>dani.grump</code>之間的關係（ <a href="#fig-fig10-2">圖&nbsp;<span>12.2</span></a> ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我本人或我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，<code>dani.sleep</code>和<code>dani.grump</code>之間的關係比<code>baby.sleep</code>和<code>dani.grump</code>之間的關係更強：左圖比右圖更加<em>整齊</em>。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。</p>
<!---We can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let's compare the relationship between dani.sleep and dani.grump (@fig-fig10-1), left) with that between baby.sleep and dani.grump (@fig-fig10-2), right). When looking at these two plots side by side, it's clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it's also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the left is "neater" than the one on the right. What it feels like is that if you want to predict what my mood is, it'd help you a little bit to know how many hours my son slept, but it'd be more helpful to know how many hours I slept. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-2" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-2.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.2: 左圖是<code>dani.sleep</code>(老爸睡眠小時數)與<code>dani.grump</code>(老爸的沮喪程度)的散佈圖,右圖是<code>baby.sleep</code>(小嬰兒睡眠小時數)與<code>dani.grump</code>(老爸的沮喪程度)的散佈圖。</figcaption>
</figure>
</div>
</div>
</div>
<p>相反地， <a href="#fig-fig10-3">圖&nbsp;<span>12.3</span></a> 的兩個散佈圖告訴我們另一個角度的故事。比較<code>baby.sleep</code> 與 <code>dani.grump</code>的散佈圖（左）和<code>baby.sleep</code> 與 <code>dani.sleep</code>的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。</p>
<!---In contrast, let's consider the two scatterplots shown in @fig-fig10-3. If we compare the scatterplot of "baby.sleep v dani.grump" (left) to the scatterplot of "'baby.sleep v dani.sleep" (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps more then I get less grumpy (negative relationship, left hand side). --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-3" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-3.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.3: 左圖是<code>baby.sleep</code>(小嬰兒睡眠小時數)與<code>dani.grump</code>(老爸的沮喪程度)的散佈圖,右圖是<code>baby.sleep</code>(小嬰兒睡眠小時數)與<code>dani.sleep</code>(老爸睡眠小時數)的散佈圖。</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="相關係數" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="相關係數"><span class="header-section-number">12.1.3</span> 相關係數</h3>
<p>現在我們要進一步延伸上述的概念，也就是正式認識 <strong>相關係數(correlation coefficient)</strong>。本節主要介紹皮爾森相關係數（Pearson’s correlation），樣本資料的書寫符號是 <span class="math inline">\(r\)</span>。在下一節，我們會用更精確符號 <span class="math inline">\(r_{XY}\)</span> ，表示兩個變項 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 之間的相關係數，值域涵蓋-1到1。當<span class="math inline">\(r = -1\)</span>時，表示變項之間是完全的負相關；當<span class="math inline">\(r = 1\)</span>時，表示變項之間是完全的正相關；當<span class="math inline">\(r = 0\)</span>時，表示變項之間是完全沒有關係。 <a href="#fig-fig10-4">圖&nbsp;<span>12.4</span></a> 展示幾種不同相關性的散佈圖。</p>
<p>[其他技術細節 <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>]</p>
<!--- We can make these ideas a bit more explicit by introducing the idea of a **相關係數(correlation coefficient)** (or, more specifically, Pearson's correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables $X$ and $Y$ (sometimes denoted $r_{XY}$ ), which we'll define more precisely in the next section, is a measure that varies from -1 to 1. When $r = -1$ it means that we have a perfect negative relationship, and when $r = 1$ it means we have a perfect positive relationship. When $r = 0$, there's no relationship at all. If you look at @fig-fig10-4, you can see several plots showing what different correlations look like.

[Additional technical detail [^correlation-and-linear-regression-3]]

[^correlation-and-linear-regression-3]: The formula for the Pearson's correlation coefficient can be written in several different ways. I think the simplest way to write down the formula is to break it into two steps. Firstly, let's introduce the idea of a **covariance**. The covariance between two variables $X$ and $Y$ is a generalisation of the notion of the variance and is a mathematically simple way of describing the relationship between two variables that isn't terribly informative to humans $$Cov(X,Y)=\frac{1}{N-1}\sum_{i=1}^N(X_i-\bar{X})(Y_i-\bar{Y})$$ Because we're multiplying (i.e., taking the "product" of) a quantity that depends on X by a quantity that depends on Y and then averaging$^a$, you can think of the formula for the covariance as an "average cross product" between $X$ and $Y$. The covariance has the nice property that, if $X$ and $Y$ are entirely unrelated, then the covariance is exactly zero. If the relationship between them is positive (in the sense shown in @fig-fig10-4 then the covariance is also positive, and if the relationship is negative then the covariance is also negative. In other words, the covariance captures the basic qualitative idea of correlation. Unfortunately, the raw magnitude of the covariance isn't easy to interpret as it depends on the units in which $X$ and $Y$ are expressed and, worse yet, the actual units that the covariance itself is expressed in are really weird. For instance, if $X$ refers to the dani.sleep variable (units: hours) and $Y$ refers to the dani.grump variable (units: grumps), then the units for their covariance are $hours \times grumps$. And I have no freaking idea what that would even mean. The Pearson correlation coefficient r fixes this interpretation problem by standardising the covariance, in pretty much the exact same way that the z-score standardises a raw score, by dividing by the standard deviation. However, because we have two variables that contribute to the covariance, the standardisation only works if we divide by both standard deviations.$^b$ In other words, the correlation between $X$ and $Y$ can be written as follows: $$r_{XY}=\frac{Cov(X,Y)}{\hat{\sigma}_X\hat{\sigma}_Y}$$<br />---<br />$^a$ Just like we saw with the variance and the standard deviation, in practice we divide by $N - 1$ rather than $N$. <br /> $^b$ This is an oversimplification, but it'll do for our purposes.  --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-4" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-4.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為<span class="math inline">\(0, .33, .66, 1\)</span>。右欄的相關係數由上而下為<span class="math inline">\(0, -.33, -.66, -1\)</span>。</figcaption>
</figure>
</div>
</div>
</div>
<p>標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: <span class="math inline">\(r = 1\)</span>代表著完美的正相關，<span class="math inline">\(r = -1\)</span>代表著完美的負相關。稍後<a href="#sec-interpreting-a-correlation">解讀相關係數</a>這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。</p>
<!---By standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and $r = -1$ implies a perfect negative relationship. I'll expand a little more on this point later, in the section on [Interpreting a correlation]( @sec-interpreting-a-correlation ). But before I do, let's look at how to calculate correlations in jamovi. --->
</section>
<section id="相關係數計算實務" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="相關係數計算實務"><span class="header-section-number">12.1.4</span> 相關係數計算實務</h3>
<p>只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 <a href="#fig-fig10-5">圖&nbsp;<span>12.5</span></a> 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。</p>
<!---Calculating correlations in jamovi can be done by clicking on the 'Regression' - 'Correlation Matrix' button. Transfer all four continuous variables across into the box on the right to get the output in @fig-fig10-5. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-5" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-5.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-interpreting-a-correlation" class="level3" data-number="12.1.5">
<h3 data-number="12.1.5" class="anchored" data-anchor-id="sec-interpreting-a-correlation"><span class="header-section-number">12.1.5</span> 解讀相關係數</h3>
<p>在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀<span class="math inline">\(r = 0.4\)</span>的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於<span class="math inline">\(0.95\)</span>的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到<span class="math inline">\(0.9\)</span>，任何使用這個測驗預測的理論就會失效<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過<span class="math inline">\(0.3\)</span>，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 <a href="#tbl-tab10-2">表&nbsp;<span>12.2</span></a> 的概略式解讀原則。</p>
<!--- Naturally, in real life you don't see many correlations of $1$. So how should you interpret a correlation of, say, r = $.4$? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than $.95$ is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can't achieve a correlation of at least $.9$ really isn't deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above $.3$ you're doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in @tbl-tab10-2 is pretty typical. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab10-2" class="anchored">
<table class="lightable-paper table table-sm table-striped small" data-quarto-postprocess="true">
<caption>表&nbsp;12.2: 解讀相關係數的<strong>粗略</strong>指南。強調<strong>粗略</strong>是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。</caption>
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">相關係數</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">強度</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">方向</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">-1.0 ~ -0.9</td>
<td style="text-align: center;">非常強</td>
<td style="text-align: center;">負相關</td>
</tr>
<tr class="even">
<td style="text-align: center;">-0.9 ~ -0.7</td>
<td style="text-align: center;">強</td>
<td style="text-align: center;">負相關</td>
</tr>
<tr class="odd">
<td style="text-align: center;">-0.7 to -0.4</td>
<td style="text-align: center;">中等</td>
<td style="text-align: center;">負相關</td>
</tr>
<tr class="even">
<td style="text-align: center;">-0.4 ~ -0.2</td>
<td style="text-align: center;">弱</td>
<td style="text-align: center;">負相關</td>
</tr>
<tr class="odd">
<td style="text-align: center;">-0.2 ~ 0</td>
<td style="text-align: center;">微弱</td>
<td style="text-align: center;">負相關</td>
</tr>
<tr class="even">
<td style="text-align: center;">0 ~ 0.2</td>
<td style="text-align: center;">微弱</td>
<td style="text-align: center;">正相關</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0.2 ~ 0.4</td>
<td style="text-align: center;">弱</td>
<td style="text-align: center;">正相關</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.4 ~ 0.7</td>
<td style="text-align: center;">中等</td>
<td style="text-align: center;">正相關</td>
</tr>
<tr class="odd">
<td style="text-align: center;">0.7 ~ 0.9</td>
<td style="text-align: center;">強</td>
<td style="text-align: center;">正相關</td>
</tr>
<tr class="even">
<td style="text-align: center;">0.9 ~ 1.0</td>
<td style="text-align: center;">非常強</td>
<td style="text-align: center;">正相關</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<p>然而，有一件點任何一位統計學教師都會不厭其煩地提醒學生，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)<span class="citation" data-cites="Anscombe1973">(<a href="References.html#ref-Anscombe1973" role="doc-biblioref">Anscombe, 1973</a>)</span>，其中有四個資料集。每個資料集都有兩個變項， <span class="math inline">\(X\)</span> 與 <span class="math inline">\(Y\)</span>。四個資料集的 <span class="math inline">\(X\)</span> 平均值都是 <span class="math inline">\(9\)</span>， <span class="math inline">\(Y\)</span> 的平均值都是 <span class="math inline">\(7.5\)</span>。所有 <span class="math inline">\(X\)</span> 變項的標準差幾乎相同，<span class="math inline">\(Y\)</span> 變項的標準差也是一致。每種資料集的<span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 相關係數均為 <span class="math inline">\(r = 0.816\)</span>。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。</p>
<p>也許你認為這四個資料集看起來很相似，其實上並非如此。從 <a href="#fig-fig10-6">圖&nbsp;<span>12.6</span></a> 的散佈圖可以發現，所有四個資料集的<span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始資訊」（見 <a href="05-Drawing-graphs.html"><span>單元&nbsp;5</span></a> ）。</p>
<!---However, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is "Anscombe's Quartet" [@Anscombe1973], a collection of four data sets. Each data set has two variables, an $X$ and a $Y$. For all four data sets the mean value for $X$ is $9$ and the mean for $Y$ is $7.5$. The standard deviations for all $X$ variables are almost identical, as are those for the Y variables. And in each case the correlation between $X$ and $Y$ is $r = 0.816$. You can verify this yourself, since I happen to have saved it in a file called anscombe.csv.

 http://localhost:7972/10-Correlation-and-linear-regression.html#%E7%9B%B8%E9%97%9C%E4%BF%82%E6%95%B8

You'd think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of $X$ against $Y$ for all four variables, as shown in @fig-fig10-6, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is "always graph your raw data" (see @sec-Drawing-graphs). --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-6" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-6.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="斯皮爾曼等級相關" class="level3" data-number="12.1.6">
<h3 data-number="12.1.6" class="anchored" data-anchor-id="斯皮爾曼等級相關"><span class="header-section-number">12.1.6</span> 斯皮爾曼等級相關</h3>
<p>皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。</p>
<p>線性關係是當一個變項<span class="math inline">\(X\)</span>的數值增加，也能反映另一個變項<span class="math inline">\(Y\)</span>的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（<span class="math inline">\(X\)</span>）準備一個科目，那麼他排名的成績應該只有0％（<span class="math inline">\(Y\)</span>）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。</p>
<p>我們用 <a href="#fig-fig10-7">圖&nbsp;<span>12.7</span></a> 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的<strong>次序關係(ordinal relationship)</strong>。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數<span class="math inline">\(r=0.91\)</span>能表達的。</p>
<!--- The Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say "relationship", and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn't.

One very common situation where the Pearson correlation isn't quite the right thing to use arises when an increase in one variable $X$ really is reflected in an increase in another variable Y , but the nature of the relationship isn't necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort ($X$) into learning a subject then you should expect a grade of $0\%$ ($Y$). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don't get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of $90\%$ than it takes to get a grade of $55\%$. What this means is that, if I've got data looking at study effort and grades, there's a pretty good chance that Pearson correlations will be misleading. 

To illustrate, consider the data plotted in @fig-fig10-7, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of $0.91$. However, this doesn't actually capture the observation that increasing hours worked always increases the grade. There's a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a "relationship" is. What we're looking for is something that captures the fact that there is a perfect **ordinal relationship** here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That's not what a correlation of $r = .91$ says at all.--->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-7" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-7.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.7: 此散佈圖展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數<span class="math inline">\(r = .91\)</span>。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數<span class="math inline">\(\rho = 1\)</span>。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。</figcaption>
</figure>
</div>
</div>
</div>
<p>我們要如何解決這個問題呢？其實很簡單。要評估變項之間次序關係的話，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”衡量學生的努力程度，而是按照這10名學生的讀書時間長度排序。也就是說，學生<span class="math inline">\(2\)</span>花在讀書的時間最少（<span class="math inline">\(2\)</span>個小時），所以他獲得了最低的排名（排名=<span class="math inline">\(1\)</span>）。接下來最懶惰的是學生<span class="math inline">\(4\)</span>，整個學期只讀了<span class="math inline">\(6\)</span>個小時的書，所以他獲得了次低的排名（排名=<span class="math inline">\(2\)</span>）。請注意，在此用“排名=<span class="math inline">\(1\)</span>”來表示“低排名”。在日常言談裡，多數人使用“排名=<span class="math inline">\(1\)</span>”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！</p>
<p>好的，讓我們從最努力且最成功的學生開始排名。 <a href="#tbl-tab10-3">表&nbsp;<span>12.3</span></a> 顯示從最努力且最成功的學生排名的次序值。<!---[^10-translation-3]

[^10-translation-3]: 譯註~由於原文的排名與 @tbl-tab10-3 有出入，中文版做調整使其一致。使用jamovi的話，可在Variable面板註記。---></p>
<!--- How should we address this? Actually, it's really easy. If we're looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of "hours worked", lets rank all $10$ of our students in order of hours worked. That is, student $1$ did the least work out of anyone ($2$ hours) so they get the lowest rank (rank = $1$). Student $4$ was the next laziest, putting in only $6$ hours of work over the whole semester, so they get the next lowest rank (rank = $2$). Notice that I'm using "rank =1" to mean "low rank". Sometimes in everyday language we talk about "rank = $1$" to mean "top rank" rather than "bottom rank". So be careful, you can rank "from smallest value to largest value" (i.e., small equals rank $1$) or you can rank "from largest value to smallest value" (i.e., large equals rank 1). In this case, I'm ranking from smallest to largest, but as it's really easy to forget which way you set things up you have to put a bit of effort into remembering!

Okay, so let's have a look at our students when we rank them from worst to best in terms of effort and reward @tbl-tab10-3. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab10-3" class="anchored">
<table class="lightable-paper table table-sm table-striped small" data-quarto-postprocess="true">
<caption>表&nbsp;12.3: 十位學生的工作時間與得分數值次序</caption>
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">學生編號</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">讀書時間序列</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">成績序列</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">學生 1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr class="even">
<td style="text-align: center;">學生 2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">學生 3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td style="text-align: center;">學生 4</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="odd">
<td style="text-align: center;">學生 5</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="even">
<td style="text-align: center;">學生 6</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="odd">
<td style="text-align: center;">學生 7</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">7</td>
</tr>
<tr class="even">
<td style="text-align: center;">學生 8</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="odd">
<td style="text-align: center;">學生 9</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="even">
<td style="text-align: center;">學生 10</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<p>有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。</p>
<p>至此我們等於重新發現 <strong>斯皮爾曼等級相關(Spearman’s rank order correlation)</strong>，通常用符號 <span class="math inline">\(\rho\)</span> 表示，以區分皮爾森相關係數<span class="math inline">\(r\)</span>。我們可以在jamovi的“相關矩陣”選單選擇“Spearman”，計算斯皮爾曼等級相關係數。<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>

<!--- Hmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.

What we've just re-invented is **斯皮爾曼等級相關(Spearman's rank order correlation)**, usually denoted $\rho$ to distinguish it from the Pearson correlation r. We can calculate Spearman's $\rho$ using jamovi simply by clicking the 'Spearman' check box in the 'Correlation Matrix' screen. --->
</section>
</section>
<section id="sec-the-scatter-plot" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="sec-the-scatter-plot"><span class="header-section-number">12.2</span> 散佈圖</h2>
<p>散佈圖是一種簡單但有效的視覺化工具，用於具現兩個變項之間的關係，就像<a href="#sec-what-is-correlation">相關</a>這一節所展示的圖表。通常提到“散佈圖”這個術語時，指的是兩個變項的具體視覺化。在散佈圖中，每個觀察值都是對應一個資料點。一個點的水平位置表示一個變項的觀察值，垂直位置表示觀察值在另一個變項的數值。在許多使用情境，我們對於變項間的因果關係並沒有清晰的看法（例如，A是否引起B，還是B引起A，還是其他變項C控制A和B）。若是這樣，x軸和y軸上代表那個變項並不重要。然而在許多情境，研究者對於那個變項最有可能是原因或結果，會有一個相當明確的想法，或者對於何者為因至少有一些懷疑。若是這樣，用x軸代表原因的自變項，用y軸代表效應的應變項是一種傳統的繪圖規範。了解這樣的規範，讓我們來看一下如何合理運用jamovi繪製散佈圖，同樣使用在<a href="#sec-what-is-correlation">相關</a>這一節做為示範的資料集（parenthood.csv）。</p>
<p>假定我的目標是繪製一個顯示本人睡眠時間<code>dani.sleep</code>與隔天沮喪程度<code>dani.grump</code>兩個變項關係的散佈圖，我們有兩種不同的方法使用jamovi得到我們想要的圖。第一種方法是設定’Regression’ - ‘Correlation Matrix’選單下方的’Plot’選項，這樣可以得到如圖 <a href="#fig-fig10-8">圖&nbsp;<span>12.8</span></a> 的結果。請注意，jamovi會繪製一條通過資料點的直線，稍後在<a href="#sec-what-is-linear-model">認識線性迴歸模型</a>這一節進一步說明。以這種方法繪製散佈圖也能繪製’變項密度’，這個選項會添加一條密度曲線，顯示每個變項的資料分佈狀況。</p>
<!--- **Scatterplots** are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on [Correlations](10-Correlation-and-linear-regression.html#sec-what-is-correlation). It's this latter application that we usually have in mind when we use the term "scatterplot". In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don't really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that's the case, it doesn't really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it's conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let's look at how to draw scatterplots in jamovi, using the same parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.

Suppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we're after. The first way is to use the 'Plot' option under the 'Regression' - 'Correlation Matrix' button, giving us the output shown in @fig-fig10-8.  Note that jamovi draws a line through the points, we'll come onto this a bit later in the section on [What is a linear regression model?]. Plotting a scatterplot in this way also allow you to specify 'Densities for variables' and this option adds a density curve showing how the data in each variable is distributed. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-8" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-8.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.8: 使用jamovi相關分析模組的’Correlation Matrix’所繪製的散佈圖。</figcaption>
</figure>
</div>
</div>
</div>
<p>第二種方法是使用jamovi的附加模組之一<code>scatr</code>，只要點擊jamovi介面右上角的那個大「<span class="math inline">\(+\)</span>」，在jamovi模組庫裡找到<code>scatr</code>，然後點擊「install」進行安裝。安裝成功後，在「Exploration」的選單下方會多出新的「Scatterplot」選項。這種方法繪製的散佈圖和第一種方法不大一樣，如同 <a href="#fig-fig10-9">圖&nbsp;<span>12.9</span></a> 所顯示，但是透露的訊息是一樣的。</p>
<!--- The second way do to it is to use one of the jamovi add-on modules. This module is called 'scatr' and you can install it by clicking on the large '$+$' icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find 'scatr' and clicking 'install'. When you have done this, you will find a new 'Scatterplot' command available under the 'Exploration' button. This plot is a bit different than the first way, see @fig-fig10-9, but the important information is the same. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-9" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-9.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.9: 使用jamovi擴充模組’scatr’繪製的散佈圖</figcaption>
</figure>
</div>
</div>
</div>
<section id="更多解讀散佈圖的方法" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="更多解讀散佈圖的方法"><span class="header-section-number">12.2.1</span> 更多解讀散佈圖的方法</h3>
<p>通常我們會需要查看多個變項之間的關係，可以在 jamovi 的 ‘Correlation Matrix’選單下方的’Plot’ 選項，勾選繪制<strong>散佈圖矩陣</strong>。只要加入另一個變項到要變項列表，例如 <code>baby.sleep</code>，jamovi 就會生成一個散佈圖矩陣，如同 <a href="#fig-fig10-10">圖&nbsp;<span>12.10</span></a> 的示範。</p>
<!--- Often you will want to look at the relationships between several variables at once, using a **scatterplot matrix** (in jamovi via the 'Correlation Matrix' - 'Plot' command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in @fig-fig10-10. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-10" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-10.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.10: jamovi繪製的散佈圖矩陣</figcaption>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="sec-what-is-linear-model" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="sec-what-is-linear-model"><span class="header-section-number">12.3</span> 認識線性迴歸模型</h2>
<!---
> **譯者註** 20230415初步以ChatGPT-4完成翻譯，以下內容待編修。 --->
<p>我們可以將線性迴歸模型理解為稍微複雜一點的皮爾森相關係數分析程序（請見<a href="#sec-what-is-correlation">相關</a>這一節），稍後我們會看到，迴歸模型是用途更廣泛的統計方法。</p>
<p>由於迴歸模型的基本觀念與相關係數緊密相關，以下同樣使用parenthood.csv資料集進行介紹及示範。回想一下，我們分析這個資料集的目的是，找出我(原作者Dani)為什麼總是非常沮喪的原因，而我的研究假設是我沒有得到足夠的睡眠。所以畫了一些散佈圖，檢示實際睡眠時間與隔天沮喪程度之間的關係，就像 <a href="#fig-fig10-9">圖&nbsp;<span>12.9</span></a> 展示的散佈圖，兩者之間的相關係數達到<span class="math inline">\(r=-0.90\)</span>。但是，我想描述的變項間關係更像 <a href="#fig-fig10-11">圖&nbsp;<span>12.11</span></a> (a) ，也就是有一條直線穿過資料點的中間。這條直線的統計學術語是<strong>迴歸線</strong>。請注意，由於我不是統計新手，因此畫出的迴歸線一定會穿過資料散佈區域的中間地帶，絕不會認為是像 <a href="#fig-fig10-11">圖&nbsp;<span>12.11</span></a> (b) 的樣子。</p>
<!---Stripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see [Correlations]), though as we'll see regression models are much more powerful tools.

Since the basic ideas in regression are closely tied to correlation, we'll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I'm not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in @fig-fig10-9, and as we saw previously this corresponds to a correlation of $r = -.90$, but what we find ourselves secretly imagining is something that looks closer to @fig-fig10-11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we're drawing is called a **regression line**. Notice that, since we're not idiots, the regression line goes through the middle of the data. We don't find ourselves imagining anything like the rather silly plot shown in @fig-fig10-11 (b). --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-11" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-11.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.11: 圖a展示同 <a href="#fig-fig10-9">圖&nbsp;<span>12.9</span></a> 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。</figcaption>
</figure>
</div>
</div>
</div>
<p>解讀變項間關係的迴歸線，並不需要什麼厲害的技巧。 <a href="#fig-fig10-11">圖&nbsp;<span>12.11</span></a> （b）的那條線與資料的適合度(fittedness)並不高，用來解讀資料沒有太大的意義，對吧？迴歸線能很直覺地呈現變項間的關係，若是再應用迴歸線的數學理論解讀資料，會變成非常強大的分析工具。我們複習一下高中數學，至少幾十年前澳洲的高中數學課是這樣教的，一條直線的公式可以寫成這樣的線性迴歸式：</p>
<!--- This is not highly surprising. The line that I've drawn in @fig-fig10-11 (b) doesn't "fit" the data very well, so it doesn't make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let's start with a refresher of some high school maths. The formula for a straight line is usually written like this --->
<p><span class="math display">\[y=a+bx\]</span></p>
<p>兩個資料變項用 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span>代表，搭配兩個係數 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 形成變項之間的等價性。<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>係數 <span class="math inline">\(a\)</span> 代表迴歸線的截距，係數 <span class="math inline">\(b\)</span> 代表迴歸線的斜率。努力回憶一下高中曾學過的內容（很抱歉，某些讀者也許已經離開高中校園很久了），記得截距被解釋為“當 <span class="math inline">\(x=0\)</span> 時得到的 <span class="math inline">\(y\)</span> 值”。同樣地，斜率 <span class="math inline">\(b\)</span> 若為正值，代表增加 <span class="math inline">\(x\)</span> 的數值一個單位， <span class="math inline">\(y\)</span> 值會增加 <span class="math inline">\(b\)</span> 個單位；而斜率 <span class="math inline">\(b\)</span> 若為負值，則代表 <span class="math inline">\(y\)</span> 值會下降而不是上升。啊，是的，我們現在全都回想起來了。現在我們的記憶已經回來，所以自然會發現可以使用完全相同的公式計算迴歸線。如果 <span class="math inline">\(Y\)</span> 是預測變項（依變項），<span class="math inline">\(X\)</span> 是應變項（自變項），那麼描述示範資料的迴歸線等式就會像是這樣：</p>
<!---Or, at least, that's what it was when I went to high school all those years ago. The two variables are $x$ and $y$, and we have two coefficients, $a$ and $b$.[^correlation-and-linear-regression-4] The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as "the value of y that you get when $x = 0$". Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it's all coming back to me now. Now that we've remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If $Y$ is the outcome variable (the DV) and X is the predictor variable (the $IV$), then the formula that describes our regression is written like this

[^correlation-and-linear-regression-4]: Also sometimes written as $y = mx + c$ where m is the slope coefficient and $c$ is the intercept (constant) coefficient. --->
<p><span class="math display">\[\hat{Y}_i=b_0+b_1X_i\]</span></p>
<p>嗯，這看起來這跟曾在高中教科書看到的公式一模一樣，只是多了些花俏的下標符號，讓我們來了解這些符號的意思。首先，請注意我使用 <span class="math inline">\(X_i\)</span> 和 <span class="math inline">\(Y_i\)</span>，而不是 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span>，這是因為有下標符號的代數通常代表實際的資料。在這個公式裡，<span class="math inline">\(X_i\)</span> 代表第 i 個觀察值的預測變項的值（例如我在第 i 天紀錄的睡眠時間），而 <span class="math inline">\(Y_i\)</span> 則是對應的應變項數值（例如我當天的沮喪程度）。雖然公式裡沒有明確說明，但我們假設這個公式對資料集裡的所有觀察值都通用（即 i 對應所有 觀察日數）。其次，請注意我寫的是 <span class="math inline">\(\hat{Y}_i\)</span> 而不是 <span class="math inline">\(Y_i\)</span>，這是因為我們要區分實際數值 <span class="math inline">\(Y_i\)</span> 與被預測數值 <span class="math inline">\(\hat{Y}_i\)</span>（也就是經由迴歸線預測的數值）。第三，我將代表係數的符號從 a 和 b 改成 <span class="math inline">\(b_0\)</span> 和 <span class="math inline">\(b_1\)</span>，這是統計學家喜歡呈現迴歸模型的方式。我不知道為什麼他們選擇用 b 這個字母，但這就是統計學的慣例。無論如何，<span class="math inline">\(b_0\)</span> 總是代表截距，<span class="math inline">\(b_1\)</span> 則是代表斜率。</p>
<p>跟上來的話就很好。接著我們會注意到，無論是好的迴歸線還是壞的迴歸線，資料都是不完美地落在迴歸線。換句話說，實際數值<span class="math inline">\(Y_i\)</span>不完全等於迴歸模型預測的數值<span class="math inline">\(\hat{Y}_i\)</span>。由於統計學家喜歡給一切符號冠上字母、名稱和數字，讓我們稱呼模型預測的數值與實際數值之間的差異為殘差(Residuals)，代表符號為<span class="math inline">\(\epsilon_i\)</span>。<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> 使用數學公式表示的話，殘差可被定義為：</p>
<!--- Hmm. Looks like the same formula, but there's some extra frilly bits in this version. Let's make sure we understand them. Firstly, notice that I've written $X_i$ and $Y_i$ rather than just plain old $X$ and $Y$ . This is because we want to remember that we're dealing with actual data. In this equation, $X_i$ is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and $Y_i$ is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven't said so explicitly in the equation, what we're assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote $\hat{Y}_i$ and not $Y_i$ . This is because we want to make the distinction between the actual data $Y_i$, and the estimate $\hat{Y}_i$ (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and $b$ to $b_0$ and $b_1$. That's just the way that statisticians like to refer to the coefficients in a regression model. I've no idea why they chose b, but that's what they did. In any case $b_0$ always refers to the intercept term, and $b_1$ refers to the slope.

Excellent, excellent. Next, I can't help but notice that, regardless of whether we're talking about the good regression line or the bad one, the data don't fall perfectly on the line. Or, to say it another way, the data $Y_i$ are not identical to the predictions of the regression model $\hat{Y}_i$. Since statisticians love to attach letters, names and numbers to everything, let's refer to the difference between the model prediction and that actual data point as a residual, and we'll refer to it as $\epsilon_i$.[^correlation-and-linear-regression-5] Written using mathematics, the residuals are defined as

[^correlation-and-linear-regression-5]: The $\epsilon$ symbol is the Greek letter epsilon. It's traditional to use $\epsilon_i$ or $e_i$ to denote a residual. --->
<p><span class="math display">\[\epsilon_i=Y_i-\hat{Y}_i\]</span></p>
<p>接著我們就可以寫出完整的線性迴歸模型：</p>
<!---which in turn means that we can write down the complete linear regression model as --->
<p><span class="math display">\[Y_i=b_0+b_1X_i+\epsilon_i\]</span></p>
</section>
<section id="sec-the-regression-coefficients-parameters" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="sec-the-regression-coefficients-parameters"><span class="header-section-number">12.4</span> 線性迴歸模型的參數估計</h2>
<!---
> **譯者註** 20230415初步以ChatGPT-4完成翻譯，以下內容待編修。 --->
<p>好的，現在讓我們重新繪製散佈圖，這次會添加一些線條顯示所有觀察值的殘差。當迴歸線的適合度(fittedness)最佳時，每個殘差數值（實心黑線的長度）看起來都非常小且接近，如同 <a href="#fig-fig10-12">圖&nbsp;<span>12.12</span></a> (a) ，但是當迴歸線的適合度不夠好，每個殘差之間的差異就會非常大，可以從 <a href="#fig-fig10-12">圖&nbsp;<span>12.12</span></a> (b)看到這樣的差別。嗯，也許在尋找一條最好的迴歸模型時，我們會希望得到<em>儘可能小</em>的殘差。是的，這確實有道理。在統計實務，我們可以說「最適合」的迴歸線是具有所有殘差最小的線。或者更好的說法是，因為統計學家似乎喜歡將所有數值都用<em>平方(sqaured)</em>處理，也就是說：</p>
<blockquote class="blockquote">
<p>以資料估計的迴歸係數 <span class="math inline">\(\hat{b}_0\)</span> 和 <span class="math inline">\(\hat{b}_1\)</span> 是殘差平方和最小得到時得到的估計值，我們可以兩者的公式展開寫成 <span class="math inline">\(\sum_i (Y_i - \hat{Y}_i)^2\)</span> 與 <span class="math inline">\(\sum_i \epsilon_i^2\)</span> 。</p>
</blockquote>
<!--- Okay, now let's redraw our pictures but this time I'll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in @fig-fig10-12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at @fig-fig10-12 (b). Hmm. Maybe what we "want" in a regression model is *small* residuals. Yes, that does seem to make sense. In fact, I think I'll go so far as to say that the "best fitting" regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:

> The estimated regression coefficients, $\hat{b}_0$ and $\hat{b}_1$, are those that minimise the sum of the squared residuals, which we could either write as $\sum_i (Y_i - \hat{Y}_i)^2$ or as $\sum_i \epsilon_i^2$. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-12" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-12.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.12: 圖(a)展示各觀察值資料點與穿越資料點中心地帶的最佳迴歸線之殘差，圖(b)展示觀察值資料點與最差迴歸線的殘差。前者的殘差總和明顯小於後者。</figcaption>
</figure>
</div>
</div>
</div>
<p>是的沒錯，這樣說明起來更有學問一些。而且我將這段話縮排，表示這樣說可能是正確的解答。既然這是正確解答，那麼要值得注意的是，迴歸線的係數都是估計值（請復習 <a href="08-Estimating-unknown-quantities-from-a-sample.html"><span>單元&nbsp;8</span></a> ，使用點估計方法猜測一個母群的參數！），這也是為什麼我要在代表係數的符號上頭加個小帽子 ^ ，區別會放在報告的是<span class="math inline">\(\hat{b}_0\)</span>和<span class="math inline">\(\hat{b}_1\)</span>，而不是 <span class="math inline">\(b_0\)</span> 和 <span class="math inline">\(b_1\)</span>。最後，我還要指出，由於實際上有許多方法來估計迴歸模型，這一節說明的估計方法正式名稱是<strong>普通最小平方法（Ordinary Least Squares，OLS）</strong>。</p>
<p>至此，我們已經得到「最佳」迴歸係數 <span class="math inline">\(\hat{b}_0\)</span> 和 <span class="math inline">\(\hat{b}_1\)</span> 的具體定義。下一個問題自然是：如果最佳迴歸係數是那些符合最小化殘差平方和的係數，我們要如何算出這些數值呢？實際上，這個問題的答案比較複雜，並且無法幫助你理解迴歸的邏輯。<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>這一次，我放過各位同學，直接介紹 jamovi 操作方法，瑣碎的讓jamovi來處理。</p>
<!---Yes, yes that sounds even better. And since I've indented it like that, it probably means that this is the right answer. And since this is the right answer, it's probably worth making a note of the fact that our regression coefficients are estimates (we're trying to guess the parameters that describe a population!), which is why I've added the little hats, so that we get $\hat{b}_0$ and $\hat{b}_1$ rather than $b_0$ and $b_1$. Finally, I should also note that, since there's actually more than one way to estimate a regression model, the more technical name for this estimation process is **ordinary least squares (OLS) regression**.

At this point, we now have a concrete definition for what counts as our "best" choice of regression coefficients, $\hat{b}_0$ and $\hat{b}_1$. The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn't help you understand the logic of regression.[^correlation-and-linear-regression-6] This time I'm going to let you off the hook. Instead of showing you the long and tedious way first and then "revealing" the wonderful shortcut that jamovi provides, let's cut straight to the chase and just use jamovi to do all the heavy lifting.

[^correlation-and-linear-regression-6]: Or at least, I'm assuming that it doesn't help most people. But on the off chance that someone reading this is a proper kung fu master of linear algebra (and to be fair, I always have a few of these people in my intro stats class), it will help you to know that the solution to the estimation problem turns out to be $\hat{b} = (X^{'}X)^{-1}X^{'}y$, where $\hat{b}$ is a vector containing the estimated regression coefficients, $X$ is the "design matrix" that contains the predictor variables (plus an additional column containing all ones; strictly $X$ is a matrix of the regressors, but I haven't discussed the distinction yet), and y is a vector containing the outcome variable. For everyone else, this isn't exactly helpful and can be downright scary. However, since quite a few things in linear regression can be written in linear algebra terms, you'll see a bunch of footnotes like this one in this chapter. If you can follow the maths in them, great. If not, ignore it. --->
<section id="實作線性迴歸模型" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="實作線性迴歸模型"><span class="header-section-number">12.4.1</span> 實作線性迴歸模型</h3>
<p>以下是用parenthood.csv 資料檔案執行線性迴歸分析的步驟，請打開 jamovi 的 ‘Regression’ - ‘Linear Regression’ 選單 。接著，將 dani.grump 指定為 ‘Dependent Variable’，dani.sleep 輸入到 ‘Covariates’ 對話框。報表介面將出現如 <a href="#fig-fig10-13">圖&nbsp;<span>12.13</span></a> 的結果，結果顯示截距 <span class="math inline">\(\hat{b}_0 = 125.96\)</span> 和斜率 <span class="math inline">\(\hat{b}_1 = -8.94\)</span>。換言之， <a href="#fig-fig10-11">圖&nbsp;<span>12.11</span></a> 的最適合迴歸線的公式為：</p>
<!--- To run my linear regression, open up the 'Regression' - 'Linear Regression' analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the 'Dependent Variable' and dani.sleep as the variable entered in the 'Covariates' box. This gives the results shown in @fig-fig10-13, showing an intercept $\hat{b}_0 = 125.96$ and the slope $\hat{b}_1 = -8.94$. In other words, the best fitting regression line that I plotted in @fig-fig10-11 has this formula: --->
<p><span class="math display">\[\hat{Y}_i=125.96+(-8.94 X_i)\]</span></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-13" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-13.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.13: jamovi的線性迴歸分析示範畫面。</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="解讀線性迴歸模型參數估計" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="解讀線性迴歸模型參數估計"><span class="header-section-number">12.4.2</span> 解讀線性迴歸模型參數估計</h3>
<p>最後要知道的是如何解釋這些係數。讓我們從 <span class="math inline">\(\hat{b}_1\)</span> 開始，也就是斜率。回想一下斜率的定義，<span class="math inline">\(\hat{b}_1=-8.94\)</span> 代表將 <span class="math inline">\(X_i\)</span> 增加 1， <span class="math inline">\(Y_i\)</span> 就會減少 8.94。換言之，多睡一個小時的話，我的心情就會改善，我的沮喪程度就會降低 8.94 。那麼截距呢？由於 <span class="math inline">\(\hat{b}_0\)</span> 代表「當 <span class="math inline">\(X_i\)</span> 為 0 時 <span class="math inline">\(Y_i\)</span> 的期望值」，這就是說如果我一夜都沒睡 (<span class="math inline">\(X_i = 0\)</span>)，我的沮喪程度就會瘋狂升高到不敢想像的數值 (<span class="math inline">\(Y_i = 125.96\)</span>)。我想我最好避免這種狀況。</p>
<!--- The most important thing to be able to understand is how to interpret these coefficients. Let's start with $\hat{b}_1$, the slope. If we remember the definition of the slope, a regression coefficient of $\hat{b}_1 = -8.94$ means that if I increase Xi by 1, then I'm decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since $\hat{b}_0$ corresponds to "the expected value of $Y_i$ when $X_i$ equals 0", it's pretty straightforward. It implies that if I get zero hours of sleep ($X_i = 0$) then my grumpiness will go off the scale, to an insane value of ($Y_i = 125.96$). Best to be avoided, I think. --->
<!---
> 1. 還有關於等級資料(Rank data)的迴歸分析，請參考**線性模型的學習取向**的[相關與線性迴歸](Prelude-Part-V.html#%E7%9B%B8%E9%97%9C%E8%88%87%E7%B7%9A%E6%80%A7%E8%BF%B4%E6%AD%B8)這一節。--->
<blockquote class="blockquote">
<p><strong>譯註</strong>: 以下 <a href="#sec-multiple-regression"><span>小單元&nbsp;12.5</span></a> 、 <a href="#sec-regression-Model-diagnosis"><span>小單元&nbsp;12.10</span></a> 以及 <a href="#sec-multiple-variables-combinaion"><span>小單元&nbsp;12.11</span></a> 等三個小單元，是屬於傳統高等統計課程的範圍，其他單元在多數教科書被劃分為基礎統計的範圍。不過接下來的單元裡原作者都是混合一元迴歸與多元迴歸的示範案例，譯者將在屬於多元迴歸的小單元開頭明示譯註，提供使用這本電子書學習的學生與教學的老師，根據自身的學習目標決定如何運用該節內容。</p>
</blockquote>
</section>
</section>
<section id="sec-multiple-regression" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="sec-multiple-regression"><span class="header-section-number">12.5</span> 多元線性迴歸</h2>
<!---
> 譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。
--->
<p>我們至此討論過的簡單線性迴歸模型，都是假設只有一個自變項，也就是這一章範例資料的<code>dani.sleep</code>。同樣地，基礎統計會學到的大部分統計方法都是假設只有一個自變項和一個應變項。然而，大多數研究項目實際要處理許多自變項。如果是這樣，學習包含多個自變項線性迴歸模型可能比較好。也許使用多元迴歸模型會更適合這一章的範例資料？</p>
<p>多元迴歸的概念非常簡單，只要在簡單迴歸模型加入更多自變項。假如我們對資料中的兩個自變項都有興趣，也許可以用dani.sleep和baby.sleep預測依變項dani.grump。就像之前的說明一樣，我們用<span class="math inline">\(Y_{i}\)</span>表示第i天的煩躁程度。只是現在有兩個$ X <span class="math inline">\(變項：第一個對應我的睡眠時間，第二個對應我兒子的睡眠時間。所以我們用\)</span>X_{i1}<span class="math inline">\(表示第i天我的睡眠時間，\)</span>X_{i2}$表示那一天我兒子的睡眠時間。那麼我們可以這樣改寫迴歸模型：</p>
<!--- The simple linear regression model that we've discussed up to this point assumes that there's a single predictor variable that you're interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we've talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of **multiple regression** model would be in order?

Multiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let's suppose that we've got two variables that we're interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let $Y_{i}$ refer to my grumpiness on the i-th day. But now we have two \$ X \$ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we'll let $X_{i1}$ refer to the hours I slept on the i-th day and $X_{i2}$ refers to the hours that the baby slept on that day. If so, then we can write our regression model like this: --->
<p><span class="math display">\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\epsilon_i\]</span></p>
<p>就像前一節的說，<span class="math inline">\(\epsilon_i\)</span>是與第i個觀察值相關的殘差，<span class="math inline">\(\epsilon_i = Y_i - \hat{Y}_i\)</span>。多元迴歸模型有三個需要估計的係數：<span class="math inline">\(b_0\)</span>是截距，<span class="math inline">\(b_1\)</span>是代表我的睡眠時間的迴歸係數，<span class="math inline">\(b_2\)</span>是代表我兒子的睡眠時間的迴歸係數。然而，儘管需要估計的迴歸係數變多，估計的基本原理還是一樣：我們要估計的係數<span class="math inline">\(\hat{b}_0\)</span>、<span class="math inline">\(\hat{b}_1\)</span>和<span class="math inline">\(\hat{b}_2\)</span> 算的都是能得到最小殘差平方和的係數。</p>
<!--- As before, $\epsilon_i$ is the residual associated with the i-th observation, $\epsilon_i = Y_i - \hat{Y}_i$. In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son's sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients $\hat{b}_0$, $\hat{b}_1$ and $\hat{b}_2$ are those that minimise the sum squared residuals. --->
<section id="jamovi實作示範" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="jamovi實作示範"><span class="header-section-number">12.5.1</span> jamovi實作示範</h3>
<p>使用jamovi計算多元迴歸的程序與簡單迴歸完全一樣。我們要做的就是在jamovi的’自變項’對話框添加更多自變項。像是要使用<code>dani.sleep</code>和<code>baby.sleep</code>作為預測變項來解釋為什麼我如此沮喪，接著將baby.sleep巷移動到與巷dani.sleep巷相鄰的’共變項’對話框。jamovi的預設是一開始的線性模型應該包括一個截距。這次我們得到的係數顯示在 <a href="#tbl-tab10-4">表&nbsp;<span>12.4</span></a> 中。</p>
<!--- Multiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the 'Covariates' box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I'm so grumpy, then move baby.sleep across into the 'Covariates' box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in @tbl-tab10-4. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="tbl-tab10-4" class="anchored">
<table class="lightable-paper table table-sm table-striped small" data-quarto-postprocess="true">
<caption>表&nbsp;12.4: 增加預測變項迴歸係數的示</caption>
<thead>
<tr class="header">
<th style="text-align: center;" data-quarto-table-cell-role="th">截距</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">老爸睡眠小時數</th>
<th style="text-align: center;" data-quarto-table-cell-role="th">小嬰兒睡眠小時數</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">125.97</td>
<td style="text-align: center;">-8.95</td>
<td style="text-align: center;">0.01</td>
</tr>
</tbody>
</table>
</div>


</div>
</div>
<p><code>dani.sleep</code>的係數值非常大，表示每少睡一個小時，我會變得更沮喪。然而，<code>baby.sleep</code>的係數值非常小，表示我的兒子睡多少其實無關緊要。就我的煩躁程度而言，重要的是我睡多少。為了讓您對這個多元迴歸模型有所了解， <a href="#fig-fig10-14">圖&nbsp;<span>12.14</span></a> 是一幅三維圖，包括所有三個變項以及迴歸模型本身。</p>
<!--- The coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn't really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, @fig-fig10-14 shows a 3D plot that plots all three variables, along with the regression model itself. --->
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-14" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-14.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.14: 這張立體視覺圖以三個維度展示多元迴歸模型。此模型有兩個預測變項， <code>dani.sleep</code> 和 <code>baby.sleep</code>，結果變項是 <code>dani.grump</code>，這三個變項構成三維空間。每筆資料都是這個空間中的一個點。就像簡單線性迴歸模型在二維散佈圖形成一條線一樣，多元迴歸模型在三維空間構成一個平面。估計迴歸係數，就是找到一個盡可能靠近所有資料點的平面。</figcaption>
</figure>
</div>
</div>
</div>
<p>[附加技術細節<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>]</p>
<!--- [Additional technical detail[^correlation-and-linear-regression-7]]

[^correlation-and-linear-regression-7]: The formula for the general case: The equation that I gave in the main text shows you what a multiple regression model looks like when you include two predictors. Not surprisingly then, if you want more than two predictors all you have to do is add more X terms and more b coefficients. In other words, if you have K predictor variables in the model then the regression equation look like this $$Y_i=b_0+(\sum_{k=1}^{K}b_k X_{ik})+\epsilon_i$$ --->
</section>
</section>
<section id="sec-regression-model-fittedness" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="sec-regression-model-fittedness"><span class="header-section-number">12.6</span> 迴歸模型的適合度</h2>
<p>現在已經知道如何估計線性迴歸模型的係數，那麼要如何知道這個迴歸模型是否有效。例如，根據一號模型，多睡一小時的原作者本人，情緒會大大改善，儘管這可能只是廢話。請記住，迴歸模型只能生成個人情緒的預測值 <span class="math inline">\(\hat{Y}_i\)</span>，實際的情緒量測值是 <span class="math inline">\(Y_i\)</span>。如果兩種數值非常接近，表示這套迴歸模型非常適合預測我的情緒變化。如果兩種數值差異很大，那麼這個模型就並不太適合用來預測我的情緒變化。</p>
<!--- So we now know how to estimate the coefficients of a linear regression model. The problem is, we don't yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction $\hat{Y}_i$ about what my mood is like, but my actual mood is $Y_i$ . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job. --->
<section id="sec-The-R2-value" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="sec-The-R2-value"><span class="header-section-number">12.6.1</span> <span class="math inline">\(R^2\)</span></h3>
<p>我們要再介紹一些數學知識來說明如何評估模型的適合度。首先來認識殘差平方和</p>
<!--- Once again, let's wrap a little bit of mathematics around this. Firstly, we've got the sum of the squared residuals --->
<p><span class="math display">\[SS_{res}=\sum_i (Y_i-\hat{Y_i})^2\]</span></p>
<p>實務上會期望殘差平方和越小越好。具體地說，殘差平方和佔總變異平方和的比例越小越好</p>
<!--- which we would hope to be pretty small. Specifically, what we'd like is for it to be very small in comparison to the total variability in the outcome variable --->
<p><span class="math display">\[SS_{tot}=\sum_i(Y_i-\bar{Y})^2\]</span></p>
<p>談到這裡，我們可以逐步計算這些數值，不過不是用手算。而是使用Excel或其他試算表軟體。在Excel中打開parenthood.csv這份檔案，再另存新檔為parenthood rsquared.xls就能進行計算。計算步驟第一步是計算 <span class="math inline">\(\hat{Y}\)</span> 值，按照以下步驟，我們可以得到以我的睡眠時間預測情緒的簡單迴歸模型：</p>
<ol type="1">
<li>使用公式<code>= 125.97 + (-8.94 * dani.sleep)</code>創建新欄位<code>Y.pred</code>。</li>
<li>創建新欄位<code>(Y-Y.pred)^2</code>，使用公式<code>= (dani.grump - Y.pred)^2</code>計算SS(resid)。</li>
<li>在<code>(Y-Y.pred)^2</code>的最後一列使用公式<code>sum( ( Y-Y.pred)^2 )</code>計算這些值的總和。</li>
<li>在<code>dani.grump</code>的最後一列，計算<code>dani.grump</code>的平均值（留意Excel的函數是’AVERAGE’而不是’mean’）。</li>
<li>創建新欄位<code>(Y - mean(Y))^2 )</code>，使用公式<code>= (dani.grump - AVERAGE(dani.grump))^2</code>。</li>
<li>在<code>(Y - mean(Y))^2 )</code>最後一列，使用<code>= sum( (Y - mean(Y))^2 )</code>計算總和。</li>
<li>在一個空白儲存格中輸入<code>= 1 - (SS(resid) / SS(tot) )</code>，計算R.squared。</li>
</ol>
<p>至此我們算出 <span class="math inline">\(R^2\)</span> 的數值 = 0.8161018。 有些教科書稱呼<span class="math inline">\(R^2\)</span>為<strong>決定係數</strong><a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>，為何叫這個稱號有一個簡單的解釋：預測變項總變異解釋依變項總變異的比例。因此，在這裡得到的 <span class="math inline">\(R^2 = .816\)</span> 代表預測變項<code>my.sleep</code>解釋依變項<code>my.grump</code>總變異的81.6％。</p>
<p>想當然而，如果讀者想計算迴歸模型的 <span class="math inline">\(R^2\)</span> ，實際上不需要自己用 Excel 計算。稍後在 <a href="#sec-jamovi-regression-hypothesis-testing">用 jamovi 執行假設檢定</a>，讀者會看到只要在模組選單裡勾選指定選項即可。現在，讓我們暫時擱置計算問題，我們要談談 <span class="math inline">\(R^2\)</span> 的另一個性質。</p>
<!--- While we're here, let's calculate these values ourselves, not by hand though. Let's use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the $\hat{Y}$ values, and for the simple model that uses only a single predictor we would do the following:

1.  create a new column called 'Y.pred' using the formula '= 125.97 + (-8.94 $\times$ dani.sleep)'
2.  calculate the SS(resid) by creating a new column called '(Y-Y.pred)\^2' using the formula ' = (dani.grump - Y.pred)\^2 '.
3.  Then, at the bottom of this column calculate the sum of these values, i.e. ' sum( ( Y-Y.pred)\^2 ) .
4.  At the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ' AVERAGE ' rather than 'mean' in its function).
5.  Then create a new column, called ' (Y - mean(Y))\^2 )' using the formula ' = (dani.grump - Accccccccccc(dani.grump))\^2 '.
6.  Then, at the bottom of this column calculate the sum of these values, i.e. 'sum( (Y - mean(Y))\^2 )'.
7.  Calculate R.squared by typing into a blank cell the following: '= 1 - (SS(resid) / SS(tot) )'.

This gives a value for $R^2$ of '0.8161018'. The $R^2$ value, sometimes called the **coefficient of determination**[^correlation-and-linear-regression-8] has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained $R^2 = .816$ means that the predictor (my.sleep) explains $81.6\%$ of the variance in the outcome (my.grump).

[^correlation-and-linear-regression-8]: And by "sometimes" I mean "almost never". In practice everyone just calls it "R-squared".

Naturally, you don't actually need to type all these commands into Excel yourself if you want to obtain the $R^2$ value for your regression model. As we'll see later on in the section on [Running the hypothesis tests in jamovi], all you need to do is specify this as an option in jamovi. However, let's put that to one side for the moment. There's another property of $R^2$ that I want to point out. --->
</section>
<section id="迴歸與相關的關聯" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="迴歸與相關的關聯"><span class="header-section-number">12.6.2</span> 迴歸與相關的關聯</h3>
<p>現在可以重新回顧在這一章開始，曾說到迴歸與相關基本上是同一回事<!---，在迄今為止我討論過的這種非常簡單的形式--->。在接下來的小單元裡，符號 <span class="math inline">\(r\)</span> 大多是表示皮爾森相關。那麼皮爾森相關係數 <span class="math inline">\(r\)</span> 和線性迴歸的 <span class="math inline">\(R^2\)</span> 有存在某種關係嗎？當然有：只要將相關係數開平方， <span class="math inline">\(r^2\)</span> 與只有一個預測變項的線性迴歸 <span class="math inline">\(R^2\)</span> 數值是相同的。換句話說，計算皮爾森相關與計算僅有一個預測變項的線性迴歸模型基本上是相同的。</p>
<!--- At this point we can revisit my earlier claim that regression, in this very simple form that I've discussed so far, is basically the same thing as a correlation. Previously, we used the symbol $r$ to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient $r$ and the $R^2$ value from linear regression? Of course there is: the squared correlation $r^2$ is identical to the $R^2$ value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable. --->
</section>
<section id="校正後-r2" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="校正後-r2"><span class="header-section-number">12.6.3</span> 校正後 <span class="math inline">\(R^2\)</span></h3>
<blockquote class="blockquote">
<p>譯註：只有分析多元迴歸的場景，才要了解校正後 <span class="math inline">\(R^2\)</span>。</p>
</blockquote>
<p>在繼續到下個單元之前，我最後要指出的是，統計實務通常會報告一個稱為“校正後 <span class="math inline">\(R^2\)</span> ”的計量值。計算及報告校正後 <span class="math inline">\(R^2\)</span> 值的理由是，到將兩個以上預測變項添加到模型中，總是會增加（或至少不降低） <span class="math inline">\(R^2\)</span> 。</p>
<p>[額外技術細節<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>]</p>
<p>校正的目的是為了處理自由度。校正後 <span class="math inline">\(R^2\)</span> 的主用用途是，往模型添加更多預測變後，只有能提高模型預測能力的新變項，才會顯著增加校正後 <span class="math inline">\(R^2\)</span>的數值。然而，校正後 <span class="math inline">\(R^2\)</span>就無法像一開始的 <span class="math inline">\(R^2\)</span> 那樣的直接解釋。據我所知，調整後的 <span class="math inline">\(R^2\)</span> 沒有任何相等意義的解釋。</p>
<p>那麼統計實務中應該要報告 <span class="math inline">\(R^2\)</span> 還是校正後 <span class="math inline">\(R^2\)</span>？這可能是因人而異。如果同學比較想解釋報告裡的數值，那麼 <span class="math inline">\(R^2\)</span> 較好。如果在乎校正模型的預測偏差，那麼校正後 <span class="math inline">\(R^2\)</span> 可能比較好。就我(原作者)自己而言，我更喜歡 <span class="math inline">\(R^2\)</span>，因為我覺得最重要的是能夠解釋模型預測能力的計量。此外，我們將在 <a href="#sec-regression-hypothesis-testing"><span>小單元&nbsp;12.7</span></a> <em>迴歸模型的假設檢定</em>這個小單元看到，如果想知道添加預測變項後增加的 <span class="math inline">\(R^2\)</span> 是由於機遇還是因為模型預測能力真的改善了，那麼我們可以用假設檢定來做判斷。</p>
<!--- One final thing to point out before moving on. It's quite common for people to report a slightly different measure of model performance, known as "adjusted $R^2$". The motivation behind calculating the adjusted $R^2$ value is the observation that adding more predictors into the model will always cause the $R^2$ value to increase (or at least not decrease).

[Additional technical detail[^correlation-and-linear-regression-9]]

[^correlation-and-linear-regression-9]: The adjusted $R^2$ value introduces a slight change to the calculation, as follows. For a regression model with $K$ predictors, fit to a data set containing $N$ observations, the adjusted $R^2$ is: $$\text{adj.}R^2=1-(\frac{SS_{res}}{SS_{tot}} \times \frac{N-1}{N-K-1})$$

This adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted $R^2$ value is that when you add more predictors to the model, the adjusted $R^2$ value will only increase if the new variables improve the model performance more than you'd expect by chance. The big disadvantage is that the adjusted $R^2$ value can't be interpreted in the elegant way that $R^2$ can. $R^2$ has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted $R^2$.

An obvious question then is whether you should report $R^2$ or adjusted $R^2$ . This is probably a matter of personal preference. If you care more about interpretability, then $R^2$ is better. If you care more about correcting for bias, then adjusted $R^2$ is probably better. Speaking just for myself, I prefer $R^2$. My feeling is that it's more important to be able to interpret your measure of model performance. Besides, as we'll see in [Hypothesis tests for regression models], if you're worried that the improvement in $R^2$ that you get by adding a predictor is just due to chance and not because it's a better model, well we've got hypothesis tests for that. --->
</section>
</section>
<section id="sec-regression-hypothesis-testing" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="sec-regression-hypothesis-testing"><span class="header-section-number">12.7</span> 迴歸模型的假設檢定</h2>
<p>至此我們已經學到什麼是迴歸模型，如何估計迴歸模型的係數，以及量化模型預測效能的方法（順便說一下，相關係數與迴歸係數就是一種效果量的估計值）。接下來學習課題的是假設檢定。我們要學習兩種不同（但相關）的假設檢定：一種是檢驗包合所有預測變項的迴歸模型是否顯著優於只有截距的模型，另一種是我們檢驗只有單一預測變項的模型，迴歸係數是否顯著不等於零。</p>
<section id="檢定所有預測變項的模型" class="level3" data-number="12.7.1">
<h3 data-number="12.7.1" class="anchored" data-anchor-id="檢定所有預測變項的模型"><span class="header-section-number">12.7.1</span> 檢定所有預測變項的模型</h3>
<blockquote class="blockquote">
<p>譯註：只有分析多元迴歸的場景，才要進行這種檢定。內容文字編修排在本書最後階段進行。</p>
</blockquote>
<p>好吧，假設你已經估計了你的迴歸模型。你可能會嘗試的第一個假設檢定是虛無假設，即預測變項和結果之間沒有關係，而對立假設是資訊的分佈完全符合迴歸模型的預測。</p>
<p>[額外的技術細節<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>]</p>
<p>我們將在 <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>單元&nbsp;13</span></a> 中看到更多 F 統計量，但目前只需知道我們可以將較大的 F 值解釋為虛無假設與對立假設相比表現不佳。過一會兒，我將向您展示如何用 jamovi 輕鬆進行檢驗，但首先讓我們看一下單個迴歸係數的檢驗。</p>
</section>
<section id="單一迴歸係數的檢定" class="level3" data-number="12.7.2">
<h3 data-number="12.7.2" class="anchored" data-anchor-id="單一迴歸係數的檢定"><span class="header-section-number">12.7.2</span> 單一迴歸係數的檢定</h3>
<p>前一節介紹的 F 檢定對於檢查整個線性模型是否優於隨機截距模型很有用。如果您的迴歸模型並未在 F 檢定看到顯著結果，那麼這套迴歸模型可能不是有效解讀資料的好模型（或者是要分析的資料可能並不夠好）。然而，儘管這個檢定失敗是表示模型是否可用的明顯指標，但是通過檢定（也就是拒絕虛無假設）並不表示這個模型是真的好模型！也許同學會想知道為什麼？答案在前面 <a href="#sec-multiple-regression"><span>小單元&nbsp;12.5</span></a> <em>多元線性迴歸</em> 已經有討論過的。</p>
<p>注意一下 <a href="#tbl-tab10-4">表&nbsp;<span>12.4</span></a> 的數值，與 dani.sleep 變項的迴歸係數估計數值（<span class="math inline">\(-8.95\)</span>）相比，baby.sleep 變項的迴歸係數估計數值非常小（<span class="math inline">\(0.01\)</span>）。考慮到這兩個變項的度量尺度都是一樣的（都是以“睡眠小時數”），我發現這很有啟發性。其實我看了 <a href="#tbl-tab10-4">表&nbsp;<span>12.4</span></a> 就有想到，要預測我的沮喪程度，真正重要的變項應該只有我自己的睡眠時間長度。我們可以使用之前學到的假設檢定方法，確認我的懷疑<!---即 t 檢定---><a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>。我們想檢定的虛無假設是設定迴歸係數為零（<span class="math inline">\(b = 0\)</span>），與迴歸係數不是零（<span class="math inline">\(b \neq 0\)</span>）的對立假設進行檢定。也就是說：</p>
<p><span class="math display">\[H_0:b=0\]</span> <span class="math display">\[H_1:b \neq 0\]</span></p>
<p>這個檢定要如何進行？好吧，如果還記得中心極限定理，同學可能會猜到迴歸係數的估計值<span class="math inline">\(\hat{b}\)</span>是一種取樣分佈，而且是以 <span class="math inline">\(b\)</span> 為中心的常態分佈。這表示如果虛無假設是真的，那麼<span class="math inline">\(\hat{b}\)</span>的取樣分佈平均值為零並且標準差是未知數。假設我們可以找到迴歸係數的標準誤差估計值，<span class="math inline">\(se(\hat{b})\)</span>，那麼我們就很幸運。這正好可以用 <a href="11-Comparing-two-means.html"><span>單元&nbsp;11</span></a> 介紹的單一樣本 t 檢定處理。現在可以定義以下的 t 統計值</p>
<p><span class="math display">\[t=\frac{\hat{b}}{SE(\hat{b})}\]</span></p>
<p>這裡不詳細說明為什麼能這樣做的原因，但在這種狀況，自由度是 <span class="math inline">\(df = N - K - 1\)</span>。令初學者厭煩的通常是，迴歸係數的標準誤估計值，<span class="math inline">\(se(\hat{b})\)</span>，並不像 <a href="11-Comparing-two-means.html"><span>單元&nbsp;11</span></a> 介紹的單一樣本 t 檢定的平均值標準誤那樣容易計算。真實的公式長得有點醜陋，看起來並不那麼平易近人。<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> 對於我們要真正完成的目標，只需要知道迴歸係數估計值的標準誤取決於預測變項和依變項，並且要留意有沒有違反變異數相等的適用條件 （稍後 <a href="#sec-regression-assumptions"><span>小單元&nbsp;12.9</span></a> 就會討論）。</p>
<p>無論如何，這個t統計值可以按照 <a href="11-Comparing-two-means.html"><span>單元&nbsp;11</span></a> 介紹的檢定方法解釋結果。若是設定做雙尾檢定（也就是說，你不在乎是b &gt; 0還是b &lt; 0），那麼極端的t值（即遠小於零或遠大於零的值）表示你應該拒絕虛無假設。</p>
</section>
<section id="sec-jamovi-regression-hypothesis-testing" class="level3" data-number="12.7.3">
<h3 data-number="12.7.3" class="anchored" data-anchor-id="sec-jamovi-regression-hypothesis-testing"><span class="header-section-number">12.7.3</span> 用 jamovi 執行假設檢定</h3>
<p>要計算以上介紹的統計量數，只需要在jamovi迴歸模組選單勾選對應的選項。要選擇的選項如同 <a href="#fig-fig10-15">圖&nbsp;<span>12.15</span></a> 的示範，會得到一系列有用的報表輸出。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-15" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-15.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.15: jamovi示範畫面，顯示一個多元線性迴歸分析，請留意其中勾選的選項</figcaption>
</figure>
</div>
</div>
</div>
<p>在jamovi分析結果的“模型係數”表格(Model Coefficients)顯示迴歸模型的係數。此表中的每一行都是對應迴歸模型的其中一個係數。第一行是截距，後面每一行是每個預測變項的檢定結果。每一列標示各種統計訊息。第一列是<span class="math inline">\(b\)</span>的實際估計值（例如，截距為<span class="math inline">\(125.97\)</span>，預測變項dani.sleep 為<span class="math inline">\(-8.95\)</span>）。第二列是標準誤的估計值<span class="math inline">\(\hat{\sigma}_b\)</span>。第三和第四列是關於係數估計值的95%信賴區間的下限和上限（稍後對此有更多說明）。第五列是t統計值，值得注意的是，在這個表格中，<span class="math inline">\(t=\frac{\hat{b}} {se({\hat{b}})}\)</span>每次分析結果都是成立的。最後一列呈現這些檢定結果的<em>p</em>值。<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<p>模型係數表唯一沒有列出的是t檢定的自由度，不過其值始終是<span class="math inline">\(N - K - 1\)</span>，並且輸出到報表標題後的“模型適合度度量”(Model Fit Measures)表格中<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>。從這個表格中，我們可以看到模型的表現顯著優於機會水平（<span class="math inline">\(F(2,97) = 215.24, p&lt; .001\)</span>），其實這並不奇怪：<span class="math inline">\(R^2 = .81\)</span>值表示迴歸模型能解釋依變項變異量的<span class="math inline">\(81\%\)</span>（調整後的<span class="math inline">\(R^2\)</span>為<span class="math inline">\(82\%\)</span> ）。然而，當我們回顧每個個別係數的t檢定時，我們有相當有力的證據表明baby.sleep變項沒有顯著效果。這個模型的主要預測效力都是來由dani.sleep變項。綜合這些結果，我們可以結論這個多元迴歸模型實際上並不是能有效解 資料的模型，最好的模型應該排除baby.sleep這個預測變項。換句話說，開始的簡單迴歸模型是更好的模型。</p>

<!--- So far we've talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero. 

### Testing the model as a whole

Okay, suppose you've estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.

[Additional technical detail[^correlation-and-linear-regression-10]]

[^correlation-and-linear-regression-10]: Formally, our "null model" corresponds to the fairly trivial "regression" model in which we include 0 predictors and only include the intercept term $b_0$: $H_0:Y_0=b_0+\epsilon_i$ If our regression model has $K$ predictors, the "alternative model" is described using the usual formula for a multiple regression model: $$H_1:Y_i=b_0+(\sum_{k=1}^K b_k X_{ik})+\epsilon_i$$ How can we test these two hypotheses against each other? The trick is to understand that it's possible to divide up the total variance $SStot$ into the sum of the residual variance SSres and the regression model variance SSmod. I'll skip over the technicalities, since we'll get to that later when we look at ANOVA in @sec-Comparing-several-means-one-way-ANOVA. But just note that $SS_{mod}=SS_{tot}-SS_{res}$ And we can convert the sums of squares into mean squares by dividing by the degrees of freedom. $$MS_{mod}=\frac{SS_{mod}}{df_{mod}}$$ $$MS_{res}=\frac{SS_{res}}{df_{res}}$$ So, how many degrees of freedom do we have? As you might expect the df associated with the model is closely tied to the number of predictors that we've included. In fact, it turns out that $df_mod = K$. For the residuals the total degrees of freedom is $df_res = N - K - 1$. Now that we've got our mean square values we can calculate an F-statistic like this $$F=\frac{MS_{mod}}{MS_{res}}$$ and the degrees of freedom associated with this are $K$ and $N - K - 1$.

We'll see much more of the F statistic in @sec-Comparing-several-means-one-way-ANOVA, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I'll show you how to do the test in jamovi the easy way, but first let's have a look at the tests for the individual regression coefficients.

### Tests for individual coefficients

The F-test that we've just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn't produce a significant result for the F-test then you probably don't have a very good regression model (or, quite possibly, you don't have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn't imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the [Multiple linear regression] model we have already looked at (@tbl-tab10-4)

I can't help but notice that the estimated regression coefficient for the baby.sleep variable is tiny ($0.01$), relative to the value that we get for dani.sleep ($-8.95$). Given that these two variables are absolutely on the same scale (they're both measured in "hours slept"), I find this illuminating. In fact, I'm beginning to suspect that it's really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we're interested in has a null hypothesis that the true regression coefficient is zero ($b = 0$), which is to be tested against the alternative hypothesis that it isn't ($b \neq 0$). That is:

$$H_0:b=0$$ $$H_1:b \neq 0$$

How can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of $\hat{b}$, the estimated regression coefficient, is a normal distribution with mean centred on $b$. What that would mean is that if the null hypothesis were true, then the sampling distribution of $\hat{b}$ has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, $se(\hat{b})$, then we're in luck. That's exactly the situation for which we introduced the one-sample t-test back in @sec-Comparing-two-means. So let's define a t-statistic like this

$$t=\frac{\hat{b}}{SE(\hat{b})}$$

I'll skip over the reasons why, but our degrees of freedom in this case are $df = N - K - 1$. Irritatingly, the estimate of the standard error of the regression coefficient, $se(\hat{b})$, is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in @sec-Comparing-two-means. In fact, the formula is somewhat ugly, and not terribly helpful to look at.[^correlation-and-linear-regression-11] For our purposes it's sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).

[^correlation-and-linear-regression-11]: For advanced readers only. The vector of residuals is $\epsilon=y - X\hat{b}$. For K predictors plus the intercept, the estimated residual variance is $\hat{\sigma}^2 = \frac{\epsilon^{'}\epsilon}{(N - K - 1)}$. The estimated covariance matrix of the coefficients is $\hat{\sigma}^{2}(X^{'}X)^{-1}$, the main diagonal of which is $se(\hat{b})$, our estimated standard errors.

In any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in @sec-Comparing-two-means. Assuming that you have a two-sided alternative (i.e., you don't really care if b $>$ 0 or b $<$ 0), then it's the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.

### Running the hypothesis tests in jamovi

To compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in @fig-fig10-15, we get a whole bunch of useful output.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked](images/fig12-15.png){#fig-fig10-15eng fig-align='left' width=80%}
:::
:::


The 'Model Coefficients' at the bottom of the jamovi analysis results shown in @fig-fig10-15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of $b$ (e.g., $125.97$ for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate $\hat{\sigma}_b$. The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it's worth noticing that in this table $t=\frac{\hat{b}} {se({\hat{b}})}$ every time. Finally, the last column gives you the actual p-value for each of these tests.[^correlation-and-linear-regression-12]

[^correlation-and-linear-regression-12]: Note that, although jamovi has done multiple tests here, it hasn't done a Bonferroni correction or anything (see @sec-Comparing-several-means-one-way-ANOVA). These are standard one-sample t-tests with a two-sided alternative. If you want to make corrections for multiple tests, you need to do that yourself.

The only thing that the coefficients table itself doesn't list is the degrees of freedom used in the t-test, which is always $N - K - 1$ and is listed in the table at the top of the output, labelled 'Model Fit Measures'. We can see from this table that the model performs significantly better than you'd expect by chance ($F(2,97) = 215.24, p< .001$), which isn't all that surprising: the $R^2 = .81$ value indicate that the regression model accounts for $81\%$ of the variability in the outcome measure (and $82\%$ for the adjusted $R^2$ ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You'd probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model.--->
</section>
</section>
<section id="sec-regression-estimations" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="sec-regression-estimations"><span class="header-section-number">12.8</span> 迴歸係數的估計值</h2>
<p>在討論線性迴歸的適用條件，以及如何檢查一種模型是否滿足條件之前，這裡先簡單討論兩個與迴歸係數有關的主題。首先是如何計算迴歸係數的信賴區間。然後是如何確定哪個預測變項最重要。</p>
<section id="迴歸係數的信賴區間" class="level3" data-number="12.8.1">
<h3 data-number="12.8.1" class="anchored" data-anchor-id="迴歸係數的信賴區間"><span class="header-section-number">12.8.1</span> 迴歸係數的信賴區間</h3>
<p>就像任何人口變項一樣，迴歸係數b無法從樣本資料精確地估算出來；這就是為什麼需要使用假設檢定的一部分原因。有鑑於此，信賴區間能夠呈現捕捉<span class="math inline">\(b\)</span>真實數值的不確定性，是非常有用的工具。這在嘗試找出變項<span class="math inline">\(X\)</span>與變項<span class="math inline">\(Y\)</span>之間的關係強度的研究問題尤其有用，因為在這些研究裡，主要關注的是迴歸權重<span class="math inline">\(b\)</span>(regression weight)。</p>
<p>[額外技術細節<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>]</p>
<p>在jamovi的操作界面，我們可以指定“95％信賴區間”，如 <a href="#fig-fig10-15">圖&nbsp;<span>12.15</span></a> 的示範，如果想要更嚴謹的話，我們可以輕鬆選擇另一種區間，例如“99％信賴區間”。</p>
</section>
<section id="標準化迴歸係數的計算方法" class="level3" data-number="12.8.2">
<h3 data-number="12.8.2" class="anchored" data-anchor-id="標準化迴歸係數的計算方法"><span class="header-section-number">12.8.2</span> 標準化迴歸係數的計算方法</h3>
<p>有經驗的使用者可能還會計算“標準化”迴歸係數，通常報告中用<span class="math inline">\(\beta\)</span>表示。標準化係數的基本原理是：在很多情況下，每個變項的測量尺度是不一樣的。例如，如果有個迴歸模型要探討受教育程度（受教育年數）和收入作為預測變項，來預測受測者的智力測驗得分。顯然，受教育程度和收入的計量尺度是不相同的。一般人的教育年限可能只有10多年，而收入差距可能高達10,000美元（或更多）。計量單位對迴歸係數有很大影響，只有預測變項和依變項的計量單位一致時，迴歸係數才具有意義，否則比較不同預測變項的迴歸係數將會非常困難。然而，有時我們希望能比較不同變項的係數。具體來說，研究者最想找到那些預測變項與依變項之間相關性最強力的標準衡量指標，這就是為什麼有<strong>標準化迴歸係數</strong>。</p>
<p><strong>標準化迴歸係數</strong>的思路很簡單；如果在執行迴歸分析之前將所有變項轉換為z分數，標準化係數就是您會得到的係數。<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>這裡的想法是，將所有預測變項數值轉換為z分數，使得迴歸模型的生成的機率分佈具有可對應的比例，進而消除不同尺度的變項產生的問題。無論變項的原始尺度是什麼，<span class="math inline">\(\beta\)</span>值為1都代表增加預測變項的1個標準差，就是導致依變項的對應數值增加1個標準差。因此，如果預測變項A的<span class="math inline">\(\beta\)</span>絕對值大於預測變項B的<span class="math inline">\(\beta\)</span>，研究者至少能主張預測變項A與依變項的相關性更強。值得小心的是，對於所有變項變異基本相同的這個條件 ，其實非常依賴“1個標準差變化”，並不是什麼形式的變項都可見。</p>
<p>[額外技術細節<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>]</p>
<p>為了讓簡化分析程序，只要在jamovi模組選單裡，從”Model Coefficients”的選項中勾選”Standardized estimate”，就能計算<span class="math inline">\(\beta\)</span>，如同 <a href="#fig-fig10-16">圖&nbsp;<span>12.16</span></a> 的示範。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-16" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-16.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.16: 多元線性迴歸的標準化係數及其95％信賴區間</figcaption>
</figure>
</div>
</div>
</div>
<p>從輸出結果可明顯看出，dani.sleep是比baby.sleep有更強預測效力的變項。然而，這可能正是一個更適合使用原始係數b而不是標準化係數<span class="math inline">\(\beta\)</span>的完美例子。畢竟，我的睡眠時間和寶寶的睡眠時間已經是同一個計量尺度。為什麼要還要將它們轉換為z分數來讓事情變得更複雜呢？</p>
<!--- Before moving on to discuss the assumptions underlying linear regression and what you can do to check if they're being met, there's two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I'll discuss the somewhat murky question of how to determine which predictor is most important.

### 迴歸係數的信賴區間

Like any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that's part of why we need hypothesis tests. Given this, it's quite useful to be able to report confidence intervals that capture our uncertainty about the true value of $b$. This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable $X$ is related to variable $Y$ , since in those situations the interest is primarily in the regression weight $b$.

[Additional technical detail[^correlation-and-linear-regression-13]]

[^correlation-and-linear-regression-13]: Fortunately, confidence intervals for the regression weights can be constructed in the usual fashion $CI(b)=\hat{b} \pm (t_{crit} \times SE(\hat{b}))$ where $se(\hat{b})$ is the standard error of the regression coefficient, and t_crit is the relevant critical value of the appropriate t distribution. For instance, if it's a 95% confidence interval that we want, then the critical value is the $97.5$th quantile of a t distribution with $N -K -1$ degrees of freedom. In other words, this is basically the same approach to calculating confidence intervals that we've used throughout.

In jamovi we had already specified the '95% Confidence interval' as shown in @fig-fig10-15, although we could easily have chosen another value, say a '99% Confidence interval' if that is what we decided on.

### 標準化迴歸係數的計算方法

One more thing that you might want to do is to calculate "standardised" regression coefficients, often denoted $\beta$. The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people's $IQ$ scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by $10,000s$ of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what **standardised coefficients** aim to do.

The basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you'd converted all the variables to z-scores before running the regression.[^correlation-and-linear-regression-14] The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a $\beta$ value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of $\beta$ than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that's the idea. It's worth being a little cautious here, since this does rely very heavily on the assumption that "a 1 standard deviation change" is fundamentally the same kind of thing for all variables. It's not always obvious that this is true.

[^correlation-and-linear-regression-14]: Strictly, you standardise all the *regressors*. That is, every "thing" that has a regression coefficient associated with it in the model. For the regression models that I've talked about so far, each predictor variable maps onto exactly one regressor, and vice versa. However, that's not actually true in general and we'll see some examples of this later in @sec-Factorial-ANOVA. But, for now we don't need to care too much about this distinction.

[Additional technical detail[^correlation-and-linear-regression-15]]

[^correlation-and-linear-regression-15]: Leaving aside the interpretation issues, let's look at how it's calculated. What you could do is standardise all the variables yourself and then run a regression, but there's a much simpler way to do it. As it turns out, the $\beta$ coefficient for a predictor $X$ and outcome $Y$ has a very simple formula, namely $\beta_X=b_X \times \frac{\sigma_X}{\sigma_Y}$ where $\sigma_X$ is the standard deviation of the predictor, and σY is the standard deviation of the outcome variable Y. This makes matters a lot simpler.

To make things even simpler, jamovi has an option that computes the $\beta$ coefficients for you using the 'Standardized estimate' checkbox in the 'Model Coefficients' options, see results in @fig-fig10-16.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![Standardised coefficients, with 95% confidence intervals, for multiple linear regression](images/fig12-16.png){#fig-fig10-16eng fig-align='left' width=80%}
:::
:::


These results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients $\beta$. After all, my sleep and the baby's sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores? --->
</section>
</section>
<section id="sec-regression-assumptions" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="sec-regression-assumptions"><span class="header-section-number">12.9</span> 迴歸模型的適用條件</h2>
<p>線性迴歸模型必須符合幾個適用條件(assumptions)，才能解讀分析結果。在 <a href="#sec-regression-Model-diagnosis"><span>小單元&nbsp;12.10</span></a> <em>診斷迴歸模型的適用條件</em>這個單元，我們將會學習如何檢查這些假設是否得到滿足，首先簡單說明每個假設的涵意。<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></p>
<ul>
<li><strong>線性</strong>。線性迴歸模型的最基本的適用條件是<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>之間的關係必須是線性的！無論是簡單迴歸還是多元迴歸，我們都要假設變項之間的關係是線性的。</li>
<li><strong>獨立性</strong>：這是說變項資料的殘差彼此獨立。實際上這是條“總括一切”的適用條件，更白話地說“在殘差中找不到任何有意思的東西了”。如果還能分析出一些意料之外的資訊（例如，所有殘差都與某些未測量的變項存在明顯相關性），可能會破壞分析後的結論。</li>
<li><strong>常態性</strong>。就像驅動許多統計方法的機率模型一樣，基本的簡單或多元線性迴歸也要符合常態性。具體來說，這條是指殘差的次數分佈逼近或符合常態分佈。實際上，即使預測變項<span class="math inline">\(X\)</span>和依變項<span class="math inline">\(Y\)</span>的實際資料次數分佈不符合常態分佈，只要殘差<span class="math inline">\(\epsilon\)</span>的次數分佈符合常態的就可以了。 <a href="#sec-regression-Model-diagnosis"><span>小單元&nbsp;12.10</span></a> <em>診斷迴歸模型的適用條件</em>有進一步說明及示範。</li>
<li><strong>變異相等</strong>（或稱’同質性’）。嚴格來說，符合這個條件 的迴歸模型生成的所有殘差<span class="math inline">\(\epsilon_i\)</span>，都是來自一個平均值為0的常態分佈，更重要的是，每個殘差來源的機率分佈標準差<span class="math inline">\(\sigma\)</span>都是相同的。在實務中，檢驗每個殘差都是來自同一個機率分佈是不大可能做到的事。相反地，我們真正關心的是殘差的標準差相對於所有預測值<span class="math inline">\(\hat{Y}\)</span>是相同的，特別是多元迴歸模型的每個預測變項<span class="math inline">\(X\)</span>所生成的預測值是相同的。</li>
</ul>
<p>所以，要執行有效的線性迴歸分析，首先要檢查是不是符合這四個適用條件 （剛好可以縮寫成<strong>LINE</strong>）。此外，還有一些需要檢查的條件 ：</p>
<ul>
<li><p>沒有“不良”極端值。其實這並非必要的適用條件，但是極端值可能造成潛在的問題。就是迴歸模型雖然不會因為一兩個異常極端值，造成不符合上述任何一條適用條件(譯註~特別是<strong>線性</strong>)，但是在某些情況下會引起對模型的適當性和資料可靠性的質疑。詳細說明及示範請見 <a href="#sec-three-kinds-of-anomalous-data"><span>小單元&nbsp;12.10.2</span></a> <em>三種異常資料</em>。</p></li>
<li><p>預測變項之間無相關性。執行多元迴歸模型分析時，我們不希望預測變項彼此間的存在高度相關。這並非迴歸模型的“必要”適用條件，但在統計實務是必需的。預測變項之間相關性過強（通常稱為“共線性”）可能會造成錯誤解讀分析結果。詳細說明及示範請見 <a href="#sec-check-regression-collinearity"><span>小單元&nbsp;12.10.4</span></a> <em>檢查共線性</em>。</p></li>
</ul>
<!--- The linear regression model that I've been discussing relies on several assumptions. In [Model checking] we'll talk a lot more about how to check that these assumptions are being met, but first let's have a look at each of them.

-   **L**inearity. A pretty fundamental assumption of the linear regression model is that the relationship between $X$ and $Y$ actually is linear! Regardless of whether it's a simple regression or a multiple regression, we assume that the relationships involved are linear.
-   **I**ndependence: residuals are independent of each other. This is really just a "catch all" assumption, to the effect that "there's nothing else funny going on in the residuals". If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.
-   **N**ormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It's actually okay if the predictors $X$ and the outcome $Y$ are non-normal, so long as the residuals $\epsilon$ are normal. See the [Checking the normality of the residuals] section.
- **E**quality (or 'homogeneity') of variance. Strictly speaking, the regression model assumes that each residual $\epsilon_i$ is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation $\sigma$ that is the same for every single residual. In practice, it's impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of $\hat{Y}$ , and (if we're being especially paranoid) all values of every predictor $X$ in the model.

So, we have four main assumptions for linear regression (that neatly form the acronym '**LINE**'). And there are also a couple of other things we should also check for:

-   Uncorrelated predictors. The idea here is that, in a multiple regression model, you don't want your predictors to be too strongly correlated with each other. This isn't "technically" an assumption of the regression model, but in practice it's required. Predictors that are too strongly correlated with each other (referred to as "collinearity") can cause problems when evaluating the model. See the [Checking for collinearity] section.
-   No "bad" outliers. Again, not actually a technical assumption of the model (or rather, it's sort of implied by all the others), but there is an implicit assumption that your regression model isn't being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on [Three kinds of anomalous data]. --->
</section>
<section id="sec-regression-Model-diagnosis" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="sec-regression-Model-diagnosis"><span class="header-section-number">12.10</span> 診斷迴歸模型的適用條件</h2>
<blockquote class="blockquote">
<p>譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。</p>
</blockquote>
<p>本節的主要焦點是<strong>迴歸診斷</strong>，這個術語是指檢查迴歸模型假設是否得到滿足、在假設被違反時如何修正模型以及一般情況下檢查是否存在不尋常情況的技術。我將這稱為模型檢查的“藝術”，理由很充分。這並不容易，儘管有許多相當標準化的工具可以用來診斷甚至可能治愈困擾模型的問題（如果存在的話！），但在這方面真的需要運用一定程度的判斷力。在檢查這件事情或那件事情的所有細節中容易迷失，試圖記住所有不同的事物是相當耗費精力的。這會產生一個非常令人討厭的副作用，很多人在試圖學習所有工具時會感到沮喪，所以他們決定不做任何模型檢查。這有點令人擔憂！</p>
<p>在本節中，我描述了一些方法，用於檢查迴歸模型是否按照預期工作。它並沒有涵蓋所有您可能做的事情，但仍然比我在實踐中看到的大多數人所做的事情要詳細得多，即使在我的初級統計課程中，我通常也不會涵蓋所有這些內容。但是，我確實認為您應該了解可供您使用的工具，所以我將在這裡嘗試介紹一部分。最後，我應該指出，本節很大程度上借鑒了 <span class="citation" data-cites="Fox2011">Fox &amp; Weisberg (<a href="References.html#ref-Fox2011" role="doc-biblioref">2011</a>)</span> ，即與在 R 中進行迴歸分析的“car”包相關的書籍。 “car”包以提供一些出色的迴歸診斷工具而著稱，而該書本身以極為清晰的方式談論了這些工具。我不想聽起來太過於誇大，但我確實認為即使在 R 和不是 jamovi 的情況下， <span class="citation" data-cites="Fox2011">Fox &amp; Weisberg (<a href="References.html#ref-Fox2011" role="doc-biblioref">2011</a>)</span> 都值得一讀。</p>
<section id="三種殘差" class="level3" data-number="12.10.1">
<h3 data-number="12.10.1" class="anchored" data-anchor-id="三種殘差"><span class="header-section-number">12.10.1</span> 三種殘差</h3>
<p>大多數迴歸診斷都圍繞著觀察殘差，到目前為止，你可能已經對統計學形成了足夠悲觀的理論，能夠猜到，正因為我們非常關心殘差，我們可能會考慮幾種不同類型的殘差。特別地，在本節中，我們將提到以下三種殘差：“普通殘差”、“標準化殘差”和“學生化殘差”。還有第四種你會在一些圖中看到的，稱為“皮爾森殘差”。然而，對於我們在本章中討論的模型，皮爾森殘差與普通殘差相同。</p>
<p>首先，我們關心的最簡單類型的殘差是<strong>普通殘差</strong>。這些就是我在本章前面一直提到的實際原始殘差。普通殘差僅僅是擬合值 <span class="math inline">\(\hat{Y}_i\)</span> 和觀察值 <span class="math inline">\(Y_i\)</span> 之間的差。我一直用符號 <span class="math inline">\(\epsilon_i\)</span> 表示第 i 個普通殘差，並且我將繼續堅持使用它。考慮到這一點，我們有非常簡單的方程式</p>
<p><span class="math display">\[\epsilon_i=Y_i-\hat{Y_i}\]</span></p>
<p>這當然是我們之前看到的，除非我特別提到其他類型的殘差，否則我就是在談論這個。所以這裡沒有新的東西。我只是想重申一下。使用普通殘差的一個缺點是，它們總是在不同的尺度上，取決於結果變項是什麼以及迴歸模型有多好。也就是說，除非你決定在沒有截距項的情況下運行迴歸模型，否則普通殘差的均值將為 0，但每個迴歸的方差都不同。在很多情境下，特別是當你只對殘差的模式感興趣，而不是它們的實際值時，估計<strong>標準化殘差</strong>很方便，這些殘差經過規範化後標準差為 1。</p>
<p>[額外技術細節<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>]</p>
<p>第三種殘差是<strong>學生化殘差</strong>（也稱為 “剃刀切割殘差”），它們比標準化殘差更高級。同樣，目的是將普通殘差除以某個量，以估計殘差的某種標準化概念。<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></p>
<p>在繼續之前，我應該指出，即使這些殘差是幾乎所有迴歸診斷的核心，你通常也不需要自己獲得這些殘差。大多數時候，提供診斷或假設檢查的各種選項將為您處理這些計算。即使如此，知道如何實際自己獲得這些東西，以防你需要進行一些非標準操作，總是很好的。</p>
</section>
<section id="sec-three-kinds-of-anomalous-data" class="level3" data-number="12.10.2">
<h3 data-number="12.10.2" class="anchored" data-anchor-id="sec-three-kinds-of-anomalous-data"><span class="header-section-number">12.10.2</span> 三種異常資料</h3>
<p>使用線性迴歸模型時，您可能會遇到一個危險，那就是您的分析可能會對一小部分”不尋常”或”異常”的觀測值過於敏感。我之前在 <a href="05-Drawing-graphs.html#sec-Using-box-plots-to-detect-outliers"><span>小單元&nbsp;5.2.3</span></a> 的上下文中討論過這個想法，當時是在討論用 ‘探索’ - ‘描述統計’ 下的 boxplot 選項自動識別的異常值，但這次我們需要更精確。在線性迴歸的背景下，有三個概念上不同的方式可以將觀測值稱為”異常”。這三者都很有趣，但對你的分析有很不同的影響。</p>
<p>第一種不尋常的觀測值是<strong>異常值</strong>。在這種情況下，異常值的定義是與迴歸模型預測的結果相差很大的觀測值。<a href="#fig-fig10-17">圖&nbsp;<span>12.17</span></a> 中有一個例子。在實踐中，我們通過說一個異常值是具有非常大的Studentised殘差的觀測值，<span class="math inline">\(\epsilon_i^*\)</span>，來實現這個概念。異常值很有趣：一個很大的異常值可能對應垃圾資訊，例如，變項在資訊集中可能被錯誤地記錄，或者可能檢測到其他缺陷。請注意，僅僅因為它是一個異常值，你不應該丟掉這個觀測值。但是，它是一個異常值，這經常是一個線索，讓你更仔細地查看該案例，並嘗試找出它為什麼如此不同。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-17" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-17.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.17: 異常值的示例。虛線繪製了不包括異常觀測值的迴歸線，以及相應的殘差（即Studentised殘差）。實線顯示了包含異常觀測值的迴歸線。異常值在結果值（y軸位置）上具有不尋常的值，但在預測變項（x軸位置）上並不不尋常，並且距離迴歸線很遠</figcaption>
</figure>
</div>
</div>
</div>
<p>觀測值不尋常的第二種方式是具有高<strong>槓桿作用(leverage)</strong>，這發生在觀測值與所有其他觀測值非常不同的情況下。這不一定要對應大的殘差。如果觀測值在所有變項上的不尋常程度恰好相同，則實際上可能非常接近迴歸線。這方面的一個例子如@fig-fig10-18所示。觀測值的槓桿作用通常用帽子值表示，通常寫作<span class="math inline">\(h_i\)</span>。帽子值的公式相當複雜<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>，但它的解釋並不複雜：<span class="math inline">\(h_i\)</span>是衡量第i個觀測值“控制”迴歸線走向的程度。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-18" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-18.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.18: 高槓桿點的示例。在這種情況下，異常觀測值在預測變項（x軸）和結果變項（y軸）方面都不尋常，但這種不尋常性與其他觀測值之間存在的相關性模式高度一致。觀測值非常接近迴歸線並且不會使其變形</figcaption>
</figure>
</div>
</div>
</div>
<p>一般來說，如果觀測值在預測變項方面遠離其他觀測值，它將具有較大的帽子值（作為粗略指南，高槓桿是指帽子值大於平均值的2-3倍；注意帽子值的總和被限制為等於<span class="math inline">\(K + 1\)</span>）。高槓桿點也值得更詳細地查看，但除非它們也是異常值，否則它們不太可能引起擔憂。</p>
<p>這讓我們來到了不尋常程度的第三個衡量指標，即觀測值的<strong>影響力(influence)</strong>。高影響力的觀測值是具有高槓桿的異常值。也就是說，它在某些方面與所有其他觀測值非常不同，並且距離迴歸線很遠。這在@fig-fig10-19中有所體現。注意與前兩個圖形的對比。異常值並未使迴歸線發生很大變化，高槓桿點也是如此。但既是異常值又具有高槓桿的情況，會對迴歸線產生很大影響。這就是為什麼我們稱這些點具有高影響力，而且它們是最令人擔憂的。我們用稱為<strong>Cook’s distance</strong>的衡量指標來度量影響力。<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-19" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-19.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.19: 高影響力點的示例。在這種情況下，異常觀測值在預測變項（x軸）上非常不尋常，並且距離迴歸線很遠。因此，即使在這種情況下，異常觀測值在結果變項（y軸）上完全正常，迴歸線也會受到很大影響</figcaption>
</figure>
</div>
</div>
</div>
<p>要具有較大的Cook’s距離，觀測值必須是相當大的異常值並具有高槓桿。作為粗略指南，大於1的Cook’s距離通常被認為很大（這是我通常用作快速而簡單的規則）。</p>
<p>在jamovi中，可以通過單擊’Assumption Checks’ - ’Data Summary’選項下的’Cook’s Distance’複選框來計算有關Cook’s距離的信息。當你這樣做時，對於我們在本章中作為示例使用的多元迴歸模型，你將得到如@fig-fig10-20所示的結果。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-20" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-20.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.20: jamovi輸出顯示Cook’s距離統計表格</figcaption>
</figure>
</div>
</div>
</div>
<p>您可以看到，在這個例子中，平均Cook’s距離值為<span class="math inline">\(0.01\)</span>，範圍從<span class="math inline">\(0.00\)</span>到<span class="math inline">\(0.11\)</span>，因此這與前面提到的指標相去甚遠，即大於1的Cook’s距離被認為很大。</p>
<p>接下來明顯要問的問題是，如果您確實擁有很大的Cook’s距離值，您應該怎麼辦？一如既往，沒有固定不變的規則。可能首先要做的是嘗試運行迴歸，排除具有最大Cook’s距離的異常值<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>，看看模型性能和迴歸係數會發生什麼變化。如果它們確實有很大不同，那就該開始深入研究您的資訊集和您在運行研究時無疑在抄寫的筆記。嘗試找出該點為何如此不同。如果您開始確信這個資訊點嚴重扭曲了您的結果，那麼您可能會考慮將其排除在外，但除非您對於這個特定案例與其他案例有本質上的不同並因此應該單獨處理，否則這種做法是不理想的。</p>
<!--- 大多數迴歸診斷都圍繞著**殘差**的概念。在某種程度上，這很有道理：迴歸模型的目標是找到一條能夠最小化殘差的線，因此我們當然會關注殘差的性質。然而，當我們討論殘差時，其實有很多不同類型的殘差可以關注，以下是其中三個最重要的類型：

1. 原始殘差：這些是最簡單的殘差，也是我們迄今為止討論的唯一一種殘差。對於每個觀察值 i，原始殘差 e_i 被定義為觀察值 y_i 與預測值 ŷ_i 之間的差異。

2. 標準化殘差：這些殘差被標準化，這意味著它們的值被除以標準誤。標準化殘差的計算方式是將原始殘差除以觀察值的標準誤。因此，標準化殘差可以用來確定原始殘差值是否異常高或低。

3. 學生化殘差：這些殘差與標準化殘差相似，但它們在計算標準誤時排除了所檢查的觀察值。這意味著學生化殘差是將原始殘差除以觀察值的標準誤後，再除以（1-h_i），其中 h_i 是觀察值的槓桿值。學生化殘差可以幫助我們確定觀察值是否對模型的擬合產生了極大影響。

### 畫圖：殘差圖和擬合值圖

要確保迴歸模型的適用性，繪製殘差圖和擬合值圖是一種常用方法。這兩種圖表可以幫助我們檢查殘差是否符合我們對其的期望。例如，我們希望殘差是隨機分布的，並且其變異數是恆定的。通過檢查殘差圖和擬合值圖，我們可以快速確定是否滿足這些條件。

1. **殘差圖**：殘差圖是一個以觀察值的順序為橫軸，以殘差為縱軸的散點圖。通常，我們希望這些點沒有特定的模式，並且它們在橫軸上的分布應該是均勻的。如果我們在殘差圖中看到某種模式，那麼這可能表明迴歸模型未能捕捉到某種趨勢。

2. **擬合值圖**：擬合值圖也是一個散點圖，但是它將擬合值（模型預測）作為橫軸，而將殘差作為縱軸。這種圖表的目的是確保殘差在擬合值的範圍內隨機分布，並且具有恆定的變異數。如果我們在擬合值圖中看到某種模式或變異數不恆定的現象，那麼這可能表明迴歸模型的某些假設被違反了。

總之，迴歸分析提供了一個強大的方法來找出自變項與應變項之間的關係。通過分析殘差和擬合值，我們可以檢查迴歸模型的適用性和有效性。此外，繪製殘差圖和擬合值圖有助於我們確保模型符合其基本假設。當模型被證明是有效且合適時，我們就可以使用它來預測或解釋因變項的變化。 --->

<!--- The main focus of this section is **regression diagnostics**, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing "funny" is going on. I refer to this as the "art" of model checking with good reason. It's not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgement when doing this. It's easy to get lost in all the details of checking this thing or that thing, and it's quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn all the tools, so instead they decide not to do any model checking. This is a bit of a worry!

In this section I describe several different things you can do to check that your regression model is doing what it's supposed to. It doesn't cover the full space of things you could do, but it's still much more detailed than what I see a lot of people doing in practice, and even I don't usually cover all of this in my intro stats class either. However, I do think it's important that you get a sense of what tools are at your disposal, so I'll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from @Fox2011, the book associated with the 'car' package that is used to conduct regression analysis in R. The 'car' package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don't want to sound too gushy about it, but I do think that @Fox2011 is well worth reading, even if some of the advanced diagnostic techniques are only available in R and not jamovi. 

### 三種殘差 

The majority of regression diagnostics revolve around looking at the residuals, and by now you've probably formed a sufficiently pessimistic theory of statistics to be able to guess that, precisely because of the fact that we care a lot about the residuals, there are several different kinds of residual that we might consider. In particular, the following three kinds of residuals are referred to in this section: "ordinary residuals", "standardised residuals", and "Studentised residuals". There is a fourth kind that you'll see referred to in some of the Figures, and that's the "Pearson residual". However, for the models that we're talking about in this chapter the Pearson residual is identical to the ordinary residual.

The first and simplest kind of residuals that we care about are **ordinary residuals**. These are the actual raw residuals that I've been talking about throughout this chapter so far. The ordinary residual is just the difference between the fitted value $\hat{Y}_i$ and the observed value $Y_i$. I've been using the notation $\epsilon_i$ to refer to the i-th ordinary residual, and by gum I'm going to stick to it. With this in mind, we have the very simple equation

$$\epsilon_i=Y_i-\hat{Y_i}$$

This is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I'm talking about. So there's nothing new here. I just wanted to repeat myself. One drawback to using ordinary residuals is that they're always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you've decided to run a regression model without an intercept term, the ordinary residuals will have mean 0 but the variance is different for every regression. In a lot of contexts, especially where you're only interested in the pattern of the residuals and not their actual values, it's convenient to estimate the **standardised residuals**, which are normalised in such a way as to have standard deviation of 1.

[Additional technical detail[^correlation-and-linear-regression-16]]

[^correlation-and-linear-regression-16]: The way we calculate these is to divide the ordinary residual by an estimate of the (population) standard deviation of these residuals. For technical reasons, mumble mumble, the formula for this is $$\epsilon_i^{'}=\frac{\epsilon_i}{\hat{\sigma}\sqrt{1-h_i}}$$ where $\hat{\sigma}$ in this context is the estimated population standard deviation of the ordinary residuals, and $h_i$ is the "hat value" of the $i$th observation. I haven't explained hat values to you yet (but have no fear$^c$ it's coming shortly), so this won't make a lot of sense. For now, it's enough to interpret the standardised residuals as if we'd converted the ordinary residuals to z-scores. In fact, that is more or less the truth, it's just that we're being a bit fancier. \+++\ $^c$Or have no hope, as the case may be.

The third kind of residuals are **Studentised residuals** (also called "jackknifed residuals") and they're even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual. [^correlation-and-linear-regression-17]

[^correlation-and-linear-regression-17]: The formula for doing the calculations this time is subtly different $\epsilon _i^*=\frac{\epsilon_i}{\hat{\sigma}_{(-i)}\sqrt{1-h_i}}$ Notice that our estimate of the standard deviation here is written $\hat{\sigma}_{(-i)}$. What this corresponds to is the estimate of the residual standard deviation that you would have obtained if you just deleted the ith observation from the data set. This sounds like the sort of thing that would be a nightmare to calculate, since it seems to be saying that you have to run N new regression models (even a modern computer might grumble a bit at that, especially if you've got a large data set). Fortunately, some terribly clever person has shown that this standard deviation estimate is actually given by the following equation: $\hat{\sigma}_{(-i)}= \hat{\sigma}\sqrt{\frac{N-K-1-{\epsilon_i^{'}}^2}{N-K-2}}$ Isn't that a pip?

Before moving on, I should point out that you don't often need to obtain these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various options that provide the diagnostics, or assumption checks, will take care of these calculations for you. Even so, it's always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.

### 三種異常資料{#sec-three-kinds-of-anomalous-data}

One danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of "unusual" or "anomalous" observations. I discussed this idea previously in @sec-Using-box-plots-to-detect-outliers in the context of discussing the outliers that get automatically identified by the boxplot option under 'Exploration' - 'Descriptives', but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called "anomalous". All three are interesting, but they have rather different implications for your analysis.

The first kind of unusual observation is an **outlier**. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in @fig-fig10-17. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, $\epsilon_i^*$. Outliers are interesting: a big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. Note that you shouldn't throw an observation away just because it's an outlier. But the fact that it's an outlier is often a cue to look more closely at that case and try to find out why it's so different.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line](images/fig12-17.png){#fig-fig10-17eng fig-align='left' width=80%}
:::
:::


The second way in which an observation can be unusual is if it has high **槓桿作用(leverage)**, which happens when the observation is very different from all the other observations. This doesn't necessarily have to correspond to a large residual. If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in @fig-fig10-18. The leverage of an observation is operationalised in terms of its hat value, usually written $h_i$ . The formula for the hat value is rather complicated[^correlation-and-linear-regression-18] but its interpretation is not: $h_i$ is a measure of the extent to which the i-th observation is "in control" of where the regression line ends up going.

[^correlation-and-linear-regression-18]: Again, for the linear algebra fanatics: the "hat matrix" is defined to be that matrix $H$ that converts the vector of observed values $y$ into a vector of fitted values $\hat{y}$, such that $\hat{y} = Hy$. The name comes from the fact that this is the matrix that "puts a hat on y". The hat value of the i-th observation is the i-th diagonal element of this matrix (so technically I should be writing it as $h_{ii}$ rather than $h_i$). Oh, and in case you care, here's how it's calculated: $H = X(X^{'}X)^{1}X^{'}$. Pretty, isn't it?


::: {.cell layout-align="left"}
::: {.cell-output-display}
![An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it](images/fig12-18.png){#fig-fig10-18eng fig-align='left' width=80%}
:::
:::


In general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to $K + 1$). High leverage points are also worth looking at in more detail, but they're much less likely to be a cause for concern unless they are also outliers.

This brings us to our third measure of unusualness, the **影響力(influence)** of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in @fig-fig10-19. Notice the contrast to the previous two figures. Outliers don't move the regression line much and neither do high leverage points. But something that is both an outlier and has high leverage, well that has a big effect on the regression line. That's why we call these points high influence, and it's why they're the biggest worry. We operationalise influence in terms of a measure known as **Cook's distance**. [^correlation-and-linear-regression-19]

[^correlation-and-linear-regression-19]: $D_i=\frac{{\epsilon_i^*}^2}{K+1} \times \frac{h_i}{1-h_i}$ Notice that this is a multiplication of something that measures the outlier-ness of the observation (the bit on the left), and something that measures the leverage of the observation (the bit on the right).


::: {.cell layout-align="left"}
::: {.cell-output-display}
![An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)](images/fig12-19.png){#fig-fig10-19eng fig-align='left' width=80%}
:::
:::


In order to have a large Cook's distance an observation must be a fairly substantial outlier and have high leverage. As a rough guide, Cook's distance greater than 1 is often considered large (that's what I typically use as a quick and dirty rule).

In jamovi, information about Cook's distance can be calculated by clicking on the 'Cook's Distance' checkbox in the 'Assumption Checks' - 'Data Summary' options. When you do this, for the multiple regression model we have been using as an example in this chapter, you get the results as shown in @fig-fig10-20.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![jamovi output showing the table for the Cook's distance statistics](images/fig12-20.png){#fig-fig10-20eng fig-align='left' width=80%}
:::
:::


You can see that, in this example, the mean Cook's distance value is $0.01$, and the range is from $0.00$ to $0.11$, so this is some way off the rule of thumb figure mentioned above that a Cook's distance greater than 1 is considered large.

An obvious question to ask next is, if you do have large values of Cook's distance what should you do? As always, there's no hard and fast rule. Probably the first thing to do is to try running the regression with the outlier with the greatest Cook's distance[^correlation-and-linear-regression-20] excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it's time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study. Try to figure out why the point is so different. If you start to become convinced that this one data point is badly distorting your results then you might consider excluding it, but that's less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.

[^correlation-and-linear-regression-20]: although currently there isn't a very easy way to do this in jamovi, so a more powerful regression program such as the 'car' package in R would be better for this more advanced analysis --->
</section>
<section id="檢測殘差常態性" class="level3" data-number="12.10.3">
<h3 data-number="12.10.3" class="anchored" data-anchor-id="檢測殘差常態性"><span class="header-section-number">12.10.3</span> 檢測殘差常態性</h3>
<p>像我們在本書中討論過的許多統計工具一樣，迴歸模型依賴於常態性假設。在這種情況下，我們假設殘差呈常態分布。首先，我們可以通過 ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ 選項繪製一個 QQ 圖。輸出顯示在 <a href="#fig-fig10-21">圖&nbsp;<span>12.21</span></a>，顯示了標準化殘差作為其根據迴歸模型的理論分位數的函數的圖。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-21" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-21.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.21: 模型理論分位數與標準化殘差的分位數之間的圖，由 jamovi 生成</figcaption>
</figure>
</div>
</div>
</div>
<p>我們還應該檢查擬合值和殘差本身之間的關係。我們可以使用 ‘Residuals Plots’ 選項讓 jamovi 做到這一點，該選項為每個預測變項、結果變項以及擬合值與殘差提供了散點圖，見 <a href="#fig-fig10-22">圖&nbsp;<span>12.22</span></a>。在這些圖中，我們要尋找的是點的分佈相對均勻，沒有明顯的聚集或圖案。觀察這些圖，沒有什麼特別令人擔憂的，因為點在整個圖中分佈得相當均勻。在圖 (b) 中可能存在一點非均勻性，但偏差不大，可能不值得擔憎。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-22" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-22.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.22: 由 jamovi 生成的殘差圖</figcaption>
</figure>
</div>
</div>
</div>
<p>如果我們擔憂的話，那麼在很多情況下解決這個問題（以及其他許多問題）的方法是對一個或多個變項進行轉換。我們在 <a href="06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable"><span>小單元&nbsp;6.3</span></a> 中討論了變項轉換的基本知識，但我想特別提一下我之前沒有完全解釋的一個額外可能性：Box-Cox 轉換。Box-Cox 函數相當簡單，並且被廣泛使用。<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a></p>
<p>您可以在 jamovi 的 ‘Compute’ 變數屏幕中使用 BOXCOX 函數進行計算。</p>
</section>
<section id="sec-check-regression-collinearity" class="level3" data-number="12.10.4">
<h3 data-number="12.10.4" class="anchored" data-anchor-id="sec-check-regression-collinearity"><span class="header-section-number">12.10.4</span> 檢測共線性</h3>
<p>在本章中，我將討論的最後一種迴歸診斷方法是使用<strong>方差膨脹因子</strong>（VIF），這對於確定迴歸模型中的預測變項是否彼此相關性過高很有用。模型中每個預測變項 <span class="math inline">\(X_k\)</span> 都有一個相應的方差膨脹因子。<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></p>
<p>VIF 的平方根具有很好的解釋性。它告訴您相應的係數 bk 的信賴區間相對於預測變項彼此完全不相關時所期望的值要寬多少。如果您只有兩個預測變項，VIF 值將始終相同，正如我們在 jamovi 的 ‘Regression’ - ‘Assumptions’ 選項中選中 ‘Collinearity’ 後可以看到的那樣。對於 dani.sleep 和 baby.sleep，VIF 均為 1.65。而由於 1.65 的平方根為 1.28，我們可以看到我們兩個預測變項之間的相關性並未造成太大問題。</p>
<p>為了給出我們可能會遇到具有更大共線性問題的模型的感覺，假設我要運行一個更無趣的迴歸模型，在該模型中，我試圖預測資訊收集的日期，作為資訊集中所有其他變項的函數。要理解這為什麼會有問題，讓我們看一下所有四個變項的相關矩陣（<a href="#fig-fig10-24">圖&nbsp;<span>12.23</span></a>）。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-24" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-25.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.23: 四個資料變項之間的相關係數矩陣</figcaption>
</figure>
</div>
</div>
</div>
<p>我們的預測變項之間有一些相當大的相關性！當我們運行迴歸模型並查看 VIF 值時，我們可以看到共線性對係數的不確定性造成了很大影響。首先，運行迴歸，如 <a href="#fig-fig10-23">圖&nbsp;<span>12.24</span></a> 所示，從 VIF 值可以看出，是的，這是一些非常好的共線性。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-23" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-23.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.24: jamovi 生成的多元迴歸共線性統計</figcaption>
</figure>
</div>
</div>
</div>

<!---
### 檢測殘差常態性

Like many of the statistical tools we've discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the 'Assumption Checks' - 'Assumption Checks' - 'Q-Q plot of residuals' option. The output is shown in @fig-fig10-21, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi](images/fig12-21.png){#fig-fig10-21eng fig-align='left' width=80%}
:::
:::


Another thing we should check is the relationship between the fitted values and the residuals themselves. We can get jamovi to do this using the 'Residuals Plots' option, which provides a scatterplot for each predictor variable, the outcome variable, and the fitted values against residuals, see @fig-fig10-22. In these plots we are looking for a fairly uniform distribution of 'dots', with no clear bunching or patterning of the 'dots'. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![Residuals plots produced in jamovi](images/fig12-22.png){#fig-fig10-22eng fig-align='left' width=80%}
:::
:::


If we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in @sec-Transforming-and-recoding-a-variable, but I do want to make special note of one additional possibility that I didn't explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it's very widely used. [^correlation-and-linear-regression-21]

[^correlation-and-linear-regression-21]: $f(x,\lambda)=\frac{x^{\lambda}-1}{\lambda}$ for all values of $\lambda$ except $\lambda = 0$. When $\lambda = 0$ we just take the natural logarithm (i.e., ln($x$).

You can calculate it using the BOXCOX function in the 'Compute' variables screen in jamovi.

### 檢測共線性 {#sec-check-regression-collinearity}

The last kind of regression diagnostic that I'm going to discuss in this chapter is the use of **variance inflation factors** (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor $X_k$ in the model. [^correlation-and-linear-regression-22]

[^correlation-and-linear-regression-22]: The formula for the k-th VIF is: $VIF_k=\frac{1}{1-R_{-k}^2}$ where $R^2_(-k)$ refers to R-squared value you would get if you ran a regression using $X_k$ as the outcome variable, and all the other X variables as the predictors. The idea here is that $R^2_(-k)$ is a very good measure of the extent to which $X_k$ is correlated with all the other variables in the model.

The square root of the VIF is pretty interpretable. It tells you how much wider the confidence interval for the corresponding coefficient bk is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you've only got two predictors, the VIF values are always going to be the same, as we can see if we click on the 'Collinearity' checkbox in the 'Regression' - 'Assumptions' options in jamovi. For both dani.sleep and baby.sleep the VIF is $1.65$. And since the square root of $1.65$ is $1.28$, we see that the correlation between our two predictors isn't causing much of a problem.

To give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let's have a look at the correlation matrix for all four variables (@fig-fig10-24).


::: {.cell layout-align="left"}
::: {.cell-output-display}
![四個資料變項之間的相關係數矩陣](images/fig12-25.png){#fig-fig10-24eng fig-align='left' width=80%}
:::
:::


We have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in @fig-fig10-23 and you can see from the VIF values that, yep, that's some mighty fine collinearity there.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![Collinearity statistics for multiple regression, produced in jamovi](images/fig12-23.png){#fig-fig10-23eng fig-align='left' width=80%}
:::
:::


--->
</section>
</section>
<section id="sec-multiple-variables-combinaion" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="sec-multiple-variables-combinaion"><span class="header-section-number">12.11</span> 決定線性模型的變項組合</h2>
<blockquote class="blockquote">
<p>譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。</p>
</blockquote>
<p>剩下的一個相當重要的問題是 “模型選擇” 的問題。也就是說，如果我們有一個包含幾個變項的資料集，哪些變項應該作為預測變項，哪些不應該包括在內？換句話說，我們有一個<strong>變項選擇</strong>的問題。通常，模型選擇是一個複雜的過程，但如果我們將問題限制在選擇應該包含在模型中的變項子集上，情況會變得簡單一些。儘管如此，我不打算試圖詳細涵蓋甚至這個範疇。相反，我將談論您需要考慮的兩個廣泛原則，然後討論一個具體的工具，jamovi 提供了這個工具，可以幫助您選擇要包含在模型中的變項子集。首先，兩個原則：</p>
<ul>
<li><p>為您的選擇提供實質性的依據是很好的。也就是說，在很多情況下，您作為研究人員有充分的理由挑選一個較小的可能的回歸模型數量，這些模型在您的領域背景下具有合理的解釋。永遠不要低估這一點的重要性。統計學為科學過程服務，而不是反過來。</p></li>
<li><p>在您的選擇依賴統計推斷的程度上，簡單性和適合度之間存在權衡。當您向模型添加更多預測變項時，模型變得更複雜。每個預測因子都添加了一個新的自由參數（即，一個新的回歸係數），每個新參數都會增加模型對於隨機變異的吸收能力。因此，適合度（例如，<span class="math inline">\(R^2\)</span>）隨著您添加更多預測因子而持續上升，無論如何都是如此。如果您希望模型能夠很好地概括新的觀察結果，則需要避免加入過多的變項。</p></li>
</ul>
<p>後者原則通常被稱為<strong>奧卡姆剃刀</strong>，並通常用以下簡潔的說法來概括：不要在必要之外繁殖實體。在這個情境下，這意味著不要僅僅為了提高你的 R2 而將一堆大致無關的預測因子扔進去。嗯，原來的說法更好。</p>
<p>無論如何，我們需要一個實際的數學標準，以便在選擇回歸模型時實現奧卡姆剃刀背後的定性原則。事實證明，有幾種可能性。我將談論的一個是<strong>赤池資訊量準則</strong>（Akaike information criterion）<span class="citation" data-cites="Akaike1974">(<a href="References.html#ref-Akaike1974" role="doc-biblioref">Akaike, 1974</a>)</span>，僅僅是因為它可以作為一個選項在 jamovi 中使用。<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></p>
<p>AIC 值越小，模型性能越好。如果我們忽略低水平的細節，AIC 的作用就非常明顯了。左邊的項隨著模型預測變差而增加；右邊的項隨著模型複雜度的增加而增加。最佳模型是用盡量少的預測變項（低 K，右側）來擬合資料（低殘差，左側）。簡而言之，這是奧卡姆剃刀的簡單實現。</p>
<p>當選中 “AIC” 複選框時，AIC 可以添加到 “Model Fit Measures” 輸出表中，評估不同模型的一種笨拙方式是查看如果從回歸模型中移除一個或多個預測因子，“AIC” 值是否更低。這是 jamovi 目前實現的唯一方法，但在其他更強大的程序中，如 R，有替代方法。這些替代方法可以自動化有選擇地移除（或添加）預測變項以找到最佳 AIC 的過程。儘管這些方法在 jamovi 中尚未實現，但我將在下面簡要介紹它們，以便您了解它們。</p>
<section id="逐步排除法" class="level3" data-number="12.11.1">
<h3 data-number="12.11.1" class="anchored" data-anchor-id="逐步排除法"><span class="header-section-number">12.11.1</span> 逐步排除法</h3>
<p>在逐步排除法中，您從完整的迴歸模型開始，包括所有可能的預測因子。然後，在每個「步驟」中，我們嘗試所有可能的刪除一個變項的方法，並選擇其中最好的（就最低AIC值而言）。這將成為我們的新迴歸模型，然後我們再試驗從新模型中刪除所有可能的選項，同樣選擇具有最低AIC的選項。這個過程將持續進行，直到我們得到一個具有比刪除一個預測因子的其他可能模型更低AIC值的模型。</p>
</section>
<section id="逐步納入法" class="level3" data-number="12.11.2">
<h3 data-number="12.11.2" class="anchored" data-anchor-id="逐步納入法"><span class="header-section-number">12.11.2</span> 逐步納入法</h3>
<p>作為替代方法，您還可以嘗試<strong>逐步納入法</strong>。這次我們從最小可能的模型作為起點，僅考慮可能添加到模型中的選項。然而，還有一個麻煩。您還需要指定您願意接受的最大可能模型。</p>
<p>儘管向後和向前選擇可能導致相同的結論，但它們並不總是如此。</p>
</section>
<section id="使用警告" class="level3" data-number="12.11.3">
<h3 data-number="12.11.3" class="anchored" data-anchor-id="使用警告"><span class="header-section-number">12.11.3</span> 使用警告</h3>
<p>自動變項選擇方法是具有誘惑力的東西，特別是當它們被捆綁在強大的統計程序中的（相對）簡單函數中時。它們為您的模型選擇提供了一定程度的客觀性，這很好。不幸的是，它們有時被用作掩蓋思考的藉口。您不再需要仔細考慮要添加到模型中的哪些預測因子以及它們可能包含的理論基礎。一切都通過AIC的魔力解決了。如果我們開始丟出像奧卡姆剃刀這樣的短語，那麼一切都被包裹在一個整潔的小包裹裡，沒有人可以反駁。</p>
<p>或者，也許不是。首先，對於什麼算作合適的模型選擇標準，幾乎沒有一致的看法。當我在本科時代被教授逐步排除法時，我們使用了F檢驗來執行它，因為那是軟件所使用的默認方法。我描述了使用AIC，並且因為這是一本入門教材，所以我只描述了這種方法，但AIC絕非統計之神的話語。它是一個近似值，在某些假設下得出的，並且只有在大樣本中滿足這些假設時才能保證起作用。改變那些假設，您就會得到不同的標準，比如BIC（在jamovi中也可用）。再換一種方法，您就會得到NML標準。決定成為貝葉斯，您將基於後驗概率比進行模型選擇。然後還有一堆我沒提到的迴歸特定工具。等等。所有這些不同的方法都有優點和缺點，有些比其他方法更容易計算（AIC可能是最容易的，這可能解釋了它的受歡迎程度）。幾乎所有方法在答案是“明顯”的情況下都會產生相同的結果，但在模型選擇問題變得困難時，存在相當多的分歧。</p>
<p>在實踐中，這意味著什麼？好吧，您可以花幾年時間教自己模型選擇理論，學習所有的技巧，最終決定您個人認為什麼是正確的。作為實際做過這件事的人，我不建議這樣做。您可能會在結束時更加困惑。更好的策略是表現出一點常識。如果您盯著自動向後或向前選擇過程的結果，有意義的模型接近具有最小AIC值，但被一個毫無意義的模型以微弱的優勢擊敗，那麼相信您的直覺。統計模型選擇是一個不精確的工具，正如我一開始說的，可解釋性很重要。</p>
</section>
<section id="比較迴歸模型" class="level3" data-number="12.11.4">
<h3 data-number="12.11.4" class="anchored" data-anchor-id="比較迴歸模型"><span class="header-section-number">12.11.4</span> 比較迴歸模型</h3>
<p>與使用自動模型選擇程序的方法相反，研究人員可以明確地選擇兩個或多個迴歸模型以相互比較。您可以用幾種不同的方法做到這一點，具體取決於您要回答的研究問題。假設我們想知道我兒子睡眠的多少是否與我煩躁的程度有關，超出了我自己睡眠的影響。我們還希望確保測量的那一天對這種關係沒有影響。也就是說，我們對baby.sleep和dani.grump之間的關係感興趣，從這個角度看，dani.sleep和day是我們想控制的<strong>協變項</strong>。在這種情況下，我們想知道dani.grump ~ dani.sleep + day + baby.sleep（我將其稱為Model 2，或M2）是否比dani.grump ~ dani.sleep + day（我將其稱為Model 1，或M1）更適合這些資訊。我們可以用兩種不同的方式來比較這兩個模型，一種基於像AIC這樣的模型選擇標準，另一種基於顯式假設檢定。我首先向您展示基於AIC的方法，因為它更簡單，並且自然地延續了上一節的討論。首先，我需要實際運行兩個迴歸，注意每個迴歸的AIC，然後選擇AIC值較小的模型，因為它被認為是這些資訊的更好模型。實際上，不要立即這樣做。繼續閱讀，因為jamovi中有一種簡單的方法可以在一個表格中獲取不同模型的AIC值。<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></p>
<p>基於假設檢定框架的某種不同方法來解決這個問題。假設您有兩個迴歸模型，其中一個（Model 1）包含另一個（Model 2）的一部分預測變項。也就是說，Model 2包含Model 1中包含的所有預測變項，再加上一個或多個其他預測變項。當這種情況發生時，我們說Model 1嵌套在Model 2中，或者可能說Model 1是Model 2的子模型。無論使用哪種術語，這意味著我們可以將Model 1視為虛無假設，將Model 2視為替代假設。事實上，我們可以用相當簡單的方式構建一個F檢驗。<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></p>
<p>那麼，這就是我們用來比較兩個迴歸模型的假設檢定。現在，我們如何在jamovi中進行呢？答案是使用“Model Builder”選項，在“Block 1”中指定Model 1預測變項dani.sleep和day，然後將Model 2中的其他預測變項（baby.sleep）添加到“Block 2”，如@fig-fig10-25所示。這在“Model Comparisons”表格中顯示了比較Model 1和Model 2的結果，<span class="math inline">\(F(1,96) = 0.00\)</span>，<span class="math inline">\(p = 0.954\)</span>。由於我們的p &gt; .05，我們保留虛無假設（M1）。這種將我們所有協變數添加到零模型中，然後將感興趣的變項添加到替代模型中，然後在假設檢定框架中比較兩個模型的迴歸方法通常被稱為<strong>分層迴歸</strong>。</p>
<p>我們還可以使用此“Model Comparison”選項顯示一個表格，顯示每個模型的AIC和BIC，方便比較並確定哪個模型具有最低的值，如@fig-fig10-25所示。</p>
<div class="cell" data-layout-align="left">
<div class="cell-output-display">
<div id="fig-fig10-25" class="quarto-figure quarto-figure-left anchored">
<figure class="figure">
<p><img src="images/fig12-24.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">圖&nbsp;12.25: 使用jamovi的“Model Builder”選項進行模型比較</figcaption>
</figure>
</div>
</div>
</div>

<!--- 
## 決定線性模型的變項組合 {#sec-multiple-variables-combinaion}

One fairly major problem that remains is the problem of "model selection". That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of **variable selection**. In general, model selection is a complex business but it's made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I'm not going to try covering even this reduced topic in a lot of detail. Instead, I'll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:

-   It's nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.

-   To the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model's capacity to "absorb" random variations. So the goodness of fit (e.g., $R^2$ ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.

This latter principle is often referred to as **Occam's razor** and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don't chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.

In any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam's razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I'll talk about is the **Akaike information criterion** [@Akaike1974] simply because it's available as an option in jamovi. [^correlation-and-linear-regression-23]

[^correlation-and-linear-regression-23]: In the context of a linear regression model (and ignoring terms that don't depend on the model in any way!), the AIC for a model that has K predictor variables plus an intercept is $AIC=\frac{SS_{res}}{\hat{\sigma}^2}+2K$

The smaller the AIC value, the better the model performance. If we ignore the low level details it's fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham's razor.

AIC can be added to the 'Model Fit Measures' output Table when the 'AIC' checkbox is clicked, and a rather clunky way of assessing different models is seeing if the 'AIC' value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.

### 逐步排除法

In backward elimination you start with the complete regression model, including all possible predictors. Then, at each "step" we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.

### 逐步納入法

As an alternative, you can also try **forward selection**. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there's one complication. You also need to specify what the largest possible model you're willing to entertain is.

Although backward and forward selection can lead to the same conclusion, they don't always.

### 使用警告

Automated variable selection methods are seductive things, especially when they're bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that's kind of nice. Unfortunately, they're sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham's razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.

Or, perhaps not. Firstly, there's very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I've described using AIC, and since this is an introductory text that's the only method I've described, but the AIC is hardly the Word of the Gods of Statistics. It's an approximation, derived under certain assumptions, and it's guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you're a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven't mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is "obvious" but there's a fair amount of disagreement when the model selection problem becomes hard.

What does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn't recommend it. You'll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you're staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn't make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.

### 比較迴歸模型

An alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you're trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we're interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or **covariates** that we want to control for. In this situation, what we would like to know is whether dani.grump \~ dani.sleep + day + baby .sleep (which I'll call Model 2, or M2) is a better regression model for these data than dani.grump \~ dani.sleep + day (which I'll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I'll show you the AIC based approach first because it's simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don't do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.[^correlation-and-linear-regression-24]

[^correlation-and-linear-regression-24]: While I'm on this topic I should point out that the empirical evidence suggests that BIC is a better criterion than AIC. In most simulation studies that I've seen, BIC does a much better job of selecting the correct model.

A somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. [^correlation-and-linear-regression-25]

[^correlation-and-linear-regression-25]: We can fit both models to the data and obtain a residual sum of squares for both models. I'll denote these as $SS_{res}^{(1)}$ and $SS_{res}^{(2)}$ respectively. The superscripting here just indicates which model we're talking about. Then our F statistic is $$F= \frac {\frac {SS _{res}^{(1)} - SS_{res}^{(2)}} {k}}   {\frac{SS_{res}^2} {N-p-1} }$$ where $N$ is the number of observations, $p$ is the number of predictors in the full model (not including the intercept), and $k$ is the difference in the number of parameters between the two models. $^d$ The degrees of freedom here are $k$ and $N -p-1$. Note that it's often more convenient to think about the difference between those two SS values as a sum of squares in its own right. That is $$SS_\Delta=SS_{res}^{(1)}-SS_{res}^{(2)}$$ The reason why this is helpful is that we can express $SS_\Delta$ as a measure of the extent to which the two models make different predictions about the the outcome variable. Specifically, $$SS_\Delta=\sum_i{(\hat{y}_i^{(2)}-\hat{y}_i^{(1)})^2}$$ where $\hat{y}_{i^{(1)}}$ is the fitted value for $y_i$ according to model $M_1$ and $\hat{y}_{i^{(2)}}$ is the fitted value for $y_i$ according to model $M_2$. <br /> --- <br /> $^d$ It's worth noting in passing that this same F statistic can be used to test a much broader range of hypotheses than those that I'm mentioning here. Very briefly, notice that the nested model M1 corresponds to the full model M2 when we constrain some of the regression coefficients to zero. It is sometimes useful to construct sub-models by placing other kinds of constraints on the regression coefficients. For instance, maybe two different coefficients might have to sum to zero, or something like that. You can construct hypothesis tests for those kind of constraints too, but it is somewhat more complicated and the sampling distribution for F can end up being something known as the non-central F distribution, which is waaaaay beyond the scope of this book! All I want to do is alert you to this possibility.

Okay, so that's the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the 'Model Builder' option and specify the Model 1 predictors dani.sleep and day in 'Block 1' and then add the additional predictor from Model 2 (baby.sleep) in 'Block 2', as in @fig-fig10-25. This shows, in the 'Model Comparisons' Table, that for the comparisons between Model 1 and Model 2, $F(1,96) = 0.00$, $p = 0.954$. Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as **hierarchical regression**.

We can also use this 'Model Comparison' option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in @fig-fig10-25.


::: {.cell layout-align="left"}
::: {.cell-output-display}
![Model comparison in jamovi using the 'Model Builder' option](images/fig12-24.png){#fig-fig10-25eng fig-align='left' width=80%}
:::
:::


--->
</section>
</section>
<section id="本章小結" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="本章小結"><span class="header-section-number">12.12</span> 本章小結</h2>
<ul>
<li>想了解兩個變項之間的關聯性有多強？就計算<a href="12-Correlation-and-linear-regression.html#相關">相關係數</a></li>
<li><a href="12-Correlation-and-linear-regression.html#散佈圖">散佈圖</a>繪製方法</li>
<li>前進下一章前必學的課題：<a href="12-Correlation-and-linear-regression.html#認識線性迴歸模型">什麼是線性迴歸模型</a> 以及<a href="12-Correlation-and-linear-regression.html#線性迴歸模型的參數估計">使用線性迴歸模型估計參數</a></li>
<li><a href="http://localhost:7933/12-Correlation-and-linear-regression.html#%E5%A4%9A%E5%85%83%E7%B7%9A%E6%80%A7%E8%BF%B4%E6%AD%B8">多元線性迴歸</a></li>
<li><a href="http://localhost:7933/12-Correlation-and-linear-regression.html#%E9%87%8F%E5%8C%96%E8%BF%B4%E6%AD%B8%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%81%A9%E9%85%8D%E6%80%A7">量化迴歸模型的適配性</a> 要了解 <span class="math inline">\(R^2\)</span> 。</li>
<li><a href="12-Correlation-and-linear-regression.html#迴歸模型的假設檢定">迴歸模型的假設檢定</a></li>
<li>在<a href="12-Correlation-and-linear-regression.html#迴歸係數的更多資訊">迴歸係數的更多資訊</a> 這一節，我們學習如何計算<a href="12-Correlation-and-linear-regression.html#迴歸係數的信賴區間">迴歸係數的信賴區間</a>以及<a href="12-Correlation-and-linear-regression.html#標準化迴歸係數的計算方法">標準化迴歸係數的計算方法</a></li>
<li><a href="12-Correlation-and-linear-regression.html#迴歸模型的適用條件">迴歸模型的適用條件</a> 以及<a href="http://localhost:7933/12-Correlation-and-linear-regression.html#sec-Model-checking">診斷適用條件</a></li>
<li><a href="12-Correlation-and-linear-regression.html#決定線性模型的變項組合">決定線性模型的變項組合</a></li>
</ul>
<!---
-   Want to know how strong the relationship is between two variables? Calculate [Correlations]
-   Drawing [Scatterplots]
-   Basic ideas about [What is a linear regression model?] and [Estimating a linear regression model]
-   [Multiple linear regression]
-   [Quantifying the fit of the regression model] using $R^2$.
-   [Hypothesis tests for regression models]
-   In [Regarding regression coefficients] we talked about calculating [Confidence intervals for the coefficients] and [Calculating standardised regression coefficients]
-   The [Assumptions of regression] and [Model checking]
-   Regression [Model selection]
--->


<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="list" style="display: none">
<div id="ref-Akaike1974" class="csl-entry" role="listitem">
Akaike, H. (1974). A new look at the statistical model identification. <em>IEEE Transactions on Automatic Control</em>, <em>19</em>, 716–723.
</div>
<div id="ref-Anscombe1973" class="csl-entry" role="listitem">
Anscombe, F. J. (1973). Graphs in statistical analysis. <em>American Statistician</em>, <em>27</em>, 17–21.
</div>
<div id="ref-Fox2011" class="csl-entry" role="listitem">
Fox, J., &amp; Weisberg, S. (2011). <em>An <span>R</span> companion to applied regression</em> (2nd ed.). Sage.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>jamovi 2.0版之後的版本，使用者可以指定 “ID” 的專屬變項類型，但是對於分析資料的目的來說，指定 ID 的變項性質並不重要，因為通常不會將其包含在任何分析中。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>即使表格中的資訊對某些人來說已經足夠了，但是大多數人只需要知道一個集中量數和一個變異量數就可以了。<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>皮爾森相關係數的公式可以用幾種不同的方式來表示。最簡單的方式是將公式分為兩個部分。第一個部分是<strong>共變異數（covariance）</strong>(譯註：許多台灣中文教科書稱<strong>共變數</strong>)的概念。兩個變項<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的共變異數的數學公式是變異數公式的一般化，就是簡單描述兩個變項之間的關係，但是計算結果無法提供什麼有意義的訊息: <span class="math display">\[Cov(X,Y)=\frac{1}{N-1}\sum_{i=1}^N(X_i-\bar{X})(Y_i-\bar{Y})\]</span> 由於我們要將<span class="math inline">\(X\)</span>的數值乘以<span class="math inline">\(Y\)</span>的數值，再取乘積的平均值<span class="math inline">\(^a\)</span>，因此共變異數的公式可以視為<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>之間的“平均外積”。共變異數的特性之一是，如果<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>之間完全沒有關係，那麼共變異數就是零。如果變項之間是正相關（見 <a href="#fig-fig10-4">圖&nbsp;<span>12.4</span></a> ），則共變異數的數值為正，如果是負相關，共變異數的數值為負。換句話說，共變異數能捕捉相關性的基本定性概念。不幸的是，共變異數的原始數值並不容易解讀，因為大小取決於<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>所的計量單位，而且更糟糕的是，共變異數數值的單位非常奇怪。例如，如果<span class="math inline">\(X\)</span>是dani.sleep變項（單位：小時）而<span class="math inline">\(Y\)</span>是<code>dani.grump</code>變項（單位：沮喪程度），那麼共變異數的單位會是<span class="math inline">\(hours \times grumps\)</span>，我們無法解讀這是什麼意思。皮爾森相關係數則解決了這個問題，與z分數標準化原始分數的方法非常相似，將差異值除以標準差轉換為標準化分數。但是，由於共變異數來自兩個變項的數值，必須除以兩個變項的標準差才能做標準化轉換。<span class="math inline">\(^b\)</span>換句話說，兩個變項<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的相關係數可以寫成這個公式：<span class="math display">\[r_{XY}=\frac{Cov(X,Y)}{\hat{\sigma}_X\hat{\sigma}_Y}\]</span><br>—<br><span class="math inline">\(^a\)</span> 和變異數、標準差一樣，實際上我們在計算時除以的是 <span class="math inline">\(N-1\)</span> 而不是 <span class="math inline">\(N\)</span>。<br> <span class="math inline">\(^b\)</span> 這是一個過於簡化的說法，但對於我們的目的而言已足夠。<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>譯註~這一句原文略談測驗信效度，但是原文說法太模糊，中文翻譯做適度改寫。<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>譯註~原版電子書並未提供 <a href="#tbl-tab10-3">表&nbsp;<span>12.3</span></a> 的資料<!---，中文版配合線性模型教程，採用另一套線上教程提供的範例檔案，請見[相關與線性迴歸](Prelude-Part-V.html#%E7%9B%B8%E9%97%9C%E8%88%87%E7%B7%9A%E6%80%A7%E8%BF%B4%E6%AD%B8)這一節的補充--->。<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>也可以寫成 <span class="math inline">\(y = mx + c\)</span>，其中 <span class="math inline">\(m\)</span> 是斜率，<span class="math inline">\(c\)</span> 是截距（常數）。<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><span class="math inline">\(\epsilon\)</span> 符號是希臘字母的 epsilon。統計報告慣例使用 <span class="math inline">\(\epsilon_i\)</span> 或 <span class="math inline">\(e_i\)</span> 表示殘差。<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>原作者設想大多數讀者並不想知道普通最小平方法的細節。不過讀者若是線性代數的高手（客觀地說，很多在排名前段的大學開設統計課的教師，總是會在課堂遇到厲害的學生），會很想知道，計算迴歸係數估計值的公式是<span class="math inline">\(\hat{b} = (X^{'}X)^{-1}X^{'}y\)</span>，其中<span class="math inline">\(\hat{b}\)</span>是包含估計迴歸係數的數值向量，<span class="math inline">\(X\)</span>是包含預測變項（嚴格來說，<span class="math inline">\(X\)</span>是迴歸變項的矩陣，是實際資料數值加上一個全部都是1的數列，但這裡不討論這個區別），而<span class="math inline">\(y\)</span>是包含依變項的數值向量。對於不是高手的讀者，這些解說並沒有太大幫助，甚至可能造成障礙。然而，由於線性迴歸有許多細節可以用線性代數的術語來表示，讀者將在本章後半部看到許多像這樣的註釋。如果讀者有能力理解其中的數學涵意，那就非常好。如果不能，可以忽略這些註釋。<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>一般情況下的公式：本文的方程式顯示包括兩個預測變項時的多元迴歸模型。如果一個模型有不止兩個預測變項，只需要添加更多的X變項和對應的迴歸係數b。換句話說，如果一個模型有K個預測變項，那麼迴歸方程式就是 <span class="math display">\[Y_i=b_0+(\sum_{k=1}^{K}b_k X_{ik})+\epsilon_i\]</span><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>“有些”指的是“非常少”(譯註：其實很多中文教科書都這樣稱呼)。在真實的統計工作，多數分析人員都直接叫“R-squared”。<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>調整後的 <span class="math inline">\(R^2\)</span> 計算公式包括一個小小的校正公式。對於有 <span class="math inline">\(k\)</span> 個預測變項的迴歸模型，適用於包含 <span class="math inline">\(N\)</span> 個觀察值的資料集，調整後的 <span class="math inline">\(R^2\)</span> 為：<span class="math display">\[\text{adj.}R^2=1-(\frac{SS_{res}}{SS_{tot}} \times \frac{N-1}{N-K-1})\]</span><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>正式地，我們的「零模型」對應於相當簡單的「迴歸」模型，其中我們包括 0 預測變項並且僅包括截距項 <span class="math inline">\(b_0\)</span>：<span class="math inline">\(H_0:Y_0=b_0+\epsilon_i\)</span> 如果我們的迴歸模型具有 <span class="math inline">\(K\)</span> 預測變項，那麼「對立模型」使用多元迴歸模型的標準公式描述：<span class="math display">\[H_1:Y_i=b_0+(\sum_{k=1}^K b_k X_{ik})+\epsilon_i\]</span> 我們如何在這兩個假設之間進行檢驗呢？訣竅在於理解將總變異 <span class="math inline">\(SStot\)</span> 劃分為殘差變異 SSres 和迴歸模型變異 SSmod 的可能性。我將跳過技術細節，因為我們稍後在 <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>單元&nbsp;13</span></a> 中研究 ANOVA 時會研究這個問題。但是只需注意 <span class="math inline">\(SS_{mod}=SS_{tot}-SS_{res}\)</span> 我們可以通過將平方和除以自由度將其轉換為平均平方。 <span class="math display">\[MS_{mod}=\frac{SS_{mod}}{df_{mod}}\]</span> <span class="math display">\[MS_{res}=\frac{SS_{res}}{df_{res}}\]</span> 那麼，我們有多少自由度呢？您可能會預料到，與模型相關的 df 與我們所包含的預測變項數量密切相關。實際上，<span class="math inline">\(df_mod = K\)</span>。對於殘差，總自由度為 <span class="math inline">\(df_res = N - K - 1\)</span>。現在我們已經有了平均平方值，我們可以像這樣計算 F 統計量 <span class="math display">\[F=\frac{MS_{mod}}{MS_{res}}\]</span> 並且與此相關的自由度為 <span class="math inline">\(K\)</span> 和 <span class="math inline">\(N - K - 1\)</span>。<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>譯註~這些適用條件在所有基礎統計方法幾乎都是必要的，這也是本書中文版將<strong>相關與線性迴歸</strong>調整為第一個學習的統計方法原因之一。<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>給認真的高手讀者做個補充。殘差向量是<span class="math inline">\(\epsilon=y - X\hat{b}\)</span>。對於K個預測變項加截距，估計的殘差平方差是<span class="math inline">\(\hat{\sigma}^2 = \frac{\epsilon^{'}\epsilon}{(N - K - 1)}\)</span>。係數估計值的共變異數矩陣是<span class="math inline">\(\hat{\sigma}^{2}(X^{'}X)^{-1}\)</span>，矩 陣的主對角線是<span class="math inline">\(se(\hat{b})\)</span>，就是標準誤估計值。<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>注意，儘管jamovi會自動執行多次檢定，但是沒有執行Bonferroni校正或其他類似操作（參考 <a href="13-Comparing-several-means-one-way-ANOVA.html"><span>單元&nbsp;13</span></a> ）。這些是預設對立假設為雙側的標準單一樣本t檢定。如果想要校正多次檢定的結果，使用者需要自行操作。<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>譯注~必須要勾選“Model Fit”選單裡的”F test”才能顯示自由度等資訊。<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>幸運的是，迴歸權重的信賴區間可以通用的方法計算出來<span class="math inline">\(CI(b)=\hat{b} \pm (t_{crit} \times SE(\hat{b}))\)</span>，其中<span class="math inline">\(se(\hat{b})\)</span>是迴歸係數的標準誤，<span class="math inline">\(t_{crit}\)</span>是t分佈的對應臨界值。例如，如果我們要計算的是95％的信賴區間，則臨界值是具有<span class="math inline">\(N - K -1\)</span>自由度的t分佈中<span class="math inline">\(97.5\)</span>的百分位數。換句話說，這與其他統計方法使用的信賴區間的計算方法相同。<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>嚴格來說，研究者需要將所有<em>迴歸變項</em>標準化。也就是說，在模型中與依變 項相關的每個“事物”都具有迴歸係數。對於到目前為止談過的迴歸模型，每個預測變項都恰好有一個迴歸變項，反之亦然。但是，統計實務中這並不是一般情況，稍後在 <a href="14-Factorial-ANOVA.html"><span>單元&nbsp;14</span></a> ，我們會看到一些這方面的例子。但是，目前我們不需要太在意這種區別。(譯註~之後會提到的線性模性版範例，全都有線性迴歸模型。)<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>撇開這個小單元的解釋，我們可以嘗試看看實際的計算步驟。同學們可以先將所有變項資料標準化，然後再執行迴歸分析。而且其實有一種更簡單的方法可以做到這一點，預測變項<span class="math inline">\(X\)</span>和依變項<span class="math inline">\(Y\)</span>的<span class="math inline">\(\beta\)</span>係數有個非常簡單的公式：<span class="math inline">\(\beta_X=b_X \times \frac{\sigma_X}{\sigma_Y}\)</span>，其中<span class="math inline">\(\sigma_X\)</span>是預測變項的標準差，<span class="math inline">\(\sigma_Y\)</span>是依變項Y的標準差。<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>譯註~這些適用條件在所有基礎統計方法幾乎都是必要的，這也是本書中文版將<strong>相關與線性迴歸</strong>調整為第一個學習的統計方法原因之一。<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>我們計算這些值的方法是將普通殘差除以這些殘差的（群體）標準差的估計值。由於技術原因，嘟囔囔，公式為 <span class="math display">\[\epsilon_i^{'}=\frac{\epsilon_i}{\hat{\sigma}\sqrt{1-h_i}}\]</span> 其中，<span class="math inline">\(\hat{\sigma}\)</span> 在此上下文中是普通殘差的估計群體標準差，<span class="math inline">\(h_i\)</span> 是第 i 個觀察值的“帽值”。我還沒有向你解釋帽值（但不用害怕<span class="math inline">\(^c\)</span>，它很快就會到來），所以這個公式現在可能看起來不太合理。目前，只需將標準化殘差理解為如果我們將普通殘差轉換為 z 分數。實際上，這幾乎就是事實，只不過我們的表達稍微高級一些。+++<span class="math inline">\(^c\)</span> 或者沒有希望，也可能是這樣。<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>這次進行計算的公式略有不同 <span class="math inline">\(\epsilon _i^*=\frac{\epsilon_i}{\hat{\sigma}_{(-i)}\sqrt{1-h_i}}\)</span> 注意，我們這裡的標準差估計值寫作 <span class="math inline">\(\hat{\sigma}_{(-i)}\)</span>。這對應的是，如果你從資訊集中刪除第 i 個觀察值，則將獲得的殘差標準差估計。這聽起來好像是一個很難計算的東西，因為它似乎在說你必須運行 N 個新的迴歸模型（即使是現代計算機也可能對此有些怨言，特別是如果你有一個大資訊集）。幸運的是，一些非常聰明的人已經證明了這個標準差估計實際上是由以下公式給出的：<span class="math inline">\(\hat{\sigma}_{(-i)}= \hat{\sigma}\sqrt{\frac{N-K-1-{\epsilon_i^{'}}^2}{N-K-2}}\)</span> 難道這不是一個妙計嗎？<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>再次，對於線性代數狂熱分子：定義帽矩陣為將觀測值向量<span class="math inline">\(y\)</span>轉換為擬合值向量<span class="math inline">\(\hat{y}\)</span>的矩陣<span class="math inline">\(H\)</span>，使得<span class="math inline">\(\hat{y} = Hy\)</span>。這個名字來自這是將帽子放在y上的矩陣。第i個觀測值的帽值是這個矩陣的第i個對角元素（所以從技術上講，我應該將其寫為<span class="math inline">\(h_{ii}\)</span>而不是<span class="math inline">\(h_i\)</span>）。哦，如果你在意，這是它的計算方法：<span class="math inline">\(H = X(X^{'}X)^{1}X^{'}\)</span>。漂亮，不是嗎？<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p><span class="math inline">\(D_i=\frac{{\epsilon_i^*}^2}{K+1} \times \frac{h_i}{1-h_i}\)</span> 請注意，這是衡量觀測值的異常程度（左邊的部分）和衡量觀測值的槓桿（右邊的部分）的乘積。<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>雖然目前在jamovi中進行此操作的方法不太容易，因此更強大的迴歸程序（例如R中的’car’套件）將更適合進行此更高級的分析。<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>對於所有的 λ 值，除了 λ = 0，有 <span class="math inline">\(f(x,\lambda)=\frac{x^{\lambda}-1}{\lambda}\)</span>。當 λ = 0 時，我們只取自然對數（即，ln（x））。<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>第 k 個 VIF 的公式為：<span class="math inline">\(VIF_k=\frac{1}{1-R_{-k}^2}\)</span>，其中 <span class="math inline">\(R^2_(-k)\)</span> 指的是如果您將 <span class="math inline">\(X_k\)</span> 作為結果變項，並將所有其他 X 變項作為預測變項來運行迴歸，則可以得到的 R 平方值。這裡的想法是 <span class="math inline">\(R^2_(-k)\)</span> 是 <span class="math inline">\(X_k\)</span> 與模型中所有其他變項相關程度的非常好的衡量指標。<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>在線性回歸模型的上下文中（忽略與模型無關的項！），具有 K 個預測變項和截距的模型的 AIC 為 <span class="math inline">\(AIC=\frac{SS_{res}}{\hat{\sigma}^2}+2K\)</span><a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>在此主題上，我應該指出，經驗證據表明，BIC比AIC更好。在我看到的大多數模擬研究中，BIC在選擇正確模型方面做得更好。<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>我們可以將兩個模型都應用於資訊並獲得兩個模型的殘差平方和。我將它們分別表示為<span class="math inline">\(SS_{res}^{(1)}\)</span>和<span class="math inline">\(SS_{res}^{(2)}\)</span>。上標表示我們正在談論哪個模型。然後我們的F統計量是 <span class="math display">\[F= \frac {\frac {SS _{res}^{(1)} - SS_{res}^{(2)}} {k}}   {\frac{SS_{res}^2} {N-p-1} }\]</span>，其中<span class="math inline">\(N\)</span>是觀察次數，<span class="math inline">\(p\)</span>是完整模型中的預測變項數量（不包括截距），<span class="math inline">\(k\)</span>是兩個模型之間參數的差異。這裡的自由度是<span class="math inline">\(k\)</span>和<span class="math inline">\(N -p-1\)</span>。值得注意的是，將這兩個SS值之間的差異表示為它自己的平方和經常更方便。也就是說 <span class="math display">\[SS_\Delta=SS_{res}^{(1)}-SS_{res}^{(2)}\]</span>。這樣有幫助的原因是我們可以將<span class="math inline">\(SS_\Delta\)</span>表示為兩個模型對結果變項的預測有所不同的程度。具體來說，<span class="math display">\[SS_\Delta=\sum_i{(\hat{y}_i^{(2)}-\hat{y}_i^{(1)})^2}\]</span>，其中<span class="math inline">\(\hat{y}_{i^{(1)}}\)</span>是根據模型<span class="math inline">\(M_1\)</span>對<span class="math inline">\(y_i\)</span>的預測值，<span class="math inline">\(\hat{y}_{i^{(2)}}\)</span>是根據模型<span class="math inline">\(M_2\)</span>對<span class="math inline">\(y_i\)</span>的預測值。 <br> — <br> <span class="math inline">\(^d\)</span>順便提一下，這個相同的F統計量可以用來檢驗比我在這裡提到的更廣泛的範圍的假設。非常簡要地說，注意嵌套模型M1對應於當我們將某些迴歸係數限制為零時的完整模型M2。在某些情況下，通過對迴歸係數施加其他類型的約束來構建子模型是有用的。例如，也許兩個不同的係數可能必須相加為零，或類似的情況。您可以為這些類型的約束構建假設檢定，但這有點更複雜，而且F的抽樣分布可能會是所謂的非中心F分布，這已經超出了本書的範疇！我想做的只是提醒您這種可能性。<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已複製");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已複製");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("https:\/\/scgeeker\.github\.io\/lsj-book-zh_tw\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./11-Comparing-two-means.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">比較單一與兩組平均值</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./13-Comparing-several-means-one-way-ANOVA.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">比較多組平均值(單因子變異數分析)</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left"><a href="https://learnstatswithjamovi.com">用jamovi上手統計學</a></div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">電子書使用 <a href="https://quarto.org/">Quarto套件</a>創建</div>
  </div>
</footer>



</body></html>