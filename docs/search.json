[{"path":"index.html","id":"section","chapter":"","heading":"","text":"learning statistics jamovi covers contents introductory statistics class, typically taught undergraduate psychology, health social science students. book covers get started jamovi well giving introduction data manipulation. statistical perspective, book discusses descriptive statistics graphing first, followed chapters probability theory, sampling estimation, null hypothesis testing. introducing theory, book covers analysis contingency tables, correlation, t-tests, regression, ANOVA factor analysis. Bayesian statistics touched end book.book adaptation DJ Navarro (2018). Learning statistics R: tutorial psychology students beginners. (Version 0.6). https://learningstatisticswithr.com/.book released creative commons CC -SA 4.0 licence. means book can reused, remixed, retained, revised redistributed (including commercially) long appropriate credit given authors. remix, modify original version open textbook, must redistribute versions open textbook license - CC -SA.Citation: Navarro DJ Foxcroft DR (2022). learning statistics jamovi: tutorial psychology students beginners. (Version 0.75). https://dx.doi.org/10.24384/hgc3-7p15","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"","code":""},{"path":"preface.html","id":"preface-to-version-0.75","chapter":"Preface","heading":"Preface to Version 0.75","text":"version updated figures, images text maintain\ncompatibility latest versions jamovi (2.2); many thanks Peter\nFisk help . Also tweaked corrected \nsections improvements suggested readers. \nmainly included fixing typos also places correcting conceptual\ndetail, example updated information kurtosis \nreflect isn’t really distribution “pointiness” instead\nkurtosis whether data distributions thin fat tails.\nThanks readers made suggestions, either \ncontacting email, raising issue github.David Foxcroft\nFebruary 9th, 2022","code":""},{"path":"preface.html","id":"preface-to-version-0.70","chapter":"Preface","heading":"Preface to Version 0.70","text":"update version 0.65 introduces new analyses. ANOVA\nchapters added sections repeated measures ANOVA analysis\ncovariance (ANCOVA). new chapter introduced Factor\nAnalysis related techniques. Hopefully style new\nmaterial consistent rest book, though eagle-eyed\nreaders might spot bit emphasis conceptual practical\nexplanations, bit less algebra. ’m sure good thing,\nmight add algebra bit later. reflects \napproach understanding teaching statistics, also \nfeedback received students course teach. line \n, also rest book tried \nseparate algebra putting box frame. ’s\nstuff important useful, students\nmay wish skip therefore boxing parts\nhelp readers.version grateful comments feedback received\nstudents colleagues, notably Wakefield Morys-Carter, \nalso numerous people world sent small\nsuggestions corrections - much appreciated, keep coming!\nOne pretty neat new feature example data files book\ncan now loaded jamovi add-module - thanks Jonathon\nLove helping .David Foxcroft\nFebruary 1st, 2019","code":""},{"path":"preface.html","id":"preface-to-version-0.65","chapter":"Preface","heading":"Preface to Version 0.65","text":"adaptation excellent ‘Learning statistics R’, \nDanielle Navarro, replaced statistical software used \nanalyses examples jamovi. Although R powerful statistical\nprogramming language, first choice every instructor\nstudent beginning statistical learning. \ninstructors students tend prefer point--click style \nsoftware, ’s jamovi comes . jamovi software aims\nsimplify two aspects using R. offers point--click\ngraphical user interface (GUI), also provides functions \ncombine capabilities many others, bringing SPSS- \nSAS-like method programming R. Importantly, jamovi always \nfree open - ’s one core values - jamovi made\nscientific community, scientific community.version grateful help others \nread drafts provided excellent suggestions corrections,\nparticularly Dr David Emery Kirsty Walter.David Foxcroft\nJuly 1st, 2018","code":""},{"path":"preface.html","id":"preface-to-version-0.6","chapter":"Preface","heading":"Preface to Version 0.6","text":"book hasn’t changed much since 2015 released Version 0.5 –\n’s probably fair say ’ve changed . moved\nAdelaide Sydney 2016 teaching profile UNSW \ndifferent Adelaide, haven’t really chance\nwork since arriving ! ’s little strange looking back \nactually. quick comments…Weirdly, book consistently misgenders , suppose \nblame one :-) ’s now brief footnote\npage 12 mentions issue; real life ’ve working\ngender affirmation process last two years \nmostly go /pronouns. , however, just lazy ever\nhaven’t bothered updating text book.Version 0.6 haven’t changed much ’ve made minor changes\npeople pointed typos errors. particular\n’s worth noting issue associated etaSquared function\nlsr package (isn’t really maintained ) \nSection 14.4. function works fine simple examples \nbook, definitely bugs haven’t found\ntime check! please take care one.biggest change really licensing! ’ve released \nCreative Commons licence (CC -SA 4.0, specifically), placed\nsource files associated GitHub repository, anyone\nwants adapt .Maybe someone like write version makes use \ntidyverse… hear ’s become rather important R days :-)Best,Danielle Navarro","code":""},{"path":"preface.html","id":"preface-to-version-0.5","chapter":"Preface","heading":"Preface to Version 0.5","text":"Another year, another update. time around, update focused\nalmost entirely theory sections book. Chapters 9, 10 \n11 rewritten, hopefully better. Along lines,\nChapter 17 entirely new, focuses Bayesian statistics. think\nchanges improved book great deal. ’ve always felt\nuncomfortable fact inferential statistics \nbook presented orthodox perspective, even though almost\nalways present Bayesian data analyses work. Now ’ve\nmanaged squeeze Bayesian methods book somewhere, ’m\nstarting feel better book whole. wanted get \nthings done update, usual ’m running teaching\ndeadlines, update go way !Danielle Navarro\nFebruary 16, 2015","code":""},{"path":"preface.html","id":"preface-to-version-0.4","chapter":"Preface","heading":"Preface to Version 0.4","text":"year gone since wrote last preface. book changed\nimportant ways: Chapters 3 4 better job documenting\ntime saving features Rstudio, Chapters 12 13 now make\nuse new functions lsr package running chi-square tests \nt tests, discussion correlations adapted refer \nnew functions lsr package. soft copy 0.4 now \nbetter internal referencing (.e., actual hyperlinks sections),\nthough introduced 0.3.1. ’s tweaks \n, many typo corrections (thank everyone pointed \ntypos!), overall 0.4 isn’t massively different 0.3.wish ’d time last 12 months add content.\nabsence discussion repeated measures ANOVA mixed\nmodels generally really annoy . excuse lack \nprogress second child born start 2013, \nspent last year just trying keep head water. \nconsequence, unpaid side projects like book got sidelined favour\nthings actually pay salary! Things little calmer now,\nluck version 0.5 bigger step forward.One thing surprised number downloads book\ngets. finally got basic tracking information website \ncouple months ago, (excluding obvious robots) book \naveraging 90 downloads per day. ’s encouraging: ’s\nleast people find book useful!Danielle Navarro\nFebruary 4, 2014","code":""},{"path":"preface.html","id":"preface-to-version-0.3","chapter":"Preface","heading":"Preface to Version 0.3","text":"’s part really doesn’t want publish book. ’s\nfinished.say , mean . referencing spotty best, \nchapter summaries just lists section titles, ’s index,\nexercises reader, organisation suboptimal,\ncoverage topics just comprehensive enough \nliking. Additionally, sections content ’m happy\n, figures really need redrawn, ’ve almost \ntime hunt inconsistencies, typos, errors. words,\nbook finished. didn’t looming teaching deadline\nbaby due weeks, really wouldn’t making \navailable .means academic looking teaching\nmaterials, Ph.D. student looking learn R, just member \ngeneral public interested statistics, advise \ncautious. ’re looking first draft, may serve\npurposes. living days publishing \nexpensive internet wasn’t around, never consider\nreleasing book form. thought someone shelling $80\n(commercial publisher told retail\noffered distribute ) makes feel little\nuncomfortable. However, ’s 21st century, can post pdf \nwebsite free, can distribute hard copies via \nprint--demand service less half textbook publisher\ncharge. guilt assuaged, ’m willing share!\nmind, can obtain free soft copies cheap hard copies\nonline, following webpages:Soft copy:\nwww.compcogscisydney.com/learning-statistics--r.html\nHard copy:\nwww.lulu.com/content/13570633\n[Ed: links defunct, try instead:\nlearningstatisticswithr.com]Even , warning still stands: looking Version\n0.3 work progress. hits Version 1.0, \nwilling stand behind work say, yes, textbook \nencourage people use. point, ’ll probably start\nshamelessly flogging thing internet generally acting like\ntool. day comes, ’d like made clear ’m\nreally ambivalent work stands.said, one group people can\nenthusiastically endorse book : psychology students taking\nundergraduate research methods classes (DRIP DRIP:) 2013.\n, book ideal, written accompany \nstats lectures. problem arises due shortcoming notes,\ncan adapt content fly fix problem.\nEffectively, ’ve got textbook written specifically \nclasses, distributed free (electronic copy) near-cost prices\n(hard copy). Better yet, notes tested: Version 0.1 \nnotes used 2011 class, Version 0.2 used 2012\nclass, now ’re looking new improved Version 0.3. ’m\nsaying notes titanium plated awesomeness stick –\nthough wanted say student evaluation forms, \n’re totally welcome – ’re . saying \n’ve tried previous years seem work okay.\nBesides, ’s group us around troubleshoot problems\ncome , can guarantee least one lecturers \nread whole thing cover cover!Okay, way, say something \nbook aims . core, introductory statistics\ntextbook pitched primarily psychology students. , covers\nstandard topics ’d expect book: study design,\ndescriptive statistics, theory hypothesis testing, t-tests, χ 2\ntests, ANOVA regression. However, also several chapters\ndevoted R statistical package, including chapter data\nmanipulation another one scripts programming. Moreover, \nlook content presented book, ’ll notice lot \ntopics traditionally swept carpet teaching\nstatistics psychology students. Bayesian/frequentist divide \nopenly disussed probability chapter, disagreement \nNeyman Fisher hypothesis testing makes appearance. \ndifference probability density discussed. detailed\ntreatment Type , II III sums squares unbalanced factorial\nANOVA provided. look Epilogue, \nclear intention add lot advanced content.reasons pursuing approach pretty simple: students\ncan handle , even seem enjoy . last years\n’ve pleasantly surprised just little difficulty ’ve \ngetting undergraduate psych students learn R. ’s certainly easy\n, ’ve found need little charitable setting\nmarking standards, eventually get . Similarly, \ndon’t seem lot problems tolerating ambiguity complexity\npresentation statistical ideas, long assured \nassessment standards set fashion appropriate\n. students can handle , teach ? \npotential gains pretty enticing. learn R, students get\naccess CRAN, perhaps largest comprehensive\nlibrary statistical tools existence. learn \nprobability theory detail, ’s easier switch \northodox null hypothesis testing Bayesian methods want .\nBetter yet, learn data analysis skills can take \nemployer without dependent expensive proprietary software.Sadly, book isn’t silver bullet makes possible.\n’s work progress, maybe finished \nuseful tool. One among many, think. number \nbooks try provide basic introduction statistics using R,\n’m arrogant enough believe mine better. Still, \nrather like book, maybe people find useful,\nincomplete though .Danielle Navarro\nJanuary 13, 2013","code":""},{"path":"why-do-we-learn-statistics.html","id":"why-do-we-learn-statistics","chapter":"1 Why do we learn statistics","heading":"1 Why do we learn statistics","text":"“Thou shalt answer questionnairesOr quizzes upon World Affairs,complianceTake test. Thou shalt sitWith statisticians commitA social science”\n– W.H. Auden1","code":""},{"path":"why-do-we-learn-statistics.html","id":"on-the-psychology-of-statistics","chapter":"1 Why do we learn statistics","heading":"1.1 On the psychology of statistics","text":"surprise many students, statistics fairly significant part psychological education. surprise -one, statistics rarely favourite part one’s psychological education. , really loved idea statistics, ’d probably enrolled statistics class right now, psychology class. , surprisingly, ’s pretty large proportion student base isn’t happy fact psychology much statistics . view , thought right place start might answer common questions people stats.big part issue hand relates idea statistics. ? ’s ? scientists bloody obsessed ? good questions, think . let’s start last one. group, scientists seem bizarrely fixated running statistical tests everything. fact, use statistics often sometimes forget explain people . ’s kind article faith among scientists – especially social scientists – findings can’t trusted ’ve done stats. Undergraduate students might forgiven thinking ’re completely mad, -one takes time answer one simple question:statistics? don’t scientists just use common sense?’s naive question ways, good questions . ’s lot good answers ,2 money, best answer really simple one: don’t trust enough. worry ’re human, susceptible biases, temptations frailties humans suffer . Much statistics basically safeguard. Using “common sense” evaluate evidence means trusting gut instincts, relying verbal arguments using raw power human reason come right answer. scientists don’t think approach likely work.fact, come think , sounds lot like psychological question , since work psychology department, seems like good idea dig little deeper . really plausible think “common sense” approach trustworthy? Verbal arguments constructed language, languages biases – things harder say others, necessarily ’re false (e.g., quantum electrodynamics good theory, hard explain words). instincts “gut” aren’t designed solve scientific problems, ’re designed handle day day inferences – given biological evolution slower cultural change, say ’re designed solve day day problems different world one live . fundamentally, reasoning sensibly requires people engage “induction”, making wise guesses going beyond immediate evidence senses make generalisations world. think can without influenced various distractors, well, bridge London ’d like sell . Heck, next section shows, can’t even solve “deductive” problems (ones guessing required) without influenced pre-existing biases.","code":""},{"path":"why-do-we-learn-statistics.html","id":"the-curse-of-belief-bias","chapter":"1 Why do we learn statistics","heading":"1.1.1 The curse of belief bias","text":"People mostly pretty smart. ’re certainly smarter species share planet (though many people might disagree). minds quite amazing things, seem capable incredible feats thought reason. doesn’t make us perfect though. among many things psychologists shown years really find hard neutral, evaluate evidence impartially without swayed pre-existing biases. good example belief bias effect logical reasoning: ask people decide whether particular argument logically valid (.e., conclusion true premises true), tend influenced believability conclusion, even shouldn’t. instance, ’s valid argument conclusion believable:cigarettes expensive (Premise 1)\naddictive things inexpensive (Premise 2)\nTherefore, addictive things cigarettes (Conclusion)’s valid argument conclusion believable:addictive things expensive (Premise 1)\ncigarettes inexpensive (Premise 2)\nTherefore, cigarettes addictive (Conclusion)logical structure argument #2 identical structure argument #1, ’re valid. However, second argument, good reasons think premise 1 incorrect, result ’s probably case conclusion also incorrect. ’s entirely irrelevant topic hand; argument deductively valid conclusion logical consequence premises. , valid argument doesn’t involve true statements.hand, ’s invalid argument believable conclusion:addictive things expensive (Premise 1)\ncigarettes inexpensive (Premise 2)\nTherefore, addictive things cigarettes (Conclusion)finally, invalid argument unbelievable conclusion:cigarettes expensive (Premise 1)\naddictive things inexpensive (Premise 2)\nTherefore, cigarettes addictive (Conclusion)Now, suppose people really perfectly able set aside pre-existing biases true isn’t, purely evaluate argument logical merits. ’d expect 100% people say valid arguments valid, 0% people say invalid arguments valid. ran experiment looking , ’d expect see data Table 1.1.Table 1.1:  Validity argumentsIf psychological data looked like (even good approximation ), might feel safe just trusting gut instincts. , ’d perfectly okay just let scientists evaluate data based common sense, bother murky statistics stuff. However, guys taken psych classes, now probably know going.classic study, J. St. B. T. Evans, Barston, Pollard (1983) ran experiment looking exactly . found pre-existing biases (.e., beliefs) agreement structure data, everything went way ’d hope (Table 1.2).Table 1.2:  Pre-existing biases argument validityNot perfect, ’s pretty good. look happens intuitive feelings truth conclusion run logical structure argument (Table 1.3):Table 1.3:  Intuition argument validityOh dear, ’s good. Apparently, people presented strong argument contradicts pre-existing beliefs, find pretty hard even perceive strong argument (people 46% time). Even worse, people presented weak argument agrees pre-existing biases, almost -one can see argument weak (people got one wrong 92% time!).3If think , ’s data horribly damning. Overall, people better chance compensating prior biases, since 60% people’s judgements correct (’d expect 50% chance). Even , professional “evaluator evidence”, someone came along offered magic tool improves chances making right decision 60% (say) 95%, ’d probably jump , right? course . Thankfully, actually tool can . ’s magic, ’s statistics. ’s reason #1 scientists love statistics. ’s just easy us “believe want believe”. instead, want “believe data”, ’re going need bit help keep personal biases control. ’s statistics , helps keep us honest.","code":""},{"path":"why-do-we-learn-statistics.html","id":"the-cautionary-tale-of-simpsons-paradox","chapter":"1 Why do we learn statistics","heading":"1.2 The cautionary tale of Simpson’s paradox","text":"following true story (think!). 1973, University California, Berkeley worries admissions students postgraduate courses. Specifically, thing caused problem gender breakdown admissions (Table 1.4).Table 1.4:  Berkeley students genderGiven , worried sued!4 Given nearly 13,000 applicants, difference 9% admission rates males females just way big coincidence. Pretty compelling data, right? say data actually reflect weak bias favour women (sort !), ’d probably think either crazy sexist.Oddly, ’s actually sort true. people started looking carefully admissions data told rather different story (Bickel, Hammel, O’Connell 1975). Specifically, looked department department basis, turned departments actually slightly higher success rate female applicants male applicants. Table 1.5 shows admission figures six largest departments (names departments removed privacy reasons):Table 1.5:  Berkeley students gender six largest DepartmentsRemarkably, departments higher rate admissions females males! Yet overall rate admission across university females lower males. can ? can statements true time?’s ’s going . Firstly, notice departments equal one another terms admission percentages: departments (e.g., , B) tended admit high percentage qualified applicants, whereas others (e.g., F) tended reject candidates, even high quality. , among six departments shown , notice department generous, followed B, C, D, E F order. Next, notice males females tended apply different departments. rank departments terms total number male applicants, get >B>D>C>F>E (“easy” departments bold). whole, males tended apply departments high admission rates. Now compare female applicants distributed . Ranking departments terms total number female applicants produces quite different ordering C>E>D>F>>B. words, data seem suggesting female applicants tended apply “harder” departments. fact, look Figure 1.1 see trend systematic, quite striking. effect known Simpson’s paradox. ’s common, happen real life, people surprised first encounter , many people refuse even believe ’s real. real. lots subtle statistical lessons buried , want use make much important point: research hard, lots subtle, counter-intuitive traps lying wait unwary. ’s reason #2 scientists love statistics, teach research methods. science hard, truth sometimes cunningly hidden nooks crannies complicated data.leaving topic entirely, want point something else really critical often overlooked research methods class. Statistics solves part problem. Remember started concern Berkeley’s admissions processes might unfairly biased female applicants. looked “aggregated” data, seem like university discriminating women, “disaggregate” looked individual behaviour departments, turned actual departments , anything, slightly biased favour women. gender bias total admissions caused fact women tended self-select harder departments. legal perspective, probably put university clear. Postgraduate admissions determined level individual department, good reasons . level individual departments decisions less unbiased (weak bias favour females level small, consistent across departments). Since university can’t dictate departments people choose apply , decision making takes place level department can hardly held accountable biases choices produce.\nFigure 1.1: Berkeley 1973 college admissions data. figure plots admission rate 85 departments least one female applicant, function percentage applicants female. plot redrawing Figure 1 Bickel, Hammel, O’Connell (1975). Circles plot departments 40 applicants; area circle proportional total number applicants. crosses plot departments fewer 40 applicants\nbasis somewhat glib remarks earlier, ’s exactly whole story, ? , ’re interested sociological psychological perspective, might want ask strong gender differences applications. males tend apply engineering often females, reversed English department? case departments tend female-application bias tend lower overall admission rates departments male-application bias? Might still reflect gender bias, even though every single department unbiased? might. Suppose, hypothetically, males preferred apply “hard sciences” females prefer “humanities”. suppose reason humanities departments low admission rates government doesn’t want fund humanities (Ph.D. places, instance, often tied government funded research projects). constitute gender bias? just unenlightened view value humanities? someone high level government cut humanities funds felt humanities “useless chick stuff”. seems pretty blatantly gender biased. None falls within purview statistics, matters research project. ’re interested overall structural effects subtle gender biases, probably want look aggregated disaggregated data. ’re interested decision making process Berkeley ’re probably interested disaggregated data.basis somewhat glib remarks earlier, ’s exactly whole story, ? , ’re interested sociological psychological perspective, might want ask strong gender differences applications. males tend apply engineering often females, reversed English department? case departments tend female-application bias tend lower overall admission rates departments male-application bias? Might still reflect gender bias, even though every single department unbiased? might. Suppose, hypothetically, males preferred apply “hard sciences” females prefer “humanities”. suppose reason humanities departments low admission rates government doesn’t want fund humanities (Ph.D. places, instance, often tied government funded research projects). constitute gender bias? just unenlightened view value humanities? someone high level government cut humanities funds felt humanities “useless chick stuff”. seems pretty blatantly gender biased. None falls within purview statistics, matters research project. ’re interested overall structural effects subtle gender biases, probably want look aggregated disaggregated data. ’re interested decision making process Berkeley ’re probably interested disaggregated data.short lot critical questions can’t answer statistics, answers questions huge impact analyse interpret data. reason always think statistics tool help learn data. less. ’s powerful tool end, ’s substitute careful thought.","code":""},{"path":"why-do-we-learn-statistics.html","id":"statistics-in-psychology","chapter":"1 Why do we learn statistics","heading":"1.3 Statistics in psychology","text":"hope discussion helped explain science general focused statistics. ’m guessing lot questions role statistics plays psychology, specifically psychology classes always devote many lectures stats. ’s attempt answer …psychology much statistics?perfectly honest, ’s different reasons, better others. important reason psychology statistical science. mean “things” study people. Real, complicated, gloriously messy, infuriatingly perverse people. “things” physics include objects like electrons, sorts complexities arise physics, electrons don’t minds . don’t opinions, don’t differ weird arbitrary ways, don’t get bored middle experiment, don’t get angry experimenter deliberately try sabotage data set (’ve ever done !). fundamental level psychology harder physics.5 Basically, teach statistics psychologists need better stats physicists. ’s actually saying used sometimes physics, effect “experiment needs statistics, done better experiment”. luxury able say objects study pathetically simple comparison vast mess confronts social scientists. ’s just psychology. social sciences desperately reliant statistics. ’re bad experimenters, ’ve picked harder problem solve. teach stats really, really need .Can’t someone else statistics?extent, completely. ’s true don’t need become fully trained statistician just psychology, need reach certain level statistical competence. view, ’s three reasons every psychological researcher able basic statistics:Firstly, ’s fundamental reason: statistics deeply intertwined research design. want good designing psychological studies, need least understand basics stats.Secondly, want good psychological side research, need able understand psychological literature, right? almost every paper psychological literature reports results statistical analyses. really want understand psychology, need able understand people data. means understanding certain amount statistics.Thirdly, ’s big practical problem dependent people statistics: statistical analysis expensive. ever get bored want look much Australian government charges university fees, ’ll notice something interesting: statistics designated “national priority” category, fees much, much lower area study. ’s massive shortage statisticians . , perspective psychological researcher, laws supply demand aren’t exactly side ! result, almost real life situation want psychological research, cruel facts don’t enough money afford statistician. economics situation mean pretty self-sufficient.Note lot reasons generalise beyond researchers. want practicing psychologist stay top field, helps able read scientific literature, relies pretty heavily statistics.don’t care jobs, research, clinical work. need statistics?Okay, now ’re just messing . Still, think matter . Statistics matter way statistics matter everyone. live 21st century, data everywhere. Frankly, given world live days, basic knowledge statistics pretty damn close survival tool! topic next section.","code":""},{"path":"why-do-we-learn-statistics.html","id":"statistics-in-everyday-life","chapter":"1 Why do we learn statistics","heading":"1.4 Statistics in everyday life","text":"“drowning information,starved knowledge”\n- Various authors, original probably John NaisbittWhen started writing lecture notes took 20 recent news articles posted ABC news website. 20 articles, turned 8 involved discussion something call statistical topic 6 made mistake. common error, ’re curious, failing report baseline data (e.g., article mentions 5% people situation X characteristic Y, doesn’t say common characteristic everyone else!). point ’m trying make isn’t journalists bad statistics (though almost always ), ’s basic knowledge statistics helpful trying figure someone else either making mistake even lying . fact, one biggest things knowledge statistics cause get angry newspaper internet far frequent basis. can find good example section real life example chapter Descriptive statistics. later versions book ’ll try include anecdotes along lines.","code":""},{"path":"why-do-we-learn-statistics.html","id":"theres-more-to-research-methods-than-statistics","chapter":"1 Why do we learn statistics","heading":"1.5 There’s more to research methods than statistics","text":"far, ’ve talked statistics, ’d forgiven thinking statistics care life. fair, wouldn’t far wrong, research methodology broader concept statistics. research methods courses cover lot topics relate much pragmatics research design, particular issues encounter trying research humans. However, 99% student fears relate statistics part course, ’ve focused stats discussion, hopefully ’ve convinced statistics matters, importantly, ’s feared. said, ’s pretty typical introductory research methods classes stats-heavy. (usually) lecturers evil people. Quite contrary, fact. Introductory classes focus lot statistics almost always find needing statistics need research methods training. ? almost assignments classes rely statistical training, much greater extent rely methodological tools. ’s common undergraduate assignments require design study ground (case need know lot research design), common assignments ask analyse interpret data collected study someone else designed (case need statistics). sense, perspective allowing well classes, statistics urgent.note “urgent” different “important” – matter. really want stress research design just important data analysis, book spend fair amount time . However, statistics kind universality, provides set core tools useful types psychological research, research methods side isn’t quite universal. general principles everyone think , lot research design idiosyncratic, specific area research want engage . extent ’s details matter, details don’t usually show introductory stats research methods class.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"a-brief-introduction-to-research-design","chapter":"2 A brief introduction to research design","heading":"2 A brief introduction to research design","text":"“consult statistician experiment finished often merely ask conduct post mortem examination. can perhaps say experiment died .”\n– Sir Ronald Fisher6In chapter, ’re going start thinking basic ideas go designing study, collecting data, checking whether data collection works, . won’t give enough information allow design studies , give lot basic tools need assess studies done people. However, since focus book much data analysis data collection, ’m giving brief overview. Note chapter “special” two ways. Firstly, ’s much psychology specific later chapters. Secondly, focuses much heavily scientific problem research methodology, much less statistical problem data analysis. Nevertheless, two problems related one another, ’s traditional stats textbooks discuss problem little detail. chapter relies heavily Campbell Stanley (1963) Stevens (1946) discussion scales measurement.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"introduction-to-psychological-measurement","chapter":"2 A brief introduction to research design","heading":"2.1 Introduction to psychological measurement","text":"first thing understand data collection can thought kind measurement. , ’re trying measure something human behaviour human mind. mean “measurement”?","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"some-thoughts-about-psychological-measurement","chapter":"2 A brief introduction to research design","heading":"2.1.1 Some thoughts about psychological measurement","text":"Measurement subtle concept, basically comes finding way assigning numbers, labels, kind well-defined descriptions, “stuff”. , following count psychological measurement:age 33 years.like anchovies.chromosomal gender male.self-identified gender female.short list , bolded part “thing measured”, italicised part “measurement ”. fact, can expand little bit, thinking set possible measurements arisen case:age (years) 0, 1, 2, 3 …, etc. upper bound age possibly bit fuzzy, practice ’d safe saying largest possible age 150, since human ever lived long.asked like anchovies, might said , , opinion, sometimes .chromosomal gender almost certainly going male (\\(XY\\)) female (\\(XX\\)), possibilities. also Klinfelter’s syndrome (\\(XXY\\)), similar male female. imagine possibilities .self-identified gender also likely male female, doesn’t agree chromosomal gender. may also choose identify neither, explicitly call transgender.can see, things (like age) seems fairly obvious set possible measurements , whereas things gets bit tricky. want point even case someone’s age ’s much subtle . instance, example assumed okay measure age years. ’re developmental psychologist, ’s way crude, often measure age years months (child 2 years 11 months usually written “2;11”). ’re interested newborns might want measure age days since birth, maybe even hours since birth. words, way specify allowable measurement values important.Looking bit closely, might also realise concept “age” isn’t actually precise. general, say “age” implicitly mean “length time since birth”. ’s always right way . Suppose ’re interested newborn babies control eye movements. ’re interested kids young, might also start worry “birth” meaningful point time care . Baby Alice born 3 weeks premature Baby Bianca born 1 week late, really make sense say “age” encountered “2 hours birth”? one sense, yes. social convention use birth reference point talking age everyday life, since defines amount time person operating independent entity world. scientific perspective ’s thing care . think biology human beings, ’s often useful think organisms growing maturing since conception, perspective Alice Bianca aren’t age . might want define concept “age” two different ways: length time since conception length time since birth. dealing adults won’t make much difference, dealing newborns might.Moving beyond issues, ’s question methodology. specific “measurement method” going use find someone’s age? , lots different possibilities:just ask people “old ?” method self-report fast, cheap easy. works people old enough understand question, people lie age.ask authority (e.g., parent) “old child?” method fast, dealing kids ’s hard since parent almost always around. doesn’t work well want know “age since conception”, since lot parents can’t say sure conception took place. , might need different authority (e.g., obstetrician).look official records, example birth death certificates. time consuming frustrating endeavour, uses (e.g., person now dead).","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"operationalisation-defining-your-measurement","chapter":"2 A brief introduction to research design","heading":"2.1.2 Operationalisation: defining your measurement","text":"ideas discussed previous section relate concept operationalisation. bit precise idea, operationalisation process take meaningful somewhat vague concept turn precise measurement. process operationalisation can involve several different things:precise trying measure. instance, “age” mean “time since birth” “time since conception” context research?precise trying measure. instance, “age” mean “time since birth” “time since conception” context research?Determining method use measure . use self-report measure age, ask parent, look official record? ’re using self-report, phrase question?Determining method use measure . use self-report measure age, ask parent, look official record? ’re using self-report, phrase question?Defining set allowable values measurement can take. Note values don’t always numerical, though often . measuring age values numerical, still need think carefully numbers allowed. want age years, years months, days, hours? types measurements (e.g., gender) values aren’t numerical. , just , need think values allowed. ’re asking people self-report gender, options allow choose ? enough allow “male” “female”? need “” option? give people specific options instead let answer words? open set possible values include verbal response, interpret answers?Defining set allowable values measurement can take. Note values don’t always numerical, though often . measuring age values numerical, still need think carefully numbers allowed. want age years, years months, days, hours? types measurements (e.g., gender) values aren’t numerical. , just , need think values allowed. ’re asking people self-report gender, options allow choose ? enough allow “male” “female”? need “” option? give people specific options instead let answer words? open set possible values include verbal response, interpret answers?Operationalisation tricky business, ’s “one, true way” . way choose operationalise informal concept “age” “gender” formal measurement depends need use measurement . Often ’ll find community scientists work area fairly well-established ideas go . words, operationalisation needs thought case case basis. Nevertheless, lot issues specific individual research project, aspects pretty general.moving want take moment clear terminology, process introduce one term. four different things closely related :theoretical construct. thing ’re trying take measurement , like “age”, “gender” “opinion”. theoretical construct can’t directly observed, often ’re actually bit vague.measure. measure refers method tool use make observations. question survey, behavioural observation brain scan count measure.operationalisation. term “operationalisation” refers logical connection measure theoretical construct, process try derive measure theoretical construct.variable. Finally, new term. variable end apply measure something world. , variables actual “data” end data sets.practice, even scientists tend blur distinction things, ’s helpful try understand differences.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"scales-of-measurement","chapter":"2 A brief introduction to research design","heading":"2.2 Scales of measurement","text":"previous section indicates, outcome psychological measurement called variable. variables qualitative type ’s useful understand types . useful concept distinguishing different types variables ’s known scales measurement.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"nominal-scale","chapter":"2 A brief introduction to research design","heading":"2.2.1 Nominal scale","text":"nominal scale variable (also referred categorical variable) one particular relationship different possibilities. kinds variables doesn’t make sense say one “bigger’ ”better” one, absolutely doesn’t make sense average . classic example “eye colour”. Eyes can blue, green brown, amongst possibilities, none “bigger” one. result, feel really weird talk “average eye colour”. Similarly, gender nominal : male isn’t better worse female. Neither make sense try talk “average gender”. short, nominal scale variables thing can say different possibilities different. ’s .Let’s take slightly closer look . Suppose research people commute work. One variable measure kind transportation people use get work. “transport type” variable quite possible values, including: “train”, “bus”, “car”, “bicycle”. now, let’s suppose four possibilities. imagine ask 100 people got work today, result (Table 2.1).Table 2.1:  100 people get work todaySo, ’s average transportation type? Obviously, answer isn’t one. ’s silly question ask. can say travel car popular method, travel train least popular method, ’s . Similarly, notice order list options isn’t interesting. chosen display data like Table 2.2.Table 2.2:  100 people get work today, different view…nothing really changes.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"ordinal-scale","chapter":"2 A brief introduction to research design","heading":"2.2.2 Ordinal scale","text":"Ordinal scale variables bit structure nominal scale variables, lot. ordinal scale variable one natural, meaningful way order different possibilities, can’t anything else. usual example given ordinal variable “finishing position race”. can say person finished first faster person finished second, don’t know much faster. consequence know 1st \\(>\\) 2nd, know 2nd \\(>\\) 3rd, difference 1st 2nd might much larger difference 2nd 3rd.’s psychologically interesting example. Suppose ’m interested people’s attitudes climate change. go ask people pick statement (four listed statements) closely matches beliefs:Temperatures rising human activityTemperatures rising don’t know whyTemperatures rising humansTemperatures risingNotice four statements actually natural ordering, terms “extent agree current science”. Statement 1 close match, statement 2 reasonable match, statement 3 isn’t good match, statement 4 strong opposition current science. , terms thing ’m interested (extent people endorse science), can order items 1 \\(>\\) 2 \\(>\\) 3 \\(>\\) 4. Since ordering exists, weird list options like …Temperatures rising humansTemperatures rising human activityTemperatures risingTemperatures rising don’t know …seems violate natural “structure” question., let’s suppose asked 100 people questions, got answers shown Table 2.3.Table 2.3:  Attitudes climate changeWhen analysing data seems quite reasonable try group (1), (2) (3) together, say 81 100 people willing least partially endorse science. ’s also quite reasonable group (2), (3) (4) together say 49 100 people registered least disagreement dominant scientific view. However, entirely bizarre try group (1), (2) (4) together say 90 100 people said… ? ’s nothing sensible allows group responses together .said, notice can use natural ordering items construct sensible groupings, can’t average . instance, simple example , “average” response question 1.97. can tell means ’d love know, seems like gibberish !","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"interval-scale","chapter":"2 A brief introduction to research design","heading":"2.2.3 Interval scale","text":"contrast nominal ordinal scale variables, interval scale ratio scale variables variables numerical value genuinely meaningful. case interval scale variables differences numbers interpretable, variable doesn’t “natural” zero value. good example interval scale variable measuring temperature degrees celsius. instance, 15\\(^{\\circ}\\) yesterday 18\\(^{\\circ}\\) today, 3\\(^{\\circ}\\) difference two genuinely meaningful. Moreover, 3\\(^{\\circ}\\) difference exactly 3\\(^{\\circ}\\) difference 7\\(^{\\circ}\\) 10\\(^{\\circ}\\). short, addition subtraction meaningful interval scale variables.7However, notice 0\\(^{\\circ}\\) mean “temperature ”. actually means “temperature water freezes”, pretty arbitrary. consequence becomes pointless try multiply divide temperatures. wrong say 20\\(^{\\circ}\\) twice hot 10\\(^{\\circ}\\), just weird meaningless try claim 20\\(^{\\circ}\\) negative two times hot -10\\(^{\\circ}\\)., lets look psychological example. Suppose ’m interested looking attitudes first-year university students changed time. Obviously, ’m going want record year student started. interval scale variable. student started 2003 arrive 5 years student started 2008. However, completely daft divide 2008 2003 say second student started “1.0024 times later” first one. doesn’t make sense .","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"ratio-scale","chapter":"2 A brief introduction to research design","heading":"2.2.4 Ratio scale","text":"fourth final type variable consider ratio scale variable, zero really means zero, ’s okay multiply divide. good psychological example ratio scale variable response time (RT). lot tasks ’s common record amount time somebody takes solve problem answer question, ’s indicator difficult task . Suppose Alan takes 2.3 seconds respond question, whereas Ben takes 3.1 seconds. interval scale variable, addition subtraction meaningful . Ben really take 3.1 - 2.3 = 0.8 seconds longer Alan . However, notice multiplication division also make sense : Ben took 3.1/2.3 = 1.35 times long Alan answer question. reason can ratio scale variable RT “zero seconds” really mean “time ”.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"continuous-versus-discrete-variables","chapter":"2 A brief introduction to research design","heading":"2.2.5 Continuous versus discrete variables","text":"’s second kind distinction need aware , regarding types variables can run . distinction continuous variables discrete variables (Table 2.4). difference follows:continuous variable one , two values can think , ’s always logically possible another value .discrete variable , effect, variable isn’t continuous. discrete variable ’s sometimes case ’s nothing middle.Table 2.4:  relationship scales measurement discrete/continuity distinction. Cells tick mark correspond things possibleThese definitions probably seem bit abstract, ’re pretty simple see examples. instance, response time continuous. Alan takes 3.1 seconds Ben takes 2.3 seconds respond question, Cameron’s response time lie took 3.0 seconds. course also possible David take 3.031 seconds respond, meaning RT lie Cameron’s Alan’s. practice might impossible measure RT precisely, ’s certainly possible principle. can always find new value RT two ones regard RT continuous measure.Discrete variables occur rule violated. example, nominal scale variables always discrete. isn’t type transportation falls “” trains bicycles, strict mathematical way 2.3 falls 2 3. transportation type discrete. Similarly, ordinal scale variables always discrete. Although “2nd place” fall “1st place” “3rd place”, ’s nothing can logically fall “1st place” “2nd place”. Interval scale ratio scale variables can go either way. saw , response time (ratio scale variable) continuous. Temperature degrees celsius (interval scale variable) also continuous. However, year went school (interval scale variable) discrete. ’s year 2002 2003. number questions get right true--false test (ratio scale variable) also discrete. Since true--false question doesn’t allow “partially correct”, ’s nothing 5/10 6/10. Table 2.4 summarises relationship scales measurement discrete/continuity distinction. Cells tick mark correspond things possible. ’m trying hammer point home, () textbooks get wrong, (b) people often say things like “discrete variable” mean “nominal scale variable”. ’s unfortunate.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"some-complexities","chapter":"2 A brief introduction to research design","heading":"2.2.6 Some complexities","text":"Okay, know ’re going shocked hear , real world much messier little classification scheme suggests. variables real life actually fall nice neat categories, need kind careful treat scales measurement hard fast rules. doesn’t work like . ’re guidelines, intended help think situations treat different variables differently. Nothing .let’s take classic example, maybe classic example, psychological measurement tool: Likert scale. humble Likert scale bread butter tool survey design. filled hundreds, maybe thousands, odds ’ve even used one . Suppose survey question looks like :following best describes opinion statement “pirates freaking awesome”?options presented participant :Strongly disagreeDisagreeNeither agree disagreeAgreeStrongly agreeThis set items example 5-point Likert scale, people asked choose among one several (case 5) clearly ordered possibilities, generally verbal descriptor given case. However, ’s necessary items explicitly described. perfectly good example 5-point Likert scale :Strongly disagreeStrongly agreeLikert scales handy, somewhat limited, tools. question kind variable ? ’re obviously discrete, since can’t give response 2.5. ’re obviously nominal scale, since items ordered; ’re ratio scale either, since ’s natural zero.ordinal scale interval scale? One argument says can’t really prove difference “strongly agree” “agree” size difference “agree” “neither agree disagree”. fact, everyday life ’s pretty obvious ’re . suggests treat Likert scales ordinal variables. hand, practice participants seem take whole “scale 1 5” part fairly seriously, tend act differences five response options fairly similar one another. consequence, lot researchers treat Likert scale data interval scale.8 ’s interval scale, practice ’s close enough usually think quasi-interval scale.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"assessing-the-reliability-of-a-measurement","chapter":"2 A brief introduction to research design","heading":"2.3 Assessing the reliability of a measurement","text":"point ’ve thought little bit operationalise theoretical construct thereby create psychological measure. ’ve seen applying psychological measures end variables, can come many different types. point, start discussing obvious question: measurement good? ’ll terms two related ideas: reliability validity. Put simply, reliability measure tells precisely measuring something, whereas validity measure tells accurate measure . section ’ll talk reliability; ’ll talk validity section Assessing validity study.Reliability actually simple concept. refers repeatability consistency measurement. measurement weight means “bathroom scale” reliable. step scales , ’ll keep giving answer. Measuring intelligence means “asking mum” unreliable. days tells ’m bit thick, days tells ’m complete idiot. Notice concept reliability different question whether measurements correct (correctness measurement relates ’s validity). ’m holding sack potatos step bathroom scales measurement still reliable: always give answer. However, highly reliable answer doesn’t match true weight , therefore ’s wrong. technical terms, reliable invalid measurement. Similarly, whilst mum’s estimate intelligence bit unreliable, might right. Maybe ’m just bright, estimate intelligence fluctuates pretty wildly day day, ’s basically right. unreliable valid measure. course, mum’s estimates unreliable ’s going hard figure one many claims intelligence actually right one. extent, , unreliable measure tends end invalid practical purposes; much many people say reliability necessary (sufficient) ensure validity.Okay, now ’re clear distinction reliability validity, let’s think different ways might measure reliability:Test-retest reliability. relates consistency time. repeat measurement later date get answer?Inter-rater reliability. relates consistency across people. someone else repeats measurement (e.g., someone else rates intelligence) produce answer?Parallel forms reliability. relates consistency across theoretically-equivalent measurements. use different set bathroom scales measure weight give answer?Internal consistency reliability. measurement constructed lots different parts perform similar functions (e.g., personality questionnaire result added across several questions) individual parts tend give similar answers. ’ll look particular form reliability later book, section Internal consistency reliability analysis.measurements need possess forms reliability. instance, educational assessment can thought form measurement. One subjects teach, Computational Cognitive Science, assessment structure research component exam component (plus things). exam component intended measure something different research component, assessment whole low internal consistency. However, within exam several questions intended (approximately) measure things, tend produce similar outcomes. exam fairly high internal consistency. . demand reliability situations want measuring thing!","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"the-role-of-variables-predictors-and-outcomes","chapter":"2 A brief introduction to research design","heading":"2.4 The “role” of variables: predictors and outcomes","text":"’ve got one last piece terminology need explain moving away variables. Normally, research end lots different variables. , analyse data, usually try explain variables terms variables. ’s important keep two roles “thing explaining” “thing explained” distinct. let’s clear now. First, might well get used idea using mathematical symbols describe variables, since ’s going happen . Let’s denote “explained” variable \\(Y\\), denote variables “explaining” \\(X_1 , X_2\\), etc.analysis different names \\(X\\) \\(Y\\), since play different roles analysis. classical names roles independent variable (IV) dependent variable (DV). IV variable use explaining (.e., \\(X\\)) DV variable explained (.e.,$Y $). logic behind names goes like : really relationship \\(X\\) \\(Y\\) can say \\(Y\\)depends \\(X\\), designed study “properly” \\(X\\) isn’t dependent anything else. However, personally find names horrible. ’re hard remember ’re highly misleading () IV never actually “independent everything else”, (b) ’s relationship DV doesn’t actually depend IV. fact, ’m person thinks IV DV just awful names, number alternatives find appealing. terms ’ll use book predictors outcomes. idea ’re trying use \\(X\\) (predictors) make guesses \\(Y\\) (outcomes).9 summarised Table 2.5.Table 2.5:  Variable distinctions","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"experimental-and-non-experimental-research","chapter":"2 A brief introduction to research design","heading":"2.5 Experimental and non-experimental research","text":"One big distinctions aware distinction “experimental research” “non-experimental research”. make distinction, ’re really talking degree control researcher exercises people events study.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"experimental-research","chapter":"2 A brief introduction to research design","heading":"2.5.1 Experimental research","text":"key feature experimental research researcher controls aspects study, especially participants experience study. particular, researcher manipulates varies predictor variables (IVs) allows outcome variable (DV) vary naturally. idea deliberately vary predictors (IVs) see causal effects outcomes. Moreover, order ensure ’s possibility something predictor variables causing outcomes, everything else kept constant way “balanced”, ensure effect results. practice, ’s almost impossible think everything else might influence outcome experiment, much less keep constant. standard solution randomisation. , randomly assign people different groups, give group different treatment (.e., assign different values predictor variables). ’ll talk randomisation later, now ’s enough say randomisation minimise (eliminate) possibility systematic difference groups.Let’s consider simple, completely unrealistic grossly unethical example. Suppose wanted find smoking causes lung cancer. One way find people smoke people don’t smoke look see smokers higher rate lung cancer. proper experiment, since researcher doesn’t lot control isn’t smoker. really matters. instance, might people choose smoke cigarettes also tend poor diets, maybe tend work asbestos mines, whatever. point groups (smokers non-smokers) actually differ lots things, just smoking. might higher incidence lung cancer among smokers caused something else, smoking per se. technical terms things (e.g. diet) called “confounders”, ’ll talk just moment.meantime, let’s consider proper experiment might look like. Recall concern smokers non-smokers might differ lots ways. solution, long ethics, control smokes doesn’t. Specifically, randomly divide young non-smokers two groups force half become smokers, ’s unlikely groups differ respect fact half smoke. way, smoking group gets cancer higher rate non-smoking group, can feel pretty confident () smoking cause cancer (b) ’re murderers.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"non-experimental-research","chapter":"2 A brief introduction to research design","heading":"2.5.2 Non-experimental research","text":"Non-experimental research broad term covers “study researcher doesn’t much control experiment”. Obviously, control something scientists like , previous example illustrates lots situations can’t shouldn’t try obtain control. Since ’s grossly unethical (almost certainly criminal) force people smoke order find get cancer, good example situation really shouldn’t try obtain experimental control. reasons . Even leaving aside ethical issues, “smoking experiment” issues. instance, suggested “force” half people become smokers, talking starting sample non-smokers, forcing become smokers. sounds like kind solid, evil experimental design mad scientist love, might sound way investigating effect real world. instance, suppose smoking causes lung cancer people poor diets, suppose also people normally smoke tend poor diets. However, since “smokers” experiment aren’t “natural” smokers (.e., forced non-smokers become smokers, didn’t take normal, real life characteristics smokers might tend possess) probably better diets. , silly example wouldn’t get lung cancer experiment fail, violates structure “natural” world (technical name “artefactual” result).One distinction worth making two types non-experimental research difference quasi-experimental research case studies. example discussed earlier, wanted examine incidence lung cancer among smokers non-smokers without trying control smokes doesn’t, quasi-experimental design. , ’s experiment don’t control predictors (IVs). can still use statistics analyse results, lot careful circumspect.alternative approach, case studies, aims provide detailed description one instances. general, can’t use statistics analyse results case studies ’s usually hard draw general conclusions “people general” isolated examples. However, case studies useful situations. Firstly, situations don’t alternative. Neuropsychology issue lot. Sometimes, just can’t find lot people brain damage specific brain area, thing can describe cases much detail much care can. However, ’s also genuine advantages case studies. don’t many people study ability invest lots time effort trying understand specific factors play case. valuable thing . consequence, case studies can complement statistically-oriented approaches see experimental quasi-experimental designs. won’t talk much case studies book, nevertheless valuable tools!","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"assessing-the-validity-of-a-study","chapter":"2 A brief introduction to research design","heading":"2.6 Assessing the validity of a study","text":"thing, scientist wants research “valid”. conceptual idea behind validity simple. Can trust results study? , study invalid. However, whilst ’s easy state, practice ’s much harder check validity check reliability. honesty, ’s precise, clearly agreed upon notion validity actually . fact, lots different kinds validity, raises ’s issues. forms validity relevant studies. ’m going talk five different types validity:Internal validityExternal validityConstruct validityFace validityEcological validityFirst, quick guide matters . (1) Internal external validity important, since tie directly fundamental question whether study really works. (2) Construct validity asks whether ’re measuring think . (3) Face validity isn’t terribly important except insofar care “appearances”. (4) Ecological validity special case face validity corresponds kind appearance might care lot.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"internal-validity","chapter":"2 A brief introduction to research design","heading":"2.6.1 Internal validity","text":"Internal validity refers extent able draw correct conclusions causal relationships variables. ’s called “internal” refers relationships things “inside” study. Let’s illustrate concept simple example. Suppose ’re interested finding whether university education makes write better. , get group first year students, ask write 1000 word essay, count number spelling grammatical errors make. find third-year students, obviously university education first-years, repeat exercise. let’s suppose turns third-year students produce fewer errors. conclude university education improves writing skills. Right? Except big problem experiment third-year students older ’ve experience writing things. ’s hard know sure causal relationship . older people write better? people writing experience? people education? true cause superior performance third-years? Age? Experience? Education? can’t tell. example failure internal validity, study doesn’t properly tease apart causal relationships different variables.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"external-validity","chapter":"2 A brief introduction to research design","heading":"2.6.2 External validity","text":"External validity relates generalisability applicability findings. , extent expect see pattern results “real life” saw study. put bit precisely, study psychology involve fairly specific set questions tasks, occur specific environment, involve participants drawn particular subgroup (disappointingly often college students!). , turns results don’t actually generalise apply people situations beyond ones studied, ’ve got lack external validity.classic example issue fact large proportion studies psychology use undergraduate psychology students participants. Obviously, however, researchers don’t care psychology students. care people general. Given , study uses psychology students participants always carries risk lacking external validity. , ’s something “special” psychology students makes different general population relevant respect, may start worrying lack external validity.said, absolutely critical realise study uses psychology students necessarily problem external validity. ’ll talk later, ’s common mistake ’m going mention . external validity study threatened choice population () population sample participants narrow (e.g., psychology students), (b) narrow population sampled systematically different general population respect relevant psychological phenomenon intend study. italicised part bit lots people forget. true psychology undergraduates differ general population lots ways, study uses psychology students may problems external validity. However, differences aren’t relevant phenomenon ’re studying, ’s nothing worry . make bit concrete two extreme examples:want measure “attitudes general public towards psychotherapy”, participants psychology students. study almost certainly problem external validity.want measure effectiveness visual illusion, participants psychology students. study unlikely problem external validityHaving just spent last couple paragraphs focusing choice participants, since ’s big issue everyone tends worry , ’s worth remembering external validity broader concept. following also examples things might pose threat external validity, depending kind study ’re :People might answer “psychology questionnaire” manner doesn’t reflect real life.lab experiment (say) “human learning” different structure learning problems people face real life.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"construct-validity","chapter":"2 A brief introduction to research design","heading":"2.6.3 Construct validity","text":"Construct validity basically question whether ’re measuring want measuring. measurement good construct validity actually measuring correct theoretical construct, bad construct validity doesn’t. give simple (ridiculous) example, suppose ’m trying investigate rates university students cheat exams. way attempt measure asking cheating students stand lecture theatre can count . class 300 students 0 people claim cheaters. therefore conclude proportion cheaters class 0%. Clearly bit ridiculous. point deep methodological example, rather explain construct validity . problem measure ’m trying measure “proportion people cheat” ’m actually measuring “proportion people stupid enough cheating, bloody minded enough pretend ”. Obviously, aren’t thing! study gone wrong, measurement poor construct validity.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"face-validity","chapter":"2 A brief introduction to research design","heading":"2.6.4 Face validity","text":"Face validity simply refers whether measure “looks like” ’s ’s supposed , nothing . design test intelligence, people look say “, test doesn’t measure intelligence”, measure lacks face validity. ’s simple . Obviously, face validity isn’t important pure scientific perspective. , care whether measure actually ’s supposed , whether looks like ’s supposed . consequence, generally don’t care much face validity. said, concept face validity serves three useful pragmatic purposes:Sometimes, experienced scientist “hunch” particular measure won’t work. sorts hunches strict evidentiary value, ’s often worth paying attention . often times people knowledge can’t quite verbalise, might something worry even can’t quite say . words, someone trust criticises face validity study, ’s worth taking time think carefully design see can think reasons might go awry. Mind , don’t find reason concern, probably worry. , face validity really doesn’t matter much.Often (often), completely uninformed people also “hunch” research crap. ’ll criticise internet something. close inspection may notice criticisms actually focused entirely study “looks”, anything deeper. concept face validity useful gently explaining people need substantiate arguments .Expanding last point, beliefs untrained people critical (e.g., often case applied research actually want convince policy makers something ) care face validity. Simply , whether like , lot people use face validity proxy real validity. want government change law scientific psychological grounds, won’t matter good studies “really” . lack face validity ’ll find politicians ignore . course, ’s somewhat unfair policy often depends appearance fact, ’s things go.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"ecological-validity","chapter":"2 A brief introduction to research design","heading":"2.6.5 Ecological validity","text":"Ecological validity different notion validity, similar external validity, less important. idea , order ecologically valid, entire set study closely approximate real world scenario investigated. sense, ecological validity kind face validity. relates mostly whether study “looks” right, bit rigour . ecologically valid study look right fairly specific way. idea behind intuition study ecologically valid likely externally valid. ’s guarantee, course. nice thing ecological validity ’s much easier check whether study ecologically valid check whether study externally valid. simple example eyewitness identification studies. studies tend done university setting, often fairly simple array faces look , rather line . length time seeing “criminal” asked identify suspect “line ” usually shorter. “crime” isn’t real ’s chance witness scared, police officers present ’s much chance feeling pressured. things mean study definitely lacks ecological validity. might (might ) mean also lacks external validity.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"confounds-artefacts-and-other-threats-to-validity","chapter":"2 A brief introduction to research design","heading":"2.7 Confounds, artefacts and other threats to validity","text":"look issue validity general fashion two biggest worries confounders artefacts. two terms defined following way:Confounder: confounder additional, often unmeasured variable10 turns related predictors outcome. existence confounders threatens internal validity study can’t tell whether predictor causes outcome, confounding variable causes .Artefact: result said “artefactual” holds special situation happened test study. possibility result artefact describes threat external validity, raises possibility can’t generalise apply results actual population care .general rule confounders bigger concern non-experimental studies, precisely ’re proper experiments. definition, ’re leaving lots things uncontrolled, ’s lot scope confounders present study. Experimental research tends much less vulnerable confounders. control happens study, can prevent confounders affecting results. random allocation, example, confounders distributed randomly, evenly, different groups.However, always swings roundabouts start thinking artefacts rather confounders shoe firmly foot. part, artefactual results tend concern experimental studies non-experimental studies. see , helps realise reason lot studies non-experimental precisely researcher trying examine human behaviour naturalistic context. working real-world context lose experimental control (making vulnerable confounders), tend studying human psychology “wild” reduce chances getting artefactual result. , put another way, take psychology wild bring lab (usually gain experimental control), always run risk accidentally studying something different wanted study.warned though. rough guide . ’s absolutely possible confounders experiment, get artefactual results non-experimental studies. can happen sorts reasons, least experimenter researcher error. practice, ’s really hard think everything ahead time even good researchers make mistakes.Although ’s sense almost threat validity can characterised confounder artefact, ’re pretty vague concepts. let’s look common examples.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"history-effects","chapter":"2 A brief introduction to research design","heading":"2.7.1 History effects","text":"History effects refer possibility specific events may occur study might influence outcome measure. instance, something might happen pretest post-test. -testing participant 23 participant 24. Alternatively, might ’re looking paper older study perfectly valid time, world changed enough since conclusions longer trustworthy. Examples things count history effects :’re interested people think risk uncertainty. started data collection December 2010. finding participants collecting data takes time, ’re still finding new people February 2011. Unfortunately (even unfortunately others), Queensland floods occurred January 2011 causing billions dollars damage killing many people. surprisingly, people tested February 2011 express quite different beliefs handling risk people tested December 2010. () reflects “true” beliefs participants? think answer probably . Queensland floods genuinely changed beliefs Australian public, though possibly temporarily. key thing “history” people tested February quite different people tested December.’re interested people think risk uncertainty. started data collection December 2010. finding participants collecting data takes time, ’re still finding new people February 2011. Unfortunately (even unfortunately others), Queensland floods occurred January 2011 causing billions dollars damage killing many people. surprisingly, people tested February 2011 express quite different beliefs handling risk people tested December 2010. () reflects “true” beliefs participants? think answer probably . Queensland floods genuinely changed beliefs Australian public, though possibly temporarily. key thing “history” people tested February quite different people tested December.’re testing psychological effects new anti-anxiety drug. measure anxiety administering drug (e.g., self-report, taking physiological measures). administer drug, afterwards take measures. middle however, lab Los Angeles, ’s earthquake increases anxiety participants.’re testing psychological effects new anti-anxiety drug. measure anxiety administering drug (e.g., self-report, taking physiological measures). administer drug, afterwards take measures. middle however, lab Los Angeles, ’s earthquake increases anxiety participants.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"maturation-effects","chapter":"2 A brief introduction to research design","heading":"2.7.2 Maturation effects","text":"history effects, maturational effects fundamentally change time. However, maturation effects aren’t response specific events. Rather, relate people change time. get older, get tired, get bored, etc. examples maturation effects :developmental psychology research need aware children grow quite rapidly. , suppose want find whether educational trick helps vocabulary size among 3 year olds. One thing need aware vocabulary size children age growing incredible rate (multiple words per day) . design study without taking maturational effect account, won’t able tell educational trick works.running long experiment lab (say, something goes 3 hours) ’s likely people begin get bored tired, maturational effect cause performance decline regardless anything else going experiment","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"repeated-testing-effects","chapter":"2 A brief introduction to research design","heading":"2.7.3 Repeated testing effects","text":"important type history effect effect repeated testing. Suppose want take two measurements psychological construct (e.g., anxiety). One thing might worried first measurement effect second measurement. words, history effect “event” influences second measurement first measurement ! uncommon. Examples include:Learning practice: e.g., “intelligence” time 2 might appear go relative time 1 participants learned general rules solve “intelligence-test-style” questions first testing session.Familiarity testing situation: e.g., people nervous time 1, might make performance go . sitting first testing situation might calm lot precisely ’ve seen testing looks like.Auxiliary changes caused testing: e.g., questionnaire assessing mood boring mood rating measurement time 2 likely “bored” precisely boring measurement made time 1.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"selection-bias","chapter":"2 A brief introduction to research design","heading":"2.7.4 Selection bias","text":"Selection bias pretty broad term. Suppose ’re running experiment two groups participants group gets different “treatment”, want see different treatments lead different outcomes. However, suppose , despite best efforts, ’ve ended gender imbalance across groups (say, group 80% females group B 50% females). might sound like never happen , trust , can. example selection bias, people “selected ” two groups different characteristics. characteristics turns relevant (say, treatment works better females males) ’re lot trouble.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"differential-attrition","chapter":"2 A brief introduction to research design","heading":"2.7.5 Differential attrition","text":"thinking effects attrition, sometimes helpful distinguish two different types. first homogeneous attrition, attrition effect groups, treatments conditions. example gave , attrition homogeneous () easily bored participants dropping conditions experiment rate. general, main effect homogeneous attrition likely makes sample unrepresentative. , biggest worry ’ll generalisability results decreases. words, lose external validity.second type attrition heterogeneous attrition, attrition effect different different groups. often called differential attrition, kind selection bias caused study . Suppose , first time ever history psychology, manage find perfectly balanced representative sample people. start running “Dani’s incredibly long tedious experiment” perfect sample , study incredibly long tedious, lots people start dropping . can’t stop . Participants absolutely right stop experiment, time, whatever reason feel like, researchers morally (professionally) obliged remind people right. , suppose “Dani’s incredibly long tedious experiment” high drop rate. suppose odds drop random? Answer: zero. Almost certainly people remain conscientious, tolerant boredom, etc., leave. extent (say) conscientiousness relevant psychological phenomenon care , attrition can decrease validity results.’s another example. Suppose design experiment two conditions. “treatment” condition, experimenter insults participant gives questionnaire designed measure obedience. “control” condition, experimenter engages bit pointless chitchat gives questionnaire. Leaving aside questionable scientific merits dubious ethics study, let’s think might go wrong . general rule, someone insults face tend get much less co-operative. , ’s pretty good chance lot people going drop treatment condition control condition. drop isn’t going random. people likely drop probably people don’t care much importance obediently sitting experiment. Since bloody minded disobedient people left treatment group control group, ’ve introduced confound: people actually took questionnaire treatment group already likely dutiful obedient people control group. short, study insulting people doesn’t make obedient. makes disobedient people leave experiment! internal validity experiment completely shot.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"non-response-bias","chapter":"2 A brief introduction to research design","heading":"2.7.6 Non-response bias","text":"Non-response bias closely related selection bias differential attrition. simplest version problem goes like . mail survey 1000 people 300 reply. 300 people replied almost certainly random subsample. People respond surveys systematically different people don’t. introduces problem trying generalise 300 people replied population large, since now non-random sample. issue non-response bias general , though. Among (say) 300 people respond survey, might find everyone answers every question. (say) 80 people chose answer one questions, introduce problems? always, answer maybe. question wasn’t answered last page questionnaire, 80 surveys returned last page missing, ’s good chance missing data isn’t big deal; probably pages just fell . However, question 80 people didn’t answer confrontational invasive personal question questionnaire, almost certainly ’ve got problem. essence, ’re dealing ’s called problem missing data. data missing “lost” randomly, ’s big problem. ’s missing systematically, can big problem.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"regression-to-the-mean","chapter":"2 A brief introduction to research design","heading":"2.7.7 Regression to the mean","text":"Regression mean refers situation select data based extreme value measure. variable natural variation almost certainly means take subsequent measurement later measurement less extreme first one, purely chance.’s example. Suppose ’m interested whether psychology education adverse effect smart kids. , find 20 psychology students best high school grades look well ’re university. turns ’re lot better average, ’re topping class university even though top classes high school. ’s going ? natural first thought must mean psychology classes must adverse effect students. However, might well explanation, ’s likely ’re seeing example “regression mean”. see works, let’s take moment think required get best mark class, regardless whether class high school university. ’ve got big class going lots smart people enrolled. get best mark smart, work hard, bit lucky. exam ask just right questions idiosyncratic skills, avoid making dumb mistakes (sometimes) answering . ’s thing, whilst intelligence hard work transferable one class next, luck isn’t. people got lucky high school won’t people get lucky university. ’s definition “luck”. consequence select people extreme values one measurement (top 20 students), ’re selecting hard work, skill luck. luck doesn’t transfer second measurement (skill work), people expected drop little bit measure second time (university). scores fall back little bit, back towards everyone else. regression mean.Regression mean surprisingly common. instance, two tall people kids children tend taller average tall parents. reverse happens short parents. Two short parents tend short children, nevertheless kids tend taller parents. can also extremely subtle. instance, studies done suggested people learn better negative feedback positive feedback. However, way people tried show give people positive reinforcement whenever good, negative reinforcement bad. see positive reinforcement people tended worse, negative reinforcement tended better. notice ’s selection bias ! people well, ’re selecting “high” values, expect, regression mean, performance next trial worse regardless whether reinforcement given. Similarly, bad trial, people tend improve . apparent superiority negative feedback artefact caused regression mean (see Kahneman Tversky (1973), discussion).","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"experimenter-bias","chapter":"2 A brief introduction to research design","heading":"2.7.8 Experimenter bias","text":"Experimenter bias can come multiple forms. basic idea experimenter, despite best intentions, can accidentally end influencing results experiment subtly communicating “right answer” “desired behaviour” participants. Typically, occurs experimenter special knowledge participant , example right answer questions asked knowledge expected pattern performance condition participant . classic example happening case study “Clever Hans”, dates back 1907 (Pfungst 1911). Clever Hans horse apparently able read count perform human like feats intelligence. Clever Hans became famous, psychologists started examining behaviour closely. turned , surprisingly, Hans didn’t know maths. Rather, Hans responding human observers around , humans know count horse learned change behaviour people changed .general solution problem experimenter bias engage double blind studies, neither experimenter participant knows condition participant knows desired behaviour . provides good solution problem, ’s important recognise ’s quite ideal, hard pull perfectly. instance, obvious way try construct double blind study one Ph.D. students (one doesn’t know anything experiment) run study. feels like enough. person () knows details (e.g., correct answers questions, assignments participants conditions) interaction participants, person talking people (Ph.D. student) doesn’t know anything. Except reality last part unlikely true. order Ph.D. student run study effectively need briefed , researcher. , happens, Ph.D. student also knows knows bit general beliefs people psychology (e.g., tend think humans much smarter psychologists give credit ). result , ’s almost impossible experimenter avoid knowing little bit expectations . even little bit knowledge can effect. Suppose experimenter accidentally conveys fact participants expected well task. Well, ’s thing called “Pygmalion effect”, expect great things people ’ll tend rise occasion. expect fail ’ll . words, expectations become self-fulfilling prophesy.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"demand-effects-and-reactivity","chapter":"2 A brief introduction to research design","heading":"2.7.9 Demand effects and reactivity","text":"talking experimenter bias, worry experimenter’s knowledge desires experiment communicated participants, can change people’s behaviour (Rosenthal 1966). However, even manage stop happening, ’s almost impossible stop people knowing ’re part psychological study. mere fact knowing someone watching studying can pretty big effect behaviour. generally referred reactivity demand effects. basic idea captured Hawthorne effect: people alter performance attention study focuses . effect takes name study took place “Hawthorne Works” factory outside Chicago (see Adair (1984)). study, 1920s, looked effects factory lighting worker productivity. , importantly, change worker behaviour occurred workers knew studied, rather effect factory lighting.get bit specific ways mere fact study can change people behave, helps think like social psychologist look roles people might adopt experiment might adopt corresponding events occurring real world:good participant tries helpful researcher. seeks figure experimenter’s hypotheses confirm .negative participant exact opposite good participant. seeks break destroy study hypothesis way.faithful participant unnaturally obedient. seeks follow instructions perfectly, regardless might happened realistic setting.apprehensive participant gets nervous tested studied, much behaviour becomes highly unnatural, overly socially desirable.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"placebo-effects","chapter":"2 A brief introduction to research design","heading":"2.7.10 Placebo effects","text":"placebo effect specific type demand effect worry lot . refers situation mere fact treated causes improvement outcomes. classic example comes clinical trials. give people completely chemically inert drug tell ’s cure disease, tend get better faster people aren’t treated . words, people’s belief treated causes improved outcomes, drug.However, current consensus medicine true placebo effects quite rare previously considered placebo effect fact combination natural healing (people just get better ), regression mean quirks study design. interest psychology strongest evidence least placebo effect self-reported outcomes, notably treatment pain (Hróbjartsson Gøtzsche 2010).","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"situation-measurement-and-sub-population-effects","chapter":"2 A brief introduction to research design","heading":"2.7.11 Situation, measurement and sub-population effects","text":"respects, terms catch-term “threats external validity”. refer fact choice sub-population draw participants, location, timing manner run study (including collects data) tools use make measurements might influencing results. Specifically, worry things might influencing results way results won’t generalise wider array people, places measures.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"fraud-deception-and-self-deception","chapter":"2 A brief introduction to research design","heading":"2.7.12 Fraud, deception and self-deception","text":"difficult get man understand something, salary depends understanding .\n- Upton SinclairThere’s one final thing feel mention. reading textbooks often say assessing validity study couldn’t help notice seem make assumption researcher honest. find hilarious. vast majority scientists honest, experience least, .11 , mentioned earlier, scientists immune belief bias. ’s easy researcher end deceiving believing wrong thing, can lead conduct subtly flawed research hide flaws write . need consider (probably unlikely) possibility outright fraud, also (probably quite common) possibility research unintentionally “slanted”. opened standard textbooks didn’t find much discussion problem, ’s attempt list ways issues can arise:Data fabrication. Sometimes, people just make data. occasionally done “good” intentions. instance, researcher believes fabricated data reflect truth, may actually reflect “slightly cleaned ” versions actual data. occasions, fraud deliberate malicious. high-profile examples data fabrication alleged shown include Cyril Burt (psychologist thought fabricated data), Andrew Wakefield (accused fabricating data connecting MMR vaccine autism) Hwang Woo-suk (falsified lot data stem cell research).Hoaxes. Hoaxes share lot similarities data fabrication, differ intended purpose. hoax often joke, many intended (eventually) discovered. Often, point hoax discredit someone field. ’s quite well known scientific hoaxes occurred years (e.g., Piltdown man) deliberate attempts discredit particular fields research (e.g., Sokal affair).Data misrepresentation. fraud gets headlines, ’s much common experience see data misrepresented. say ’m referring newspapers getting wrong (, almost always). ’m referring fact often data don’t actually say researchers think say. guess , almost always, isn’t result deliberate dishonesty instead due lack sophistication data analyses. instance, think back example Simpson’s paradox discussed beginning book. ’s common see people present “aggregated” data kind sometimes, dig deeper find raw data find aggregated data tell different story disaggregated data. Alternatively, might find aspect data hidden, tells inconvenient story (e.g., researcher might choose refer particular variable). ’s lot variants , many hard detect.Study “misdesign”. Okay, one subtle. Basically, issue researcher designs study built-flaws flaws never reported paper. data reported completely real correctly analysed, produced study actually quite wrongly put together. researcher really wants find particular effect study set way make “easy” (artefactually) observe effect. One sneaky way , case ’re feeling like dabbling bit fraud , design experiment ’s obvious participants ’re “supposed” , let reactivity work magic . want can add trappings double blind experimentation won’t make difference since study materials subtly telling people want . write results fraud won’t obvious reader. ’s obvious participant ’re experimental context isn’t always obvious person reading paper. course, way ’ve described makes sound like ’s always fraud. Probably cases done deliberately, experience bigger concern unintentional misdesign. researcher believes study just happens end built flaw, flaw magically erases study written publication.Data mining & post hoc hypothesising. Another way authors study can less misrepresent data engaging ’s referred “data mining” (see Gelman Loken 2014, broader discussion part “garden forking paths” statistical analysis). ’ll discuss later, keep trying analyse data lots different ways, ’ll eventually find something “looks” like real effect isn’t. referred “data mining”. used quite rare data analysis used take weeks, now everyone powerful statistical software computers ’s becoming common. Data mining per se isn’t “wrong”, bigger risk ’re taking. thing wrong, suspect common, unacknowledged data mining. , researcher runs every possible analysis known humanity, finds one works, pretends analysis ever conducted. Worse yet, often “invent” hypothesis looking data cover data mining. clear. ’s wrong change beliefs looking data, reanalyse data using new “post hoc” hypotheses. wrong (suspect common) failing acknowledge ’ve done. acknowledge researchers able take behaviour account. don’t, can’t. makes behaviour deceptive. Bad!Publication bias & self-censoring. Finally, pervasive bias “non-reporting” negative results. almost impossible prevent. Journals don’t publish every article submitted . prefer publish articles find “something”. , 20 people run experiment looking whether reading Finnegans Wake causes insanity humans, 19 find doesn’t, one think going get published? Obviously, ’s one study find Finnegans Wake causes insanity.12 example publication bias. Since -one ever published 19 studies didn’t find effect, naive reader never know existed. Worse yet, researchers “internalise” bias end self-censoring research. Knowing negative results aren’t going accepted publication, never even try report . friend mine says “every experiment get published, also 10 failures”. ’s right. catch , (maybe ) studies failures boring reasons (e.g. stuffed something ) others might genuine “null” results acknowledge write “good” experiment. telling often hard . good place start paper Ioannidis (2005) depressing title “published research findings false”. ’d also suggest taking look work Kühberger, Fritz, Scherndl (2014) presenting statistical evidence actually happens psychology.’s probably lot issues like think , ’ll start . really want point blindingly obvious truth real world science conducted actual humans, gullible people automatically assumes everyone else honest impartial. Actual scientists aren’t usually naive, reason world likes pretend , textbooks usually write seem reinforce stereotype.","code":""},{"path":"a-brief-introduction-to-research-design.html","id":"summary","chapter":"2 A brief introduction to research design","heading":"2.8 Summary","text":"chapter isn’t really meant provide comprehensive discussion psychological research methods. require another volume just long one justice topic. However, real life statistics study design tightly intertwined ’s handy discuss key topics. chapter, ’ve briefly discussed following topics:Introduction psychological measurement. mean operationalise theoretical construct? mean variables take measurements?Scales measurement types variables. Remember two different distinctions . ’s difference discrete continuous data, ’s difference four different scale types (nominal, ordinal, interval ratio).Assessing reliability measurement. measure “” thing twice, expect see result? measure reliable. mean talk “” thing? Well, ’s different types reliability. Make sure remember .“role” variables: predictors outcomes. roles variables play analysis? Can remember difference predictors outcomes? Dependent independent variables? Etc.Experimental non-experimental research designs. makes experiment experiment? nice white lab coat, something researcher control variables?Assessing validity study. study measure want ? might things go wrong? imagination, long list possible ways things can go wrong?make clear study design critical part research methodology. built chapter classic little book Campbell Stanley (1963), course large number textbooks research design. Spend minutes favourite search engine ’ll find dozens.","code":""},{"path":"getting-started-with-jamovi.html","id":"getting-started-with-jamovi","chapter":"3 Getting started with jamovi","heading":"3 Getting started with jamovi","text":"Robots nice work .\n– Roger Zelazny13In chapter ’ll discuss get started jamovi. ’ll briefly talk download install jamovi, chapter focused getting started finding way around jamovi GUI. goal chapter learn statistical concepts: ’re just trying learn basics jamovi works get comfortable interacting system. ’ll spend bit time looking datasets variables. , ’ll get bit feel ’s like work jamovi.However, going specifics, ’s worth talking little might want use jamovi . Given ’re reading ’ve probably got reasons. However, reasons “’s stats class uses”, might worth explaining little lecturer chosen use jamovi class. course, don’t really know people choose jamovi ’m really talking use .’s sort obvious worth saying anyway: statistics computer faster, easier powerful statistics hand. Computers excel mindless repetitive tasks, lot statistical calculations mindless repetitive. people reason ever statistical calculations pencil paper learning purposes. class occasionally suggest calculations way, real value pedagogical. help get “feel” statistics calculations , ’s worth . !statistics conventional spreadsheet (e.g., Microsoft Excel) generally bad idea long run. Although many people likely feel familiar , spreadsheets limited terms analyses allow . get habit trying real life data analysis using spreadsheets ’ve dug deep hole.“student versions” (crippled versions real thing) cheaply, sell full powered “educational versions” price makes wince. also sell commercial licences staggeringly high price tag. business model suck student days leave dependent tools go real world. ’s hard blame trying, personally ’m favour shelling thousands dollars can avoid . can avoid . make use packages like jamovi open source free never get trapped pay exorbitant licensing fees.Something might appreciate now, love later anything involving data analysis, fact jamovi basically sophisticated front end free R statistical programming language. download install R get basic “packages” powerful . However, R open widely used, ’s become something standard tool statistics lots people write packages extend system. freely available . One consequences , ’ve noticed, look recent advanced data analysis textbooks lot use R.main reasons use jamovi. ’s without flaws, though. ’s relatively new14 huge set textbooks resources support , annoying quirks ’re pretty much stuck , whole think strengths outweigh weakness; option ’ve encountered far.","code":""},{"path":"getting-started-with-jamovi.html","id":"installing-jamovi","chapter":"3 Getting started with jamovi","heading":"3.1 Installing jamovi","text":"Okay, enough sales pitch. Let’s get started. Just piece software, jamovi needs installed “computer”, magical box cool things delivers free ponies. something along lines; may confusing computers iPad marketing campaigns. Anyway, jamovi freely distributed online can download jamovi homepage, : https://www.jamovi.org/top page, heading “Download”, ’ll see separate links Windows users, Mac users, Linux users. follow relevant link ’ll see online instructions pretty self-explanatory. writing, current version jamovi 0.9, usually issue updates every months, ’ll probably newer version.15","code":""},{"path":"getting-started-with-jamovi.html","id":"starting-up-jamovi","chapter":"3 Getting started with jamovi","heading":"3.1.1 Starting up jamovi","text":"One way another, regardless operating system ’re using, ’s time open jamovi get started. first starting jamovi presented user interface looks something like Figure 3.1.\nFigure 3.1: jamovi starts !\nleft spreadsheet view, right results statistical tests appear. middle bar separating two regions can dragged left right change sizes.possible simply begin typing values jamovi spreadsheet spreadsheet software. Alternatively, existing data sets CSV (.csv) file format can opened jamovi. Additionally, can easily import SPSS, SAS, Stata JASP files directly jamovi. open file select File tab (three horizontal lines signify tab) top left hand corner, select ‘Open’ choose files listed ‘Browse’ depending whether want open example file stored computer.","code":""},{"path":"getting-started-with-jamovi.html","id":"analyses","chapter":"3 Getting started with jamovi","heading":"3.2 Analyses","text":"Analyses can selected analysis ribbon menu along top. Selecting analysis present ‘options panel’ particular analysis, allowing assign different variables different parts analysis, select different options. time, results analysis appear right ‘Results panel’ update real-time make changes options.analysis set correctly can dismiss analysis options clicking arrow top right optional panel. wish return options, can click results produced. way, can return analysis (say, colleague) created earlier.decide longer need particular analysis, can remove results context menu. Right-clicking analysis results bring menu selecting ‘Analysis’ ‘Remove’ analysis can removed. later. First, let’s take detailed look spreadsheet view.","code":""},{"path":"getting-started-with-jamovi.html","id":"the-spreadsheet","chapter":"3 Getting started with jamovi","heading":"3.3 The spreadsheet","text":"jamovi data represented spreadsheet column representing ‘variable’ row representing ‘case’ ‘participant’.","code":""},{"path":"getting-started-with-jamovi.html","id":"variables","chapter":"3 Getting started with jamovi","heading":"3.3.1 Variables","text":"commonly used variables jamovi ‘Data Variables’, variables simply contain data either loaded data file, ‘typed ’ user. Data variables can one several measurement levels (Figure 3.2).\nFigure 3.2: measurement levels\nlevels designated symbol header variable’s column. ID variable type unique jamovi. ’s intended variables contain identifiers almost never want analyse. example, persons name, participant ID. Specifying ID variable type can improve performance interacting large data sets.Nominal variables categorical variables text labels, example column called Gender values Male Female nominal. person’s name. Nominal variable values can also numeric value. variables used often importing data codes values numbers rather text. example, column dataset may contain values 1 males, 2 females. possible add nice ‘human-readable’ labels values variable editor (later).Ordinal variables like Nominal variables, except values specific order. example Likert scale 3 ‘strongly agree’ -3 ‘strongly disagree’.Continuous variables variables exist continuous scale. Examples might height weight. also referred ‘Interval’ ‘Ratio scale’.addition, can also specify different data types: variables data type either ‘Text’, ‘Integer’ ‘Decimal’.starting blank spreadsheet typing values variable type change automatically depending data enter. good way get feel variable types go sorts data. Similarly, opening data file jamovi try guess variable type data column. cases automatic approach may correct, may necessary manually specify variable type variable editor.variable editor can opened selecting ‘Setup’ data tab double-clicking variable column header. variable editor allows change name variable , data variables, variable type, order levels, label displayed level. Changes can applied clicking ‘tick’ top right. variable editor can dismissed clicking ‘Hide’ arrow.New variables can inserted appended data set using ‘add’ button data ribbon. ‘add’ button also allows addition computed variables.","code":""},{"path":"getting-started-with-jamovi.html","id":"computed-variables","chapter":"3 Getting started with jamovi","heading":"3.3.2 Computed variables","text":"Computed Variables take value performing computation variables. Computed Variables can used range purposes, including log transforms, z-scores, sum-scores, negative scoring means.Computed variables can added data set ‘add’ button available data tab. produce formula box can specify formula. usual arithmetic operators available. examples formulas :+ B LOG10(len) MEAN(, B) (dose - VMEAN(dose)) / VSTDEV(dose)order, sum B, log (base 10) transform len, mean B, z-score variable dose. Figure 3.3 shows jamovi screen new variable computed z-score dose (‘Tooth Growth’ example data set).\nFigure 3.3: newly computed variable, z-score ‘dose’\n","code":""},{"path":"getting-started-with-jamovi.html","id":"v-functions","chapter":"3 Getting started with jamovi","heading":"3.3.2.1 V-functions","text":"Several functions already available jamovi available drop box labelled fx. number functions appear pairs, one prefixed V . V functions perform calculation variable whole, non-V functions perform calculation row row. example, MEAN(, B) produce mean B row. VMEAN() gives mean values .","code":""},{"path":"getting-started-with-jamovi.html","id":"copy-and-paste","chapter":"3 Getting started with jamovi","heading":"3.3.3 Copy and Paste","text":"jamovi produces nice American Psychological Association (APA) formatted tables attractive plots. often useful able copy paste , perhaps Word document, email colleague. copy results right click object interest menu select exactly want copy. menu allows choose copy image entire analysis. Selecting “copy” copies content clipboard can pasted programs usual way. can practice later analyses.","code":""},{"path":"getting-started-with-jamovi.html","id":"syntax-mode","chapter":"3 Getting started with jamovi","heading":"3.3.4 Syntax mode","text":"jamovi also provides “R Syntax Mode”. mode jamovi produces equivalent R code analysis. change syntax mode, select Application menu top right jamovi (button three vertical dots) click “Syntax mode” checkbox . can turn syntax mode clicking second time.syntax mode analyses continue operate now produce R syntax, ‘ascii output’ like R session. Like results objects jamovi, can right click items (including R syntax) copy paste , example R session. present, provided R syntax include data import step must performed manually R. many resources explaining import data R interested recommend take look ; just search interweb.","code":""},{"path":"getting-started-with-jamovi.html","id":"loading-data-in-jamovi","chapter":"3 Getting started with jamovi","heading":"3.4 Loading data in jamovi","text":"several different types files likely relevant us data analysis. two particular especially important perspective book:jamovi files .omv file extension. standard kind file jamovi uses store data, variables analyses.jamovi files .omv file extension. standard kind file jamovi uses store data, variables analyses.Comma separated value (csv) files .csv file extension. just regular old text files can opened many different software programs. ’s quite typical people store data csv files, precisely ’re simple.Comma separated value (csv) files .csv file extension. just regular old text files can opened many different software programs. ’s quite typical people store data csv files, precisely ’re simple.also several kinds data file might want import jamovi. instance, might want open Microsoft Excel spreadsheets (.xls files), data files saved native file formats statistics software, SPSS SAS. Whichever file formats using, ’s good idea create folder folders especially jamovi data sets analyses make sure keep backed regularly.","code":""},{"path":"getting-started-with-jamovi.html","id":"importing-data-from-csv-files","chapter":"3 Getting started with jamovi","heading":"3.4.1 Importing data from csv files","text":"One quite commonly used data format humble “comma separated value” file, also called csv file, usually bearing file extension .csv. csv files just plain old-fashioned text files store basically just table data. illustrated Figure 3.4, shows file called booksales.csv ’ve created. can see, row represents book sales data one month. first row doesn’t contain actual data though, names variables.\nFigure 3.4: booksales.csv data file. left opened file using spreadsheet program (OpenOffice), shows file basically table. right file open standard text editor (TextEdit program Mac), shows file formatted. entries table wrapped quote marks separated commas\n’s easy open csv files jamovi. top left menu (button three parallel lines) choose ‘Open’ browse stored csv file computer. ’re Mac, ’ll look like usual Finder window use choose file; Windows looks like Explorer window. example looks like Mac shown Figure 3.5. ’m assuming ’re familiar computer, problem finding csv file want import! Find one want, click “Open” button.\nFigure 3.5: dialog box Mac asking select csv file jamovi try import. Mac users recognise immediately, ’s usual way Mac asks find file. Windows users won’t see , instead ’ll see usual explorer window Windows always gives wants select file\nthings can check make sure data gets imported correctly:Heading. first row file contain names variable - ‘header’ row? booksales.csv file header, ’s yes.Decimal. character used specify decimal point? English speaking countries almost always period (.e., .). ’s universally true though, many European countries use comma.Quote. character used denote block text? ’s usually going double quote mark (“). booksales.csv file.","code":""},{"path":"getting-started-with-jamovi.html","id":"importing-unusual-data-files","chapter":"3 Getting started with jamovi","heading":"3.5 Importing unusual data files","text":"Throughout book ’ve assumed data stored jamovi .omv file “properly” formatted csv file. However, real life ’s terribly plausible assumption make ’d better talk possibilities might run .","code":""},{"path":"getting-started-with-jamovi.html","id":"loading-data-from-text-files","chapter":"3 Getting started with jamovi","heading":"3.5.1 Loading data from text files","text":"first thing point data saved text file aren’t quite proper csv format ’s still pretty good chance jamovi able open . just need try see works. Sometimes though need change formatting. ones ’ve often found needing change :header. lot time ’re storing data csv file first row actually contains column names data. ’s true ’s good idea open csv file spreadsheet programme Open Office add header row manually.sep. name “comma separated value” indicates, values row csv file usually separated commas. isn’t universal, however. Europe decimal point typically written , instead . consequence somewhat awkward use , separator. Therefore unusual use ; instead , separator. times, ’ve seen TAB character used.quote. ’s conventional csv files include quoting character textual data. can see looking booksales.csv file, usually double quote character, “. sometimes quoting character , might see single quote mark ’ used instead.skip. ’s actually common receive CSV files first rows nothing actual data. Instead, provide human readable summary data came , maybe include technical info doesn’t relate data.missing values. Often ’ll get given data missing values. one reason another, entries table missing. data file needs include “special” value indicate entry missing. default jamovi assumes value 9916, numeric text data, make sure , necessary, missing values csv file replaced 99 (-9999; whichever choose) opening / importing file jamovi. opened / imported file jamovi missing values converted blank greyed cells jamovi spreadsheet view. can also change missing value variable option Data - Setup view.","code":""},{"path":"getting-started-with-jamovi.html","id":"loading-data-from-spss-and-other-statistics-packages","chapter":"3 Getting started with jamovi","heading":"3.5.2 Loading data from SPSS (and other statistics packages)","text":"commands listed main ones ’ll need data files book. real life many possibilities. example, might want read data files statistics programs. Since SPSS probably widely used statistics package psychology, ’s worth mentioning jamovi can also import SPSS data files (file extension .sav). Just follow instructions open csv file, time navigate .sav file want import. SPSS files, jamovi regard values missing regarded “system missing” files SPSS. ‘Default missings’ value seem work expected importing SPSS files, aware - might need another step: import SPSS file jamovi, export csv file re-opening jamovi.17And ’s pretty much , least far SPSS goes. far statistical software goes, jamovi can also directly open / import SAS STATA files.","code":""},{"path":"getting-started-with-jamovi.html","id":"loading-excel-files","chapter":"3 Getting started with jamovi","heading":"3.5.3 Loading Excel files","text":"different problem posed Excel files. Despite years yelling people sending data encoded proprietary data format, get sent lot Excel files. way handle Excel files open first Excel another spreadsheet programme can handle Excel files, export data csv file opening / importing csv file jamovi.","code":""},{"path":"getting-started-with-jamovi.html","id":"changing-data-from-one-level-to-another","chapter":"3 Getting started with jamovi","heading":"3.6 Changing data from one level to another","text":"Sometimes want change variable level. can happen sorts reasons. Sometimes import data files, can come wrong format. Numbers sometimes get imported nominal, text values. Dates may get imported text. ParticipantID values can sometimes read continuous: nominal values can sometimes read ordinal even continuous. ’s good chance sometimes ’ll want convert variable one measurement level another one. , use correct term, want coerce variable one class another.Earlier saw specify different variable levels, want change variable’s measurement level can jamovi data view variable. Just click check box measurement level want - continuous, ordinal, nominal.","code":""},{"path":"getting-started-with-jamovi.html","id":"installing-add-on-modules-into-jamovi","chapter":"3 Getting started with jamovi","heading":"3.7 Installing add-on modules into jamovi","text":"really great feature jamovi ability install add-modules jamovi library. add-modules developed jamovi community, .e., jamovi users developers created special software add-ons , usually advanced, analyses go beyond capabilities base jamovi program.install add-modules, just click large \\(+\\) top right jamovi window, select “jamovi-library” browse various add-modules available. Choose one(s) want, install , Figure 3.6. ’s easy. newly installed modules can accessed “Analyses” button bar. Try …useful add-modules install include “scatr” (added “Descriptives”) \\(R_j\\).\nFigure 3.6: Installing add-modules jamovi\n","code":""},{"path":"getting-started-with-jamovi.html","id":"quitting-jamovi","chapter":"3 Getting started with jamovi","heading":"3.8 Quitting jamovi","text":"’s one last thing cover chapter: quit jamovi. ’s hard, just close program way program. However, might want quit save work! two parts : saving changes data set, saving analyses ran.good practice save changes data set new data set. way can always go back original data. save changes jamovi, select ‘Export’…‘Data’ main jamovi menu (button three horizontal bars top left) create new file name changed data set.Alternatively, can save changed data analyses undertaken saving jamovi file. , main jamovi menu select ‘Save ’ type file name ‘jamovi file (.omv)’. Remember save file location can find later. usually create new folder specific data sets analyses.","code":""},{"path":"getting-started-with-jamovi.html","id":"summary-1","chapter":"3 Getting started with jamovi","heading":"3.9 Summary","text":"Every book tries teach new statistical software program novices cover roughly topics, roughly order. exception, grand tradition just way everyone else , chapter covered following topics:Installing jamovi. downloaded installed jamovi, started .Analyses. briefly oriented part jamovi analyses done results appear, deferred later book.spreadsheet. spent time looking spreadsheet part jamovi, considered different variable types, compute new variables.Loading data jamovi. also saw load data files jamovi.Importing unusual data files. figured open data files, different file types.Changing data one level another. saw sometimes need coerce data one type another.Installing add-modules jamovi. Installing add-modules jamovi community really extends jamovi capabilities.Quitting jamovi. Finally, looked good practice terms saving data set analyses finished quit jamovi.still haven’t arrived anything resembles data analysis. Maybe next Chapter get us bit closer!","code":""},{"path":"descriptive-statistics.html","id":"descriptive-statistics","chapter":"4 Descriptive statistics","heading":"4 Descriptive statistics","text":"time get new data set look one first tasks find ways summarising data compact, easily understood fashion. descriptive statistics (opposed inferential statistics) . fact, many people term “statistics” synonymous descriptive statistics. topic ’ll consider chapter, going details, let’s take moment get sense need descriptive statistics. , let’s open aflsmall_margins file see variables stored file, see Figure 4.1.\nFigure 4.1: screenshot jamovi showing variables stored aflsmallmargins.csv file\nfact, just one variable , afl.margins. ’ll focus bit variable chapter, ’d better tell . Unlike data sets book, actually real data, relating Australian Football League (AFL).18 afl.margins variable contains winning margin (number points) 176 home away games played 2010 season.output doesn’t make easy get sense data actually saying. Just “looking data” isn’t terribly effective way understanding data. order get idea data actually saying need calculate descriptive statistics (chapter) draw nice pictures (Chapter 5). Since descriptive statistics easier two topics ’ll start , nevertheless ’ll show histogram afl.margins data since help get sense data ’re trying describe actually look like, see Figure 4.2. ’ll talk lot draw histograms section Histograms next chapter. now, ’s enough look histogram note provides fairly interpretable representation afl.margins data.\nFigure 4.2: histogram AFL 2010 winning margin data (afl.margins variable). might expect, larger winning margin less frequently tend see \n","code":""},{"path":"descriptive-statistics.html","id":"measures-of-central-tendency","chapter":"4 Descriptive statistics","heading":"4.1 Measures of central tendency","text":"Drawing pictures data, Figure 4.2, excellent way convey “gist” data trying tell . ’s often extremely useful try condense data simple “summary” statistics. situations, first thing ’ll want calculate measure central tendency. , ’d like know something “average” “middle” data lies. three commonly used measures mean, median mode. ’ll explain turn, discuss useful.","code":""},{"path":"descriptive-statistics.html","id":"the-mean","chapter":"4 Descriptive statistics","heading":"4.1.1 The mean","text":"mean set observations just normal, old-fashioned average. Add values , divide total number values. first five AFL winning margins 56, 31, 56, 8 32, mean observations just:\\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\]course, definition mean isn’t news anyone. Averages (.e., means) used often everyday life pretty familiar stuff. However, since concept mean something everyone already understands, ’ll use excuse start introducing mathematical notation statisticians use describe calculation, talk calculations done jamovi.first piece notation introduce \\(N\\), ’ll use refer number observations ’re averaging (case \\(N = 5\\)). Next, need attach label observations . ’s traditional use X , use subscripts indicate observation ’re actually talking . , ’ll use \\(X_1\\) refer first observation, \\(X_2\\) refer second observation, way \\(X_N\\) last one. , say thing slightly abstract way, use \\(X_i\\) refer -th observation. Just make sure ’re clear notation, Table 4.1 lists 5 observations afl.margins variable, along mathematical symbol used refer actual value observation corresponds .Table 4.1:  Observations afl.margins variable[Additional technical detail 19]","code":""},{"path":"descriptive-statistics.html","id":"calculating-the-mean-in-jamovi","chapter":"4 Descriptive statistics","heading":"4.1.2 Calculating the mean in jamovi","text":"Okay, ’s maths. get magic computing box work us? number observations starts become large ’s much easier sorts calculations using computer. calculate mean using data can use jamovi. first step click ‘Exploration’ button click ‘Descriptives’. can highlight afl.margins variable click ‘right arrow’ move across ‘Variables box’. soon Table appears right hand side screen containing default ‘Descriptives’ information; see Figure 4.3.\nFigure 4.3: Default descriptives AFL 2010 winning margin data (afl.margins variable)\ncan see Figure 4.3, mean value afl.margins variable 35.30. information presented includes total number observations (N=176), number missing values (none), Median, Minimum Maximum values variable.","code":""},{"path":"descriptive-statistics.html","id":"the-median","chapter":"4 Descriptive statistics","heading":"4.1.3 The median","text":"second measure central tendency people use lot median, ’s even easier describe mean. median set observations just middle value. let’s imagine interested first 5 AFL winning margins: \\(56\\), \\(31\\), \\(56\\), \\(8\\) \\(32\\). figure median sort numbers ascending order:8, 31, 32, 56, 56From inspection, ’s obvious median value 5 observations 32 since ’s middle one sorted list (’ve put bold make even obvious). Easy stuff. interested first 6 games rather first 5? Since sixth game season winning margin 14 points, sorted list now8, 31, 32, 56, 56and two middle numbers, 31 32. median defined average two numbers, course 31.5. , ’s tedious hand ’ve got lots numbers. real life, course, -one actually calculates median sorting data looking middle value. real life use computer heavy lifting us, jamovi provided us Median value 30.50 afl.margins variable (Figure 4.3).","code":""},{"path":"descriptive-statistics.html","id":"mean-or-median-whats-the-difference","chapter":"4 Descriptive statistics","heading":"4.1.4 Mean or median? What’s the difference?","text":"Knowing calculate means medians part story. also need understand one saying data, implies use one. illustrated Figure 4.4. mean kind like “centre gravity” data set, whereas median “middle value” data. implies, far one use, depends little type data ’ve got ’re trying achieve. rough guide:data nominal scale probably shouldn’t using either mean median. mean median rely idea numbers assigned values meaningful. numbering scheme arbitrary ’s probably best use Mode instead.data ordinal scale ’re likely want use median mean. median makes use order information data (.e., numbers bigger) doesn’t depend precise numbers involved. ’s exactly situation applies data ordinal scale. mean, hand, makes use precise numeric values assigned observations, ’s really appropriate ordinal data.interval ratio scale data either one generally acceptable. one pick depends bit ’re trying achieve. mean advantage uses information data (useful don’t lot data). ’s sensitive extreme, outlying values.Let’s expand last part little. One consequence systematic differences mean median histogram asymmetric (Skew kurtosis). illustrated Figure 4.4. Notice median (right hand side) located closer “body” histogram, whereas mean (left hand side) gets dragged towards “tail” (extreme values ). give concrete example, suppose Bob (income $50,000), Kate (income $60,000) Jane (income $65,000) sitting table. average income table $58,333 median income $60,000. Bill sits (income $100,000,000). average income now jumped $25,043,750 median rises $62,500. ’re interested looking overall income table mean might right answer. ’re interested counts typical income table median better choice .\nFigure 4.4: illustration difference mean median interpreted. mean basically ‘centre gravity’ data set. imagine histogram data solid object, point balance (see-saw) mean. contrast, median middle observation, half observations smaller half observations larger\n","code":""},{"path":"descriptive-statistics.html","id":"a-real-life-example","chapter":"4 Descriptive statistics","heading":"4.1.5 A real life example","text":"try get sense need pay attention differences mean median let’s consider real life example. Since tend mock journalists poor scientific statistical knowledge, give credit credit due. excellent article ABC news website20 24 September, 2010:Senior Commonwealth Bank executives travelled world past couple weeks presentation showing Australian house prices, key price income ratios, compare favourably similar countries. “Housing affordability actually going sideways last five six years,” said Craig James, chief economist bank’s trading arm, CommSec.probably comes huge surprise anyone mortgage, wants mortgage, pays rent, isn’t completely oblivious ’s going Australian housing market last several years. Back article:CBA waged war believes housing doomsayers graphs, numbers international comparisons. presentation, bank rejects arguments Australia’s housing relatively expensive compared incomes. says Australia’s house price household income ratio 5.6 major cities, 4.3 nationwide, comparable many developed nations. says San Francisco New York ratios 7, Auckland’s 6.7, Vancouver comes 9.3.excellent news! Except, article goes make observation :Many analysts say led bank use misleading figures comparisons. go page four CBA’s presentation read source information bottom graph table, notice additional source international comparison – Demographia. However, Commonwealth Bank also used Demographia’s analysis Australia’s house price income ratio, come figure closer 9 rather 5.6 4.3That’s, um, rather serious discrepancy. One group people say 9, another says 4-5. just split difference say truth lies somewhere ? Absolutely ! situation right answer wrong answer. Demographia correct, Commonwealth Bank wrong. article points :[] obvious problem Commonwealth Bank’s domestic price income figures compare average incomes median house prices (unlike Demographia figures compare median incomes median prices). median mid-point, effectively cutting highs lows, means average generally higher comes incomes asset prices, includes earnings Australia’s wealthiest people. put another way: Commonwealth Bank’s figures count Ralph Norris’ multi-million dollar pay packet income side, (doubt) expensive house property price figures, thus understating house price income ratio middle-income Australians.Couldn’t put better . way Demographia calculated ratio right thing . way Bank incorrect. extremely quantitatively sophisticated organisation major bank made elementary mistake, well… can’t say sure since special insight thinking. article happen mention following facts, may may relevant:[] Australia’s largest home lender, Commonwealth Bank one biggest vested interests house prices rising. effectively owns massive swathe Australian housing security home loans well many small business loans., .","code":""},{"path":"descriptive-statistics.html","id":"mode","chapter":"4 Descriptive statistics","heading":"4.1.6 Mode","text":"mode sample simple. value occurs frequently. can illustrate mode using different AFL variable: played finals? Open aflsmall finalists file take look afl.finalists variable, see Figure 4.5. variable contains names 400 teams played 200 finals matches played period 1987 2010.read 400 entries count number occasions team name appears list finalists, thereby producing frequency table. However, mindless boring: exactly sort task computers great . let’s use jamovi us. ‘Exploration’ - ‘Descriptives’ click small check box labelled ‘Frequency tables’ get something like Figure 4.6.Now frequency table can just look see , 24 years data, Geelong played finals team. Thus, mode afl.finalists data “Geelong”. can see Geelong (39 finals) played finals team 1987-2010 period. ’s also worth noting ‘Descriptives’ Table results calculated Mean, Median, Minimum Maximum. afl.finalists variable nominal text variable makes sense calculate values.\nFigure 4.5: screenshot jamovi showing variables stored aflsmall finalists.csv file\n\nFigure 4.6: screenshot jamovi showing frequency table afl.finalists variable\nOne last point make regarding mode. Whilst mode often calculated nominal data, means medians useless sorts variables, situations really want know mode ordinal, interval ratio scale variable. instance, let’s go back afl.margins variable. variable clearly ratio scale (’s clear , may help re-read section Scales measurement), situations mean median measure central tendency want. consider scenario: friend offering bet pick football game random. Without knowing playing guess exact winning margin. guess correctly win $50. don’t lose $1. consolation prizes “almost” getting right answer. guess exactly right margin. bet, mean median completely useless . mode bet . calculate mode afl.margins variable jamovi, go back data set ‘Exploration’ - ‘Descriptives’ screen see can expand section marked ‘Statistics’. Click checkbox marked ‘Mode’ see modal value presented ‘Descriptives’ Table, Figure 4.7. 2010 data suggest bet 3 point margin.\nFigure 4.7: screenshot jamovi showing modal value afl.margins variable\n","code":""},{"path":"descriptive-statistics.html","id":"measures-of-variability","chapter":"4 Descriptive statistics","heading":"4.2 Measures of variability","text":"statistics ’ve discussed far relate central tendency. , talk values “middle” “popular” data. However, central tendency type summary statistic want calculate. second thing really want measure variability data. , “spread ” data? “far” away mean median observed values tend ? now, let’s assume data interval ratio scale, ’ll continue use afl.margins data. ’ll use data discuss several different measures spread, different strengths weaknesses.","code":""},{"path":"descriptive-statistics.html","id":"range","chapter":"4 Descriptive statistics","heading":"4.2.1 Range","text":"statistics ’ve discussed far relate central tendency. , talk values “middle” “popular” data. However, central tendency type summary statistic want calculate. second thing really want measure variability data. , “spread ” data? “far” away mean median observed values tend ? now, let’s assume data interval ratio scale, ’ll continue use afl.margins data. ’ll use data discuss several different measures spread, different strengths weaknesses.range variable simple. ’s biggest value minus smallest value. AFL winning margins data maximum value 116 minimum value 0. Although range simplest way quantify notion “variability”, ’s one worst. Recall discussion mean want summary measure robust. data set one two extremely bad values ’d like statistics unduly influenced cases. example, variable containing extreme outliers-100, 2, 3, 4, 5, 6, 7, 8, 9, 10it clear range robust. variable range 110 outlier removed range 8.","code":""},{"path":"descriptive-statistics.html","id":"interquartile-range","chapter":"4 Descriptive statistics","heading":"4.2.2 Interquartile range","text":"interquartile range (IQR) like range, instead difference biggest smallest value difference 25th percentile 75th percentile taken. don’t already know percentile , 10th percentile data set smallest number x 10% data less x. fact, ’ve already come across idea. median data set 50th percentile! jamovi can easily specify 25th, 50th 75th percentiles clicking checkbox ‘Quartiles’ ‘Exploration’ - ‘Descriptives’ - ‘Statistics’ screen.surprisingly, Figure 4.8 50th percentile median value. , noting \\(50.50 - 12.75 = 37.75\\), can see interquartile range 2010 AFL winning margins data 37.75. ’s obvious interpret range ’s little less obvious interpret IQR. simplest way think like : interquartile range range spanned “middle half” data. , one quarter data falls 25th percentile one quarter data 75th percentile, leaving “middle half” data lying two. IQR range covered middle half.\nFigure 4.8: screenshot jamovi showing Quartiles afl.margins variable\n","code":""},{"path":"descriptive-statistics.html","id":"mean-absolute-deviation","chapter":"4 Descriptive statistics","heading":"4.2.3 Mean absolute deviation","text":"two measures ’ve looked far, range interquartile range, rely idea can measure spread data looking percentiles data. However, isn’t way think problem. different approach select meaningful reference point (usually mean median) report “typical” deviations reference point. mean “typical” deviation? Usually, mean median value deviations. practice, leads two different measures: “mean absolute deviation” (mean) “median absolute deviation” (median). ’ve read, measure based median seems used statistics seem better two. honest don’t think ’ve seen used much psychology. measure based mean occasionally show psychology though. section ’ll talk first one, ’ll come back talk second one later.Since previous paragraph might sound little abstract, let’s go mean absolute deviation mean little slowly. One useful thing measure name actually tells exactly calculate . Let’s think AFL winning margins data, ’ll start pretending 5 games total, winning margins 56, 31, 56, 8 32. Since calculations rely examination deviation reference point (case mean), first thing need calculate mean, \\(\\bar{X}\\). five observations, mean \\(\\bar{X} = 36.6\\). next step convert observations \\(X_i\\) deviation score. calculating difference observation \\(X_i\\) mean \\(\\bar{X}\\). , deviation score defined \\(X_i - \\bar{X}\\). first observation sample, equal \\(56 - 36.6 = 19.4\\). Okay, ’s simple enough. next step process convert deviations absolute deviations, converting negative values positive ones. Mathematically, denote absolute value \\(-3\\) \\(\\mid -3 \\mid\\), say \\(\\mid -3 \\mid = 3\\). use absolute value don’t really care whether value higher mean lower mean, ’re just interested close mean. help make process obvious possible, Table 4.2 shows calculations five observations.Table 4.2:  Measures variabilityNow calculated absolute deviation score every observation data set, calculate mean scores. Let’s :\\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\]’re done. mean absolute deviation five scores 15.52.[Additional technical detail 21]","code":""},{"path":"descriptive-statistics.html","id":"variance","chapter":"4 Descriptive statistics","heading":"4.2.4 Variance","text":"Although average absolute deviation measure uses, ’s best measure variability use. purely mathematical perspective solid reasons prefer squared deviations rather absolute deviations. obtain measure called variance, lot really nice statistical properties ’m going ignore,22 one massive psychological flaw ’m going make big deal moment. variance data set \\(X\\) sometimes written Var( \\(X\\) ), ’s commonly denoted \\(s^2\\) (reason become clearer shortly). [Additional technical detail 23]Now ’ve got basic idea, let’s look concrete example. , let’s use first five AFL games data. follow approach took last time, end information shown Table 4.3.Table 4.3:  Measures  variability first five AFL gamesThat last column contains squared deviations, average . hand, .e. using calculator, end variance \\(324.64\\). Exciting, isn’t ? moment, let’s ignore burning question ’re probably thinking (.e., heck variance \\(324.64\\) actually mean?) instead talk bit calculations jamovi, reveal something weird. Start new jamovi session clicking main menu button (three horizontal lines top left corner selecting ‘New’. Now type first five values afl.margins data set column (\\(56\\), \\(31\\), \\(56\\), \\(8\\), \\(32\\). Change variable type ‘Continuous’ ‘Descriptives’ click ‘Variance’ check box, get values variance one calculated hand (\\(324.64\\)). , wait, get completely different answer (\\(405.80\\)) - see Figure 4.9. ’s just weird. jamovi broken? typo? idiot?\nFigure 4.9: screenshot jamovi showing Variance first 5 values afl.margins variable\nhappens, answer .24 ’s typo, jamovi making mistake. fact, ’s simple explain jamovi , slightly trickier explain jamovi . let’s start “”. jamovi evaluating slightly different formula one showed . Instead averaging squared deviations, requires divide number data points N, jamovi chosen divide \\(N - 1\\).[Additional technical detail 25]’s . real question jamovi dividing \\(N - 1\\) \\(N\\). , variance supposed mean squared deviation, right? shouldn’t dividing N, actual number observations sample? Well, yes, . However, ’ll discuss chapter [Estimating unknown quantities sample], ’s subtle distinction “describing sample” “making guesses population sample came”. point, ’s distinction without difference. Regardless whether ’re describing sample drawing inferences population, mean calculated exactly way. variance, standard deviation, many measures besides. outlined initially (.e., take actual average, thus divide \\(N\\)) assumes literally intend calculate variance sample. time, however, ’re terribly interested sample . Rather, sample exists tell something world. , ’re actually starting move away calculating “sample statistic” towards idea estimating “population parameter”. However, ’m getting ahead . now, let’s just take faith jamovi knows ’s , ’ll revisit question later talk estimation chapter [Estimating unknown quantities sample].Okay, one last thing. section far read bit like mystery novel. ’ve shown calculate variance, described weird “\\(N - 1\\)” thing jamovi hinted reason ’s , haven’t mentioned single important thing. interpret variance? Descriptive statistics supposed describe things, , right now variance really just gibberish number. Unfortunately, reason haven’t given human-friendly interpretation variance really isn’t one. serious problem variance. Although elegant mathematical properties suggest really fundamental quantity expressing variation, ’s completely useless want communicate actual human. Variances completely uninterpretable terms original variable! numbers squared don’t mean anything anymore. huge issue. instance, according Table 4.3, margin game 1 “376.36 points-squared higher average margin”. exactly stupid sounds, calculate variance \\(324.64\\) ’re situation. ’ve watched lot footy games, time anyone ever referred “points squared”. ’s real unit measurement, since variance expressed terms gibberish unit, totally meaningless human.","code":""},{"path":"descriptive-statistics.html","id":"standard-deviation","chapter":"4 Descriptive statistics","heading":"4.2.5 Standard deviation","text":"Okay, suppose like idea using variance nice mathematical properties haven’t talked , since ’re human robot ’d like measure expressed units data (.e., points, points squared). ? solution problem obvious! Take square root variance, known standard deviation, also called “root mean squared deviation”, RMSD. solves problem fairly neatly. Whilst nobody clue “variance 324.68 points-squared” really means, ’s much easier understand “standard deviation 18.01 points” since ’s expressed original units. traditional refer standard deviation sample data s, though “sd” “std dev.” also used times.[Additional technical detail 26]However, might guessed discussion variance, jamovi actually calculates slightly different formula given . Just like saw variance, jamovi calculates version divides \\(N - 1\\) rather \\(N\\).[Additional technical detail 27]Interpreting standard deviations slightly complex. standard deviation derived variance, variance quantity little meaning makes sense us humans, standard deviation doesn’t simple interpretation. consequence, us just rely simple rule thumb. general, expect 68% data fall within 1 standard deviation mean, 95% data fall within 2 standard deviation mean, 99.7% data fall within 3 standard deviations mean. rule tends work pretty well time, ’s exact. ’s actually calculated based assumption histogram symmetric “bell shaped”.[^04.5] can tell looking AFL winning margins histogram Figure 4.2, isn’t exactly true data! Even , rule approximately correct. turns , 65.3% AFL margins data fall within one standard deviation mean. shown visually 4.10.","code":""},{"path":"descriptive-statistics.html","id":"which-measure-to-use","chapter":"4 Descriptive statistics","heading":"4.2.6 Which measure to use?","text":"’ve discussed quite measures spread: range, IQR, mean absolute deviation, variance standard deviation; hinted strengths weaknesses. ’s quick summary:Range. Gives full spread data. ’s vulnerable outliers consequence isn’t often used unless good reasons care extremes data.Interquartile range. Tells “middle half” data sits. ’s pretty robust complements median nicely. used lot.Mean absolute deviation. Tells far “average” observations mean. ’s interpretable minor issues (discussed ) make less attractive statisticians standard deviation. Used sometimes, often.Variance. Tells average squared deviation mean. ’s mathematically elegant probably “right” way describe variation around mean, ’s completely uninterpretable doesn’t use units data. Almost never used except mathematical tool, ’s buried “hood” large number statistical tools.Standard deviation. square root variance. ’s fairly elegant mathematically ’s expressed units data can interpreted pretty well. situations mean measure central tendency, default. far popular measure variation.short, IQR standard deviation easily two common measures used report variability data. situations others used. ’ve described book ’s fair chance ’ll run somewhere.\nFigure 4.10: illustration standard deviation AFL winning margins data. shaded bars histogram show much data fall within one standard deviation mean. case, 65.3% data set lies within range, pretty consistent ‘approximately 68% rule’ discussed main text\n","code":""},{"path":"descriptive-statistics.html","id":"skew-and-kurtosis","chapter":"4 Descriptive statistics","heading":"4.3 Skew and kurtosis","text":"two descriptive statistics sometimes see reported psychological literature: skew kurtosis. practice, neither one used anywhere near frequently measures central tendency variability ’ve talking . Skew pretty important, see mentioned fair bit, ’ve actually never seen kurtosis reported scientific article date.\nFigure 4.11: illustration skewness. left negatively skewed data set (\\(skewness = -.93\\)), middle data set skew (well, hardly : \\(skewness = -.006\\)), right positively skewed data set (\\(skewness = .93\\))\nSince ’s interesting two, let’s start talking skewness. Skewness basically measure asymmetry easiest way explain drawing pictures. Figure 4.11 illustrates, data tend lot extreme small values (.e., lower tail “longer” upper tail) many extremely large values (left panel) say data negatively skewed. hand, extremely large values extremely small ones (right panel) say data positively skewed. ’s qualitative idea behind skewness. relatively values far greater mean, distribution positively skewed right skewed, tail stretching right. Negative left skew opposite. symmetric distribution skewness 0. skewness value positively skewed distribution positive, negative value negatively skewed distribution.[Additional technical detail 28]Perhaps helpfully, can use jamovi calculate skewness: ’s check box ‘Statistics’ options ‘Exploration’ - ‘Descriptives’. afl.margins variable, skewness figure \\(0.780\\). divide skewness estimate Std. error skewness indication skewed data . Especially small samples (N \\(<\\) 50), one rule thumb suggests value 2 less can mean data skewed, value 2 sufficient skew data possibly limit use statistical analyses. Though clear agreement interpretation. said, indicate AFL winning margins data somewhat skewed (\\(\\frac{0.780}{0.183} = 4.262\\)).final measure sometimes referred , though rarely practice, kurtosis data set. Put simply, kurtosis measure thin fat tails distribution , illustrated Figure @ref(fig:fig4.12). convention, say “normal curve” (black lines) zero kurtosis, degree kurtosis assessed relative curve.\nFigure 4.12: illustration kurtosis. left, ‘platykurtic’ distribution (kurtosis = -.95) meaning distribution ‘thin’ flat tails. middle ‘mesokurtic’ distribution (kurtosis almost exactly 0) means tails neither thin fat. Finally, right, ‘leptokurtic’ distribution (kurtosis = 2.12) indicating distribution ‘fat’ tails. Note kurtosis measured respect normal curve (black line)\nFigure, data left pretty flat distribution, thin tails, kurtosis negative call data platykurtic. data right distribution fat tails, kurtosis positive say data leptokurtic. data middle neither think fat tails, say mesokurtic kurtosis zero. summarised table :Table 4.4:  Thin fat tails illustrate kurtosis[Additional technical detail 29]point, jamovi check box kurtosis just check box skewness, gives value kurtosis \\(0.101\\) standard error \\(0.364\\). means AFL winning margins data small kurtosis, ok.","code":""},{"path":"descriptive-statistics.html","id":"descriptive-statistics-separately-for-each-group","chapter":"4 Descriptive statistics","heading":"4.4 Descriptive statistics separately for each group","text":"commonly case find needing look descriptive statistics broken grouping variable. pretty easy jamovi. instance, let’s say want look descriptive statistics clinical trial data, broken separately therapy type. new data set, one ’ve never seen . data stored clinicaltrial.csv file ’ll use lot later chapter Comparing several means (one-way ANOVA) (can find complete description data start chapter). Let’s load see ’ve got:Evidently three drugs: placebo, something called “anxifree” something called “joyzepam”, 6 people administered drug. 9 people treated using cognitive behavioural therapy (CBT) 9 people received psychological treatment. can see looking ‘Descriptives’ mood.gain variable people show mood gain (\\(mean = 0.88\\)), though without knowing scale ’s hard say much . Still, ’s bad. Overall feel learned something .can also go ahead look descriptive statistics, time separately type therapy. jamovi, check Std. deviation, Skewness Kurtosis ‘Statistics’ options. time, transfer therapy variable ‘Split ’ box, get something like Figure 4.14\nFigure 4.13: screenshot jamovi showing variables stored clinicaltrial.csv file\nmultiple grouping variables? Suppose want look average mood gain separately possible combinations drug therapy. possible adding another variable, drug, ‘Split ’ box. Easy peasy, though sometimes split much isn’t enough data breakdown combination make meaningful calculations. case jamovi tells stating something like NaN Inf. 30\nFigure 4.14: screenshot jamovi showing Descriptives split therapy type\n","code":""},{"path":"descriptive-statistics.html","id":"standard-scores","chapter":"4 Descriptive statistics","heading":"4.5 Standard scores","text":"Suppose friend putting together new questionnaire intended measure “grumpiness”. survey \\(50\\) questions can answer grumpy way . Across big sample (hypothetically, let’s imagine million people !) data fairly normally distributed, mean grumpiness score \\(17\\) \\(50\\) questions answered grumpy way, standard deviation \\(5\\). contrast, take questionnaire answer \\(35\\) \\(50\\) questions grumpy way. , grumpy ? One way think say grumpiness \\(\\frac{35}{50}\\), might say ’m 70% grumpy. ’s bit weird, think . friend phrased questions bit differently people might answered different way, overall distribution answers easily move depending precise way questions asked. , ’m 70% grumpy respect set survey questions. Even ’s good questionnaire isn’t informative statement.simpler way around describe grumpiness comparing people. Shockingly, friend’s sample \\(1,000,000\\) people, \\(159\\) people grumpy (’s unrealistic, frankly) suggesting ’m top 0.016% people grumpiness. makes much sense trying interpret raw data. idea, describe grumpiness terms overall distribution grumpiness humans, qualitative idea standardisation attempts get . One way exactly just describe everything terms percentiles. However, problem “’s lonely top”. Suppose friend collected sample \\(1000\\) people (still pretty big sample purposes testing new questionnaire, ’d like add), time gotten, let’s say, mean \\(16\\) \\(50\\) standard deviation \\(5\\). problem almost certainly single person sample grumpy .However, lost. different approach convert grumpiness score standard score, also referred z-score. standard score defined number standard deviations mean grumpiness score lies. phrase “pseudomaths” standard score calculated like :\\[\n\\text{standard score} = \\frac{\\text{raw score} - mean}{\\text{standard deviation}}\n\\][Additional technical detail 31], going back grumpiness data, can now transform Dani’s raw grumpiness standardised grumpiness score.\\[ z =\\frac{35 - 17}{5} = 3.6 \\] interpret value, recall rough heuristic provided section Standard deviation noted 99.7% values expected lie within 3 standard deviations mean. fact grumpiness corresponds z score 3.6 indicates ’m grumpy indeed. fact suggests ’m grumpier 99.98% people. Sounds right.addition allowing interpret raw score relation larger population (thereby allowing make sense variables lie arbitrary scales), standard scores serve second useful function. Standard scores can compared one another situations raw scores can’t. Suppose, instance, friend also another questionnaire measured extraversion using \\(24\\) item questionnaire. overall mean measure turns 13 standard deviation \\(4\\), scored \\(2\\). can imagine, doesn’t make lot sense try compare raw score \\(2\\) extraversion questionnaire raw score 35 grumpiness questionnaire. raw scores two variables “” fundamentally different things, like comparing apples oranges.standard scores? Well, little different. calculate standard scores get \\((z = \\frac{(35-17)}{5}=3.6)\\) grumpiness \\((z = \\frac{(2-13)}{4}=-2.75)\\) extraversion. two numbers can compared .32 ’m much less extraverted people (\\(z = -2.75\\)) much grumpier people (\\(z=3.6\\)). extent unusualness much extreme grumpiness, since \\(3.6\\) bigger number \\(2.75\\). standardised score statement observation falls relative population, possible compare standardised scores across completely different variables.","code":""},{"path":"descriptive-statistics.html","id":"summary-2","chapter":"4 Descriptive statistics","heading":"4.6 Summary","text":"Calculating basic descriptive statistics one first things analysing real data, descriptive statistics much simpler understand inferential statistics, like every statistics textbook ’ve started descriptives. chapter, talked following topics:Measures central tendency. Broadly speaking, central tendency measures tell data . ’s three measures typically reported literature: mean, median mode.Measures variability. contrast, measures variability tell “spread ” data . key measures : range, standard deviation, interquartile range.Skew kurtosis. also looked assymetry variable’s distribution (skew) thin fat tailed distributions (kurtosis).Descriptive statistics separately group. Since book focuses data analysis jamovi, spent bit time talking descriptive statistics computed different subgroups.Standard scores. z-score slightly unusual beast. ’s quite descriptive statistic, quite inference. Make sure understand section. ’ll come later.next Chapter ’ll move discussion draw pictures! Everyone loves pretty picture, right? , want end important point. traditional first course statistics spends small proportion class descriptive statistics, maybe one two lectures . vast majority lecturer’s time spent inferential statistics ’s hard stuff . makes sense, hides practical everyday importance choosing good descriptives. mind…","code":""},{"path":"drawing-graphs.html","id":"drawing-graphs","chapter":"5 Drawing graphs","heading":"5 Drawing graphs","text":"else show data.\n– Edward Tufte33Visualising data one important tasks facing data analyst. ’s important two distinct closely related reasons. Firstly, ’s matter drawing “presentation graphics”, displaying data clean, visually appealing fashion makes easier reader understand ’re trying tell . Equally important, perhaps even important, fact drawing graphs helps understand data. end, ’s important draw “exploratory graphics” help learn data go analysing . points might seem pretty obvious count number times ’ve seen people forget .give sense importance chapter, want start classic illustration just powerful good graph can . end, Figure 5.1 shows redrawing one famous data visualisations time. John Snow’s 1854 map cholera deaths. map elegant simplicity. background street map helps orient viewer. top see large number small dots, one representing location cholera case. larger symbols show location water pumps, labelled name. Even casual inspection graph makes clear source outbreak almost certainly Broad Street pump. Upon viewing graph Dr Snow arranged handle removed pump ended outbreak killed 500 people. power good data visualisation.goals chapter twofold. First, discuss several fairly standard graphs use lot analysing presenting data, second show create graphs jamovi. graphs tend pretty straightforward, one respect chapter pretty simple. people usually struggle learning produce graphs, especially learning produce good graphs. Fortunately, learning draw graphs jamovi reasonably simple long ’re picky graph looks like. mean say jamovi lot good default graphs, plots, time produce clean, high-quality graphic. However, occasions want something non-standard, need make highly specific changes figure, graphics functionality jamovi yet capable supporting advanced work detail editing.\nFigure 5.1: stylised redrawing John Snow’s original cholera map. small dot represents location cholera case large circle shows location well. plot makes clear, cholera outbreak centred closely Broad St pump\n","code":""},{"path":"drawing-graphs.html","id":"histograms","chapter":"5 Drawing graphs","heading":"5.1 Histograms","text":"Let’s begin humble histogram. Histograms one simplest useful ways visualising data. make sense interval ratio scale variable (e.g., afl.margins data Descriptive statistics chapter want get overall impression variable. probably know histograms work, since ’re widely used, sake completeness ’ll describe . divide possible values bins count number observations fall within bin. count referred frequency density bin displayed vertical bar. Ihe AFL winning margins data 33 games winning margin less 10 points fact represented height leftmost bar showed earlier Descriptive statistics chapter, Figure 4.2. earlier graphs used advanced plotting package R , now, beyond capability jamovi. jamovi gets us close, drawing histogram jamovi pretty straightforward. Open ‘plots’ options ‘Exploration’ - ‘Descriptives’ click ‘histogram’ check box, Figure 5.1. jamovi defaults labelling y-axis ‘density’ x-axis variable name. bins selected automatically, scale, count, information y-axis unlike previous Figure 4.2. matter much really interested impression shape distribution: normally distributed skew kurtosis? first impressions characteristics come drawing histogram.\nFigure 5.2: jamovi screen showing histogram check box\nOne additional feature jamovi provides ability plot ‘Density’ curve. can clicking ‘Density’ check box ‘Plots’ options (unchecking ‘Histogram’), gives us plot shown Figure 5.3. density plot visualises distribution data continuous interval time period. chart variation histogram uses kernel smoothing plot values, allowing smoother distributions smoothing noise. peaks density plot help display values concentrated interval. advantage density plots histograms better determining distribution shape ’re affected number bins used (bar used typical histogram). histogram comprising 4 bins wouldn’t produce distinguishable enough shape distribution 20-bin histogram . However, density plots, isn’t issue.\nFigure 5.3: density plot afl.margins variable plotted jamovi\nAlthough image need lot cleaning order make good presentation graphic (.e., one ’d include report), nevertheless pretty good job describing data. fact, big strength histogram density plot (properly used) show entire spread data, can get pretty good sense looks like. downside histograms aren’t compact. Unlike plots ’ll talk ’s hard cram 20-30 histograms single image without overwhelming viewer. course, data nominal scale histograms useless.","code":""},{"path":"drawing-graphs.html","id":"boxplots","chapter":"5 Drawing graphs","heading":"5.2 Boxplots","text":"Another alternative histograms boxplot, sometimes called “box whiskers” plot. Like histograms ’re suited interval ratio scale data. idea behind boxplot provide simple visual depiction median, interquartile range, range data. fairly compact way boxplots become popular statistical graphic, especially exploratory stage data analysis ’re trying understand data . Let’s look work, using afl.margins data example.\nFigure 5.4: box plot afl.margins variable plotted jamovi\neasiest way describe boxplot looks like just draw one. Click ‘Box plot’ check box get plot shown lower right Figure 5.4. jamovi drawn basic boxplot possible. look plot interpret : thick line middle box median; box spans range 25th percentile 75th percentile; “whiskers” go extreme data point doesn’t exceed certain bound. default, value 1.5 times interquartile range (IQR), calculated 25th percentile - (1.5*IQR) lower boundary, 75th percentile + (1.5*IQR) upper boundary. observation whose value falls outside range plotted circle dot instead covered whiskers, commonly referred outlier. AFL margins data two observations fall outside range, observations plotted dots (upper boundary 107, looking data column spreadsheet two observations values higher , 108 116, dots).","code":""},{"path":"drawing-graphs.html","id":"violin-plots","chapter":"5 Drawing graphs","heading":"5.2.1 Violin plots","text":"\nFigure 5.5: violin plot afl.margins variable plotted jamovi, also showing box plot data points\nvariation traditional box plot violin plot. Violin plots similar box plots except also show kernel probability density data different values. Typically, violin plots include marker median data box indicating interquartile range, standard box plots. jamovi can achieve sort functionality checking ‘Violin’ ‘Box plot’ check boxes. See Figure 5.5, also ‘Data’ check box turned show actual data points plot. tend make graph bit busy though, opinion. Clarity simplicity, practice might better just use simple box plot.","code":""},{"path":"drawing-graphs.html","id":"drawing-multiple-boxplots","chapter":"5 Drawing graphs","heading":"5.2.2 Drawing multiple boxplots","text":"One last thing. want draw multiple boxplots ? Suppose, instance, wanted separate boxplots showing AFL margins just 2010 every year 1987 2010. first thing ’ll find data. stored aflmarginbyyear.csv file. let’s load jamovi see . see pretty big data set. contains 4296 games variables ’re interested . want jamovi draw boxplots margin variable, plotted separately year. way move year variable across ‘Split ’ box, Figure 5.6.\nFigure 5.6: jamovi screen shot showing ‘Split ’ window\nresult shown Figure 5.7. version box plot, split year, gives sense ’s sometimes useful choose box plots instead histograms. ’s possible get good sense data look like year year without getting overwhelmed much detail. Now imagine happened ’d tried cram 24 histograms space: chance reader going learn anything useful.\nFigure 5.7: Multiple boxplots plotted jamovi, margin year variables aflsmall2 data set\n","code":""},{"path":"drawing-graphs.html","id":"using-box-plots-to-detect-outliers","chapter":"5 Drawing graphs","heading":"5.2.3 Using box plots to detect outliers","text":"boxplot automatically separates observations lie outside certain range, depicting dot jamovi, people often use informal method detecting outliers: observations “suspiciously” distant rest data. ’s example. Suppose ’d drawn boxplot AFL margins data came looking like Figure 5.8. ’s pretty clear something funny going two observations. Apparently, two games margin 300 points! doesn’t sound right . Now ’ve become suspicious ’s time look bit closely data. jamovi can quickly find observations suspicious can go back raw data see mistake data entry. need set filter observations values certain threshold included. example, threshold 300, filter create. First, click ‘Filters’ button top jamovi window, type ‘margin > 300’ filter field, Figure 5.9.\nFigure 5.8: boxplot showing two suspicious outliers!\nfilter creates new column spreadsheet view observations pass filter included. One neat way quickly identify observations tell jamovi produce ‘Frequency table’ (‘Exploration’ - ‘Descriptives’ window) ID variable (must nominal variable otherwise Frequency table produced). Figure 5.10 can see ID values observations margin 300 14 134. suspicious cases, observations, go back original data source find going .\nFigure 5.9: jamovi filter screen\n\nFigure 5.10: Frequency table ID showing ID numbers two suspicious outliers: 14 134\nUsually find someone just typed wrong number. Whilst might seem like silly example, stress kind thing actually happens lot. Real world data sets often riddled stupid errors, especially someone type something computer point. fact, ’s actually name phase data analysis practice can take huge chunk time: data cleaning. involves searching typing mistakes (“typos”), missing data sorts obnoxious errors raw data files.less extreme values, even flagged boxplot outliers, decision whether include outliers exclude analysis depends heavily think data look way want use data . really need exercise good judgement . outlier looks legitimate , keep . case, ’ll return topic section Model checking chapter Correlation linear regression.","code":""},{"path":"drawing-graphs.html","id":"bar-graphs","chapter":"5 Drawing graphs","heading":"5.3 Bar graphs","text":"Another form graph often want plot bar graph. Let’s use afl.finalists data set afl.finalists variable introduced section Mode. want draw bar graph displays number finals team played time spanned afl.finalists data set. lots teams, particularly interested just four: Brisbane, Carlton, Fremantle Richmond. first step set filter just four teams included bar graph. straightforward jamovi can using ‘Filters’ function used previously. Open ‘Filters’ screen type following:afl.finalists \\(==\\) ‘Brisbane’ afl.finalists \\(==\\) ‘Carlton’ afl.finalists \\(==\\) ‘Fremantle’ afl.finalists \\(==\\) ‘Richmond’ 34When done see, ‘Data’ view, jamovi filtered values apart specified. Next, open ‘Exploration’ - ‘Descriptives’ window click ‘Bar plot’ check box (remember move ‘afl.finalists’ variable across ‘Variables’ box jamovi knows variable use). get bar graph, something like shown Figure 5.11.\nFigure 5.11: Filtering include just four AFL teams, drawing bar plot jamovi\n","code":""},{"path":"drawing-graphs.html","id":"saving-image-files-using-jamovi","chapter":"5 Drawing graphs","heading":"5.4 Saving image files using jamovi","text":"Hold , might thinking. ’s good able draw pretty pictures jamovi can’t save send friends brag awesome data ? save picture? Simples. Just right click plot image export file, either ‘png’, ‘eps’, ‘svg’ ‘pdf’. formats produce nice images can send friends, include assignments papers.","code":""},{"path":"drawing-graphs.html","id":"summary-3","chapter":"5 Drawing graphs","heading":"5.5 Summary","text":"Perhaps ’m simple minded person, love pictures. Every time write new scientific paper one first things sit think pictures . head article really just sequence pictures linked together story. rest just window dressing. ’m really trying say human visual system powerful data analysis tool. Give right kind information supply human reader massive amount knowledge quickly. nothing saying “picture worth thousand words”. mind, think one important chapters book. topics covered :Common plots. Much chapter focused standard graphs statisticians like produce: Histograms, Boxplots Bar graphsSaving image files using jamovi. Importantly, also covered export pictures.One final thing point . Whilst jamovi produces really neat default graphics, editing plots currently possible. advanced graphics plotting capability packages available R much powerful. One popular graphics systems provided ggplot2 package (see https://ggplot2.tidyverse.org/), loosely based “grammar graphics” (Wilkinson et al. 2006). ’s novices. need pretty good grasp R can start using , even takes really get hang . ’re ready ’s worth taking time teach , ’s much powerful cleaner system.","code":""},{"path":"pragmatic-matters.html","id":"pragmatic-matters","chapter":"6 Pragmatic matters","heading":"6 Pragmatic matters","text":"garden life never seems confine plots philosophers laid convenience. Maybe tractors trick.\n– Roger Zelazny35This somewhat strange chapter, even standards. goal chapter talk bit honestly realities working data ’ll see anywhere else book. problem real world data sets messy. often data file start doesn’t variables stored right format analysis want . Sometimes might lot missing values data set. Sometimes want analyse subset data. Et cetera. words, ’s lot data manipulation need just get variables data set format need . purpose chapter provide basic introduction pragmatic topics. Although chapter motivated kinds practical issues arise manipulating real data, ’ll stick practice ’ve adopted book rely small, toy data sets illustrate underlying issue. chapter essentially collection techniques doesn’t tell single coherent story, may useful start list topics:Tabulating cross-tabulating dataLogical expressions jamoviTransforming recoding variableA mathematical functions operationsExtracting subset dataAs can see, list topics chapter covers pretty broad, ’s lot content . Even though one longest hardest chapters book, ’m really scratching surface several fairly different important topics. advice, usual, read chapter try follow much can. Don’t worry much can’t grasp , especially later sections. rest book lightly reliant chapter can get away just understanding basics. However, ’ll probably find later ’ll need flick back chapter order understand concepts refer .","code":""},{"path":"pragmatic-matters.html","id":"tabulating-and-cross-tabulating-data","chapter":"6 Pragmatic matters","heading":"6.1 Tabulating and cross-tabulating data","text":"common task analysing data construction frequency tables, crosstabulation one variable another. tasks can achieved jamovi ’ll show section.","code":""},{"path":"pragmatic-matters.html","id":"creating-tables-for-single-variables","chapter":"6 Pragmatic matters","heading":"6.1.1 Creating tables for single variables","text":"Let’s start simple example. father small child naturally spend lot time watching TV shows like Night Garden. nightgarden.csv file, ’ve transcribed short section dialogue. file contains two variables interest, speaker utterance. Open data set jamovi take look data ‘spreadsheet’ view. see data looks something like :‘speaker’ variable: upsy-daisy upsy-daisy upsy-daisy upsy-daisy tombliboo tombliboo makka-pakka makka-pakka makka-pakka makka-pakka ‘utterance’ variable: pip pip onk onk ee oo pip pip onk onkLooking becomes clear happened sanity! data, one task might find needing construct frequency count number words character speaks show. jamovi ‘Descriptives’ screen check box called ‘Frequency tables’ just , see Table 6.1.Table 6.1:  Frequency table speaker variableThe output tells us first line ’re looking tabulation speaker variable. ‘Levels’ column lists different speakers exist data, ‘Counts’ column tells many times speaker appears data. words, ’s frequency table.jamovi, ‘Frequency tables’ check box produce table single variables. table two variables, example combining speaker utterance can see many times speaker said particular utterance, need cross-tabulation contingency table. jamovi can selecting ‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’ analysis, moving speaker variable ‘Rows’ box, utterances variable ‘Columns’ box. contingency table like one shown Figure 6.1.\nFigure 6.1: Contingency table speaker utterances variables\nDon’t worry “\\(\\chi^2\\) Tests” table produced. going cover later chapter Categorical data analysis. interpreting contingency table remember counts, fact first row second column numbers corresponds value 2 indicates Makka-Pakka (row 1) says “onk” (column 2) twice data set.","code":""},{"path":"pragmatic-matters.html","id":"adding-percentages-to-a-contingency-table","chapter":"6 Pragmatic matters","heading":"6.1.2 Adding percentages to a contingency table","text":"contingency table shown Figure 6.1 shows table raw frequencies. , count total number cases different combinations levels specified variables. However, often want data organised terms percentages well counts. can find check boxes different percentages ‘Cells’ option ‘Contingency Tables’ window. First, click ‘Row’ check box Contingency Table output window change one Figure 6.2.\nFigure 6.2: Contingency table speaker utterances variables, row percentages\n’re looking percentage utterances made character. words, 50% Makka-Pakka’s utterances “pip”, 50% “onk”. Let’s contrast table get calculate column percentages (uncheck ‘Row’ check ‘Column’ Cells options window), see Figure 6.3. version, ’re seeing percentage characters associated utterance. instance, whenever utterance “ee” made (data set), 100% time ’s Tombliboo saying .\nFigure 6.3: Contingency table speaker utterances variables, column percentages\n","code":""},{"path":"pragmatic-matters.html","id":"logical-expressions-in-jamovi","chapter":"6 Pragmatic matters","heading":"6.2 Logical expressions in jamovi","text":"key concept lot data transformations jamovi rely idea logical value. logical value assertion whether something true false. implemented jamovi pretty straightforward way. two logical values, namely TRUE FALSE. Despite simplicity, logical values useful things. Let’s see work.","code":""},{"path":"pragmatic-matters.html","id":"assessing-mathematical-truths","chapter":"6 Pragmatic matters","heading":"6.2.1 Assessing mathematical truths","text":"George Orwell’s classic book 1984 one slogans used totalitarian Party “two plus two equals five”. idea political domination human freedom becomes complete possible subvert even basic truths. ’s terrifying thought, especially protagonist Winston Smith finally breaks torture agrees proposition. “Man infinitely malleable”, book says. ’m pretty sure isn’t true humans36 ’s definitely true jamovi. jamovi infinitely malleable, rather firm opinions topic isn’t true, least regards basic mathematics. ask calculate \\(2 + 2\\)37, always gives answer, ’s bloody 5!course, far jamovi just calculations. haven’t asked explicitly assert \\(2 + 2 = 4\\) true statement. want jamovi make explicit judgement, can use command like : \\(2 + 2 == 4\\)’ve done use equality operator, \\(==\\), force jamovi make “true false” judgement.38 Okay, let’s see jamovi thinks Party slogan, type compute new variable ‘formula’ box:\\[2 + 2 == 5\\]get? whole set ‘false’ values spreadsheet column newly computed variable. Booyah! Freedom ponies ! something like . Anyway, worth look happens try force jamovi believe two plus two five making statement like \\(2 + 2 = 5\\). know another program, say R, throws error message. wait, jamovi get whole set ‘false’ values. going ? Well, seems jamovi pretty smart realises testing whether TRUE FALSE \\(2 + 2 = 5\\), regardless whether use correct equality operator, \\(==\\), equals sign “\\(=\\)”.Anyway, worth look happens try force jamovi believe two plus two five making statement like \\(2 + 2 = 5\\). know another program, say R, throws error message. wait, jamovi get whole set ‘false’ values. going ? Well, seems jamovi pretty smart realises testing whether TRUE FALSE \\(2 + 2 = 5\\), regardless whether use correct equality operator, \\(==\\), equals sign “\\(=\\)”.","code":""},{"path":"pragmatic-matters.html","id":"logical-operations","chapter":"6 Pragmatic matters","heading":"6.2.2 Logical operations","text":"now ’ve seen logical operations work. far ’ve seen simplest possible example. probably won’t surprised discover can combine logical operations operations functions complicated way, like : \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) \\(SQRT(25) == 5\\), Table 6.2 illustrates, several logical operators can use corresponding basic mathematical concepts. Hopefully pretty self-explanatory. example, less operator < checks see number left less number right. ’s less, jamovi returns answer TRUE, two numbers equal, one right larger, jamovi returns answer FALSE.contrast, less equal operator \\(<=\\) exactly says. returns value TRUE number left hand side less equal number right hand side. point hope ’s pretty obvious greater operator \\(<\\) greater equal operator \\(<=\\) !Next list logical operators equal operator != , others, says . returns value TRUE things either side identical . Therefore, since \\(2 + 2\\) isn’t equal \\(5\\), get ‘true’ value newly computed variable. Try see:\\[2 + 2 \\text{ != } 5\\]’re quite done yet. three logical operations worth knowing , listed Table 6.3. operator !, operator , operator . Like logical operators, behaviour less exactly ’d expect given names. instance, ask assess claim “either \\(2 + 2 = 4\\) \\(2 + 2 = 5\\)” ’d say ’s true. Since ’s “either-” statement, need one two parts true. ’s operator :39Table 6.2:  logical operatorsTable 6.3:  logical operators\\[(2+2 == 4) \\text{ } (2+2 == 5)\\]hand, ask assess claim “\\(2 + 2 = 4\\) \\(2 + 2 = 5\\)” ’d say ’s false. Since statement need parts true. ’s operator :\\[(2+2 == 4) \\text{ } (2+2 == 5)\\]Finally, ’s operator, simple annoying describe English. ask assess claim “true \\(2 + 2 = 5\\)” say claim true, actually claim “\\(2 + 2 = 5\\) false”. ’m right. write jamovi use :\\[(2+2 == 5)\\]words, since \\(2+2 == 5\\) FALSE statement, must case \\((2+2 == 5)\\) TRUE one. Essentially, ’ve really done claim “false” thing “true”. Obviously, isn’t really quite right real life. jamovi lives much black white world. jamovi everything either true false. shades grey allowed.course, \\(2 + 2 = 5\\) example, didn’t really need use “” operator \\(\\) “equals ” operator \\(==\\) two separate operators. just used “equals ” operator \\(!=\\) like :\\[2+2 \\text{ != } 5\\]","code":""},{"path":"pragmatic-matters.html","id":"applying-logical-operation-to-text","chapter":"6 Pragmatic matters","heading":"6.2.3 Applying logical operation to text","text":"also want briefly point can apply logical operators text well logical data. ’s just need bit careful understanding jamovi interprets different operations. section ’ll talk equal operator \\(==\\) applies text, since important one. Obviously, equal operator != gives exact opposite answers \\(==\\) ’m implicitly talking one , won’t give specific commands showing use \\(!=\\).Okay, let’s see works. one sense, ’s simple. instance, can ask jamovi word “cat” word “dog”, like :“cat” \\(==\\) “dog” ’s pretty obvious, ’s good know even jamovi can figure . Similarly, jamovi recognise “cat” “cat”: “cat” \\(==\\) “cat” , ’s exactly ’d expect. However, need keep mind jamovi tolerant comes grammar spacing. two strings differ way whatsoever, jamovi say ’re equal , following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c t”can also use logical operators . instance jamovi also allows use > > operators determine two text ‘strings’ comes first, alphabetically speaking. Sort . Actually, ’s bit complicated , let’s start simple example:“cat” \\(<\\) “dog”jamovi, example evaluates ‘true’. “cat” come “dog” alphabetically, jamovi judges statement true. However, ask jamovi tell us “cat” comes “anteater” evaluate expression false. far, good. text data bit complicated dictionary suggests. “cat” “CAT”? comes first? Try find :“CAT” \\(<\\) “cat”fact evaluates ‘true’. words, jamovi assumes uppercase letters come lowercase ones. Fair enough. -one likely surprised . might find surprising jamovi assumes uppercase letters come lowercase ones. , “anteater” \\(<\\) “zebra” true statement, uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” also true, true say “anteater” \\(<\\) “ZEBRA”, following extract illustrates. Try :“anteater” \\(<\\) “ZEBRA”evaluates ‘false’, may seem slightly counter-intuitive. mind, may help quick look Table 6.4 lists various text characters order jamovi processes .Table 6.4:  Text characters order jamovi processes ","code":""},{"path":"pragmatic-matters.html","id":"transforming-and-recoding-a-variable","chapter":"6 Pragmatic matters","heading":"6.3 Transforming and recoding a variable","text":"’s uncommon real world data analysis find one variables isn’t quite equivalent variable really want. instance, ’s often convenient take continuous-valued variable (e.g., age) break smallish number categories (e.g., younger, middle, older). times, may need convert numeric variable different numeric variable (e.g., may want analyse absolute value original variable). section ’ll describe key ways can things jamovi.","code":""},{"path":"pragmatic-matters.html","id":"creating-a-transformed-variable","chapter":"6 Pragmatic matters","heading":"6.3.1 Creating a transformed variable","text":"first trick discuss idea transforming variable. Taken literally, anything variable transformation, practice usually means apply relatively simple mathematical function original variable order create new variable either () provides better way describing thing ’re actually interested , (b) closely agreement assumptions statistical tests want . Since, stage, haven’t talked statistical tests assumptions, ’ll show example based first case.Suppose ’ve run short study ask 10 people single question:\nscale 1 (strongly disagree) 7 (strongly agree), extent agree proposition “Dinosaurs awesome”?Now let’s load look data. data file likert.omv contains single variable contains raw Likert-scale responses 10 people. However, think , isn’t best way represent responses. fairly symmetric way set response scale, ’s sense midpoint scale coded 0 (opinion), two endpoints `3 (strongly agree) ´3 (strongly disagree). recoding data way ’s bit reflective really think responses. recoding pretty straightforward, just subtract 4 raw scores. jamovi can computing new variable: click ‘Data’ - ‘Compute’ button see new variable added spreadsheet. Let’s call new variable likert.centred (go ahead type ) add following formula box, like Figure 6.4: ‘likert.raw - 4’\nFigure 6.4: Creating new computed variable jamovi\nOne reason might useful data format lot situations might prefer analyse strength opinion separately direction opinion. can two different transformations likert.centred variable order distinguish two different concepts. First, compute opinion.strength variable, want take absolute value centred data (using ‘ABS’ function).40 jamovi, create another new variable using ‘Compute’ button. Name variable opinion.strength time click fx button next ‘Formula’ box. shows different ‘Functions’ ‘Variables’ can add ‘Formula’ box, double click ‘ABS’ double click “likert.centred’ see ‘Formula’ box populated ABS(likert.centred) new variable created spreadsheet view, Figure 6.5.\nFigure 6.5: Using \\(f_x\\) button select functions variables\nSecond, compute variable contains direction opinion ignores strength, want calculate ‘sign’ variable. jamovi can use function . Create another new variable using ‘Compute’ button, name one opinion.sign, type following function box:(likert.centred \\(==\\) 0, 0, likert.centred / opinion.strength) done, ’ll see negative numbers likert.centred variable converted -1, positive numbers converted 1 zero stays 0, like :-1 1 -1 0 0 0 -1 1 1 1Let’s break ‘’ command . jamovi three parts ‘’ statement, written ‘(expression, value, else)’. first part, ‘expression’ can logical mathematical statement. example, specified ‘likert.centred \\(==\\) 0’, TRUE values likert.centred zero. next part, ‘value’, new value expression part one TRUE. example, said values likert.centred zero, keep zero. next part, ‘else’, can enter another logical mathematical statement used part one evaluates FALSE, .e. likert.centred zero. example divided likert.centred opinion.strength give ‘-1’ ‘+1’ depending sign original value likert.centred.41And ’re done. now three shiny new variables, useful transformations original likert.raw data.","code":""},{"path":"pragmatic-matters.html","id":"collapsing-a-variable-into-a-smaller-number-of-discrete-levels-or-categories","chapter":"6 Pragmatic matters","heading":"6.3.2 Collapsing a variable into a smaller number of discrete levels or categories","text":"One pragmatic task comes quite often problem collapsing variable smaller number discrete levels categories. instance, suppose ’m interested looking age distribution people social gathering:60,58,24,26,34,42,31,30,33,2,9In situations can quite helpful group smallish number categories. example, group data three broad categories: young (0-20), adult (21-40) older (41-60). quite coarse-grained classification, labels ’ve attached make sense context data set (e.g., viewed generally, 42 year old wouldn’t consider “older”). can slice variable quite easily using jamovi ‘’ function already used. time specify nested ‘’ statements, meaning simply first logical expression TRUE, insert first value, second logical expression TRUE, insert second value, third logical expression TRUE, insert third value. can written :(Age >= 0 Age <= 20, 1, (Age >= 21 Age <= 40, 2, (Age >= 41 Age <= 60, 3 )))Note three left parentheses used nesting, whole statement end three right parentheses otherwise get error message. jamovi screen shot data manipulation, along accompanying frequency table, shown Figure 6.6.\nFigure 6.6: Collapsing variable smaller number discrete levels using jamovi ‘’ function\n’s important take time figure whether resulting categories make sense terms research project. don’t make sense meaningful categories, data analysis uses categories likely just meaningless. generally, practice ’ve noticed people strong desire carve (continuous messy) data (discrete simple) categories, run analyses using categorised data instead original data.42 wouldn’t go far say inherently bad idea, fairly serious drawbacks times, advise caution thinking .","code":""},{"path":"pragmatic-matters.html","id":"creating-a-transformation-that-can-be-applied-to-multiple-variables","chapter":"6 Pragmatic matters","heading":"6.3.3 Creating a transformation that can be applied to multiple variables","text":"Sometimes want apply transformation one variable, example multiple questionnaire items need recalculated recoded way. one neat features jamovi can create transformation, using ‘Data’ - ‘Transform’ button, can saved applied multiple variables. Let’s go back first example , using data file likert.omv contains single variable raw Likert-scale responses 10 people. create transformation can save apply across multiple variables (assuming variables like data file), first spreadsheet editor select (.e., click) variable want use initially create transformation. example likert.raw. Next click ‘Transform’ button jamovi ‘Data’ ribbon, ’ll see something like Figure 6.7.Give new variable name, let’s call opinion.strength click ‘using transform’ selection box select ‘Create New Transform…’. create, name, transformation can re-applied many variables like. transformation automatically named us ‘Transform 1’ (imaginative, huh. can change like). type expression “ABS($source - 4)” function text box, Figure 6.8, press Enter Return keyboard , hey presto, created new transformation applied likert.raw variable! Good, eh. Note instead using variable label expression, instead used ‘$source’. can use transformation many different variables like - jamovi requires use ‘$source’ refer source variable transforming. transformation also saved can re-used time like (providing save dataset ‘.omv’ file, otherwise ’ll lose !).can also create transformation second example looked , age distribution people social gathering. Go , know want ! Remember collapsed variable three groups: younger, adult older. time achieve thing, using jamovi ‘Transform’ - ‘Add condition’ button. data set (go back create didn’t save ) set new variable transformation. Call transformed variable AgeCats transformation create Agegroupings. click big “\\(+\\)” sign next function box. ‘Add condition’ button ’ve stuck big red arrow onto Figure 6.9 can see exactly . Re-create transformation shown Figure 6.9 done, see new values appear spreadsheet window. ’s , Age groupings transformation saved can re-applied time like. Ok, know ’s unlikely one ‘Age’ variable, get idea now set transformations jamovi, can follow idea sorts variables. typical scenario questionnaire scale , say, 20 items (variables) item originally scored 1 6 , reason quirk data decide recode items 1 3. can easily jamovi creating re-applying transformation variable want recode.\nFigure 6.7: Creating new variable transformation using jamovi ‘Transform’ command\n\nFigure 6.8: Specifying transformation jamovi, saved imaginatively named ‘Transform 1’\n\nFigure 6.9: jamovi transformation three age categories, using ‘Add condition’ button\nTable 6.5:  mathematical operators","code":""},{"path":"pragmatic-matters.html","id":"a-few-more-mathematical-functions-and-operations","chapter":"6 Pragmatic matters","heading":"6.4 A few more mathematical functions and operations","text":"section Transforming recoding variable discussed ideas behind variable transformations showed lot transformations might want apply data based fairly simple mathematical functions operations. section want return discussion mention several mathematical functions arithmetic operations actually quite useful lot real world data analysis. Table 6.5 gives brief overview various mathematical functions want talk , later.43 Obviously doesn’t even come close cataloguing range possibilities available, cover range functions used regularly data analysis available jamovi.","code":""},{"path":"pragmatic-matters.html","id":"logarithms-and-exponentials","chapter":"6 Pragmatic matters","heading":"6.4.1 Logarithms and exponentials","text":"’ve mentioned earlier, jamovi useful range mathematical functions built really wouldn’t much point trying describe even list . part, ’ve focused functions strictly necessary book. However want make exception logarithms exponentials. Although aren’t needed anywhere else book, everywhere statistics broadly. , lot situations convenient analyse logarithm variable (.e., take “log-transform” variable). suspect many (maybe ) readers book encountered logarithms exponentials , past experience know ’s substantial proportion students take social science statistics class haven’t touched logarithms since high school, appreciate bit refresher.order understand logarithms exponentials, easiest thing actually calculate see relate simple calculations. three jamovi functions particular want talk , namely LN(), LOG10() EXP(). start , let’s consider LOG10(), known “logarithm base 10”. trick understanding logarithm understand ’s basically “opposite” taking power. Specifically, logarithm base 10 closely related powers 10. let’s start noting 10-cubed 1000. Mathematically, write :\\[10^3=1000\\]trick understanding logarithm recognise statement “10 power 3 equal 1000” equivalent statement “logarithm (base 10) 1000 equal 3”. Mathematically, write follows,\\[log_{10}(1000)=3\\]Okay, since LOG10() function related powers 10, might expect logarithms (bases 10) related powers . course ’s true: ’s really anything mathematically special number 10. happen find useful decimal numbers built around number 10, big bad world mathematics scoffs decimal numbers. Sadly, universe doesn’t actually care write numbers. Anyway, consequence cosmic indifference ’s nothing particularly special calculating logarithms base 10. , instance, calculate logarithms base 2. Alternatively, third type logarithm, one see lot statistics either base 10 base 2, called natural logarithm, corresponds logarithm base e. Since might one day run , ’d better explain e . number e, known Euler’s number, one annoying “irrational” numbers whose decimal expansion infinitely long, considered one important numbers mathematics. first digits e :\\[e = 2.718282 \\]quite situation statistics require us calculate powers \\(e\\), though none appear book. Raising e power \\(x\\) called exponential \\(x\\), ’s common see \\(e^x\\) written exppxq. ’s surprise jamovi function calculates exponentials, called EXP(). number e crops often statistics, natural logarithm (.e., logarithm base e) also tends turn . Mathematicians often write \\(log_e(x)\\) \\(ln(x)\\). fact, jamovi works way: LN() function corresponds natural logarithm., think ’ve quite enough exponentials logarithms book!","code":""},{"path":"pragmatic-matters.html","id":"extracting-a-subset-of-the-data","chapter":"6 Pragmatic matters","heading":"6.5 Extracting a subset of the data","text":"One important kind data handling able extract particular subset data. instance, might interested analysing data one experimental condition, may want look closely data people 50 years age. , first step getting jamovi filter subset data corresponding observations ’re interested .section returns nightgarden.csv data set. ’re reading whole chapter one sitting, already data set loaded jamovi window. section, let’s focus two variables speaker utterance (see Tabulating cross-tabulating data) ’ve forgotten variables look like). Suppose want pull utterances made Makka-Pakka. end, need specify filter jamovi. First open filter window clicking ‘Filters’ main jamovi ‘Data’ toolbar. , ‘Filter 1’ text box, next ‘=’ sign, type following:speaker == ‘makka-pakka’\nFigure 6.10: Creating subset nightgarden data using jamovi ‘Filters’ option\ndone , see new column added spreadsheet window (see Figure 6.10), labelled ‘Filter 1’, cases speaker ‘makka-pakka’ greyed-(.e., filtered ) , conversely, cases speaker ‘makka-pakka’ green check mark indicating filtered . can test running ‘Exploration’ - ‘Descriptives’ - ‘Frequency tables’ speaker variable seeing shows. Go , try !Following simple example, can also build complex filters using logical expressions jamovi. instance, suppose wanted keep cases utterance either “pip” “oo”. case ‘Filter 1’ text box, next ‘=’ sign, type following:utterance == ‘pip’ utterance == ‘oo’","code":""},{"path":"pragmatic-matters.html","id":"summary-4","chapter":"6 Pragmatic matters","heading":"6.6 Summary","text":"Obviously, ’s real coherence chapter. ’s just grab bag topics tricks can handy know , best wrap can give just repeat list:Tabulating cross-tabulating dataLogical expressions jamoviTransforming recoding variableA mathematical functions operationsExtracting subset data","code":""},{"path":"prelude.html","id":"prelude","chapter":"Prelude","heading":"Prelude","text":"Part IV book far theoretical, focusing \ntheory statistical inference. next three chapters \ngoal give Introduction probability theory, sampling estimation chapter [Estimating unknown quantities sample] statistical Hypothesis testing. get started though, want\nsay something big picture. Statistical inference \nprimarily learning data. goal longer merely \ndescribe data use data draw conclusions \nworld. motivate discussion want spend bit time talking\nphilosophical puzzle known riddle induction, \nspeaks issue pop throughout\nbook: statistical inference relies assumptions. sounds like\nbad thing. everyday life people say things like “never\nmake assumptions”, psychology classes often talk assumptions\nbiases bad things try avoid. bitter\npersonal experience learned never say things around\nphilosophers!","code":""},{"path":"prelude.html","id":"on-the-limits-of-logical-reasoning","chapter":"Prelude","heading":"On the limits of logical reasoning","text":"whole art war consists getting side\nhill, , words, learning know \n.\n- Arthur Wellesley, 1st Duke WellingtonI told quote came consequence carriage\nride across countryside.44 companion, J.\nW. Croker, playing guessing game, trying predict \nside hill. every case turned \nWellesley right Croker wrong. Many years later \nWellesley asked game explained “whole art \nwar consists getting side hill”.\nIndeed, war special respect. life guessing\ngame one form another, getting day day basis\nrequires us make good guesses. let’s play guessing game \n.Suppose observing Wellesley-Croker competition \nevery three hills predict win next\none, Wellesley Croker. Let’s say W refers Wellesley victory\nC refers Croker victory. three hills, data set looks\nlike :\\(WWW\\)conversation goes like :: Three row doesn’t mean much. suppose Wellesley might \nbetter Croker, might just luck. Still, ’m bit\ngambler. ’ll bet Wellesley.: agree three row isn’t informative see reason prefer Wellesley’s guesses \nCroker’s. can’t justify betting stage. Sorry. bet .gamble paid : three hills go Wellesley wins three. Going next round game score 1-0 favour data set looks like : \\(WWW\\) \\(WWW\\) ’ve organised data blocks three can see batch corresponds observations available step little side game. seeing new batch, conversation continues:: Six wins row Duke Wellesley. starting feel \nbit suspicious. ’m still certain, reckon ’s going \nwin next one .: guess don’t see . Sure, agree \nWellesley won six row, don’t see logical reason \nmeans ’ll win seventh one. bet. : really think\n? Fair enough, bet worked last time ’m okay \nchoice.second time right, second time wrong. Wellesley wins next three hills, extending winning record Croker 9-0. data set available us now : \\(WWW\\) \\(WWW\\) \\(WWW\\) conversation goes like :: Okay, pretty obvious. Wellesley way better game.\nagree ’s going win next hill, right?: \nreally logical evidence ? started game, \nlots possibilities first 10 outcomes, idea\none expect. \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\) one possibility, \\(WCC\\)\n\\(CWC\\) \\(WWC\\) \\(C\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) even \\(CCC\\) \\(CCC\\) \\(CCC\\) \\(C\\). idea\nhappen ’d said equally likely. \nassume , right? mean, ’s means say\n“idea”, isn’t ?: suppose .: Well , \nobservations ’ve made logically rule possibilities except two:\n\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). perfectly consistent\nevidence ’ve encountered far, aren’t ?: Yes, \ncourse . going ? : ’s changed\n? start game, ’d agreed \nequally plausible none evidence ’ve encountered \ndiscriminated two possibilities. Therefore, \npossibilities remain equally plausible see logical reason \nprefer one . yes, agree \nWellesley’s run 9 wins row remarkable, can’t think \ngood reason think ’ll win 10th hill. bet.: see point, ’m still willing chance . ’m betting Wellesley.Wellesley’s winning streak continues next three hills. score Wellesley-Croker game now 12-0, score game now 3-0. approach fourth round game, data set : \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) conversation continues:: Oh yeah! Three wins Wellesley another victory .\nAdmit , right ! guess ’re betting Wellesley\ntime around, right?: don’t know think. feel like\n’re situation last round, nothing much \nchanged. two legitimate possibilities sequence \n13 hills haven’t already ruled , \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) \\(WWW\\)\n\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). ’s just like said last time. possible outcomes\nequally sensible game started, shouldn’t two \nequally sensible now given observations don’t rule either\none? agree feels like Wellesley amazing winning\nstreak, ’s logical evidence streak continue?: think ’re unreasonable. take look \nscorecard, need evidence? ’re expert statistics \n’ve using fancy logical analysis, fact ’re\nlosing. ’m just relying common sense ’m winning. Maybe \nswitch strategies.: Hmm, good point don’t want\nlose game, ’m afraid don’t see logical evidence \nstrategy better mine. seems \nsomeone else watching game, ’d observed run \nthree wins . data look like : \\(YYY\\). Logically, \ndon’t see different first round watching\nWellesley Croker. Three wins doesn’t seem like lot \nevidence, see reason think strategy working \nbetter mine. didn’t think \\(WWW\\) good evidence \nWellesley better Croker game, surely \nreason now think YYY good evidence ’re better \n?: Okay, now think ’re jerk.: don’t see logical evidence .","code":""},{"path":"prelude.html","id":"learning-without-making-assumptions-is-a-myth","chapter":"Prelude","heading":"Learning without making assumptions is a myth","text":"lots different ways dissect \ndialogue, since statistics book pitched psychologists\nintroduction philosophy psychology reasoning,\n’ll keep brief. ’ve described sometimes referred \nriddle induction. seems entirely reasonable think \n12-0 winning record Wellesley pretty strong evidence \nwin 13th game, easy provide proper logical\njustification belief. contrary, despite obviousness\nanswer, ’s actually possible justify betting \nWellesley without relying assumption don’t \nlogical justification .riddle induction associated philosophical work\nDavid Hume recently Nelson Goodman, can find\nexamples problem popping fields diverse literature\n(Lewis Carroll) machine learning (“free lunch” theorem).\nreally something weird trying “learn \nknow know”. critical point assumptions \nbiases unavoidable want learn anything world.\nescape , just true statistical\ninference human reasoning. dialogue taking aim\nperfectly sensible inferences human , common\nsense reasoning relied different \nstatistician done. “common sense” half dialog\nrelied implicit assumption exists difference \nskill Wellesley Croker, trying\nwork difference skill level . “logical\nanalysis” rejects assumption entirely. willing accept\nsequences wins losses know\nsequences observed. Throughout dialogue kept\ninsisting logically possible data sets equally plausible\nstart Wellesely-Croker game, way \never revised beliefs eliminate possibilities \nfactually inconsistent observations.sounds perfectly sensible terms. fact, even sounds\nlike hallmark good deductive reasoning. Like Sherlock Holmes, \napproach rule impossible hope \nleft truth. Yet saw, ruling impossible\nnever led make prediction. terms everything said \nhalf dialogue entirely correct. inability make \npredictions logical consequence making “assumptions”. \nend lost game make assumptions \nassumptions turned right. Skill real thing, \nbelieved existence skill able learn \nWellesley Croker. relied less sensible\nassumption drive learning might won game.Ultimately two things take away . First,\n’ve said, avoid making assumptions want learn\nanything data. second, realise assumptions\nnecessary becomes important make sure make right ones!\ndata analysis relies assumptions necessarily better\none makes many assumptions, depends whether \nassumptions good ones data. go rest \nbook ’ll often point assumptions underpin \nparticular statistical technique, can check whether \nassumptions sensible.","code":""},{"path":"introduction-to-probability.html","id":"introduction-to-probability","chapter":"7 Introduction to probability","heading":"7 Introduction to probability","text":"[God] afforded us twilight … Probability.\n– John LockeUp point book ’ve discussed key ideas experimental design, ’ve talked little can summarise data set. lot people statistics: collecting numbers, calculating averages, drawing pictures, putting report somewhere. Kind like stamp collecting numbers. However, statistics covers much . fact, descriptive statistics one smallest parts statistics one least powerful. bigger useful part statistics provides information lets make inferences data.start thinking statistics terms, statistics help us draw inferences data, start seeing examples everywhere. instance, ’s tiny extract newspaper article Sydney Morning Herald (30 Oct 2010):“tough job,” Premier said response poll found government now unpopular Labor administration polling history, primary vote just 23 per cent.kind remark entirely unremarkable papers everyday life, let’s think entails. polling company conducted survey, usually pretty big one can afford . ’m lazy track original survey let’s just imagine called 1000 New South Wales (NSW) voters random, 230 (23%) claimed intended vote Australian Labor Party (ALP). 2010 Federal election Australian Electoral Commission reported 4,610,795 enrolled voters NSW, opinions remaining 4,609,795 voters (99.98% voters) remain unknown us. Even assuming -one lied polling company thing can say 100% confidence true ALP primary vote somewhere 230/4610795 (0.005%) 4610025/4610795 (99.83%). , basis legitimate polling company, newspaper, readership conclude ALP primary vote 23%?answer question pretty obvious. call 1000 people random, 230 say intend vote ALP, seems unlikely 230 people entire voting public actually intend vote ALP. words, assume data collected polling company pretty representative population large. representative? surprised discover true ALP primary vote actually 24%? 29%? 37%? point everyday intuition starts break bit. -one surprised 24%, everybody surprised 37%, ’s bit hard say whether 29% plausible. need powerful tools just looking numbers guessing.Inferential statistics provides tools need answer sorts questions, since kinds questions lie heart scientific enterprise, take lions share every introductory course statistics research methods. However, theory statistical inference built top probability theory. probability theory must now turn. discussion probability theory basically background detail. ’s lot statistics per se chapter, don’t need understand material much depth chapters part book. Nevertheless, probability theory underpin much statistics, ’s worth covering basics.","code":""},{"path":"introduction-to-probability.html","id":"how-are-probability-and-statistics-different","chapter":"7 Introduction to probability","heading":"7.1 How are probability and statistics different?","text":"start talking probability theory, ’s helpful spend moment thinking relationship probability statistics. two disciplines closely related ’re identical. Probability theory “doctrine chances”. ’s branch mathematics tells often different kinds events happen. example, questions things can answer using probability theory:chances fair coin coming heads 10 times row?roll six sided dice twice, likely ’ll roll two sixes?likely five cards drawn perfectly shuffled deck hearts?chances ’ll win lottery?Notice questions something common. case “truth world” known question relates “kind events” happen. first question know coin fair ’s 50% chance individual coin flip come heads. second question know chance rolling 6 single die 1 6. third question know deck shuffled properly. fourth question know lottery follows specific rules. get idea. critical point probabilistic questions start known model world, use model calculations. underlying model can quite simple. instance, coin flipping example can write model like :\\[P(head)=0.5\\]can read “probability heads 0.5”. ’ll see later, way percentages numbers range 0% 100%, probabilities just numbers range 0 1. using probability model answer first question don’t actually know exactly ’s going happen. Maybe ’ll get 10 heads, like question says. maybe ’ll get three heads. ’s key thing. probability theory model known data .’s probability. statistics? Statistical questions work way around. statistics know truth world. data data want learn truth world. Statistical questions tend look like :friend flips coin 10 times gets 10 heads playing trick ?five cards top deck hearts likely deck shuffled?lottery commissioner’s spouse wins lottery likely lottery rigged?time around thing data. know saw friend flip coin 10 times came heads every time. want infer whether conclude just saw actually fair coin flipped 10 times row, whether suspect friend playing trick . data look like :H H H H H H H H H H Hand ’m trying work “model world” put trust . coin fair model adopt one says probability heads 0.5, P(heads) = 0.5. coin fair conclude probability heads 0.5, write \\(P(heads)\\ne{0.5}\\). words, statistical inference problem figure probability models right. Clearly, statistical question isn’t probability question, ’re deeply connected one another. , good introduction statistical theory start discussion probability works.","code":""},{"path":"introduction-to-probability.html","id":"what-does-probability-mean","chapter":"7 Introduction to probability","heading":"7.2 What does probability mean?","text":"Let’s start first questions. “probability”? might seem surprising statisticians mathematicians (mostly) agree rules probability , ’s much less consensus word really means. seems weird ’re comfortable using words like “chance”, “likely”, “possible” “probable”, doesn’t seem like difficult question answer. ’ve ever experience real life might walk away conversation feeling like didn’t quite get right, (like many everyday concepts) turns don’t really know ’s .’ll go . Let’s suppose want bet soccer game two teams robots, Arduino Arsenal C Milan. thinking , decide 80% probability Arduino Arsenal winning. mean ? three possibilities:’re robot teams can make play , Arduino Arsenal win 8 every 10 games average.given game, agree betting game “fair” $1 bet C Milan gives $5 payoff (.e. get $1 back plus $4 reward correct), $4 bet Arduino Arsenal (.e., $4 bet plus $1 reward).subjective “belief” “confidence” Arduino Arsenal victory four times strong belief C Milan victory.seems sensible. However, ’re identical every statistician endorse . reason different statistical ideologies (yes, really!) depending one subscribe , might say statements meaningless irrelevant. section give brief introduction two main approaches exist literature. means approaches, ’re two big ones.","code":""},{"path":"introduction-to-probability.html","id":"the-frequentist-view","chapter":"7 Introduction to probability","heading":"7.2.1 The frequentist view","text":"first two major approaches probability, dominant one statistics, referred frequentist view defines probability long-run frequency. Suppose try flipping fair coin . definition coin \\(P(H) = 0.5\\). might observe? One possibility first 20 flips might look like :T,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,HIn case 11 20 coin flips (55%) came heads. Now suppose ’d keeping running tally number heads (’ll call \\(N_H\\)) ’ve seen, across first N flips, calculate proportion heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows ’d get (literally flip coins produce !):Table 7.1:  Coin flips proportion headsNotice start sequence proportion heads fluctuates wildly, starting .00 rising high \\(.80\\). Later , one gets impression dampens bit, values actually pretty close “right” answer \\(.50\\). frequentist definition probability nutshell. Flip fair coin , N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) proportion heads converge 50%. subtle technicalities mathematicians care , qualitatively speaking ’s frequentists define probability. Unfortunately, don’t infinite number coins infinite patience required flip coin infinite number times. However, computer computers excel mindless repetitive tasks. asked computer simulate flipping coin 1000 times drew picture happens proportion \\(\\frac{N_H}{N}\\) \\(N\\) increases. Actually, four times just make sure wasn’t fluke. results shown Figure 7.1. can see, proportion observed heads eventually stops fluctuating settles . , number finally settles true probability heads.frequentist definition probability desirable characteristics. First, objective. probability event necessarily grounded world. way probability statements can make sense refer (sequence ) events occur physical universe.45 Secondly, unambiguous. two people watching sequence events unfold, trying calculate probability event, must inevitably come answer.However, also undesirable characteristics. First, infinite sequences don’t exist physical world. Suppose picked coin pocket started flip . Every time lands impacts ground. impact wears coin bit. Eventually coin destroyed. , one might ask whether really makes sense pretend “infinite” sequence coin flips even meaningful concept, objective one. can’t say “infinite sequence” events real thing physical universe, physical universe doesn’t allow infinite anything. seriously, frequentist definition narrow scope. lots things human beings happy assign probability everyday language, (even theory) mapped onto hypothetical sequence events. instance, meteorologist comes TV says “probability rain Adelaide 2 November 2048 60%” humans happy accept . ’s clear define frequentist terms. ’s one city Adelaide, one 2 November 2048. ’s infinite sequence events , just one-thing. Frequentist probability genuinely forbids us making probability statements single event. frequentist perspective either rain tomorrow . “probability” attaches single non-repeatable event. Now, said clever tricks frequentists can use get around . One possibility meteorologist means something like “category days predict 60% chance rain, look across days make prediction, 60% days actually rain”. ’s weird counter-intuitive think way, see frequentists sometimes. come later book (e.g. Estimating confidence interval).\nFigure 7.1: illustration frequentist probability works. flip fair coin proportion heads ’ve seen eventually settles converges true probability \\(0.5\\). panel shows four different simulated experiments. case pretend flipped coin \\(1000\\) times kept track proportion flips heads went along. Although none sequences actually ended exact value \\(.5\\), ’d extended experiment infinite number coin flips \n","code":""},{"path":"introduction-to-probability.html","id":"the-bayesian-view","chapter":"7 Introduction to probability","heading":"7.2.2 The Bayesian view","text":"Bayesian view probability often called subjectivist view, although minority view among statisticians steadily gaining traction last several decades. many flavours Bayesianism, making hard say exactly “” Bayesian view . common way thinking subjective probability define probability event degree belief intelligent rational agent assigns truth event. perspective, probabilities don’t exist world rather thoughts assumptions people intelligent beings.However, order approach work need way operationalising “degree belief”. One way can formalise terms “rational gambling”, though many ways. Suppose believe ’s 60% probability rain tomorrow. someone offers bet rains tomorrow win $5, doesn’t rain lose $5. Clearly, perspective, pretty good bet. hand, think probability rain 40% ’s bad bet take. can operationalise notion “subjective probability” terms bets ’m willing accept.advantages disadvantages Bayesian approach? main advantage allows assign probabilities event want . don’t need limited events repeatable. main disadvantage (many people) can’t purely objective. Specifying probability requires us specify entity relevant degree belief. entity might human, alien, robot, even statistician. intelligent agent believes things. many people uncomfortable, seems make probability arbitrary. Whilst Bayesian approach requires agent question rational (.e., obey rules probability), allow everyone beliefs. can believe coin fair don’t , even though ’re rational. frequentist view doesn’t allow two observers attribute different probabilities event. happens least one must wrong. Bayesian view prevent occurring. Two observers different background knowledge can legitimately hold different beliefs event. short, frequentist view sometimes considered narrow (forbids lots things want assign probabilities ), Bayesian view sometimes thought broad (allows many differences observers).","code":""},{"path":"introduction-to-probability.html","id":"whats-the-difference-and-who-is-right","chapter":"7 Introduction to probability","heading":"7.2.3 What’s the difference? And who is right?","text":"Now ’ve seen two views independently ’s useful make sure can compare two. Go back hypothetical robot soccer game start section. think frequentist Bayesian say three statements? statement frequentist say correct definition probability? one Bayesian opt ? statements meaningless frequentist Bayesian? ’ve understood two perspectives sense answer questions.Okay, assuming understand difference might wondering right? Honestly, don’t know right answer. far can tell ’s nothing mathematically incorrect way frequentists think sequences events, ’s nothing mathematically incorrect way Bayesians define beliefs rational agent. fact, dig details Bayesians frequentists actually agree lot things. Many frequentist methods lead decisions Bayesians agree rational agent make. Many Bayesian methods good frequentist properties.part, ’m pragmatist ’ll use statistical method trust. turns , makes prefer Bayesian methods reasons ’ll explain towards end book. ’m fundamentally opposed frequentist methods. everyone quite relaxed. instance, consider Sir Ronald Fisher, one towering figures 20th century statistics vehement opponent things Bayesian, whose paper mathematical foundations statistics referred Bayesian probability “impenetrable jungle [] arrests progress towards precision statistical concepts” (Fisher 1922b) p.311. psychologist Paul Meehl, suggests relying frequentist methods turn “potent sterile intellectual rake leaves merry path long train ravished maidens viable scientific offspring” (Meehl 1967) p.114]. history statistics, might gather, devoid entertainment.case, whilst personally prefer Bayesian view, majority statistical analyses based frequentist approach. reasoning pragmatic. goal book cover roughly territory typical undergraduate stats class psychology, want understand statistical tools used psychologists ’ll need good grasp frequentist methods. promise isn’t wasted effort. Even end wanting switch Bayesian perspective, really read least one book “orthodox” frequentist view. Besides, won’t completely ignore Bayesian perspective. Every now ’ll add commentary Bayesian point view, ’ll revisit topic depth chapter Bayesian statistics.","code":""},{"path":"introduction-to-probability.html","id":"basic-probability-theory","chapter":"7 Introduction to probability","heading":"7.3 Basic probability theory","text":"Ideological arguments Bayesians frequentists notwithstanding, turns people mostly agree rules probabilities obey. lots different ways arriving rules. commonly used approach based work Andrey Kolmogorov, one great Soviet mathematicians 20th century. won’t go lot detail, ’ll try give bit sense works. order ’m going talk trousers.","code":""},{"path":"introduction-to-probability.html","id":"introducing-probability-distributions","chapter":"7 Introduction to probability","heading":"7.3.1 Introducing probability distributions","text":"One disturbing truths life 5 pairs trousers. Three pairs jeans, bottom half suit, pair tracksuit pants. Even sadder, ’ve given names: call \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) \\(X_5\\). really , ’s call Mister Imaginative. Now, given day, pick exactly one pair trousers wear. even ’m stupid try wear two pairs trousers, thanks years training never go outside without wearing trousers anymore. describe situation using language probability theory, refer pair trousers (.e., \\(X\\)) elementary event. key characteristic elementary events every time make observation (e.g., every time put pair trousers) outcome one one events. Like said, days always wear exactly one pair trousers trousers satisfy constraint. Similarly, set possible events called sample space. Granted, people call “wardrobe”, ’s ’re refusing think trousers probabilistic terms. Sad.Okay, now sample space (wardrobe), built lots possible elementary events (trousers), want assign probability one elementary events. event \\(X\\), probability event \\(P(X)\\) number lies 0 1. bigger value \\(P(X)\\), likely event occur. , example, \\(P(X) = 0\\) means event \\(X\\) impossible (.e., never wear trousers). hand, \\(P(X) = 1\\) means event \\(X\\) certain occur (.e., always wear trousers). probability values middle means sometimes wear trousers. instance, \\(P(X) = 0.5\\) means wear trousers half time.point, ’re almost done. last thing need recognise “something always happens”. Every time put trousers, really end wearing trousers (crazy, right?). somewhat trite statement means, probabilistic terms, probabilities elementary events need add 1. known law total probability, us really care. importantly, requirements satisfied probability distribution. example, Table 7.2 shows example probability distribution.Table 7.2:  probability distribution trouser wearingEach events probability lies 0 1, add probability events sum 1. Awesome. can even draw nice bar graph (see Bar graphs section) visualise distribution, shown Figure 7.2. , point, ’ve achieved something. ’ve learned probability distribution , ’ve finally managed find way create graph focuses entirely trousers. Everyone wins! thing need point probability theory allows talk non elementary events well elementary ones. easiest way illustrate concept example. trousers example ’s perfectly legitimate refer probability wear jeans. scenario, “Dani wears jeans” event said happened long elementary event actually occur one appropriate ones. case “blue jeans”, “black jeans” “grey jeans”. mathematical terms defined “jeans” event \\(E\\) correspond set elementary events \\((X1, X2, X3)\\). elementary events occurs \\(E\\) also said occurred. decided write definition E way, ’s pretty straightforward state probability P(E) , since probabilities blue, grey black jeans respectively \\(.5\\), \\(.3\\) \\(.1\\), probability wear jeans equal \\(.9\\). : just add everything . particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] point might thinking terribly obvious simple ’d right. ’ve really done wrap basic mathematics around common sense intuitions. However, simple beginnings ’s possible construct extremely powerful mathematical tools. ’m definitely going go details book, list, Table 7.3, rules probabilities satisfy. rules can derived simple assumptions ’ve outlined , since don’t actually use rules anything book won’t .\nFigure 7.2: visual depiction ‘trousers’ probability distribution. five ‘elementary events’, corresponding five pairs trousers . event probability occurring: probability number 0 1. sum probabilities 1\nTable 7.3:  rules probabilities satisfy","code":""},{"path":"introduction-to-probability.html","id":"the-binomial-distribution","chapter":"7 Introduction to probability","heading":"7.4 The binomial distribution","text":"might imagine, probability distributions vary enormously ’s enormous range distributions . However, aren’t equally important. fact, vast majority content book relies one five distributions: binomial distribution, normal distribution, t distribution, \\(\\chi^2\\) (“chi-square”) distribution F distribution. Given , ’ll next sections provide brief introduction five , paying special attention binomial normal. ’ll start binomial distribution since ’s simplest five.","code":""},{"path":"introduction-to-probability.html","id":"introducing-the-binomial","chapter":"7 Introduction to probability","heading":"7.4.1 Introducing the binomial","text":"theory probability originated attempt describe games chance work, seems fitting discussion binomial distribution involve discussion rolling dice flipping coins. Let’s imagine simple “experiment”. hot little hand ’m holding 20 identical six-sided dice. one face die ’s picture skull, five faces blank. proceed roll 20 dice, ’s probability ’ll get exactly 4 skulls? Assuming dice fair, know chance one die coming skulls 1 6. say another way, skull probability single die approximately .167. enough information answer question, let’s look ’s done.usual, ’ll want introduce names notation. ’ll let \\(N\\) denote number dice rolls experiment, often referred size parameter binomial distribution. ’ll also use \\(\\theta\\) refer probability single die comes skulls, quantity usually called success probability binomial.46 Finally, ’ll use \\(X\\) refer results experiment, namely number skulls get roll dice. Since actual value \\(X\\) due chance refer random variable. case, now terminology notation can use state problem little precisely. quantity want calculate probability \\(X = 4\\) given know \\(\\theta = .167\\) \\(N = 20\\). general “form” thing ’m interested calculating written \\[P(X|\\theta,N)\\]’re interested special case $X = 4, = .167 $ \\(N = 20\\).[Additional technical detail 47]Yeah, yeah. know ’re thinking: notation, notation, notation. Really, cares? readers book notation, probably move talk use binomial distribution. ’ve included formula binomial distribution Table 7.2, since readers may want play , since people probably don’t care much don’t need formula book, won’t talk detail. Instead, just want show binomial distribution looks like.end, Figure 7.3 plots binomial probabilities possible values \\(X\\) dice rolling experiment, \\(X = 0\\) (skulls) way \\(X = 20\\) (skulls). Note basically bar chart, different “trousers probability” plot drew Figure 7.2. horizontal axis possible events, vertical axis can read probability events. , probability rolling \\(4\\) skulls \\(20\\) \\(0.20\\) (actual answer \\(0.2022036\\), ’ll see moment). words, ’d expect happen 20% times repeated experiment.give feel binomial distribution changes alter values \\(theta\\) \\(N\\), let’s suppose instead rolling dice ’m actually flipping coins. time around, experiment involves flipping fair coin repeatedly outcome ’m interested number heads observe. scenario, success probability now \\(\\theta = \\frac{1}{2}\\). Suppose flip coin \\(N = 20\\) times. example, ’ve changed success probability kept size experiment . binomial distribution? Well, Figure 7.4 shows, main effect shift whole distribution, ’d expect. Okay, flipped coin \\(N = 100\\) times? Well, case get Figure 7.4b. distribution stays roughly middle ’s bit variability possible outcomes.\nFigure 7.3: binomial distribution size parameter \\(N = 20\\) underlying success probability \\(\\theta = \\frac{1}{6}\\). vertical bar depicts probability one specific outcome (.e., one possible value X). probability distribution, probabilities must number 0 1, heights bars must sum 1 well\nflip coin \\(N = 20\\) times. example, ’ve changed success probability kept size experiment . binomial distribution? Well, Figure 7.4a shows, main effect shift whole distribution, ’d expect. Okay, flipped coin \\(N = 100\\) times? Well, case get Figure 7.4a. distribution stays roughly middle ’s bit variability possible outcomes.","code":""},{"path":"introduction-to-probability.html","id":"the-normal-distribution","chapter":"7 Introduction to probability","heading":"7.5 The normal distribution","text":"binomial distribution conceptually simplest distribution understand, ’s important one. particular honour goes normal distribution, also referred “bell curve” “Gaussian distribution”. normal distribution described using two parameters: mean distribution µ standard deviation distribution \\(\\sigma\\).\nFigure 7.4: Two binomial distributions, involving scenario ’m flipping fair coin, underlying success probability \\(\\theta = \\frac{1}{2}\\). panel (), assume ’m flipping coin \\(N = 20\\) times. panel (b) assume coin flipped \\(N = 100\\) times\n\nFigure 7.5: normal distribution mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\). x-axis corresponds value variable, y-axis tells us something likely observe value. However, notice y-axis labelled Probability Density Probability. subtle somewhat frustrating characteristic continuous distributions makes \\(y\\) axis behave bit oddly: height curve isn’t actually probability observing particular x value. hand, true heights curve tells \\(x\\) values likely (higher ones!). (see Probability density section annoying details)\n[Additional technical detail 48]Let’s try get sense means variable normally distributed. end, look Figure 7.5 plots normal distribution mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\). can see name “bell curve” comes ; looks bit like bell. Notice , unlike plots drew illustrate binomial distribution, picture normal distribution Figure 7.5 shows smooth curve instead “histogram-like” bars. isn’t arbitrary choice, normal distribution continuous whereas binomial discrete. instance, die rolling example last section possible get 3 skulls 4 skulls, impossible get 3.9 skulls. figures drew previous section reflected fact. Figure 7.3, instance, ’s bar located \\(X = 3\\) another one \\(X = 4\\) ’s nothing . Continuous quantities don’t constraint. instance, suppose ’re talking weather. temperature pleasant Spring day 23 degrees, 24 degrees, 23.9 degrees, anything since temperature continuous variable. normal distribution might quite appropriate describing Spring temperatures49\nFigure 7.6: illustration happens change mean normal distribution. solid line depicts normal distribution mean \\(\\mu = 4\\). dashed line shows normal distribution mean \\(\\mu = 7\\). cases, standard deviation \\(\\sigma = 1\\). surprisingly, two distributions shape, dashed line shifted right\nmind, let’s see can’t get intuition normal distribution works. First, let’s look happens play around parameters distribution. end, Figure 7.6 plots normal distributions different means standard deviation. might expect, distributions “width”. difference ’ve shifted left right. every respect ’re identical. contrast, increase standard deviation keeping mean constant, peak distribution stays place distribution gets wider, can see Figure 7.7. Notice, though, widen distribution height peak shrinks. happen, way heights bars used draw discrete binomial distribution sum 1, total area curve normal distribution must equal 1. moving , want point one important characteristic normal distribution. Irrespective actual mean standard deviation , 68.3% area falls within 1 standard deviation mean. Similarly, \\(95.4\\\\%\\) distribution falls within 2 standard deviations mean, \\((99.7\\\\%\\) distribution within 3 standard deviations. idea illustrated Figure 7.8; see also Figure 7.9.\nFigure 7.7: illustration happens change standard deviation normal distribution. distributions plotted figure mean \\(\\mu = 5\\), different standard deviations. solid line plots distribution standard deviation \\(\\sigma = 1\\), dashed line shows distribution standard deviation \\(\\sigma = 2\\). consequence, distributions ‘centred’ spot, dashed line wider solid one\n\nFigure 7.8: area curve tells probability observation falls within particular range. solid lines plot normal distributions mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\). shaded areas illustrate ‘areas curve’ two important cases. panel , can see 68.3% chance observation fall within one standard deviation mean. panel b, see 95.4% chance observation fall within two standard deviations mean\n\nFigure 7.9: Two examples ‘area curve idea’. 15.9% chance observation one standard deviation mean smaller (panel ), 34.1% chance observation somewhere one standard deviation mean mean (panel b). Notice add two numbers together get 15.9% + 34.1% = 50%. normally distributed data, 50% chance observation falls mean. course also implies 50% chance falls mean\n","code":""},{"path":"introduction-to-probability.html","id":"probability-density","chapter":"7 Introduction to probability","heading":"7.5.1 Probability density","text":"’s something ’ve trying hide throughout discussion normal distribution, something introductory textbooks omit completely. might right . “thing” ’m hiding weird counter-intuitive even admittedly distorted standards apply statistics. Fortunately, ’s something need understand deep level order basic statistics. Rather, ’s something starts become important later move beyond basics. , doesn’t make complete sense, don’t worry much, try make sure follow gist .Throughout discussion normal distribution ’s one two things don’t quite make sense. Perhaps noticed y-axis figures labelled “Probability Density” rather density. Maybe noticed used \\(P(X)\\) instead \\(P(X)\\) giving formula normal distribution.turns , presented isn’t actually probability, ’s something else. understand something spend little time thinking really means say \\(X\\) continuous variable. Let’s say ’re talking temperature outside. thermometer tells ’s \\(23\\) degrees, know ’s really true. ’s exactly \\(23\\) degrees. Maybe ’s \\(23.1\\) degrees, think . know ’s really true either might actually \\(23.09\\) degrees. know … well, get idea. tricky thing genuinely continuous quantities never really know exactly .Now think implies talk probabilities. Suppose tomorrow’s maximum temperature sampled normal distribution mean \\(23\\) standard deviation 1. ’s probability temperature exactly \\(23\\) degrees? answer “zero”, possibly “number close zero might well zero”. ? ’s like trying throw dart infinitely small dart board. matter good aim, ’ll never hit . real life ’ll never get value exactly \\(23\\). ’ll always something like \\(23.1\\) \\(22.99998\\) suchlike. words, ’s completely meaningless talk probability temperature exactly \\(23\\) degrees. However, everyday language told \\(23\\) degrees outside turned \\(22.9998\\) degrees probably wouldn’t call liar. everyday language “\\(23\\) degrees” usually means something like “somewhere \\(22.5\\) \\(23.5\\) degrees”. doesn’t feel meaningful ask probability temperature exactly \\(23\\) degrees, seem sensible ask probability temperature lies \\(22.5\\) \\(23.5\\), \\(20\\) \\(30\\), range temperatures.point discussion make clear ’re talking continuous distributions ’s meaningful talk probability specific value. However, can talk probability value lies within particular range values. find probability associated particular range need calculate “area curve”. ’ve seen concept already, Figure 7.8 shaded areas shown depict genuine probabilities (e.g., Figure 7.8 shows probability observing value falls within 1 standard deviation mean).Okay, explains part story. ’ve explained little bit continuous probability distributions interpreted (.e., area curve key thing). formula ppxq described earlier actually mean? Obviously, \\(P(x)\\) doesn’t describe probability, ? name quantity \\(P(x)\\) probability density, terms plots ’ve drawing corresponds height curve. densities aren’t meaningful , ’re “rigged” ensure area curve always interpretable genuine probabilities. honest, ’s much really need know now.50","code":""},{"path":"introduction-to-probability.html","id":"other-useful-distributions","chapter":"7 Introduction to probability","heading":"7.6 Other useful distributions","text":"normal distribution distribution statistics makes use (reasons discussed shortly), binomial distribution useful one lots purposes. world statistics filled probability distributions, ’ll run passing. particular, three appear book t distribution, \\(\\chi^2\\) distribution F distribution. won’t give formulas , talk much detail, show pictures: Figures 7.10, 7.11 7.12).\nFigure 7.10: \\(t\\) distribution 3 degrees freedom (solid line). looks similar normal distribution, ’s quite . comparison purposes ’ve plotted standard normal distribution dashed line\n\nFigure 7.11: \\(\\chi^2\\) distribution 3 degrees freedom. Notice observed values must always greater zero, distribution pretty skewed. key features chi-square distribution\n\nFigure 7.12: \\(F\\) distribution 3 5 degrees freedom. Qualitatively speaking, looks pretty similar chi-square distribution, ’re quite general\n\\(t\\) distribution continuous distribution looks similar normal distribution, see Figure 7.10. Note “tails” t distribution “heavier” (.e., extend outwards) tails normal distribution). ’s important difference two. distribution tends arise situations think data actually follow normal distribution, don’t know mean standard deviation. ’ll run distribution chapter Comparing two means.\\(\\chi^2\\) distribution another distribution turns lots different places. situation ’ll see Categorical data analysis, ’s one things actually pops place. dig maths (doesn’t love ?), turns main reason \\(\\chi^2\\) distribution turns place bunch variables normally distributed, square values add (procedure referred taking “sum squares”), sum \\(\\chi^2\\) distribution. ’d amazed often fact turns useful. Anyway, ’s \\(\\chi^2\\) distribution looks like: Figure 7.11.\\(F\\) distribution looks bit like \\(\\chi^2\\) distribution, arises whenever need compare two \\(\\chi^2\\) distributions one another. Admittedly, doesn’t exactly sound like something sane person want , turns important real world data analysis. Remember said \\(\\chi^2\\) turns key distribution ’re taking “sum squares”? Well, means want compare two different “sums squares”, ’re probably talking something F distribution. course, yet still haven’t given example anything involves sum squares, Chapter Comparing several means (one-way ANOVA). ’s ’ll run F distribution. Oh, ’s picture Figure 7.12.Okay, time wrap section . ’ve seen three new distributions: \\(\\chi^2\\)), \\(t\\) \\(F\\). ’re continuous distributions, ’re closely related normal distribution. main thing purposes grasp basic idea distributions deeply related one another, normal distribution. Later book ’re going run data normally distributed, least assumed normally distributed. want understand right now , make assumption data normally distributed, shouldn’t surprised see \\(\\chi^2\\), \\(t\\) \\(F\\) distributions popping place start trying data analysis.","code":""},{"path":"introduction-to-probability.html","id":"summary-5","chapter":"7 Introduction to probability","heading":"7.7 Summary","text":"chapter ’ve talked probability. ’ve talked probability means statisticians can’t agree means. talked rules probabilities obey. introduced idea probability distribution spent good chunk chapter talking important probability distributions statisticians work . section section breakdown looks like :Probability theory versus statistics: probability statistics different?frequentist view versus Bayesian view probabilityBasic probability theoryThe binomial distribution, normal distribution, [distributions]’d expect, coverage means exhaustive. Probability theory large branch mathematics right, entirely separate application statistics data analysis. , thousands books written subject universities generally offer multiple classes devoted entirely probability theory. Even “simpler” task documenting standard probability distributions big topic. ’ve described five standard probability distributions chapter, sitting bookshelf 45-chapter book called “Statistical Distributions” (M. Evans, Hastings, Peacock 2011) lists lot . Fortunately , little necessary. ’re unlikely need know dozens statistical distributions go real world data analysis, definitely won’t need book, never hurts know ’s possibilities .Picking last point, ’s sense whole chapter something digression. Many undergraduate psychology classes statistics skim content quickly (know mine ), even advanced classes often “forget” revisit basic foundations field. academic psychologists know difference probability density, recently aware difference Bayesian frequentist probability. However, think ’s important understand things moving onto applications. example, lot rules ’re “allowed” say statistical inference many can seem arbitrary weird. However, start make sense understand Bayesian vs. frequentist distinction. Similarly, chapter Comparing two means ’re going talk something called t-test, really want grasp mechanics t-test really helps sense t-distribution actually looks like. get idea, hope.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"estimating-unkown-quantities-from-a-sample","chapter":"8 Estimating unkown quantities from a sample","heading":"8 Estimating unkown quantities from a sample","text":"start last chapter highlighted critical distinction descriptive statistics inferential statistics. discussed chapter Descriptive statistics, role descriptive statistics concisely summarise know. contrast, purpose inferential statistics “learn know ”. Now foundation probability theory good position think problem statistical inference. kinds things like learn ? learn ? questions lie heart inferential statistics, traditionally divided two “big ideas”: estimation hypothesis testing. goal chapter introduce first big ideas, estimation theory, ’m going witter sampling theory first estimation theory doesn’t make sense understand sampling. consequence, chapter divides naturally two parts, first three sections focused sampling theory, last two sections make use sampling theory discuss statisticians think estimation.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"samples-populations-and-sampling","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1 Samples, populations and sampling","text":"[Prelude Part IV] discussed riddle induction highlighted fact learning requires make assumptions. Accepting true, first task come fairly general assumptions data make sense. sampling theory comes . probability theory foundations upon statistical theory builds, sampling theory frame around can build rest house. Sampling theory plays huge role specifying assumptions upon statistical inferences rely. order talk “making inferences” way statisticians think need bit explicit ’re drawing inferences (sample) ’re drawing inferences (population).almost every situation interest available us researchers sample data. might run experiment number participants, polling company might phoned number people ask questions voting intentions, . way data set available us finite incomplete. can’t possibly get every person world experiment, example polling company doesn’t time money ring every voter country. earlier discussion Descriptive statistics sample thing interested . goal find ways describing, summarising graphing sample. change.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"defining-a-population","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1.1 Defining a population","text":"sample concrete thing. can open data file ’s data sample. population, hand, abstract idea. refers set possible people, possible observations, want draw conclusions generally much bigger sample. ideal world researcher begin study clear idea population interest , since process designing study testing hypotheses data depend population want make statements.Sometimes ’s easy state population interest. instance, “polling company” example opened chapter population consisted voters enrolled time study, millions people. sample set 1000 people belong population. studies situation much less straightforward. typical psychological experiment determining population interest bit complicated. Suppose run experiment using 100 undergraduate students participants. goal, cognitive scientist, try learn something mind works. , following count “population”:undergraduate psychology students University Adelaide?Undergraduate psychology students general, anywhere world?Australians currently living?Australians similar ages sample?Anyone currently alive?human , past, present future?biological organism sufficient degree intelligence operating terrestrial environment?intelligent ?defines real group mind-possessing entities, might interest cognitive scientist, ’s clear one true population interest. another example, consider Wellesley-Croker game discussed [Prelude Part IV]. sample specific sequence 12 wins 0 losses Wellesley. population? , ’s obvious population .outcomes Wellesley Croker arrived destination?outcomes Wellesley Croker played game rest lives?outcomes Wellseley Croker lived forever played game world ran hills?outcomes created infinite set parallel universes Wellesely/Croker pair made guesses 12 hills universe?","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"simple-random-samples","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1.2 Simple random samples","text":"Irrespective define population, critical point sample subset population goal use knowledge sample draw inferences properties population. relationship two depends procedure sample selected. procedure referred sampling method important understand matters.keep things simple, let’s imagine bag containing 10 chips. chip unique letter printed can distinguish 10 chips. chips come two colours, black white. set chips population interest depicted graphically left Figure 8.1. can see looking picture 4 black chips 6 white chips, course real life wouldn’t know unless looked bag. Now imagine run following “experiment”: shake bag, close eyes, pull 4 chips without putting back bag. First comes chip (black), c chip (white), j (white) finally b (black). wanted put chips back bag repeat experiment, depicted right hand side Figure 8.1. time get different results procedure identical case. fact procedure can lead different results time refer random process.51 However, shook bag pulling chips , seems reasonable think every chip chance selected. procedure every member population chance selected called simple random sample. fact put chips back bag pulling means can’t observe thing twice, cases observations said sampled without replacement.\nFigure 8.1: Simple random sampling without replacement finite population\nhelp make sure understand importance sampling procedure, consider alternative way experiment run. Suppose 5-year old son opened bag decided pull four black chips without putting back bag. biased sampling scheme depicted Figure 8.2. Now consider evidential value seeing 4 black chips 0 white chips. Clearly depends sampling scheme, ? know sampling scheme biased select black chips sample consists black chips doesn’t tell much population! reason statisticians really like data set can considered simple random sample, makes data analysis much easier.\nFigure 8.2: Biased sampling without replacement finite population\nthird procedure worth mentioning. time around close eyes, shake bag, pull chip. time, however, record observation put chip back bag. close eyes, shake bag, pull chip. repeat procedure 4 chips. Data sets generated way still simple random samples, put chips back bag immediately drawing referred sample replacement. difference situation first one possible observe population member multiple times, illustrated Figure 8.3.\nFigure 8.3: Simple random sampling replacement finite population\nexperience, psychology experiments tend sampling without replacement, person allowed participate experiment twice. However, statistical theory based assumption data arise simple random sample replacement. real life rarely matters. population interest large (e.g., 10 entities!) difference sampling - without- replacement small concerned . difference simple random samples biased samples, hand, easy thing dismiss.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"most-samples-are-not-simple-random-samples","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1.3 Most samples are not simple random samples","text":"can see looking list possible populations showed , almost impossible obtain simple random sample populations interest. run experiments ’d consider minor miracle participants turned random sampling undergraduate psychology students Adelaide university, even though far narrowest population might want generalise . thorough discussion types sampling schemes beyond scope book, give sense ’s ’ll list important ones.Stratified sampling. Suppose population (can ) divided several different sub-populations, strata. Perhaps ’re running study several different sites, example. Instead trying sample randomly population whole, instead try collect separate random sample strata. Stratified sampling sometimes easier simple random sampling, especially population already divided distinct strata. can also efficient simple random sampling, especially sub-populations rare. instance, studying schizophrenia much better divide population two 52 strata (schizophrenic -schizophrenic) sample equal number people group. selected people randomly get schizophrenic people sample study useless. specific kind stratified sampling referred oversampling makes deliberate attempt -represent rare groupsSnowball sampling technique especially useful sampling “hidden” hard access population especially common social sciences. instance, suppose researchers want conduct opinion poll among transgender people. research team might contact details trans folks, survey starts asking participate (stage 1). end survey participants asked provide contact details people might want participate. stage 2 new contacts surveyed. process continues researchers sufficient data. big advantage snowball sampling gets data situations might otherwise impossible get . statistical side, main disadvantage sample highly non-random, non-random ways difficult address. real life side, disadvantage procedure can unethical handled well, hidden populations often hidden reason. chose transgender people example highlight issue. weren’t careful might end outing people don’t want outed (, bad form), even don’t make mistake can still intrusive use people’s social networks study . ’s certainly hard get people’s informed consent contacting , yet many cases simple act contacting saying “hey want study ” can hurtful. Social networks complex things, just can use get data doesn’t always mean .Convenience sampling less sounds like. samples chosen way convenient researcher, selected random population interest. Snowball sampling one type convenience sampling, many others. common example psychology studies rely undergraduate psychology students. samples generally non-random two respects. First, reliance undergraduate psychology students automatically means data restricted single sub-population. Second, students usually get pick studies participate , sample self selected subset psychology students randomly selected subset. real life studies convenience samples one form another. sometimes severe limitation, always.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"how-much-does-it-matter-if-you-dont-have-a-simple-random-sample","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1.4 How much does it matter if you don’t have a simple random sample?","text":"Okay, real world data collection tends involve nice simple random samples. matter? little thought make clear can matter data simple random sample. Just think difference Figures 8.1 8.2. However, ’s quite bad sounds. types biased samples entirely unproblematic. instance, using stratified sampling technique actually know bias created deliberately, often increase effectiveness study, statistical techniques can use adjust biases ’ve introduced (covered book!). situations ’s problem.generally though, ’s important remember random sampling means end, end . Let’s assume ’ve relied convenience sample, can assume ’s biased. bias sampling method problem causes draw wrong conclusions. viewed perspective, ’d argue don’t need sample randomly generated every respect, need random respect psychologically-relevant phenomenon interest. Suppose ’m study looking working memory capacity. study 1, actually ability sample randomly human beings currently alive, one exception: can sample people born Monday. study 2, able sample randomly Australian population. want generalise results population living humans. study better? answer, obviously, study 1. ? reason think “born Monday” interesting relationship working memory capacity. contrast, can think several reasons “Australian” might matter. Australia wealthy, industrialised country well-developed education system. People growing system life experiences much similar experiences people designed tests working memory capacity. shared experience might easily translate similar beliefs “take test”, shared assumption psychological experimentation works, . things might actually matter. instance, “test taking” style might taught Australian participants direct attention exclusively fairly abstract test materials much people haven’t grown similar environment. therefore lead misleading picture working memory capacity .two points hidden discussion. First, designing studies, ’s important think population care try hard sample way appropriate population. practice, ’re usually forced put “sample convenience” (e.g., psychology lecturers sample psychology students ’s least expensive way collect data, coffers aren’t exactly overflowing gold), least spend time thinking dangers practice might . Second, ’re going criticise someone else’s study ’ve used sample convenience rather laboriously sampling randomly entire human population, least courtesy offer specific theory might distorted results.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"population-parameters-and-sample-statistics","chapter":"8 Estimating unkown quantities from a sample","heading":"8.1.5 Population parameters and sample statistics","text":"Okay. Setting aside thorny methodological issues associated obtaining random sample, let’s consider slightly different issue. point talking populations way scientist might. psychologist population might group people. ecologist population might group bears. cases populations scientists care concrete things actually exist real world. Statisticians, however, funny lot. one hand, interested real world data real science way scientists . hand, also operate realm pure abstraction way mathematicians . consequence, statistical theory tends bit abstract population defined. much way psychological researchers operationalise abstract theoretical ideas terms concrete measurements (Introduction psychological measurement), statisticians operationalise concept “population” terms mathematical objects know work . ’ve already come across objects chapter Introduction probability. ’re called probability distributions.idea quite simple. Let’s say ’re talking IQ scores. psychologist population interest group actual humans IQ scores. statistician “simplifies” operationally defining population probability distribution depicted Figure 8.4a. IQ tests designed average IQ 100, standard deviation IQ scores 15, distribution IQ scores normal. values referred population parameters characteristics entire population. , say population mean µ 100 population standard deviation σ 15.\nFigure 8.4: population distribution IQ scores (panel ) two samples drawn randomly . panel b sample 100 observations, panel c sample 10,000 observations\nNow suppose run experiment. select 100 people random administer IQ test, giving simple random sample population. sample consist collection numbers like :106 101 98 80 74 … 107 72 100Each IQ scores sampled normal distribution mean 100 standard deviation 15. plot histogram sample get something like one shown Figure 8.4b. can see, histogram roughly right shape ’s crude approximation true population distribution shown Figure 8.4a. calculate mean sample, get number fairly close population mean 100 identical. case, turns people sample mean IQ 98.5, standard deviation IQ scores 15.9. sample statistics properties data set, although fairly similar true population values . general, sample statistics things can calculate data set population parameters things want learn . Later chapter ’ll talk Estimating population parameters using sample statistics also Estimating confidence interval get ’s ideas sampling theory need know ","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"the-law-of-large-numbers","chapter":"8 Estimating unkown quantities from a sample","heading":"8.2 The law of large numbers","text":"previous section showed results one fictitious IQ experiment sample size N = 100. results somewhat encouraging true population mean 100 sample mean 98.5 pretty reasonable approximation . many scientific studies level precision perfectly acceptable, situations need lot precise. want sample statistics much closer population parameters, can ? obvious answer collect data. Suppose ran much larger experiment, time measuring IQs 10,000 people. can simulate results experiment using jamovi. IQsim.omv file jamovi data file. file generated 10,000 random numbers sampled normal distribution population mean = 100 sd = 15. done computing new variable using = NORM(100,15) function. histogram density plot shows larger sample much better approximation true population distribution smaller one. reflected sample statistics. mean IQ larger sample turns 99.68 standard deviation 14.90. values now close true population. See Figure 8.5.\nFigure 8.5: random sample drawn normal distribution using jamovi\nfeel bit silly saying , thing want take away large samples generally give better information. feel silly saying ’s bloody obvious shouldn’t need said. fact, ’s obvious point Jacob Bernoulli, one founders probability theory, formalised idea back 1713 kind jerk . ’s described fact share intuition:even stupid men, instinct nature, without instruction (remarkable thing), convinced observations made, less danger wandering one’s goal (Stigler (1986), p65).Okay, passage comes across bit condescending (mention sexist), main point correct. really feel obvious data give better answers. question , ? surprisingly, intuition share turns correct, statisticians refer law large numbers. law large numbers mathematical law applies many different sample statistics simplest way think law averages. sample mean obvious example statistic relies averaging (’s mean … average), let’s look . applied sample mean law large numbers states sample gets larger, sample mean tends get closer true population mean. , say little bit precisely, sample size “approaches” infinity (written \\(N \\longrightarrow \\infty\\)), sample mean approaches population mean \\(\\bar{X} \\longrightarrow \\mu\\))53I don’t intend subject proof law large numbers true, ’s one important tools statistical theory. law large numbers thing can use justify belief collecting data eventually lead us truth. particular data set sample statistics calculate wrong, law large numbers tells us keep collecting data sample statistics tend get closer closer true population parameters.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"sampling-distributions-and-the-central-limit-theorem","chapter":"8 Estimating unkown quantities from a sample","heading":"8.3 Sampling distributions and the central limit theorem","text":"law large numbers powerful tool ’s going good enough answer questions. Among things, gives us “long run guarantee”. long run, somehow able collect infinite amount data, law large numbers guarantees sample statistics correct. John Maynard Keynes famously argued economics, long run guarantee little use real life.[] long run misleading guide current affairs. long run dead. Economists set easy, useless task, tempestuous seasons can tell us, storm long past, ocean flat . (Keynes (1923), p. 80).economics, psychology statistics. enough know eventually arrive right answer calculating sample mean. Knowing infinitely large data set tell exact value population mean cold comfort actual data set sample size \\(N = 100\\). real life, , must know something behaviour sample mean calculated modest data set!","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"sampling-distribution-of-the-mean","chapter":"8 Estimating unkown quantities from a sample","heading":"8.3.1 Sampling distribution of the mean","text":"mind, let’s abandon idea studies sample sizes 10,000 consider instead modest experiment indeed. time around ’ll sample \\(N = 5\\) people measure IQ scores. , can simulate experiment jamovi = NORM(100,15) function, need 5 participant IDs time, 10,000. five numbers jamovi generated:90 82 94 99 110The mean IQ sample turns exactly 95. surprisingly, much less accurate previous experiment. Now imagine decided replicate experiment. , repeat procedure closely possible randomly sample 5 new people measure IQ. , jamovi allows simulate results procedure, generates five numbers:78 88 111 111 117This time around, mean IQ sample 101. repeat experiment 10 times obtain results shown Table 8.1, can see sample mean varies one replication next.Table 8.1:  Ten replications IQ experiment, sample size $( N = 5 )$Now suppose decided keep going fashion, replicating “five IQ scores” experiment . Every time replicate experiment write sample mean. time, ’d amassing new data set, every experiment generates single data point. first 10 observations data set sample means listed Table 8.1, data set starts like :95.0 101.0 101.6 103.8 104.4 …continued like 10,000 replications, drew histogram. Well ’s exactly , can see results Figure 8.6. picture illustrates, average 5 IQ scores usually 90 110. importantly, highlights replicate experiment , end distribution sample means! (Table 8.1) distribution special name statistics, ’s called sampling distribution mean.\nFigure 8.6: sampling distribution mean ‘five IQ scores experiment’. sample 5 people random calculate average IQ ’ll almost certainly get number 80 120, even though quite lot individuals IQs 120 80. comparison, black line plots population distribution IQ scores\nSampling distributions another important theoretical idea statistics, ’re crucial understanding behaviour small samples. instance, ran first “five IQ scores” experiment, sample mean turned 95. sampling distribution Figure 8.6 tells us, though, “five IQ scores” experiment accurate. repeat experiment, sampling distribution tells can expect see sample mean anywhere 80 120.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"sampling-distributions-exist-for-any-sample-statistic","chapter":"8 Estimating unkown quantities from a sample","heading":"8.3.2 Sampling distributions exist for any sample statistic!","text":"One thing keep mind thinking sampling distributions sample statistic might care calculate sampling distribution. example, suppose time replicated “five IQ scores” experiment wrote largest IQ score experiment. give data set started like :110 117 122 119 113 …give different sampling distribution, namely sampling distribution maximum. sampling distribution maximum 5 IQ scores shown Figure 8.7. surprisingly, pick 5 people random find person highest IQ score, ’re going average IQ. time ’ll end someone whose IQ measured 100 140 range.\nFigure 8.7: sampling distribution maximum ‘five IQ scores experiment’. sample 5 people random select one highest IQ score ’ll probably see someone IQ 100 140\n\nFigure 8.8: illustration sampling distribution mean depends sample size. panel generated 10,000 samples IQ data calculated mean IQ observed within data sets. histograms plots show distribution means (.e., sampling distribution mean). individual IQ score drawn normal distribution mean 100 standard deviation 15, shown solid black line. panel , data set contained single observation, mean sample just one person’s IQ score. consequence, sampling distribution mean course identical population distribution IQ scores. However, raise sample size 2 mean one sample tends closer population mean one person’s IQ score, histogram (.e., sampling distribution) bit narrower population distribution. time raise sample size 10 (panel c), can see distribution sample means tend fairly tightly clustered around true population mean\n","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"the-central-limit-theorem","chapter":"8 Estimating unkown quantities from a sample","heading":"8.3.3 The central limit theorem","text":"point hope pretty good sense sampling distributions , particular sampling distribution mean . section want talk sampling distribution mean changes function sample size. Intuitively, already know part answer. observations, sample mean likely quite inaccurate. replicate small experiment recalculate mean ’ll get different answer. words, sampling distribution quite wide. replicate large experiment recalculate sample mean ’ll probably get answer got last time, sampling distribution narrow. can see visually Figure 8.8, showing bigger sample size, narrower sampling distribution gets. can quantify effect calculating standard deviation sampling distribution, referred standard error. standard error statistic often denoted SE, since ’re usually interested standard error sample mean, often use acronym SEM. can see just looking picture, sample size \\(N\\) increases, SEM decreases.Okay, ’s one part story. However, ’s something ’ve glossing far. examples point based “IQ scores” experiments, IQ scores roughly normally distributed ’ve assumed population distribution normal. isn’t normal? happens sampling distribution mean? remarkable thing , matter shape population distribution , N increases sampling distribution mean starts look like normal distribution. give sense ran simulations. , started “ramped” distribution shown histogram Figure 8.9. can see comparing triangular shaped histogram bell curve plotted black line, population distribution doesn’t look much like normal distribution . Next, simulated results large number experiments. experiment took \\(N = 2\\) samples distribution, calculated sample mean. Figure 8.9b plots histogram sample means (.e., sampling distribution mean \\(N = 2\\)). time, histogram produces \\(\\chi^2\\)-shaped distribution. ’s still normal, ’s lot closer black line population distribution Figure 8.9a. increase sample size \\(N = 4\\), sampling distribution mean close normal (Figure 8.9c), time reach sample size N = 8 ’s almost perfectly normal. words, long sample size isn’t tiny, sampling distribution mean approximately normal matter population distribution looks like!\nFigure 8.9: demonstration central limit theorem. panel , non-normal population distribution, panels b-d show sampling distribution mean samples size 2,4 8 data drawn distribution panel . can see, even though original population distribution non-normal sampling distribution mean becomes pretty close normal time sample even 4 observations\nbasis figures, seems like evidence following claims sampling distribution mean.mean sampling distribution mean populationThe standard deviation sampling distribution (.e., standard error) gets smaller sample size increasesThe shape sampling distribution becomes normal sample size increasesAs happens, statements true, famous theorem statistics proves three , known central limit theorem. Among things, central limit theorem tells us population distribution mean µ standard deviation σ, sampling distribution mean also mean µ standard error mean \\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]divide population standard deviation σ square root sample size N, SEM gets smaller sample size increases. also tells us shape sampling distribution becomes normal.54This result useful sorts things. tells us large experiments reliable small ones, gives us explicit formula standard error tells us much reliable large experiment . tells us normal distribution , well, normal. real experiments, many things want measure actually averages lots different quantities (e.g., arguably, “general” intelligence measured IQ average large number “specific” skills abilities), happens, averaged quantity follow normal distribution. mathematical law, normal distribution pops real data.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"estimating-population-parameters","chapter":"8 Estimating unkown quantities from a sample","heading":"8.4 Estimating population parameters","text":"IQ examples previous sections actually knew population parameters ahead time. every undergraduate gets taught first lecture measurement intelligence, IQ scores defined mean 100 standard deviation 15. However, bit lie. know IQ scores true population mean 100? Well, know people designed tests administered large samples, “rigged” scoring rules sample mean 100. ’s bad thing course, ’s important part designing psychological measurement. However, ’s important keep mind theoretical mean 100 attaches population test designers used design tests. Good test designers actually go lengths provide “test norms” can apply lots different populations (e.g., different age groups, nationalities etc).handy, course almost every research project interest involves looking different population people used test norms. instance, suppose wanted measure effect low level lead poisoning cognitive functioning Port Pirie, South Australian industrial town lead smelter. Perhaps decide want compare IQ scores among people Port Pirie comparable sample Whyalla, South Australian industrial town steel refinery.55 Regardless town ’re thinking , doesn’t make lot sense simply assume true population mean IQ 100. -one , knowledge, produced sensible norming data can automatically applied South Australian industrial towns. ’re going estimate population parameters sample data. ?","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"estimating-the-population-mean","chapter":"8 Estimating unkown quantities from a sample","heading":"8.4.1 Estimating the population mean","text":"Suppose go Port Pirie 100 locals kind enough sit IQ test. average IQ score among people turns \\(\\bar{X}=98.5\\). true mean IQ entire population Port Pirie? Obviously, don’t know answer question. 97.2, also 103.5. sampling isn’t exhaustive give definitive answer. Nevertheless, forced gunpoint give “best guess” ’d say 98.5. ’s essence statistical estimation: giving best guess.example estimating unknown poulation parameter straightforward. calculate sample mean use estimate population mean. ’s pretty simple, next section ’ll explain statistical justification intuitive answer. However, moment want make sure recognise sample statistic estimate population parameter conceptually different things. sample statistic description data, whereas estimate guess population. mind, statisticians often different notation refer . instance, true population mean denoted \\(\\mu\\), use \\(\\hat{mu}\\) refer estimate population mean. contrast, sample mean denoted \\(\\bar{X}\\) sometimes m. However, simple random samples estimate population mean identical sample mean. observe sample mean \\(\\bar{X}=98.5\\) estimate population mean also \\(\\hat{\\mu}=98.5\\). help keep notation clear, ’s handy table (Table 8.2):Table 8.2:  Notation mean","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"estimating-the-population-standard-deviation","chapter":"8 Estimating unkown quantities from a sample","heading":"8.4.2 Estimating the population standard deviation","text":"far, estimation seems pretty simple, might wondering forced read stuff sampling theory. case mean estimate population parameter (.e. \\(\\hat{\\mu}\\)) turned identical corresponding sample statistic (.e. \\(\\bar{X}\\)). However, ’s always true. see , let’s think construct estimate population standard deviation, ’ll denote \\(\\hat{\\sigma}\\). shall use estimate case? first thought might thing estimating mean, just use sample statistic estimate. ’s almost right thing , quite.’s . Suppose sample contains single observation. example, helps consider sample intuitions true population values might , let’s use something completely fictitious. Suppose observation question measures cromulence shoes. turns shoes cromulence \\(20\\). ’s sample:perfectly legitimate sample, even sample size \\(N = 1\\). sample mean \\(20\\) every observation sample equal sample mean (obviously!) sample standard deviation 0. description sample seems quite right, sample contains single observation therefore variation observed within sample. sample standard deviation \\(s = 0\\) right answer . estimate population standard deviation feels completely insane, right? Admittedly, don’t know anything “cromulence” , know something data. reason don’t see variability sample sample small display variation! , sample size \\(N = 1\\) feels like right answer just say “idea ”.Notice don’t intuition comes sample mean population mean. forced make best guess population mean doesn’t feel completely insane guess population mean \\(20\\). Sure, probably wouldn’t feel confident guess one observation work , ’s still best guess can make.Let’s extend example little. Suppose now make second observation. data set now \\(N = 2\\) observations cromulence shoes, complete sample now looks like :\\[20, 22\\]time around, sample just large enough us able observe variability: two observations bare minimum number needed variability observed! new data set, sample mean \\(\\bar{X} = 21\\), sample standard deviation \\(s = 1\\). intuitions population? , far population mean goes, best guess can possibly make sample mean. forced guess ’d probably guess population mean cromulence \\(21\\). standard deviation? little complicated. sample standard deviation based two observations, ’re like probably intuition , two observations haven’t given population “enough chance” reveal true variability us. ’s just suspect estimate wrong, two observations expect wrong degree. worry error systematic. Specifically, suspect sample standard deviation likely smaller population standard deviation.intuition feels right, nice demonstrate somehow. fact mathematical proofs confirm intuition, unless right mathematical background don’t help much. Instead, ’ll simulate results experiments. mind, let’s return IQ studies. Suppose true population mean IQ \\(100\\) standard deviation \\(15\\). First ’ll conduct experiment measure \\(N = 2\\) IQ scores ’ll calculate sample standard deviation. , plot histogram sample standard deviations, sampling distribution standard deviation. ’ve plotted distribution Figure 8.10. Even though true population standard deviation 15 average sample standard deviations 8.5. Notice different result found Figure 8.8b plotted sampling distribution mean, population mean \\(100\\) average sample means also \\(100\\).\nFigure 8.10: sampling distribution sample standard deviation ‘two IQ scores’ experiment. true population standard deviation 15 (dashed line), can see histogram vast majority experiments produce much smaller sample standard deviation . average, experiment produce sample standard deviation 8.5, well true value! words, sample standard deviation biased estimate population standard deviation\nNow let’s extend simulation. Instead restricting situation \\(N=2\\), let’s repeat exercise sample sizes 1 10. plot average sample mean average sample standard deviation function sample size, get results shown Figure 8.11. left hand side (panel ) ’ve plotted average sample mean right hand side (panel b) ’ve plotted average standard deviation. two plots quite different:average, average sample mean equal population mean. unbiased estimator, essentially reason best estimate population mean sample mean.56 plot right quite different: average, sample standard deviation \\(s\\) smaller population standard deviation \\(\\sigma\\). biased estimator. words, want make “best guess” \\(\\hat{\\sigma}\\) value population standard deviation \\(\\hat{\\sigma}\\) make sure guess little bit larger sample standard deviation \\(s\\).\nFigure 8.11: illustration fact sample mean unbiased estimator population mean (panel ), sample standard deviation biased estimator population standard deviation (panel b). figure generated \\(10,000\\) simulated data sets 1 observation , \\(10,000\\) 2 observations, sample size 10. data set consisted fake IQ data, data normally distributed true population mean 100 standard deviation 15. average, sample means turn 100, regardless sample size (panel ). However, sample standard deviations turn systematically small (panel b), especially small sample sizes\nfix systematic bias turns simple. ’s works. tackling standard deviation let’s look variance. recall section Estimating population parameters, sample variance defined average squared deviations sample mean. : \\[s^2=\\frac{1}{N} \\sum_{=1}^{N}(X_i-\\bar{X})^2\\] sample variance \\(s^2\\) biased estimator population variance \\(\\sigma^2\\). turns , need make tiny tweak transform unbiased estimator. divide \\(N-1\\) rather \\(N\\).unbiased estimator population variance \\(\\sigma\\). Moreover, finally answers question raised Estimating population parameters. jamovi give us slightly different answers variance? ’s jamovi calculates \\(\\hat{\\sigma}^2 \\text{ } s^2\\), ’s . similar story applies standard deviation. divide \\(N - 1\\) rather \\(N\\) estimate population standard deviation unbiased, use jamovi’s built standard deviation function, ’s calculating \\(\\hat{\\sigma}\\) \\(s\\).57One final point. practice, lot people tend refer \\(\\hat{\\sigma}\\) (.e., formula divide \\(N - 1\\)) sample standard deviation. Technically, incorrect. sample standard deviation equal s (.e., formula divide N). aren’t thing, either conceptually numerically. One property sample, estimated characteristic population. However, almost every real life application actually care estimate population parameter, people always report \\(\\hat{\\sigma}\\) rather s. right number report, course. ’s just people tend get little bit imprecise terminology write , “sample standard deviation” shorter “estimated population standard deviation”. ’s big deal, practice thing everyone else . Nevertheless, think ’s important keep two concepts separate. ’s never good idea confuse “known properties sample” “guesses population came”. moment start thinking \\(s\\) \\(\\hat{\\sigma}\\) thing, start exactly .finish section , ’s another couple tables help keep things clear (Tables 8.3 8.4.Table 8.3:  Notation standard deviationTable 8.4:  Notation variance","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"estimating-a-confidence-interval","chapter":"8 Estimating unkown quantities from a sample","heading":"8.5 Estimating a confidence interval","text":"Statistics means never say ’re certain\n– Unknown origin 58Up point chapter, ’ve outlined basics sampling theory statisticians rely make guesses population parameters basis sample data. discussion illustrates, one reasons need sampling theory every data set leaves us uncertainty, estimates never going perfectly accurate. thing missing discussion attempt quantify amount uncertainty attaches estimate. ’s enough able guess , say, mean IQ undergraduate psychology students \\(115\\) (yes, just made number ). also want able say something expresses degree certainty guess. example, nice able say \\(95\\%\\) chance true mean lies \\(109\\) \\(121\\). name confidence interval mean.Armed understanding sampling distributions, constructing confidence interval mean actually pretty easy. ’s works. Suppose true population mean µ standard deviation σ. ’ve just finished running study N participants, mean IQ among participants \\(\\bar{X}\\). know discussion central limit theorem sampling distribution mean approximately normal. also know discussion normal distribution \\(95\\%\\) chance normally-distributed quantity fall within two standard deviations true mean.precise, correct answer \\(95\\%\\) chance normally distributed quantity fall within \\(1.96\\) standard deviations true mean. Next, recall standard deviation sampling distribution referred standard error, standard error mean written SEM. put pieces together, learn 95% probability sample mean \\(\\bar{X}\\) actually observed lies within \\(1.96\\) standard errors population mean.course, ’s nothing special number \\(1.96\\). just happens multiplier need use want \\(95\\%\\) confidence interval. ’d wanted \\(70\\%\\) confidence interval, used \\(1.04\\) magic number rather \\(1.96\\).[Additional technical detail 59]course, ’s nothing special number 1.96. just happens multiplier need use want 95% confidence interval. ’d wanted 70% confidence interval, used 1.04 magic number rather 1.96.","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"interpreting-a-confidence-interval","chapter":"8 Estimating unkown quantities from a sample","heading":"8.5.1 Interpreting a confidence interval","text":"hardest thing confidence intervals understanding mean. Whenever people first encounter confidence intervals, first instinct almost always say “95% probability true mean lies inside confidence interval”. ’s simple seems capture common sense idea means say “95% confident”. Unfortunately, ’s quite right. intuitive definition relies heavily personal beliefs value population mean. say 95% confident beliefs. everyday life ’s perfectly okay, remember back section probability mean?, ’ll notice talking personal belief confidence Bayesian idea. However, confidence intervals Bayesian tools. Like everything else chapter, confidence intervals frequentist tools, going use frequentist methods ’s appropriate attach Bayesian interpretation . use frequentist methods, must adopt frequentist interpretations! Okay, ’s right answer, ? Remember said frequentist probability. way allowed make “probability statements” talk sequence events, count frequencies different kinds events. perspective, interpretation 95% confidence interval must something replication. Specifically, replicated experiment computed 95% confidence interval replication, 95% intervals contain true mean. generally, 95% confidence intervals constructed using procedure contain true population mean. idea illustrated Figure 8.12, shows 50 confidence intervals constructed “measure 10 IQ scores” experiment (top panel) another 50 confidence intervals “measure 25 IQ scores” experiment (bottom panel). bit fortuitously, across 100 replications simulated, turned exactly 95 contained true mean. critical difference Bayesian claim makes probability statement population mean (.e., refers uncertainty population mean), allowed frequentist interpretation probability can’t “replicate” population! frequentist claim, population mean fixed probabilistic claims can made . Confidence intervals, however, repeatable can replicate experiments. Therefore frequentist allowed talk probability confidence interval (random variable) contains true mean, allowed talk probability true population mean (repeatable event) falls within confidence interval know seems little pedantic, matter. matters difference interpretation leads difference mathematics. Bayesian alternative confidence intervals, known credible intervals. situations credible intervals quite similar confidence intervals, cases drastically different. promised, though, ’ll talk Bayesian perspective chapter Bayesian statistics.\nFigure 8.12: 95% confidence intervals. top (panel ) shows 50 simulated replications experiment measure IQs 10 people. dot marks location sample mean line shows 95% confidence interval. total 47 50 confidence intervals contain true mean (.e., 100), three intervals marked asterisks . lower graph (panel b) shows similar simulation, time simulate replications experiment measures IQs 25 people\n","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"calculating-confidence-intervals-in-jamovi","chapter":"8 Estimating unkown quantities from a sample","heading":"8.5.2 Calculating confidence intervals in jamovi","text":"jamovi include simple way calculate confidence intervals mean part ‘Descriptives’ functionality. ‘Descriptives’-‘Statistics’ check box ‘Std. error Mean’ ‘Confidence interval mean’, can use find 95% confidence interval (default). , example, load IQsim.omv file, check ‘Confidence interval mean’, can see confidence interval associated simulated mean IQ: Lower 95% CI = 99.39 Upper 95% CI = 99.97 , simulated large sample data N=10,000, mean IQ score 99.68 95% CI 99.39 99.97.comes plotting confidence intervals jamovi, can specify mean included option box plot. Moreover, get onto learning specific statistical tests, example chapter Comparing several means (one-way ANOVA), see can also plot confidence intervals part data analysis. ’s pretty cool, ’ll show later .","code":""},{"path":"estimating-unkown-quantities-from-a-sample.html","id":"summary-6","chapter":"8 Estimating unkown quantities from a sample","heading":"8.6 Summary","text":"chapter ’ve covered two main topics. first half chapter talks sampling theory, second half talks can use sampling theory construct estimates population parameters. section breakdown looks like :Basic ideas Samples, populations samplingStatistical theory sampling: law large numbers Sampling distributions central limit theoremEstimating population parameters. Means standard deviationsEstimating confidence intervalAs always, ’s lot topics related sampling estimation aren’t covered chapter, introductory psychology class fairly comprehensive think. applied researchers won’t need much theory . One big question haven’t touched chapter don’t simple random sample. lot statistical theory can draw handle situation, ’s well beyond scope book.","code":""},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"9 Hypothesis testing","heading":"9 Hypothesis testing","text":"process induction process assuming simplest law can made harmonize experience. process, however, logical foundation psychological one. clear grounds believing simplest course events really happen. hypothesis sun rise tomorrow: means know whether rise.\n– Ludwig Wittgenstein 60In last chapter discussed ideas behind estimation, one two “big ideas” inferential statistics. ’s now time turn attention big idea, hypothesis testing. abstract form, hypothesis testing really simple idea. researcher theory world wants determine whether data actually support theory. However, details messy people find theory hypothesis testing frustrating part statistics. structure chapter follows. First, ’ll describe hypothesis testing works fair amount detail, using simple running example show hypothesis test “built”. ’ll try avoid dogmatic , focus instead underlying logic testing procedure.61 Afterwards, ’ll spend bit time talking various dogmas, rules heresies surround theory hypothesis testing.","code":""},{"path":"hypothesis-testing.html","id":"a-menagerie-of-hypotheses","chapter":"9 Hypothesis testing","heading":"9.1 A menagerie of hypotheses","text":"Eventually succumb madness. , day arrive ’m finally promoted full professor. Safely ensconced ivory tower, happily protected tenure, finally able take leave senses (speak) indulge thoroughly unproductive line psychological research, search extrasensory perception (ESP).62Let’s suppose glorious day come. first study simple one seek test whether clairvoyance exists. participant sits table shown card experimenter. card black one side white . experimenter takes card away places table adjacent room. card placed black side white side completely random, randomisation occurring experimenter left room participant. second experimenter comes asks participant side card now facing upwards. ’s purely one-shot experiment. person sees one card gives one answer, stage participant actually contact someone knows right answer. data set, therefore, simple. asked question N people number \\(X\\) people given correct response. make things concrete, let’s suppose tested \\(N = 100\\) people \\(X = 62\\) got answer right. surprisingly large number, sure, large enough feel safe claiming ’ve found evidence ESP? situation hypothesis testing comes useful. However, talk test hypotheses, need clear mean hypotheses.","code":""},{"path":"hypothesis-testing.html","id":"research-hypotheses-versus-statistical-hypotheses","chapter":"9 Hypothesis testing","heading":"9.1.1 Research hypotheses versus statistical hypotheses","text":"first distinction need keep clear mind research hypotheses statistical hypotheses. ESP study overall scientific goal demonstrate clairvoyance exists. situation clear research goal: hoping discover evidence ESP. situations might actually lot neutral , might say research goal determine whether clairvoyance exists. Regardless want portray , basic point ’m trying convey research hypothesis involves making substantive, testable scientific claim. psychologist research hypotheses fundamentally psychological constructs. following count research hypotheses:Listening music reduces ability pay attention things. claim causal relationship two psychologically meaningful concepts (listening music paying attention things), ’s perfectly reasonable research hypothesis.Intelligence related personality. Like last one, relational claim two psychological constructs (intelligence personality), claim weaker: correlational causalIntelligence speed information processing. hypothesis quite different character. ’s actually relational claim . ’s ontological claim fundamental character intelligence (’m pretty sure one actually. ’s usually easier think construct experiments test research hypotheses form “\\(X\\) affect \\(Y\\)?” address claims like “\\(X\\)?” practice usually happens find ways testing relational claims follow ontological ones. instance, believe intelligence speed information processing brain, experiments often involve looking relationships measures intelligence measures speed. consequence everyday research questions tend relational nature, ’re almost always motivated deeper ontological questions state nature.Notice practice, research hypotheses overlap lot. ultimate goal ESP experiment might test ontological claim like “ESP exists”, might operationally restrict narrower hypothesis like “people can ‘see’ objects clairvoyant fashion”. said, things really don’t count proper research hypotheses meaningful sense:Love battlefield. vague testable. Whilst ’s okay research hypothesis degree vagueness , possible operationalise theoretical ideas. Maybe ’m just creative enough see , can’t see can converted concrete research design. ’s true isn’t scientific research hypothesis, ’s pop song. doesn’t mean ’s interesting. lot deep questions humans fall category. Maybe one day science able construct testable theories love, test see God exists, . right now can’t, wouldn’t bet ever seeing satisfying scientific approach either.first rule tautology club first rule tautology club. substantive claim kind. ’s true definition. conceivable state nature possibly inconsistent claim. say unfalsifiable hypothesis, outside domain science. Whatever else science claims must possibility wrong.people experiment say “yes” “”. one fails research hypothesis ’s claim data set, psychology (unless course actual research question whether people kind “yes” bias!). Actually, hypothesis starting sound like statistical hypothesis research hypothesis.can see, research hypotheses can somewhat messy times ultimately scientific claims. Statistical hypotheses neither two things. Statistical hypotheses must mathematically precise must correspond specific claims characteristics data generating mechanism (.e., “population”). Even , intent statistical hypotheses bear clear relationship substantive research hypotheses care ! instance, ESP study research hypothesis people able see walls whatever. want “map” onto statement data generated. let’s think statement . quantity ’m interested within experiment \\(P(correct)\\), true--unknown probability participants experiment answer question correctly. Let’s use Greek letter \\(\\theta\\) (theta) refer probability. four different statistical hypotheses:ESP doesn’t exist experiment well designed participants just guessing. expect get right half time statistical hypothesis true probability choosing correctly \\(\\theta=0.5\\) .Alternatively, suppose ESP exist participants can see card. ’s true people perform better chance statistical hypothesis \\(\\theta > 0.5\\).third possibility ESP exist, colours reversed people don’t realise (okay, ’s wacky, never know). ’s works ’d expect people’s performance chance. correspond statistical hypothesis \\(\\theta < 0.5\\).Finally, suppose ESP exists idea whether people seeing right colour wrong one. case claim make data probability making correct answer equal 0.5. corresponds statistical hypothesis \\(\\theta \\neq 0.5\\).legitimate examples statistical hypothesis statements population parameter meaningfully related experiment.discussion makes clear, hope, attempting construct statistical hypothesis test researcher actually two quite distinct hypotheses consider. First, research hypothesis (claim psychology), corresponds statistical hypothesis (claim data generating population). ESP example might shown Table 9.1.Table 9.1:  Research statistical hypothesesAnd key thing recognise . statistical hypothesis test test statistical hypothesis, research hypothesis. study badly designed link research hypothesis statistical hypothesis broken. give silly example, suppose ESP study conducted situation participant can actually see card reflected window. happens able find strong evidence \\(\\theta \\neq 0.5\\), tell us nothing whether “ESP exists”.","code":""},{"path":"hypothesis-testing.html","id":"null-hypotheses-and-alternative-hypotheses","chapter":"9 Hypothesis testing","heading":"9.1.2 Null hypotheses and alternative hypotheses","text":"far, good. research hypothesis corresponds want believe world, can map onto statistical hypothesis corresponds want believe data generated. ’s point things get somewhat counter-intuitive lot people. ’m invent new statistical hypothesis (“null” hypothesis, \\(H_0\\) ) corresponds exact opposite want believe, focus exclusively almost neglect thing ’m actually interested (now called “alternative” hypothesis, H1). ESP example, null hypothesis \\(\\theta = 0.5\\), since ’s ’d expect ESP didn’t exist. hope, course, ESP totally real alternative null hypothesis \\(\\theta \\neq 0.5\\). essence, ’re dividing possible values \\(\\theta\\) two groups: values really hope aren’t true (null), values ’d happy turn right (alternative). done , important thing recognise goal hypothesis test show alternative hypothesis (probably) true. goal show null hypothesis (probably) false. people find pretty weird.best way think , experience, imagine hypothesis test criminal trial63, trial null hypothesis. null hypothesis defendant, researcher prosecutor, statistical test judge. Just like criminal trial, presumption innocence. null hypothesis deemed true unless , researcher, can prove beyond reasonable doubt false. free design experiment however like (within reason, obviously!) goal maximise chance data yield conviction crime false. catch statistical test sets rules trial rules designed protect null hypothesis, specifically ensure null hypothesis actually true chances false conviction guaranteed low. pretty important. , null hypothesis doesn’t get lawyer, given researcher trying desperately prove false someone protect .","code":""},{"path":"hypothesis-testing.html","id":"two-types-of-errors","chapter":"9 Hypothesis testing","heading":"9.2 Two types of errors","text":"going details statistical test constructed ’s useful understand philosophy behind . hinted pointing similarity null hypothesis test criminal trial, now explicit. Ideally, like construct test never make errors. Unfortunately, since world messy, never possible. Sometimes ’re just really unlucky. instance, suppose flip coin 10 times row comes heads 10 times. feels like strong evidence conclusion coin biased, course ’s 1 1024 chance happen even coin totally fair. words, real life always accept ’s chance made mistake. consequence goal behind statistical hypothesis testing eliminate errors, minimise .point, need bit precise mean “errors”. First, let’s state obvious. either case null hypothesis true false, test either retain null hypothesis reject .64 , Table 9.2 illustrates, run test make choice one four things might happened:Table 9.2:  Null hypothesis statistical testing (NHST)consequence actually two different types error . reject null hypothesis actually true made type error. hand, retain null hypothesis fact false made type II error.Remember said statistical testing kind like criminal trial? Well, meant . criminal trial requires establish “beyond reasonable doubt” defendant . evidential rules (theory, least) designed ensure ’s (almost) chance wrongfully convicting innocent defendant. trial designed protect rights defendant, English jurist William Blackstone famously said, “better ten guilty persons escape one innocent suffer.” words, criminal trial doesn’t treat two types error way. Punishing innocent deemed much worse letting guilty go free. statistical test pretty much . single important design principle test control probability type error, keep fixed probability. probability, denoted \\(\\alpha\\), called significance level test. ’ll say , central whole set-, hypothesis test said significance level \\(\\alpha\\) type error rate larger \\(\\alpha\\)., type II error rate? Well, ’d also like keep control , denote probability \\(\\beta\\). However, ’s much common refer power test, probability reject null hypothesis really false, \\(1 - \\beta\\). help keep straight, ’s table relevant numbers added (Table 9.3):Table 9.3:  Null hypothesis statistical testing (NHST) - additional detailA “powerful” hypothesis test one small value \\(\\beta\\), still keeping \\(\\alpha\\) fixed (small) desired level. convention, scientists make use three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) \\(.001\\). Notice asymmetry ; tests designed ensure \\(\\alpha\\) level kept small ’s corresponding guarantee regarding \\(\\beta\\). ’d certainly like type II error rate small try design tests keep small, typically secondary overwhelming need control type error rate. Blackstone might said statistician, “better retain 10 false null hypotheses reject single true one”. honest, don’t know agree philosophy. situations think makes sense, situations think doesn’t, ’s neither . ’s tests built.","code":""},{"path":"hypothesis-testing.html","id":"test-statistics-and-sampling-distributions","chapter":"9 Hypothesis testing","heading":"9.3 Test statistics and sampling distributions","text":"point need start talking specifics hypothesis test constructed. end, let’s return ESP example. Let’s ignore actual data obtained, moment, think structure experiment. Regardless actual numbers , form data \\(X\\) \\(N\\) people correctly identified colour hidden card. Moreover, let’s suppose moment null hypothesis really true, ESP doesn’t exist true probability anyone picks correct colour exactly \\(\\theta = 0.5\\). expect data look like? Well, obviously ’d expect proportion people make correct response pretty close \\(50\\%\\). , phrase mathematical terms, ’d say \\(\\frac{X}{N}\\) approximately \\(0.5\\). course, wouldn’t expect fraction exactly \\(0.5\\). , example, tested \\(N = 100\\) people \\(X = 53\\) got question right, ’d probably forced concede data quite consistent null hypothesis. hand, \\(X = 99\\) participants got question right ’d feel pretty confident null hypothesis wrong. Similarly, \\(X = 3\\) people got answer right ’d similarly confident null wrong. Let’s little technical . quantity \\(X\\) can calculate looking data. looking value \\(X\\) make decision whether believe null hypothesis correct, reject null hypothesis favour alternative. name thing calculate guide choices test statistic.chosen test statistic, next step state precisely values test statistic cause reject null hypothesis, values cause us keep . order need determine sampling distribution test statistic null hypothesis actually true (talked sampling distributions earlier section Sampling distribution mean. need ? distribution tells us exactly values X null hypothesis lead us expect. , therefore, can use distribution tool assessing closely null hypothesis agrees data.actually determine sampling distribution test statistic? lot hypothesis tests step actually quite complicated, later book ’ll see slightly evasive tests (don’t even understand ). However, sometimes ’s easy. , fortunately us, ESP example provides us one easiest cases. population parameter \\(\\theta\\) just overall probability people respond correctly asked question, test statistic \\(X\\) count number people sample size N. ’ve seen distribution like , section [Binomial distribution], ’s exactly binomial distribution describes! , use notation terminology introduced section, say null hypothesis predicts \\(X\\) binomially distributed, written\\[X \\sim Binomial(\\theta,N)\\]Since null hypothesis states \\(\\theta = 0.5\\) experiment \\(N = 100\\) people, sampling distribution need. sampling distribution plotted Figure 9.1. surprises really, null hypothesis says \\(X = 50\\) likely outcome, says ’re almost certain see somewhere \\(40\\) \\(60\\) correct responses.\nFigure 9.1: sampling distribution test statistic X null hypothesis true. ESP scenario binomial distribution. surprisingly, since null hypothesis says probability correct response \\(\\theta = .5\\), sampling distribution says likely value 50 (100) correct responses. probability mass lies 40 60\n","code":""},{"path":"hypothesis-testing.html","id":"making-decisions","chapter":"9 Hypothesis testing","heading":"9.4 Making decisions","text":"Okay, ’re close finished. ’ve constructed test statistic \\((X)\\) chose test statistic way ’re pretty confident \\(X\\) close \\(\\frac{N}{2}\\) retain null, reject . question remains . Exactly values test statistic associate null hypothesis, exactly values go alternative hypothesis? ESP study, example, ’ve observed value \\(X = 62\\). decision make? choose believe null hypothesis alternative hypothesis?","code":""},{"path":"hypothesis-testing.html","id":"critical-regions-and-critical-values","chapter":"9 Hypothesis testing","heading":"9.4.1 Critical regions and critical values","text":"answer question need introduce concept critical region test statistic X. critical region test corresponds values X lead us reject null hypothesis (critical region also sometimes called rejection region). find critical region? Well, let’s consider know:\\(X\\) big small order reject null hypothesisIf null hypothesis true, sampling distribution \\(X\\) \\(Binomial(0.5, N)\\)\\(\\alpha = .05\\), critical region must cover 5% sampling distribution.’s important make sure understand last point. critical region corresponds values \\(X\\) reject null hypothesis, sampling distribution question describes probability obtain particular value \\(X\\) null hypothesis actually true. Now, let’s suppose chose critical region covers \\(20\\%\\) sampling distribution, suppose null hypothesis actually true. probability incorrectly rejecting null? answer course \\(20\\%\\). , therefore, built test α level \\(0.2\\). want \\(\\alpha = .05\\), critical region allowed cover 5% sampling distribution test statistic.turns three things uniquely solve problem. critical region consists extreme values, known tails distribution. illustrated Figure 9.2. want \\(\\alpha = .05\\) critical regions correspond \\(X \\leq 40\\) \\(X \\geq 60\\).65 , number people saying “true” 41 59, retain null hypothesis. number \\(0\\) \\(40\\), \\(60\\) \\(100\\), reject null hypothesis. numbers \\(40\\) \\(60\\) often referred critical values since define edges critical region\nFigure 9.2: critical region associated hypothesis test ESP study, hypothesis test significance level \\(\\alpha = .05\\). plot shows sampling distribution \\(X\\) null hypothesis (.e., Figure 9.1). grey bars correspond values \\(X\\) retain null hypothesis. blue (darker shaded) bars show critical region, values \\(X\\) reject null. alternative hypothesis two sided (.e., allows \\(\\theta < .5\\) \\(\\theta > .5\\), critical region covers tails distribution. ensure \\(\\alpha\\) level \\(.05\\), need ensure two regions encompasses \\(2.5\\%\\) sampling distribution\npoint, hypothesis test essentially complete:choose α level (e.g., \\(\\alpha = .05\\));Come test statistic (e.g., \\(X\\)) good job (meaningful sense) comparing \\(H_0\\) \\(H_1\\);Figure sampling distribution test statistic assumption null hypothesis true (case, binomial); thenCalculate critical region produces appropriate α level (0-40 60-100).now calculate value test statistic real data (e.g., X = 62) compare critical values make decision. Since 62 greater critical value 60 reject null hypothesis. , phrase slightly differently, say test produced statistically significant result.","code":""},{"path":"hypothesis-testing.html","id":"a-note-on-statistical-significance","chapter":"9 Hypothesis testing","heading":"9.4.2 A note on statistical “significance”","text":"Like occult techniques divination, statistical method private jargon deliberately contrived obscure methods non-practitioners.\n– Attributed G. O. Ashley 66A brief digression order point, regarding word “significant”. concept statistical significance actually simple one, unfortunate name. data allow us reject null hypothesis, say “result statistically significant”, often shortened “result significant”. terminology rather old dates back time “significant” just meant something like “indicated”, rather modern meaning much closer “important”. result, lot modern readers get confused start learning statistics think “significant result” must important one. doesn’t mean . “statistically significant” means data allowed us reject null hypothesis. Whether result actually important real world different question, depends sorts things.","code":""},{"path":"hypothesis-testing.html","id":"the-difference-between-one-sided-and-two-sided-tests","chapter":"9 Hypothesis testing","heading":"9.4.3 The difference between one sided and two sided tests","text":"’s one thing want point hypothesis test ’ve just constructed. take moment think statistical hypotheses ’ve using, \\[H_0: \\theta=0.5\\] \\[H_1:\n\\theta \\neq 0.5\\] notice alternative hypothesis covers possibility \\(\\theta < .5\\) possibility \\(\\theta \\> .5.\\) makes sense really think ESP produce either better-thanchance performance worse--chance performance (people think ). statistical language example two-sided test. ’s called alternative hypothesis covers area “sides” null hypothesis, consequence critical region test covers tails sampling distribution (2.5% either side α = .05), illustrated earlier Figure 9.2. However, ’s possibility. might willing believe ESP produces better chance performance. , alternative hypothesis covers possibility θ ą .5, consequence null hypothesis now becomes \\[H_0: \\theta \\leq\n0.5\\] \\[H_1: \\theta \\gt 0.5\\] happens, ’s called one-sided test critical region covers one tail sampling distribution. illustrated Figure 9.3.\nFigure 9.3: critical region one sided test. case, alternative hypothesis \\(\\theta \\geq .5\\) reject null hypothesis large values \\(X\\). consequence, critical region covers upper tail sampling distribution, specifically upper \\(5\\%\\) distribution. Contrast two-sided version Figure 9.2\n","code":""},{"path":"hypothesis-testing.html","id":"the-p-value-of-a-test","chapter":"9 Hypothesis testing","heading":"9.5 The p value of a test","text":"one sense, hypothesis test complete. ’ve constructed test statistic, figured sampling distribution null hypothesis true, constructed critical region test. Nevertheless, ’ve actually omitted important number , p value. topic now turn. two somewhat different ways interpreting p value, one proposed Sir Ronald Fisher Jerzy Neyman. versions legitimate, though reflect different ways thinking hypothesis tests. introductory textbooks tend give Fisher’s version , think ’s bit shame. mind, Neyman’s version cleaner actually better reflects logic null hypothesis test. might disagree though, ’ve included . ’ll start Neyman’s version.","code":""},{"path":"hypothesis-testing.html","id":"a-softer-view-of-decision-making","chapter":"9 Hypothesis testing","heading":"9.5.1 A softer view of decision making","text":"One problem hypothesis testing procedure ’ve described makes distinction result “barely significant” “highly significant”. instance, ESP study data obtained just fell inside critical region, get significant effect pretty near thing. contrast, suppose ’d run study \\(X = 97\\) \\(N = 100\\) participants got answer right. obviously significant much larger margin, ’s really ambiguity . procedure already described makes distinction two. adopt standard convention allowing \\(\\alpha = .05\\) acceptable Type error rate, significant results.p value comes handy. understand works, let’s suppose ran lots hypothesis tests data set, different value α case. original ESP data ’d get something like Table 9.4.Table 9.4:  Rejecting NH different levels $\\alpha$test ESP data (\\(X = 62\\) successes \\(N = 100\\) observations), using \\(\\alpha\\) levels \\(.03\\) , ’d always find rejecting null hypothesis. \\(\\alpha\\) levels \\(.02\\) always end retaining null hypothesis. Therefore, somewhere \\(.02\\) \\(.03\\) must smallest value \\(\\alpha\\) allow us reject null hypothesis data. \\(p\\) value. turns ESP data \\(p = .021\\). short, \\(p\\) defined smallest Type error rate (\\(\\alpha\\)) willing tolerate want reject null hypothesis.turns p describes error rate find intolerable, must retain null. ’re comfortable error rate equal \\(p\\), ’s okay reject null hypothesis favour preferred alternative.effect, \\(p\\) summary possible hypothesis tests run, taken across possible α values. consequence effect “softening” decision process. tests p ď α rejected null hypothesis, whereas tests p ą α retained null. ESP study obtained \\(X = 62\\) consequence ’ve ended \\(p = .021\\). error rate tolerate \\(2.1\\%\\). contrast, suppose experiment yielded \\(X = 97\\). happens p value now? time ’s shrunk \\(p = 1.36\\) x \\(10^{-25}\\), tiny, tiny67 Type error rate. second case able reject null hypothesis lot confidence, “willing” tolerate type error rate \\(1\\) \\(10\\) trillion trillion order justify decision reject.","code":""},{"path":"hypothesis-testing.html","id":"the-probability-of-extreme-data","chapter":"9 Hypothesis testing","heading":"9.5.2 The probability of extreme data","text":"second definition p-value comes Sir Ronald Fisher, ’s actually one tend see introductory statistics textbooks. Notice , constructed critical region, corresponded tails (.e., extreme values) sampling distribution? ’s coincidence, almost “good” tests characteristic (good sense minimising type II error rate, \\(\\beta\\)). reason good critical region almost always corresponds values test statistic least likely observed null hypothesis true. rule true, can define p-value probability observed test statistic least extreme one actually get. words, data extremely implausible according null hypothesis, null hypothesis probably wrong.","code":""},{"path":"hypothesis-testing.html","id":"a-common-mistake","chapter":"9 Hypothesis testing","heading":"9.5.3 A common mistake","text":"Okay, can see two rather different legitimate ways interpret \\(p\\) value, one based Neyman’s approach hypothesis testing based Fisher’s. Unfortunately, third explanation people sometimes give, especially ’re first learning statistics, absolutely completely wrong. mistaken approach refer \\(p\\) value “probability null hypothesis true”. ’s intuitively appealing way think, ’s wrong two key respects. First, null hypothesis testing frequentist tool frequentist approach probability allow assign probabilities null hypothesis. According view probability, null hypothesis either true , “\\(5\\%\\) chance” true. Second, even within Bayesian approach, let assign probabilities hypotheses, p value correspond probability null true. interpretation entirely inconsistent mathematics p value calculated. Put bluntly, despite intuitive appeal thinking way, justification interpreting \\(p\\) value way. Never .","code":""},{"path":"hypothesis-testing.html","id":"reporting-the-results-of-a-hypothesis-test","chapter":"9 Hypothesis testing","heading":"9.6 Reporting the results of a hypothesis test","text":"writing results hypothesis test ’s usually several pieces information need report, varies fair bit test test. Throughout rest book ’ll spend little time talking report results different tests (see section particularly detailed example: [report results test]), can get feel ’s usually done. However, regardless test ’re , one thing always say something \\(p\\) value whether outcome significant.fact unsurprising, ’s whole point test. might surprising fact contention exactly ’re supposed . Leaving aside people completely disagree entire framework underpinning null hypothesis testing, ’s certain amount tension exists regarding whether report exact \\(p\\) value obtained, state \\(p < \\alpha\\) significance level chose advance (e.g., \\(p < .05\\)).","code":""},{"path":"hypothesis-testing.html","id":"the-issue","chapter":"9 Hypothesis testing","heading":"9.6.1 The issue","text":"see issue, key thing recognise p values terribly convenient. practice, fact can compute p value means don’t actually specify \\(\\alpha\\) level order run test. Instead, can calculate p value interpret directly. get \\(p = .062\\), means ’d willing tolerate Type error rate \\(6.2\\%\\) justify rejecting null. personally find \\(6.2\\%\\) intolerable retain null. Therefore, argument goes, don’t just report actual \\(p\\) value let reader make minds acceptable Type error rate ? approach big advantage “softening” decision making process. fact, accept Neyman definition p value, ’s whole point p value. longer fixed significance level \\(\\alpha = .05\\) bright line separating “accept” “reject” decisions, removes rather pathological problem forced treat \\(p = .051\\) fundamentally different way \\(p = .049\\).flexibility advantage disadvantage \\(p\\) value. reason lot people don’t like idea reporting exact \\(p\\) value gives researcher bit much freedom. particular, lets change mind error tolerance ’re willing put look data. instance, consider ESP experiment. Suppose ran test ended \\(p\\) value \\(.09\\). accept reject? Now, honest, haven’t yet bothered think level Type error ’m “really” willing accept. don’t opinion topic. opinion whether ESP exists, definitely opinion whether research published reputable scientific journal. amazingly, now ’ve looked data ’m starting think \\(9\\%\\) error rate isn’t bad, especially compared annoying admit world experiment failed. , avoid looking like just made fact, now say \\(\\alpha\\) .1, argument \\(10\\%\\) type error rate isn’t bad level test significant! win.words, worry might best intentions, honest people, temptation just “shade” things little bit really, really strong. anyone ever run experiment can attest, ’s long difficult process often get attached hypotheses. ’s hard let go admit experiment didn’t find wanted find. ’s danger . use “raw” p-value, people start interpreting data terms want believe, data actually saying , allow , even bothering science ? let everyone believe whatever like anything, regardless facts ? Okay, ’s bit extreme, ’s worry comes . According view, really must specify \\(\\alpha\\) value advance report whether test significant . ’s way keep honestTable 9.5:  Typical translations p value levels","code":""},{"path":"hypothesis-testing.html","id":"two-proposed-solutions","chapter":"9 Hypothesis testing","heading":"9.6.2 Two proposed solutions","text":"practice, ’s pretty rare researcher specify single α level ahead time. Instead, convention scientists rely three standard significance levels: \\(.05\\), \\(.01\\) \\(.001\\). reporting results, indicate () significance levels allow reject null hypothesis. summarised Table 9.4. allows us soften decision rule little bit, since \\(p < .01\\) implies data meet stronger evidential standard \\(p < .05\\) . Nevertheless, since levels fixed advance convention, prevent people choosing α level looking dataNevertheless, quite lot people still prefer report exact p values. many people, advantage allowing reader make mind interpret p = .06 outweighs disadvantages. practice, however, even among researchers prefer exact p values quite common just write \\(p < .001\\) instead reporting exact value small p. part lot software doesn’t actually print p value ’s small (e.g., SPSS just writes \\(p = .000\\) whenever \\(p < .001\\)), part small p value can kind misleading. human mind sees number like .0000000001 ’s hard suppress gut feeling evidence favour alternative hypothesis near certainty. practice however, usually wrong. Life big, messy, complicated thing, every statistical test ever invented relies simplifications, approximations assumptions. consequence, ’s probably reasonable walk away statistical analysis feeling confidence stronger \\(p < .001\\) implies. words, \\(p < .001\\) really code “far test concerned, evidence overwhelming.”light , might wondering exactly . ’s fair bit contradictory advice topic, people arguing report exact p value, people arguing use tiered approach illustrated Table 9.1. result, best advice can give suggest look papers/reports written field see convention seems . doesn’t seem consistent pattern, use whichever method prefer.","code":""},{"path":"hypothesis-testing.html","id":"running-the-hypothesis-test-in-practice","chapter":"9 Hypothesis testing","heading":"9.7 Running the hypothesis test in practice","text":"point might wondering “real” hypothesis test, just toy example made . ’s real. previous discussion built test first principles, thinking simplest possible problem might ever encounter real life. However, test already exists. ’s called binomial test, ’s implemented jamovi one statistical analyses available hit ‘Frequencies’ button. test null hypothesis response probability one-half \\(p = .5\\),68 using data \\(x =62\\) \\(n = 100\\) people made correct response, available binomialtest.omv data file, get results shown Figure 9.4.\nFigure 9.4: Binomial test analysis results jamovi\nRight now, output looks pretty unfamiliar , can see ’s telling less right things. Specifically, p-value \\(0.02\\) less usual choice \\(\\alpha = .05\\), can reject null. ’ll talk lot read sort output go along, ’ll hopefully find quite easy read understand.","code":""},{"path":"hypothesis-testing.html","id":"effect-size-sample-size-and-power","chapter":"9 Hypothesis testing","heading":"9.8 Effect size, sample size and power","text":"previous sections ’ve emphasised fact major design principle behind statistical hypothesis testing try control Type error rate. fix \\(\\alpha = .05\\) attempting ensure \\(5\\%\\) true null hypotheses incorrectly rejected. However, doesn’t mean don’t care Type II errors. fact, researcher’s perspective, error failing reject null actually false extremely annoying one. mind, secondary goal hypothesis testing try minimise \\(\\beta\\), Type II error rate, although don’t usually talk terms minimising Type II errors. Instead, talk maximising power test. Since power defined \\(1 - \\beta\\), thing.","code":""},{"path":"hypothesis-testing.html","id":"the-power-function","chapter":"9 Hypothesis testing","heading":"9.8.1 The power function","text":"\nFigure 9.5: Sampling distribution alternative hypothesis population parameter value \\(\\theta = 0.55\\). reasonable proportion distribution lies rejection region\nLet’s take moment think Type II error actually . Type II error occurs alternative hypothesis true, nevertheless unable reject null hypothesis. Ideally, ’d able calculate single number \\(\\beta\\) tells us Type II error rate, way can set \\(\\alpha = .05\\) Type error rate. Unfortunately, lot trickier . see , notice ESP study alternative hypothesis actually corresponds lots possible values \\(\\theta\\). fact, alternative hypothesis corresponds every value \\(\\theta\\) except 0.5. Let’s suppose true probability someone choosing correct response 55% (.e., \\(\\theta = .55\\)). , true sampling distribution \\(X\\) one null hypothesis predicts, likely value \\(X\\) now \\(55\\) 100. , whole sampling distribution now shifted, shown Figure 9.5. critical regions, course, change. definition critical regions based null hypothesis predicts. ’re seeing figure fact null hypothesis wrong, much larger proportion sampling distribution distribution falls critical region. course ’s happen. probability rejecting null hypothesis larger null hypothesis actually false! However \\(\\theta = .55\\) possibility consistent alternative hypothesis. Let’s instead suppose true value \\(\\theta\\) actually \\(0.7\\). happens sampling distribution occurs? answer, shown Figure 9.6, almost entirety sampling distribution now moved critical region. Therefore, \\(\\theta = 0.7\\), probability us correctly rejecting null hypothesis (.e., power test) much larger \\(\\theta = 0.55\\). short, \\(\\theta = .55\\) \\(\\theta = .70\\) part alternative hypothesis, Type II error rate different.\nFigure 9.6: Sampling distribution alternative hypothesis population parameter value \\(\\theta = 0.70\\)). Almost distribution lies rejection region\nmeans power test (.e., \\(1 - \\beta\\)) depends true value \\(\\theta\\). illustrate , ’ve calculated expected probability rejecting null hypothesis values \\(\\theta\\), plotted Figure 9.7. plot describes usually called power function test. ’s nice summary good test , actually tells power \\((1 - \\beta\\)) possible values \\(\\theta\\). can see, true value \\(\\theta\\) close \\(0.5\\), power test drops sharply, away, power large.\nFigure 9.7: probability reject null hypothesis, plotted function true value \\(\\theta\\). Obviously, test powerful (greater chance correct rejection) true value \\(\\theta\\) different value null hypothesis specifies (.e., \\(\\theta = .5\\) ). Notice \\(\\theta\\) actually equal \\(.5\\) (plotted black dot), null hypothesis fact true rejecting null hypothesis instance Type error\n","code":""},{"path":"hypothesis-testing.html","id":"the-power-function-1","chapter":"9 Hypothesis testing","heading":"9.8.2 The power function","text":"Since models wrong scientist must alert importantly wrong. inappropriate concerned mice tigers abroad\n- George Box (Box 1976, p. 792)plot shown Figure 9.7 captures fairly basic point hypothesis testing. true state world different null hypothesis predicts power high, true state world similar null (identical) power test going low. Therefore, ’s useful able way quantifying “similar” true state world null hypothesis. statistic called measure effect size (e.g., Cohen (1988); Ellis (2010)). Effect size defined slightly differently different contexts (section just talks general terms) qualitative idea tries capture always (see e.g. Table 9.6). big difference true population parameters parameter values assumed null hypothesis? ESP example, let \\(\\theta_0 = 0.5\\) denote value assumed null hypothesis let \\(\\theta\\) denote true value, simple measure effect size something like difference true value null (.e., \\(\\theta - \\theta_0\\)), possibly just magnitude difference, \\(abs(\\theta - \\theta_0)\\).Table 9.6:  crude guide understanding relationship statistical significance effect sizes. Basically, significant result effect size pretty meaningless evidence even real. hand, significant effect effect size small pretty good chance result (although real) interesting. However, guide crude. depends lot exactly studying. Small effects can massive practical importance situations. take table seriously. rough guide best.calculate effect size? Let’s assume ’ve run experiment, collected data, gotten significant effect ran hypothesis test. Isn’t enough just say ’ve gotten significant effect? Surely ’s point hypothesis testing? Well, sort . Yes, point hypothesis test try demonstrate null hypothesis wrong, ’s hardly thing ’re interested . null hypothesis claimed \\(\\theta = .5\\) show ’s wrong, ’ve really told half story. Rejecting null hypothesis implies believe \\(\\theta \\neq .5\\), ’s big difference \\(\\theta = .51\\) \\(\\theta = .8\\). find \\(\\theta = .8\\), found null hypothesis wrong, appears wrong. hand, suppose ’ve successfully rejected null hypothesis, looks like true value \\(\\theta\\) .51 (possible large study). Sure, null hypothesis wrong ’s clear actually care effect size small. context ESP study might still care since demonstration real psychic powers actually pretty cool69, contexts \\(1\\%\\) difference usually isn’t interesting, even real difference. instance, suppose ’re looking differences high school exam scores males females turns female scores \\(1\\%\\) higher average males. ’ve got data thousands students difference almost certainly statistically significant, regardless small p value ’s just interesting. ’d hardly want go around proclaiming crisis boys education basis tiny difference ? ’s reason becoming standard (slowly, surely) report kind standard measure effect size along results hypothesis test. hypothesis test tells whether believe effect observed real (.e., just due chance), whereas effect size tells whether care.","code":""},{"path":"hypothesis-testing.html","id":"increasing-the-power-of-your-study","chapter":"9 Hypothesis testing","heading":"9.8.3 Increasing the power of your study","text":"surprisingly, scientists fairly obsessed maximising power experiments. want experiments work want maximise chance rejecting null hypothesis false (course usually want believe false!). ’ve seen, one factor influences power effect size. first thing can increase power increase effect size. practice, means want design study way effect size gets magnified. instance, ESP study might believe psychic powers work best quiet, darkened room fewer distractions cloud mind. Therefore try conduct experiments just environment. can strengthen people’s ESP abilities somehow true value \\(\\theta\\) go 70 therefore effect size larger. short, clever experimental design one way boost power, can alter effect size.Unfortunately, ’s often case even best experimental designs may small effect. Perhaps, example, ESP really exist even best conditions ’s weak. circumstances best bet increasing power increase sample size. general, observations available, likely can discriminate two hypotheses. ran ESP experiment 10 participants 7 correctly guessed colour hidden card wouldn’t terribly impressed. ran 10,000 participants, 7,000 got answer right, much likely think discovered something. words, power increases sample size. illustrated Figure 9.8, shows power test true parameter \\(\\theta = 0.7\\) sample sizes \\(N\\) \\(1\\) \\(100\\), ’m assuming null hypothesis predicts \\(\\theta_0 = 0.5\\).power important, whenever ’re contemplating running experiment pretty useful know much power ’re likely . ’s never possible know sure since can’t possibly know real effect size . However, ’s often (well, sometimes) possible guess big . , can guess sample size need! idea called power analysis, ’s feasible ’s helpful. can tell something whether enough time money able run experiment successfully. ’s increasingly common see people arguing power analysis required part experimental design, ’s worth knowing . don’t discuss power analysis book, however. partly boring reason partly substantive one. boring reason haven’t time write power analysis yet. substantive one ’m still little suspicious power analysis. Speaking researcher, rarely found position able one. ’s either case () experiment bit non-standard don’t know define effect size properly, (b) literally little idea effect size wouldn’t know interpret answers. , extensive conversations someone stats consulting living (wife, happens), can’t help notice practice time anyone ever asks power analysis ’s helping someone write grant application. words, time scientist ever seems want power analysis real life ’re forced bureaucratic process. ’s part anyone’s day day work. short, ’ve always view whilst power important concept, power analysis useful people make sound, except rare cases () someone figured calculate power actual experimental design (b) pretty good idea effect size likely .71 Maybe people better experiences , ’ve personally never situation () (b) true. Maybe ’ll convinced otherwise future, probably future version book include detailed discussion power analysis, now much ’m comfortable saying topic.\nFigure 9.8: power test plotted function sample size \\(N\\). case, true value \\(\\theta\\) 0.7 null hypothesis \\(\\theta = 0.5\\). Overall, larger \\(N\\) means greater power. (small zig-zags function occur odd interactions \\(\\theta\\), \\(\\alpha\\) fact binomial distribution discrete, doesn’t matter serious purpose)\n","code":""},{"path":"hypothesis-testing.html","id":"some-issues-to-consider","chapter":"9 Hypothesis testing","heading":"9.9 Some issues to consider","text":"’ve described chapter orthodox framework null hypothesis significance testing (NHST). Understanding NHST works absolute necessity dominant approach inferential statistics ever since came prominence early 20th century. ’s vast majority working scientists rely data analysis, even hate need know . However, approach without problems. number quirks framework, historical oddities came , theoretical disputes whether framework right, lot practical traps unwary. ’m going go lot detail topic, think ’s worth briefly discussing issues.","code":""},{"path":"hypothesis-testing.html","id":"neyman-versus-fisher","chapter":"9 Hypothesis testing","heading":"9.9.1 Neyman versus Fisher","text":"first thing aware orthodox NHST actually mash-two rather different approaches hypothesis testing, one proposed Sir Ronald Fisher proposed Jerzy Neyman (see Lehmann (2011) historical summary). history messy Fisher Neyman real people whose opinions changed time, point either offer “definitive statement” interpret work many decades later. said, ’s quick summary take two approaches .First, let’s talk Fisher’s approach. far can tell, Fisher assumed one hypothesis (null) want find null hypothesis inconsistent data. perspective, check see data “sufficiently unlikely” according null. fact, remember back earlier discussion, ’s Fisher defines p-value. According Fisher, null hypothesis provided poor account data safely reject . , since don’t hypotheses compare , ’s way “accepting alternative” don’t necessarily explicitly stated alternative. ’s less .contrast, Neyman thought point hypothesis testing guide action approach somewhat formal Fisher’s. view multiple things (accept null accept alternative) point test tell one data support. perspective, critical specify alternative hypothesis properly. don’t know alternative hypothesis , don’t know powerful test , even action makes sense. framework genuinely requires competition different hypotheses. Neyman, \\(p\\) value didn’t directly measure probability data (data extreme) null, abstract description “possible tests” telling accept null, “possible tests” telling accept alternative.can see, today odd mishmash two. talk null hypothesis alternative (Neyman), usually72 define \\(p\\) value terms exreme data (Fisher), still α values (Neyman). statistical tests explicitly specified alternatives (Neyman) others quite vague (Fisher). , according people least, ’re allowed talk accepting alternative (Fisher). ’s mess, hope least explains ’s mess.","code":""},{"path":"hypothesis-testing.html","id":"bayesians-versus-frequentists","chapter":"9 Hypothesis testing","heading":"9.9.2 Bayesians versus frequentists","text":"Earlier chapter quite emphatic fact interpret p value probability null hypothesis true. NHST fundamentally frequentist tool (see chapter [Probability testing]) allow assign probabilities hypotheses. null hypothesis either true . Bayesian approach statistics interprets probability degree belief, ’s totally okay say \\(10\\%\\) chance null hypothesis true. ’s just reflection degree confidence hypothesis. aren’t allowed within frequentist approach. Remember, ’re frequentist, probability can defined terms happens large number independent replications (.e., long run frequency). interpretation probability, talking “probability” null hypothesis true complete gibberish: null hypothesis either true false. ’s way can talk long run frequency statement. talk “probability null hypothesis” meaningless “colour freedom”. doesn’t one!importantly, isn’t purely ideological matter. decide Bayesian ’re okay making probability statements hypotheses, follow Bayesian rules calculating probabilities. ’ll talk chapter Bayesian statistics, now want point p value terrible approximation probability \\(H_0\\) true. want know probability null, p value ’re looking !","code":""},{"path":"hypothesis-testing.html","id":"traps","chapter":"9 Hypothesis testing","heading":"9.9.3 Traps","text":"can see, theory behind hypothesis testing mess, even now arguments statistics “” work. However, disagreements among statisticians real concern . real concern practical data analysis. “orthodox” approach null hypothesis significance testing many drawbacks, even unrepentant Bayesian like agree can useful used responsibly. time give sensible answers can use learn interesting things. Setting aside various ideologies historical confusions ’ve discussed, fact remains biggest danger statistics thoughtlessness. don’t mean stupidity, literally mean thoughtlessness. rush interpret result without spending time thinking test actually says data, checking whether ’s consistent ’ve interpreted . ’s biggest trap lies.give example , consider following example (see Gelman Stern (2006)). Suppose ’m running ESP study ’ve decided analyse data separately male participants female participants. male participants, \\(33\\) \\(50\\) guessed colour card correctly. significant effect (\\(p = .03\\)). female participants, \\(29\\) \\(50\\) guessed correctly. significant effect (\\(p = .32\\)). Upon observing , extremely tempting people start wondering difference males females terms psychic abilities. However, wrong. think , haven’t actually run test explicitly compares males females. done compare males chance (binomial test significant) compared females chance (binomial test non significant). want argue real difference males females, probably run test null hypothesis difference! can using different hypothesis test,73 turns evidence males females significantly different (\\(p = .54\\)). Now think ’s anything fundamentally different two groups? course . ’s happened data groups (male female) pretty borderline. pure chance one happened end magic side \\(p = .05\\) line, one didn’t. doesn’t actually imply males females different. mistake common always wary . difference significant -significant evidence real difference. want say ’s difference two groups, test difference!example just , example. ’ve singled ’s common one, bigger picture data analysis can tricky get right. Think want test, want test , whether answers test gives possibly make sense real world.","code":""},{"path":"hypothesis-testing.html","id":"summary-7","chapter":"9 Hypothesis testing","heading":"9.10 Summary","text":"Null hypothesis testing one ubiquitous elements statistical theory. vast majority scientific papers report results hypothesis test another. consequence almost impossible get science without least cursory understanding p-value means, making one important chapters book. usual, ’ll end chapter quick recap key ideas ’ve talked :Hypothesis testing. Research hypotheses statistical hypotheses. Null alternative hypotheses.Two types errors. Type Type II.Test statistics sampling distributions.Hypothesis testing Making decisionsThe p value test. p-values “soft” decisionsReporting results hypothesis testRunning hypothesis test practiceEffect size, sample size powerSome issues consider regarding hypothesis testingLater book, chapter Bayesian statistics, ’ll revisit theory null hypothesis tests Bayesian perspective introduce number new tools can use aren’t particularly fond orthodox approach. now, though, ’re done abstract statistical theory, can start discussing specific data analysis tools.","code":""},{"path":"categorical-data-analysis.html","id":"categorical-data-analysis","chapter":"10 Categorical data analysis","heading":"10 Categorical data analysis","text":"Now ’ve covered basic theory behind hypothesis testing ’s time start looking specific tests commonly used psychology. start? every textbook agrees start, ’m going start “\\(\\chi^2\\) tests” (chapter, pronounced “chi-square”74 “t-tests” next chapter (Comparing two means). tools frequently used scientific practice, whilst ’re powerful “regression” “analysis variance” cover later chapters, ’re much easier understand.term “categorical data” just another name “nominal scale data”. ’s nothing haven’t already discussed, ’s just context data analysis people tend use term “categorical data” rather “nominal scale data”. don’t know . case, categorical data analysis refers collection tools can use data nominal scale. However, lot different tools can used categorical data analysis, chapter covers common ones.","code":""},{"path":"categorical-data-analysis.html","id":"the-chi2-chi-square-goodness-of-fit-test","chapter":"10 Categorical data analysis","heading":"10.1 The \\(\\chi^2\\) (chi-square) goodness-of-fit test","text":"\\(\\chi^2\\) goodness--fit test one oldest hypothesis tests around. invented Karl Pearson around turn century (Pearson 1900), corrections made later Sir Ronald Fisher (Fisher 1922a). tests whether observed frequency distribution nominal variable matches expected frequency distribution. example, suppose group patients undergoing experimental treatment health assessed see whether condition improved, stayed worsened. goodness--fit test used determine whether numbers category - improved, change, worsened - match numbers expected given standard treatment option. Let’s think , psychology.","code":""},{"path":"categorical-data-analysis.html","id":"the-cards-data","chapter":"10 Categorical data analysis","heading":"10.1.1 The cards data","text":"years many studies showing humans find difficult simulate randomness. Try might “act” random, think terms patterns structure , asked “something random”, people actually anything random. consequence, study human randomness (non-randomness, case may ) opens lot deep psychological questions think world. mind, let’s consider simple study. Suppose asked people imagine shuffled deck cards, mentally pick one card imaginary deck “random”. ’ve chosen one card ask mentally select second one. choices ’re going look suit (hearts, clubs, spades diamonds) people chose. asking, say, \\(N = 200\\) people , ’d like look data figure whether cards people pretended select really random. data contained randomness.csv file , open jamovi take look spreadsheet view, see three variables. : id variable assigns unique identifier participant, two variables choice_1 choice_2 indicate card suits people chose.moment, let’s just focus first choice people made. ’ll use Frequency tables option ‘Exploration’ - ‘Descriptives’ count number times observed people choosing suit. get (Table 10.1):Table 10.1:  Number times suit chosenThat little frequency table quite helpful. Looking , ’s bit hint people might likely select hearts clubs, ’s completely obvious just looking whether ’s really true, just due chance. ’ll probably kind statistical analysis find , ’m going talk next section.Excellent. point , ’ll treat table data ’re looking analyse. However, since ’m going talk data mathematical terms (sorry!) might good idea clear notation . mathematical notation, shorten human-readable word “observed” letter \\(O\\), use subscripts denote position observation. second observation table written \\(O_2\\) maths. relationship English descriptions mathematical symbols illustrated Table 10.2.Table 10.2:  Relationship English descriptions mathematical symbolsHopefully ’s pretty clear. ’s also worth noting mathematicians prefer talk general rather specific things, ’ll also see notation \\(O_i\\), refers number observations fall within -th category (1, 2, 3 4). Finally, want refer set observed frequencies, statisticians group observed values vector 75, ’ll refer \\(O\\).\\[O = (O_1, O_2, O_3, O_4)\\], nothing new interesting. ’s just notation. say \\(O = (35, 51, 64, 50)\\) ’m describing table observed frequencies (.e., observed), ’m referring using mathematical notation.","code":""},{"path":"categorical-data-analysis.html","id":"the-null-hypothesis-and-the-alternative-hypothesis","chapter":"10 Categorical data analysis","heading":"10.1.2 The null hypothesis and the alternative hypothesis","text":"last section indicated, research hypothesis “people don’t choose cards randomly”. ’re going want now translate statistical hypotheses construct statistical test hypotheses. test ’m going describe Pearson’s \\(\\chi^2\\) (chi-square) goodness--fit test, often case begin carefully constructing null hypothesis. case, ’s pretty easy. First, let’s state null hypothesis words:\\[H_0: \\text{ four suits chosen equal probability}\\]Now, statistics, able say thing mathematical way. , let’s use notation \\(P_j\\) refer true probability j-th suit chosen. null hypothesis true, four suits 25% chance selected. words, null hypothesis claims \\(P_1 = .25\\), \\(P_2 = .25\\), \\(P3 = .25\\) finally \\(P_4 = .25\\) . However, way can group observed frequencies vector O summarises entire data set, can use P refer probabilities correspond null hypothesis. let vector \\(P = (P_1, P_2, P_3, P_4)\\) refer collection probabilities describe null hypothesis, :\\[H_0: P =(.25, .25, .25, .25)\\]particular instance, null hypothesis corresponds vector probabilities P probabilities equal one another. doesn’t case. instance, experimental task people imagine drawing deck twice many clubs suit, null hypothesis correspond something like \\(P = (.4, .2, .2, .2)\\). long probabilities positive numbers, sum 1, ’s perfectly legitimate choice null hypothesis. However, common use goodness--fit test test null hypothesis categories equally likely, ’ll stick example.alternative hypothesis, \\(H_1\\)? ’re really interested demonstrating probabilities involved aren’t identical (, people’s choices weren’t completely random). consequence, “human friendly” versions hypotheses look like :\\(H_0: \\text{ four suits chosen equal probability}\\)\n\\(H_1: \\text{ least one suit-choice probabilities isn’t 0.25}\\)…“mathematician friendly” version :\\(H_0: P= (.25, .25, .25, .25)\\)\n\\(H_1: P \\neq (.25, .25, .25, .25)\\)","code":""},{"path":"categorical-data-analysis.html","id":"the-goodness-of-fit-test-statistic","chapter":"10 Categorical data analysis","heading":"10.1.3 The “goodness-of-fit” test statistic","text":"point, observed frequencies O collection probabilities P corresponding null hypothesis want test. now want construct test null hypothesis. always, want test \\(H_0\\) \\(H_1\\), ’re going need test statistic. basic trick goodness--fit test uses construct test statistic measures “close” data null hypothesis. data don’t resemble ’d “expect” see null hypothesis true, probably isn’t true. Okay, null hypothesis true, expect see? , use correct terminology, expected frequencies. \\(N = 200\\) observations, (null true) probability one choosing heart \\(P_3 = .25\\), guess ’re expecting \\(200 \\times .25 = 50\\) hearts, right? , specifically, let Ei refer “number category responses ’re expecting null true”, \\[E_i=N \\times P_i\\]pretty easy calculate.200 observations can fall four categories, think four categories equally likely, average ’d expect see 50 observations category, right?Now, translate test statistic? Clearly, want compare expected number observations category (\\(E_i\\)) observed number observations category (\\(O_i\\)). basis comparison able come good test statistic. start , let’s calculate difference null hypothesis expected us find actually find. , calculate “observed minus expected” difference score, \\(O_i - E_i\\) . illustrated Table 10.3.Table 10.3:  Expected observed frequenciesSo, based calculations, ’s clear people chose hearts fewer clubs null hypothesis predicted. However, moment’s thought suggests raw differences aren’t quite ’re looking . Intuitively, feels like ’s just bad null hypothesis predicts observations (happened hearts) predicts many (happened clubs). ’s bit weird negative number clubs positive number hearts. One easy way fix square everything, now calculate squared differences, \\((E_i - O_i)^2\\) . , can hand (Table 10.4).Table 10.4:  Squaring difference scoresNow ’re making progress. ’ve got now collection numbers big whenever null hypothesis makes bad prediction (clubs hearts), small whenever makes good one (diamonds spades). Next, technical reasons ’ll explain moment, let’s also divide numbers expected frequency Ei , ’re actually calculating \\(\\frac{(E_i-O_i)^2}{E_i}\\) . Since \\(E_i = 50\\) categories example, ’s interesting calculation, let’s anyway (Table 10.5).Table 10.5:  Dividing squared difference scores expected frequency provide 'error' scoreIn effect, ’ve got four different “error” scores, one telling us big “mistake” null hypothesis made tried use predict observed frequencies. , order convert useful test statistic, one thing just add numbers . result called goodness--fit statistic, conventionally referred either \\(\\chi^2\\) (chi-square) GOF. can calculate Table 10.6.\\[\\sum( (observed - expected)^2 / expected )\\]gives us value 8.44.[Additional technical detail 76]’ve seen calculations, cards data set ’ve got value \\(\\chi^2\\) = 8.44. now question becomes big enough value reject null?","code":""},{"path":"categorical-data-analysis.html","id":"the-sampling-distribution-of-the-gof-statistic","chapter":"10 Categorical data analysis","heading":"10.1.4 The sampling distribution of the GOF statistic","text":"determine whether particular value \\(\\chi^2\\) large enough justify rejecting null hypothesis, ’re going need figure sampling distribution \\(\\chi^2\\) null hypothesis true. ’s ’m going section. ’ll show fair amount detail sampling distribution constructed, , next section, use build hypothesis test. want cut chase willing take faith sampling distribution \\(\\chi^2\\) (chi-square) distribution \\(k - 1\\) degrees freedom, can skip rest section. However, want understand goodness--fit test works way , read .Okay, let’s suppose null hypothesis actually true. , true probability observation falls -th category \\(P_i\\) . , ’s pretty much definition null hypothesis. Let’s think actually means. kind like saying “nature” makes decision whether observation ends category flipping weighted coin (.e., one probability getting head \\(P_j\\) ). therefore can think observed frequency \\(O_i\\) imagining nature flipped N coins (one observation data set), exactly \\(O_i\\) came heads. Obviously, pretty weird way think experiment. (hope) remind ’ve actually seen scenario . ’s exactly set gave rise binomial distribution earlier chapter. words, null hypothesis true, follows observed frequencies generated sampling binomial distribution:\\[O_i \\sim Binomial(P_i,N) \\]Now, remember discussion central limit theorem binomial distribution starts look pretty much identical normal distribution, especially \\(N\\) large \\(P_i\\) isn’t close 0 1. words long \\(N^P_i\\) large enough. , put another way, expected frequency Ei large enough theoretical distribution \\(O_i\\) approximately normal. Better yet, \\(O_i\\) normally distributed, \\((O_i-E_i)/\\sqrt{(E_i)}\\) . Since \\(E_i\\) fixed value, subtracting Ei dividing ? Ei changes mean standard deviation normal distribution ’s . Okay, now let’s look goodness--fit statistic actually . ’re taking bunch things normally-distributed, squaring , adding . Wait. ’ve seen ! discussed section useful distributions, take bunch things standard normal distribution (.e., mean 0 standard deviation 1), square add , resulting quantity chi-square distribution. now know null hypothesis predicts sampling distribution goodness--fit statistic chi-square distribution. Cool.’s one last detail talk , namely degrees freedom. remember back section useful distributions, said number things ’re adding k, degrees freedom resulting chi-square distribution k. Yet, said start section actual degrees freedom chi-square goodness--fit test \\(k - 1\\). ’s ? answer ’re supposed looking number genuinely independent things getting added together. , ’ll go talk next section, even though k things ’re adding \\(k - 1\\) truly independent, degrees freedom actually \\(k - 1\\). ’s topic next section77.","code":""},{"path":"categorical-data-analysis.html","id":"degrees-of-freedom","chapter":"10 Categorical data analysis","heading":"10.1.5 Degrees of freedom","text":"\nFigure 10.1: \\(\\chi^2\\) (chi-square) distributions different values ‘degrees freedom’\nintroduced chi-square distribution useful distributions, bit vague “degrees freedom” actually means. Obviously, matters. Looking Figure 10.1, can see change degrees freedom chi-square distribution changes shape quite substantially. exactly ? , introduced distribution explained relationship normal distribution, offer answer: ’s number “normally distributed variables” ’m squaring adding together. , people, ’s kind abstract entirely helpful. really need try understand degrees freedom terms data. goes.basic idea behind degrees freedom quite simple. calculate counting number distinct “quantities” used describe data subtracting “constraints” data must satisfy.78 bit vague, let’s use cards data concrete example. describe data using four numbers, \\(O1, O2, O3\\) O4 corresponding observed frequencies four different categories (hearts, clubs, diamonds, spades). four numbers random outcomes experiment. experiment actually fixed constraint built : sample size \\(N\\). 79 , knowhow many people chose hearts, many chose diamonds many chose clubs, ’d able figure exactly many chose spades. words, although data described using four numbers, actually correspond \\(4 - 1 = 3\\) degrees freedom. slightly different way thinking notice four probabilities ’re interested (, corresponding four different categories), probabilities must sum one, imposes constraint. Therefore degrees freedom \\(4 - 1 = 3\\). Regardless whether want think terms observed frequencies terms probabilities, answer . general, running \\(\\chi^2\\)(chi-square) goodness--fit test experiment involving \\(k\\) groups, degrees freedom \\(k - 1\\).","code":""},{"path":"categorical-data-analysis.html","id":"testing-the-null-hypothesis","chapter":"10 Categorical data analysis","heading":"10.1.6 Testing the null hypothesis","text":"final step process constructing hypothesis test figure rejection region . , values \\(\\chi^2\\) lead us reject null hypothesis. saw earlier, large values \\(\\chi^2\\) imply null hypothesis done poor job predicting data experiment, whereas small values \\(\\chi^2\\) imply ’s actually done pretty well. Therefore, pretty sensible strategy say critical value \\(\\chi^2\\) bigger critical value reject null, \\(\\chi^2\\) smaller value retain null. words, use language introduced chapter Hypothesis testing chi-square goodness--fit test always one-sided test. Right, figure critical value . ’s pretty straightforward. want test significance level \\(\\alpha = .05\\) (, willing tolerate Type error rate \\(5%\\)), choose critical value 5% chance \\(\\chi^2\\) get big null hypothesis true. illustrated Figure 10.2.\nFigure 10.2: Illustration hypothesis testing works \\(\\chi^2\\) (chi-square) goodness -fit test\nAh , hear ask, find critical value chi-square distribution \\(k-1\\) degrees freedom? Many many years ago first took psychology statistics class used look critical values book critical value tables, like one Figure 10.3. Looking Figure, can see critical value \\(\\chi^2\\) distribution 3 degrees freedom, p=0.05 7.815.\nFigure 10.3: Table critical values chi-square distribution\n, calculated \\(\\chi^2\\) statistic bigger critical value \\(7.815\\), can reject null hypothesis (remember null hypothesis, \\(H_0\\), four suits chosen equal probability). Since actually already calculated (.e., \\(\\chi^2\\) = 8.44) can reject null hypothesis. ’s , basically. now know “Pearson’s \\(\\chi^2\\) test goodness--fit”. Lucky .","code":""},{"path":"categorical-data-analysis.html","id":"doing-the-test-in-jamovi","chapter":"10 Categorical data analysis","heading":"10.1.7 Doing the test in jamovi","text":"surprisingly, jamovi provides analysis calculations . main ‘Analyses’ toolbar select ‘Frequencies’ - ‘One Sample Proportion Tests’ - ‘\\(N\\) Outcomes’. analysis window appears move variable want analyse (choice 1 across ‘Variable’ box. Also, click ‘Expected counts’ check box shown results table. done , see analysis results jamovi Figure 10.4. surprise jamovi provides expected counts statistics calculated hand , \\(\\chi^2\\) value \\((8.44\\) \\(3\\) d.f. \\(p=0.038\\). Note don’t need look critical p-value threshold value , jamovi gives us actual p-value calculated \\(\\chi^2\\) \\(3\\) d.f.\nFigure 10.4: \\(\\chi^2\\) One Sample Proportion Test jamovi, table showing observed expected frequencies proportions\n","code":""},{"path":"categorical-data-analysis.html","id":"specifying-a-different-null-hypothesis","chapter":"10 Categorical data analysis","heading":"10.1.8 Specifying a different null hypothesis","text":"point might wondering want run goodness--fit test null hypothesis categories equally likely. instance, let’s suppose someone made theoretical prediction people choose red cards \\(60\\%\\) time, black cards \\(40\\%\\) time (’ve idea ’d predict ), preferences. case, null hypothesis expect \\(30\\%\\) choices hearts, \\(30\\%\\) diamonds, \\(20\\%\\) spades \\(20\\%\\) clubs. words expect hearts diamonds appear 1.5 times often spades clubs (ratio \\(30\\%\\) : \\(20\\%\\) 1.5 : 1). seems like silly theory , ’s pretty easy test explicitly specified null hypothesis data jamovi analysis. analysis window (labelled ‘Proportion Test (N Outcomes)’ Figure 10.4 can expand options ‘Expected Proportions’. , options entering different ratio values variable selected, case choice 1. Change ratio reflect new null hypothesis, Figure 10.5, see results change.\nFigure 10.5: Changing expected proportions \\(\\chi^2\\) One Sample Proportion Test jamovi\nexpected counts now shown Table 10.6.Table 10.6:  Expected counts different null hypothesisand \\(\\chi^2\\) statistic 4.74, 3 d.f., \\(p = 0.182\\). Now, results updated hypotheses expected frequencies different last time. consequence \\(\\chi^2\\) test statistic different, p-value different . Annoyingly, p-value \\(.182\\), can’t reject null hypothesis (look back p value test remind ). Sadly, despite fact null hypothesis corresponds silly theory, data don’t provide enough evidence .","code":""},{"path":"categorical-data-analysis.html","id":"how-to-report-the-results-of-the-test","chapter":"10 Categorical data analysis","heading":"10.1.9 How to report the results of the test","text":"now know test works, know test using wonderful jamovi flavoured magic computing box. next thing need know write results. , ’s point designing running experiment analysing data don’t tell anyone ! let’s now talk need reporting analysis. Let’s stick card-suits example. wanted write result paper something, conventional way report write something like :200 participants experiment, 64 selected hearts first choice, 51 selected diamonds, 50 selected spades, 35 selected clubs. chi-square goodness--fit test conducted test whether choice probabilities identical four suits. results significant (\\(\\chi^2(3) = 8.44, p< .05)\\), suggesting people select suits purely random.pretty straightforward hopefully seems pretty unremarkable. said, ’s things note description:statistical test preceded descriptive statistics. , told reader something data look like going test. general, good practice. Always remember reader doesn’t know data anywhere near well . , unless describe properly, statistical tests won’t make sense ’ll get frustrated cry.description tells null hypothesis tested . honest, writers don’t always ’s often good idea situations ambiguity exists, can’t rely readership intimately familiar statistical tools ’re using. Quite often reader might know (remember) details test using, ’s kind politeness “remind” ! far goodness--fit test goes, can usually rely scientific audience knowing works (since ’s covered intro stats classes). However, ’s still good idea explicit stating null hypothesis (briefly!) null hypothesis can different depending ’re using test . instance, cards example null hypothesis four suit probabilities identical (.e., \\(P1 = P2 = P3 = P4 = 0.25\\)), ’s nothing special hypothesis. just easily tested null hypothesis \\(P_1 = 0.7\\) \\(P2 = P3 = P4 = 0.1\\) using goodness--fit test. ’s helpful reader explain null hypothesis . Also, notice described null hypothesis words, maths. ’s perfectly acceptable. can describe maths like, since readers find words easier read symbols, writers tend describe null using words can.“stat block” included. reporting results test , didn’t just say result significant, included “stat block” (.e., dense mathematical looking part parentheses) reports “key” statistical information. chi-square goodness--fit test, information gets reported test statistic (goodness--fit statistic 8.44), information distribution used test (\\(\\chi^2\\) 3 degrees freedom usually shortened \\(\\chi^2\\)(3)), information whether result significant (case \\(p< .05\\)). particular information needs go stat block different every test, time introduce new test ’ll show stat block look like.80 However general principle always provide enough information reader check test results really wanted .results interpreted. addition indicating result significant, provided interpretation result (.e., people didn’t choose randomly). also kindness reader, tells something believe ’s going data. don’t include something like , ’s really hard reader understand ’s going .81As everything else, overriding concern explain things reader. Always remember point reporting results communicate another human . tell just many times ’ve seen results section report thesis even scientific article just gibberish, writer focused solely making sure ’ve included numbers forgotten actually communicate human reader.Satan delights equally statistics quoting scripture82\n– H.G. Wells","code":""},{"path":"categorical-data-analysis.html","id":"the-chi2-test-of-independence-or-association","chapter":"10 Categorical data analysis","heading":"10.2 The \\(\\chi^2\\) test of independence (or association)","text":"GUARDBOT 1: Halt!\nGUARDBOT 2: robot human?\nLEELA: Robot….\nFRY: Uh, yup! Just two robots roboting ! Eh?\nGUARDBOT 1: Administer test.\nGUARDBOT 2: following prefer? : puppy, B: pretty flower sweetie, C: large properly-formatted data file?\nGUARDBOT 1: Choose!Futurama, “Fear Bot Planet”day watching animated documentary examining quaint customs natives planet Chapek 9. Apparently, order gain access capital city visitor must prove ’re robot, human. order determine whether visitor human, natives ask whether visitor prefers puppies, flowers, large, properly formatted data files. “Pretty clever,” thought “humans robots preferences? probably wouldn’t good test , ?” happens, got hands testing data civil authorities Chapek 9 used check . turns simple. found bunch robots bunch humans asked preferred. saved data file called chapek9.omv, can now load jamovi. well ID variable identifies individual people, two nominal text variables, species choice. total 180 entries data set, one person (counting robots humans “people”) asked make choice. Specifically, 93 humans 87 robots, overwhelmingly preferred choice data file. can check asking jamovi Frequency Tables, ‘Exploration’ - ‘Descriptives’ button. However, summary address question ’re interested . , need detailed description data. want look choices broken species. , need cross-tabulate data (see Tabulating cross-tabulating data). jamovi using ‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’ analysis, get table something like Table 10.7.Table 10.7:  Cross-tabulating dataFrom , ’s quite clear vast majority humans chose data file, whereas robots tended lot even preferences. Leaving aside question humans might likely choose data file moment (seem quite odd, admittedly), first order business determine discrepancy human choices robot choices data set statistically significant.","code":""},{"path":"categorical-data-analysis.html","id":"constructing-our-hypothesis-test","chapter":"10 Categorical data analysis","heading":"10.2.1 Constructing our hypothesis test","text":"analyse data? Specifically, since research hypothesis “humans robots answer question different ways”, can construct test null hypothesis “humans robots answer question way”? , begin establishing notation describe data (Table 10.8).Table 10.8:  Notation describe dataIn notation say \\(O_{ij}\\) count (observed frequency) number respondents species j (robots human) gave answer (puppy, flower data) asked make choice. total number observations written \\(N\\), usual. Finally, ’ve used \\(R_i\\) denote row totals (e.g., \\(R_1\\) total number people chose flower), \\(C_j\\) denote column totals (e.g., \\(C_1\\) total number robots).83So now let’s think null hypothesis says. robots humans responding way question, means probability “robot says puppy” probability “human says puppy”, two possibilities. , use \\(P_{ij}\\) denote “probability member species j gives response ” null hypothesis :\\[\n\\begin{aligned}\nH_0 &: \\text{following true:} \\\\\n&P_{11} = P_{12}\\text{ (probability saying “puppy”),} \\\\\n&P_{21} = P_{22}\\text{ (probability saying “flower”), } \\\\\n&P_{31} = P_{32}\\text{ (probability saying “data”).}\n\\end{aligned}\n\\]actually, since null hypothesis claiming true choice probabilities don’t depend species person making choice, can let Pi refer probability, e.g., P1 true probability choosing puppy.Next, much way goodness--fit test, need calculate expected frequencies. , observed counts \\(O_{ij}\\) , need figure null hypothesis tell us expect. Let’s denote expected frequency \\(E_{ij}\\). time, ’s little bit trickier. total \\(C_j\\) people belong species \\(j\\), true probability anyone (regardless species) choosing option \\(\\) \\(P_i\\) , expected frequency just:\\[E_{ij}=C_j \\times P_i\\]Now, well good, problem. Unlike situation goodness--fit test, null hypothesis doesn’t actually specify particular value Pi .’s something estimate (see earlier chapter [Estimating unknown quantities sample]) data! Fortunately, pretty easy . 28 180 people selected flowers, natural estimate probability choosing flowers \\(\\frac{28}{180}\\), approximately \\(.16\\). phrase mathematical terms, ’re saying estimate probability choosing option just row total divided total sample size:\\[\\hat{P}_{}= \\frac{R_i}{N}\\]Therefore, expected frequency can written product (.e. multiplication) row total column total, divided total number observations:84\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\][Additional technical detail 85], large values \\(X^2\\) indicate null hypothesis provides poor description data, whereas small values \\(X^2\\) suggest good job accounting data. Therefore, just like last time, want reject null hypothesis \\(X^2\\) large.surprisingly, statistic \\(\\chi^2\\) distributed. need figure many degrees freedom involved, actually isn’t hard. mentioned , can (usually) think degrees freedom equal number data points ’re analysing, minus number constraints. contingency table r rows c columns contains total \\(r^{c}\\) observed frequencies, ’s total number observations. constraints? , ’s slightly trickier. answer always \\[df=(r-1)(c-1)\\]explanation degrees freedom takes value different depending experimental design. sake argument, let’s suppose honestly intended survey exactly 87 robots 93 humans (column totals fixed experimenter), left row totals free vary (row totals random variables). Let’s think constraints apply . Well, since deliberately fixed column totals Act Experimenter, \\(c\\) constraints right . , ’s actually . Remember null hypothesis free parameters (.e., estimate Pi values)? matter . won’t explain book, every free parameter null hypothesis rather like additional constraint. , many ? Well, since probabilities sum 1, ’s \\(r - 1\\) . total degrees freedom :\\[ \\begin{equation} \\begin{split} df & = \\text{(number \nobservations) - (number constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\n\\end{equation} \\]Alternatively, suppose thing experimenter fixed total sample size N. , quizzed first 180 people saw just turned 87 robots 93 humans. time around reasoning slightly different, still lead us answer. null hypothesis still \\(r - 1\\) free parameters corresponding choice probabilities, now also \\(c - 1\\) free parameters corresponding species probabilities, ’d also estimate probability randomly sampled person turns robot.86 Finally, since actually fix total number observations N, ’s one constraint. , now rc observations, \\((c-1)+(r-1)+1\\) constraints. give?\\[ \\begin{equation} \\begin{split} df & = \\text{(number \nobservations) - (number constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r - 1)(c - 1) \\end{split} \\end{equation}\n\\] Amazing.","code":""},{"path":"categorical-data-analysis.html","id":"doing-the-test-in-jamovi-1","chapter":"10 Categorical data analysis","heading":"10.2.2 Doing the test in jamovi","text":"Okay, now know test works let’s look ’s done jamovi. tempting lead tedious calculations ’re forced learn long way, figure ’s point. already showed long way goodness--fit test last section, since test independence isn’t conceptually different, won’t learn anything new long way. instead ’ll go straight showing easy way. run test jamovi (‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’), look underneath contingency table jamovi results window \\(\\chi^2\\) statistic . shows \\(\\chi^2\\) statistic value 10.72, 2 d.f. p-value = 0.005.easy, wasn’t ! can also ask jamovi show expected counts - just click check box ‘Counts’ - ‘Expected’ ‘Cells’ options expected counts appear contingency table. whilst , effect size measure helpful. ’ll choose Cramér’s \\(V\\), can specify check box ‘Statistics’ options, gives value Cramér’s \\(V\\) \\(0.24\\). talk just moment.output gives us enough information write result:Pearson’s \\(\\chi^2\\) revealed significant association species choice (\\(\\chi^2(2) = 10.7, p< .01)\\). Robots appeared likely say prefer flowers, humans likely say prefer data.Notice , , provided little bit interpretation help human reader understand ’s going data. Later discussion section ’d provide bit context. illustrate difference, ’s ’d probably say later :fact humans appeared stronger preference raw data files robots somewhat counter-intuitive. However, context makes sense, civil authority Chapek 9 unfortunate tendency kill dissect humans identified. seems likely human participants respond honestly question, avoid potentially undesirable consequences. considered substantial methodological weakness.classified rather extreme example reactivity effect, suppose. Obviously, case problem severe enough study less worthless tool understanding difference preferences among humans robots. However, hope illustrates difference getting statistically significant result (null hypothesis rejected favour alternative), finding something scientific value (data tell us nothing interest research hypothesis due big methodological flaw).","code":""},{"path":"categorical-data-analysis.html","id":"the-continuity-correction","chapter":"10 Categorical data analysis","heading":"10.3 The continuity correction","text":"Okay, time little bit digression. ’ve lying little bit far. ’s tiny change need make calculations whenever 1 degree freedom. ’s called “continuity correction”, sometimes Yates correction. Remember pointed earlier: \\(\\chi^2\\) test based approximation, specifically assumption binomial distribution starts look like normal distribution large \\(N\\). One problem often doesn’t quite work, especially ’ve got 1 degree freedom (e.g., ’re test independence \\(2 \\times 2\\) contingency table). main reason true sampling distribution \\(X^{2}\\) statistic actually discrete (’re dealing categorical data!) \\(\\chi^2\\) distribution continuous. can introduce systematic problems. Specifically, N small \\(df = 1\\), goodness--fit statistic tends “big”, meaning actually bigger α value think (, equivalently, p values bit small).far can tell reading Yates’ paper87, correction basically hack. ’s derived principled theory. Rather, ’s based examination behaviour test, observing corrected version seems work better. can specify correction jamovi check box ‘Statistics’ options, called ‘\\(\\chi^2\\) continuity correction’.","code":""},{"path":"categorical-data-analysis.html","id":"effect-size","chapter":"10 Categorical data analysis","heading":"10.4 Effect size","text":"discussed earlier (Effect size, sample size power), ’s becoming commonplace ask researchers report measure effect size. , let’s suppose ’ve run chi-square test, turns significant. now know association variables (independence test) deviation specified probabilities (goodness--fit test). Now want report measure effect size. , given association deviation, strong ?several different measures can choose report, several different tools can use calculate . won’t discuss instead focus commonly reported measures effect size.default, two measures people tend report frequently \\(\\phi\\) statistic somewhat superior version, known Cramér’s \\(V\\) .[Additional technical detail 88]’re done. seems fairly popular measure, presumably ’s easy calculate, gives answers aren’t completely silly. Cramér’s \\(V\\), know value4 really range 0 (association ) 1 (perfect association).","code":""},{"path":"categorical-data-analysis.html","id":"assumptions-of-the-tests","chapter":"10 Categorical data analysis","heading":"10.5 Assumptions of the test(s)","text":"statistical tests make assumptions, ’s usually good idea check assumptions met. chi-square tests discussed far chapter, assumptions :Expected frequencies sufficiently large. Remember previous section saw \\(\\chi^2\\) sampling distribution emerges binomial distribution pretty similar normal distribution? Well, like discussed chapter Introduction probability true number observations sufficiently large. means practice expected frequencies need reasonably big. big reasonably big? Opinions differ, default assumption seems generally like see expected frequencies larger 5, though larger tables probably okay least 80% expected frequencies 5 none 1. However, ’ve able discover (e.g., Cochran (1954)) seem proposed rough guidelines, hard fast rules, seem somewhat conservative (Larntz 1978).Data independent one another. One somewhat hidden assumption chi-square test genuinely believe observations independent. ’s mean. Suppose ’m interested proportion babies born particular hospital boys. walk around maternity wards observe 20 girls 10 boys. Seems like pretty convincing difference, right? later , turns ’d actually walked ward 10 times fact ’d seen 2 girls 1 boy. convincing, ? original 30 observations massively non-independent, fact equivalent 3 independent observations. Obviously extreme (extremely silly) example, illustrates basic issue. Non-independence “stuffs things ”. Sometimes causes falsely reject null, silly hospital example illustrates, can go way . give slightly less stupid example, let’s consider happen ’d done cards experiment slightly differently Instead asking 200 people try imagine sampling one card random, suppose asked 50 people select 4 cards. One possibility everyone selects one heart, one club, one diamond one spade (keeping “representativeness heuristic” (Tversky Kahneman 1974). highly non-random behaviour people, case get observed frequency 50 four suits. example fact observations non-independent (four cards pick related ) actually leads opposite effect, falsely retaining null.happen find situation independence violated, may possible use McNemar test (’ll discuss) Cochran test (won’t). Similarly, expected cell counts small, check Fisher exact test. topics now turn.","code":""},{"path":"categorical-data-analysis.html","id":"the-fisher-exact-test","chapter":"10 Categorical data analysis","heading":"10.6 The Fisher exact test","text":"cell counts small, ’d still like test null hypothesis two variables independent? One answer “collect data”, ’s far glib lot situations either infeasible unethical . , statisticians kind moral obligation provide scientists better tests. instance, Fisher (1922a) kindly provided right answer question. illustrate basic idea let’s suppose ’re analysing data field experiment looking emotional status people accused Witchcraft, currently burned stake.89 Unfortunately scientist (rather fortunately general populace), ’s actually quite hard find people process set fire, cell counts awfully small cases. contingency table salem.csv data illustrates point (Table 10.9).Table 10.9:  Contingency table salem.csv dataLooking data, ’d hard pressed suspect people fire likely happy people fire. However, chi-square test makes hard test small sample size. , speaking someone doesn’t want set fire, ’d really like able get better answer . Fisher’s exact test (Fisher 1922a) comes handy.Fisher exact test works somewhat differently chi-square test (fact hypothesis tests talk book) insofar doesn’t test statistic, calculates p-value “directly”. ’ll explain basics test works \\(2 \\times 2\\) contingency table. , let’s notation (Table 10.10).Table 10.10:  Notation Fisher exact testIn order construct test Fisher treats row column totals \\((R_1, R_2, C_1 \\text{ } C_2)\\) known, fixed quantities calculates probability obtained observed frequencies \\((O_{11}, O_{12}, O_{21} \\text{ } O_{22})\\) given totals. notation developed chapter Introduction probability written:\\[P(O_{11}, O_{12}, O_{21}, O_{22}  \\text{ | } R_1, R_2, C_1, C_2)\\] might imagine, ’s slightly tricky exercise figure probability . turns probability described distribution known hypergeometric distribution. calculate p-value calculate probability observing particular table table “extreme”. 90 Back 1920s, computing sum daunting even simplest situations, days ’s pretty easy long tables aren’t big sample size isn’t large. conceptually tricky issue figure means say one contingency table “extreme” another. easiest solution say table lowest probability extreme. gives us p-value.can specify test jamovi check box ‘Statistics’ options ‘Contingency Tables’ analysis. data salem.csv file, Fisher exact test statistic shown results. main thing ’re interested p-value, case small enough (p = .036) justify rejecting null hypothesis people fire just happy people fire. See Figure 10.6.\nFigure 10.6: Fisher exact test analysis jamovi\n","code":""},{"path":"categorical-data-analysis.html","id":"the-mcnemar-test","chapter":"10 Categorical data analysis","heading":"10.7 The McNemar test","text":"Suppose ’ve hired work Australian Generic Political Party (AGPP), part job find effective AGPP political advertisements . decide put together sample \\(N = 100\\) people ask watch AGPP ads. see anything, ask intend vote AGPP, showing ads ask see anyone changed minds. Obviously, ’re good job, ’d also whole lot things , let’s consider just one simple experiment. One way describe data via contingency table shown Table 10.11.Table 10.11:  Contingency table data AGPP political advertisementsAt first pass, might think situation lends Pearson \\(\\chi^2\\) test independence (per \\(\\chi^2\\) test independence (association)). However, little bit thought reveals ’ve got problem. 100 participants 200 observations. person provided us answer column column. means 200 observations aren’t independent . voter says “yes” first time voter B says “”, ’d expect voter likely say “yes” second time voter B! consequence usual \\(\\chi^2\\) test won’t give trustworthy answers due violation independence assumption. Now, really uncommon situation, wouldn’t bothering waste time talking . ’s uncommon . standard repeated measures design, none tests ’ve considered far can handle . Eek.solution problem published McNemar (1947). trick start tabulating data slightly different way (Table 10.12).Table 10.12:  Tabulate data different way repeated measures dataNext, let’s think null hypothesis : ’s “” test “” test proportion people saying “Yes, vote AGPP”. way rewritten data, means ’re now testing hypothesis row totals column totals come distribution. Thus, null hypothesis McNemar’s test “marginal homogeneity”. , row totals column totals distribution: \\(P_a + P_b = P_a + P_c\\) similarly \\(P_c + P_d = P_b + P_d\\). Notice means null hypothesis actually simplifies Pb = Pc. words, far McNemar test concerned, ’s -diagonal entries table (.e., b c) matter! noticing , McNemar test marginal homogeneity different usual \\(\\chi^2\\) test. applying Yates correction, test statistic becomes:\\[\\chi^2=\\frac{(|b-c|-0.5)^2}{b+c}\\] , revert notation used earlier chapter:\\[\\chi^2=\\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}\\] statistic \\(\\chi^2\\) distribution (approximately) df = 1. However, remember just like \\(\\chi^2\\) tests ’s approximation, need reasonably large expected cell counts work.","code":""},{"path":"categorical-data-analysis.html","id":"doing-the-mcnemar-test-in-jamovi","chapter":"10 Categorical data analysis","heading":"10.7.1 Doing the McNemar test in jamovi","text":"Now know McNemar test , lets actually run one. agpp.csv file contains raw data discussed previously. agpp data set contains three variables, id variable labels participant data set (’ll see ’s useful moment), response_before variable records person’s answer asked question first time, response_after variable shows answer gave asked question second time. Notice participant appears data set. Go ‘Analyses’ - ‘Frequencies’ - ‘Contingency Tables’ - ‘Paired Samples’ analysis jamovi, move response_before ‘Rows’ box, response_after ‘Columns’ box. get contingency table results window, statistic McNemar test just , see Figure 10.7.\nFigure 10.7: McNemar test output jamovi\n’re done. ’ve just run McNemar’s test determine people just likely vote AGPP ads hand. test significant (\\(\\chi^2(1)= 12.03, p< .001)\\), suggesting . , fact looks like ads negative effect: people less likely vote AGPP seeing ads. makes lot sense consider quality typical political advertisement.","code":""},{"path":"categorical-data-analysis.html","id":"whats-the-difference-between-mcnemar-and-independence","chapter":"10 Categorical data analysis","heading":"10.8 What’s the difference between McNemar and independence?","text":"Let’s go way back beginning chapter look cards data set . recall, actual experimental design described involved people making two choices. information first choice second choice everyone made, can construct following contingency table cross-tabulates first choice second choice (Table 10.13).Table 10.13:  Cross-tabulating first second choice cards dataSuppose wanted know whether choice make second time dependent choice made first time. test independence useful, ’re trying see ’s relationship rows columns table.Alternatively, suppose wanted know average, frequencies suit choices different second time first time. situation, ’m really trying see row totals different column totals. ’s use McNemar test.different statistics produced different analyses shown Figure @ref(fig:fig(10-8). Notice results different! aren’t test.\nFigure 10.8: Independent vs. Paired (McNemar) test output jamovi\n","code":""},{"path":"categorical-data-analysis.html","id":"summary-8","chapter":"10 Categorical data analysis","heading":"10.9 Summary","text":"key ideas discussed chapter :\\(\\chi^2\\) (chi-square) goodness--fit test used table observed frequencies different categories, null hypothesis gives set “known” probabilities compare .\\(\\chi^2\\) test independence (association) used contingency table (cross-tabulation) two categorical variables. null hypothesis relationship association variables.Effect size contingency table can measured several ways. particular noted Cramér’s \\(V\\) statistic.versions Pearson test rely two assumptions: expected frequencies sufficiently large, observations independent (Assumptions test(s). Fisher exact test can used expected frequencies small. McNemar test can used kinds violations independence.’re interested learning categorical data analysis good first choice Agresti (1996) , title suggests, provides Introduction Categorical Data Analysis. introductory book isn’t enough (can’t solve problem ’re working ) consider Agresti (2002), Categorical Data Analysis. latter advanced text, ’s probably wise jump straight book one.","code":""},{"path":"comparing-two-means.html","id":"comparing-two-means","chapter":"11 Comparing two means","heading":"11 Comparing two means","text":"Categorical data analysis covered situation outcome variable nominal scale predictor variable also nominal scale. Lots real world situations character, ’ll find chi-square tests particular quite widely used. However, ’re much likely find situation outcome variable interval scale higher, ’re interested whether average value outcome variable higher one group another. instance, psychologist might want know anxiety levels higher among parents non-parents, working memory capacity reduced listening music (relative listening music). medical context might want know new drug increases decreases blood pressure. agricultural scientist might want know whether adding phosphorus Australian native plants kill .91 situations outcome variable fairly continuous, interval ratio scale variable, predictor binary “grouping” variable. words, want compare means two groups.standard answer problem comparing means use t-test, several varieties depending exactly question want solve. consequence, majority chapter focuses different types t-test: one sample t-tests, independent samples t-tests paired samples t-tests. ’ll talk one sided tests , , ’ll talk bit Cohen’s d, standard measure effect size t-test. later sections chapter focus assumptions t-tests, possible remedies violated. However, discussing useful things, ’ll start discussion z-test.","code":""},{"path":"comparing-two-means.html","id":"the-one-sample-z-test","chapter":"11 Comparing two means","heading":"11.1 The one-sample z-test","text":"section ’ll describe one useless tests statistics: z-test. Seriously – test almost never used real life. real purpose , teaching statistics, ’s convenient stepping stone along way towards t-test, probably ()used tool statistics.","code":""},{"path":"comparing-two-means.html","id":"the-inference-problem-that-the-test-addresses","chapter":"11 Comparing two means","heading":"11.1.1 The inference problem that the test addresses","text":"introduce idea behind z-test, let’s use simple example. friend mine, Dr Zeppo, grades introductory statistics class curve. Let’s suppose average grade class \\(67.5\\), standard deviation \\(9.5\\). many hundreds students, turns 20 also take psychology classes. curiosity, find wondering psychology students tend get grades everyone else (.e., mean \\(67.5\\)) tend score higher lower? emails zeppo.csv file, use look grades students, jamovi spreadsheet view,calculate mean ‘Exploration’ - ‘Descriptives’ 92. mean value \\(72.3\\).50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89Hmm. might psychology students scoring bit higher normal. sample mean \\(\\bar{X} = 72.3\\) fair bit higher hypothesised population mean \\(\\mu = 67.5\\) , hand, sample size \\(N = 20\\) isn’t big. Maybe ’s pure chance.answer question, helps able write think know. Firstly, know sample mean \\(\\bar{X} = 72.3\\). ’m willing assume psychology students standard deviation rest class can say population standard deviation \\(\\sigma = 9.5\\). ’ll also assume since Dr Zeppo grading curve, psychology student grades normally distributed.Next, helps clear want learn data. case research hypothesis relates population mean µ psychology student grades, unknown. Specifically, want know \\(\\mu = 67.5\\) . Given know, can devise hypothesis test solve problem? data, along hypothesised distribution thought arise, shown Figure @ref(fig:fig11.1). entirely obvious right answer , ? , going need statistics.\nFigure 11.1: theoretical distribution (solid line) psychology student grades (bars) supposed generated\n","code":""},{"path":"comparing-two-means.html","id":"constructing-the-hypothesis-test","chapter":"11 Comparing two means","heading":"11.1.2 Constructing the hypothesis test","text":"first step constructing hypothesis test clear null alternative hypotheses . isn’t hard . null hypothesis, \\(H_0\\), true population mean \\(\\mu\\) psychology student grades \\(67.5\\%\\), alternative hypothesis population mean isn’t \\(67.5\\%\\). write mathematical notation, hypotheses become:\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]though honest notation doesn’t add much understanding problem, ’s just compact way writing ’re trying learn data. null hypotheses \\(H_0\\) alternative hypothesis \\(H_1\\) test illustrated Figure 11.2. addition providing us hypotheses, scenario outlined provides us fair amount background knowledge might useful. Specifically, two special pieces information can add:psychology grades normally distributed.true standard deviation scores £$ known 9.5.moment, ’ll act absolutely trustworthy facts. real life, kind absolutely trustworthy background knowledge doesn’t exist, want rely facts ’ll just make assumption things true. However, since assumptions may may warranted, might need check . now though, ’ll keep things simple.\nFigure 11.2: Graphical illustration null alternate hypotheses assumed one sample \\(z\\)-test (two sided version, ). null alternate hypotheses assume population distribution normal, additionally assumes population standard deviation known (fixed value \\(\\sigma_0\\)). null hypothesis (left) population mean \\(\\mu\\) equal specified value \\(\\mu_0\\). alternative hypothesis (right) population mean differs value, \\(\\mu \\neq \\mu_0\\)\nnext step figure good choice diagnostic test statistic, something help us discriminate \\(H_0\\) \\(H_1\\). Given hypotheses refer population mean \\(\\mu\\), ’d feel pretty confident sample mean \\(\\bar{X}\\) pretty useful place start. look difference sample mean \\(\\bar{X}\\) value null hypothesis predicts population mean. example mean calculate \\(\\bar{X} - 67.5\\). generally, let \\(\\mu_0\\) refer value null hypothesis claims population mean, ’d want calculate\\[\\bar{X}-\\mu_0\\]quantity equals close 0, things looking good null hypothesis. quantity long way away 0, ’s looking less likely null hypothesis worth retaining. far away zero us reject H0?figure need bit sneaky, ’ll need rely two pieces background knowledge wrote previously; namely raw data normally distributed know value population standard deviation \\(\\sigma\\). null hypothesis actually true, true mean \\(\\mu_0\\), facts together mean know complete population distribution data: normal distribution mean \\(\\mu_0\\) standard deviation \\(\\sigma\\).93Okay, ’s true, can say distribution \\(\\bar{X}\\)? Well, discussed earlier (see central limit theorem), sampling distribution mean \\(\\bar{X}\\) also normal, mean \\(\\mu\\). standard deviation sampling distribution \\(\\\\{se(\\bar{X})\\\\}\\), called standard error mean, 94\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]Now comes trick. can convert sample mean \\(\\bar{X}\\) standard score (see Standard scores section). conventionally written z, now ’m going refer \\(z_{\\bar{X}}\\). reason using expanded notation help remember ’re calculating standardised version sample mean, standardised version single observation, z-score usually refers ). z-score sample mean \\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\] , equivalently \\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]z-score test statistic. nice thing using test statistic like z-scores, standard normal distribution:95In words, regardless scale original data , z-statistic always interpretation: ’s equal number standard errors separate observed sample mean \\(\\bar{X}\\) population mean \\(\\mu_0\\) predicted null hypothesis. Better yet, regardless population parameters raw scores actually , 5% critical regions z-test always , illustrated Figure 11.3. meant, way back days people statistics hand, someone publish table like Table 11.1. , turn, meant researchers calculate z-statistic hand look critical value text book.\\[z_{\\bar{X}} \\sim Normal(0,1) \\]Table 11.1:  Critical values different \\( \\alpha \\) levels","code":""},{"path":"comparing-two-means.html","id":"a-worked-example-by-hand","chapter":"11 Comparing two means","heading":"11.1.3 A worked example, by hand","text":"Now, mentioned earlier, z-test almost never used practice. ’s rarely used real life basic installation jamovi doesn’t built function . However, test incredibly simple ’s really easy one manually. Let’s go back data Dr Zeppo’s class. loaded grades data, first thing need calculate sample mean, ’ve already done (\\(72.3\\)). already known population standard deviation (\\(\\sigma = 9.5\\)), value population mean null hypothesis specifies (\\(\\mu_0 = 67.5\\)), know sample size (\\(N=20\\)).\nFigure 11.3: Rejection regions two-sided z-test (panel ) one-sided z-test (panel b)\nNext, let’s calculate (true) standard error mean (easily done calculator):\\[ \\begin{equation} \\begin{split} sem.true & = \\frac{sd.true}{\\sqrt{N}}\n\\\\\\\\ & = \\frac{9.5}{\\sqrt{20}} \\\\\\\\ & = 2.124265 \\end{split}\n\\end{equation} \\]finally, calculate z-score:\\[ \\begin{equation} \\begin{split} z.score & = \\frac{sample.mean -\nmu.null}{sem.true} \\\\\\\\ & = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\ & =\n2.259606 \\end{split} \\end{equation} \\]point, traditionally look value \\(2.26\\) table critical values. original hypothesis two-sided (didn’t really theory whether psych students better worse statistics students) hypothesis test two-sided (two-tailed) also. Looking little table showed earlier, can see \\(2.26\\) bigger critical value \\(1.96\\) required significant \\(\\alpha = .05\\), smaller value \\(2.58\\) required significant level \\(\\alpha = .01\\). Therefore, can conclude significant effect, might write saying something like :mean grade \\(73.2\\) sample psychology students, assuming true population standard deviation \\(9.5\\), can conclude psychology students significantly different statistics scores class average (\\(z = 2.26, N = 20, p<.05\\)).","code":""},{"path":"comparing-two-means.html","id":"assumptions-of-the-z-test","chapter":"11 Comparing two means","heading":"11.1.4 Assumptions of the z-test","text":"’ve said , statistical tests make assumptions. tests make reasonable assumptions, tests . test ’ve just described, one sample z-test, makes three basic assumptions. :Normality. usually described, z-test assumes true population distribution normal.96 often pretty reasonable assumption, ’s also assumption can check feel worried (see Section Checking normality sample).Independence. second assumption test observations data set correlated , related funny way. isn’t easy check statistically, relies bit good experimental design. obvious (stupid) example something violates assumption data set “copy” observation data file end massive “sample size”, consists one genuine observation. realistically, ask ’s really plausible imagine observation completely random sample population ’re interested . practice assumption never met, try best design studies minimise problems correlated data.Known standard deviation. third assumption z-test true standard deviation population known researcher. just stupid. real world data analysis problem know standard deviation σ population completely ignorant mean \\(\\mu\\). words, assumption always wrong.view stupidity assuming \\(\\alpha\\) known, let’s see can live without . takes us dreary domain z-test, magical kingdom t-test, unicorns fairies leprechauns!","code":""},{"path":"comparing-two-means.html","id":"the-one-sample-t-test","chapter":"11 Comparing two means","heading":"11.2 The one-sample t-test","text":"thought, decided might safe assume psychology student grades necessarily standard deviation students Dr Zeppo’s class. , ’m entertaining hypothesis don’t mean, believe absolutely standard deviation? view , really stop assuming know true value \\(\\sigma\\). violates assumptions z-test, one sense ’m back square one. However, ’s like ’m completely bereft options. , ’ve still got raw data, raw data give estimate population standard deviation, 9.52. words, can’t say know \\(\\sigma = 9.5\\), can say \\(\\hat{\\sigma}\\) = 9.52.Okay, cool. obvious thing might think run z-test, using estimated standard deviation \\(9.52\\) instead relying assumption true standard deviation \\(9.5\\). probably wouldn’t surprised hear still give us significant result. approach close, ’s quite correct. now relying estimate population standard deviation need make adjustment fact uncertainty true population standard deviation actually . Maybe data just fluke…maybe true population standard deviation \\(11\\), instance. actually true, ran z-test assuming \\(\\sigma=11\\), result end non-significant. ’s problem, ’s one ’re going address.\nFigure 11.4: Graphical illustration null alternative hypotheses assumed (two sided) one sample t-test. Note similarity z-test (Figure 11.2). null hypothesis population mean \\(\\mu\\) equal specified value \\(\\mu_0\\), alternative hypothesis . Like z-test, assume data normally distributed, assume population standard deviation \\(\\sigma\\) known advance\n","code":""},{"path":"comparing-two-means.html","id":"introducing-the-t-test","chapter":"11 Comparing two means","heading":"11.2.1 Introducing the t-test","text":"ambiguity annoying, resolved 1908 guy called William Sealy Gosset (Student 1908), working chemist Guinness brewery time (see J. F. Box (1987)). Guinness took dim view employees publishing statistical analysis (apparently felt trade secret), published work pseudonym “Student” , day, full name t-test actually Student’s t-test. key thing Gosset figured accommodate fact aren’t completely sure true standard deviation .97 answer subtly changes sampling distribution. t-test test statistic, now called t-statistic, calculated exactly way mentioned . null hypothesis true mean µ, sample mean \\(\\bar{X}\\) estimate population standard deviation \\(\\hat{\\sigma}\\), t statistic :\\[t=\\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}\\]\nFigure 11.5: t distribution 2 degrees freedom (left) 10 degrees freedom (right), standard normal distribution (.e., mean 0 std dev 1) plotted dotted lines comparison purposes. Notice t distribution heavier tails (leptokurtic: higher kurtosis) normal distribution; effect quite exaggerated degrees freedom small, negligible larger values. words, large \\(df\\) t distribution essentially identical normal distribution\nthing changed equation instead using known true value \\(\\sigma\\), use estimate \\(\\hat{\\sigma}\\). estimate constructed N observations, sampling distribution turns t-distribution \\(N-1\\) degrees freedom (df). t distribution similar normal distribution, “heavier” tails, discussed earlier section useful distributions illustrated Figure 11.5. Notice, though, df gets larger, t-distribution starts look identical standard normal distribution. : sample size \\(N = 70,000,000\\) “estimate” standard deviation pretty much perfect, right? , expect large \\(N\\), t-test behave exactly way z-test. ’s exactly happens!","code":""},{"path":"comparing-two-means.html","id":"doing-the-test-in-jamovi-2","chapter":"11 Comparing two means","heading":"11.2.2 Doing the test in jamovi","text":"might expect, mechanics t-test almost identical mechanics z-test. ’s much point going tedious exercise showing calculations using low level commands. ’s pretty much identical calculations earlier, except use estimated standard deviation test hypothesis using t distribution rather normal distribution. instead going calculations tedious detail second time, ’ll jump straight showing t-tests actually done. jamovi comes dedicated analysis t-tests flexible (can run lots different kinds t-tests). ’s pretty straightforward use; need specify ‘Analyses’ - ‘T-Tests’ - ‘One Sample T-Test’, move variable interested (X) across ‘Variables’ box, type mean value null hypothesis (‘67.5’) ‘Hypothesis’ - ‘Test value’ box. Easy enough. See Figure 11.6, , amongst things get moment, gives t-test statistic = 2.25, 19 degrees freedom associated p-value \\(0.036\\).\nFigure 11.6: jamovi one-sample t-test\nAlso reported two things might care : 95% confidence interval measure effect size (’ll talk effect sizes later). seems straightforward enough. Now output? Well, since ’re pretending actually care toy example, ’re overjoyed discover result statistically significant (.e. p value .05). report result saying something like :mean grade \\(72.3\\), psychology students scored slightly higher average grade \\(67.5\\) (\\(t(19) = 2.25\\), \\(p = .036\\)); mean difference \\(4.80\\) \\(95\\%\\) confidence interval \\(0.34\\) \\(9.26\\).…\\(t(19)\\) shorthand notation t-statistic \\(19\\) degrees freedom. said, ’s often case people don’t report confidence interval, using much compressed form ’ve done . instance, ’s uncommon see confidence interval included part stat block reporting mean difference, like :\\[t(19)=2.25, p = .036, CI_{95} = [0.34, 9.26]\\] much jargon crammed half line, know must really smart.98","code":""},{"path":"comparing-two-means.html","id":"assumptions-of-the-one-sample-t-test","chapter":"11 Comparing two means","heading":"11.2.3 Assumptions of the one sample t-test","text":"Okay, assumptions one-sample t-test make? Well, since t-test basically z-test assumption known standard deviation removed, shouldn’t surprised see makes assumptions z-test, minus one known standard deviation. isNormality. ’re still assuming population distribution normal99, noted earlier, standard tools can use check see assumption met (Checking normality sample), tests can ’s place assumption violated (Testing non-normal data).Independence. , assume observations sample generated independently one another. See earlier discussion z-test specifics (Assumptions z-test).Overall, two assumptions aren’t terribly unreasonable, consequence onesample t-test pretty widely used practice way comparing sample mean hypothesised population mean.","code":""},{"path":"comparing-two-means.html","id":"the-independent-samples-t-test-student-test","chapter":"11 Comparing two means","heading":"11.3 The independent samples t-test (Student test)","text":"Although one sample t-test uses, ’s typical example t-test100. much common situation arises ’ve got two different groups observations. psychology, tends correspond two different groups participants, group corresponds different condition study. person study measure outcome variable interest, research question ’re asking whether two groups population mean. situation independent samples t-test designed .","code":""},{"path":"comparing-two-means.html","id":"the-data","chapter":"11 Comparing two means","heading":"11.3.1 The data","text":"Suppose 33 students taking Dr Harpo’s statistics lectures, Dr Harpo doesn’t grade curve. Actually, Dr Harpo’s grading bit mystery, don’t really know anything average grade class whole. two tutors class, Anastasia Bernadette. N1 = 15 students Anastasia’s tutorials, \\(N_2 = 18\\) Bernadette’s tutorials. research question ’m interested whether Anastasia Bernadette better tutor, doesn’t make much difference. Dr Harpo emails course grades, harpo.csv file. usual, ’ll load file jamovi look variables contains - three variables, ID, grade tutor. grade variable contains student’s grade, imported jamovi correct measurement level attribute, need change regarded continuous variable (see earlier section Changing data one level another). tutor variable factor indicates student’s tutor - either Anastasia Bernadette.can calculate means standard deviations, using ‘Exploration’ - ‘descriptives’ analysis, ’s nice little summary table (Table 11.2).Table 11.2:  Descriptives summary tableTo give detailed sense ’s going , ’ve plotted histograms (jamovi, using R) showing distribution grades tutors (Figure 11.7), well simpler plot showing means corresponding confidence intervals groups students (Figure 11.8).","code":""},{"path":"comparing-two-means.html","id":"introducing-the-test","chapter":"11 Comparing two means","heading":"11.3.2 Introducing the test","text":"independent samples t-test comes two different forms, Student’s Welch’s. original Student t-test, one ’ll describe section, simpler two relies much restrictive assumptions Welch t-test. Assuming moment want run two-sided test, goal determine whether two “independent samples” data drawn populations mean (null hypothesis) different means (alternative hypothesis). say “independent” samples, really mean ’s special relationship observations two samples. probably doesn’t make lot sense right now, clearer come talk paired samples t-test later . now, let’s just point experimental design participants randomly allocated one two groups, want compare two groups’ mean performance outcome measure, independent samples t-test (rather paired samples t-test) ’re .\nFigure 11.7: Histograms showing distribution grades students Anastasia’s (panel ) Bernadette’s (panel b) classes. Visually, suggest students Anastasia’s class may getting slightly better grades average, though also seem bit variable\n\nFigure 11.8: plots show mean grade students Anastasia’s Bernadette’s tutorials. Error bars depict \\(95\\%\\) confidence intervals around mean. Visually, look like ’s real difference groups, though ’s hard say sure\nOkay, let’s let \\(\\mu_1\\) denote true population mean group 1 (e.g., Anastasia’s students), \\(\\mu_2\\) true population mean group 2 (e.g., Bernadette’s students),101 usual ’ll let \\(\\bar{X_1}\\) \\(\\bar{X_2}\\) denote observed sample means groups. null hypothesis states two population means identical (\\(\\mu_1 = \\mu_2\\)) alternative (\\(\\mu_1 \\neq \\mu_2\\)) (Figure 11.9). Written mathematical-ese, :\nFigure 11.9: Graphical illustration null alternative hypotheses assumed Student t-test. null hypothesis assumes groups mean \\(\\mu\\), whereas alternative assumes different means \\(\\mu_1\\) \\(\\mu_2\\). Notice assumed population distributions normal, , although alternative hypothesis allows group different means, assumes standard deviation\n\\[H_0: \\mu_1=\\mu_2 \\] \\[H_0: \\mu_1 \\neq \\mu_2 \\] construct hypothesis test handles scenario start noting null hypothesis true, difference population means exactly zero, \\(\\mu_1-\\mu_2 = 0\\). consequence, diagnostic test statistic based difference two sample means. null hypothesis true, ’d expect \\(\\bar{X}_1 - \\bar{X}_2\\) pretty close zero. However, just like saw one-sample tests (.e., one-sample z-test one-sample t-test) precise exactly close zero difference . solution problem less one. calculate standard error estimate (SE), just like last time, divide difference means estimate. t-statistic form:\\[t=\\frac{\\bar{X_1}-\\bar{X_2}}{SE}\\]just need figure standard error estimate actually . bit trickier case either two tests ’ve looked far, need go lot carefully understand works.","code":""},{"path":"comparing-two-means.html","id":"a-pooled-estimate-of-the-standard-deviation","chapter":"11 Comparing two means","heading":"11.3.3 A “pooled estimate” of the standard deviation","text":"original “Student t-test”, make assumption two groups population standard deviation. , regardless whether population means , assume population standard deviations identical, \\(\\sigma_1 = \\sigma_2\\). Since ’re assuming two standard deviations , drop subscripts refer \\(\\sigma\\). estimate ? construct single estimate standard deviation two samples? answer , basically, average . Well, sort . Actually, take weighted average variance estimates, use pooled estimate variance. weight assigned sample equal number observations sample, minus 1.[Additional technical detail 102]","code":""},{"path":"comparing-two-means.html","id":"completing-the-test","chapter":"11 Comparing two means","heading":"11.4 Completing the test","text":"Regardless way want think , now pooled estimate standard deviation. now , ’ll drop silly p subscript, just refer estimate \\(\\hat{\\sigma}\\). Great. Let’s now go back thinking bloody hypothesis test, shall ? whole reason calculating pooled estimate knew helpful calculating standard error estimate. standard error ? one-sample t-test standard error sample mean, \\(se(\\bar{X})\\), since \\(se(\\bar{X}) = \\frac{\\sigma}{\\sqrt{N}}\\) ’s denominator t-statistic looked like. time around, however, two sample means. ’re interested , specifically, difference two \\(\\bar{X}_1-\\bar{X}_2\\) consequence, standard error need divide fact standard error difference means.[Additional technical detail 103]Just saw one-sample test, sampling distribution t-statistic t-distribution (shocking, isn’t ?) long null hypothesis true assumptions test met. degrees freedom, however, slightly different. usual, can think degrees freedom equal number data points minus number constraints. case, N observations (\\(N_1\\) sample 1, \\(N_2\\) sample 2), 2 constraints (sample means). total degrees freedom test \\(N - 2\\).","code":""},{"path":"comparing-two-means.html","id":"doing-the-test-in-jamovi-3","chapter":"11 Comparing two means","heading":"11.4.1 Doing the test in jamovi","text":"surprisingly, can run independent samples t-test easily jamovi. outcome variable test student grade, groups defined terms tutor class. probably won’t surprised jamovi go relevant analysis (‘Analyses’ - ‘T-Tests’ - ‘Independent Samples T-Test’) move grade variable across ‘Dependent Variables’ box, tutor variable across ‘Grouping Variable’ box, shown Figure 11.10.\nFigure 11.10: Independent t-test jamovi, options checked useful results\noutput familiar form. First, tells test run, tells name dependent variable used. reports test results. Just like last time test results consist t-statistic, degrees freedom, p-value. final section reports two things: gives confidence interval effect size. ’ll talk effect sizes later. confidence interval, however, talk now.’s pretty important clear confidence interval actually refers . confidence interval difference group means. example, Anastasia’s students average grade \\(74.53\\), Bernadette’s students average grade \\(69.06\\), difference two sample means \\(5.48\\). course difference population means might bigger smaller . confidence interval reported Figure 11.10 tells ’s replicated study , \\(95\\%\\) time true difference means lie \\(0.20\\) \\(10.76\\). Look back section Estimating confidence interval reminder confidence intervals mean.case, difference two groups significant (just barely), might write result using text like :mean grade Anastasia’s class \\(74.5\\%\\) (std dev = \\(9.0\\)), whereas mean Bernadette’s class \\(69.1\\%\\) (std dev = \\(5.8\\)). Student’s independent samples t-test showed \\(5.4\\%\\) difference significant \\((t(31) = 2.1, p<.05, CI_{95} = [0.2, 10.8], d = .74)\\), suggesting genuine difference learning outcomes occurred.Notice ’ve included confidence interval effect size stat block. People don’t always . bare minimum, ’d expect see t-statistic, degrees freedom p value. include something like minimum: \\(t(31) = 2.1, p< .05\\). statisticians way, everyone also report confidence interval probably effect size measure , useful things know. real life doesn’t always work way statisticians want make judgement based whether think help readers , ’re writing scientific paper, editorial standard journal question. journals expect report effect sizes, others don’t. Within scientific communities standard practice report confidence intervals, others . ’ll need figure audience expects. , just sake clarity, ’re taking class, default position ’s usually worth including effect size confidence interval.","code":""},{"path":"comparing-two-means.html","id":"positive-and-negative-t-values","chapter":"11 Comparing two means","heading":"11.4.2 Positive and negative t values","text":"moving talk assumptions t-test, ’s one additional point want make use t-tests practice. first one relates sign t-statistic (, whether positive number negative one). One common worry students start running first t-test often end negative values t-statistic don’t know interpret . fact, ’s uncommon two people working independently end results almost identical, except one person negative t values one positive t value. Assuming ’re running two-sided test p-values identical. closer inspection, students notice confidence intervals also opposite signs. perfectly okay. Whenever happens, ’ll find two versions results arise slightly different ways running t-test. ’s happening simple. t-statistic calculate always form\\[t=\\frac{\\text{mean 1-mean 2}}{SE}\\]“mean 1” larger “mean 2” t statistic positive, whereas “mean 2” larger t statistic negative. Similarly, confidence interval jamovi reports confidence interval difference “(mean 1) minus (mean 2)”, reverse ’d get calculating confidence interval difference “(mean 2) minus (mean 1)”.Okay, ’s pretty straightforward think , now consider t-test comparing Anastasia’s class Bernadette’s class. one call “mean 1” one call “mean 2”. ’s arbitrary. However, really need designate one “mean 1” one “mean 2”. surprisingly, way jamovi handles also pretty arbitrary. earlier versions book used try explain , gave , ’s really important honest can never remember . Whenever get significant t-test result, want figure mean larger one, don’t try figure looking t-statistic. bother ? ’s foolish. ’s easier just look actual group means since jamovi output actually shows !’s important thing. really doesn’t matter jamovi shows , usually try report t-statistic way numbers match text. Suppose want write report : Anastasia’s class higher grades Bernadette’s class. phrasing implies Anastasia’s group comes first, makes sense report t-statistic Anastasia’s class corresponded group 1. , write Anastasia’s class higher grades Bernadette’s class \\((t(31) = 2.1, p = .04)\\).(wouldn’t actually underline word “higher” real life, ’m just emphasise point “higher” corresponds positive t values). hand, suppose phrasing wanted use Bernadette’s class listed first. , makes sense treat class group 1, , write looks like : Bernadette’s class lower grades Anastasia’s class \\((t(31) = -2.1, p = .04)\\).’m talking one group “lower” scores time around, sensible use negative form t-statistic. just makes read cleanly.One last thing: please note can’t types test statistics. works t-tests, wouldn’t meaningful chi-square tests, F-tests indeed tests talk book. don’t -generalise advice! ’m really just talking t-tests nothing else!","code":""},{"path":"comparing-two-means.html","id":"assumptions-of-the-test","chapter":"11 Comparing two means","heading":"11.4.3 Assumptions of the test","text":"always, hypothesis test relies assumptions. ? Student t-test three assumptions, saw previously context one sample t-test (see Assumptions one sample t-test):Normality. Like one-sample t-test, assumed data normally distributed. Specifically, assume groups normally distributed. section Checking normality sample ’ll discuss test normality, Testing non-normal data ’ll discuss possible solutions.Independence. , assumed observations independently sampled. context Student test two aspects . Firstly, assume observations within sample independent one another (exactly one-sample test). However, also assume cross-sample dependencies. , instance, turns included participants experimental conditions study (e.g., accidentally allowing person sign different conditions), cross sample dependencies ’d need take account.Homogeneity variance (also called “homoscedasticity”). third assumption population standard deviation groups. can test assumption using Levene test, ’ll talk later book (Checking homogeneity variance assumption). However, ’s simple remedy assumption worried, ’ll talk next section.","code":""},{"path":"comparing-two-means.html","id":"the-independent-samples-t-test-welch-test","chapter":"11 Comparing two means","heading":"11.5 The independent samples t-test (Welch test)","text":"biggest problem using Student test practice third assumption listed previous section. assumes groups standard deviation. rarely true real life. two samples don’t means, expect standard deviation? ’s really reason expect assumption true. ’ll talk little bit can check assumption later crop different places, just t-test. right now ’ll talk different form t-test (Welch 1947) rely assumption. graphical illustration Welch t test assumes data shown Figure 11.11, provide contrast Student test version Figure 11.9. ’ll admit ’s bit odd talk cure talking diagnosis, happens Welch test can specified one ‘Independent Samples T-Test’ options jamovi, probably best place discuss .\nFigure 11.11: Graphical illustration null alternative hypotheses assumed Welch t-test. Like Student test (Figure 11.9) assume samples drawn normal population; alternative hypothesis longer requires two populations equal variance\nWelch test similar Student test. example, t-statistic use Welch test calculated much way Student test. , take difference sample means divide estimate standard error difference\\[t=\\frac{\\bar{X}_1-\\bar{X}_2}{SE(\\bar{X}_1-\\bar{X}_2)}\\]main difference standard error calculations different. two populations different standard deviations, ’s complete nonsense try calculate pooled standard deviation estimate, ’re averaging apples oranges.104[Additional technical detail 105]second difference Welch Student degrees freedom calculated different way. Welch test, “degrees freedom” doesn’t whole number , doesn’t correspond closely “number data points minus number constraints” heuristic ’ve using point.","code":""},{"path":"comparing-two-means.html","id":"doing-the-welch-test-in-jamovi","chapter":"11 Comparing two means","heading":"11.5.1 Doing the Welch test in jamovi","text":"tick check box Welch test analysis , gives (Figure 11.12).\nFigure 11.12: Results showing Welch test alongside default Student’s t-test jamovi\ninterpretation output fairly obvious. read output Welch’s test way Student’s test. ’ve got descriptive statistics, test results information. ’s pretty easy.Except, except…result isn’t significant anymore. ran Student test get significant effect, Welch test data set \\((t(23.02) = 2.03, p = .054)\\). mean? panic? sky burning? Probably . fact one test significant isn’t doesn’t mean much, especially since kind rigged data happen. general rule, ’s good idea go way try interpret explain difference p-value \\(.049\\) p-value \\(.051\\). sort thing happens real life, difference p-values almost certainly due chance. matter take little bit care thinking test use. Student test Welch test different strengths weaknesses. two populations really equal variances, Student test slightly powerful (lower Type II error rate) Welch test. However, don’t variances, assumptions Student test violated may able trust ; might end higher Type error rate. ’s trade . However, real life tend prefer Welch test, almost -one actually believes population variances identical.","code":""},{"path":"comparing-two-means.html","id":"assumptions-of-the-test-1","chapter":"11 Comparing two means","heading":"11.5.2 Assumptions of the test","text":"assumptions Welch test similar made Student t-test (see Assumptions test, except Welch test assume homogeneity variance. leaves assumption normality assumption independence. specifics assumptions Welch test Student test.","code":""},{"path":"comparing-two-means.html","id":"the-paired-samples-t-test","chapter":"11 Comparing two means","heading":"11.6 The paired-samples t-test","text":"Regardless whether ’re talking Student test Welch test, independent samples t-test intended used situation two samples , well, independent one another. situation arises naturally participants assigned randomly one two experimental conditions, provides poor approximation sorts research designs. particular, repeated measures design, participant measured (respect outcome variable) experimental conditions, suited analysis using independent samples t-tests. example, might interested whether listening music reduces people’s working memory capacity. end, measure person’s working memory capacity two conditions: music, without music. experimental design one, 106 participant appears groups. requires us approach problem different way, using paired samples t-test.","code":""},{"path":"comparing-two-means.html","id":"the-data-1","chapter":"11 Comparing two means","heading":"11.6.1 The data","text":"data set ’ll use time comes Dr Chico’s class.107 class students take two major tests, one early semester one later semester. hear tell , runs hard class, one students find challenging. argues setting hard assessments students encouraged work harder. theory first test bit “wake call” students. realise hard class really , ’ll work harder second test get better mark. right? test , let’s import chico.csv file jamovi. time jamovi good job import attributing measurement levels correctly. chico data set contains three variables: id variable identifies student class, grade_test1 variable records student grade first test, grade_test2 variable grades second test.look jamovi spreadsheet seem like class hard one (grades 50% 60%), look like ’s improvement first test second one.take quick look descriptive statistics, Figure 11.13, see impression seems supported. Across 20 students mean grade first test 57%, rises 58% second test. Although, given standard deviations 6.6% 6.4% respectively, ’s starting feel like maybe improvement just illusory; maybe just random variation. impression reinforced see means confidence intervals plotted Figure 11.14a. rely plot alone, looking wide confidence intervals , ’d tempted think apparent improvement student performance pure chance.\nFigure 11.13: Descriptives two grade test variables chico data set\nNevertheless, impression wrong. see , take look scatterplot grades test 1 grades test 2, shown Figure 11.14b. plot dot corresponds two grades given student. grade test 1 (x co-ordinate) equals grade test 2 (y co-ordinate), dot falls line. Points falling line students performed better second test. Critically, almost data points fall diagonal line: almost students seem improved grade, small amount. suggests looking improvement made student one test next treating raw data. , ’ll need create new variable improvement student makes, add chico data set. easiest way compute new variable, expression grade test2 - grade test1.\nFigure 11.14: Mean grade test 1 test 2, associated 95% confidence intervals (panel ). Scatterplot showing individual grades test 1 test 2 (panel b). Histogram showing improvement made student Dr Chico’s class (panel c). panel c, notice almost entire distribution zero: vast majority students improve performance first test second one\ncomputed new improvement variable can draw histogram showing distribution improvement scores, shown Figure 11.14c. look histogram, ’s clear real improvement . vast majority students scored higher test 2 test 1, reflected fact almost entire histogram zero.","code":""},{"path":"comparing-two-means.html","id":"what-is-the-paired-samples-t-test","chapter":"11 Comparing two means","heading":"11.6.2 What is the paired samples t-test?","text":"light previous exploration, let’s think construct appropriate t test. One possibility try run independent samples t-test using grade_test1 grade_test2 variables interest. However, clearly wrong thing independent samples t-test assumes particular relationship two samples. Yet clearly ’s true case repeated measures structure data. use language introduced last section, try independent samples t-test, conflating within subject differences (’re interested testing) subject variability ().solution problem obvious, hope, since already hard work previous section. Instead running independent samples t-test grade_test1 grade_test2, run one-sample t-test within-subject difference variable, improvement. formalise slightly, \\(X_{i1}\\) score -th participant obtained first variable, \\(X_{i2}\\) score person obtained second one, difference score :\\[D_i=X_{i1}-X_{i2}\\]Notice difference scores variable 1 minus variable 2 way around, want improvement correspond positive valued difference, actually want “test 2” “variable 1”. Equally, say \\(\\mu_D = \\mu_1 - \\mu_2\\) population mean difference variable. , convert hypothesis test, null hypothesis mean difference zero alternative hypothesis \\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]assuming ’re talking two-sided test . less identical way described hypotheses one-sample t-test. difference specific value null hypothesis predicts 0. t-statistic defined less way . let {D} denote mean difference scores, \\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\] \\[t=\\frac{\\bar{D}}{\\frac{\\hat{\\sigma}_D}{\\sqrt{N}}}\\]\\(\\hat{\\sigma}_D\\) standard deviation difference scores. Since just ordinary, one-sample t-test, nothing special , degrees freedom still \\(N - 1\\). ’s . paired samples t-test really isn’t new test . ’s one-sample t-test, applied difference two variables. ’s actually simple. reason merits discussion long one ’ve just gone need able recognise paired samples test appropriate, understand ’s better independent samples t test.","code":""},{"path":"comparing-two-means.html","id":"doing-the-test-in-jamovi-4","chapter":"11 Comparing two means","heading":"11.6.3 Doing the test in jamovi","text":"paired samples t-test jamovi? One possibility follow process outlined . , create “difference” variable run one sample t-test . Since ’ve already created variable called improvement, let’s see get, Figure 11.15.\nFigure 11.15: Results showing one sample t-test paired difference scores\noutput shown Figure 11.15 (obviously) formatted exactly last time used one-sample t-Test analysis, confirms intuition. ’s average improvement \\(1.4\\%\\) test 1 test 2, significantly different \\(0\\) \\((t(19) = 6.48, p< .001)\\).However, suppose ’re lazy don’t want go effort creating new variable. perhaps just want keep difference one-sample paired samples tests clear head. , can use jamovi ‘Paired Samples T-Test’ analysis, getting results shown Figure 11.16.\nFigure 11.16: Results showing paired sample t-test. Compare Figure 11.15\nnumbers identical come one sample test, course given paired samples t-test just one sample test hood.","code":""},{"path":"comparing-two-means.html","id":"one-sided-tests","chapter":"11 Comparing two means","heading":"11.7 One-sided tests","text":"introducing theory null hypothesis tests, mentioned situations ’s appropriate specify one-sided test (see section difference one sided two sided tests). far t-tests two-sided tests. instance, specified one sample t-test grades Dr Zeppo’s class null hypothesis true mean \\(67.5\\%\\). alternative hypothesis true mean greater less \\(67.5\\%\\). Suppose interested finding true mean greater \\(67.5\\%\\), interest whatsoever testing find true mean lower \\(67.5\\%\\). , null hypothesis true mean \\(67.5\\%\\) less, alternative hypothesis true mean greater \\(67.5\\%\\). jamovi, ‘One Sample T-Test’ analysis, can specify clicking ‘\\(>\\) Test Value’ option, ‘Hypothesis’. done , get results shown Figure 11.17.\nFigure 11.17: jamovi results showing ‘One Sample T-Test’ actual hypothesis one sided, .e. true mean greater \\(67.5\\%\\)\nNotice changes output saw last time. important fact actual hypothesis changed, reflect different test. second thing note although t-statistic degrees freedom changed, p-value . one-sided test different rejection region two-sided test. ’ve forgotten means, may find helpful read back chapter Hypothesis testing, difference one sided two sided tests particular. third thing note confidence interval different : now reports “one-sided” confidence interval rather two-sided one. two-sided confidence interval ’re trying find numbers b ’re confident , repeat study many times, \\(95\\%\\) time mean lie b. one-sided confidence interval, ’re trying find single number ’re confident \\(95\\%\\) time true mean greater (less selected Measure 1 < Measure 2 ‘Hypothesis’ section).’s one-sided one sample t-test. However, versions t-test can one-sided. independent samples t test, one-sided test ’re interested testing see group higher scores group B, interest finding group B higher scores group . Let’s suppose , Dr Harpo’s class, wanted see Anastasia’s students higher grades Bernadette’s. analysis, ‘Hypothesis’ options, specify ‘Group 1 > Group2’. get results shown Figure 11.18.\nFigure 11.18: jamovi results showing ‘Independent Samples t-Test’ actual hypothesis one sided, .e. Anastasia’s students higher grades Bernadette’s\n, output changes predictable way. definition alternative hypothesis changed, p-value changed, now reports one-sided confidence interval rather two-sided one.paired samples t-test? Suppose wanted test hypothesis grades go test 1 test 2 Dr Zeppo’s class, prepared consider idea grades go . jamovi specifying, ‘Hypotheses’ option, grade_test2 (‘Measure 1’ jamovi, copied first paired variables box) > grade test1 (‘Measure 2’ jamovi). get results shown Figure 11.19.\nFigure 11.19: jamovi results showing ‘Paired Samples T-Test’ actual hypothesis one sided, .e. grade test2 (‘Measure 1’) \\(>\\) grade test1 (‘Measure 2’)\nYet , output changes predictable way. hypothesis changed, p-value changed, confidence interval now one-sided.","code":""},{"path":"comparing-two-means.html","id":"effect-size-1","chapter":"11 Comparing two means","heading":"11.8 Effect size","text":"commonly used measure effect size t-test Cohen’s d (Cohen 1988). ’s simple measure principle, quite wrinkles start digging details. Cohen defined primarily context independent samples t-test, specifically Student test. context, natural way defining effect size divide difference means estimate standard deviation. words, ’re looking calculate something along lines :\\[d=\\frac{(\\text{mean 1})-(\\text{mean 2})}{\\text{std dev}}\\]suggested rough guide interpreting \\(d\\) Table 11.3.Table 11.3:  () rough guide interpreting Cohen's d. personal recommendation use blindly. d statistic natural interpretation . re-describes difference means number standard deviations separates means. generally good idea think means practical terms. contexts 'small' effect big practical importance. situations 'large' effect may interestingYou’d think pretty unambiguous, ’s . largely Cohen wasn’t specific thought used measure standard deviation (defence trying make broader point book, nitpick tiny details). discussed McGrath Meyer (2006), several different versions common usage, author tends adopt slightly different notation. sake simplicity (opposed accuracy), ’ll use d refer statistic calculate sample, use \\(\\delta\\) refer theoretical population effect. Obviously, mean several different things called d.suspicion time want Cohen’s d ’re running t-test, jamovi option calculate effect size different flavours t-test provides.","code":""},{"path":"comparing-two-means.html","id":"cohens-d-from-one-sample","chapter":"11 Comparing two means","heading":"11.8.1 Cohen’s d from one sample","text":"simplest situation consider one corresponding one-sample t-test. case, one sample mean \\(\\bar{X}\\) one (hypothesised) population mean \\(\\mu_0\\) compare . , ’s really one sensible way estimate population standard deviation. just use usual estimate \\(\\hat{\\sigma}\\). Therefore, end following way calculate \\(d\\)\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]look back results Figure 11.6, effect size value Cohen’s \\(d = 0.50\\). Overall, , psychology students Dr Zeppo’s class achieving grades (\\(mean = 72.3\\%\\)) .5 standard deviations higher level ’d expect (\\(67.5\\%\\)) performing level students. Judged Cohen’s rough guide, moderate effect size.","code":""},{"path":"comparing-two-means.html","id":"cohens-d-from-a-students-t-test","chapter":"11 Comparing two means","heading":"11.8.2 Cohen’s d from a Student’s t test","text":"majority discussions Cohen’s \\(d\\) focus situation analogous Student’s independent samples t test, ’s context story becomes messier, since several different versions \\(d\\) might want use situation. understand multiple versions \\(d\\), helps take time write formula corresponds true population effect size \\(\\delta\\). ’s pretty straightforward,\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\], usual, \\(\\mu_1\\) \\(\\mu_2\\) population means corresponding group 1 group 2 respectively, \\(\\sigma\\) standard deviation (populations). obvious way estimate \\(\\delta\\) exactly thing t-test , .e., use sample means top line pooled standard deviation estimate bottom line\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\\(\\hat{\\sigma}_p\\) exact pooled standard deviation measure appears t-test. commonly used version Cohen’s d applied outcome Student t-test, one provided jamovi. sometimes referred Hedges’ \\(g\\) statistic (Hedges 1981).However, possibilities ’ll briefly describe. Firstly, may reason want use one two groups basis calculating standard deviation. approach (often called Glass’ \\(\\triangle\\), pronounced delta) makes sense good reason treat one two groups purer reflection “natural variation” . can happen , instance, one two groups control group. Secondly, recall usual calculation pooled standard deviation divide \\(N - 2\\) correct bias sample variance. one version Cohen’s d correction omitted, instead divide \\(N\\). version makes sense primarily ’re trying calculate effect size sample rather estimating effect size population. Finally, version called Hedge’s g, based Hedges Olkin (1985), point small bias usual (pooled) estimation Cohen’s d.108In case, ignoring variations make use wanted, let’s look default version jamovi. Figure 11.10 Cohen’s \\(d = 0.74\\), indicating grade scores students Anastasia’s class , average, \\(0.74\\) standard deviations higher grade scores students Bernadette’s class. Welch test, estimated effect size (Figure 11.12).","code":""},{"path":"comparing-two-means.html","id":"cohens-d-from-a-paired-samples-test","chapter":"11 Comparing two means","heading":"11.8.3 Cohen’s d from a paired-samples test","text":"Finally, paired samples t-test? case, answer depends ’re trying . jamovi assumes want measure effect sizes relative distribution difference scores, measure d calculate :\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\\(\\hat{\\sigma}_D\\) estimate standard deviation differences. Figure 11.16 Cohen’s \\(d = 1.45\\), indicating time 2 grade scores , average, \\(1.45\\) standard deviations higher time 1 grade scores.version Cohen’s \\(d\\) gets reported jamovi ‘Paired Samples T-Test’ analysis. wrinkle figuring whether measure want . extent care practical consequences research, often want measure effect size relative original variables, difference scores (e.g., 1% improvement Dr Chico’s class time pretty small measured amount -student variation grades), case use versions Cohen’s d use Student Welch test. ’s straightforward jamovi; essentially change structure data spreadsheet view won’t go here109, Cohen’s d perspective quite different: \\(0.22\\) quite small assessed scale original variables.","code":""},{"path":"comparing-two-means.html","id":"checking-the-normality-of-a-sample","chapter":"11 Comparing two means","heading":"11.9 Checking the normality of a sample","text":"tests discussed far chapter assumed data normally distributed. assumption often quite reasonable, [Central limit theorem] tend ensure many real world quantities normally distributed. time suspect variable actually average lots different things, ’s pretty good chance normally distributed, least close enough normal can get away using t-tests. However, life doesn’t come guarantees, besides lots ways can end variables highly non-normal. example, time think variable actually minimum lots different things, ’s good chance end quite skewed. psychology, response time (RT) data good example . suppose lots things trigger response human participant, actual response occur first time one trigger events occurs.110 means RT data systematically non-normal. Okay, normality assumed tests, mostly always satisfied (least approximately) real world data, can check normality sample? section discuss two methods: QQ plots Shapiro-Wilk test.","code":""},{"path":"comparing-two-means.html","id":"qq-plots","chapter":"11 Comparing two means","heading":"11.9.1 QQ plots","text":"One way check whether sample violates normality assumption draw “QQ plot” (Quantile-Quantile plot). allows visually check whether ’re seeing systematic violations. QQ plot, observation plotted single dot. x coordinate theoretical quantile observation fall data normally distributed (mean variance estimated sample), y co-ordinate actual quantile data within sample. data normal, dots form straight line. instance, lets see happens generate data sampling normal distribution, drawing QQ plot. results shown Figure 11.20.\nFigure 11.20: Histogram (panel ) normal QQ plot (panel b) normal.data, normally distributed sample \\(100\\) observations. Shapiro-Wilk statistic associated data \\(W = .99\\), indicating significant departures normality detected (\\(p = .73)\\)\ncan see, data form pretty straight line; surprise given sampled normal distribution! contrast, look two data sets shown Figure 11.21. top panels show histogram QQ plot data set highly skewed: QQ plot curves upwards. lower panels show plots heavy tailed (.e., high kurtosis) data set: case QQ plot flattens middle curves sharply either end.\nFigure 11.21: top row, histogram (panel ) normal QQ plot (panel b) \\(100\\) observations skewed.data set. skewness data \\(1.94\\), reflected QQ plot curves upwards. consequence, Shapiro-Wilk statistic \\(W = .80\\), reflecting significant departure normality (\\(p< .001\\)). bottom row shows plots heavy tailed data set, consisting 100 observations. case heavy tails data produce high kurtosis (\\(2.80\\)), cause QQ plot flatten middle, curve away sharply either side. resulting Shapiro-Wilk statistic \\(W = .93\\), reflecting significant non-normality (\\(p<.001)\\)\n","code":""},{"path":"comparing-two-means.html","id":"shapiro-wilk-tests","chapter":"11 Comparing two means","heading":"11.9.2 Shapiro-Wilk tests","text":"QQ plots provide nice way informally check normality data, sometimes ’ll want something bit formal Shapiro-Wilk test (Shapiro Wilk 1965) probably ’re looking .111 ’d expect, null hypothesis tested set \\(N\\) observations normally distributed.[Additional technical detail 112]’s little hard explain maths behind \\(W\\) statistic, better idea give broad brush description behaves. Unlike test statistics ’ll encounter book, ’s actually small values \\(W\\) indicate departure normality. \\(W\\) statistic maximum value 1, occurs data look “perfectly normal”. smaller value \\(W\\) less normal data . However, sampling distribution \\(W\\), one standard ones discussed Introduction probability chapter fact complete pain arse work , depend sample size \\(N\\). give feel sampling distributions look like, ’ve plotted three Figure 11.22. Notice , sample size starts get large, sampling distribution becomes tightly clumped near \\(W = 1\\), consequence, larger samples \\(W\\) doesn’t much smaller 1 order test significant.\nFigure 11.22: Sampling distribution Shapiro-Wilk \\(W\\) statistic, null hypothesis data normally distributed, samples size 10, 20 50. Note small values \\(W\\) indicate departure normality\nget Shapiro-Wilk statistic jamovi t-tests, check option ‘Normality’ listed ‘Assumptions’. randomly sampled data (\\(N = 100\\)) used QQ plot, value Shapiro-Wilk normality test statistic \\(W = 0.99\\) p-value \\(0.69\\). , surprisingly, evidence data depart normality. reporting results Shapiro-Wilk test, (usual) make sure include test statistic \\(W\\) p value, though given sampling distribution depends heavily \\(N\\) probably politeness include \\(N\\) well.","code":""},{"path":"comparing-two-means.html","id":"example","chapter":"11 Comparing two means","heading":"11.9.3 Example","text":"meantime, ’s probably worth showing example happens QQ plot Shapiro-Wilk test data turn non-normal. , let’s look distribution AFL winning margins data, remember back chapter Descriptive statistics didn’t look like came normal distribution . ’s happens QQ plot (Figure 11.23).\nFigure 11.23: QQ plot showing non-normality AFL winning margins data\nrun Shapiro-Wilk test AFL margins data, get value Shapiro-Wilk normality test statistic \\(W = 0.94\\), p-value = \\(9.481\\)x\\(10^{-07}\\). Clearly significant effect!","code":""},{"path":"comparing-two-means.html","id":"testing-non-normal-data","chapter":"11 Comparing two means","heading":"11.10 Testing non-normal data","text":"Okay, suppose data turn pretty substantially non-normal, still want run something like t-test? situation occurs lot real life. AFL winning margins data, instance, Shapiro-Wilk test made clear normality assumption violated. situation want use Wilcoxon tests.Like t-test, Wilcoxon test comes two forms, one-sample two-sample, ’re used less exact situations corresponding t-tests. Unlike t-test, Wilcoxon test doesn’t assume normality, nice. fact, don’t make assumptions kind distribution involved. statistical jargon, makes nonparametric tests. avoiding normality assumption nice, ’s drawback: Wilcoxon test usually less powerful t-test (.e., higher Type II error rate). won’t discuss Wilcoxon tests much detail t-tests, ’ll give brief overview.","code":""},{"path":"comparing-two-means.html","id":"two-sample-mann-whitney-u-test","chapter":"11 Comparing two means","heading":"11.10.1 Two sample Mann-Whitney U test","text":"’ll start describing Mann-Whitney U test, since ’s actually simpler one sample version. Suppose ’re looking scores 10 people test. Since imagination now failed completely, let’s pretend ’s “test awesomeness” two groups people, “” “B”. ’m curious know group awesome. data included file awesome.csv, two variables apart usual ID variable: scores group.long ties (.e., people exact awesomeness score) test want surprisingly simple. construct table compares every observation group every observation group B. Whenever group datum larger, place check mark table (Table 11.4).Table 11.4:  Comparing observations group two-sample Mann-Whitney U testWe count number checkmarks. test statistic, W. 113 actual sampling distribution W somewhat complicated, ’ll skip details. purposes, ’s sufficient note interpretation W qualitatively interpretation \\(t\\) \\(z\\). , want two-sided test reject null hypothesis W large small, directional (.e., one-sided) hypothesis use one .jamovi, run ‘Independent Samples T-Test’ scores dependent variable. group grouping variable, options ‘tests’ check option ’Mann-Whitney \\(U^{’}\\), get results showing \\(U = 3\\) (.e., number checkmarks shown ), p-value = \\(0.05556\\).","code":""},{"path":"comparing-two-means.html","id":"one-sample-wilcoxon-test","chapter":"11 Comparing two means","heading":"11.10.2 One sample Wilcoxon test","text":"one sample Wilcoxon test (equivalently, paired samples Wilcoxon test)? Suppose ’m interested finding whether taking statistics class effect happiness students. data happiness.csv file. ’ve measured happiness student taking class taking class, change score difference two. Just like saw t-test, ’s fundamental difference paired-samples test using , versus onesample test using change scores. , simplest way think test construct tabulation. way time take change scores positive differences, tabulate complete sample. end table looks like Table 11.5.Table 11.5:  Comparing observations group one-sample Wilcoxon U testCounting tick marks time get test statistic \\(W = 7\\). , test two sided, reject null hypothesis W large small. far running jamovi goes, ’s pretty much ’d expect. one-sample version, specify ‘Wilcoxon rank’ option ‘Tests’ ‘One Sample T-Test’ analysis window. gives Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). shows, significant effect. Evidently, taking statistics class effect happiness. Switching paired samples version test won’t give us different answer, course; see Figure 11.24.\nFigure 11.24: jamovi screen showing results one sample paired sample Wilcoxon nonparametric tests\n","code":""},{"path":"comparing-two-means.html","id":"summary-9","chapter":"11 Comparing two means","heading":"11.11 Summary","text":"one-sample t-test used compare single sample mean hypothesised value population mean.independent samples t-test used compare means two groups, tests null hypothesis mean. comes two forms: independent samples t-test (Student test) assumes groups standard deviation, independent samples t-test (Welch test) .paired-samples t-test used two scores person, want test null hypothesis two scores mean. equivalent taking difference two scores person, running one sample t-test difference scores.One-sided tests perfectly legitimate long pre-planned (like tests!).Effect size calculations difference means can calculated via Cohen’s d statistic.Checking normality sample using QQ plots Shapiro-Wilk test.data non-normal, can use Mann-Whitney Wilcoxon tests instead t-tests Testing non-normal data.","code":""},{"path":"correlation-and-linear-regression.html","id":"correlation-and-linear-regression","chapter":"12 Correlation and linear regression","heading":"12 Correlation and linear regression","text":"goal chapter introduce correlation linear regression. standard tools statisticians rely analysing relationship continuous predictors continuous outcomes.","code":""},{"path":"correlation-and-linear-regression.html","id":"correlations","chapter":"12 Correlation and linear regression","heading":"12.1 Correlations","text":"section ’ll talk describe relationships variables data. , want talk mostly correlation variables. first, need data (Table 12.1).","code":""},{"path":"correlation-and-linear-regression.html","id":"the-data-2","chapter":"12 Correlation and linear regression","heading":"12.1.1 The data","text":"Table 12.1:  Data correlation analysis: descriptive statistics parenthood dataLet’s turn topic close every parent’s heart: sleep. data set ’ll use fictitious, based real events. Suppose ’m curious find much infant son’s sleeping habits affect mood. Let’s say can rate grumpiness precisely, scale 0 (grumpy) \\(100\\) (grumpy , grumpy old man woman). lets also assume ’ve measuring grumpiness, sleeping patterns son’s sleeping patterns quite time now. Let’s say, \\(100\\) days. , nerd, ’ve saved data file called parenthood.csv. load data can see file contains four variables dani.sleep, baby.sleep, dani.grump day. Note first load data set jamovi may guessed data type variable correctly, case fix : dani.sleep, baby.sleep, dani.grump day can specified continuous variables, ID nominal(integer) variable.114Next, ’ll take look basic descriptive statistics , give graphical depiction three interesting variables looks like, Figure 12.1 plots histograms. One thing note: just jamovi can calculate dozens different statistics doesn’t mean report . writing report, ’d probably pick statistics interest (readership), put nice, simple table like one Table 12.1.115 Notice put table, gave everything “human readable” names. always good practice. Notice also ’m getting enough sleep. isn’t good practice, parents tell ’s pretty standard.\nFigure 12.1: Histograms three interesting variables parenthood data set\n","code":""},{"path":"correlation-and-linear-regression.html","id":"the-strength-and-direction-of-a-relationship","chapter":"12 Correlation and linear regression","heading":"12.1.2 The strength and direction of a relationship","text":"can draw scatterplots give us general sense closely related two variables . Ideally though, might want say bit . instance, let’s compare relationship dani.sleep dani.grump (Figure 12.1, left) baby.sleep dani.grump (Figure 12.2, right). looking two plots side side, ’s clear relationship qualitatively cases: sleep equals less grump! However, ’s also pretty obvious relationship dani.sleep dani.grump stronger relationship baby.sleep dani.grump. plot left “neater” one right. feels like want predict mood , ’d help little bit know many hours son slept, ’d helpful know many hours slept.\nFigure 12.2: Scatterplots showing relationship dani.sleep dani.grump (left) relationship baby.sleep dani.grump (right)\ncontrast, let’s consider two scatterplots shown Figure 12.3. compare scatterplot “baby.sleep v dani.grump” (left) scatterplot “’baby.sleep v dani.sleep” (right), overall strength relationship , direction different. , son sleeps , get sleep (positive relationship, right hand side), sleeps get less grumpy (negative relationship, left hand side).\nFigure 12.3: Scatterplots showing relationship baby.sleep dani.grump (left), compared relationship baby.sleep dani.sleep (right)\n","code":""},{"path":"correlation-and-linear-regression.html","id":"the-correlation-coefficient","chapter":"12 Correlation and linear regression","heading":"12.1.3 The correlation coefficient","text":"can make ideas bit explicit introducing idea correlation coefficient (, specifically, Pearson’s correlation coefficient), traditionally denoted r. correlation coefficient two variables \\(X\\) \\(Y\\) (sometimes denoted \\(r_{XY}\\) ), ’ll define precisely next section, measure varies -1 1. \\(r = -1\\) means perfect negative relationship, \\(r = 1\\) means perfect positive relationship. \\(r = 0\\), ’s relationship . look Figure 12.4, can see several plots showing different correlations look like.[Additional technical detail 116]\nFigure 12.4: Illustration effect varying strength direction correlation. left hand column, correlations \\(0, .33, .66\\) \\(1\\). right hand column, correlations \\(0, -.33, -.66\\) \\(-1\\)\nstandardising covariance, keep nice properties covariance discussed earlier, actual values r meaningful scale: r = 1 implies perfect positive relationship \\(r = -1\\) implies perfect negative relationship. ’ll expand little point later, section Interpreting correlation. , let’s look calculate correlations jamovi.","code":""},{"path":"correlation-and-linear-regression.html","id":"calculating-correlations-in-jamovi","chapter":"12 Correlation and linear regression","heading":"12.1.4 Calculating correlations in jamovi","text":"Calculating correlations jamovi can done clicking ‘Regression’ - ‘Correlation Matrix’ button. Transfer four continuous variables across box right get output Figure 12.5.\nFigure 12.5: jamovi screenshot showing correlations variables parenthood.csv file\n","code":""},{"path":"correlation-and-linear-regression.html","id":"interpreting-a-correlation","chapter":"12 Correlation and linear regression","heading":"12.1.5 Interpreting a correlation","text":"Naturally, real life don’t see many correlations \\(1\\). interpret correlation , say, r = \\(.4\\)? honest answer really depends want use data , strong correlations field tend . friend mine engineering argued correlation less \\(.95\\) completely useless (think exaggerating, even engineering). hand, real cases, even psychology, really expect correlations strong. instance, one benchmark data sets used test theories people judge similarities clean theory can’t achieve correlation least \\(.9\\) really isn’t deemed successful. However, looking (say) elementary correlates intelligence (e.g., inspection time, response time), get correlation \\(.3\\) ’re well. short, interpretation correlation depends lot context. said, rough guide Table 12.2 pretty typical.Table 12.2:  rough guide interpreting correlations. Note say rough guide. hard fast rules counts strong weak relationships. depends context.However, something can never stressed enough always look scatterplot attaching interpretation data. correlation might mean think means. classic illustration “Anscombe’s Quartet” (Anscombe 1973), collection four data sets. data set two variables, \\(X\\) \\(Y\\). four data sets mean value \\(X\\) \\(9\\) mean \\(Y\\) \\(7.5\\). standard deviations \\(X\\) variables almost identical, Y variables. case correlation \\(X\\) \\(Y\\) \\(r = 0.816\\). can verify , since happen saved file called anscombe.csv.’d think four data sets look pretty similar one another. . draw scatterplots \\(X\\) \\(Y\\) four variables, shown Figure 12.6, see four spectacularly different . lesson , many people seem forget real life, “always graph raw data” (see Drawing graphs).\nFigure 12.6: Anscombe’s quartet. four data sets Pearson correlation r = .816, qualitatively different one another\n","code":""},{"path":"correlation-and-linear-regression.html","id":"spearmans-rank-correlations","chapter":"12 Correlation and linear regression","heading":"12.1.6 Spearman’s rank correlations","text":"Pearson correlation coefficient useful lot things, shortcomings. One issue particular stands : actually measures strength linear relationship two variables. words, gives measure extent data tend fall single, perfectly straight line. Often, pretty good approximation mean say “relationship”, Pearson correlation good thing calculate. Sometimes though, isn’t.One common situation Pearson correlation isn’t quite right thing use arises increase one variable \\(X\\) really reflected increase another variable Y , nature relationship isn’t necessarily linear. example might relationship effort reward studying exam. put zero effort (\\(X\\)) learning subject expect grade \\(0\\%\\) (\\(Y\\)). However, little bit effort cause massive improvement. Just turning lectures means learn fair bit, just turn classes scribble things grade might rise 35%, without lot effort. However, just don’t get effect end scale. everyone knows, takes lot effort get grade \\(90\\%\\) takes get grade \\(55\\%\\). means , ’ve got data looking study effort grades, ’s pretty good chance Pearson correlations misleading.illustrate, consider data plotted Figure 12.7, showing relationship hours worked grade received 10 students taking class. curious thing (highly fictitious) data set increasing effort always increases grade. might lot might little, increasing effort never decrease grade. run standard Pearson correlation, shows strong relationship hours worked grade received, correlation coefficient \\(0.91\\). However, doesn’t actually capture observation increasing hours worked always increases grade. ’s sense want able say correlation perfect somewhat different notion “relationship” . ’re looking something captures fact perfect ordinal relationship . , student 1 works hours student 2, can guarantee student 1 get better grade. ’s correlation \\(r = .91\\) says .\nFigure 12.7: relationship hours worked grade received toy data set consisting 10 students (circle corresponds one student). dashed line middle shows linear relationship two variables. produces strong Pearson correlation \\(r = .91\\). However, interesting thing note ’s actually perfect monotonic relationship two variables. toy example, increasing hours worked always increases grade received, illustrated solid line. reflected Spearman correlation \\(\\rho = 1\\). small data set, however, ’s open question version better describes actual relationship involved\naddress ? Actually, ’s really easy. ’re looking ordinal relationships treat data ordinal scale! , instead measuring effort terms “hours worked”, lets rank \\(10\\) students order hours worked. , student \\(1\\) least work anyone (\\(2\\) hours) get lowest rank (rank = \\(1\\)). Student \\(4\\) next laziest, putting \\(6\\) hours work whole semester, get next lowest rank (rank = \\(2\\)). Notice ’m using “rank =1” mean “low rank”. Sometimes everyday language talk “rank = \\(1\\)” mean “top rank” rather “bottom rank”. careful, can rank “smallest value largest value” (.e., small equals rank \\(1\\)) can rank “largest value smallest value” (.e., large equals rank 1). case, ’m ranking smallest largest, ’s really easy forget way set things put bit effort remembering!Okay, let’s look students rank worst best terms effort reward Table 12.3.Table 12.3:  Students ranked terms effort rewardHmm. identical. student put effort got best grade, student least effort got worst grade, etc. table shows, two rankings identical, now correlate get perfect relationship, correlation 1.0.’ve just re-invented Spearman’s rank order correlation, usually denoted \\(\\rho\\) distinguish Pearson correlation r. can calculate Spearman’s \\(\\rho\\) using jamovi simply clicking ‘Spearman’ check box ‘Correlation Matrix’ screen.","code":""},{"path":"correlation-and-linear-regression.html","id":"scatterplots","chapter":"12 Correlation and linear regression","heading":"12.2 Scatterplots","text":"Scatterplots simple effective tool visualising relationship two variables, like saw figures section Correlations. ’s latter application usually mind use term “scatterplot”. kind plot observation corresponds one dot. horizontal location dot plots value observation one variable, vertical location displays value variable. many situations don’t really clear opinions causal relationship (e.g., cause B, B cause , variable C control B). ’s case, doesn’t really matter variable plot x-axis one plot y-axis. However, many situations pretty strong idea variable think likely causal, least suspicions direction. , ’s conventional plot cause variable x-axis, effect variable y-axis. mind, let’s look draw scatterplots jamovi, using parenthood data set (.e. parenthood.csv) used introducing correlations.Suppose goal draw scatterplot displaying relationship amount sleep get (dani.sleep) grumpy next day (dani.grump). two different ways can use jamovi get plot ’re . first way use ‘Plot’ option ‘Regression’ - ‘Correlation Matrix’ button, giving us output shown Figure @ref(fig:fig12.8). Note jamovi draws line points, ’ll come onto bit later section [linear regression model]. Plotting scatterplot way also allow specify ‘Densities variables’ option adds density curve showing data variable distributed.\nFigure 12.8: Scatterplot via ‘Correlation Matrix’ command jamovi]\nsecond way use one jamovi add-modules. module called ‘scatr’ can install clicking large ‘\\(+\\)’ icon top right jamovi screen, opening jamovi library, scrolling find ‘scatr’ clicking ‘install’. done , find new ‘Scatterplot’ command available ‘Exploration’ button. plot bit different first way, see Figure 12.9, important information .\nFigure 12.9: Scatterplot via ‘scatr’ add-module - jamovi]\n","code":""},{"path":"correlation-and-linear-regression.html","id":"more-elaborate-options","chapter":"12 Correlation and linear regression","heading":"12.2.1 More elaborate options","text":"Often want look relationships several variables , using scatterplot matrix (jamovi via ‘Correlation Matrix’ - ‘Plot’ command). Just add another variable, example baby.sleep list variables correlated, jamovi create scatterplot matrix , just like one Figure 12.10.\nFigure 12.10: matrix scatterplots produced using jamovi\n","code":""},{"path":"correlation-and-linear-regression.html","id":"what-is-a-linear-regression-model","chapter":"12 Correlation and linear regression","heading":"12.3 What is a linear regression model?","text":"Stripped bare essentials, linear regression models basically slightly fancier version Pearson correlation (see Correlations), though ’ll see regression models much powerful tools.Since basic ideas regression closely tied correlation, ’ll return parenthood.csv file using illustrate correlations work. Recall , data set trying find Dani grumpy time working hypothesis ’m getting enough sleep. drew scatterplots help us examine relationship amount sleep get grumpiness following day, Figure 12.9, saw previously corresponds correlation \\(r = -.90\\), find secretly imagining something looks closer Figure 12.11a. , mentally draw straight line middle data. statistics, line ’re drawing called regression line. Notice , since ’re idiots, regression line goes middle data. don’t find imagining anything like rather silly plot shown Figure 12.11b.\nFigure 12.11: Panel shows sleep-grumpiness scatterplot Figure 12.9 best fitting regression line drawn top. surprisingly, line goes middle data. contrast, panel b shows data, poor choice regression line drawn top\nhighly surprising. line ’ve drawn Figure 12.11b doesn’t “fit” data well, doesn’t make lot sense propose way summarising data, right? simple observation make, turns powerful start trying wrap just little bit maths around . , let’s start refresher high school maths. formula straight line usually written like \\[y=+bx\\], least, ’s went high school years ago. two variables \\(x\\) \\(y\\), two coefficients, \\(\\) \\(b\\).117 coefficient represents y-intercept line, coefficient b represents slope line. Digging back decaying memories high school (sorry, us high school long time ago), remember intercept interpreted “value y get \\(x = 0\\)”. Similarly, slope b means increase x-value 1 unit, y-value goes b units, negative slope means y-value go rather . Ah yes, ’s coming back now. Now ’ve remembered come surprise discover use exact formula regression line. \\(Y\\) outcome variable (DV) X predictor variable (\\(IV\\)), formula describes regression written like \\[\\hat{Y}_i=b_0+b_1X_i\\]Hmm. Looks like formula, ’s extra frilly bits version. Let’s make sure understand . Firstly, notice ’ve written \\(X_i\\) \\(Y_i\\) rather just plain old \\(X\\) \\(Y\\) . want remember ’re dealing actual data. equation, \\(X_i\\) value predictor variable ith observation (.e., number hours sleep got day little study), \\(Y_i\\) corresponding value outcome variable (.e., grumpiness day). although haven’t said explicitly equation, ’re assuming formula works observations data set (.e., ). Secondly, notice wrote \\(\\hat{Y}_i\\) \\(Y_i\\) . want make distinction actual data \\(Y_i\\), estimate \\(\\hat{Y}_i\\) (.e., prediction regression line making). Thirdly, changed letters used describe coefficients \\(b\\) \\(b_0\\) \\(b_1\\). ’s just way statisticians like refer coefficients regression model. ’ve idea chose b, ’s . case \\(b_0\\) always refers intercept term, \\(b_1\\) refers slope.Excellent, excellent. Next, can’t help notice , regardless whether ’re talking good regression line bad one, data don’t fall perfectly line. , say another way, data \\(Y_i\\) identical predictions regression model \\(\\hat{Y}_i\\). Since statisticians love attach letters, names numbers everything, let’s refer difference model prediction actual data point residual, ’ll refer \\(\\epsilon_i\\).118 Written using mathematics, residuals defined \\[\\epsilon_i=Y_i-\\hat{Y}_i\\]turn means can write complete linear regression model \\[Y_i=b_0+b_1X_i+\\epsilon_i\\]","code":""},{"path":"correlation-and-linear-regression.html","id":"estimating-a-linear-regression-model","chapter":"12 Correlation and linear regression","heading":"12.4 Estimating a linear regression model","text":"Okay, now let’s redraw pictures time ’ll add lines show size residual observations. regression line good, residuals (lengths solid black lines) look pretty small, shown Figure 12.12a, regression line bad one residuals lot larger, can see looking Figure 12.12b. Hmm. Maybe “want” regression model small residuals. Yes, seem make sense. fact, think ’ll go far say “best fitting” regression line one smallest residuals. , better yet, since statisticians seem like take squares everything say :estimated regression coefficients, \\(\\hat{b}_0\\) \\(\\hat{b}_1\\), minimise sum squared residuals, either write \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) \\(\\sum_i \\epsilon_i^2\\).\nFigure 12.12: depiction residuals associated best fitting regression line (panel ), residuals associated poor regression line (panel b). residuals much smaller good regression line. , surprise given good line one goes right middle data\nYes, yes sounds even better. since ’ve indented like , probably means right answer. since right answer, ’s probably worth making note fact regression coefficients estimates (’re trying guess parameters describe population!), ’ve added little hats, get \\(\\hat{b}_0\\) \\(\\hat{b}_1\\) rather \\(b_0\\) \\(b_1\\). Finally, also note , since ’s actually one way estimate regression model, technical name estimation process ordinary least squares (OLS) regression.point, now concrete definition counts “best” choice regression coefficients, \\(\\hat{b}_0\\) \\(\\hat{b}_1\\). natural question ask next , optimal regression coefficients minimise sum squared residuals, find wonderful numbers? actual answer question complicated doesn’t help understand logic regression.119 time ’m going let hook. Instead showing long tedious way first “revealing” wonderful shortcut jamovi provides, let’s cut straight chase just use jamovi heavy lifting.","code":""},{"path":"correlation-and-linear-regression.html","id":"linear-regression-in-jamovi","chapter":"12 Correlation and linear regression","heading":"12.4.1 Linear regression in jamovi","text":"run linear regression, open ‘Regression’ - ‘Linear Regression’ analysis jamovi, using parenthood.csv data file. specify dani.grump ‘Dependent Variable’ dani.sleep variable entered ‘Covariates’ box. gives results shown Figure 12.13, showing intercept \\(\\hat{b}_0 = 125.96\\) slope \\(\\hat{b}_1 = -8.94\\). words, best fitting regression line plotted Figure 12.11 formula:\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\nFigure 12.13: jamovi screenshot showing simple linear regression analysis\n","code":""},{"path":"correlation-and-linear-regression.html","id":"interpreting-the-estimated-model","chapter":"12 Correlation and linear regression","heading":"12.4.2 Interpreting the estimated model","text":"important thing able understand interpret coefficients. Let’s start \\(\\hat{b}_1\\), slope. remember definition slope, regression coefficient \\(\\hat{b}_1 = -8.94\\) means increase Xi 1, ’m decreasing Yi 8.94. , additional hour sleep gain improve mood, reducing grumpiness 8.94 grumpiness points. intercept? Well, since \\(\\hat{b}_0\\) corresponds “expected value \\(Y_i\\) \\(X_i\\) equals 0”, ’s pretty straightforward. implies get zero hours sleep (\\(X_i = 0\\)) grumpiness go scale, insane value (\\(Y_i = 125.96\\)). Best avoided, think.","code":""},{"path":"correlation-and-linear-regression.html","id":"multiple-linear-regression","chapter":"12 Correlation and linear regression","heading":"12.5 Multiple linear regression","text":"simple linear regression model ’ve discussed point assumes ’s single predictor variable ’re interested , case dani.sleep. fact, point every statistical tool ’ve talked assumed analysis uses one predictor variable one outcome variable. However, many (perhaps ) research projects actually multiple predictors want examine. , nice able extend linear regression framework able include multiple predictors. Perhaps kind multiple regression model order?Multiple regression conceptually simple. add terms regression equation. Let’s suppose ’ve got two variables ’re interested ; perhaps want use dani.sleep baby.sleep predict dani.grump variable. , let \\(Y_{}\\) refer grumpiness -th day. now two $ X $ variables: first corresponding amount sleep got second corresponding amount sleep son got. ’ll let \\(X_{i1}\\) refer hours slept -th day \\(X_{i2}\\) refers hours baby slept day. , can write regression model like :\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\], \\(\\epsilon_i\\) residual associated -th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). model, now three coefficients need estimated: b0 intercept, b1 coefficient associated sleep, b2 coefficient associated son’s sleep. However, although number coefficients need estimated changed, basic idea estimation works unchanged: estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) \\(\\hat{b}_2\\) minimise sum squared residuals.","code":""},{"path":"correlation-and-linear-regression.html","id":"doing-it-in-jamovi","chapter":"12 Correlation and linear regression","heading":"12.5.1 Doing it in jamovi","text":"Multiple regression jamovi different simple regression. add additional variables ‘Covariates’ box jamovi. example, want use dani.sleep baby.sleep predictors attempt explain ’m grumpy, move baby.sleep across ‘Covariates’ box alongside dani.sleep. default, jamovi assumes model include intercept. coefficients get time shown Table 12.4.Table 12.4:  Adding multiple variables predictors regressionThe coefficient associated dani.sleep quite large, suggesting every hour sleep lose makes lot grumpier. However, coefficient baby.sleep small, suggesting doesn’t really matter much sleep son gets. matters far grumpiness goes much sleep get. get sense multiple regression model looks like, Figure 12.14 shows 3D plot plots three variables, along regression model .\nFigure 12.14: 3D visualisation multiple regression model. two predictors model, dani.sleep baby.sleep outcome variable dani.grump. Together, three variables form 3D space. observation (dot) point space. much way simple linear regression model forms line 2D space, multiple regression model forms plane 3D space. estimate regression coefficients ’re trying find plane close blue dots possible\nTechnical detail 120","code":""},{"path":"correlation-and-linear-regression.html","id":"quantifying-the-fit-of-the-regression-model","chapter":"12 Correlation and linear regression","heading":"12.6 Quantifying the fit of the regression model","text":"now know estimate coefficients linear regression model. problem , don’t yet know regression model good. example, regression.1 model claims every hour sleep improve mood quite lot, might just rubbish. Remember, regression model produces prediction \\(\\hat{Y}_i\\) mood like, actual mood \\(Y_i\\) . two close, regression model done good job. different, done bad job.","code":""},{"path":"correlation-and-linear-regression.html","id":"the-r2-value","chapter":"12 Correlation and linear regression","heading":"12.6.1 The \\(R^2\\) value","text":", let’s wrap little bit mathematics around . Firstly, ’ve got sum squared residuals\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]hope pretty small. Specifically, ’d like small comparison total variability outcome variable\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]’re , let’s calculate values , hand though. Let’s use something like Excel another standard spreadsheet programme. done opening parenthood.csv file Excel saving parenthood rsquared.xls can work . first thing calculate values, simple model uses single predictor following:create new column called ‘Y.pred’ using formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’calculate SS(resid) creating new column called ‘(Y-Y.pred)^2’ using formula ’ = (dani.grump - Y.pred)^2 ’., bottom column calculate sum values, .e. ’ sum( ( Y-Y.pred)^2 ) .bottom dani.grump column, calculate mean value dani.grump (NB Excel uses word ’ AVERAGE ’ rather ‘mean’ function).create new column, called ’ (Y - mean(Y))^2 )’ using formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’., bottom column calculate sum values, .e. ‘sum( (Y - mean(Y))^2 )’.Calculate R.squared typing blank cell following: ‘= 1 - (SS(resid) / SS(tot) )’.gives value \\(R^2\\) ‘0.8161018’. \\(R^2\\) value, sometimes called coefficient determination121 simple interpretation: proportion variance outcome variable can accounted predictor. , case fact obtained \\(R^2 = .816\\) means predictor (.sleep) explains \\(81.6\\%\\) variance outcome (.grump).Naturally, don’t actually need type commands Excel want obtain \\(R^2\\) value regression model. ’ll see later section Running hypothesis tests jamovi, need specify option jamovi. However, let’s put one side moment. ’s another property \\(R^2\\) want point .","code":""},{"path":"correlation-and-linear-regression.html","id":"the-relationship-between-regression-and-correlation","chapter":"12 Correlation and linear regression","heading":"12.6.2 The relationship between regression and correlation","text":"point can revisit earlier claim regression, simple form ’ve discussed far, basically thing correlation. Previously, used symbol \\(r\\) denote Pearson correlation. Might relationship value correlation coefficient \\(r\\) \\(R^2\\) value linear regression? course : squared correlation \\(r^2\\) identical \\(R^2\\) value linear regression single predictor. words, running Pearson correlation less equivalent running linear regression model uses one predictor variable.","code":""},{"path":"correlation-and-linear-regression.html","id":"the-adjusted-r2-value","chapter":"12 Correlation and linear regression","heading":"12.6.3 The adjusted \\(R^2\\) value","text":"One final thing point moving . ’s quite common people report slightly different measure model performance, known “adjusted \\(R^2\\)”. motivation behind calculating adjusted \\(R^2\\) value observation adding predictors model always cause \\(R^2\\) value increase (least decrease).Technical detail 122This adjustment attempt take degrees freedom account. big advantage adjusted \\(R^2\\) value add predictors model, adjusted \\(R^2\\) value increase new variables improve model performance ’d expect chance. big disadvantage adjusted \\(R^2\\) value can’t interpreted elegant way \\(R^2\\) can. \\(R^2\\) simple interpretation proportion variance outcome variable explained regression model. knowledge, equivalent interpretation exists adjusted \\(R^2\\).obvious question whether report \\(R^2\\) adjusted \\(R^2\\) . probably matter personal preference. care interpretability, \\(R^2\\) better. care correcting bias, adjusted \\(R^2\\) probably better. Speaking just , prefer \\(R^2\\). feeling ’s important able interpret measure model performance. Besides, ’ll see Hypothesis tests regression models, ’re worried improvement \\(R^2\\) get adding predictor just due chance ’s better model, well ’ve got hypothesis tests .","code":""},{"path":"correlation-and-linear-regression.html","id":"hypothesis-tests-for-regression-models","chapter":"12 Correlation and linear regression","heading":"12.7 Hypothesis tests for regression models","text":"far ’ve talked regression model , coefficients regression model estimated, quantify performance model (last , incidentally, basically measure effect size). next thing need talk hypothesis tests. two different (related) kinds hypothesis tests need talk : test whether regression model whole performing significantly better null model, test whether particular regression coefficient significantly different zero.","code":""},{"path":"correlation-and-linear-regression.html","id":"testing-the-model-as-a-whole","chapter":"12 Correlation and linear regression","heading":"12.7.1 Testing the model as a whole","text":"Okay, suppose ’ve estimated regression model. first hypothesis test might try null hypothesis relationship predictors outcome, alternative hypothesis data distributed exactly way regression model predicts.Technical detail 123We’ll see much F statistic chapter Comparing several means (one-way ANOVA), now just know can interpret large F values indicating null hypothesis performing poorly comparison alternative hypothesis. moment ’ll show test jamovi easy way, first let’s look tests individual regression coefficients.","code":""},{"path":"correlation-and-linear-regression.html","id":"tests-for-individual-coefficients","chapter":"12 Correlation and linear regression","heading":"12.7.2 Tests for individual coefficients","text":"F-test ’ve just introduced useful checking model whole performing better chance. regression model doesn’t produce significant result F-test probably don’t good regression model (, quite possibly, don’t good data). However, failing test pretty strong indicator model problems, passing test (.e., rejecting null) doesn’t imply model good! , might wondering? answer can found looking coefficients Multiple linear regression model already looked (Table 12.4)can’t help notice estimated regression coefficient baby.sleep variable tiny (\\(0.01\\)), relative value get dani.sleep (\\(-8.95\\)). Given two variables absolutely scale (’re measured “hours slept”), find illuminating. fact, ’m beginning suspect ’s really amount sleep get matters order predict grumpiness. can re-use hypothesis test discussed earlier, t-test. test ’re interested null hypothesis true regression coefficient zero (\\(b = 0\\)), tested alternative hypothesis isn’t (\\(b \\neq 0\\)). :\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]can test ? Well, central limit theorem kind us might able guess sampling distribution \\(\\hat{b}\\), estimated regression coefficient, normal distribution mean centred \\(b\\). mean null hypothesis true, sampling distribution \\(\\hat{b}\\) mean zero unknown standard deviation. Assuming can come good estimate standard error regression coefficient, \\(se(\\hat{b})\\), ’re luck. ’s exactly situation introduced one-sample t-test back Comparing two means chapter. let’s define t-statistic like \\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]’ll skip reasons , degrees freedom case \\(df = N - K - 1\\). Irritatingly, estimate standard error regression coefficient, \\(se(\\hat{b})\\), easy calculate standard error mean used simpler t-tests Comparing two means. fact, formula somewhat ugly, terribly helpful look .124 purposes ’s sufficient point standard error estimated regression coefficient depends predictor outcome variables, somewhat sensitive violations homogeneity variance assumption (discussed shortly).case, t-statistic can interpreted way t-statistics discussed Comparing two means. Assuming two-sided alternative (.e., don’t really care b \\(>\\) 0 b \\(<\\) 0), ’s extreme values t (.e., lot less zero lot greater zero) suggest reject null hypothesis.","code":""},{"path":"correlation-and-linear-regression.html","id":"running-the-hypothesis-tests-in-jamovi","chapter":"12 Correlation and linear regression","heading":"12.7.3 Running the hypothesis tests in jamovi","text":"compute statistics talked far, need make sure relevant options checked jamovi run regression. , Figure 12.15, get whole bunch useful output.\nFigure 12.15: jamovi screenshot showing multiple linear regression analysis, useful options checked\n‘Model Coefficients’ bottom jamovi analysis results shown 12.15 provides coefficients regression model. row table refers one coefficients regression model. first row intercept term, later ones look predictors. columns give relevant information. first column actual estimate \\(b\\) (e.g., \\(125.97\\) intercept, -8.95 dani.sleep predictor). second column standard error estimate \\(\\hat{\\sigma}_b\\). third fourth columns provide lower upper values 95% confidence interval around b estimate (later). fifth column gives t-statistic, ’s worth noticing table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, last column gives actual p-value tests.125The thing coefficients table doesn’t list degrees freedom used t-test, always \\(N - K - 1\\) listed table top output, labelled ‘Model Fit Measures’. can see table model performs significantly better ’d expect chance (\\(F(2,97) = 215.24, p< .001\\)), isn’t surprising: \\(R^2 = .81\\) value indicate regression model accounts \\(81\\%\\) variability outcome measure (\\(82\\%\\) adjusted \\(R^2\\) ). However, look back t-tests individual coefficients, pretty strong evidence baby.sleep variable significant effect. work model done dani.sleep variable. Taken together, results suggest regression model actually wrong model data. ’d probably better dropping baby.sleep predictor entirely. words, simple regression model started better model.","code":""},{"path":"correlation-and-linear-regression.html","id":"regarding-regression-coefficients","chapter":"12 Correlation and linear regression","heading":"12.8 Regarding regression coefficients","text":"moving discuss assumptions underlying linear regression can check ’re met, ’s two topics want briefly discuss, relate regression coefficients. first thing talk calculating confidence intervals coefficients. , ’ll discuss somewhat murky question determine predictor important.","code":""},{"path":"correlation-and-linear-regression.html","id":"confidence-intervals-for-the-coefficients","chapter":"12 Correlation and linear regression","heading":"12.8.1 Confidence intervals for the coefficients","text":"Like population parameter, regression coefficients b estimated complete precision sample data; ’s part need hypothesis tests. Given , ’s quite useful able report confidence intervals capture uncertainty true value \\(b\\). especially useful research question focuses heavily attempt find strongly variable \\(X\\) related variable \\(Y\\) , since situations interest primarily regression weight \\(b\\).Technical detail 126In jamovi already specified ‘95% Confidence interval’ shown Figure 12.15, although easily chosen another value, say ‘99% Confidence interval’ decided .","code":""},{"path":"correlation-and-linear-regression.html","id":"calculating-standardised-regression-coefficients","chapter":"12 Correlation and linear regression","heading":"12.8.2 Calculating standardised regression coefficients","text":"One thing might want calculate “standardised” regression coefficients, often denoted \\(\\beta\\). rationale behind standardised coefficients goes like . lot situations, variables fundamentally different scales. Suppose, example, regression model aims predict people’s \\(IQ\\) scores using educational attainment (number years education) income predictors. Obviously, educational attainment income scales. number years schooling might vary 10s years, whereas income can vary \\(10,000s\\) dollars (). units measurement big influence regression coefficients. b coefficients make sense interpreted light units, predictor variables outcome variable. makes difficult compare coefficients different predictors. Yet situations really want make comparisons different coefficients. Specifically, might want kind standard measure predictors strongest relationship outcome. standardised coefficients aim .basic idea quite simple; standardised coefficients coefficients obtained ’d converted variables z-scores running regression.127 idea , converting predictors z-scores, go regression scale, thereby removing problem variables different scales. Regardless original variables , \\(\\beta\\) value 1 means increase predictor 1 standard deviation produce corresponding 1 standard deviation increase outcome variable. Therefore, variable larger absolute value \\(\\beta\\) variable B, deemed stronger relationship outcome. least ’s idea. ’s worth little cautious , since rely heavily assumption “1 standard deviation change” fundamentally kind thing variables. ’s always obvious true.Technical detail 128To make things even simpler, jamovi option computes \\(\\beta\\) coefficients using ‘Standardized estimate’ checkbox ‘Model Coefficients’ options, see results Figure 12.16.\nFigure 12.16: Standardised coefficients, 95% confidence intervals, multiple linear regression\nresults clearly show dani.sleep variable much stronger effect baby.sleep variable. However, perfect example situation probably make sense use original coefficients b rather standardised coefficients \\(\\beta\\). , sleep baby’s sleep already scale: number hours slept. complicate matters converting z-scores?","code":""},{"path":"correlation-and-linear-regression.html","id":"assumptions-of-regression","chapter":"12 Correlation and linear regression","heading":"12.9 Assumptions of regression","text":"linear regression model ’ve discussing relies several assumptions. Model checking ’ll talk lot check assumptions met, first let’s look .Normality. Like many models statistics, basic simple multiple linear regression relies assumption normality. Specifically, assumes residuals normally distributed. ’s actually okay predictors \\(X\\) outcome \\(Y\\) non-normal, long residuals \\(\\epsilon\\) normal. See [Checking normality residuals] section.Linearity. pretty fundamental assumption linear regression model relationship \\(X\\) \\(Y\\) actually linear! Regardless whether ’s simple regression multiple regression, assume relationships involved linear.Homogeneity variance. Strictly speaking, regression model assumes residual \\(\\epsilon_i\\) generated normal distribution mean 0, (importantly current purposes) standard deviation σ every single residual. practice, ’s impossible test assumption every residual identically distributed. Instead, care standard deviation residual values \\(\\hat{Y}\\) , (’re especially paranoid) values every predictor \\(X\\) model.Uncorrelated predictors. idea , multiple regression model, don’t want predictors strongly correlated . isn’t “technically” assumption regression model, practice ’s required. Predictors strongly correlated (referred “collinearity”) can cause problems evaluating model. See Checking collinearity section.Residuals independent . really just “catch ” assumption, effect “’s nothing else funny going residuals”. something weird (e.g., residuals depend heavily unmeasured variable) going , might screw things .“bad” outliers. , actually technical assumption model (rather, ’s sort implied others), implicit assumption regression model isn’t strongly influenced one two anomalous data points raises questions adequacy model trustworthiness data cases. See section [Three kinds anamolous data].","code":""},{"path":"correlation-and-linear-regression.html","id":"model-checking","chapter":"12 Correlation and linear regression","heading":"12.10 Model checking","text":"main focus section regression diagnostics, term refers art checking assumptions regression model met, figuring fix model assumptions violated, generally check nothing “funny” going . refer “art” model checking good reason. ’s easy, lot fairly standardised tools can use diagnose maybe even cure problems ail model (, !), really need exercise certain amount judgement . ’s easy get lost details checking thing thing, ’s quite exhausting try remember different things . nasty side effect lot people get frustrated trying learn tools, instead decide model checking. bit worry!section describe several different things can check regression model ’s supposed . doesn’t cover full space things , ’s still much detailed see lot people practice, even don’t usually cover intro stats class either. However, think ’s important get sense tools disposal, ’ll try introduce bunch . Finally, note section draws quite heavily Fox Weisberg (\\(2011\\)) text, book associated car package used conduct regression analysis R. car package notable providing excellent tools regression diagnostics, book talks admirably clear fashion. don’t want sound gushy , think Fox et al. (\\(2011\\)) well worth reading, even advanced diagnostic techniques available R jamovi.","code":""},{"path":"correlation-and-linear-regression.html","id":"three-kinds-of-residuals","chapter":"12 Correlation and linear regression","heading":"12.10.1 Three kinds of residuals","text":"majority regression diagnostics revolve around looking residuals, now ’ve probably formed sufficiently pessimistic theory statistics able guess , precisely fact care lot residuals, several different kinds residual might consider. particular, following three kinds residuals referred section: “ordinary residuals”, “standardised residuals”, “Studentised residuals”. fourth kind ’ll see referred Figures, ’s “Pearson residual”. However, models ’re talking chapter Pearson residual identical ordinary residual.first simplest kind residuals care ordinary residuals. actual raw residuals ’ve talking throughout chapter far. ordinary residual just difference fitted value \\(\\hat{Y}_i\\) observed value \\(Y_i\\). ’ve using notation \\(\\epsilon_i\\) refer -th ordinary residual, gum ’m going stick . mind, simple equation\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]course saw earlier, unless specifically refer kind residual, one ’m talking . ’s nothing new . just wanted repeat . One drawback using ordinary residuals ’re always different scale, depending outcome variable good regression model . , unless ’ve decided run regression model without intercept term, ordinary residuals mean 0 variance different every regression. lot contexts, especially ’re interested pattern residuals actual values, ’s convenient estimate standardised residuals, normalised way standard deviation 1.Technical detail 129The third kind residuals Studentised residuals (also called “jackknifed residuals”) ’re even fancier standardised residuals. , idea take ordinary residual divide quantity order estimate standardised notion residual. 130Before moving , point don’t often need obtain residuals , even though heart almost regression diagnostics. time various options provide diagnostics, assumption checks, take care calculations . Even , ’s always nice know actually get hold things case ever need something non-standard.","code":""},{"path":"correlation-and-linear-regression.html","id":"three-kinds-of-anomalous-data","chapter":"12 Correlation and linear regression","heading":"12.10.2 Three kinds of anomalous data","text":"One danger can run linear regression models analysis might disproportionately sensitive smallish number “unusual” “anomalous” observations. discussed idea previously section [Using box plots detect outlieres] context discussing outliers get automatically identified boxplot option ‘Exploration’ - ‘Descriptives’, time need much precise. context linear regression, three conceptually distinct ways observation might called “anomalous”. three interesting, rather different implications analysis.first kind unusual observation outlier. definition outlier (context) observation different regression model predicts. example shown Figure 12.17. practice, operationalise concept saying outlier observation large Studentised residual, \\(\\epsilon_i^*\\). Outliers interesting: big outlier might correspond junk data, e.g., variables might recorded incorrectly data set, defect may detectable. Note shouldn’t throw observation away just ’s outlier. fact ’s outlier often cue look closely case try find ’s different.\nFigure 12.17: illustration outliers. dotted lines plot regression line estimated without anomalous observation included, corresponding residual (.e., Studentised residual). solid line shows regression line anomalous observation included. outlier unusual value outcome (y axis location) predictor (x axis location), lies long way regression line\nsecond way observation can unusual high leverage, happens observation different observations. doesn’t necessarily correspond large residual. observation happens unusual variables precisely way, can actually lie close regression line. example shown Figure 12.18. leverage observation operationalised terms hat value, usually written \\(h_i\\) . formula hat value rather complicated131 interpretation : \\(h_i\\) measure extent -th observation “control” regression line ends going.\nFigure 12.18: illustration high leverage points. anomalous observation case unusual terms predictor (x axis) outcome (y axis), unusualness highly consistent pattern correlations exists among observations. observation falls close regression line distort \ngeneral, observation lies far away ones terms predictor variables, large hat value (rough guide, high leverage hat value 2-3 times average; note sum hat values constrained equal \\(K + 1\\)). High leverage points also worth looking detail, ’re much less likely cause concern unless also outliers.brings us third measure unusualness, influence observation. high influence observation outlier high leverage. , observation different ones respect, also lies long way regression line. illustrated Figure 12.19. Notice contrast previous two figures. Outliers don’t move regression line much neither high leverage points. something outlier high leverage, well big effect regression line. ’s call points high influence, ’s ’re biggest worry. operationalise influence terms measure known Cook’s distance. 132\nFigure 12.19: illustration high influence points. case, anomalous observation highly unusual predictor variable (x axis), falls long way regression line. consequence, regression line highly distorted, even though (case) anomalous observation entirely typical terms outcome variable (y axis)\norder large Cook’s distance observation must fairly substantial outlier high leverage. rough guide, Cook’s distance greater 1 often considered large (’s typically use quick dirty rule).jamovi, information Cook’s distance can calculated clicking ‘Cook’s Distance’ checkbox ‘Assumption Checks’ - ‘Data Summary’ options. , multiple regression model using example chapter, get results shown Figure @ref(fig:fig12.20).\nFigure 12.20: jamovi output showing table Cook’s distance statistics\ncan see , example, mean Cook’s distance value \\(0.01\\), range \\(0.00000262\\) \\(0.11\\), way rule thumb figure mentioned Cook’s distance greater 1 considered large.obvious question ask next , large values Cook’s distance ? always, ’s hard fast rule. Probably first thing try running regression outlier greatest Cook’s distance133 excluded see happens model performance regression coefficients. really substantially different, ’s time start digging data set notes doubt scribbling ran study. Try figure point different. start become convinced one data point badly distorting results might consider excluding , ’s less ideal unless solid explanation particular case qualitatively different others therefore deserves handled separately.","code":""},{"path":"correlation-and-linear-regression.html","id":"checking-the-normality-of-the-residuals","chapter":"12 Correlation and linear regression","heading":"12.10.3 Checking the normality of the residuals","text":"Like many statistical tools ’ve discussed book, regression models rely normality assumption. case, assume residuals normally distributed. first thing can draw QQ-plot via ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot residuals’ option. output shown Figure 12.21, showing standardised residuals plotted function theoretical quantiles according regression model.\nFigure 12.21: Plot theoretical quantiles according model, quantiles standardised residuals, produced jamovi\nAnother thing check relationship fitted values residuals . can get jamovi using ‘Residuals Plots’ option, provides scatterplot predictor variable, outcome variable, fitted values residuals, see Figure 12.22. plots looking fairly uniform distribution ‘dots’, clear bunching patterning ‘dots’. Looking plots, nothing particularly worrying dots fairly evenly spread across whole plot. may little bit non-uniformity plot (b), strong deviation probably worth worrying .\nFigure 12.22: Residuals plots produced jamovi\nworried, lot cases solution problem (many others) transform one variables. discussed basics variable transformation Transforming recoding variable, want make special note one additional possibility didn’t explain fully earlier: Box-Cox transform. Box-Cox function fairly simple one ’s widely used. 134You can calculate using BOXCOX function ‘Compute’ variables screen jamovi.","code":""},{"path":"correlation-and-linear-regression.html","id":"checking-for-collinearity","chapter":"12 Correlation and linear regression","heading":"12.10.4 Checking for collinearity","text":"last kind regression diagnostic ’m going discuss chapter use variance inflation factors (VIFs), useful determining whether predictors regression model highly correlated . variance inflation factor associated predictor \\(X_k\\) model. 135The square root VIF pretty interpretable. tells much wider confidence interval corresponding coefficient bk , relative expected predictors nice uncorrelated one another. ’ve got two predictors, VIF values always going , can see click ‘Collinearity’ checkbox ‘Regression’ - ‘Assumptions’ options jamovi. dani.sleep baby.sleep VIF \\(1.65\\). since square root \\(1.65\\) \\(1.28\\), see correlation two predictors isn’t causing much problem.give sense end model bigger collinearity problems, suppose run much less interesting regression model, tried predict day data collected, function variables data set. see bit problem, let’s look correlation matrix four variables (Table 12.5).Table 12.5:  Correlation matrix four variablesWe fairly large correlations predictor variables! run regression model look VIF values, see collinearity causing lot uncertainty coefficients. First, run regression, Figure 12.23 can see VIF values , yep, ’s mighty fine collinearity .\nFigure 12.23: Collinearity statistics multiple regression, produced jamovi\n","code":""},{"path":"correlation-and-linear-regression.html","id":"model-selection","chapter":"12 Correlation and linear regression","heading":"12.11 Model selection","text":"One fairly major problem remains problem “model selection”. , data set contains several variables, ones include predictors, ones include? words, problem variable selection. general, model selection complex business ’s made somewhat simpler restrict problem choosing subset variables included model. Nevertheless, ’m going try covering even reduced topic lot detail. Instead, ’ll talk two broad principles need think , discuss one concrete tool jamovi provides help select subset variables include model. First, two principles:’s nice actual substantive basis choices. , lot situations researcher good reasons pick smallish number possible regression models theoretical interest. models sensible interpretation context field. Never discount importance . Statistics serves scientific process, way around.’s nice actual substantive basis choices. , lot situations researcher good reasons pick smallish number possible regression models theoretical interest. models sensible interpretation context field. Never discount importance . Statistics serves scientific process, way around.extent choices rely statistical inference, trade simplicity goodness fit. add predictors model make complex. predictor adds new free parameter (.e., new regression coefficient), new parameter increases model’s capacity “absorb” random variations. goodness fit (e.g., \\(R^2\\) ) continues rise, sometimes trivially chance, add predictors matter . want model able generalise well new observations need avoid throwing many variables.extent choices rely statistical inference, trade simplicity goodness fit. add predictors model make complex. predictor adds new free parameter (.e., new regression coefficient), new parameter increases model’s capacity “absorb” random variations. goodness fit (e.g., \\(R^2\\) ) continues rise, sometimes trivially chance, add predictors matter . want model able generalise well new observations need avoid throwing many variables.latter principle often referred Occam’s razor often summarised terms following pithy saying: multiply entities beyond necessity. context, means don’t chuck bunch largely irrelevant predictors just boost R2 . Hmm. Yeah, original better.case, need actual mathematical criterion implement qualitative principle behind Occam’s razor context selecting regression model. turns several possibilities. one ’ll talk Akaike information criterion (Akaike 1974) simply ’s available option jamovi. 136The smaller AIC value, better model performance. ignore low level details ’s fairly obvious AIC . left term increases model predictions get worse; right term increases model complexity increases. best model one fits data well (low residuals, left hand side) using predictors possible (low K, right hand side). short, simple implementation Ockham’s razor.AIC can added ‘Model Fit Measures’ output Table ‘AIC’ checkbox clicked, rather clunky way assessing different models seeing ‘AIC’ value lower remove one predictors regression model. way currently implemented jamovi, alternatives powerful programmes, R. alternative methods can automate process selectively removing (adding) predictor variables find best AIC. Although methods implemented jamovi, mention briefly just know .","code":""},{"path":"correlation-and-linear-regression.html","id":"backward-elimination","chapter":"12 Correlation and linear regression","heading":"12.11.1 Backward elimination","text":"backward elimination start complete regression model, including possible predictors. , “step” try possible ways removing one variables, whichever best (terms lowest AIC value) accepted. becomes new regression model, try possible deletions new model, choosing option lowest AIC. process continues end model lower AIC value possible models produce deleting one predictors.","code":""},{"path":"correlation-and-linear-regression.html","id":"forward-selection","chapter":"12 Correlation and linear regression","heading":"12.11.2 Forward selection","text":"alternative, can also try forward selection. time around start smallest possible model start point, consider possible additions model. However, ’s one complication. also need specify largest possible model ’re willing entertain .Although backward forward selection can lead conclusion, don’t always.","code":""},{"path":"correlation-and-linear-regression.html","id":"a-caveat","chapter":"12 Correlation and linear regression","heading":"12.11.3 A caveat","text":"Automated variable selection methods seductive things, especially ’re bundled (fairly) simple functions powerful statistical programmes. provide element objectivity model selection, ’s kind nice. Unfortunately, ’re sometimes used excuse thoughtlessness. longer think carefully predictors add model theoretical basis inclusion might . Everything solved magic AIC. start throwing around phrases like Ockham’s razor, well sounds like everything wrapped nice neat little package -one can argue ., perhaps . Firstly, ’s little agreement counts appropriate model selection criterion. taught backward elimination undergraduate, used F-tests , default method used software. ’ve described using AIC, since introductory text ’s method ’ve described, AIC hardly Word Gods Statistics. ’s approximation, derived certain assumptions, ’s guaranteed work large samples assumptions met. Alter assumptions get different criterion, like BIC instance (also available jamovi). Take different approach get NML criterion. Decide ’re Bayesian get model selection based posterior odds ratios. bunch regression specific tools haven’t mentioned. . different methods strengths weaknesses, easier calculate others (AIC probably easiest lot, might account popularity). Almost produce answers answer “obvious” ’s fair amount disagreement model selection problem becomes hard.mean practice? Well, go spend several years teaching theory model selection, learning ins outs finally decide personally think right thing . Speaking someone actually , wouldn’t recommend . ’ll probably come side even confused started. better strategy show bit common sense. ’re staring results automated backwards forwards selection procedure, model makes sense close smallest AIC narrowly defeated model doesn’t make sense, trust instincts. Statistical model selection inexact tool, said beginning, interpretability matters.","code":""},{"path":"correlation-and-linear-regression.html","id":"comparing-two-regression-models","chapter":"12 Correlation and linear regression","heading":"12.11.4 Comparing two regression models","text":"alternative using automated model selection procedures researcher explicitly select two regression models compare . can different ways, depending research question ’re trying answer. Suppose want know whether amount sleep son got relationship grumpiness, might expect amount sleep got. also want make sure day took measurement influence relationship. , ’re interested relationship baby.sleep dani.grump, perspective dani.sleep day nuisance variable covariates want control . situation, like know whether dani.grump ~ dani.sleep + day + baby .sleep (’ll call Model 2, M2) better regression model data dani.grump ~ dani.sleep + day (’ll call Model 1, M1). two different ways can compare two models, one based model selection criterion like AIC, based explicit hypothesis test. ’ll show AIC based approach first ’s simpler, follows naturally discussion last section. first thing need actually run two regressions, note AIC one, select model smaller AIC value judged better model data. Actually, don’t just yet. Read easy way jamovi get AIC values different models included one table.137A somewhat different approach problem comes hypothesis testing framework. Suppose two regression models, one (Model 1) contains subset predictors one (Model 2). , Model 2 contains predictors included Model 1, plus one additional predictors. happens say Model 1 nested within Model 2, possibly Model 1 submodel Model 2. Regardless terminology, means can think Model 1 null hypothesis Model 2 alternative hypothesis. fact can construct F test fairly straightforward fashion. 138Okay, ’s hypothesis test use compare two regression models one another. Now, jamovi? answer use ‘Model Builder’ option specify Model 1 predictors dani.sleep day ‘Block 1’ add additional predictor Model 2 (baby.sleep) ‘Block 2’, Figure 12.24. shows, ‘Model Comparisons’ Table, comparisons Model 1 Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since p > .05 retain null hypothesis (M1). approach regression, add covariates null model, add variables interest alternative model, compare two models hypothesis testing framework, often referred hierarchical regression.can also use ‘Model Comparison’ option display table shows AIC BIC model, making easy compare identify model lowest value, Figure 12.24.\nFigure 12.24: Model comparison jamovi using ‘Model Builder’ option\n","code":""},{"path":"correlation-and-linear-regression.html","id":"summary-10","chapter":"12 Correlation and linear regression","heading":"12.12 Summary","text":"Want know strong relationship two variables? Calculate CorrelationsDrawing ScatterplotsBasic ideas linear regression model? Estimating linear regression modelMultiple linear regressionQuantifying fit regression model using \\(R^2\\).Hypothesis tests regression modelsIn Regarding regression coefficients talked calculating Confidence intervals coefficients Calculating standardised regression coefficientsThe Assumptions regression Model checkingRegression Model selection","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"comparing-several-means-one-way-anova","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13 Comparing several means (one-way ANOVA)","text":"chapter introduces one widely used tools psychological statistics, known “analysis variance”, usually referred ANOVA. basic technique developed Sir Ronald Fisher early 20th century owe rather unfortunate terminology. term ANOVA little misleading, two respects. Firstly, although name technique refers variances, ANOVA concerned investigating differences means. Secondly, several different things referred ANOVAs, tenuous connection one another. Later book ’ll encounter range different ANOVA methods apply quite different situations, purposes chapter ’ll consider simplest form ANOVA, several different groups observations, ’re interested finding whether groups differ terms outcome variable interest. question addressed one-way ANOVA.structure chapter follows: first ’ll introduce fictitious data set ’ll use running example throughout chapter. introducing data, ’ll describe mechanics one-way ANOVA actually works (ANOVA works) focus can run one jamovi (Running ANOVA jamovi). two sections core chapter.remainder chapter discusses range important topics inevitably arise running ANOVA, namely calculate effect sizes, post hoc tests corrections multiple comparisons assumptions ANOVA relies upon. ’ll also talk check assumptions things can assumptions violated. ’ll cover repeated measures ANOVA.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"an-illustrative-data-set","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.1 An illustrative data set","text":"Suppose ’ve become involved clinical trial testing new antidepressant drug called Joyzepam. order construct fair test drug’s effectiveness, study involves three separate drugs administered. One placebo, existing antidepressant / anti-anxiety drug called Anxifree. collection 18 participants moderate severe depression recruited initial testing. drugs sometimes administered conjunction psychological therapy, study includes 9 people undergoing cognitive behavioural therapy (CBT) 9 . Participants randomly assigned (doubly blinded, course) treatment, 3 CBT people 3 -therapy people assigned 3 drugs. psychologist assesses mood person 3 month run drug, overall improvement person’s mood assessed scale ranging \\(-5\\) \\(+5\\). study design, let’s now load data file clinicaltrial.csv. can see data set contains three variables drug, therapy mood.gain.purposes chapter, ’re really interested effect drug mood.gain. first thing calculate descriptive statistics draw graphs. Descriptive statistics chapter showed , descriptive statistics can calculate jamovi shown Figure 13.1\nFigure 13.1: Descriptives mood gain, box plots drug administered\nplot makes clear, larger improvement mood participants Joyzepam group either Anxifree group placebo group. Anxifree group shows larger mood gain control group, difference isn’t large. question want answer difference “real”, just due chance?","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"how-anova-works","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.2 How ANOVA works","text":"order answer question posed clinical trial data ’re going run one-way ANOVA. ’m going start showing hard way, building statistical tool ground showing didn’t access cool built-ANOVA functions jamovi. hope ’ll read carefully, try long way twice make sure really understand ANOVA works, ’ve grasped concept never ever way .experimental design described previous section strongly suggests ’re interested comparing average mood change three different drugs. sense, ’re talking analysis similar t-test (see chapter Comparing two means involving two groups. let \\(\\mu_P\\) denote population mean mood change induced placebo, let \\(\\mu_A\\) \\(\\mu_J\\) denote corresponding means two drugs, Anxifree Joyzepam, (somewhat pessimistic) null hypothesis want test three population means identical. , neither two drugs effective placebo. can write null hypothesis :\\[H_0: \\text{ true } \\mu_P=\\mu_A=\\mu_J\\]consequence, alternative hypothesis least one three different treatments different others. ’s bit tricky write mathematically, (’ll discuss) quite different ways null hypothesis can false. now ’ll just write alternative hypothesis like :\\[H_1: \\text{ } \\underline{ \\text{ } } \\text{ true }\n\\mu_P=\\mu_A=\\mu_J\\]null hypothesis lot trickier test ones ’ve seen previously. shall ? sensible guess “ANOVA”, since ’s title chapter, ’s particularly clear “analysis variances” help us learn anything useful means. fact, one biggest conceptual difficulties people first encountering ANOVA. see works, find helpful start talking variances, specifically group variability within-group variability (Figure 13.2).\nFigure 13.2: Graphical illustration ‘groups’ variation (panel ) ‘within groups’ variation (panel b). left arrows show differences group means. right arrows highlight variability within group\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"two-formulas-for-the-variance-of-y","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.2.1 Two formulas for the variance of Y","text":"First, let’s start introducing notation. ’ll use G refer total number groups. data set three drugs, \\(G = 3\\) groups. Next, ’ll use \\(N\\) refer total sample size; total \\(N = 18\\) people data set. Similarly, let’s use \\(N_k\\) denote number people k-th group. fake clinical trial, sample size \\(N_k = 6\\) three groups.139 Finally, ’ll use Y denote outcome variable. case, Y refers mood change. Specifically, ’ll use Yik refer mood change experienced -th member k-th group. Similarly, ’ll use \\(\\bar{Y}\\) average mood change, taken across 18 people experiment, \\(\\bar{Y}_k\\) refer average mood change experienced 6 people group \\(k\\).Now ’ve got notation sorted can start writing formulas. start , let’s recall formula variance used section Measures variability, way back kinder days just descriptive statistics. sample variance Y defined follows \\[Var(Y)=\\frac{1}{N}\\sum_{k=1}^{G}\\sum_{=1}^{N_k}(Y_{ik}-\\bar{Y})^2\\] formula looks pretty much identical formula variance Measures variability. difference time around ’ve got two summations : ’m summing groups (.e., values \\(k\\)) people within groups (.e., values \\(\\)). purely cosmetic detail. ’d instead used notation \\(Y_p\\) refer value outcome variable person p sample, ’d single summation. reason double summation ’ve classified people groups, assigned numbers people within groups.concrete example might useful . Let’s consider Table 13.2, total \\(N = 5\\) people sorted \\(G = 2\\) groups. Arbitrarily, let’s say “cool” people group 1 “uncool” people group 2. turns three cool people (\\(N_1 = 3\\)) two uncool people (\\(N_2 = 2\\))Table 13.1:  Grumpiness people cool uncool groupsNotice ’ve constructed two different labelling schemes . “person” variable p perfectly sensible refer Yp grumpiness p-th person sample. instance, table shows Tim fourth ’d say \\(p = 4\\). , talking grumpiness \\(Y\\) “Tim” person, whoever might , refer grumpiness saying \\(Y_p = 91\\), person \\(p = 4\\) . However, ’s way refer Tim. alternative note Tim belongs “uncool” group (\\(k = 2\\)), fact first person listed uncool group (\\(= 1\\)). ’s equally valid refer Tim’s grumpiness saying \\(Y_{ik} = 91\\), \\(k = 2\\) \\(= 1\\).words, person p corresponds unique ik combination, formula gave actually identical original formula variance, \\[Var(Y)=\\frac{1}{N}\\sum_{p=1}^{N}(Y_p-\\bar{Y})^2\\] formulas, ’re summing observations sample. time just use simpler Yp notation; equation using \\(Y_p\\) clearly simpler two. However, ANOVA ’s important keep track participants belong groups, need use Yik notation .","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"from-variances-to-sums-of-squares","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.2.2 From variances to sums of squares","text":"Okay, now ’ve got good grasp variance calculated, let’s define something called total sum squares, denoted SStot. simple. Instead averaging squared deviations, calculating variance, just add .140When talk analysing variances context ANOVA, ’re really working total sums squares rather actual variance. 141Next, can define third notion variation captures differences groups. looking differences group means \\(\\bar{Y}_k\\) grand mean \\(\\bar{Y}\\). 142It’s difficult show total variation among people experiment (\\(SS_{tot}\\) actually sum differences groups \\(SS_b\\) variation inside groups \\(SS_w\\). ,\\[SS_w+SS_b=SS_{tot}\\] Yay.Okay, found ? ’ve discovered total variability associated outcome variable (\\(SS_{tot}\\)) can mathematically carved sum “variation due differences sample means different groups” (\\(SS_b\\)) plus “rest variation” (\\(SS_w\\)) 143.help find whether groups different population means? Um. Wait. Hold second. Now think , exactly looking . null hypothesis true ’d expect sample means pretty similar , right? imply ’d expect \\(SS_b\\) really small, least ’d expect lot smaller “variation associated everything else”, \\(SS_w\\). Hmm. detect hypothesis test coming .","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"from-sums-of-squares-to-the-f-test","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.2.3 From sums of squares to the F-test","text":"saw last section, qualitative idea behind ANOVA compare two sums squares values \\(SS_b\\) \\(SS_w\\) . -group variation \\(SS_b\\) large relative within-group variation \\(SS_w\\) reason suspect population means different groups aren’t identical . order convert workable hypothesis test, ’s little bit “fiddling around” needed. ’ll first show calculate test statistic, F ratio, try give feel way.order convert SS values F-ratio first thing need calculate degrees freedom associated \\(SS_b\\) \\(SS_w\\) values. usual, degrees freedom corresponds number unique “data points” contribute particular calculation, minus number “constraints” need satisfy. within-groups variability ’re calculating variation individual observations (\\(N\\) data points) around group means (\\(G\\) constraints). contrast, groups variability ’re interested variation group means (G data points) around grand mean (1 constraint). Therefore, degrees freedom :\\[df_b=G-1\\] \\[df_w=N-G\\]Okay, seems simple enough. next convert summed squares value “mean squares” value, dividing degrees freedom:\\[MS_b=\\frac{SS_b}{df_b}\\] \\[MS_w=\\frac{SS_w}{df_w}\\]Finally, calculate F-ratio dividing -groups MS within-groups MS:\\[F=\\frac{MS_b}{MS_w}\\]general level, intuition behind F statistic straightforward. Bigger values F means -groups variation large relative within-groups variation. consequence, larger value F evidence null hypothesis. large \\(F\\) order actually reject \\(H_0\\)? order understand , need slightly deeper understanding ANOVA mean squares values actually .next section discusses bit detail, readers aren’t interested details test actually measuring ’ll cut chase. order complete hypothesis test need know sampling distribution F null hypothesis true. surprisingly, sampling distribution F statistic null hypothesis \\(F\\) distribution. recall discussion F distribution chapter Introduction probability, \\(F\\) distribution two parameters, corresponding two degrees freedom involved. first one \\(df_1\\) groups degrees freedom \\(df_b\\), second one \\(df_2\\) within groups degrees freedom \\(df_w\\).Table 13.2:  key quantities involved ANOVA organised 'standard' ANOVA table. formulas quantities (except p-value ugly formula nightmarishly hard calculate without computer) shownA summary key quantities involved one-way ANOVA, including formulas showing calculated, shown Table 13.2.[Additional technical detail 144]","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"a-worked-example","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.2.4 A worked example","text":"previous discussion fairly abstract little technical side, think point might useful see worked example. , let’s go back clinical trial data introduced start chapter. descriptive statistics calculated beginning tell us group means: average mood gain \\(0.45\\) placebo, \\(0.72\\) Anxifree, \\(1.48\\) Joyzepam. mind, let’s party like ’s 1899 145 start pencil paper calculations. ’ll first \\(5\\) observations ’s bloody \\(1899\\) ’m lazy. Let’s start calculating \\(SS_w\\), within-group sums squares. First, let’s draw nice table help us calculations (Table 13.3)Table 13.3:  worked example...1At stage, thing ’ve included table raw data . , grouping variable (.e., drug) outcome variable (.e. mood.gain) person. Note outcome variable corresponds \\(\\bar{Y}_{ik}\\) value equation previously. next step calculation write , person study, corresponding group mean, \\(\\bar{Y}_k\\). slightly repetitive particularly difficult since already calculated group means descriptive statistics, see Table 13.4.Table 13.4:  worked example...2Now ’ve written , need calculate, every person, deviation corresponding group mean. , want subtract \\(Y_{ik} - \\bar{Y}_k\\). ’ve done , need square everything. , ’s get (Table 13.5)Table 13.5:  worked example...3The last step equally straightforward. order calculate within-group sum squares just add squared deviations across observations:\\[ \\begin{equation} \\begin{split} SS_w & = 0.0025 + 0.0225 + 0.1225 +\n0.0136 + 0.1003 \\\\ & = 0.2614 \\end{split} \\end{equation} \\]course, actually wanted get right answer ’d need 18 observations data set, just first five. continue pencil paper calculations wanted , ’s pretty tedious. Alternatively, ’s hard dedicated spreadsheet programme OpenOffice Excel. Try . one , Excel, file clinicaltrial_anova.xls. end within-group sum squares value \\(1.39\\).Okay. Now ’ve calculated within groups variation, \\(SS_w\\), ’s time turn attention -group sum squares, \\(SS_b\\). calculations case similar. main difference instead calculating differences observation Yik group mean \\(\\bar{Y}_k\\) observations, calculate differences group means \\(\\bar{Y}_k\\) grand mean \\(\\bar{Y}\\) (case \\(0.88\\)) groups (Table 13.6).Table 13.6:  worked example...4However, group calculations need multiply squared deviations \\(N_k\\), number observations group. every observation group (\\(N_k\\) ) associated group difference. six people placebo group placebo group mean differs grand mean \\(0.19\\), total group variation associated six people \\(6 \\times 0.19 = 1.14\\). extend little table calculations (Table 13.7).Table 13.7:  worked example...5And now group sum squares obtained summing “weighted squared deviations” three groups study:\\[ \\begin{equation}\n\\begin{split}\nSS_b & = 1.14 + 0.18 + 2.16 \\\\\n& = 3.48\n\\end{split}\n\\end{equation} \\]can see, group calculations lot shorter146. Now ’ve calculated sums squares values, \\(SS_b\\) \\(SS_w\\), rest ANOVA pretty painless. next step calculate degrees freedom. Since \\(G = 3\\) groups \\(N = 18\\) observations total degrees freedom can calculated simple subtraction:\\[\n\\begin{equation}\n\\begin{split}\ndf_b & = G-1 & = 2 \\\\\ndf_w & = N-G &= 15\n\\end{split}\n\\end{equation}\n\\]Next, since ’ve now calculated values sums squares degrees freedom, within-groups variability -groups variability, can obtain mean square values dividing one :\\[\n\\begin{equation}\n\\begin{split}\nMS_b & = \\frac{SS_b}{df_b} & = \\frac{3.48}{2} & = 1.74 \\\\\nMS_w & = \\frac{SS_w}{df_w} & = \\frac{1.39}{15} & = 0.09\n\\end{split}\n\\end{equation}\n\\]’re almost done. mean square values can used calculate F-value, test statistic ’re interested . dividing -groups MS value within-groups MS value.\\[\n\\begin{equation}\n\\begin{split}\nF & = \\frac{MS_b}{MS_w} & = \\frac{1.74}{0.09} \\\\\n& = 19.3\n\\end{split}\n\\end{equation}\n\\]Woohooo! terribly exciting, yes? Now test statistic, last step find whether test gives us significant result. discussed Hypothesis testing back “old days” ’d open statistics textbook flick back section actually huge lookup table find threshold \\(F\\) value corresponding particular value alpha (null hypothesis rejection region), e.g. \\(0.05\\), \\(0.01\\) \\(0.001\\), 2 15 degrees freedom. way give us threshold F value alpha \\(0.001\\) \\(11.34\\). less calculated \\(F\\) value say \\(p < 0.001\\). old days, nowadays fancy stats software calculates exact p-value . fact, exact p-value \\(0.000071\\). , unless ’re extremely conservative Type error rate, ’re pretty much guaranteed reject null hypothesis.point, ’re basically done. completed calculations, ’s traditional organise numbers ANOVA table like one Table 13.1. clinical trial data, ANOVA table look like Table 13.8.Table 13.8:  ANOVA results tableThese days, ’ll probably never much reason want construct one tables , find almost statistical software (jamovi included) tends organise output ANOVA table like , ’s good idea get used reading . However, although software output full ANOVA table, ’s almost never good reason include whole table write . pretty standard way reporting stats block result write something like :One-way ANOVA showed significant effect drug mood gain (F(2,15) = 19.3, p < .001).Sigh. much work one short sentence.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"running-an-anova-in-jamovi","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.3 Running an ANOVA in jamovi","text":"’m pretty sure know ’re thinking reading last section, especially followed advice pencil paper (.e., spreadsheet) . ANOVA calculations sucks. ’s quite lot calculations needed along way, tedious every time wanted ANOVA.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"using-jamovi-to-specify-your-anova","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.3.1 Using jamovi to specify your ANOVA","text":"make life easier , jamovi can ANOVA..hurrah! Go ‘ANOVA’ - ‘ANOVA’ analysis, move mood.gain variable across ‘Dependent Variable’ box, move drug variable across ‘Fixed Factors’ box. give results shown Figure 13.3. 147 Note also checked \\(\\eta^2\\) checkbox, pronounced “eta” squared, ‘Effect Size’ option also shown results table. come back effect sizes bit later.\nFigure 13.3: jamovi results table ANOVA mood gain drug administered\njamovi results table shows sums squares values, degrees freedom, couple quantities ’re really interested right now. Notice, however, jamovi doesn’t use names “-group” “within-group”. Instead, tries assign meaningful names. particular example, groups variance corresponds effect drug outcome variable, within groups variance corresponds “leftover” variability calls residuals. compare numbers numbers calculated hand worked example, can see ’re less , apart rounding errors. groups sums squares \\(SS_b = 3.45\\), within groups sums squares \\(SS_w = 1.39\\), degrees freedom \\(2\\) \\(15\\) respectively. also get F-value p-value , , less , give take rounding errors, numbers calculated long tedious way.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"effect-size-2","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.4 Effect size","text":"’s different ways measure effect size ANOVA, commonly used measures \\(\\eta^2\\) (eta squared) partial \\(\\eta^2\\). one way analysis variance ’re identical , moment ’ll just explain \\(\\eta^2\\) . definition \\(\\eta^2\\) actually really simple\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]’s . look ANOVA table Figure 13.3, see \\(SS_b = 3.45\\) \\(SS_tot = 3.45 + 1.39 = 4.84\\). Thus get \\(\\eta^2\\) value \\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]interpretation \\(\\eta^2\\) equally straightforward. refers proportion variability outcome variable (mood.gain) can explained terms predictor (drug). value \\(\\eta^2=0\\) means relationship two, whereas value \\(\\eta^2=1\\) means relationship perfect. Better yet, \\(\\eta^2\\) value closely related \\(R^2\\), discussed previously \\(R^2\\) value section, equivalent interpretation. Although many statistics text books suggest \\(\\eta^2\\) default effect size measure ANOVA, ’s interesting blog post Daniel Lakens suggesting eta-squared perhaps best measure effect size real world data analysis, can biased estimator. Usefully, also option jamovi specify omega-squared (\\(\\omega^2\\)), less biased, alongside eta-squared.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"multiple-comparisons-and-post-hoc-tests","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5 Multiple comparisons and post hoc tests","text":"time run ANOVA two groups end significant effect, first thing ’ll probably want ask groups actually different one another. drugs example, null hypothesis three drugs (placebo, Anxifree Joyzepam) exact effect mood. think , null hypothesis actually claiming three different things . Specifically, claims :competitor’s drug (Anxifree) better placebo (.e., \\(\\mu_A = \\mu_P\\) )drug (Joyzepam) better placebo (.e., \\(\\mu_J = \\mu_P\\) )Anxifree Joyzepam equally effective (.e., \\(\\mu_J = \\mu_A\\))one three claims false, null hypothesis also false. , now ’ve rejected null hypothesis, ’re thinking least one things isn’t true. ones? three propositions interest. Since certainly want know new drug Joyzepam better placebo, nice know well stacks existing commercial alternative (.e., Anxifree). even useful check performance Anxifree placebo. Even Anxifree already extensively tested placebos researchers, can still useful check study producing similar results earlier work.characterise null hypothesis terms three distinct propositions, becomes clear eight possible “states world” need distinguish (Table 13.9).Table 13.9:  null hypothesis eight possible 'states world'rejecting null hypothesis, ’ve decided don’t believe #1 true state world. next question ask , seven possibilities think right? faced situation, usually helps look data. instance, look plots Figure 13.1, ’s tempting conclude Joyzepam better placebo better Anxifree, ’s real difference Anxifree placebo. However, want get clearer answer , might help run tests.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"running-pairwise-t-tests","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5.1 Running “pairwise” t-tests","text":"might go solving problem? Given ’ve got three separate pairs means (placebo versus Anxifree, placebo versus Joyzepam, Anxifree versus Joyzepam) compare, run three separate t-tests see happens. easy jamovi. Go ANOVA ‘Post Hoc Tests’ options, move ‘drug’ variable across active box right, click ‘correction’ checkbox. produce neat table showing pairwise t-test comparisons amongst three levels drug variable, Figure 13.4\nFigure 13.4: Uncorrected pairwise t-tests post hoc comparisons jamovi\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"corrections-for-multiple-testing","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5.2 Corrections for multiple testing","text":"previous section hinted ’s problem just running lots lots t-tests. concern , running analyses, ’re going “fishing expedition”. ’re running lots lots tests without much theoretical guidance hope come significant. kind theory-free search group differences referred post hoc analysis (“post hoc” Latin “”).148It’s okay run post hoc analyses, lot care required. instance, analysis ran previous section avoided, individual t-test designed 5% Type error rate (.e., \\(\\alpha = .05\\)) ran three tests. Imagine happened ANOVA involved 10 different groups, decided run 45 “post hoc” t-tests try find ones significantly different , ’d expect 2 3 come significant chance alone. saw chapter Hypothesis testing, central organising principle behind null hypothesis testing seek control Type error rate, now ’m running lots t-tests order determine source ANOVA results, actual Type error rate across whole family tests gotten completely control.usual solution problem introduce adjustment p-value, aims control total error rate across family tests (see Shaffer (1995)). adjustment form, usually (always) applied one post hoc analysis, often referred correction multiple comparisons, though sometimes referred “simultaneous inference”. case, quite different ways adjustment. ’ll discuss section Post hoc tests section next chapter, aware many methods (see, e.g., Hsu (1996)).","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"bonferroni-corrections","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5.3 Bonferroni corrections","text":"simplest adjustments called Bonferroni correction (Dunn 1961), ’s simple indeed. Suppose post hoc analysis consists m separate tests, want ensure total probability making Type errors \\(\\alpha\\).149 , Bonferroni correction just says “multiply raw p-values m”. let \\(p\\) denote original p-value, let \\(p_j^{'}\\) corrected value, Bonferroni correction tells :\\[p_j^{'}=m \\times p\\]therefore, ’re using Bonferroni correction, reject null hypothesis \\(p_j^{'} < \\alpha\\). logic behind correction straightforward. ’re m different tests, arrange test Type error rate \\(\\frac{\\alpha}{m}\\), total Type error rate across tests larger \\(\\alpha\\). ’s pretty simple, much original paper, author writes:method given simple general sure must used . find , however, can conclude perhaps simplicity kept statisticians realizing good method situations (Dunn (1961), pp 52-53).use Bonferroni correction jamovi, just click ‘Bonferroni’ checkbox ‘Correction’ options, see another column added ANOVA results table showing adjusted p-values Bonferroni correction (Table 13.8. compare three p-values uncorrected, pairwise t-tests, clear thing jamovi done multiply \\(3\\).","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"holm-corrections","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5.4 Holm corrections","text":"Although Bonferroni correction simplest adjustment , ’s usually best one use. One method often used instead Holm correction (Holm 1979). idea behind Holm correction pretend ’re tests sequentially, starting smallest (raw) p-value moving onto largest one. j-th largest p-values, adjustment either\\[p_j^{'}=j \\times p_j\\](.e., biggest p-value remains unchanged, second biggest p-value doubled, third biggest p-value tripled, ), \\[p_j^{'}=p_{j+1}^{'}\\]whichever one larger. might sound little confusing, let’s go little slowly. ’s Holm correction . First, sort p-values order, smallest largest. smallest p-value multiply \\(m\\), ’re done. However, ones ’s two-stage process. instance, move second smallest p value, first multiply \\(m - 1\\). produces number bigger adjusted p-value got last time, keep . ’s smaller last one, copy last p-value. illustrate works, consider Table 13.10 shows calculations Holm correction collection five p-values.Table 13.10:  Holm corrected p valuesHopefully makes things clear.Although ’s little harder calculate, Holm correction nice properties. ’s powerful Bonferroni (.e., lower Type II error rate) , counter-intuitive might seem, Type error rate. consequence, practice ’s never reason use simpler Bonferroni correction since always outperformed slightly elaborate Holm correction. , Holm correction go multiple comparison correction. Figure 13.4 also shows Holm corrected p-values , can see, biggest p-value (corresponding comparison Anxifree placebo) unaltered. value .15, exactly value got originally applied correction . contrast, smallest p-value (Joyzepam versus placebo) multiplied three.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"writing-up-the-post-hoc-test","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.5.5 Writing up the post hoc test","text":"Finally, run post hoc analysis determine groups significantly different one another, might write result like :Post hoc tests (using Holm correction adjust p) indicated Joyzepam produced significantly larger mood change Anxifree (p = .001) placebo (\\((p = 9.0 \\times{10^{-5}}\\)). found evidence Anxifree performed better placebo (\\(p = .15\\))., don’t like idea reporting exact p-values, ’d change numbers \\(p < .01\\), \\(p < .001\\) \\(p > .05\\) respectively. Either way, key thing indicate used Holm’s correction adjust p-values. course, ’m assuming elsewhere write ’ve included relevant descriptive statistics (.e., group means standard deviations), since p-values aren’t terribly informative.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"assumptions-of-one-way-anova","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6 Assumptions of one-way ANOVA","text":"Like statistical test, analysis variance relies assumptions data, specifically residuals. three key assumptions need aware : normality, homogeneity variance independence.[Additional technical detail 150], check whether assumption residuals accurate? Well, indicated , three distinct claims buried one statement, ’ll consider separately.Homogeneity variance. Notice ’ve got one value population standard deviation (.e., \\(\\sigma\\)), rather allowing group ’s value (.e., \\(\\sigma_k\\)). referred homogeneity variance (sometimes called homoscedasticity) assumption. ANOVA assumes population standard deviation groups. ’ll talk extensively Checking homogeneity variance assumption section.Normality. residuals assumed normally distributed. saw Checking normality sample section, can assess looking QQ plots (running Shapiro-Wilk test. ’ll talk ANOVA context Checking normality assumption section.Independence. independence assumption little trickier. basically means , knowing one residual tells nothing residual. \\(\\epsilon_{ik}\\) values assumed generated without “regard ” “relationship ” ones. ’s obvious simple way test , situations clear violations . instance, repeated measures design, participant study appears one condition, independence doesn’t hold. ’s special relationship observations, namely correspond person! happens, need use something like Repeated measures one-way ANOVA.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"checking-the-homogeneity-of-variance-assumption","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.1 Checking the homogeneity of variance assumption","text":"make preliminary test variances rather like putting sea rowing boat find whether conditions sufficiently calm ocean liner leave port!\n– George Box (G. E. P. Box 1953)’s one way skin cat, saying goes, one way test homogeneity variance assumption, (though reason -one made saying ). commonly used test ’ve seen literature Levene test (Levene 1960), closely related Brown-Forsythe test (Brown Forsythe 1974).Regardless whether ’re standard Levene test Brown-Forsythe test, test statistic, sometimes denoted \\(F\\) also sometimes written \\(W\\), calculated exactly way F-statistic regular ANOVA calculated, just using \\(Z_{ik}\\) rather \\(Y_{ik}\\). mind, can go look run test jamovi.[Additional technical detail 151]","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"running-the-levene-test-in-jamovi","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.2 Running the Levene test in jamovi","text":"Okay, run Levene test? Simple really - ANOVA ‘Assumption Checks’ option, just click ‘Homogeneity tests’ checkbox. look output, shown Figure 13.5, see test non-significant (\\(F_{2,15} = 1.45, p = .266\\)), looks like homogeneity variance assumption fine. However, looks can deceptive! sample size pretty big, Levene test show significant effect (.e. p < .05) even homogeneity variance assumption violated extent troubles robustness ANOVA. point George Box making quote . Similarly, sample size quite small, homogeneity variance assumption might satisfied yet Levene test non-significant (.e. p > .05). means , alongside statistical test assumption met, always plot standard deviation around means group / category analysis…just see look fairly similar (.e. homogeneity variance) .\nFigure 13.5: Levene test output one-way ANOVA jamovi\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"removing-the-homogeneity-of-variance-assumption","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.3 Removing the homogeneity of variance assumption","text":"example, homogeneity variance assumption turned pretty safe one: Levene test came back non-significant (notwithstanding also look plot standard deviations), probably don’t need worry. However, real life aren’t always lucky. save ANOVA homogeneity variance assumption violated? recall discussion t-tests, ’ve seen problem . Student t-test assumes equal variances, solution use Welch t-test, . fact, Welch (1951) also showed can solve problem ANOVA (Welch one-way test). ’s implemented jamovi using One-Way ANOVA analysis. specific analysis approach just one-way ANOVA, run Welch one-way ANOVA example, re-run analysis previously, time use jamovi ANOVA - One Way ANOVA analysis command, check option Welch’s test (see Figure 13.6. understand ’s happening , let’s compare numbers got earlier Running ANOVA jamovi originally. save trouble flicking back, got last time: \\(F(2, 15) = 18.611, p = .00009\\), also shown Fisher’s test One-Way ANOVA shown Figure 13.6.\nFigure 13.6: Welch’s test part One Way ANOVA analysis jamovi\nOkay, originally ANOVA gave us result \\(F(2, 15) = 18.6\\), whereas Welch one way test gave us \\(F(2, 9.49) = 26.32\\). words, Welch test reduced within-groups degrees freedom 15 9.49, F-value increased 18.6 26.32.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"checking-the-normality-assumption","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.4 Checking the normality assumption","text":"Testing normality assumption relatively straightforward. covered need know section Checking normality sample. thing really need draw QQ plot , addition available, run Shapiro-Wilk test. QQ plot shown Figure 13.7 looks pretty normal . Shapiro-Wilk test significant (.e. \\(p > .05\\)) indicates assumption normality violated. However, Levene’s test, sample size large significant Shapiro-Wilk test may fact false positive, assumption normality violated substantive problematic sense analysis. , similarly, small sample can produce false negatives. ’s visual inspection QQ plot important.Alongside inspecting QQ plot deviations normality, Shapiro-Wilk test data show non-significant effect, p = 0.6053 (see Figure 13.6. therefore supports QQ plot assessment; checks find indication normality violated.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"removing-the-normality-assumption","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.5 Removing the normality assumption","text":"Now ’ve seen check normality, led naturally ask can address violations normality. context one-way ANOVA, easiest solution probably switch non-parametric test (.e., one doesn’t rely particular assumption kind distribution involved). ’ve seen non-parametric tests , chapter Comparing two means. two groups, Mann-Whitney Wilcoxon test provides non-parametric alternative need. ’ve got three groups, can use Kruskal-Wallis rank sum test (Kruskal Wallis 1952). ’s test ’ll talk next.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"the-logic-behind-the-kruskal-wallis-test","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.6 The logic behind the Kruskal-Wallis test","text":"Kruskal-Wallis test surprisingly similar ANOVA, ways. ANOVA started \\(Y_{ik}\\), value outcome variable ith person kth group. Kruskal Wallis test ’ll rank order \\(Y_{ik}\\) values conduct analysis ranked data. 152","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"additional-details","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.7 Additional details","text":"description previous section illustrates logic behind Kruskal-Wallis test. conceptual level, right way think test works. [^13-comparing-several-means-one-way-anova-17]wait, ’s ! Dear lord, always ? story ’ve told far actually true ties raw data. , two observations exactly value. ties, introduce correction factor calculations. point ’m assuming even diligent reader stopped caring (least formed opinion tie-correction factor something doesn’t require immediate attention). ’ll quickly tell ’s calculated, omit tedious details ’s done way. Suppose construct frequency table raw data, let fj number observations j-th unique value. might sound bit abstract, ’s concrete example frequency table mood.gain clinicaltrials.csv data set (Table 13.11)Table 13.11:  Frequency table mood gain clinicaltrials.csv dataLooking table, notice third entry frequency table value 2. Since corresponds mood.gain 0.3, table telling us two people’s mood increased 0.3. 153And jamovi uses tie-correction factor calculate tie-corrected Kruskall-Wallis statistic. long last, ’re actually finished theory Kruskal-Wallis test. ’m sure ’re terribly relieved ’ve cured existential anxiety naturally arises realise don’t know calculate tie-correction factor Kruskal-Wallis test. Right?\nFigure 13.7: QQ plot produced jamovi one way ANOVA options\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"how-to-run-the-kruskal-wallis-test-in-jamovi","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.6.8 How to run the Kruskal-Wallis test in jamovi","text":"Despite horror ’ve gone trying understand Kruskal Wallis test actually , turns running test pretty painless, since jamovi analysis part ANOVA analysis set called ‘Non-Parametric’ - ‘One Way ANOVA (Kruskall-Wallis)’ time ’ll data like clinicaltrial.csv data set, outcome variable mood.gain grouping variable drug. , can just go ahead run analysis jamovi. gives us Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\), Figure 13.8\nFigure 13.8: Kruskall-Wallis one-way non-parametric ANOVA jamovi\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"repeated-measures-one-way-anova","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.7 Repeated measures one-way ANOVA","text":"one-way repeated measures ANOVA test statistical method testing significant differences three groups participants used group (participant closely matched participants experimental groups). reason, always equal number scores (data points) experimental group. type design analysis can also called ‘related ANOVA’ ‘within subjects ANOVA’.logic behind repeated measures ANOVA similar independent ANOVA (sometimes called ‘-subjects’ ANOVA). ’ll remember earlier showed -subjects ANOVA total variability partitioned -groups variability (\\(SS_b\\)) within-groups variability (\\(SS_w\\)), divided respective degrees freedom give MSb MSw (see Table 13.1) F-ratio calculated :\\[F=\\frac{MS_b}{MS_w}\\]repeated measures ANOVA, F-ratio calculated similar way, whereas independent ANOVA within-group variability (\\(SS_w\\)) used basis \\(MS_w\\) denominator, repeated measures ANOVA \\(SS_w\\) partioned two parts. using subjects group, can remove variability due individual differences subjects (referred SSsubjects) within-groups variability. won’t go much technical detail done, essentially subject becomes level factor called subjects. variability within-subjects factor calculated way -subjects factor. can subtract SSsubjects \\(SS_w\\) provide smaller SSerror term:\\[\\text{Independent ANOVA: } SS_{error} = SS_w\\] \\[\\text{Repeated Measures ANOVA: } SS_{error} = SS_w - SS_{subjects}\\]\nchange \\(SS_{error}\\) term often leads powerful statistical test, depend whether reduction \\(SS_{error}\\) compensates reduction degrees freedom error term (degrees freedom go \\((n - k)\\) 154 \\((n - 1)(k - 1)\\) (remembering subjects independent ANOVA design).","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"repeated-measures-anova-in-jamovi","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.7.1 Repeated measures ANOVA in jamovi","text":"First, need data. Geschwind (1972) suggested exact nature patient’s language deficit following stroke can used diagnose specific region brain damaged. researcher concerned identifying specific communication difficulties experienced six patients suffering Broca’s Aphasia (language deficit commonly experienced following stroke) (Table 13.12).Table 13.12:  Word recognition task scores stroke patientsThe patients required complete three word recognition tasks. first (speech production) task, patients required repeat single words read aloud researcher. second (conceptual) task, designed test word comprehension, patients required match series pictures correct name. third (syntax) task, designed test knowledge correct word order, patients asked reorder syntactically incorrect sentences. patient completed three tasks. order patients attempted tasks counterbalanced participants. task consisted series 10 attempts. number attempts successfully completed patient shown Table 13.11. Enter data jamovi ready analysis (take short-cut load broca.csv file).perform one-way related ANOVA jamovi, open one-way repeated measures ANOVA dialogue box, Figure 13.9, via ANOVA - Repeated Measures ANOVA.\nFigure 13.9: Repeated measures ANOVA dialogue box jamovi]\n:Enter Repeated Measures Factor Name. label choose describe conditions repeated participants. example, describe speech, conceptual syntax tasks completed participants suitable label ‘Task’. Note new factor name represents independent variable analysis.Add third level Repeated Measures Factors text box, three levels representing three tasks: speech, conceptual syntax. Change labels levels accordingly.move levels variables across Repeated Measures Cells text box.Finally, Assumption Checks option, tick “Sphericity checks” text box.jamovi output one-way repeated measures ANOVA produced shown Figures 13.10 13.13. first output look Mauchly’s Test Sphericity, tests hypothesis variances differences conditions equal (meaning spread difference scores study conditions approximately ). Figure 13.10 Mauchly’s test significance level \\(p = .720\\). Mauchly’s test non-significant (.e. p < .05, case analysis) reasonable conclude variances differences significantly different (.e. roughly equal sphericity can assumed.).\nFigure 13.10: One-way repeated measures ANOVA output: Mauchly’s Test Sphericity\n, hand, Mauchly’s test significant (p < .05) conclude significant differences variance differences, requirement sphericity met. case, apply correction F-value obtained one-way related ANOVA analysis:Greenhouse-Geisser value “Tests Sphericity” table > .75 use Huynh-Feldt correctionBut Greenhouse-Geisser value < .75, use Greenhouse-Geisser correction.corrected F-values can specified Sphericity Corrections check boxes Assumption Checks options, corrected F-values shown results table, Figure 13.11.\nFigure 13.11: One-way repeated measures ANOVA output: Tests Within-Subjects Effects\nanalysis, saw significance Mauchly’s Test Sphericity p = .720 (.e. p > 0.05). , means can assume requirement sphericity met correction F-value needed. Therefore, can use ‘None’ Sphericity Correction output values repeated measure ‘Task’: \\(F = 6.93\\), \\(df = 2\\), \\(p = .013\\), can conclude number tests successfully completed language task vary significantly depending whether task speech, comprehension syntax based (\\(F(2, 10) = 6.93\\), \\(p = .013\\)).Post-hoc tests can also specified jamovi repeated measures ANOVA way independent ANOVA. results shown Figure 13.12. indicate significant difference Speech Syntax, levels.\nFigure 13.12: Post-hoc tests repeated measures ANOVA jamovi\nDescriptive statistics (marginal means) can reviewed help interpret results, produced jamovi output Figure 13.13. Comparison mean number trials successfully completed participants shows Broca’s Aphasics perform reasonably well speech production (mean = 7.17) language comprehension (mean = 6.17) tasks. However, performance considerably worse syntax task (mean = 4.33), significant difference post-hoc tests Speech Syntax task performance.\nFigure 13.13: One-way repeated measures ANOVA output: Descriptive Statistics\n","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"the-friedman-non-parametric-repeated-measures-anova-test","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.8 The Friedman non-parametric repeated measures ANOVA test","text":"Friedman test non-parametric version repeated measures ANOVA can used instead Kruskall-Wallis test testing differences three groups participants group, participant closely matched participants conditions. dependent variable ordinal, assumption normality met, Friedman test can used.Kruskall-Wallis test, underlying mathematics complicated, won’t presented . purpose book, sufficient note jamovi calculates tie-corrected version Friedman test, Figure 13.14 example using Broca’s Aphasia data already looked .\nFigure 13.14: ‘Repeated Measures ANOVA (Non-parametric)’ dialogue box jamovi\n’s pretty straightforward run Friedman test jamovi. Just select Analyses - ANOVA - Repeated Measures ANOVA (Non-parametric), Figure 13.14. highlight transfer names repeated measures variables wish compare (Speech, Conceptual, Syntax) ‘Measures:’ text box. produce descriptive statistics (means medians) three repeated measures variables, click Descriptives buttonThe jamovi results show descriptive statistics, chi-square value, degrees freedom, p-value (Figure 13.14. Since p-value less level conventionally used determine significance (p < .05), can conclude Broca’s Aphasics perform reasonably well speech production (median = 7.5) language comprehension (median = 6.5) tasks. However, performance considerably worse syntax task (median = 4.5), significant difference post-hoc tests Speech Syntax task performance.","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"on-the-relationship-between-anova-and-the-student-t-test","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.9 On the relationship between ANOVA and the Student t-test","text":"’s one last thing want point finishing. ’s something lot people find kind surprising, ’s worth knowing . ANOVA two groups identical Student t-test. , really. ’s just similar, actually equivalent every meaningful way. won’t try prove always true, show single concrete demonstration. Suppose , instead running ANOVA mood.gain ~ drug model, let’s instead using therapy predictor. run ANOVA get F-statistic \\(F(1,16) = 1.71\\), p-value = \\(0.21\\). Since two groups, didn’t actually need resort ANOVA, just decided run Student t-test. let’s see happens : get t-statistic \\(t(16) = -1.3068\\) \\(p-value = 0.21\\). Curiously, p-values identical. obtain value \\(p = .21\\). test statistic? run t-test instead ANOVA, get somewhat different answer, namely \\(t(16) = -1.3068\\). However, fairly straightforward relationship . square t-statistic get F-statistic : \\(-1.3068^{2} = 1.7077\\)","code":""},{"path":"comparing-several-means-one-way-anova.html","id":"summary-11","chapter":"13 Comparing several means (one-way ANOVA)","heading":"13.10 Summary","text":"’s fair bit covered chapter, ’s still lot missing 155. obviously, haven’t discussed run ANOVA interested one grouping variable, discussed lot detail next chapter, Factorial ANOVA. terms discussed, key topics :basic logic behind ANOVA works Running ANOVA jamoviHow compute Effect size ANOVA.Multiple comparisons post hoc tests multiple testing.[assumptions one-way ANOVA]Checking homogeneity variance assumption violated: Removing homogeneity variance assumptionChecking normality assumption violated: [Removing normaloity assumption]Repeated measures one-way ANOVA non-parametric equivalent, Friedman non-parametric repeated measures ANOVA test","code":""},{"path":"factorial-anova.html","id":"factorial-anova","chapter":"14 Factorial ANOVA","heading":"14 Factorial ANOVA","text":"course last chapters done quite lot. looked statistical tests can use one nominal predictor variable two groups (e.g. t-test Comparing two means three groups (Comparing several means (one-way ANOVA)). chapter [Correlation regression] introduced powerful new idea, building statistical models multiple continuous predictor variables used explain single outcome variable. instance, regression model used predict number errors student makes reading comprehension test based number hours studied test score standardised \\(IQ\\) test.goal chapter extend idea using multiple predictors ANOVA framework. instance, suppose interested using reading comprehension test measure student achievements three different schools, suspect girls boys developing different rates (expected different performance average). student classified two different ways: basis gender basis school. ’d like analyse reading comprehension scores terms grouping variables. tool generically referred factorial ANOVA. However, since two grouping variables, sometimes refer analysis two-way ANOVA, contrast one-way ANOVAs ran Chapter Comparing several means (one-way ANOVA).","code":""},{"path":"factorial-anova.html","id":"factorial-anova-1-balanced-designs-no-interactions","chapter":"14 Factorial ANOVA","heading":"14.1 Factorial ANOVA 1: balanced designs, no interactions","text":"discussed analysis variance Comparing several means (one-way ANOVA), assumed fairly simple experimental design. person one several groups want know whether groups different mean scores outcome variable. section, ’ll discuss broader class experimental designs known factorial designs, one grouping variable. gave one example kind design might arise . Another example appears Comparing several means (one-way ANOVA) chapter looking effect different drugs mood.gain experienced person. chapter find significant effect drug, end chapter also ran analysis see effect therapy. didn’t find one, ’s something bit worrying trying run two separate analyses trying predict outcome. Maybe actually effect therapy mood gain, couldn’t find “hidden” effect drug? words, ’re going want run single analysis includes drug therapy predictors. analysis person cross-classified drug given (factor 3 levels) therapy received (factor 2 levels). refer \\(3 \\times 2\\) factorial design.cross-tabulate drug therapy, using ‘Frequencies’ - ‘Contingency Tables’ analysis jamovi (see Tabulating cross-tabulating data section), get table shown Figure 14.1.\nFigure 14.1: jamovi contingency table drug therapy\ncan see, participants corresponding possible combinations two factors, indicating design completely crossed, turns equal number people group. words, balanced design. section ’ll talk analyse data balanced designs, since simplest case. story unbalanced designs quite tedious, ’ll put one side moment.","code":""},{"path":"factorial-anova.html","id":"what-hypotheses-are-we-testing","chapter":"14 Factorial ANOVA","heading":"14.1.1 What hypotheses are we testing?","text":"Like one-way ANOVA, factorial ANOVA tool testing certain types hypotheses population means. sensible place start explicit hypotheses actually . However, can even get point, ’s really useful clean simple notation describe population means. fact observations cross-classified terms two different factors, quite lot different means one might interested . see , let’s start thinking different sample means can calculate kind design. Firstly, ’s obvious idea might interested list group means (Table 14.1).Table 14.1:  Group means drug therapy groups clinicaltrial.csv dataNow, next Table (Table 14.2) shows list group means possible combinations two factors (e.g., people received placebo therapy, people received placebo getting CBT, etc.). helpful organise numbers, plus marginal grand means, single table looks like :Table 14.2:  Group total means drug therapy groups clintrial.csv dataNow, different means course sample statistic. ’s quantity pertains specific observations ’ve made study. want make inferences corresponding population parameters. , true means exist within broader population. population means can also organised similar table, ’ll need little mathematical notation (Table 14.3). usual, ’ll use symbol \\(\\mu\\) denote population mean. However, lots different means, ’ll need use subscripts distinguish .’s notation works. table defined terms two factors. row corresponds different level Factor (case drug), column corresponds different level Factor B (case therapy). let R denote number rows table, \\(C\\) denote number columns, can refer \\(R \\times C\\) factorial ANOVA. case \\(R = 3\\) \\(C = 2\\). ’ll use lowercase letters refer specific rows columns, \\(\\mu_{rc}\\) refers population mean associated \\(r\\)-th level Factor \\(\\) (.e. row number \\(r\\)) \\(c\\)-th level Factor B (column number c).156 population means now written like 14.1:Table 14.3:  Notation population means factorial tableOkay, remaining entries? instance, describe average mood gain across entire (hypothetical) population people might given Joyzepam experiment like , regardless whether CBT? use “dot” notation express . case Joyzepam, notice ’re talking mean associated third row table. , ’re averaging across two cell means (.e., \\(\\mu_{31}\\) \\(\\mu_{32}\\)). result averaging referred marginal mean, denoted \\(\\mu_3.\\) case. marginal mean CBT corresponds population mean associated second column table, use notation mean obtained averaging (marginalising157) . full table population means can written like Table 14.4.Table 14.4:  Notation population total means factorial tableNow notation, straightforward formulate express hypotheses. Let’s suppose goal find two things. First, choice drug effect mood? second, CBT effect mood? aren’t hypotheses formulate course, ’ll see really important example different kind hypothesis section Factorial ANOVA 2: balanced designs, interactions allowed, two simplest hypotheses test, ’ll start . Consider first test. drug effect expect row means identical, right? ’s null hypothesis. hand, drug matter expect row means different. Formally, write null alternative hypotheses terms equality marginal means:\\[\\text{Null hypothesis, } H_0 \\text{: row means , .e., } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\\[\\text{Alternative hypothesis, } H_1 \\text{: least one row mean different}\\]’s worth noting exactly statistical hypotheses formed ran one-way ANOVA data previous chapter Comparing several means (one-way ANOVA). Back used notation \\(\\mu \\times {P}\\) refer mean mood gain placebo group, \\(\\mu{}\\) \\(\\mu \\times {J}\\) corresponding group means two drugs, null hypothesis \\(\\mu{P} = \\mu{} = \\mu{J}\\) . ’re actually talking hypothesis, ’s just complicated ANOVA requires careful notation due presence multiple grouping variables, ’re now referring hypothesis \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\) . However, ’ll see shortly, although hypothesis identical test hypothesis subtly different due fact ’re now acknowledging existence second grouping variable.Speaking grouping variable, won’t surprised discover second hypothesis test formulated way. However, since ’re talking psychological therapy rather drugs null hypothesis now corresponds equality column means:\\[\\text{Null hypothesis, } H_0 \\text{: column means , .e., } \\mu_{ .1} = \\mu_{ .2} \\]\n\\[\\text{Alternative hypothesis, } H_1 \\text{: column means different, .e., } \\mu_{ .1} \\neq \\mu_{ .2}\\]","code":""},{"path":"factorial-anova.html","id":"running-the-analysis-in-jamovi","chapter":"14 Factorial ANOVA","heading":"14.1.2 Running the analysis in jamovi","text":"null alternative hypotheses described last section seem awfully familiar. ’re basically hypotheses testing simpler oneway ANOVAs Comparing several means (one-way ANOVA). ’re probably expecting hypothesis tests used factorial ANOVA essentially F-test Comparing several means (one-way ANOVA) chapter. ’re expecting see references sums squares (SS), mean squares (MS), degrees freedom (df), finally F-statistic can convert p-value, right? Well, ’re absolutely completely right. much ’m going depart usual approach. Throughout book, ’ve generally taken approach describing logic (extent mathematics) underpins particular analysis first introducing analysis jamovi. time ’m going way around show jamovi first. reason want highlight similarities simple one-way ANOVA tool discussed Comparing several means (one-way ANOVA), complicated approach ’re going use chapter.data ’re trying analyse correspond balanced factorial design running analysis variance easy. see easy , let’s start reproducing original analysis Comparing several means (one-way ANOVA). case ’ve forgotten, analysis using single factor (.e., drug) predict outcome variable (.e., mood.gain), got results shown Figure 14.2.\nFigure 14.2: jamovi one way anova mood.gain drug\nNow, suppose ’m also curious find therapy relationship mood.gain. light ’ve seen discussion multiple regression [Correlation regression] chapter, probably won’t surprised add therapy second ‘Fixed Factor’ analysis, see Figure 14.3.\nFigure 14.3: jamovi two way anova mood.gain drug therapy\noutput pretty simple read . first row table reports -group sum squares (SS) value associated drug factor, along corresponding -group df value. also calculates mean square value (MS), F-statistic p-value. also row corresponding therapy factor row corresponding residuals (.e., within groups variation).individual quantities pretty familiar, relationships different quantities remained unchanged, just like saw original one-way ANOVA. Note mean square value calculated dividing \\(SS\\) corresponding \\(df\\). , ’s still true \\[MS=\\frac{SS}{df}\\]regardless whether ’re talking drug, therapy residuals. see , let’s worry sums squares values calculated. Instead, let’s take faith jamovi calculated \\(SS\\) values correctly, try verify rest numbers make sense. First, note drug factor, divide \\(3.45\\) \\(2\\) end mean square value \\(1.73\\). therapy factor, ’s 1 degree freedom, calculations even simpler: dividing \\(0.47\\) (\\(SS\\) value) 1 gives us answer \\(0.47\\) (\\(MS\\) value).Turning F statistics p values, notice two ; one corresponding drug factor corresponding therapy factor. Regardless one ’re talking , F statistic calculated dividing mean square value associated factor mean square value associated residuals. use “” shorthand notation refer first factor (factor ; case drug) “R” shorthand notation refer residuals, F statistic associated factor denoted FA, calculated follows:\\[F_A=\\frac{MS_A}{MS_R}\\]equivalent formula exists factor B (.e., therapy). Note use “R” refer residuals bit awkward, since also used letter R refer number rows table, ’m going use “R” mean residuals context SSR MSR, hopefully shouldn’t confusing. Anyway, apply formula drugs factor take mean square 1.73 divide residual mean square value \\(0.07\\), gives us F-statistic 26.15. corresponding calculation therapy variable divide \\(0.47\\) \\(0.07\\) gives \\(7.08\\) F-statistic. surprisingly, course, values jamovi reported ANOVA table .Also ANOVA table calculation p values. , nothing new . two factors ’re trying test null hypothesis relationship factor outcome variable (’ll bit precise later ). end, ’ve (apparently) followed similar strategy one way ANOVA calculated F-statistic hypotheses. convert p values, need note sampling distribution F statistic null hypothesis (factor question irrelevant) F distribution. Also note two degrees freedom values corresponding factor corresponding residuals. drug factor ’re talking F distribution 2 14 degrees freedom (’ll discuss degrees freedom detail later). contrast, therapy factor sampling distribution F 1 14 degrees freedom.point, hope can see ANOVA table complicated factorial analysis read much way ANOVA table simpler one way analysis. short, ’s telling us factorial ANOVA \\(3 \\times 2\\) design found significant effect drug (\\(F_{2,14} = 26.15, p < .001\\)) well significant effect therapy (\\(F_{1,14} = 7.08, p = .02\\)). , use technically correct terminology, say two main effects drug therapy. moment, probably seems bit redundant refer “main” effects, actually make sense. Later , ’re going want talk possibility “interactions” two factors, generally make distinction main effects interaction effects.","code":""},{"path":"factorial-anova.html","id":"how-are-the-sum-of-squares-calculated","chapter":"14 Factorial ANOVA","heading":"14.1.3 How are the sum of squares calculated?","text":"previous section two goals. Firstly, show jamovi method needed factorial ANOVA pretty much used one way ANOVA. difference addition second factor. Secondly, wanted show ANOVA table looks like case, can see outset basic logic structure behind factorial ANOVA underpins one way ANOVA. Try hold onto feeling. ’s genuinely true, insofar factorial ANOVA built less way simpler one-way ANOVA model. ’s just feeling familiarity starts evaporate start digging details. Traditionally, comforting sensation replaced urge hurl abuse authors statistics textbooks.Okay, let’s start looking details. explanation gave last section illustrates fact hypothesis tests main effects (drug therapy case) F-tests, doesn’t show sum squares (SS) values calculated. tell explicitly calculate degrees freedom (df values) though ’s simple thing comparison. Let’s assume now two predictor variables, Factor Factor B. use Y refer outcome variable, use Yrci refer outcome associated -th member group rc (.e., level/row r Factor level/column c Factor B). Thus, use \\(\\bar{Y}\\) refer sample mean, can use notation refer group means, marginal means grand means. , \\(\\bar{Y}_{rc}\\) sample mean associated rth level Factor cth level Factor: \\(\\bar{Y}_{r.}\\) marginal mean rth level Factor , \\(\\bar{Y}_{.c}\\) marginal mean cth level Factor B, \\(\\bar{Y}_{..}\\) grand mean. words, sample means can organised table population means. clinical trial data, table shown Table 14.5.Table 14.5:  Notation sample means clinical trial dataAnd look sample means showed earlier, \\(\\bar{Y}_{11} = 0.30\\), \\(\\bar{Y}_{12} = 0.60\\) etc. clinical trial example, drugs factor 3 levels therapy factor 2 levels, ’re trying run \\(3 \\times 2\\) factorial ANOVA. However, ’ll little general say Factor (row factor) R levels Factor B (column factor) C levels, ’re runnning \\(R \\times C\\) factorial ANOVA.[Additional technical detail 158]","code":""},{"path":"factorial-anova.html","id":"what-are-our-degrees-of-freedom","chapter":"14 Factorial ANOVA","heading":"14.1.4 What are our degrees of freedom?","text":"degrees freedom calculated much way one-way ANOVA. given factor, degrees freedom equal number levels minus 1 (.e., \\(R - 1\\) row variable Factor \\(\\), \\(C - 1\\) column variable Factor B). , drugs factor obtain \\(df = 2\\), therapy factor obtain \\(df = 1\\). Later , discuss interpretation ANOVA regression model (see ANOVA linear model), ’ll give clearer statement arrive number. moment can use simple definition degrees freedom, namely degrees freedom equals number quantities observed, minus number constraints. , drugs factor, observe 3 separate group means, constrained 1 grand mean, therefore degrees freedom 2. residuals, logic similar, quite . total number observations experiment 18. constraints correspond 1 grand mean, 2 additional group means drug factor introduces, 1 additional group mean therapy factor introduces, degrees freedom 14. formula, \\(N - 1 - (R - 1) - (C - 1)\\), simplifies \\(N - R - C + 1\\).","code":""},{"path":"factorial-anova.html","id":"what-are-our-degrees-of-freedom-1","chapter":"14 Factorial ANOVA","heading":"14.1.5 What are our degrees of freedom?","text":"degrees freedom calculated much way one-way ANOVA. given factor, degrees freedom equal number levels minus 1 (.e., \\(R - 1\\) row variable Factor , \\(C - 1\\) column variable Factor B). , drugs factor obtain \\(df = 2\\), therapy factor obtain \\(df = 1\\). Later , discuss interpretation ANOVA regression model (see ANOVA linear model), ’ll give clearer statement arrive number. moment can use simple definition degrees freedom, namely degrees freedom equals number quantities observed, minus number constraints. , drugs factor, observe 3 separate group means, constrained 1 grand mean, therefore degrees freedom 2. residuals, logic similar, quite . total number observations experiment 18. constraints correspond 1 grand mean, 2 additional group means drug factor introduces, 1 additional group mean therapy factor introduces, degrees freedom 14. formula, \\(N - 1 - (R - 1) - (C - 1)\\), simplifies \\(N - R - C + 1\\).","code":""},{"path":"factorial-anova.html","id":"factorial-anova-versus-one-way-anovas","chapter":"14 Factorial ANOVA","heading":"14.1.6 Factorial ANOVA versus one-way ANOVAs","text":"Now ’ve seen factorial ANOVA works, ’s worth taking moment compare results one way analyses, give us really good sense ’s good idea run factorial ANOVA. Comparing several means (one-way ANOVA) chapter, ran one-way ANOVA looked see differences drugs, second one-way ANOVA see differences therapies. saw section [hypotheses testing], null alternative hypotheses tested one-way ANOVAs fact identical hypotheses tested factorial ANOVA. Looking even carefully ANOVA tables, can see sum squares associated factors identical two different analyses (3.45 drug 0.92 therapy), degrees freedom (2 drug, 1 therapy). don’t give answers! notably, ran one-way ANOVA therapy section relationship ANOVA Student t-test didn’t find significant effect (p-value .21). However, look main effect therapy within context two-way ANOVA, get significant effect (p = .019). two analyses clearly .happen? answer lies understanding residuals calculated. Recall whole idea behind F-test compare variability can attributed particular factor variability accounted (residuals). run one-way ANOVA therapy, therefore ignore effect drug, ANOVA end dumping drug-induced variability residuals! effect making data look noisy really , effect therapy correctly found significant two-way ANOVA now becomes non-significant. ignore something actually matters (e.g., drug) trying assess contribution something else (e.g., therapy) analysis distorted. course, ’s perfectly okay ignore variables genuinely irrelevant phenomenon interest. recorded colour walls, turned non-significant factor three-way ANOVA, perfectly okay disregard just report simpler two-way ANOVA doesn’t include irrelevant factor. shouldn’t drop variables actually make difference!","code":""},{"path":"factorial-anova.html","id":"what-kinds-of-outcomes-does-this-analysis-capture","chapter":"14 Factorial ANOVA","heading":"14.1.7 What kinds of outcomes does this analysis capture?","text":"ANOVA model ’ve talking far covers range different patterns might observe data. instance, two-way ANOVA design four possibilities: () Factor matters, (b) Factor B matters, (c) B matter, (d) neither B matters. example four possibilities plotted Figure 14.4.","code":""},{"path":"factorial-anova.html","id":"factorial-anova-2-balanced-designs-interactions-allowed","chapter":"14 Factorial ANOVA","heading":"14.2 Factorial ANOVA 2: balanced designs, interactions allowed","text":"four patterns data shown Figure 14.4 quite realistic. great many data sets produce exactly patterns. However, whole story ANOVA model talking point sufficient fully account table group means. ? Well, far ability talk idea drugs can influence mood, therapy can influence mood, way talking possibility interaction two. interaction \\(\\) \\(B\\) said occur whenever effect Factor \\(\\) different, depending level Factor \\(B\\) ’re talking . Several examples interaction effect context \\(2 \\times 2\\) ANOVA shown Figure 14.5. give concrete example, suppose operation Anxifree Joyzepam governed quite different physiological mechanisms. One consequence Joyzepam less effect mood regardless whether one therapy, Anxifree actually much effective administered conjunction CBT. ANOVA developed previous section capture idea. get idea whether interaction actually happening , helps plot various group means. jamovi done via ANOVA ‘Estimated Marginal Means’ option - just move drug therapy across ‘Marginal Means’ box ‘Term 1’. look something like Figure 14.6. main concern relates fact two lines aren’t parallel. effect CBT (difference solid line dotted line) drug Joyzepam (right side) appears near zero, even smaller effect CBT placebo used (left side). However, Anxifree administered, effect CBT larger placebo (middle). effect real, just random variation due chance? original ANOVA answer question, make allowances idea interactions even exist! section, ’ll fix problem.\nFigure 14.4: four different outcomes \\(2 imes 2\\) ANOVA interactions present. panel () see main effect Factor effect Factor B. Panel (b) shows main effect Factor B effect Factor . Panel (c) shows main effects Factor Factor B. Finally, panel (d) shows effect either factor\n\nFigure 14.5: Qualitatively different interactions \\(2 \\times 2\\) ANOVA\n\nFigure 14.6: jamovi screen showing generate descriptive interaction plot ANOVA using clinical trial data\n","code":""},{"path":"factorial-anova.html","id":"what-exactly-is-an-interaction-effect","chapter":"14 Factorial ANOVA","heading":"14.2.1 What exactly is an interaction effect?","text":"key idea ’re going introduce section interaction effect. ANOVA model looked far two factors involved model (.e., drug therapy). add interaction add new component model: combination drug therapy. Intuitively, idea behind interaction effect fairly simple. just means effect Factor different, depending level Factor B ’re talking . actually mean terms data? plot Figure 14.5 depicts several different patterns , although quite different , count interaction effect. ’s entirely straightforward translate qualitative idea something mathematical statistician can work .[Additional technical detail 159]","code":""},{"path":"factorial-anova.html","id":"degrees-of-freedom-for-the-interaction","chapter":"14 Factorial ANOVA","heading":"14.2.2 Degrees of freedom for the interaction","text":"Calculating degrees freedom interaction , , slightly trickier corresponding calculation main effects. start , let’s think ANOVA model whole. include interaction effects model ’re allowing every single group unique mean, \\(mu_{rc}\\). \\(R \\times C\\) factorial ANOVA, means \\(R \\times C\\) quantities interest model one constraint: group means need average grand mean. model whole needs (\\(R \\times C\\)) - 1 degrees freedom. main effect Factor \\(R - 1\\) degrees freedom, main effect Factor B \\(C - 1\\) degrees freedom. means degrees freedom associated interaction \\[\\begin{aligned}\ndf_{:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\\]just product degrees freedom associated row factor column factor.residual degrees freedom? ’ve added interaction terms absorb degrees freedom, fewer residual degrees freedom left . Specifically, note model interaction total \\((R \\times C) - 1\\), \\(N\\) observations data set constrained satisfy 1 grand mean, residual degrees freedom now become \\(N - (R \\times C) - 1 + 1\\), just \\(N - (R \\times C)\\).","code":""},{"path":"factorial-anova.html","id":"running-the-anova-in-jamovi","chapter":"14 Factorial ANOVA","heading":"14.2.3 Running the ANOVA in jamovi","text":"Adding interaction terms ANOVA model jamovi straightforward. fact straightforward default option ANOVA. means specify ANOVA two factors, e.g. drug therapy interaction component - drug \\(\\times\\) therapy - added automatically model160. run ANOVA interaction term included, get results shown Figure 14.7.\nFigure 14.7: Results full factorial model, including interaction component drug \\(\\times\\) therapy\nturns , significant main effect drug (\\(F_{2,12} = 31.7, p < .001\\)) therapy type (\\(F_{1,12} = 8.6, p = .013\\)), significant interaction two (\\(F_{2,12} = 2.5, p = 0.125\\)).","code":""},{"path":"factorial-anova.html","id":"interpreting-the-results","chapter":"14 Factorial ANOVA","heading":"14.2.4 Interpreting the results","text":"’s couple important things consider interpreting results factorial ANOVA. First, ’s issue one-way ANOVA, obtain significant main effect (say) drug, doesn’t tell anything drugs different one another. find , need run additional analyses. ’ll talk analyses can run later Sections: Different ways specify contrasts Post hoc tests. true interaction effects. Knowing ’s significant interaction doesn’t tell anything kind interaction exists. , ’ll need run additional analyses.Secondly, ’s peculiar interpretation issue arises obtain significant interaction effect corresponding main effect. happens sometimes. instance, crossover interaction shown Figure 14.5a, exactly ’d find. case, neither main effects significant, interaction effect . difficult situation interpret, people often get bit confused . general advice statisticians like give situation shouldn’t pay much attention main effects interaction present. reason say , although tests main effects perfectly valid mathematical point view, significant interaction effect main effects rarely test interesting hypotheses. Recall section [hypotheses testing] null hypothesis main effect marginal means equal , marginal mean formed averaging across several different groups. significant interaction effect know groups comprise marginal mean aren’t homogeneous, ’s really obvious even care marginal means.’s mean. , let’s stick clinical example. Suppose \\(2 \\times 2\\) design comparing two different treatments phobias (e.g., systematic desensitisation vs flooding), two different anxiety reducing drugs (e.g., Anxifree vs Joyzepam). Now, suppose found Anxifree effect desensitisation treatment, Joyzepam effect flooding treatment. pretty effective treatment. classic crossover interaction, ’d find running ANOVA main effect drug, significant interaction. Now, actually mean say ’s main effect? Well, means average two different psychological treatments, average effect Anxifree Joyzepam . anyone care ? treating someone phobias never case person can treated using “average” flooding desensitisation. doesn’t make lot sense. either get one . one treatment one drug effective, treatment drug effective. interaction important thing main effect kind irrelevant.sort thing happens lot. main effect tests marginal means, interaction present often find terribly interested marginal means imply averaging things interaction tells us shouldn’t averaged! course, ’s always case main effect meaningless interaction present. Often can get big main effect small interaction, case can still say things like “drug generally effective drug B” (big effect drug), ’d need modify bit adding “difference effectiveness different different psychological treatments”. case, main point whenever get significant interaction stop think main effect actually means context. Don’t automatically assume main effect interesting.","code":""},{"path":"factorial-anova.html","id":"effect-size-3","chapter":"14 Factorial ANOVA","heading":"14.3 Effect size","text":"effect size calculation factorial ANOVA pretty similar used one way ANOVA (see Effect size section). Specifically, can use \\(\\eta^2\\) (eta-squared) simple way measure big overall effect particular term. , \\(\\eta^2\\) defined dividing sum squares associated term total sum squares. instance, determine size main effect Factor , use following formula:\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\], can interpreted much way \\(R^2\\) regression.161 tells proportion variance outcome variable can accounted main effect Factor . therefore number ranges 0 (effect ) 1 (accounts variability outcome). Moreover, sum \\(\\eta^2\\) values, taken across terms model, sum total \\(R^2\\) ANOVA model. , instance, ANOVA model fits perfectly (.e., within-groups variability !), \\(\\eta^2\\) values sum 1. course, rarely ever happens real life.However, factorial ANOVA, second measure effect size people like report, known partial \\(\\eta^2\\). idea behind partial \\(\\eta^2\\) (sometimes denoted \\(p^{\\eta^2}\\) \\(\\eta_p^2\\)) , measuring effect size particular term (say, main effect Factor ), want deliberately ignore effects model (e.g., main effect Factor B). , pretend effect terms zero, calculate \\(\\eta^2\\) value . actually pretty easy calculate. remove sum squares associated terms denominator. words, want partial \\(\\eta^2\\) main effect Factor , denominator just sum SS values Factor residuals\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]always give larger number \\(\\eta^2\\), cynic suspects accounts popularity partial \\(\\eta^2\\). get number 0 1, 0 represents effect. However, ’s slightly trickier interpret large partial \\(\\eta^2\\) value means. particular, can’t actually compare partial \\(\\eta^2\\) values across terms! Suppose, instance, within-groups variability : , \\(SS_R = 0\\). means every term partial \\(\\eta^2\\) value 1. doesn’t mean terms model equally important, indeed equally large. mean terms model effect sizes large relative residual variation. comparable across terms.see mean , ’s useful see concrete example. First, let’s look effect sizes original ANOVA (Table 14.6 without interaction term, Figure 14.3).Table 14.6:  Effect sizes interaction term **** included ANOVA modelLooking \\(\\eta^2\\) values first, see drug accounts 71% variance (.e. \\(\\eta^2 = 0.71\\)) mood.gain, whereas therapy accounts 10%. leaves total 19% variation unaccounted (.e., residuals constitute 19% variation outcome). Overall, implies large effect162 drug modest effect therapy.Now let’s look partial \\(\\eta^2\\) values, shown Figure 14.3. effect therapy isn’t large, controlling doesn’t make much difference, partial \\(\\eta^2\\) drug doesn’t increase much, obtain value \\(p^{\\eta^2} = 0.79\\). contrast, effect drug large, controlling makes big difference, calculate partial \\(\\eta^2\\) therapy can see rises \\(p^{\\eta^2} = 0.34\\). question ask , partial \\(\\eta^2\\) values actually mean? way generally interpret partial \\(\\eta^2\\) main effect Factor interpret statement hypothetical experiment Factor varied. , even though experiment varied B, can easily imagine experiment Factor varied, partial \\(\\eta^2\\) statistic tells much variance outcome variable expect see accounted experiment. However, noted interpretation, like many things associated main effects, doesn’t make lot sense large significant interaction effect.Speaking interaction effects, Table 14.7 shows get calculate effect sizes model includes interaction term, Figure 14.7. can see, \\(\\eta^2\\) values main effects don’t change, partial \\(\\eta^2\\) values :Table 14.7:  Effect sizes interaction term **** included ANOVA model","code":""},{"path":"factorial-anova.html","id":"estimated-group-means","chapter":"14 Factorial ANOVA","heading":"14.3.1 Estimated group means","text":"many situations find wanting report estimates group means based results ANOVA, well confidence intervals associated . can use ‘Estimated Marginal Means’ option jamovi ANOVA analysis , Figure 14.8. ANOVA run saturated model (.e., contains possible main effects possible interaction effects) estimates group means actually identical sample means, though confidence intervals use pooled estimate standard errors rather use separate one group.\nFigure 14.8: jamovi screenshot showing marginal means saturated model, .e. including interaction component, clinical trial data set\noutput see estimated mean mood gain placebo group therapy \\(0.300\\), \\(95\\%\\) confidence interval \\(0.006\\) \\(0.594\\). Note confidence intervals get calculated separately group, fact ANOVA model assumes homogeneity variance therefore uses pooled estimate standard deviation.model doesn’t contain interaction term, estimated group means different sample means. Instead reporting sample mean, jamovi calculate value group means expected basis marginal means (.e., assuming interaction). Using notation developed earlier, estimate reported µrc, mean level r (row) Factor level c (column) Factor B \\(\\mu_{..} + \\alpha_r + \\beta_c\\). genuinely interactions two factors, actually better estimate population mean raw sample mean . Removing interaction term model, via ‘Model’ options jamovi ANOVA analysis, provides marginal means analysis shown Figure 14.9.\nFigure 14.9: jamovi screenshot showing marginal means unsaturated model, .e. without interaction component, clinical trial data set\n","code":""},{"path":"factorial-anova.html","id":"assumption-checking","chapter":"14 Factorial ANOVA","heading":"14.4 Assumption checking","text":"one-way ANOVA, key assumptions factorial ANOVA homogeneity variance (groups standard deviation), normality residuals, independence observations. first two things can check . third something need assess asking special relationships different observations, example repeated measures independent variable time relationship observations time one time two: observations different time points people. Additionally, aren’t using saturated model (e.g., ’ve omitted interaction terms) ’re also assuming omitted terms aren’t important. course, can check last one running ANOVA omitted terms included see ’re significant, ’s pretty easy. homogeneity variance normality residuals? turns , pretty easy check. ’s different checks one-way ANOVA.","code":""},{"path":"factorial-anova.html","id":"homogeneity-of-variance","chapter":"14 Factorial ANOVA","heading":"14.4.1 Homogeneity of variance","text":"mentioned section Checking homogeneity variance assumption last chapter, ’s good idea visually inspect plot standard deviations compared across different groups / categories, also see Levene test consistent visual inspection. theory behind Levene test discussed Checking homogeneity variance assumption, won’t discuss . test expects saturated model (.e., including relevant terms), test primarily concerned within-group variance, doesn’t really make lot sense calculate way respect full model. Levene test can specified ANOVA ‘Assumption Checks’ - ‘Homogeneity Tests’ option jamovi, result shown Figure 14.10. fact Levene test non-significant means , providing consistent visual inspection plot standard deviations, can safely assume homogeneity variance assumption violated.","code":""},{"path":"factorial-anova.html","id":"normality-of-residuals","chapter":"14 Factorial ANOVA","heading":"14.4.2 Normality of residuals","text":"one-way ANOVA can test normality residuals straightforward fashion (see Checking normality assumption). Primarily though, ’s generally good idea examine residuals graphically using QQ plot. See Figure 14.10.\nFigure 14.10: Checking assumptions ANOVA model\n","code":""},{"path":"factorial-anova.html","id":"analysis-of-covariance-ancova","chapter":"14 Factorial ANOVA","heading":"14.5 Analysis of Covariance (ANCOVA)","text":"variation ANOVA additional continuous variable think might related dependent variable. additional variable can added analysis covariate, aptly named analysis covariance (ANCOVA).ANCOVA values dependent variable “adjusted” influence covariate, “adjusted” score means tested groups usual way. technique can increase precision experiment, therefore provide “powerful” test equality group means dependent variable. ANCOVA ? Well, although covariate typically experimental interest, adjustment covariate can decrease estimate experimental error thus, reducing error variance, precision increased. means inappropriate failure reject null hypothesis (false negative type II error) less likely.Despite advantage, ANCOVA runs risk undoing real differences groups, avoided. Look Figure 14.11, example, shows plot Statistics anxiety age shows two distinct groups – students either Arts Science background preference. ANCOVA age covariate might lead conclusion statistics anxiety differ two groups. conclusion reasonable – probably ages two groups overlap analysis variance essentially “extrapolated region data” (Everitt (1996), p. 68).\nFigure 14.11: Plot Statistics anxiety age two distinct groups\nClearly, careful thought needs given analysis covariance distinct groups. applies one-way factorial designs, ANCOVA can used .","code":""},{"path":"factorial-anova.html","id":"running-ancova-in-jamovi","chapter":"14 Factorial ANOVA","heading":"14.5.1 Running ANCOVA in jamovi","text":"health psychologist interested effect routine cycling stress happiness levels, age covariate. can find dataset file ancova.csv. Open file jamovi , undertake ANCOVA, select Analyses - ANOVA - ANCOVA open ANCOVA analysis window (Figure 14.12). Highlight dependent variable ‘happiness’ transfer ‘Dependent Variable’ text box. Highlight independent variables ‘stress’ ‘commute’ transfer ‘Fixed Factors’ text box. Highlight covariate ‘age’ transfer ‘Covariates’ text box. Click Estimated Marginal Means bring plots tables options.\nFigure 14.12: jamovi ANCOVA analysis window\nANCOVA table showing Tests -Subjects Effects produced jamovi results window (Figure 14.13). F value covariate ‘age’ significant \\(p = .023\\), suggesting age important predictor dependent variable, happiness. look estimated marginal mean scores (Figure 14.14), adjustments made (compared analysis without covariate) inclusion covariate ‘age’ ANCOVA. plot (Figure 14.15) good way visualising interpreting significant effects.\nFigure 14.13: jamovi ANCOVA output happiness function stress commuting method, age covariate\n\nFigure 14.14: Table mean happiness level function stress commuting method (adjusted covariate age) 95% confidence intervals\n\\(F\\) value main effect ‘stress’ (52.61) associated probability \\(p < .001\\). \\(F\\) value main effect ‘commute’ (42.33) associated probability \\(p < .001\\). Since less probability typically used decide statistical result significant (\\(p < .05\\)) can conclude significant main effect stress (\\(F(1, 15) = 52.61, p < .001\\)) significant main effect commuting method (\\(F(1, 15) = 42.33, p < .001\\)). significant interaction stress commuting method also found (\\(F(1, 15) = 14.15, p = .002\\)).Figure 14.15 can see adjusted, marginal, mean happiness scores age covariate ANCOVA. analysis significant interaction effect, whereby people low stress cycle work happier people low stress drive people high stress whether cycle drive work. also significant main effect stress – people low stress happier high stress. also significant main effect commuting behaviour – people cycle happier, average, drive work.\nFigure 14.15: Plot mean happiness level function stress commuting method\nOne thing aware , thinking including covariate ANOVA, additional assumption: relationship covariate dependent variable similar levels independent variable. can checked adding interaction term covariate independent variable jamovi Model - Model terms option. interaction effect significant can removed. significant different advanced statistical technique might appropriate (beyond scope book might want consult friendly statistician).","code":""},{"path":"factorial-anova.html","id":"anova-as-a-linear-model","chapter":"14 Factorial ANOVA","heading":"14.6 ANOVA as a linear model","text":"One important things understand ANOVA regression ’re basically thing. surface , maybe wouldn’t think true. , way ’ve described far suggests ANOVA primarily concerned testing group differences, regression primarily concerned understanding correlations variables. , far goes ’s perfectly true. look hood, speak, underlying mechanics ANOVA regression awfully similar. fact, think , ’ve already seen evidence . ANOVA regression rely heavily sums squares (SS), make use F tests, . Looking back, ’s hard escape feeling chapters [Correlation regression] [Comparing several means (one-wasy ANOVA)] bit repetitive.reason ANOVA regression kinds linear models. case regression, kind obvious. regression equation use define relationship predictors outcomes equation straight line, ’s quite obviously linear model, equation\\[Y_p=b_0+b_1 X_{1p} +b_2 X_{2p} + \\epsilon_p\\]\\(Y_p\\) outcome value p-th observation (e.g., p-th person), \\(X_{1p}\\) value first predictor p-th observation, \\(X_{2p}\\) value second predictor p-th observation, \\(b_0\\), \\(b_1\\), \\(b_2\\) terms regression coefficients, \\(\\epsilon_p\\) p-th residual. ignore residuals \\(\\epsilon_p\\) just focus regression line , get following formula:\\[\\hat{Y}_p=b_0+b_1 X_{1p} +b_2 X_{2p} \\]\\(\\hat{Y}_p\\) value Y regression line predicts person p, opposed actually-observed value \\(Y_p\\). thing isn’t immediately obvious can write ANOVA linear model well. However, ’s actually pretty straightforward . Let’s start really simple example, rewriting \\(2 \\times 2\\) factorial ANOVA linear model.","code":""},{"path":"factorial-anova.html","id":"some-data","chapter":"14 Factorial ANOVA","heading":"14.6.1 Some data","text":"make things concrete, let’s suppose outcome variable grade student receives class, ratio-scale variable corresponding mark \\(0%\\) \\(100%\\). two predictor variables interest: whether student turned lectures (attend variable) whether student actually read textbook (reading variable). ’ll say attend = 1 student attended class, attend = 0 . Similarly, ’ll say reading = 1 student read textbook, reading = 0 .Okay, far ’s simple enough. next thing need wrap maths around (sorry!). purposes example, let \\(Y_p\\) denote grade p-th student class. quite notation used earlier chapter. Previously, ’ve used notation \\(Y_{rci}\\) refer -th person r-th group predictor 1 (row factor) c-th group predictor 2 (column factor). extended notation really handy describing SS values calculated, ’s pain current context, ’ll switch notation . Now, \\(Y_p\\) notation visually simpler \\(Y_{rci}\\), shortcoming doesn’t actually keep track group memberships! , told \\(Y_{0,0,3} = 35\\), ’d immediately know ’re talking student (3rd student, fact) didn’t attend lectures (.e., attend = 0) didn’t read textbook (.e. reading = 0), ended failing class (grade = 35). tell \\(Y_p = 35\\), know p-th student didn’t get good grade. ’ve lost key information . course, doesn’t take lot thought figure fix . ’ll instead introduce two new variables \\(X_{1p}\\) \\(X_{2p}\\) keep track information. case hypothetical student, know \\(X_{1p} = 0\\) (.e., attend = 0) \\(X_{2p} = 0\\) (.e., reading = 0). data might look like Table 14.8.Table 14.8:  Data grade, attendance reading textbookThis isn’t anything particularly special, course. ’s exactly format expect see data! See data file rtfm.csv. can use jamovi ‘Descriptives’ analysis confirm data set corresponds balanced design, 2 observations combination attend reading. way can also calculate mean grade combination. shown Figure 14.16. Looking mean scores, one gets strong impression reading text attending class matter lot.\nFigure 14.16: jamovi descriptives rtfm data set\n","code":""},{"path":"factorial-anova.html","id":"anova-with-binary-factors-as-a-regression-model","chapter":"14 Factorial ANOVA","heading":"14.6.2 ANOVA with binary factors as a regression model","text":"Okay, let’s get back talking mathematics. now data expressed terms three numeric variables: continuous variable \\(Y\\) two binary variables \\(X_1\\) \\(X_2\\). want recognise \\(2 \\times 2\\) factorial ANOVA exactly equivalent regression model\\[Y_p=b_0+b_1 X_{1p} + b_2 X_{2p} + \\epsilon_p\\], course, exact equation used earlier describe two-predictor regression model! difference \\(X_1\\) \\(X_2\\) now binary variables (.e., values can 0 1), whereas regression analysis expect \\(X_1\\) \\(X_2\\) continuous. ’s couple ways try convince . One possibility lengthy mathematical exercise proving two identical. However, ’m going go limb guess readership book find annoying rather helpful. Instead, ’ll explain basic ideas rely jamovi show ANOVA analyses regression analyses aren’t just similar, ’re identical intents purposes. Let’s start running ANOVA. , ’ll use rtfm data set, Figure 14.17 shows get run analysis jamovi.\nFigure 14.17: ANOVA rtfm.csv data set jamovi, without interaction term\n, reading key numbers ANOVA table mean scores presented earlier, can see students obtained higher grade attended class (\\(F_{1,5} = 21.6, p = .0056\\)) read textbook (\\(F_{1,5} = 52.3, p = .0008\\)). Let’s make note p-values \\(F\\) statistics.Now let’s think analysis linear regression perspective. rtfm data set, encoded attend reading numeric predictors. case, perfectly acceptable. really sense student turns class (.e. attend = 1) fact done “attendance” student (.e. attend = 0). ’s unreasonable include predictor regression model. ’s little unusual, predictor takes two possible values, doesn’t violate assumptions linear regression. ’s easy interpret. regression coefficient attend greater 0 means students attend lectures get higher grades. ’s less zero students attending lectures get lower grades. true reading variable.Wait second though. true? ’s something intuitively obvious everyone taken stats classes comfortable maths, isn’t clear everyone else first pass. see true, helps look closely specific students. Let’s start considering 6th 7th students data set (.e. \\(p = 6\\) \\(p = 7\\)). Neither one read textbook, cases can set reading = 0. , say thing mathematical notation, observe \\(X_{2,6} = 0\\) \\(X_{2,7} = 0\\). However, student number 7 turn lectures (.e., attend = 1, \\(X_{1,7} = 1\\)) whereas student number 6 (.e., attend = 0, \\(X_{1,6} = 0\\)). Now let’s look happens insert numbers general formula regression line. student number 6, regression predicts \\[\\begin{equation}\n\\begin{split}\n\\hat{Y}_6 & = b_0 + b_1 X_{1,6} + b_2 X_{2,6} \\\\\n& = b_0 + (b_1 \\times 0) + (b_2 \\times 0) \\\\\n& = b_0\n\\end{split}\n\\end{equation}\\]’re expecting student obtain grade corresponding value intercept term \\(b_0\\). student 7? time insert numbers formula regression line, obtain following\\[\\begin{equation}\n\\begin{split}\n\\hat{Y}_7 & = b_0 + b_1 X_{1,7} + b_2 X_{2,7} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 0) \\\\\n& = b_0 + b_1\n\\end{split}\n\\end{equation}\\]student attended class, predicted grade equal intercept term b0 plus coefficient associated attend variable, \\(b_1\\). , \\(b_1\\) greater zero, ’re expecting students turn lectures get higher grades students don’t. coefficient negative ’re expecting opposite: students turn class end performing much worse. fact, can push little bit . student number 1, turned class (\\(X_{1,1} = 1\\)) read textbook (\\(X_{2,1} = 1\\))? plug numbers regression get\\[\\begin{equation}\n\\begin{split}\n\\hat{Y}_1 & = b_0 + b_1 X_{1,1} + b_2 X_{2,1} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 1) \\\\\n& = b_0 + b_1 + b_2\n\\end{split}\n\\end{equation}\\]assume attending class helps get good grade (.e., \\(b1 \\> 0\\)) assume reading textbook also helps get good grade (.e., \\(b2 \\> 0\\)), expectation student 1 get grade higher student 6 student 7.point won’t suprised learn regression model predicts student 3, read book didn’t attend lectures, obtain grade \\(b_{2} + b_{0}\\). won’t bore yet another regression formula. Instead, ’ll show Table 14.9 expected grades.Table 14.9:  Expected grades regression modelAs can see, intercept term \\(b_0\\) acts like kind “baseline” grade expect students don’t take time attend class read textbook. Similarly, \\(b_1\\) represents boost ’re expected get come class, \\(b_2\\) represents boost comes reading textbook. fact, ANOVA might well want characterise b1 main effect attendance, \\(b_2\\) main effect reading! fact, simple \\(2 \\times 2\\) ANOVA ’s exactly plays .Okay, now ’re really starting see ANOVA regression basically thing, let’s actually run regression using rtfm data jamovi regression analysis convince really true. Running regression usual way gives results shown Figure 14.18.\nFigure 14.18: Regression analysis rtfm.csv data set jamovi, without interaction term\n’s interesting things note . First, notice intercept term 43.5 close “group” mean 42.5 observed two students didn’t read text attend class. Second, notice regression coefficient \\(b_1 = 18.0\\) attendance variable, suggesting students attended class scored 18% higher didn’t. expectation students turned class didn’t read textbook obtain grade \\(b_0 + b_1\\), equal \\(43.5 + 18.0 = 61.5\\). can verify thing happens look students read textbook.Actually, can push little establishing equivalence ANOVA regression. Look p-values associated attend variable reading variable regression output. ’re identical ones encountered earlier running ANOVA. might seem little surprising, since test used running regression model calculates t-statistic ANOVA calculates F-statistic. However, can remember way back Chapter Introduction probability, mentioned ’s relationship t-distribution F-distribution. quantity distributed according t-distribution k degrees freedom square , new squared quantity follows F-distribution whose degrees freedom 1 k. can check respect t statistics regression model. attend variable get t value 4.65. square number end 21.6, matches corresponding F statistic ANOVA.Finally, one last thing know. jamovi understands fact ANOVA regression examples linear models, lets extract classic ANOVA table regression model using ‘Linear Regression’ - ‘Model Coefficients’ - ‘Omnibus Test’ - ‘ANOVA Test’, give table shown Figure 14.19.\nFigure 14.19: Omnibus ANOVA Test results jamovi regression analysis\n","code":""},{"path":"factorial-anova.html","id":"how-to-encode-non-binary-factors-as-contrasts","chapter":"14 Factorial ANOVA","heading":"14.6.3 How to encode non binary factors as contrasts","text":"point, ’ve shown can view \\(2 \\times 2\\) ANOVA linear model. ’s pretty easy see generalises \\(2 \\times 2 \\times 2\\) ANOVA \\(2 \\times 2 \\times 2 \\times 2\\) ANOVA. ’s thing, really. just add new binary variable factors. begins get trickier consider factors two levels. Consider, instance, \\(3 \\times 2\\) ANOVA ran earlier chapter using clinicaltrial.csv data. can convert three-level drug factor numerical form appropriate regression?answer question pretty simple, actually. realise three-level factor can redescribed two binary variables. Suppose, instance, create new binary variable called druganxifree. Whenever drug variable equal “anxifree” set druganxifree = 1. Otherwise, set druganxifree = 0. variable sets contrast, case anxifree two drugs. , course, druganxifree contrast isn’t enough fully capture information drug variable. need second contrast, one allows us distinguish joyzepam placebo. , can create second binary contrast, called drugjoyzepam, equals 1 drug joyzepam 0 . Taken together, two contrasts allows us perfectly discriminate three possible drugs. Table 14.10 illustrates .Table 14.10:  Binary contrasts discriminate three possible drugsIf drug administered patient placebo two contrast variables equal 0. drug Anxifree druganxifree variable equal 1, drugjoyzepam 0. reverse true Joyzepam: drugjoyzepam 1 druganxifree 0.Creating contrast variables difficult using jamovi compute new variable command. example, create druganxifree variable, write logical expression compute new variable formula box: (drug == ‘anxifree’, 1, 0)‘. Similarly, create new variable drugjoyzepam use logical expression: (drug == ’joyzepam’, 1, 0). Likewise CBTtherapy: (therapy == ‘CBT’, 1, 0). can see new variables, corresponding logical expressions, jamovi data file clinicaltrial2.omv.now recoded three-level factor terms two binary variables, ’ve already seen ANOVA regression behave way binary variables. However, additional complexities arise case, ’ll discuss next section.","code":""},{"path":"factorial-anova.html","id":"the-equivalence-between-anova-and-regression-for-non-binary-factors","chapter":"14 Factorial ANOVA","heading":"14.6.4 The equivalence between ANOVA and regression for non-binary factors","text":"Now two different versions data set. original data drug variable clinicaltrial.csv file expressed single three-level factor, expanded data clinicaltrial2.omv expanded two binary contrasts. , thing want demonstrate original \\(3 \\times 2\\) factorial ANOVA equivalent regression model applied contrast variables. Let’s start re-running ANOVA, results shown Figure 14.20.\nFigure 14.20: jamovi ANOVA results, without interaction component\nObviously, surprises . ’s exact ANOVA ran earlier. Next, let’s run regression using druganxifree, drugjoyzepam CBTtherapy predictors. results shown Figure 14.21.\nFigure 14.21: jamovi regression results, contrast variables druganxifree drugjoyzepam\nHmm. isn’t output got last time. surprisingly, regression output prints results three predictors separately, just like every time conducted regression analysis. one hand can see p-value CBTtherapy variable exactly one therapy factor original ANOVA, can reassured regression model thing ANOVA . hand, regression model testing druganxifree contrast drugjoyzepam contrast separately, two completely unrelated variables. ’s surprising course, poor regression analysis way knowing drugjoyzepam druganxifree actually two different contrasts used encode three-level drug factor. far knows, drugjoyzepam druganxifree related one another drugjoyzepam therapyCBT. However, know better. stage ’re interested determining whether two contrasts individually significant. just want know ’s “overall” effect drug. , want jamovi run kind “model comparison” test, one two “drugrelated” contrasts lumped together purpose test. Sound familiar? need specify null model, case include CBTtherapy predictor, omit drug-related variables, Figure 14.22.\nFigure 14.22: Model comparison jamovi regression, null model 1 vs. contrasts model 2\nAh, ’s better. F-statistic 26.15, degrees freedom 2 14, p-value 0.00002. numbers identical ones obtained main effect drug original ANOVA. see ANOVA regression essentially . linear models, underlying statistical machinery ANOVA identical machinery used regression. importance fact understated. Throughout rest chapter ’re going rely heavily idea.Although went faff computing new variables jamovi contrasts druganxifree drugjoyzepam, just show ANOVA regression essentially , jamovi linear regression analysis actually nifty shortcut get contrasts, see Figure 14.23. jamovi allowing enter predictor variables factors , wait …factors! Smart, eh. can also specify group use reference level, via ‘Reference Levels’ option. ’ve changed ‘placebo’ ‘.therapy’, respectively, makes sense.\nFigure 14.23: Regression analysis factors contrasts jamovi, including omnibus ANOVA test results\nalso click ‘ANOVA’ test checkbox ‘Model Coefficients’ - ‘Omnibus Test’ option, see F-statistic 26.15, degrees freedom 2 14, p-value 0.00002 (Figure 14.23). numbers identical ones obtained main effect drug original ANOVA. , see ANOVA regression essentially . linear models, underlying statistical machinery ANOVA identical machinery used regression.","code":""},{"path":"factorial-anova.html","id":"degrees-of-freedom-as-parameter-counting","chapter":"14 Factorial ANOVA","heading":"14.6.5 Degrees of freedom as parameter counting!","text":"long last, can finally give definition degrees freedom happy . Degrees freedom defined terms number parameters estimated model. regression model ANOVA, number parameters corresponds number regression coefficients (.e. b-values), including intercept. Keeping mind F-test always comparison two models first df difference number parameters. example, model comparison , null model (mood.gain ~ therapyCBT) two parameters: ’s one regression coefficient therapyCBT variable, second one intercept. alternative model (mood.gain ~ druganxifree + drugjoyzepam + therapyCBT) four parameters: one regression coefficient three contrasts, one intercept. degrees freedom associated difference two models \\(df_1 = 4 - 2 = 2\\).case doesn’t seem null model? instance, might thinking F-test shows select ‘F Test’ ‘Linear Regression’ - ‘Model Fit’ options. originally described test regression model whole. However, still comparison two models. null model trivial model includes 1 regression coefficient, intercept term. alternative model contains \\(K + 1\\) regression coefficients, one K predictor variables one intercept. df value see F test equal \\(df_1 = K + 1 - 1 = K\\).second df value appears F-test? always refers degrees freedom associated residuals. possible think terms parameters , slightly counter-intuitive way. Think like . Suppose total number observations across study whole N. wanted perfectly describe N values, need using, well… N numbers. build regression model, ’re really specifying numbers need perfectly describe data. model \\(K\\) predictors intercept, ’ve specified \\(K + 1\\) numbers. , without bothering figure exactly done, many numbers think going needed transform K `1 parameter regression model perfect re-description raw data? found thinking \\((K + 1) + (N - K - 1) = N\\), answer \\(N - K - 1\\), well done! ’s exactly right. principle can imagine absurdly complicated regression model includes parameter every single data point, course provide perfect description data. model contain \\(N\\) parameters total, ’re interested difference number parameters required describe full model (.e. \\(N\\)) number parameters used simpler regression model ’re actually interested (.e., \\(K +1\\)), second degrees freedom F test \\(df_2 = N - K - 1\\), K number predictors (regression model) number contrasts (ANOVA). example gave , \\((N = 18\\) observations data set \\(K + 1 = 4\\) regression coefficients associated ANOVA model, degrees freedom residuals \\(df_2 = 18 - 4 = 14\\).","code":""},{"path":"factorial-anova.html","id":"different-ways-to-specify-contrasts","chapter":"14 Factorial ANOVA","heading":"14.7 Different ways to specify contrasts","text":"previous section, showed method converting factor collection contrasts. method showed specify set binary variables defined table like Table 14.11.Table 14.11:  Binary contrasts discriminate three possible drugsEach row table corresponds one factor levels, column corresponds one contrasts. table, always one row columns, special name. called contrast matrix. However, lots different ways specify contrast matrix. section discuss standard contrast matrices statisticians use can use jamovi. ’re planning read section [Factorial ANOVA 3: unbalanbced designs] later , ’s worth reading section carefully. , can get away skimming , choice contrasts doesn’t matter much balanced designs.","code":""},{"path":"factorial-anova.html","id":"treatment-contrasts","chapter":"14 Factorial ANOVA","heading":"14.7.1 Treatment contrasts","text":"particular kind contrasts ’ve described , one level factor special, acts kind “baseline” category (.e., placebo example), two defined. name kinds contrasts treatment contrasts, also known “dummy coding”. contrast level factor compared base reference level, base reference level value intercept.name reflects fact contrasts quite natural sensible one categories factor really special actually represent baseline. makes sense clinical trial example. placebo condition corresponds situation don’t give people real drugs, ’s special. two conditions defined relation placebo. one case replace placebo Anxifree, case replace Joyzepam.table shown matrix treatment contrasts factor 3 levels. suppose want matrix treatment contrasts factor 5 levels? set like Table 14.12.Table 14.12:  Matrix treatment contrasts 5 levelsIn example, first contrast level 2 compared level 1, second contrast level 3 compared level 1, . Notice , default, first level factor always treated baseline category (.e., ’s one zeros doesn’t explicit contrast associated ). jamovi can change category first level factor manipulating order levels variable shown ‘Data Variable’ window (double click name variable spreadsheet column bring ‘Data Variable’ view.","code":""},{"path":"factorial-anova.html","id":"helmert-contrasts","chapter":"14 Factorial ANOVA","heading":"14.7.2 Helmert contrasts","text":"Treatment contrasts useful lot situations. However, make sense situation really baseline category, want assess groups relation one. situations, however, baseline category exists, may make sense compare group mean groups. meet Helmert contrasts, generated ‘helmert’ option jamovi ‘ANOVA’ - ‘Contrasts’ selection box. idea behind Helmert contrasts compare group mean “previous” ones. , first contrast represents difference group 2 group 1, second contrast represents difference group 3 mean groups 1 2, . translates contrast matrix looks like Table 14.13 factor five levels.Table 14.13:  Matrix helmert contrasts 5 levelsOne useful thing Helmert contrasts every contrast sums zero (.e., columns sum zero). consequence , interpret ANOVA regression, intercept term corresponds grand mean \\(\\mu_{..}\\) using Helmert contrasts. Compare treatment contrasts, intercept term corresponds group mean baseline category. property can useful situations. doesn’t matter much balanced design, ’ve assuming far, turn important later consider unbalanced designs. fact, main reason ’ve even bothered include section contrasts become important want understand unbalanced ANOVA.","code":""},{"path":"factorial-anova.html","id":"sum-to-zero-contrasts","chapter":"14 Factorial ANOVA","heading":"14.7.3 Sum to zero contrasts","text":"third option briefly mention “sum zero” contrasts, called “Simple” contrasts jamovi, used construct pairwise comparisons groups. Specifically, contrast encodes difference one groups baseline category, case corresponds first group (Table 14.14).Table 14.14:  Matrix 'sum-'zero contrasts 5 levelsMuch like Helmert contrasts, see column sums zero, means intercept term corresponds grand mean ANOVA treated regression model. interpreting contrasts, thing recognise contrasts pairwise comparison group 1 one four groups. Specifically, contrast 1 corresponds “group 2 minus group 1” comparison, contrast 2 corresponds “group 3 minus group 1” comparison, .163","code":""},{"path":"factorial-anova.html","id":"optional-contrasts-in-jamovi","chapter":"14 Factorial ANOVA","heading":"14.7.4 Optional contrasts in jamovi","text":"jamovi also comes variety options can generate different kinds contrasts ANOVA. can found ‘Contrasts’ option main ANOVA analysis window, contrast types Table 14.15 listed:Table 14.15:  Contrasts types available jamovi ANOVA analysis","code":""},{"path":"factorial-anova.html","id":"post-hoc-tests","chapter":"14 Factorial ANOVA","heading":"14.8 Post hoc tests","text":"Time switch different topic. Rather pre-planned comparisons tested using contrasts, let’s suppose ’ve done ANOVA turns obtained significant effects. fact F-tests “omnibus” tests really test null hypothesis differences among groups, obtaining significant effect doesn’t tell groups different ones. discussed issue back Comparing several means (one-way ANOVA), chapter solution run t-tests possible pairs groups, making corrections multiple comparisons (e.g., Bonferroni, Holm) control Type error rate across comparisons. methods used back chapter Comparing several means (one-way ANOVA) advantage relatively simple kind tools can use lot different situations ’re testing multiple hypotheses, ’re necessarily best choices ’re interested efficient post hoc testing ANOVA context. actually quite lot different methods performing multiple comparisons statistics literature (Hsu 1996), beyond scope introductory text like one discuss detail.said, ’s one tool want draw attention , namely Tukey’s “Honestly Significant Difference”, Tukey’s HSD short. , ’ll spare formulas just stick qualitative ideas. basic idea Tukey’s HSD examine relevant pairwise comparisons groups, ’s really appropriate use Tukey’s HSD pairwise differences ’re interested .164 instance, earlier conducted factorial ANOVA using clinicaltrial.csv data set, specified main effect drug main effect therapy interested following four comparisons:difference mood gain people given Anxifree versus people given placebo.difference mood gain people given Joyzepam versus people given placebo.difference mood gain people given Anxifree versus people given Joyzepam.difference mood gain people treated CBT people given therapy.one comparisons, ’re interested true difference (population) group means. Tukey’s HSD constructs simultaneous confidence intervals four comparisons. mean 95% “simultaneous” confidence interval , repeat study many times, 95% study results confidence intervals contain relevant true value. Moreover, can use confidence intervals calculate adjusted p value specific comparison.TukeyHSD function jamovi pretty easy use. simply specify ANOVA model term want run post hoc tests . example, looking run post hoc tests main effects interaction, open ‘Post Hoc Tests’ option ANOVA analysis screen, move drug therapy variables across box right, select ‘Tukey’ checkbox list possible post hoc corrections applied. , along corresponding results table, shown Figure 14.24.\nFigure 14.24: Tukey HSD post hoc test jamovi factorial ANOVA, without interaction term\noutput shown ‘Post Hoc Tests’ results table (hope) pretty straightforward. first comparison, example, Anxifree versus placebo difference, first part output indicates observed difference group means .27. next number standard error difference, calculate 95% confidence interval wanted, though jamovi currently provide option. column degrees freedom, column t-value, finally column p-value. first comparison adjusted p-value .21. contrast, look next line, see observed difference joyzepam placebo 1.03, result significant (p < .001).far, good. situation model includes interaction terms? instance, default option jamovi allow possibility interaction drug therapy. ’s case, number pairwise comparisons need consider starts increase. , need consider three comparisons relevant main effect drug one comparison relevant main effect therapy. , want consider possibility significant interaction (try find group differences underpin significant interaction), need include comparisons following:difference mood gain people given Anxifree treated CBT, versus people given placebo treated CBTThe difference mood gain people given Anxifree given therapy, versus people given placebo given therapy.etcThere quite lot comparisons need consider. , run Tukey post hoc analysis ANOVA model, see made lot pairwise comparisons (19 total), shown Figure 14.25. can see looks pretty similar , lot comparisons made.\nFigure 14.25: Tukey HSD post hoc test jamovi factorial ANOVA interaction term\n","code":""},{"path":"factorial-anova.html","id":"the-method-of-planned-comparisons","chapter":"14 Factorial ANOVA","heading":"14.9 The method of planned comparisons","text":"Following previous sections contrasts post hoc tests ANOVA, think method planned comparisons important enough deserve quick discussion. discussions multiple comparisons, previous section back Comparing several means (one-way ANOVA), ’ve assuming tests want run genuinely post hoc. instance, drugs example , maybe thought drugs different effects mood (.e., hypothesised main effect drug), didn’t specific hypothesis different, real idea pairwise comparisons worth looking . case, really resort something like Tukey’s HSD pairwise comparisons.situation rather different, however, genuinely real, specific hypotheses comparisons interest, never ever intention look comparisons besides ones specified ahead time. true, honestly rigorously stick noble intentions run comparisons (even data look like ’re showing deliciously significant effects stuff didn’t hypothesis test ), doesn’t really make lot sense run something like Tukey’s HSD, makes corrections whole bunch comparisons never cared never intention looking . circumstances, can safely run (limited) number hypothesis tests without making adjustment multiple testing. situation known method planned comparisons, sometimes used clinical trials. However, consideration scope introductory book, least know method exists!","code":""},{"path":"factorial-anova.html","id":"factorial-anova-3-unbalanced-designs","chapter":"14 Factorial ANOVA","heading":"14.10 Factorial ANOVA 3: unbalanced designs","text":"Factorial ANOVA handy thing know . ’s one standard tools used analyse experimental data many decades, ’ll find can’t read two three papers psychology without running ANOVA somewhere. However, ’s one huge difference ANOVAs ’ll see lot real scientific articles ANOVAs ’ve described far. real life ’re rarely lucky enough perfectly balanced designs. one reason another, ’s typical end observations cells others. , put another way, unbalanced design.Unbalanced designs need treated lot care balanced designs, statistical theory underpins lot messier. might consequence messiness, might shortage time, experience undergraduate research methods classes psychology nasty tendency ignore issue completely. lot stats textbooks tend gloss . net result , think, lot active researchers field don’t actually know ’s several different “types” unbalanced ANOVAs, produce quite different answers. fact, reading psychological literature, ’m kind amazed fact people report results unbalanced factorial ANOVA don’t actually give enough details reproduce analysis. secretly suspect people don’t even realise statistical software package making whole lot substantive data analysis decisions behalf. ’s actually little terrifying think . , want avoid handing control data analysis stupid software, read .","code":""},{"path":"factorial-anova.html","id":"the-coffee-data","chapter":"14 Factorial ANOVA","heading":"14.10.1 The coffee data","text":"usual, help us work data. coffee.csv file contains hypothetical data set produces unbalanced \\(3 \\times 2\\) ANOVA. Suppose interested finding whether tendency people babble much coffee purely effect coffee , whether ’s effect milk sugar people add coffee. Suppose took 18 people gave coffee drink. amount coffee / caffeine held constant, varied whether milk added, milk binary factor two levels, “yes” “”. also varied kind sugar involved. coffee might contain “real” sugar might contain “fake” sugar (.e., artificial sweetener) might contain “none” , sugar variable three level factor. outcome variable continuous variable presumably refers psychologically sensible measure extent someone “babbling”. details don’t really matter purpose. Take look data jamovi spreadsheet view, Figure 14.26.\nFigure 14.26: coffee.csv data set jamovi, descriptive information aggregated factor levels\nLooking table means Figure 14.26 get strong impression differences groups. especially true compare means standard deviations babble variable. Across groups, standard deviation varies .14 .71, fairly small relative differences group means.165 Whilst first may seem like straightforward factorial ANOVA, problem arises look many observations group. See different Ns different groups shown Figure 14.26. violates one original assumptions, namely number people group . haven’t really discussed handle situation.","code":""},{"path":"factorial-anova.html","id":"standard-anova-does-not-exist-for-unbalanced-designs","chapter":"14 Factorial ANOVA","heading":"14.10.2 “Standard ANOVA” does not exist for unbalanced designs","text":"Unbalanced designs lead us somewhat unsettling discovery isn’t really one thing might refer standard ANOVA. fact, turns three fundamentally different ways166 might want run ANOVA unbalanced design. balanced design three versions produce identical results, sums squares, F-values, etc., conforming formulas gave start chapter. However, design unbalanced don’t give answers. Furthermore, equally appropriate every situation. methods appropriate situation others. Given , ’s important understand different types ANOVA differ one another.first kind ANOVA conventionally referred Type sum squares. ’m sure can guess two called. “sum squares” part name introduced SAS statistical software package become standard nomenclature, ’s bit misleading ways. think logic referring different types sum squares , look ANOVA tables produce, key difference numbers SS values. degrees freedom don’t change, MS values still defined SS divided df, etc. However, terminology gets wrong hides reason SS values different one another. end, ’s lot helpful think three different kinds ANOVA three different hypothesis testing strategies. different strategies lead different SS values, sure, ’s strategy important thing , SS values . Recall section ANOVA linear model particular F-test best thought comparison two linear models. , ’re looking ANOVA table, helps remember F-tests corresponds pair models compared. course, leads naturally question pair models compared. fundamental difference ANOVA Types , II III: one corresponds different way choosing model pairs tests.","code":""},{"path":"factorial-anova.html","id":"type-i-sum-of-squares","chapter":"14 Factorial ANOVA","heading":"14.10.3 Type I sum of squares","text":"Type method sometimes referred “sequential” sum squares, involves process adding terms model one time. Consider coffee data, instance. Suppose want run full \\(3 \\times 2\\) factorial ANOVA, including interaction terms. full model contains outcome variable babble, predictor variables sugar milk, interaction term sugar \\(\\times\\) milk. can written \\(babble \\sim sugar + milk + sugar {\\times} milk\\). Type strategy builds model sequentially, starting simplest possible model gradually adding terms.simplest possible model data one neither milk sugar assumed effect babbling. term included model intercept, written babble ~ 1. initial null hypothesis. next simplest model data one one two main effects included. coffee data, two different possible choices , choose add milk first add sugar first. order actually turns matter, ’ll see later, now let’s just make choice arbitrarily pick sugar. , second model sequence models babble ~ sugar, forms alternative hypothesis first test. now first hypothesis test (Table 14.16).Table 14.16:  Null alternative hypotheses outcome variable 'babble'comparison forms hypothesis test main effect sugar. next step model building exercise add main effect term, next model sequence babble ~ sugar + milk. second hypothesis test formed comparing following pair models (Table 14.17).Table 14.17:  null alternative hypotheses outcome variable 'babble'comparison forms hypothesis test main effect milk. one sense, approach elegant: alternative hypothesis first test forms null hypothesis second one. sense Type method strictly sequential. Every test builds directly results last one. However, another sense ’s inelegant, ’s strong asymmetry two tests. test main effect sugar (first test) completely ignores milk, whereas test main effect milk (second test) take sugar account. case, fourth model sequence now full model, babble ~ sugar + milk + sugar \\(\\times\\) milk, corresponding hypothesis test shown Table 14.18.Table 14.18:  possible null alternative hypotheses outcome variable 'babble'Type III sum squares default hypothesis testing method used jamovi ANOVA, run Type sum squares analysis select ‘Type 1’ ‘Sum squares’ selection box jamovi ‘ANOVA’ - ‘Model’ options. gives us ANOVA table shown Figure 14.27.\nFigure 14.27: ANOVA results table using Type sum squares jamovi\nbig problem using Type sum squares fact really depend order enter variables. Yet, many situations researcher reason prefer one ordering another. presumably case milk sugar problem. add milk first sugar first? feels exactly arbitrary data analysis question coffee-making question. may fact people firm opinions ordering, ’s hard imagine principled answer question. Yet, look happens change ordering, Figure 14.28.\nFigure 14.28: ANOVA results table using Type sum squares jamovi, factors entered different order (milk first)\np-values main effect terms changed, fairly dramatically. Among things, effect milk become significant (though one avoid drawing strong conclusions , ’ve mentioned previously). two ANOVAs one report? ’s immediately obvious.look hypothesis tests used define “first” main effect “second” one, ’s clear ’re qualitatively different one another. initial example, saw test main effect sugar completely ignores milk, whereas test main effect milk take sugar account. , Type testing strategy really treat first main effect kind theoretical primacy second one. experience rarely ever theoretically primacy kind justify treating two main effects asymmetrically.consequence Type tests rarely much interest, move discuss Type II tests Type III tests.","code":""},{"path":"factorial-anova.html","id":"type-iii-sum-of-squares","chapter":"14 Factorial ANOVA","heading":"14.10.4 Type III sum of squares","text":"just finished talking Type tests, might think natural thing next talk Type II tests. However, think ’s actually bit natural discuss Type III tests (simple default jamovi ANOVA) talking Type II tests (trickier). basic idea behind Type III tests extremely simple. Regardless term ’re trying evaluate, run F-test alternative hypothesis corresponds full ANOVA model specified user, null model just deletes one term ’re testing. instance, coffee example, full model babble ~ sugar + milk + sugar \\(\\times\\) milk, test main effect sugar correspond comparison following two models (Table 14.19).Table 14.19:  Null alternative hypotheses outcome variable 'babble', Type III sum squaresSimilarly main effect milk evaluated testing full model null model removes milk term, like Table 14.20.Table 14.20:  null alternative hypotheses outcome variable 'babble', Type III sum squaresFinally, interaction term sugar \\(\\times\\) milk evaluated exactly way. , test full model null model removes sugar \\(\\times\\) milk interaction term, like Table Table 14.21.Table 14.21:  Removing interaction term hypotheses outcome variable 'babble', Type III sum squaresThe basic idea generalises higher order ANOVAs. instance, suppose trying run ANOVA three factors, , B C, wanted consider possible main effects possible interactions, including three way interaction \\(\\times\\) B \\(\\times\\) C. (Table 14.22 shows Type III tests look like situation).Table 14.22:  Type III tests three factors main effect interaction termsAs ugly table looks, ’s pretty simple. cases, alternative hypothesis corresponds full model contains three main effect terms (e.g. ), three two-way interactions (e.g. *B) one three-way interaction (.e., *B*C)). null model always contains 6 7 terms, missing one one whose significance ’re trying test.first pass, Type III tests seem like nice idea. Firstly, ’ve removed asymmetry caused us problems running Type tests. ’re now treating terms way, results hypothesis tests depend order specify . definitely good thing. However, big problem interpreting results tests, especially main effect terms. Consider coffee data. Suppose turns main effect milk significant according Type III tests. telling us babble ~ sugar + sugar*milk better model data full model. even mean? interaction term sugar*milk also non significant, ’d tempted conclude data telling us thing matters sugar. suppose significant interaction term, non-significant main effect milk. case, assume really “effect sugar”, “interaction milk sugar”, “effect milk”? seems crazy. right answer simply must ’s meaningless167 talk main effect interaction significant. general, seems statisticians advise us , think ’s right advice. really meaningless talk non-significant main effects presence significant interaction, ’s obvious Type III tests allow null hypothesis rely model includes interaction omits one main effects make . characterised fashion, null hypotheses really don’t make much sense .Later , ’ll see Type III tests can redeemed contexts, first let’s take look ANOVA results table using Type III sum squares, see Figure 14.29.\nFigure 14.29: ANOVA results table using Type III sum squares jamovi\naware, one perverse features Type III testing strategy typically results turn depend contrasts use encode factors (see Different ways specify contrasts section ’ve forgotten different types contrasts ).168Okay, p-values typically come Type III analyses (jamovi) sensitive choice contrasts, mean Type III tests essentially arbitrary trusted? extent ’s true, turn discussion Type II tests ’ll see Type II analyses avoid arbitrariness entirely, think ’s strong conclusion. Firstly, ’s important recognise choices contrasts always produce answers (ah, happening jamovi). particular importance fact columns contrast matrix constrained sum zero, Type III analysis always give answers.Type II tests ’ll see Type II analyses avoid arbitrariness entirely, think ’s strong conclusion. Firstly, ’s important recognise choices contrasts always produce answers (ah, happening jamovi). particular importance fact columns contrast matrix constrained sum zero, Type III analysis always give answers.","code":""},{"path":"factorial-anova.html","id":"type-ii-sum-of-squares","chapter":"14 Factorial ANOVA","heading":"14.10.5 Type II sum of squares","text":"Okay, ’ve seen Type III tests now, pretty straightforward. Type tests performed gradually adding terms one time, whereas Type III tests performed taking full model looking see happens remove term. However, can limitations. Type tests dependent order enter terms, Type III tests dependent code contrasts. Type II tests little harder describe, avoid problems, result little easier interpret.Type II tests broadly similar Type III tests. Start “full” model, test particular term deleting model. However, Type II tests based marginality principle states omit lower order term model higher order ones depend . , instance, model contains two-way interaction \\(\\times\\) B (2nd order term), really contain main effects B (1st order terms). Similarly, contains three-way interaction term \\(\\times\\) B \\(\\times\\) C, model must also include main effects , B C well simpler interactions \\(\\times\\) B, \\(\\times\\) C B \\(\\times\\) C. Type III tests routinely violate marginality principle. instance, consider test main effect context three-way ANOVA includes possible interaction terms. According Type III tests, null alternative models Table 14.23.Table 14.23:  Type III tests main effect, , three-way ANOVA possible interaction termsNotice null hypothesis omits , includes \\(\\times\\) B, \\(\\times\\) C \\(\\times\\) B \\(\\times\\) C part model. , according Type II tests, good choice null hypothesis. instead, want test null hypothesis relevant outcome, specify null hypothesis complicated model rely form, even interaction. alternative hypothesis corresponds null model plus main effect term . lot closer people intuitively think “main effect ”, yields following Type II test main effect (Table 14.24). 169Table 14.24:  Type II tests main effect, , three-way ANOVA possible interaction termsAnyway, just give sense Type II tests play , ’s full table (Table 14.25) tests applied three-way factorial ANOVA:Table 14.25:  Type II tests three-way factorial modelIn context two way ANOVA ’ve using coffee data, hypothesis tests even simpler. main effect sugar corresponds F-test comparing two models (Table 14.26).Table 14.26:  Type II tests main effect sugar coffee dataThe test main effect milk Table 14.27.Table 14.27:  Type II tests main effect milk coffee dataFinally, test interaction sugar \\(\\times\\) milk Table 14.28.Table 14.28:  Type II tests sugar $\\times$ milk interaction termRunning tests straightforward. Just select ‘Type 2’ ‘Sum squares’ selection box jamovi ‘ANOVA’ - ‘Model’ options, gives us ANOVA table shown Figure 14.30.\nFigure 14.30: ANOVA results table using Type II sum squares jamovi\nType II tests clear advantages Type Type III tests. don’t depend order specify factors (unlike Type ), don’t depend contrasts use specify factors (unlike Type III). although opinions may differ last point, definitely depend ’re trying data, think hypothesis tests specify likely correspond something actually care . consequence, find ’s usually easier interpret results Type II test results Type Type III test. reason tentative advice , can’t think obvious model comparisons directly map onto research questions still want run ANOVA unbalanced design, Type II tests probably better choice Type Type III.170","code":""},{"path":"factorial-anova.html","id":"effect-sizes-and-non-additive-sums-of-squares","chapter":"14 Factorial ANOVA","heading":"14.10.6 Effect sizes (and non-additive sums of squares)","text":"jamovi also provides effect sizes \\(\\eta^2\\) partial \\(\\eta^2\\) select options, Figure 14.30. However, ’ve got unbalanced design ’s bit extra complexity involved.remember back early discussions ANOVA, one key ideas behind sums squares calculations add SS terms associated effects model, add residual SS, ’re supposed add total sum squares. , top , whole idea behind \\(\\eta^2\\) , ’re dividing one SS terms total SS value, \\(\\eta^2\\) value can interpreted proportion variance accounted particular term. straightforward unbalanced designs variance goes “missing”.seems bit odd first, ’s . unbalanced designs factors become correlated one another, becomes difficult tell difference effect Factor effect Factor B. extreme case, suppose ’d run \\(2 \\times 2\\) design number participants group Table 14.29.Table 14.29:  N participants 2 $\\times$ 2 (!) unbalanced factorial designHere spectacularly unbalanced design: 100 people milk sugar, 100 people milk sugar, ’s . 0 people milk sugar, 0 people sugar milk. Now suppose , collected data, turned large (statistically significant) difference “milk sugar” group “-milk -sugar” group. main effect sugar? main effect milk? interaction? ’s impossible tell, presence sugar perfect association presence milk. Now suppose design little balanced (Table 14.30.Table 14.30:  N participants 2 $\\times$ 2 still unbalanced factorial designThis time around, ’s technically possible distinguish effect milk effect sugar, people one . However, still pretty difficult , association sugar milk still extremely strong, observations two groups. , ’re likely situation know predictor variables (milk sugar) related outcome (babbling), don’t know nature relationship main effect one predictor, interaction.","code":""},{"path":"factorial-anova.html","id":"summary-12","chapter":"14 Factorial ANOVA","heading":"14.11 Summary","text":"[Factorial ANOVA : balanced designs, interactions] interactions includedEffect size, estimated means, confidence intervals factorial ANOVAAssumption checking ANOVAAnalysis Covariance (ANCOVA)Understanding ANOVA linear model, including Different ways specify contrastsPost hoc tests using Tukey’s HSD brief commentary method planned comparisonsFactorial ANOVA 3: unbalanced designs","code":""},{"path":"factor-analysis.html","id":"factor-analysis","chapter":"15 Factor Analysis","heading":"15 Factor Analysis","text":"Previous chapters covered statistical tests differences two groups. However, sometimes conducting research, may wish examine multiple variables co-vary. , related whether patterns relatedness suggest anything interesting meaningful. example, often interested exploring whether underlying unobserved latent factors represented observed, directly measured, variables dataset. statistics, latent factors initially hidden variables directly observed rather inferred (statistical analysis) variables observed (directly measured).chapter consider number different Factor Analysis related techniques, starting Exploratory Factor Analysis (EFA). EFA statistical technique identifying underlying latent factors data set. cover Principal Component Analysis (PCA) data reduction technique , strictly speaking, identify underlying latent factors. Instead, PCA simply produces linear combination observed variables. Following , section Confirmatory Factor Analysis (CFA) shows , unlike EFA, CFA start idea - model - variables data related . test model observed data assess good fit model . sophisticated version CFA -called Multi-Trait Multi-Method CFA approach latent factor method variance included model. useful different methodological approaches used measurement therefore method variance important consideration. Finally, cover related analysis: Internal consistency reliability analysis tests consistently scale measures psychological construct.","code":""},{"path":"factor-analysis.html","id":"exploratory-factor-analysis","chapter":"15 Factor Analysis","heading":"15.1 Exploratory Factor Analysis","text":"Exploratory Factor Analysis (EFA) statistical technique revealing hidden latent factors can inferred observed data. technique calculates extent set measured variables, example \\(V1, V2, V3, V4\\), \\(V5\\), can represented measures underlying latent factor. latent factor measured just one observed variable instead manifested relationships causes set observed variables.Figure 15.1 observed variable \\(V\\) ‘caused’ extent underlying latent factor (\\(F\\)), depicted coefficients \\(b_1\\) \\(b_5\\) (also called factor loadings). observed variable also associated error term, e1 e5. error term variance associated observed variable, \\(V_i\\) , unexplained underlying latent factor.\nFigure 15.1:  Latent factor underlying relationship several observed variables\nPsychology, latent factors represent psychological phenomena constructs difficult directly observe measure. example, personality, intelligence, thinking style. example Figure 15.1 may asked people five specific questions behaviour attitudes, able get picture personality construct called, example, extraversion. different set specific questions may give us picture individual’s introversion, conscientiousness.’s another example: may able directly measure statistics anxiety, can measure whether statistics anxiety high low set questions questionnaire. example, “\\(Q1\\): assignment statistics course”, “\\(Q2\\): Trying understand statistics described journal article”, “\\(Q3\\): Asking lecturer help understanding something course”, etc., rated low anxiety high anxiety. People high statistics anxiety tend give similarly high responses observed variables high statistics anxiety. Likewise, people low statistics anxiety give similar low responses variables low statistics anxiety.exploratory factor analysis (EFA), essentially exploring correlations observed variables uncover interesting, important underlying (latent) factors identified observed variables co-vary. can use statistical software estimate latent factors identify variables high loading171 (e.g. loading > 0.5) factor, suggesting useful measure, indicator, latent factor. Part process includes step called rotation, honest pretty weird idea luckily don’t worry understanding ; just need know helpful makes pattern loadings different factors much clearer. , rotation helps seeing clearly variables linked substantively factor. also need decide many factors reasonable given data, helpful regard something called Eigen values. ’ll come back moment, covered main assumptions EFA.","code":""},{"path":"factor-analysis.html","id":"checking-assumptions","chapter":"15 Factor Analysis","heading":"15.1.1 Checking assumptions","text":"couple assumptions need checked part analysis. first assumption sphericity, essentially checks variables dataset correlated extent can potentially summarised smaller set factors. Bartlett’s test sphericity checks whether observed correlation matrix diverges significantly zero (null) correlation matrix. , Bartlett’s test significant (\\(p < .05\\)), indicates observed correlation matrix significantly divergent null, therefore suitable EFA.second assumption sampling adequacy checked using Kaiser-MeyerOlkin (KMO) Measure Sampling Adequacy (MSA). KMO index measure proportion variance among observed variables might common variance. Using partial correlations, checks factors load just two items. seldom, ever, want EFA producing lot factors loading just two items . KMO sampling adequacy partial correlations typically seen inadequate samples. KMO index high (\\(\\approx 1\\)), EFA efficient whereas KMO low (\\(\\approx 0\\)), EFA relevant. KMO values smaller \\(0.5\\) indicates EFA suitable KMO value \\(0.6\\) present EFA considered suitable. Values \\(0.5\\) \\(0.7\\) considered adequate, values \\(0.7\\) \\(0.9\\) good values \\(0.9\\) \\(1.0\\) excellent.","code":""},{"path":"factor-analysis.html","id":"what-is-efa-good-for","chapter":"15 Factor Analysis","heading":"15.1.2 What is EFA good for?","text":"EFA provided good solution (.e. factor model), need decide shiny new factors. Researchers often use EFA psychometric scale development. develop pool questionnaire items think relate one psychological constructs, use EFA see items “go together” latent factors, assess whether items removed don’t usefully distinctly measure one latent factors.line approach, another consequence EFA combine variables load onto distinct factors factor score, sometimes known scale score. two options combining variables scale score:Create new variable score weighted factor loadings item contributes factor.Create new variable based item contributes factor, weighting equally.first option item’s contribution combined score depends strongly relates factor. second option typically just average across items contribute substantively factor create combined scale score variable. choose matter preference, though disadvantage first option loadings can vary quite bit sample sample, behavioural health sciences often interested developing using composite questionnaire scale scores across different studies different samples. case reasonable use composite measure based substantive items contributing equally rather weighting sample specific loadings different sample. case, understanding combined variable measure average items simpler intuitive using sample specific optimally-weighted combination.advanced statistical technique, one beyond scope book, undertakes regression modelling latent factors used prediction models latent factors. called “structural equation modelling” specific software programmes R packages dedicated approach. let’s get ahead ; really focus now EFA jamovi.","code":""},{"path":"factor-analysis.html","id":"efa-in-jamovi","chapter":"15 Factor Analysis","heading":"15.1.3 EFA in jamovi","text":"First, need data. Twenty-five personality self-report items (see Figure 15.2 taken International Personality Item Pool included part Synthetic Aperture Personality Assessment (SAPA) web-based personality assessment (SAPA: http://sapa-project.org) project. 25 items organized five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, Openness.item data collected using 6-point response scale:InaccurateModerately InaccurateSlightly InaccurateSlightly AccurateModerately AccurateVery Accurate.sample \\(N=250\\) responses contained dataset bfi_sample.csv. researchers, interested exploring data see whether underlying latent factors measured reasonably well \\(25\\) observed variables bfi_sample.csv data file. Open dataset check \\(25\\) variables coded continuous variables (technically ordinal though EFA jamovi mostly doesn’t matter, except decide calculate weighted factor scores case continuous variables needed). perform EFA jamovi:\nFigure 15.2: Twenty-five observed variable items organised five putative personality factors dataset bfi_sample.csv\nSelect Factor - Exploratory Factor Analysis main jamovi button bar open EFA analysis window (Figure 15.3).Select Factor - Exploratory Factor Analysis main jamovi button bar open EFA analysis window (Figure 15.3).Select 25 personality questions transfer ‘Variables’ box.Select 25 personality questions transfer ‘Variables’ box.Check appropriate options, including ‘Assumption Checks’, also Rotation ‘Method’, ‘Number Factors’ extract, ‘Additional Output’ options. See Figure 15.3 suggested options illustrative EFA, please note Rotation ‘Method’ ‘Number Factors’ extracted typically adjusted researcher analysis find best result, described .Check appropriate options, including ‘Assumption Checks’, also Rotation ‘Method’, ‘Number Factors’ extract, ‘Additional Output’ options. See Figure 15.3 suggested options illustrative EFA, please note Rotation ‘Method’ ‘Number Factors’ extracted typically adjusted researcher analysis find best result, described .\nFigure 15.3: jamovi EFA analysis window\n\nFigure 15.4: jamovi EFA assumption checks personality questionnaire data\nFirst, check assumptions (Figure 15.4). can see (1) Bartlett’s test sphericity significant, assumption satisfied; (2) KMO measure sampling adequacy (MSA) \\(0.81\\) overall, suggesting good sampling adequacy. problems !next thing check many factors use (“extract” data). Three different approaches available:One convention choose components Eigen values greater 12 . give us four factors data (try see).One convention choose components Eigen values greater 12 . give us four factors data (try see).Examination scree plot, Figure @ref(fig:fig15-5, lets identify “point inflection”. point slope scree curve clearly levels , “elbow”. give us five factors data. Interpreting scree plots bit art: Figure 15.5 noticeable step \\(5\\) \\(6\\) factors, scree plots look clear cut.Examination scree plot, Figure @ref(fig:fig15-5, lets identify “point inflection”. point slope scree curve clearly levels , “elbow”. give us five factors data. Interpreting scree plots bit art: Figure 15.5 noticeable step \\(5\\) \\(6\\) factors, scree plots look clear cut.Using parallel analysis technique, obtained Eigen values compared obtained random data. number factors extracted number Eigen values greater found random data.Using parallel analysis technique, obtained Eigen values compared obtained random data. number factors extracted number Eigen values greater found random data.\nFigure 15.5: Scree plot personality data jamovi EFA, showing noticeable inflection levelling point 5 (‘elbow’)\nthird approach good one according Fabrigar et al. (1999), although practice researchers tend look three make judgement number factors easily helpfully interpreted. can understood “meaningfulness criterion”, researchers typically examine, addition solution one approaches , solutions one two fewer factors. adopt solution makes sense .time, also consider best way rotate final solution. two main approaches rotation: orthogonal (e.g. ‘varimax’) rotation forces selected factors uncorrelated, whereas oblique (e.g. ‘oblimin’) rotation allows selected factors correlated. Dimensions interest psychologists behavioural scientists often dimensions expect orthogonal, oblique solutions arguably sensible172\nFigure 15.6: Factor summary statistics correlations five factor solution jamovi EFA\nPractically, oblique rotation factors found substantially correlated (positive negative, > 0.3), Figure 15.6 correlation two extracted factors 0.31, confirm intuition prefer oblique rotation. factors , fact, correlated, oblique rotation produce better estimate true factors better simple structure orthogonal rotation. , oblique rotation indicates factors close zero correlations one another, researcher can go ahead conduct orthogonal rotation (give solution oblique rotation).checking correlation extracted factors least one correlation greater 0.3 (Figure 15.6), oblique (‘oblimin’) rotation five extracted factors preferred. can also see Figure 15.6 proportion overall variance data accounted five factors 46%. Factor one accounts around 10% variance, factors two four around 9% , factor five just 7%. isn’t great; better overall solution accounted substantive proportion variance data.aware every EFA potentially number factors observed variables, every additional factor include add smaller amount explained variance. first factors explain good amount variance original 25 variables, factors clearly useful, simpler substitute 25 variables. can drop rest without losing much original variability. takes 18 factors (example) explain variance 25 variables, might well just use original 25.Figure 15.7 shows factor loadings. , 25 different personality items load onto five selected factors. hidden loadings less \\(0.3\\) (set options shown Figure 15.3.Factors \\(1, 2, 3\\) \\(4\\) pattern factor loadings closely matches putative factors specified Figure 15.2. Phew! factor \\(5\\) pretty close, four five observed variables putatively measure “openness” loading pretty well onto factor. Variable \\(04\\) doesn’t quite seem fit though, factor solution Figure 15.7 suggests loads onto factor \\(4\\) (albeit relatively low loading) substantively onto factor \\(5\\).thing note variables denoted “R: reverse coding” Figure 15.2 negative factor loadings. Take look items A1 (“indifferent feelings others”) A2 (“Inquire others’ well-”). can see high score \\(A1\\) indicates low Agreeableness, whereas high score \\(A2\\) (“” variables matter) indicates high Agreeableness. Therefore A1 negatively correlated “” variables, negative factor loading, shown Figure 15.7.\nFigure 15.7: Factor loadings five factor solution jamovi EFA\ncan also see Figure 15.7 “uniqueness” variable. Uniqueness proportion variance ‘unique’ variable explained factors173. example, 72% variance ‘A1’ explained factors five factor solution. contrast, ‘N1’ relatively low variance accounted factor solution (35%). Note greater ‘uniqueness’, lower relevance contribution variable factor model.honest, ’s unusual get neat solution EFA. ’s typically quite bit messy , often interpreting meaning factors challenging. ’s often clearly delineated item pool. often whole heap observed variables think may indicators underlying latent factors, don’t strong sense variables going go !, seem pretty good five factor solution, albeit accounting relatively low overall proportion observed variance. Let’s assume happy solution want use factors analysis. straightforward option calculate overall (average) score factor adding together score variable loads substantively onto factor dividing number variables. person dataset mean, example Agreeableness factor, adding together \\(A1 + A2 + A3 + A4 + A5\\), dividing 5. 174 essence, means factor score calculated based equally weighted scores included variables. can jamovi two steps:Recode A1 “A1R” reverse scoring values variable (.e. \\(6 = 1\\); \\(5 = 2\\); \\(4 = 3\\); \\(3 = 4\\); \\(2 = 5\\); \\(1 = 6\\)) using jamovi transform variable command (see Figure 15.8).Recode A1 “A1R” reverse scoring values variable (.e. \\(6 = 1\\); \\(5 = 2\\); \\(4 = 3\\); \\(3 = 4\\); \\(2 = 5\\); \\(1 = 6\\)) using jamovi transform variable command (see Figure 15.8).Compute new variable, called “Agreeableness’, calculating mean A1R, A2, A3, A4 A5. using jamovi compute new variable command (see Figure 15.9).Compute new variable, called “Agreeableness’, calculating mean A1R, A2, A3, A4 A5. using jamovi compute new variable command (see Figure 15.9).\nFigure 15.8: Recode variable using jamovi Transform command\n\nFigure 15.9: Compute new scale score variable using jamovi Computed variable command\nAnother option create optimally-weighted factor score index. can use jamovi Rj editor R. 175 , two steps:Use Rj editor run EFA R specification one jamovi (.e. five factors oblimin rotation) compute optimally weighted factor scores (Figure 15.10). Save new dataset, factor scores, file (Figure 15.11).\nFigure 15.10: Rj editor commands creating optimally weighted factor scores five factor solution\n\nFigure 15.11: newly created data file ’bfifactscores.csv’created Rj editor containing five factor score variables. Note new factor score variables labelled corresponding order factors listed factor loadings table\nOpen new file jamovi check variable types set correctly. Label new factor score variables corresponding relevant factor names definitions (NB possible factors expected order, make sure check).Now can go ahead undertake analyses, using either factor-based scores (mean scale score approach) using optimally-weighted factor scores calculated via Rj editor. choice! example, one thing might like see whether gender differences personality scales. Agreeableness score calculated using factor-based score approach, although plot (Figure 15.12) showed males less agreeable females, significant difference (Mann-Whitney \\(U = 5760.5\\), \\(p = .073\\)).\nFigure 15.12: Comparing differences Agreeableness factor-based scores males females\n","code":""},{"path":"factor-analysis.html","id":"writing-up-an-efa","chapter":"15 Factor Analysis","heading":"15.1.4 Writing up an EFA","text":"Hopefully, far given sense EFA undertake EFA jamovi. , completed EFA, write ? formal standard way write EFA, examples tend vary discipline researcher. said, fairly standard pieces information include write-:theoretical underpinnings area studying, specifically constructs interested uncovering EFA.theoretical underpinnings area studying, specifically constructs interested uncovering EFA.description sample (e.g. demographic information, sample size, sampling method).description sample (e.g. demographic information, sample size, sampling method).description type data used (e.g., nominal, continuous) descriptive statistics.description type data used (e.g., nominal, continuous) descriptive statistics.Describe went testing assumptions EFA. Details regarding sphericity checks measures sampling adequacy reported.Describe went testing assumptions EFA. Details regarding sphericity checks measures sampling adequacy reported.Explain FA extraction method (e.g. maximum likelihood) used.Explain FA extraction method (e.g. maximum likelihood) used.Explain criteria process used deciding many factors extracted final solution, items selected. Clearly explain rationale key decisions EFA process.Explain criteria process used deciding many factors extracted final solution, items selected. Clearly explain rationale key decisions EFA process.Explain rotation methods attempted, reasons , results.Explain rotation methods attempted, reasons , results.Final factor loadings reported results, table. table also report uniqueness (communality) variable (final column). Factor loadings reported descriptive labels addition item numbers. Correlations factors also included, either bottom table, separate table.Final factor loadings reported results, table. table also report uniqueness (communality) variable (final column). Factor loadings reported descriptive labels addition item numbers. Correlations factors also included, either bottom table, separate table.Meaningful names extracted factors provided. may like use previously selected factor names, examining actual items factors may think different name appropriateMeaningful names extracted factors provided. may like use previously selected factor names, examining actual items factors may think different name appropriate","code":""},{"path":"factor-analysis.html","id":"principal-component-analysis","chapter":"15 Factor Analysis","heading":"15.2 Principal Component Analysis","text":"previous section saw EFA works identify underlying latent factors. , saw, one scenario smaller number latent factors can used statistical analysis using sort combined factor scores.way EFA used “data reduction” technique. Another type data reduction technique, sometimes seen part EFA family, principal component analysis (PCA) . However, PCA identify underlying latent factors. Instead creates linear composite score larger set measured variables.PCA simply produces mathematical transformation original data assumptions variables co-vary. aim PCA calculate linear combinations (components) original variables can used summarize observed data set without losing much information. However, identification underlying structure goal analysis, EFA preferred. , saw, EFA produces factor scores can used data reduction purposes just like principal component scores (Fabrigar et al. 1999).PCA popular Psychology number reasons, therefore ’s worth covering, although nowadays EFA just easy given power desktop computers can less susceptible bias PCA, especially small number factors variables. ’ll use bfi_sample.csv data . Much procedure similar EFA, although conceptual differences, practically steps , 176 large samples sufficient number factors variables, results PCA EFA fairly similar.","code":""},{"path":"factor-analysis.html","id":"performing-pca-in-jamovi","chapter":"15 Factor Analysis","heading":"15.2.1 Performing PCA in jamovi","text":"loaded bfi_sample.csv data, select Factor - Principal Component Analysis main jamovi button bar open PCA analysis window (Figure 15.13). select 25 personality questions transfer ‘Variables’ box. Check appropriate options, including ‘Assumption Checks’, also Rotation ‘Method’, ‘Number Factors extract, ’Additional Output’ options. See Figure 15.13 suggested options PCA, please note Rotation ‘Method’ ‘Number Factors’ extracted typically adjusted analysis find best result, described .\nFigure 15.13: jamovi PCA analysis window\nFirst, checking assumptions (Figure 15.14). can see (1) Bartlett’s test sphericity significant, assumption satisfied; (2) KMO measure sampling adequacy (MSA) \\(0.81\\) overall, suggesting good sampling adequacy. problems !\nFigure 15.14: jamovi PCA assumption checks personality item data\nnext thing check many components use (“extract” data). EFA, three different approaches available:One convention choose components Eigen values greater 1.177 give us two components data.One convention choose components Eigen values greater 1.177 give us two components data.Examination scree plot, Figure 15.15, lets identify “point inflection”. point slope scree curve clearly levels , “elbow”. , give us two components levelling clearly occurs second component.Examination scree plot, Figure 15.15, lets identify “point inflection”. point slope scree curve clearly levels , “elbow”. , give us two components levelling clearly occurs second component.\nFigure 15.15: Scree plot personality item data jamovi PCA, showing levelling point, ‘elbow’, component 5\nUsing parallel analysis technique, obtained Eigen values compared obtained random data. number components extracted number Eigen values greater found random data.third approach good one according Fabrigar et al. (1999), although practice researchers tend look three make judgement number components easily helpfully interpreted. can understood “meaningfulness criterion”, researchers typically examine, addition solution one approaches , solutions one two fewer components. adopt solution makes sense .time, also consider best way rotate final solution. , EFA, two main approaches rotation: orthogonal (e.g. “varimax”) rotation forces selected components uncorrelated; whereas oblique (e.g. “oblimin”) rotation allows selected components correlated. Dimensions interest psychologists behavioural scientists often dimensions expect orthogonal, oblique solutions arguably sensible. Practically, oblique rotation components found substantially correlated (.e. > 0.3) confirm intuition prefer oblique rotation. components , fact, correlated, oblique rotation produce better estimate true components better simple structure orthogonal rotation. , oblique rotation indicates components close zero correlations one another, researcher can go ahead conduct orthogonal rotation (give solution oblique rotation). Figure 15.16 see none correlations > 0.3 appropriate switch orthogonal (varimax) rotation.\nFigure 15.16: Component summary statistics correlations five component solution jamovi PCA\nFigure 15.16 also proportion overall variance data accounted two components. Components one two account just 12%$ variance . Taken together, five component solution accounts just half variance (56%) observed data. aware every PCA potentially number components observed variables, every additional component include add smaller amount explained variance. first components explain good amount variance original 25 variables, components clearly useful, simpler substitute 25 variables. can drop rest without losing much original variability. takes 18 components explain variance 25 variables, might well just use original 25.Figure 15.17 shows component loadings. ’s , 25 different personality items load onto selected components. hidden loadings less \\(0.4\\) (set options shown Figure 15.17) interested items substantive loading setting threshold higher \\(0.4\\) value also provided cleaner, clearer solution.\nFigure 15.17: Component loadings five component solution jamovi PCA\ncomponents \\(1, 2, 3\\) \\(4\\) pattern component loadings closely matches putative factors specified Figure @ref(fig:fig15-2. component \\(5\\) pretty close, four five observed variables putatively measure “openness” loading pretty well onto component. Variable 04 doesn’t quite seem fit though, component solution Figure 15.17 suggests loads onto component \\(4\\) (albeit relatively low loading) substantively onto component \\(5\\).can also see Figure 15.17 “uniqueness” variable. Uniqueness proportion variance ‘unique’ variable explained components. example, 52% variance ‘A1’ explained components five component solution. contrast, ‘N1’ relatively low variance accounted component solution (30%). Note greater ‘uniqueness’, lower relevance contribution variable component model.Hopefully, given good first idea undertake PCA jamovi, conceptually different practically fairly similar (given right data) EFA.can go create save component scores much way EFA. However, take option create optimally-weighted component score index commands syntax jamovi Rj editor little different. See Figure 15.18.\nFigure 15.18: Rj editor commands creating optimally weighted component scores five component solution\n","code":""},{"path":"factor-analysis.html","id":"confirmatory-factor-analysis","chapter":"15 Factor Analysis","heading":"15.3 Confirmatory Factor Analysis","text":", attempt identify underlying latent factors using EFA carefully selected questions personality item pool seemed pretty successful. next step quest develop useful measure personality check latent factors identified original EFA different sample. want see factors hold , can confirm existence different data. rigorous check, see. ’s called Confirmatory Factor Analysis (CFA) , unsuprisingly, seeking confirm pre-specificied latent factor structure.178In CFA, instead analysis see data goes together exploratory sense, instead impose structure, like Figure 15.19, data see well data fits pre-specified structure. sense, undertaking confirmatory analysis, see well pre-specified model confirmed observed data.straightforward confirmatory factor analysis (CFA) personality items therefore specify five latent factors shown Figure 15.19, measured five observed variables. variable measure underlying latent factor. example, A1 predicted underlying latent factor Agreeableness. A1 perfect measure Agreeableness factor, error term, e, associated . words, e represents variance A1 accounted Agreeableness factor. sometimes called measurement error.\nFigure 15.19: Initial pre-specification latent factor structure five factor personality scales, use CFA\nnext step consider whether latent factors allowed correlate model. mentioned earlier, psychological behavioural sciences constructs often related , also think personality factors may correlated . , model, allow latent factors co-vary, shown double-headed arrows Figure 15.19.time, consider whether good, systematic, reason error terms correlated . One reason might shared methodological feature particular sub-sets observed variables observed variables might correlated methodological rather substantive latent factor reasons. ’ll return possibility later section , now, clear reasons can see justify correlating error terms otherWithout correlated error terms, model testing see well fits observed data just specified Figure 15.19. parameters included model expected found data, CFA possible parameters (coefficients) set zero. , parameters zero (example may substantial loading A1 onto latent factor Extraversion observed data, model) may find poor fit model observed data.Right, let’s take look set CFA analysis jamovi.","code":""},{"path":"factor-analysis.html","id":"cfa-in-jamovi","chapter":"15 Factor Analysis","heading":"15.3.1 CFA in jamovi","text":"Open bfi_sample2.csv file, check 25 variables coded ordinal (continuous; won’t make difference analysis). perform CFA jamovi:Select Factor - Confirmatory Factor Analysis main jamovi button bar open CFA analysis window (Figure 15.20).Select Factor - Confirmatory Factor Analysis main jamovi button bar open CFA analysis window (Figure 15.20).Select 5 variables transfer ‘Factors’ box give label “Agreeableness”.Select 5 variables transfer ‘Factors’ box give label “Agreeableness”.Create new Factor ‘Factors’ box label “Conscientiousness”. Select 5 C variables transfer ‘Factors’ box “Conscientiousness” label.Create new Factor ‘Factors’ box label “Conscientiousness”. Select 5 C variables transfer ‘Factors’ box “Conscientiousness” label.Create another new Factor ‘Factors’ box label “Extraversion”. Select 5 E variables transfer ‘Factors’ box “Extraversion” label.Create another new Factor ‘Factors’ box label “Extraversion”. Select 5 E variables transfer ‘Factors’ box “Extraversion” label.Create another new Factor ‘Factors’ box label “Neuroticism”. Select 5 N variables transfer ‘Factors’ box “Neuroticism” label.Create another new Factor ‘Factors’ box label “Neuroticism”. Select 5 N variables transfer ‘Factors’ box “Neuroticism” label.Create another new Factor ‘Factors’ box label “Openness”. Select 5 O variables transfer ‘Factors’ box “Openness” label.Create another new Factor ‘Factors’ box label “Openness”. Select 5 O variables transfer ‘Factors’ box “Openness” label.Check appropriate options, defaults ok initial work , though might want check “Path diagram” option ‘Plots’ see jamovi produce (fairly) similar diagram Figure 15.19.Check appropriate options, defaults ok initial work , though might want check “Path diagram” option ‘Plots’ see jamovi produce (fairly) similar diagram Figure 15.19.\nFigure 15.20: jamovi CFA analysis window\nset analysis can turn attention jamovi results window see ’s . first thing look model fit (Figure 15.21) tells us good fit model observed data. NB model pre-specified covariances estimated, including factor correlations default. Everything else set zero.\nFigure 15.21: jamovi CFA Model Fit results CFA model\nseveral ways assessing model fit. first chi-square statistic , small, indicates model good fit data. However, chi-squared statistic used assessing model fit pretty sensitive sample size, meaning large sample good enough fit model data almost always produces large significant (p < .05) chi-square value., need ways assessing model fit. jamovi several provided default. Comparative Fit Index (CFI), Tucker Lewis Index (TLI) Root Mean Square Error Approximation (RMSEA) together 90% confidence interval RMSEA. useful rules thumb satisfactory fit indicated CFI > 0.9, TLI > 0.9, RMSEA 0.05 0.08. good fit CFI > 0.95, TLI > 0.95, RMSEA upper CI RMSEA < 0.05., looking Figure 15.21 can see chi-square value large highly significant. sample size large, possibly indicates poor fit. CFI \\(0.762\\) TLI 0.731, indicating poor fit model data. RMSEA \\(0.085\\) \\(90\\%\\) confidence interval \\(0.077\\) \\(0.092\\), indicate good fit.Pretty disappointing, huh? perhaps surprising given earlier EFA, ran similar data set (see Exploratory Factor Analysis section), around half variance data accounted five factor model.Let’s go look factor loadings factor covariance estimates, shown Figures 15.22 15.23. Z-statistic p-value parameters indicates make reasonable contribution model (.e. zero) doesn’t appear reason remove specified variable-factor paths, factor-factor correlations model. Often standardized estimates easier interpret, can specified ‘Estimates’ option. tables can usefully incorporated written report scientific article.\nFigure 15.22: jamovi CFA Factor Loadings table CFA model\n\nFigure 15.23: jamovi CFA Factor Covariances table CFA model\nimprove model? One option go back stages think items / measures using might improved changed. Another option make post hoc tweaks model improve fit. One way use “modification indices” (Figure 15.24), specified ‘Additional output’ option jamovi.\nFigure 15.24: jamovi CFA Factor Loadings Modification Indices\nlooking highest modification index (MI) value. judge whether makes sense add additional term model, using post hoc rationalisation. example, can see Figure 15.24 largest MI factor loadings already model value 28.786 loading N4 (“Often feel blue”) onto latent factor Extraversion. indicates add path model chi-square value reduce around amount.model adding path arguably doesn’t really make theoretical methodological sense, ’s good idea (unless can come persuasive argument “Often feel blue” measures Neuroticism Extraversion). can’t think good reason. , sake argument, let’s pretend make sense add path model. Go back CFA analysis window (see Figure 15.20) add N4 Extraversion factor. results CFA now change (shown); chi-square come around 709 (drop around 30, roughly similar size MI) fit indices also improved, though bit. ’s enough: ’s still good fitting model.find adding new parameters model using MI values always re-check MI tables new addition, MIs refreshed time.also Table Residual Covariance Modification Indices produced jamovi (Figure 15.25). words, table showing correlated errors, added model, improve model fit . ’s good idea look across MI tables time, spot largest MI, think whether addition suggested parameter can reasonably justified , can, add model. can start looking biggest MI re-calculated results.\nFigure 15.25: Residual Covariance Modification Indices produced jamovi\ncan keep going way long like, adding parameters model based largest MI, eventually achieve satisfactory fit. also strong possibility created monster! model ugly deformed doesn’t theoretical sense purity. words, careful!far, checked factor structure obtained EFA using second sample CFA. Unfortunately, didn’t find factor structure EFA confirmed CFA, ’s back drawing board far development personality scale goes.Although tweaked CFA using modification indexes, really good reasons (think ) suggested additional factor loadings residual covariances included. However, sometimes good reason residuals allowed co-vary (correlate), good example shown next section Multi-Trait Multi-Method CFA. , let’s cover report results CFA.","code":""},{"path":"factor-analysis.html","id":"reporting-a-cfa","chapter":"15 Factor Analysis","heading":"15.3.2 Reporting a CFA","text":"formal standard way write CFA, examples tend vary discipline researcher. said, fairly standard pieces information include write-:theoretical empirical justification hypothesized model.theoretical empirical justification hypothesized model.complete description model specified (e.g. indicator variables latent factor, covariances latent variables, correlations error terms). path diagram, like one Figure 15.21 good include.complete description model specified (e.g. indicator variables latent factor, covariances latent variables, correlations error terms). path diagram, like one Figure 15.21 good include.description sample (e.g. demographic information, sample size, sampling method).description sample (e.g. demographic information, sample size, sampling method).description type data used (e.g., nominal, continuous) descriptive statistics.description type data used (e.g., nominal, continuous) descriptive statistics.Tests assumptions estimation method used.Tests assumptions estimation method used.description missing data missing data handled.description missing data missing data handled.software version used fit model.software version used fit model.Measures, criteria used, judge model fit.Measures, criteria used, judge model fit.alterations made original model based model fit modification indices.alterations made original model based model fit modification indices.parameter estimates (.e., loadings, error variances, latent (co)variances) standard errors, probably table.parameter estimates (.e., loadings, error variances, latent (co)variances) standard errors, probably table.","code":""},{"path":"factor-analysis.html","id":"multi-trait-multi-method-cfa","chapter":"15 Factor Analysis","heading":"15.4 Multi-Trait Multi-Method CFA","text":"section ’re going consider different measurement techniques questions can important source data variability, known method variance. , ’ll use another psychological data set, one contains data “attributional style”.Attributional Style Questionnaire (ASQ) used (Hewitt, Foxcroft, MacDonald 2004) collect psychological wellbeing data young people United Kingdom New Zealand. measured attributional style negative events, people habitually explain cause bad things happen (Peterson Seligman 1984). attributional style questionnaire (ASQ) measures three aspects attributional style:Internality extent person believes cause bad event due /actions.Internality extent person believes cause bad event due /actions.Stability refers extent person habitually believes cause bad event stable across time.Stability refers extent person habitually believes cause bad event stable across time.Globality refers extent person habitually believes cause bad event one area affect areas lives.Globality refers extent person habitually believes cause bad event one area affect areas lives.six hypothetical scenarios scenario respondents answer question aimed () internality, (b) stability (c) globality. \\(6 \\times 3 = 18\\) items overall. See Figure 15.26 details.\nFigure 15.26: Attributional Style Questionnaire (ASQ) negative events\nResearchers interested checking data see whether underlying latent factors measured reasonably well 18 observed variables ASQ.First, try EFA 18 variables (shown), matter extract rotate, can’t find good factor solution. attempt identify underlying latent factors Attributional Style Questionnaire (ASQ) proved fruitless. get results like either theory wrong (underlying latent factor structure attributional style, possible), sample relevant (unlikely given size characteristics sample young adults United Kingdom New Zealand), analysis right tool job. ’re going look third possibility.Remember three dimensions measured ASQ: Internality, Stability Globality, measured six questions shown Figure 15.27., instead analysis see data goes together exploratory sense, instead impose structure, like Figure 15.27, data see well data fits pre-specified structure. sense, undertaking confirmatory analysis, see well pre-specified model confirmed observed data.straightforward confirmatory factor analysis (CFA) ASQ therefore specify three latent factors shown columns Figure 15.27, measured six observed variables.\nFigure 15.27: Six questions ASQ Internality, Stability Globality dimensions\ndepict diagram Figure 15.28, shows variable measure underlying latent factor. example INT1 predicted underlying latent factor Internality. INT1 perfect measure Internality factor, error term, e1, associated . words, e1 represents variance INT1 accounted Internality factor. sometimes called “measurement error”.\nFigure 15.28: Initial pre-specification latent factor structure ASQ\nnext step consider whether latent factors allowed correlate model. mentioned earlier, psychological behavioural sciences constructs often related , also think Internality, Stability, Globality might correlated , model allow latent factors co-vary, shown Figure 15.29.\nFigure 15.29: Final pre-specification latent factor structure ASQ, including latent factor correlations, shared method error term correlations observed variable INT1, STAB1 GLOB1, CFA MTMM model. clarity, pre-specified error term correlations shown\ntime, consider whether good, systematic, reason error terms correlated . Thinking back ASQ questions, three different sub-questions (, b c) main question (1-6). Q1 unsuccessful job hunting plausible question distinctive artefactual methodological aspects questions (2-5), something job hunting perhaps. Similarly, Q2 helping friend problem, may distinctive artefactual methodological aspects helping friend present questions (1, 3-5)., well multiple factors, also multiple methodological features ASQ, Questions 1-6 slightly different “method”, “method” shared across sub-questions , b c. order incorporate different methodological features model can specify certain error terms correlated . example, errors associated INT1, STAB1 GLOB1 correlated reflect distinct shared methodological variance Q1a, Q1b Q1c. Looking Figure 15.27, means well latent factors represented columns, correlated measurement errors variables row Table.Whilst basic CFA model like one shown Figure 15.28 tested observed data, fact come sophisticated model, shown diagram Figure 15.29. sophisticated CFA model known Multi-Trait Multi-Method (MTMM) model, one test jamovi.","code":""},{"path":"factor-analysis.html","id":"mtmm-cfa-in-jamovi","chapter":"15 Factor Analysis","heading":"15.4.1 MTMM CFA in jamovi","text":"Open ASQ.csv file check 18 variables (six “Internality”, six “Stability” six “Globality” variables) specified continuous variables.perform MTMM CFA jamovi:Select Factor - Confirmatory Factor Analysis main jamovi button bar open CFA analysis window (Figure 15.30).Select Factor - Confirmatory Factor Analysis main jamovi button bar open CFA analysis window (Figure 15.30).Select 6 INT variables transfer ‘Factors’ box give label “Internality”.Select 6 INT variables transfer ‘Factors’ box give label “Internality”.Create new Factor ‘Factors’ box label “Stability”. Select 6 STAB variables transfer ‘Factors’ box “Stability” label.Create new Factor ‘Factors’ box label “Stability”. Select 6 STAB variables transfer ‘Factors’ box “Stability” label.Create another new Factor ‘Factors’ box label “Globality”. Select 6 GLOB variables transfer ‘Factors’ box “Globality” label.Create another new Factor ‘Factors’ box label “Globality”. Select 6 GLOB variables transfer ‘Factors’ box “Globality” label.Open Residual Covariances options, pre-specified correlations move associated variables across ‘Residual Covariances’ box right. example, highlight INT1 STAB1 click arrow move across. Now INT1 GLOB1, STAB1 GLOB1, INT2 STAB2, INT2 GLOB2, STAB2 GLOB2, INT3 STAB3, .Open Residual Covariances options, pre-specified correlations move associated variables across ‘Residual Covariances’ box right. example, highlight INT1 STAB1 click arrow move across. Now INT1 GLOB1, STAB1 GLOB1, INT2 STAB2, INT2 GLOB2, STAB2 GLOB2, INT3 STAB3, .Check appropriate options, defaults ok initial work , though might want check “Path diagram” option ‘Plots’ see jamovi produce (fairly) similar diagram Figure 15.29, including error term correlations added .Check appropriate options, defaults ok initial work , though might want check “Path diagram” option ‘Plots’ see jamovi produce (fairly) similar diagram Figure 15.29, including error term correlations added .\nFigure 15.30: jamovi CFA analysis window\nset analysis can turn attention jamovi results window see ’s . first thing look “Model fit” tells us good fit model observed data (Figure 15.31). NB model pre-specified covariances estimated, everything else set zero, model fit testing whether pre-specified “free” parameters zero, conversely whether relationships data – ones specified model – can held zero.\nFigure 15.31: jamovi CFA Model Fit results CFA MTMM model\nseveral ways assessing model fit. first chi-square statistic, small indicates model good fit data. However, chi-square statistic used assessing model fit sensitive sample size, meaning large sample (300-400 cases) good enough fit model data almost always produces large significant chi-square value., need ways assessing model fit. jamovi several provided default. Comparative Fit Index (CFI), Tucker Fit Index (TFI) Root Mean Square Error Approximation (RMSEA) together 90% confidence interval RMSEA. mentioned previously, useful rules thumb satisfactory fit indicated CFI > 0.9, TFI > 0.9, RMSEA 0.05 0.08. good fit CFI > 0.95, TFI > 0.95, RMSEA upper CI RMSEA < 0.05., looking Figure 15.31 can see chi-square value highly significant, surprise given large sample size (N = 2748). CFI 0.98 TLI also 0.98, indicating good fit. RMSEA 0.02 90% confidence interval 0.02 0.02 – pretty tight!Overall, think can satisfied pre-specified model good fit observed data, lending support MTMM model ASQ.can now go look factor loadings factor covariance estimates, Figure 15.32. Often standardized estimates easier interpret, can specified ‘Estimates’ option. tables can usefully incorporated written report scientific article.\nFigure 15.32: jamovi CFA Factor Loadings Covariances tables CFA MTMM model\ncan see Figure 15.32 pre-specified factor loadings factor covariances significantly different zero. words, seem making useful contribution model.’ve pretty lucky analysis, getting good fit first attempt. ’s pretty unusual, often CFA additional post hoc tweaks made model improve fit. One way use “modification indices”, specified ‘Additional output’ option jamovi.looking highest modification index (MI) value. judge whether makes sense add additional term model, using post hoc rationalisation. example, can see Figure 15.33 largest MI factor loadings already model value 24.52 loading INT6 onto latent factor Globality. indicates add path model chi-square value reduce 25. model adding path doesn’t really make theoretical methodological sense, therefore won’t including path revised model.\nFigure 15.33: jamovi CFA Factor Loadings Modification Indices\nLikewise, look MIs residual terms (Figure 15.34) highest MI 13.48 allowing errors INT1 INT3 co-vary – .e. included – model. , isn’t particularly high MI, reasonable justification including parameter model, already good fit; answer modification.\nFigure 15.34: jamovi CFA Residual Covariances Modification Indices\nfind adding new parameters model using MI always re-check MI tables new addition (exclusion – software MI can also suggest parameters removed model improve model fit), MIs refreshed time.","code":""},{"path":"factor-analysis.html","id":"internal-consistency-reliability-analysis","chapter":"15 Factor Analysis","heading":"15.5 Internal consistency reliability analysis","text":"process initial scale development using EFA CFA, reached stage scale holds pretty well using CFA different samples. One thing might also interested stage see well factors measured using scale combines observed variables.psychometrics use reliability analysis provide information consistently scale measures psychological construct (See earlier section Assessing reliability measurement. Internal consistency concerned , refers consistency across individual items make measurement scale. , \\(V1, V2, V3, V4\\) \\(V5\\) observed item variables, can calculate statistic tells us internally consistent items measuring underlying construct.popular statistic used check internal consistency scale Cronbach’s alpha (Chronbach 1951). Cronbach’s alpha measure equivalence (whether different sets scale items give measurement outcomes). Equivalence tested dividing scale items two groups (“split-half”) seeing whether analysis two parts gives comparable results. course, many ways set items split, possible splits made possible produce statistic reflects overall pattern split-half coefficients. Cronbach’s alpha (\\(\\alpha\\)) statistic: function split-half coefficients scale. set items measure construct (e.g. Extraversion scale) \\(\\alpha\\) \\(0.80\\), proportion error variance scale \\(0.20\\). words, scale \\(\\alpha\\) \\(0.80\\) includes approximately 20% error., (’s BIG “”), Cronbach’s alpha measure unidimensionality (.e. indicator scale measuring single factor construct rather multiple related constructs). Scales multidimensional cause alpha -estimated assessed separately dimension, high values alpha necessarily indicators unidimensionality. , \\(\\alpha\\) 0.80 mean 80% single underlying construct accounted . 80% comes one underlying construct. ’s EFA CFA useful first., another feature \\(\\alpha\\) tends sample specific: characteristic scale, rather characteristic sample scale used. biased, unrepresentative, small sample produce different \\(\\alpha\\) coefficient large, representative sample. \\(\\alpha\\) can even vary large sample large sample. Nevertheless, despite limitations, Cronbach’s \\(\\alpha\\) popular Psychology estimating internal consistency reliability. ’s pretty easy calculate, understand interpret, therefore can useful initial check scale performance administer scale different sample, different setting population, example.alternative McDonald’s omega (\\(\\omega\\)), jamovi also provides statistic. Whereas \\(\\alpha\\) makes following assumptions: () residual correlations, (b) items identical loadings, (c) scale unidimensional, \\(\\omega\\) therefore robust reliability statistic. assumptions violated \\(\\alpha\\) \\(\\omega\\) similar, \\(\\omega\\) preferred.Sometimes threshold \\(\\alpha\\) \\(\\omega\\) provided, suggesting “good enough” value. might something like \\(\\alpha\\)s \\(0.70\\) \\(0.80\\) representing “acceptable” “good” reliability, respectively. However, depend exactly scale supposed measuring, thresholds like used cautiously. better simply state \\(\\alpha\\) \\(\\omega\\) \\(0.70\\) associated 30% error variance scale, \\(\\alpha\\) \\(\\omega\\) \\(0.80\\) associated 20%.Can \\(\\alpha\\) high? Probably: getting \\(\\alpha\\) coefficient \\(0.95\\) indicates high inter-correlations items might much overly redundant specificity measurement, risk construct measured perhaps overly narrow.","code":""},{"path":"factor-analysis.html","id":"reliability-analysis-in-jamovi","chapter":"15 Factor Analysis","heading":"15.5.1 Reliability analysis in jamovi","text":"third sample personality data use undertake reliability analysis: bfi_sample3.csv file. , check 25 personality item variables coded continuous. perform reliability analysis jamovi:Select Factor - Reliability Analysis main jamovi button bar open reliability analysis window (Figure 15.35).Select Factor - Reliability Analysis main jamovi button bar open reliability analysis window (Figure 15.35).Select 5 variables transfer ‘Items’ box.Select 5 variables transfer ‘Items’ box.“Reverse Scaled Items” option, select variable A1 “Normal Scaled Items” box move across “Reverse Scaled Items” box.“Reverse Scaled Items” option, select variable A1 “Normal Scaled Items” box move across “Reverse Scaled Items” box.Check appropriate options, Figure 15.35.Check appropriate options, Figure 15.35.\nFigure 15.35: jamovi Reliability Analysis window\ndone, look across jamovi results window. see something like Figure 15.36. tells us Cronbach’s \\(\\alpha\\) coefficient Agreeableness scale 0.72. means just 30% Agreeableness scale score error variance. McDonald’s \\(\\omega\\) also given, 0.74, much different \\(\\alpha\\).\nFigure 15.36: jamovi Reliability Analysis results Agreeableness factor\ncan also check \\(\\alpha\\) \\(\\omega\\) can improved specific item dropped scale. example, \\(\\alpha\\) increase 0.74 \\(\\omega\\) 0.75 dropped item A1. isn’t big increase, probably worth .process calculating checking scale statistics (\\(\\alpha\\) \\(\\omega\\)) scales, similar reliability estimates apart Openness. Openness, amount error variance Scale score around 40%, high indicates Openness substantially less consistent reliable measure personality attribute personality scales.","code":""},{"path":"factor-analysis.html","id":"summary-13","chapter":"15 Factor Analysis","heading":"15.6 Summary","text":"chapter factor analysis related techniques introduced demonstrated statistical analyses assess pattern relationships data set. Specifically, covered:Exploratory Factor Analysis (EFA). EFA statistical technique identifying underlying latent factors data set. observed variable conceptualised representing latent factor extent, indicated factor loading. Researchers also use EFA way data reduction, .e. identifying observed variables can combined new factor variables subsequent analysis.Exploratory Factor Analysis (EFA). EFA statistical technique identifying underlying latent factors data set. observed variable conceptualised representing latent factor extent, indicated factor loading. Researchers also use EFA way data reduction, .e. identifying observed variables can combined new factor variables subsequent analysis.Principal Component Analysis (PCA) data reduction technique , strictly speaking, identify underlying latent factors. Instead, PCA simply produces linear combination observed variables.Principal Component Analysis (PCA) data reduction technique , strictly speaking, identify underlying latent factors. Instead, PCA simply produces linear combination observed variables.Confirmatory Factor Analysis (CFA). Unlike EFA, CFA start idea - model - variables data related . test model observed data assess good fit model data.Confirmatory Factor Analysis (CFA). Unlike EFA, CFA start idea - model - variables data related . test model observed data assess good fit model data.Multi-Trait Multi-Method CFA (MTMM CFA), latent factor method variance included model approach useful different methodological approaches used therefore method variance important consideration.Multi-Trait Multi-Method CFA (MTMM CFA), latent factor method variance included model approach useful different methodological approaches used therefore method variance important consideration.Internal consistency reliability analysis. form reliability analysis tests consistently scale measures measurement (psychological) construct.Internal consistency reliability analysis. form reliability analysis tests consistently scale measures measurement (psychological) construct.","code":""},{"path":"bayesian-statistics.html","id":"bayesian-statistics","chapter":"16 Bayesian statistics","heading":"16 Bayesian statistics","text":"“reasonings concerning matter fact, imaginable degrees assurance, highest certainty lowest species moral evidence. wise man, therefore, proportions belief evidence.”\n– David Hume 179The ideas ’ve presented book describe inferential statistics frequentist perspective. ’m alone . fact, almost every textbook given undergraduate psychology students presents opinions frequentist statistician theory inferential statistics, one true way things. taught way practical reasons. frequentist view statistics dominated academic field statistics 20th century, dominance even extreme among applied scientists. current practice among psychologists use frequentist methods. frequentist methods ubiquitous scientific papers, every student statistics needs understand methods, otherwise unable make sense papers saying! Unfortunately, opinion least, current practice psychology often misguided reliance frequentist methods partly blame. chapter explain think provide introduction Bayesian statistics, approach think generally superior orthodox approach.chapter comes two parts. first three sections talk Bayesian statistics , covering basic mathematical rules works well explanation think Bayesian approach useful. Afterwards, provide brief overview can Bayesian t-tests.","code":""},{"path":"bayesian-statistics.html","id":"probabilistic-reasoning-by-rational-agents","chapter":"16 Bayesian statistics","heading":"16.1 Probabilistic reasoning by rational agents","text":"Bayesian perspective statistical inference belief revision. start set candidate hypotheses h world. don’t know hypotheses true, beliefs hypotheses plausible . observe data, d, revise beliefs. data consistent hypothesis, belief hypothesis strengthened. data inconsistent hypothesis, belief hypothesis weakened. ’s ! end section ’ll give precise description Bayesian reasoning works, first want work simple example order introduce key ideas. Consider following reasoning problem.’m carrying umbrella. think rain?problem presented single piece data (d = ’m carrying umbrella), ’m asking tell belief hypothesis whether ’s raining. two alternatives, h: either rain today . solve problem?","code":""},{"path":"bayesian-statistics.html","id":"priors-what-you-believed-before","chapter":"16 Bayesian statistics","heading":"16.1.1 Priors: what you believed before","text":"first thing need ignore told umbrella, write pre-existing beliefs rain. important. want honest beliefs revised light new evidence (data) must say something believed data appeared! , might believe whether rain today? probably know live Australia much Australia hot dry. city Adelaide live Mediterranean climate, similar southern California, southern Europe northern Africa. ’m writing January can assume ’s middle summer. fact, might decided take quick look Wikipedia180 discovered Adelaide gets average 4.4 days rain across 31 days January. Without knowing anything else, might conclude probability January rain Adelaide 15%, probability dry day 85% (see Table 16.1). really believe Adelaide rainfall (now ’ve told ’m betting really believe) written prior distribution, written \\(P(h)\\).Table 16.1:  likely rain Adelaide: pre-existing beliefs based knowledge average January rainfall","code":""},{"path":"bayesian-statistics.html","id":"likelihoods-theories-about-the-data","chapter":"16 Bayesian statistics","heading":"16.1.2 Likelihoods: theories about the data","text":"solve reasoning problem need theory behaviour. Dan carry umbrella? might guess ’m complete idiot,181 try carry umbrellas rainy days. hand, also know young kids, wouldn’t surprised know ’m pretty forgetful sort thing. Let’s suppose rainy days remember umbrella 30% time (really awful ). let’s say dry days ’m 5% likely carrying umbrella. might write Table 16.2.Table 16.2:  likely carrying umbrella rainy dry daysIt’s important remember cell table describes beliefs data d observed, given truth particular hypothesis \\(h\\). “conditional probability” written \\(P(d|h)\\), can read “probability \\(d\\) given \\(h\\)”. Bayesian statistics, referred likelihood data \\(d\\) given hypothesis \\(h\\).182","code":""},{"path":"bayesian-statistics.html","id":"the-joint-probability-of-data-and-hypothesis","chapter":"16 Bayesian statistics","heading":"16.1.3 The joint probability of data and hypothesis","text":"point elements place. written priors likelihood, information need Bayesian reasoning. question now becomes use information? turns , ’s simple equation can use , ’s important understand use ’m going try build basic ideas.Let’s start one rules probability theory. listed way back Table Table 7.1, didn’t make big deal time probably ignored . rule question one talks probability two things true. example, might want calculate probability today rainy (.e., hypothesis h true) ’m carrying umbrella (.e., data \\(d\\) observed). joint probability hypothesis data written \\(P(d,h)\\), can calculate multiplying prior \\(P(h)\\) likelihood \\(P(d|h)\\). Mathematically, say \\[P(d,h)=P(d|h)P(h)\\], probability today rainy day remember carry umbrella? discussed earlier, prior tells us probability rainy day 15%, likelihood tells us probability remembering umbrella rainy day \\(30\\\\%\\). probability things true calculated multiplying twoIn words, told anything actually happened, think 4.5% probability today rainy day remember umbrella. However, course four possible things happen, right? let’s repeat exercise four. , end Table 16.3.Table 16.3:  Four possibilities combining rain () umbrella carrying ()table captures information four possibilities likely. really get full picture, though, helps add row totals column totals. gives us Table 16.4.Table 16.4:  Four possibilities combining rain () umbrella carrying (), row column totalsThis useful table, ’s worth taking moment think numbers telling us. First, notice row sums aren’t telling us anything new . example, first row tells us ignore umbrella business, chance today rainy day 15%. ’s surprising, course, ’s prior.183 important thing isn’t number . Rather, important thing gives us confidence calculations sensible! Now take look column sums notice tell us something haven’t explicitly stated yet. way row sums tell us probability rain, column sums tell us probability carrying umbrella. Specifically, first column tells us average (.e., ignoring whether ’s rainy day ) probability carrying umbrella 8.75%. Finally, notice sum across four logically-possible events, everything adds 1. words, written proper probability distribution defined possible combinations data hypothesis.Now, table useful, want make sure understand elements correspond written (Table 16.5):Table 16.5:  Four possibilities combining rain () umbrella carrying (), expressed conditional probabilitiesFinally, let’s use “proper” statistical notation. rainy day problem, data corresponds observation umbrella. ’ll let \\(d_1\\) refer possibility observe carrying umbrella, \\(d_2\\) refers observing carrying one. Similarly, \\(h_1\\) hypothesis today rainy, \\(h_2\\) hypothesis . Using notation, table looks like Table 16.6.Table 16.6:  Four possibilities combining rain () umbrella carrying (), expressed hypothjetical terms conditional probabilities","code":""},{"path":"bayesian-statistics.html","id":"updating-beliefs-using-bayes-rule","chapter":"16 Bayesian statistics","heading":"16.1.4 Updating beliefs using Bayes’ rule","text":"table laid last section powerful tool solving rainy day problem, considers four logical possibilities states exactly confident given data. ’s now time consider happens beliefs actually given data. rainy day problem, told really carrying umbrella. something surprising event. According table, probability carrying umbrella 8.75%. makes sense, right? woman carrying umbrella summer day hot dry city pretty unusual, really weren’t expecting . Nevertheless, data tells true. matter unlikely thought , must now adjust beliefs accommodate fact now know umbrella.184 reflect new knowledge, revised table must following numbers. (see Table 16.7).Table 16.7:  Revising beliefs given new data umbrella carryingIn words, facts eliminated possibility “umbrella”, put zeros cell table implies ’m carrying umbrella. Also, know fact carrying umbrella, column sum left must 1 correctly describe fact \\(P(umbrella) = 1\\).two numbers put empty cells? , let’s worry maths, instead think intuitions. wrote table first time, turned two cells almost identical numbers, right? worked joint probability “rain umbrella” 4.5%, joint probability “dry umbrella” 4.25%. words, told fact carrying umbrella, ’d said two events almost identical probability, yes? notice possibilities consistent fact actually carrying umbrella. perspective two possibilities, little changed. hope ’d agree ’s still true two possibilities equally plausible. expect see final table numbers preserve fact “rain umbrella” slightly plausible “dry umbrella”, still ensuring numbers table add . Something like Table 16.8, perhaps?Table 16.8:  Revising probabilities given new data umbrella carryingWhat table telling , told ’m carrying umbrella, believe ’s 51.4%) chance today rainy day, 48.6% chance won’t. ’s answer problem! posterior probability rain \\(P(h\\|d)\\) given carrying umbrella 51.4%calculate numbers? can probably guess. work \\(0.514\\) probability “rain”, take \\(0.045\\) probability “rain umbrella” divide \\(0.0875\\) chance “umbrella”. produces table satisfies need everything sum 1, need interfere relative plausibility two events actually consistent data. say thing using fancy statistical jargon, ’ve done divide joint probability hypothesis data \\(P(d, h)\\) marginal probability data \\(P(d)\\), gives us posterior probability hypothesis given data observed. write equation: 185\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]However, remember said start last section, namely joint probability \\(P(d, h)\\) calculated multiplying prior Pphq likelihood \\(P(d|h)\\). real life, things actually know write priors likelihood, let’s substitute back equation. gives us following formula posterior probability:\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]formula, folks, known Bayes’ rule. describes learner starts prior beliefs plausibility different hypotheses, tells beliefs revised face data. Bayesian paradigm, statistical inference flows one simple rule.","code":""},{"path":"bayesian-statistics.html","id":"bayesian-hypothesis-tests","chapter":"16 Bayesian statistics","heading":"16.2 Bayesian hypothesis tests","text":"earlier chapter described orthodox approach Hypothesis testing. took entire chapter describe, null hypothesis testing elaborate contraption people find hard make sense . contrast, Bayesian approach hypothesis testing incredibly simple. Let’s pick setting closely analogous orthodox scenario. two hypotheses want compare, null hypothesis \\(h_0\\) alternative hypothesis \\(h_1\\). Prior running experiment beliefs \\(P(h)\\) hypotheses true. run experiment obtain data d. Unlike frequentist statistics, Bayesian statistics allow us talk probability null hypothesis true. Better yet, allows us calculate posterior probability null hypothesis, using Bayes’ rule:\\[P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}\\]formula tells us exactly much belief null hypothesis observed data d. Similarly, can work much belief place alternative hypothesis using essentially equation. change subscript\\[P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}\\]’s simple feel like idiot even bothering write equations , since ’m copying Bayes rule previous section.186","code":""},{"path":"bayesian-statistics.html","id":"the-bayes-factor","chapter":"16 Bayesian statistics","heading":"16.2.1 The Bayes factor","text":"practice, Bayesian data analysts tend talk terms raw posterior probabilities \\(P(h_0|d)\\) \\(P(h_1|d)\\). Instead, tend talk terms posterior odds ratio. Think like betting. Suppose, instance, posterior probability null hypothesis 25%, posterior probability alternative 75%. alternative hypothesis three times probable null, say odds 3:1 favour alternative. Mathematically, calculate posterior odds divide one posterior probability \\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3\\], write thing terms equations \\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}\\]Actually, equation worth expanding . three different terms know. left hand side, posterior odds, tells believe relative plausibilty null hypothesis alternative hypothesis seeing data. right hand side, prior odds, indicates thought seeing data. middle, Bayes factor, describes amount evidence provided data. (Table 16.9).Table 16.9:  Posterior odds given Bsyes factor prior oddsThe Bayes factor (sometimes abbreviated BF) special place Bayesian hypothesis testing, serves similar role p-value orthodox hypothesis testing. Bayes factor quantifies strength evidence provided data, Bayes factor people tend report running Bayesian hypothesis test. reason reporting Bayes factors rather posterior odds different researchers different priors. people might strong bias believe null hypothesis true, others might strong bias believe false. , polite thing applied researcher report Bayes factor. way, anyone reading paper can multiply Bayes factor personal prior odds, can work posterior odds . case, convention like pretend give equal consideration null hypothesis alternative, case prior odds equals 1, posterior odds becomes Bayes factor","code":""},{"path":"bayesian-statistics.html","id":"interpreting-bayes-factors","chapter":"16 Bayesian statistics","heading":"16.2.2 Interpreting Bayes factors","text":"One really nice things Bayes factor numbers inherently meaningful. run experiment compute Bayes factor 4, means evidence provided data corresponds betting odds 4:1 favour alternative. However, attempts quantify standards evidence considered meaningful scientific context. two widely used Jeffreys (1961) Kass Raftery (1995). two, tend prefer Kass Raftery (1995) table ’s bit conservative. (Table 16.10).Table 16.10:  Bayes factors strength evidenceAnd perfectly honest, think even Kass Raftery (1995) standards bit charitable. , ’d called “positive evidence” category “weak evidence”. , anything range 3:1 20:1 “weak” “modest” evidence best. hard fast rules . counts strong weak evidence depends entirely conservative upon standards community insists upon willing label finding “true”.case, note numbers listed make sense Bayes factor greater 1 (.e., evidence favours alternative hypothesis). However, one big practical advantage Bayesian approach relative orthodox approach also allows quantify evidence null. happens, Bayes factor less 1. can choose report Bayes factor less 1, honest find confusing. example, suppose likelihood data null hypothesis \\(P(d|h_0)\\) equal 0.2, corresponding likelihood \\(P(d|h_1)\\) alternative hypothesis 0.1. Using equations given , Bayes factor \\[BF=\\frac{P(d|h_1)}{P(d|h_0)}=\\frac{0.1}{0.2}=0.5\\]Read literally, result tells evidence favour alternative 0.5 1. find hard understand. , makes lot sense turn equation “upside ”, report amount op evidence favour null. words, calculate \\[BF^{'}=\\frac{P(d|h_0)}{P(d|h_1)}=\\frac{0.2}{0.1}=2\\]report Bayes factor 2:1 favour null. Much easier understand, can interpret using table .","code":""},{"path":"bayesian-statistics.html","id":"why-be-a-bayesian","chapter":"16 Bayesian statistics","heading":"16.3 Why be a Bayesian?","text":"point ’ve focused exclusively logic underpinning Bayesian statistics. ’ve talked idea “probability degree belief”, implies rational agent reason world. question answer : want statistics? want orthodox statistician, relying sampling distributions p-values guide decisions? want Bayesian, relying things like prior beliefs, Bayes factors rules rational belief revision? perfectly honest, can’t answer question . Ultimately depends think right. ’s call call alone. said, can talk little prefer Bayesian approach.","code":""},{"path":"bayesian-statistics.html","id":"statistics-that-mean-what-you-think-they-mean","chapter":"16 Bayesian statistics","heading":"16.3.1 Statistics that mean what you think they mean","text":"keep using word. think means think means\n– Inigo Montoya, Princess Bride 187To , one biggest advantages Bayesian approach answers right questions. Within Bayesian framework, perfectly sensible allowable refer “probability hypothesis true”. can even try calculate probability. Ultimately, isn’t want statistical tests tell ? actual human , seem whole point statistics, .e., determine true isn’t. time aren’t exactly sure truth , use language probability theory say things like “80% chance Theory true, 20% chance Theory B true instead”.seems obvious human, yet explicitly forbidden within orthodox framework. frequentist, statements nonsense “theory true” repeatable event. theory true , probabilistic statements allowed, matter much might want make . ’s reason , back Section 9.5, repeatedly warned interpret p-value probability null hypothesis true. ’s reason almost every textbook statstics forced repeat warning. ’s people desperately want correct interpretation. Frequentist dogma notwithstanding, lifetime experience teaching undergraduates data analysis daily basis suggests actual humans think “probability hypothesis true” meaningful, ’s thing care . ’s appealing idea even trained statisticians fall prey mistake trying interpret p-value way. example, quote official Newspoll report 2013, explaining interpret (frequentist) data analysis:188Throughout report, relevant, statistically significant changes noted. significance tests based 95 percent level confidence. means change noted statistically significant, 95 percent probability real change occurred, simply due chance variation. (emphasis added)Nope! ’s p < .05 means. ’s 95% confidence means frequentist statistician. bolded section just plain wrong. Orthodox methods tell “95% chance real change occurred”, kind event frequentist probabilities may assigned. ideological frequentist, sentence meaningless. Even ’re pragmatic frequentist, ’s still wrong definition p-value. simply allowed correct thing say want rely orthodox statistical tools.hand, let’s suppose Bayesian. Although bolded passage wrong definition p-value, ’s pretty much exactly Bayesian means say posterior probability alternative hypothesis greater 95%. ’s thing. Bayesian posterior actually thing want report, even trying use orthodox methods? want make Bayesian claims, Bayesian use Bayesian tools.Speaking , found liberating thing switching Bayesian view. ’ve made jump, longer wrap head around counter-intuitive definitions p-values. don’t bother remembering can’t say ’re 95% confident true mean lies within interval. honest believed ran study report learned . Sounds nice, doesn’t ? , big promise Bayesian approach. analysis really want , express really believe data telling .","code":""},{"path":"bayesian-statistics.html","id":"evidentiary-standards-you-can-believe","chapter":"16 Bayesian statistics","heading":"16.3.2 Evidentiary standards you can believe","text":"\\(p\\) .02 strongly indicated \\(null\\) hypothesis fails account whole facts. shall often astray draw conventional line .05 consider smaller values \\(p\\) indicate real discrepancy.\n– Sir Ronald Fisher (Fisher 1925)Consider quote Sir Ronald Fisher, one founders become orthodox approach statistics. anyone ever entitled express opinion intended function p-values, ’s Fisher. passage, taken classic guide Statistical Methods Research Workers, ’s pretty clear means reject null hypothesis p < .05. opinion, take p < .05 mean “real effect”, “shall often astray”. view hardly unusual. experience, practitioners express views similar Fisher’s. essence, p < .05 convention assumed represent fairly stringent evidential standard.Well, true ? One way approach question try convert p-values Bayes factors, see two compare. ’s easy thing p-value fundamentally different kind calculation Bayes factor, don’t measure thing. However, attempts work relationship two, ’s somewhat surprising. example, Johnson (2013) presents pretty compelling case (t-tests least) p < .05 threshold corresponds roughly Bayes factor somewhere 3:1 5:1 favour alternative. ’s right, Fisher’s claim bit stretch. Let’s suppose null hypothesis true half time (.e., prior probability \\(H_0\\) 0.5), use numbers work posterior probability null hypothesis given rejected p < .05. Using data Johnson (2013), see reject null p ă .05, ’ll correct 80% time. don’t know , opinion, evidential standard ensures ’ll wrong 20% decisions isn’t good enough. fact remains , quite contrary Fisher’s claim, reject p < .05 shall quite often go astray. ’s stringent evidential threshold .","code":""},{"path":"bayesian-statistics.html","id":"the-p-value-is-a-lie.","chapter":"16 Bayesian statistics","heading":"16.3.3 The p-value is a lie.","text":"cake lie.cake lie.cake lie.cake lie.\n– Portal189Okay, point might thinking real problem orthodox statistics, just p < .05 standard. one sense, ’s true. recommendation Johnson (2013) gives “everyone must Bayesian now”. Instead, suggestion wiser shift conventional standard something like p < .01 level. ’s unreasonable view take, view problem little severe . opinion, ’s fairly big problem built way () orthodox hypothesis tests constructed. grossly naive humans actually research, p-values wrong.Sounds like absurd claim, right? Well, consider following scenario. ’ve come really exciting research hypothesis design study test . ’re diligent, run power analysis work sample size , run study. run hypothesis test pops p-value 0.072. Really bloody annoying, right?? possibilities:conclude effect try publish null resultYou guess might effect try publish “borderline significant” resultYou give try new studyYou collect data see p value goes (preferably!) drops “magic” criterion p < .05Which choose? reading , urge take time think . honest . don’t stress much, ’re screwed matter choose. Based experiences author, reviewer editor, well stories ’ve heard others, ’s happen case:Let’s start option 1. try publish null result, paper struggle published. reviewers think p = .072 really null result. ’ll argue ’s borderline significant. reviewers agree ’s null result claim even though null results publishable, isn’t. One two reviewers might even side, ’ll fighting uphill battle get .Let’s start option 1. try publish null result, paper struggle published. reviewers think p = .072 really null result. ’ll argue ’s borderline significant. reviewers agree ’s null result claim even though null results publishable, isn’t. One two reviewers might even side, ’ll fighting uphill battle get .Okay, let’s think option number 2. Suppose try publish borderline significant result. reviewers claim ’s null result published. Others claim evidence ambiguous, collect data get clear significant result. , publication process favour .Okay, let’s think option number 2. Suppose try publish borderline significant result. reviewers claim ’s null result published. Others claim evidence ambiguous, collect data get clear significant result. , publication process favour .Given difficulties publishing “ambiguous” result like p = .072, option number 3 might seem tempting: give something else. ’s recipe career suicide. give try new project every time find faced ambiguity, work never published. ’re academia without publication record can lose job. option .Given difficulties publishing “ambiguous” result like p = .072, option number 3 might seem tempting: give something else. ’s recipe career suicide. give try new project every time find faced ambiguity, work never published. ’re academia without publication record can lose job. option .looks like ’re stuck option 4. don’t conclusive results, decide collect data re-run analysis. Seems sensible, unfortunately , p-values now incorrect. . just p-values calculated study. . p-values calculated past p-values calculate future. Fortunately, -one notice. ’ll get published, ’ll lied.looks like ’re stuck option 4. don’t conclusive results, decide collect data re-run analysis. Seems sensible, unfortunately , p-values now incorrect. . just p-values calculated study. . p-values calculated past p-values calculate future. Fortunately, -one notice. ’ll get published, ’ll lied.Wait, ? can last part true? mean, sounds like perfectly reasonable strategy doesn’t ? collected data, results weren’t conclusive, now want collect data results conclusive. ’s wrong ?Honestly, ’s nothing wrong . ’s reasonable, sensible rational thing . real life, exactly every researcher . Unfortunately, theory null Hypothesis testing described earlier chapter forbids .190 reason theory assumes experiment finished data . assumes experiment , considers two possible decisions. ’re using conventional p < .05 threshold, decisions shown oin Table 16.11.Table 16.11:  Conventional Null hypothesis signicance testing (NHST) p < .05)’re adding third possible action decision making problem. Specifically, ’re using p-value reason justify continuing experiment. consequence ’ve transformed decision-making procedure one looks like Table 16.12.Table 16.12:  Carrying data collecting based p-values obtained preliminary testingThe “basic” theory null Hypothesis testing isn’t built handle sort thing, form described earlier chapter. ’re kind person choose “collect data” real life, implies making decisions accordance rules null hypothesis testing. Even happen arrive decision hypothesis test, aren’t following decision process implies, ’s failure follow process causing problem.191 p-values lie.Worse yet, ’re lie dangerous way, ’re small. give sense just bad can , consider following (worst case) scenario. Imagine ’re really super-enthusiastic researcher tight budget didn’t pay attention warnings . design study comparing two groups. desperately want see significant result \\(p < .05\\) level, really don’t want collect data (’s expensive). order cut costs start collecting data every time new observation arrives run t-test data. t-tests says \\(p < .05\\) stop experiment report significant result. , keep collecting data. keep reach pre-defined spending limit experiment. Let’s say limit kicks \\(N = 1000\\) observations. turns , truth matter real effect found: null hypothesis true. , ’s chance ’ll make end experiment (correctly) conclude effect? ideal world, answer 95%. , whole point \\(p < .05\\) criterion control Type error rate 5%, ’d hope ’s 5% chance falsely rejecting null hypothesis situation. However, ’s guarantee true. ’re breaking rules. ’re running tests repeatedly, “peeking” data see ’ve gotten significant result, bets .bad ? answer shown solid black line Figure 16.1, ’s astoundingly bad. peek data every single observation, 49% chance make Type error. ’s, um, quite bit bigger 5% ’s supposed . way comparison, imagine used following strategy. Start collecting data. Every single time observation arrives, run [Bayesian* t-tests] look Bayes factor. ’ll assume Johnson (2013) right, ’ll treat Bayes factor 3:1 roughly equivalent p-value .05.192 time around, trigger happy researcher uses following procedure. Bayes factor 3:1 favour null, stop experiment retain null. 3:1 favour alternative, stop experiment reject null. Otherwise continue testing. Now, just like last time, let’s assume null hypothesis true. happens? happens, ran simulations scenario , results shown dashed line Figure 16.1. turns Type error rate much much lower 49% rate getting using orthodox t-test.\nFigure 16.1: badly can things go wrong re-run tests every time new data arrive? frequentist, answer wrong\nways, remarkable. entire point orthodox null hypothesis testing control Type error rate. Bayesian methods aren’t actually designed . Yet, turns , faced “trigger happy” researcher keeps running hypothesis tests data come , Bayesian approach much effective. Even 3:1 standard, Bayesians consider unacceptably lax, much safer p < .05 rule.","code":""},{"path":"bayesian-statistics.html","id":"is-it-really-this-bad","chapter":"16 Bayesian statistics","heading":"16.3.4 Is it really this bad?","text":"example gave previous section pretty extreme situation. real life, people don’t run hypothesis tests every time new observation arrives. ’s fair say p < .05 threshold “really” corresponds 49% Type error rate (.e., \\(p = .49\\)). fact remains want p-values honest either switch completely different way hypothesis tests enforce strict rule peeking. allowed use data decide terminate experiment. allowed look “borderline” p-value decide collect data. aren’t even allowed change data analyis strategy looking data. strictly required follow rules, otherwise p-values calculate nonsense.yes, rules surprisingly strict. class exercise couple years back, asked students think scenario. Suppose started running study intention collecting \\(N = 80\\) people. study starts follow rules, refusing look data run tests. reach \\(N = 50\\) willpower gives … take peek. Guess ? ’ve got significant result! Now, sure, know said ’d keep running study sample size \\(N = 80\\), seems sort pointless now, right? result significant sample size \\(N = 50\\), wouldn’t wasteful inefficient keep collecting data? Aren’t tempted stop? Just little? Well, keep mind , Type error rate \\(p < .05\\) just ballooned 8%. report \\(p < .05\\) paper, ’re really saying \\(p < .08\\). ’s bad consequences “just one peek” can .Now consider . scientific literature filled t-tests, ANOVAs, regressions chi-square tests. wrote book didn’t pick tests arbitrarily. reason four tools appear introductory statistics texts bread butter tools science. None tools include correction deal “data peeking”: assume ’re . realistic assumption? real life, many people think “peeked” data experiment finished adapted subsequent behaviour seeing data looked like? Except sampling procedure fixed external constraint, ’m guessing answer “people done ”. happened, can infer reported p-values wrong. Worse yet, don’t know decision process actually followed, way know p-values . can’t compute p-value don’t know decision making procedure researcher used. reported p-value remains lie.Given , take home message? ’s Bayesian methods foolproof. researcher determined cheat, can always . Bayes’ rule stop people lying, can stop rigging experiment. ’s point . point one made beginning book Section 1.1: reason run statistical tests protect us . reason “data peeking” concern ’s tempting, even honest researchers. theory statistical inference acknowledge . Yes, might try defend p-values saying ’s fault researcher using properly, mind misses point. theory statistical inference completely naive humans doesn’t even consider possibility researcher might look data isn’t theory worth . essence, point :Good laws origins bad morals.\n– Ambrosius Macrobius 193Good rules statistical testing acknowledge human frailty. None us without sin. None us beyond temptation. good system statistical inference still work even used actual humans. Orthodox null hypothesis testing .194","code":""},{"path":"bayesian-statistics.html","id":"bayesian-t-tests","chapter":"16 Bayesian statistics","heading":"16.4 Bayesian t-tests","text":"important type statistical inference problem discussed book Comparing two means, discussed detail chapter t-tests. can remember back far, ’ll recall several versions t-test. ’ll talk little Bayesian versions independent samples t-tests paired samples t-test section.","code":""},{"path":"bayesian-statistics.html","id":"independent-samples-t-test","chapter":"16 Bayesian statistics","heading":"16.4.1 Independent samples t-test","text":"common type t-test independent samples t-test, arises data harpo.csv data set used earlier chapter t-tests. data set, two groups students, received lessons Anastasia took classes Bernadette. question want answer whether ’s difference grades received two groups students. Back chapter suggested analyse kind data using Independent Samples t-test jamovi, gave us results Figure 16.2. obtain p-value less 0.05, reject null hypothesis.\nFigure 16.2: Independent Samples t-test result jamovi\nBayesian version t-test look like? can get Bayes factor analysis selecting ‘Bayes factor’ checkbox ‘Tests’ option, accepting suggested default value ‘Prior’. gives results shown table Figure 16.3. get table Bayes factor statistic 1.75, meaning evidence provided data 1.8:1 favour alternative hypothesis.moving , ’s worth highlighting difference orthodox test results Bayesian one. According orthodox test, obtained significant result, though barely. Nevertheless, many people happily accept p = .043 reasonably strong evidence effect. contrast, notice Bayesian test doesn’t even reach 2:1 odds favour effect, considered weak evidence best. experience ’s pretty typical outcome. Bayesian methods usually require evidence rejecting null.\nFigure 16.3: Bayes factors analysis alongside Independent Samples t-Test\n","code":""},{"path":"bayesian-statistics.html","id":"paired-samples-t-test","chapter":"16 Bayesian statistics","heading":"16.4.2 Paired samples t-test","text":"Back Section 11.5 discussed chico.csv data set student grades measured two tests, interested finding whether grades went test 1 test 2. every student tests, tool used analyse data paired samples t-test. Figure 16.4 shows jamovi results table conventional paired t-test alongside Bayes factor analysis. point, hope can read output without difficulty. data provide evidence 6000:1 favour alternative. probably reject null confidence!\nFigure 16.4: Paired samples T-Test Bayes Factor result jamovi\n","code":""},{"path":"bayesian-statistics.html","id":"summary-14","chapter":"16 Bayesian statistics","heading":"16.5 Summary","text":"first half chapter focused primarily theoretical underpinnings Bayesian statistics. introduced mathematics Bayesian inference works section Probabilistic reasoning rational agents, gave basic overview Bayesian hypothesis tests]. Finally, devoted space talking think Bayesian methods worth using.gave practical example, Bayesian t-test (Section 16.4). ’re interested learning Bayesian approach, many good books look . John Kruschke’s book Bayesian Data Analysis pretty good place start (Kruschke 2011) nice mix theory practice. approach little different “Bayes factor” approach ’ve discussed , won’t covering ground. ’re cognitive psychologist, might want check Lee Wagenmakers (2014). picked two think ’re especially useful people discipline, ’s lot good books , look around!","code":""},{"path":"epilogue.html","id":"epilogue","chapter":"17 Epilogue","heading":"17 Epilogue","text":"“Begin beginning”, King said, gravely, “go till come end: stop” – Lewis CarrollIt feels somewhat strange writing chapter, little inappropriate. epilogue write book finished, book really isn’t finished. lot things still missing book. doesn’t index yet. lot references missing. “” exercises. general, feel lot things wrong presentation, organisation content book. Given , don’t want try write “proper” epilogue. haven’t finished writing substantive content yet, doesn’t make sense try bring together. version book going go online students use, may purchase hard copy , want give least veneer closure. let’s give go, shall ?","code":""},{"path":"epilogue.html","id":"the-undiscovered-statistics","chapter":"17 Epilogue","heading":"17.1 The undiscovered statistics","text":"First, ’m going talk bit content wish ’d chance cram version book, just can get sense ideas world statistics. think important even book getting close final product. One thing students often fail realise introductory statistics classes just , introduction. want go wider world real data analysis, learn whole lot new tools extend content undergraduate lectures sorts different ways. Don’t assume something can’t done just wasn’t covered undergrad. Don’t assume something right thing just covered undergrad class. stop falling victim trap, think ’s useful give bit overview ideas ","code":""},{"path":"epilogue.html","id":"omissions-within-the-topics-covered","chapter":"17 Epilogue","heading":"17.1.1 Omissions within the topics covered","text":"Even within topics covered book, lot omissions ’d like redress future version book. Just sticking things purely statistics (rather things associated jamovi), following representative exhaustive list topics ’d like expand time:types correlations. [Correlation regression] talked two types correlation: Pearson Spearman. methods assessing correlation applicable case two continuous variables want assess relationship . case variables nominal scale? one nominal scale continuous? actually methods computing correlations cases (e.g., polychoric correlation), good see included.types correlations. [Correlation regression] talked two types correlation: Pearson Spearman. methods assessing correlation applicable case two continuous variables want assess relationship . case variables nominal scale? one nominal scale continuous? actually methods computing correlations cases (e.g., polychoric correlation), good see included.detail effect sizes. general, think treatment effect sizes throughout book little cursory . almost every instance, ’ve tended just pick one measure effect size (usually popular one) describe . However, almost tests models multiple ways thinking effect size, ’d like go detail future.detail effect sizes. general, think treatment effect sizes throughout book little cursory . almost every instance, ’ve tended just pick one measure effect size (usually popular one) describe . However, almost tests models multiple ways thinking effect size, ’d like go detail future.Dealing violated assumptions. number places book ’ve talked things can find assumptions test (model) violated, think say . particular, think nice talk lot detail can tranform variables fix problems. talked bit [Pragmatic matters, discussion isn’t detailed enough think.Dealing violated assumptions. number places book ’ve talked things can find assumptions test (model) violated, think say . particular, think nice talk lot detail can tranform variables fix problems. talked bit [Pragmatic matters, discussion isn’t detailed enough think.Interaction terms regression. Factorial ANOVA talked fact can interaction terms ANOVA, also pointed ANOVA can interpreted kind linear regression model. Yet, talking regression [Correlation regression] made mention interactions . However, ’s nothing stopping including interaction terms regression model. ’s just little complicated figure “interaction” actually means ’re talking interaction two continuous predictors, can done one way. Even , liked talk little .Interaction terms regression. Factorial ANOVA talked fact can interaction terms ANOVA, also pointed ANOVA can interpreted kind linear regression model. Yet, talking regression [Correlation regression] made mention interactions . However, ’s nothing stopping including interaction terms regression model. ’s just little complicated figure “interaction” actually means ’re talking interaction two continuous predictors, can done one way. Even , liked talk little .Method planned comparison. mentioned Factorial ANOVA, ’s always appropriate using post hoc correction like Tukey’s HSD ANOVA, especially clear (limited) set comparisons cared ahead time. like talk future.Method planned comparison. mentioned Factorial ANOVA, ’s always appropriate using post hoc correction like Tukey’s HSD ANOVA, especially clear (limited) set comparisons cared ahead time. like talk future.Multiple comparison methods. Even within context talking post hoc tests multiple comparisons, liked talk methods detail, talk methods exist besides options mentioned.Multiple comparison methods. Even within context talking post hoc tests multiple comparisons, liked talk methods detail, talk methods exist besides options mentioned.","code":""},{"path":"epilogue.html","id":"statistical-models-missing-from-the-book","chapter":"17 Epilogue","heading":"17.1.2 Statistical models missing from the book","text":"Statistics huge field. core tools ’ve described book (chi-square tests, t-tests, regression ANOVA) basic tools widely used everyday data analysis, form core introductory stats books. However, lot tools . many data analysis situations tools don’t cover, great give sense just much , example:Nonlinear regression. discussing regression Chapter 12, saw regression assumes relationship predictors outcomes linear. hand, talked simpler problem correlation Chapter 4, saw exist tools (e.g., Spearman correlations) able assess non-linear relationships variables. number tools statistics can used non-linear regression. instance, non-linear regression models assume relationship predictors outcomes monotonic (e.g., isotonic regression), others assume smooth necessarily monotonic (e.g., Lowess regression), others assume relationship known form happens nonlinear (e.g., polynomial regression).Nonlinear regression. discussing regression Chapter 12, saw regression assumes relationship predictors outcomes linear. hand, talked simpler problem correlation Chapter 4, saw exist tools (e.g., Spearman correlations) able assess non-linear relationships variables. number tools statistics can used non-linear regression. instance, non-linear regression models assume relationship predictors outcomes monotonic (e.g., isotonic regression), others assume smooth necessarily monotonic (e.g., Lowess regression), others assume relationship known form happens nonlinear (e.g., polynomial regression).Logistic regression. Yet another variation regression occurs outcome variable binary, predictors continuous. instance, suppose ’re investigating social media, want know ’s possible predict whether someone Twitter function income, age, range variables. basically regression model, can’t use regular linear regression outcome variable binary (’re either Twitter ’re ). outcome variable binary, ’s way residuals possibly normally distributed. number tools statisticians can apply situation, prominent logistic regression.Logistic regression. Yet another variation regression occurs outcome variable binary, predictors continuous. instance, suppose ’re investigating social media, want know ’s possible predict whether someone Twitter function income, age, range variables. basically regression model, can’t use regular linear regression outcome variable binary (’re either Twitter ’re ). outcome variable binary, ’s way residuals possibly normally distributed. number tools statisticians can apply situation, prominent logistic regression.General Linear Model (GLM). GLM actually family models includes logistic regression, linear regression, () nonlinear regression, ANOVA many others. basic idea GLM essentially idea underpins linear models, allows idea data might normally distributed, allows nonlinear relationships predictors outcomes. lot handy analyses can run fall within GLM, ’s useful thing know .General Linear Model (GLM). GLM actually family models includes logistic regression, linear regression, () nonlinear regression, ANOVA many others. basic idea GLM essentially idea underpins linear models, allows idea data might normally distributed, allows nonlinear relationships predictors outcomes. lot handy analyses can run fall within GLM, ’s useful thing know .Survival analysis. brief introduction research design talked “differential attrition”, tendency people leave study non-random fashion. Back , talking potential methodological concern, lot situations differential attrition actually thing ’re interested . Suppose, instance, ’re interested finding long people play different kinds computer games single session. people tend play RTS (real time strategy) games longer stretches FPS (first person shooter) games? might design study like . People come lab, can play long little like. ’re finished, record time spent playing. However, due ethical restrictions, let’s suppose allow keep playing longer two hours. lot people stop playing two hour limit, know exactly long played. people run two hour limit, don’t know long kept playing ’d able continue study. consequence, data systematically censored: ’re missing long times. analyse data sensibly? problem survival analysis solves. specifically designed handle situation, ’re systematically missing one “side” data study ended. ’s widely used health research, context often literally used analyse survival. instance, may tracking people particular type cancer, received treatment others received treatment B, funding track 5 years. end study period people alive, others . context, survival analysis useful determining treatment effective, telling risk death people face time.Survival analysis. brief introduction research design talked “differential attrition”, tendency people leave study non-random fashion. Back , talking potential methodological concern, lot situations differential attrition actually thing ’re interested . Suppose, instance, ’re interested finding long people play different kinds computer games single session. people tend play RTS (real time strategy) games longer stretches FPS (first person shooter) games? might design study like . People come lab, can play long little like. ’re finished, record time spent playing. However, due ethical restrictions, let’s suppose allow keep playing longer two hours. lot people stop playing two hour limit, know exactly long played. people run two hour limit, don’t know long kept playing ’d able continue study. consequence, data systematically censored: ’re missing long times. analyse data sensibly? problem survival analysis solves. specifically designed handle situation, ’re systematically missing one “side” data study ended. ’s widely used health research, context often literally used analyse survival. instance, may tracking people particular type cancer, received treatment others received treatment B, funding track 5 years. end study period people alive, others . context, survival analysis useful determining treatment effective, telling risk death people face time.Mixed models. Repeated measures ANOVA often used situations observations clustered within experimental units. good example track individual people across multiple time points. Let’s say ’re tracking happiness time, two people. Aaron’s happiness starts 10, drops 8, 6. Belinda’s happiness starts 6, rises 8 10. two people “overall” level happiness (average across three time points 8), repeated measures ANOVA analysis treat Aaron Belinda way. ’s clearly wrong. Aaron’s happiness decreasing, whereas Belinda’s increasing. want optimally analyse data experiment people can change time, need powerful tool repeated measures ANOVA. tools people use solve problem called “mixed” models, designed learn individual experimental units (e.g. happiness individual people time) well overall effects (e.g. effect money happiness time). Repeated measures ANOVA perhaps simplest example mixed model, ’s lot can mixed models can’t repeated measures ANOVA.Mixed models. Repeated measures ANOVA often used situations observations clustered within experimental units. good example track individual people across multiple time points. Let’s say ’re tracking happiness time, two people. Aaron’s happiness starts 10, drops 8, 6. Belinda’s happiness starts 6, rises 8 10. two people “overall” level happiness (average across three time points 8), repeated measures ANOVA analysis treat Aaron Belinda way. ’s clearly wrong. Aaron’s happiness decreasing, whereas Belinda’s increasing. want optimally analyse data experiment people can change time, need powerful tool repeated measures ANOVA. tools people use solve problem called “mixed” models, designed learn individual experimental units (e.g. happiness individual people time) well overall effects (e.g. effect money happiness time). Repeated measures ANOVA perhaps simplest example mixed model, ’s lot can mixed models can’t repeated measures ANOVA.Multidimensional scaling. Factor analysis example “unsupervised learning” model. means , unlike “supervised learning” tools ’ve mentioned, can’t divide variables predictors outcomes. Regression supervised learning whereas factor analysis unsupervised learning. ’s type unsupervised learning model however. example, factor analysis one concerned analysis correlations variables. However, many situations ’re actually interested analysing similarities dissimilarities objects, items people. number tools can use situation, best known multidimensional scaling (MDS). MDS, idea find “geometric” representation items. item “plotted” point space, distance two points measure dissimilar items .Multidimensional scaling. Factor analysis example “unsupervised learning” model. means , unlike “supervised learning” tools ’ve mentioned, can’t divide variables predictors outcomes. Regression supervised learning whereas factor analysis unsupervised learning. ’s type unsupervised learning model however. example, factor analysis one concerned analysis correlations variables. However, many situations ’re actually interested analysing similarities dissimilarities objects, items people. number tools can use situation, best known multidimensional scaling (MDS). MDS, idea find “geometric” representation items. item “plotted” point space, distance two points measure dissimilar items .Clustering. Another example unsupervised learning model clustering (also referred classification), want organise items meaningful groups, similar items assigned groups. lot clustering unsupervised, meaning don’t know anything groups , just guess. “supervised clustering” situations need predict group memberships basis variables, group memberships actually observables. Logistic regression good example tool works way. However, don’t actually know group memberships, use different tools (e.g., k-means clustering). even situations want something called “semi-supervised clustering”, know group memberships items others. can probably guess, clustering pretty big topic, pretty useful thing know .Clustering. Another example unsupervised learning model clustering (also referred classification), want organise items meaningful groups, similar items assigned groups. lot clustering unsupervised, meaning don’t know anything groups , just guess. “supervised clustering” situations need predict group memberships basis variables, group memberships actually observables. Logistic regression good example tool works way. However, don’t actually know group memberships, use different tools (e.g., k-means clustering). even situations want something called “semi-supervised clustering”, know group memberships items others. can probably guess, clustering pretty big topic, pretty useful thing know .Causal models. One thing haven’t talked much book can use statistical modelling learn causal relationships variables. instance, consider following three variables might interest thinking someone died firing squad. might want measure whether execution order given (variable ), whether marksman fired gun (variable B), whether person got hit bullet (variable C). three variables correlated one another (e.g., correlation guns fired people getting hit bullets), actually want make stronger statements merely talking correlations. want talk causation. want able say execution order () causes marksman fire (B) causes someone get shot (C). can express directed arrow notation: write \\(\\rightarrow B \\rightarrow C\\). “causal chain” fundamentally different explanation events one marksman fires first, causes shooting \\(B \\rightarrow C\\), causes executioner “retroactively” issue execution order, \\(B \\rightarrow \\). “common effect” model says C caused B. can see different. first causal model, managed stop executioner issuing order (intervening change ), shooting happened. second model, shooting happened way marksman following execution order. big literature statistics trying understand causal relationships variables, number different tools exist help test different causal stories data. widely used tools (psychology least) structural equations modelling (SEM), point ’d like extend book talk .Causal models. One thing haven’t talked much book can use statistical modelling learn causal relationships variables. instance, consider following three variables might interest thinking someone died firing squad. might want measure whether execution order given (variable ), whether marksman fired gun (variable B), whether person got hit bullet (variable C). three variables correlated one another (e.g., correlation guns fired people getting hit bullets), actually want make stronger statements merely talking correlations. want talk causation. want able say execution order () causes marksman fire (B) causes someone get shot (C). can express directed arrow notation: write \\(\\rightarrow B \\rightarrow C\\). “causal chain” fundamentally different explanation events one marksman fires first, causes shooting \\(B \\rightarrow C\\), causes executioner “retroactively” issue execution order, \\(B \\rightarrow \\). “common effect” model says C caused B. can see different. first causal model, managed stop executioner issuing order (intervening change ), shooting happened. second model, shooting happened way marksman following execution order. big literature statistics trying understand causal relationships variables, number different tools exist help test different causal stories data. widely used tools (psychology least) structural equations modelling (SEM), point ’d like extend book talk .course, even listing incomplete. haven’t mentioned time series analysis, item response theory, market basket analysis, classification regression trees, huge range topics. However, list ’ve given essentially wish list book. Sure, double length book, mean scope become broad enough cover things applied researchers psychology need use.","code":""},{"path":"epilogue.html","id":"other-ways-of-doing-inference","chapter":"17 Epilogue","heading":"17.1.3 Other ways of doing inference","text":"different sense book incomplete focuses pretty heavily narrow old-fashioned view inferential statistics done. [Estimating unknown quantities sample] talked little bit idea unbiased estimators, sampling distributions . Hypothesis testing talked theory null hypothesis significance testing p-values. ideas around since early 20th century, tools ’ve talked book rely heavily theoretical ideas time. ’ve felt obligated stick topics vast majority data analysis science also reliant ideas. However, theory statistics restricted topics , whilst everyone know practical importance, many respects ideas represent best practice contemporary data analysis. One things ’m especially happy ’ve able go little beyond . Bayesian statistics now presents Bayesian perspective reasonable amount detail, book overall still pretty heavily weighted towards frequentist orthodoxy. Additionally, number approaches inference worth mentioning:Bootstrapping. Throughout book, whenever ’ve introduced hypothesis test, ’ve strong tendency just make assertions like “sampling distribution BLAH t-distribution” something like . cases, ’ve actually attempted justify assertion. example, talking \\(\\chi^2\\) tests Categorical data analysis made reference known relationship normal distributions \\(\\chi^2\\) distributions (see [Introduction probability) explain end assuming sampling distribution goodness--fit statistic \\(\\chi^2\\) . However, ’s also case lot sampling distributions , well, wrong. \\(\\chi^2\\) test good example. based assumption distribution data, assumption known wrong small sample sizes! Back early 20th century, wasn’t much situation. Statisticians developed mathematical results said “assumptions BLAH data, sampling distribution approximately BLAH”, best . lot times didn’t even . lots data analysis situations -one found mathematical solution sampling distributions need. late 20th century, corresponding tests didn’t exist didn’t work. However, computers changed now. lots fancy tricks, --fancy, can use get around . simplest bootstrapping, ’s simplest form ’s incredibly simple. simulate results experiment lots lots times, twin assumptions () null hypothesis true (b) unknown population distribution actually looks pretty similar raw data. words, instead assuming data (instance) normally distributed, just assume population looks sample, use computers simulate sampling distribution test statistic assumption holds. Despite relying somewhat dubious assumption (.e., population distribution sample!) bootstrapping quick easy method works remarkably well practice lots data analysis problems.Bootstrapping. Throughout book, whenever ’ve introduced hypothesis test, ’ve strong tendency just make assertions like “sampling distribution BLAH t-distribution” something like . cases, ’ve actually attempted justify assertion. example, talking \\(\\chi^2\\) tests Categorical data analysis made reference known relationship normal distributions \\(\\chi^2\\) distributions (see [Introduction probability) explain end assuming sampling distribution goodness--fit statistic \\(\\chi^2\\) . However, ’s also case lot sampling distributions , well, wrong. \\(\\chi^2\\) test good example. based assumption distribution data, assumption known wrong small sample sizes! Back early 20th century, wasn’t much situation. Statisticians developed mathematical results said “assumptions BLAH data, sampling distribution approximately BLAH”, best . lot times didn’t even . lots data analysis situations -one found mathematical solution sampling distributions need. late 20th century, corresponding tests didn’t exist didn’t work. However, computers changed now. lots fancy tricks, --fancy, can use get around . simplest bootstrapping, ’s simplest form ’s incredibly simple. simulate results experiment lots lots times, twin assumptions () null hypothesis true (b) unknown population distribution actually looks pretty similar raw data. words, instead assuming data (instance) normally distributed, just assume population looks sample, use computers simulate sampling distribution test statistic assumption holds. Despite relying somewhat dubious assumption (.e., population distribution sample!) bootstrapping quick easy method works remarkably well practice lots data analysis problems.Cross validation. One question pops stats classes every now , usually student trying provocative, “care inferential statistics ? just describe sample?” answer question usually something like , “true interest scientists specific sample observed past, want make predictions data might observe future”. lot issues statistical inference arise fact always expect future similar bit different past. , generally, new data won’t quite old data. , lot situations, try derive mathematical rules help us draw inferences likely correct new data, rather pick statements best describe old data. instance, given two models B, data set \\(X\\) collected today, try pick model best describe new data set \\(Y\\) ’re going collect tomorrow. Sometimes ’s convenient simulate process, ’s cross-validation . divide data set two subsets, \\(X1\\) \\(X2\\). Use subset \\(X1\\) train model (e.g., estimate regression coefficients, let’s say), assess model performance one \\(X2\\). gives measure well model generalises old data set new one, often better measure good model just fit full data set \\(X\\).Cross validation. One question pops stats classes every now , usually student trying provocative, “care inferential statistics ? just describe sample?” answer question usually something like , “true interest scientists specific sample observed past, want make predictions data might observe future”. lot issues statistical inference arise fact always expect future similar bit different past. , generally, new data won’t quite old data. , lot situations, try derive mathematical rules help us draw inferences likely correct new data, rather pick statements best describe old data. instance, given two models B, data set \\(X\\) collected today, try pick model best describe new data set \\(Y\\) ’re going collect tomorrow. Sometimes ’s convenient simulate process, ’s cross-validation . divide data set two subsets, \\(X1\\) \\(X2\\). Use subset \\(X1\\) train model (e.g., estimate regression coefficients, let’s say), assess model performance one \\(X2\\). gives measure well model generalises old data set new one, often better measure good model just fit full data set \\(X\\).Robust statistics. Life messy, nothing really works way ’s supposed . just true statistics anything else, trying analyse data ’re often stuck sorts problems data just messier ’re supposed . Variables supposed normally distributed actually normally distributed, relationships supposed linear actually linear, observations data set almost certainly junk (.e., measuring ’re supposed ). messiness ignored statistical theory developed book. However, ignoring problem doesn’t always solve . Sometimes, ’s actually okay ignore mess, types statistical tools “robust”, .e., data don’t satisfy theoretical assumptions nevertheless still work pretty well. types statistical tools robust, even minor deviations theoretical assumptions cause break. Robust statistics branch stats concerned question, talk things like “breakdown point” statistic. , messy data statistic trusted? touched places. mean robust estimator central tendency variable, median . instance, suppose told ages five best friends 34, 39, 31, 43 4003 years. old think average? , true population mean ? use sample mean estimator population mean, get answer 830 years. use sample median estimator population mean, get answer 39 years. Notice , even though ’re “technically” wrong thing second case (using median estimate mean!) ’re actually getting better answer. problem one observations clearly, obviously, lie. don’t friend aged 4003 years. ’s probably typo, probably meant type 43. typed 53 instead 43, 34 instead 43? sure typo ? Sometimes errors data subtle, can’t detect just eyeballing sample, ’re still errors contaminate data, still affect conclusions. Robust statistics concerned can make safe inferences even faced contamination don’t know . ’s pretty cool stuff.Robust statistics. Life messy, nothing really works way ’s supposed . just true statistics anything else, trying analyse data ’re often stuck sorts problems data just messier ’re supposed . Variables supposed normally distributed actually normally distributed, relationships supposed linear actually linear, observations data set almost certainly junk (.e., measuring ’re supposed ). messiness ignored statistical theory developed book. However, ignoring problem doesn’t always solve . Sometimes, ’s actually okay ignore mess, types statistical tools “robust”, .e., data don’t satisfy theoretical assumptions nevertheless still work pretty well. types statistical tools robust, even minor deviations theoretical assumptions cause break. Robust statistics branch stats concerned question, talk things like “breakdown point” statistic. , messy data statistic trusted? touched places. mean robust estimator central tendency variable, median . instance, suppose told ages five best friends 34, 39, 31, 43 4003 years. old think average? , true population mean ? use sample mean estimator population mean, get answer 830 years. use sample median estimator population mean, get answer 39 years. Notice , even though ’re “technically” wrong thing second case (using median estimate mean!) ’re actually getting better answer. problem one observations clearly, obviously, lie. don’t friend aged 4003 years. ’s probably typo, probably meant type 43. typed 53 instead 43, 34 instead 43? sure typo ? Sometimes errors data subtle, can’t detect just eyeballing sample, ’re still errors contaminate data, still affect conclusions. Robust statistics concerned can make safe inferences even faced contamination don’t know . ’s pretty cool stuff.","code":""},{"path":"epilogue.html","id":"miscellaneous-topics","chapter":"17 Epilogue","heading":"17.1.4 Miscellaneous topics","text":"Suppose ’re survey, ’re interested exercise weight. send data four people. Adam says exercises lot overweight. Briony says exercises lot overweight. Carol says exercise overweight. Tim says exercise refuses answer question weight. Elaine return survey. now missing data problem. one entire survey missing, one question missing another one, ? Ignoring missing data , general, safe thing . Let’s think Tim’s survey . Firstly, notice , basis responses, appear similar Carol (neither us exercise) Adam Briony. forced guess weight, ’d guess closer . Maybe ’d make correction fact Adam Tim males Briony Carol females. statistical name kind guessing “imputation”. imputation safely hard, ’s important, especially missing data missing systematic way. fact people overweight often pressured feel poorly weight (often thanks public health campaigns), actually reason suspect people responding likely overweight people respond. Imputing weight Tim means number overweight people sample probably rise 1 3 (ignore Tim), 2 4 (impute Tim’s weight). Clearly matters. sensibly complicated sounds. Earlier, suggested treat Tim like Carol, since gave answer exercise question. ’s quite right. systematic difference . answered question, Tim didn’t. Given social pressures faced overweight people, isn’t likely Tim overweight Carol? course still ignoring fact ’s sensible impute single weight Tim, actually knew weight. Instead, need impute range plausible guesses (referred multiple imputation), order capture fact ’re uncertain Tim’s weight Carol’s. let’s get started problem posed fact Elaine didn’t send survey. can probably guess, dealing missing data increasingly important topic. fact, ’ve told lot journals fields accept studies missing data unless kind sensible multiple imputation scheme followed.Suppose ’re survey, ’re interested exercise weight. send data four people. Adam says exercises lot overweight. Briony says exercises lot overweight. Carol says exercise overweight. Tim says exercise refuses answer question weight. Elaine return survey. now missing data problem. one entire survey missing, one question missing another one, ? Ignoring missing data , general, safe thing . Let’s think Tim’s survey . Firstly, notice , basis responses, appear similar Carol (neither us exercise) Adam Briony. forced guess weight, ’d guess closer . Maybe ’d make correction fact Adam Tim males Briony Carol females. statistical name kind guessing “imputation”. imputation safely hard, ’s important, especially missing data missing systematic way. fact people overweight often pressured feel poorly weight (often thanks public health campaigns), actually reason suspect people responding likely overweight people respond. Imputing weight Tim means number overweight people sample probably rise 1 3 (ignore Tim), 2 4 (impute Tim’s weight). Clearly matters. sensibly complicated sounds. Earlier, suggested treat Tim like Carol, since gave answer exercise question. ’s quite right. systematic difference . answered question, Tim didn’t. Given social pressures faced overweight people, isn’t likely Tim overweight Carol? course still ignoring fact ’s sensible impute single weight Tim, actually knew weight. Instead, need impute range plausible guesses (referred multiple imputation), order capture fact ’re uncertain Tim’s weight Carol’s. let’s get started problem posed fact Elaine didn’t send survey. can probably guess, dealing missing data increasingly important topic. fact, ’ve told lot journals fields accept studies missing data unless kind sensible multiple imputation scheme followed.Power analysis. Hypothesis testing discussed concept power (.e., likely able detect effect actually exists) referred power analysis, collection tools useful assessing much power study . Power analysis can useful planning study (e.g., figuring large sample ’re likely need), also serves useful role analysing data already collected. instance, suppose get significant result, estimate effect size. can use information estimate much power study actually . kind useful, especially effect size large. instance, suppose reject null hypothesis \\(p< .05\\), use power analysis figure estimated power .08. significant result means , null hypothesis fact true, 5% chance getting data like . low power means , even null hypothesis false effect size really small looks, 8% chance getting data like . suggests need pretty cautious, luck seems played big part results, one way !Power analysis. Hypothesis testing discussed concept power (.e., likely able detect effect actually exists) referred power analysis, collection tools useful assessing much power study . Power analysis can useful planning study (e.g., figuring large sample ’re likely need), also serves useful role analysing data already collected. instance, suppose get significant result, estimate effect size. can use information estimate much power study actually . kind useful, especially effect size large. instance, suppose reject null hypothesis \\(p< .05\\), use power analysis figure estimated power .08. significant result means , null hypothesis fact true, 5% chance getting data like . low power means , even null hypothesis false effect size really small looks, 8% chance getting data like . suggests need pretty cautious, luck seems played big part results, one way !Data analysis using theory-inspired models. places book ’ve mentioned response time (RT) data, record long takes someone something (e.g., make simple decision). ’ve mentioned RT data almost invariably non-normal, positively skewed. Additionally, ’s thing known speedaccuracy trade-: try make decisions quickly (low RT) ’re likely make poorer decisions (lower accuracy). measure accuracy participant’s decisions RT, ’ll probably find speed accuracy related. ’s story , course, people make better decisions others regardless fast ’re going. Moreover, speed depends cognitive processes (.e., time spent thinking) also physiological ones (e.g., fast can move muscles). ’s starting sound like analysing data complicated process. indeed , one things find dig psychological literature already exist mathematical models (called “sequential sampling models”) describe people make simple decisions, models take account lot factors mentioned . won’t find theoretically-inspired models standard statistics textbook. Standard stats textbooks describe standard tools, tools meaningfully applied lots different disciplines, just psychology. ANOVA example standard tool just applicable psychology pharmacology. Sequential sampling models , psychology-specific, less. doesn’t make less powerful tools. fact, ’re analysing data people make choices quickly really using sequential sampling models analyse data. Using ANOVA regression whatever won’t work well, theoretical assumptions underpin well-matched data. contrast, sequential sampling models explicitly designed analyse specific type data, theoretical assumptions extremely well-matched data.Data analysis using theory-inspired models. places book ’ve mentioned response time (RT) data, record long takes someone something (e.g., make simple decision). ’ve mentioned RT data almost invariably non-normal, positively skewed. Additionally, ’s thing known speedaccuracy trade-: try make decisions quickly (low RT) ’re likely make poorer decisions (lower accuracy). measure accuracy participant’s decisions RT, ’ll probably find speed accuracy related. ’s story , course, people make better decisions others regardless fast ’re going. Moreover, speed depends cognitive processes (.e., time spent thinking) also physiological ones (e.g., fast can move muscles). ’s starting sound like analysing data complicated process. indeed , one things find dig psychological literature already exist mathematical models (called “sequential sampling models”) describe people make simple decisions, models take account lot factors mentioned . won’t find theoretically-inspired models standard statistics textbook. Standard stats textbooks describe standard tools, tools meaningfully applied lots different disciplines, just psychology. ANOVA example standard tool just applicable psychology pharmacology. Sequential sampling models , psychology-specific, less. doesn’t make less powerful tools. fact, ’re analysing data people make choices quickly really using sequential sampling models analyse data. Using ANOVA regression whatever won’t work well, theoretical assumptions underpin well-matched data. contrast, sequential sampling models explicitly designed analyse specific type data, theoretical assumptions extremely well-matched data.","code":""},{"path":"epilogue.html","id":"learning-the-basics-and-learning-them-in-jamovi","chapter":"17 Epilogue","heading":"17.2 Learning the basics, and learning them in jamovi","text":"Okay, long list. even listing massively incomplete. really lot big ideas statistics haven’t covered book. can seem pretty depressing finish almost 500-page textbook told beginning, especially start suspect half stuff ’ve taught wrong. instance, lot people field strongly argue use classical ANOVA model, yet ’ve devoted two whole chapters ! Standard ANOVA can attacked Bayesian perspective, robust statistics perspective, even “’s just plain wrong” perspective (people frequently use ANOVA actually using mixed models). learn ?see , two key arguments. Firstly, ’s pure pragmatism argument. Rightly wrongly, ANOVA widely used. want understand scientific literature, need understand ANOVA. secondly, ’s “incremental knowledge” argument. way handy seen one-way ANOVA trying learn factorial ANOVA, understanding ANOVA helpful understanding advanced tools, lot tools extend modify basic ANOVA setup way. instance, although mixed models way useful ANOVA regression, ’ve never heard anyone learning mixed models work without first worked ANOVA regression. learn crawl can climb mountain.Actually, want push point bit . One thing ’ve done lot book talk fundamentals. spent lot time probability theory. talked theory estimation hypothesis tests detail needed . ? Looking back, might ask whether really needed spend time talking probability distribution , even section probability density. goal book teach run t-test ANOVA, really necessary? just huge waste everyone’s time???answer, hope ’ll agree, . goal introductory stats teach ANOVA. ’s teach t-tests, regressions, histograms, p-values. goal start path towards becoming skilled data analyst. order become skilled data analyst, need able ANOVA, t-tests, regressions histograms. need able think properly data. need able learn advanced statistical models talked last section, understand theory upon based. need access software let use advanced tools. , opinion least, extra time ’ve spent fundamentals pays . understand probability theory, ’ll find much easier switch frequentist analyses Bayesian ones.short, think big payoff learning statistics way extensibility. book covers basics data analysis, book massive overhead terms learning probability theory . ’s whole lot things pushes learn besides specific analyses book covers. goal learn run ANOVA minimum possible time, well, book wasn’t good choice. say, don’t think goal. think want learn data analysis. really goal, want make sure skills learn introductory stats class naturally cleanly extensible complicated models need real world data analysis. want make sure learn use tools real data analysts use, can learn . yeah, okay, ’re beginner right now (started book), doesn’t mean given dumbed-story, story don’t tell probability density, story don’t tell nightmare factorial ANOVA unbalanced designs. doesn’t mean given baby toys instead proper data analysis tools. Beginners aren’t dumb, just lack knowledge. need complexities real world data analysis hidden . need skills tools let handle complexities inevitably ambush real world.hope book, finished book one day turn , able help .Author’s note – ’ve mentioned , ’ll quickly mention . book’s reference list appallingly incomplete. Please don’t assume sources ’ve relied upon. final version book lot references. see anything clever sounding book doesn’t seem reference, can absolutely promise idea someone else’s. introductory textbook: none ideas original. ’ll take responsibility errors, can’t take credit good stuff. Everything smart book came someone else, deserve proper attribution excellent work. just haven’t chance give yet.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
