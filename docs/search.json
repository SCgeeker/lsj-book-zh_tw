[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "用jamovi上手統計學",
    "section": "",
    "text": "本書內容涵蓋各大學心理學、公共衛生或社會科學大學部基礎統計的學習項目。本書同時提供以jamovi做為處理資料的工具操作指引。如同一般的統計教科書，本書從描述統計及統計圖開始，接著討論機率理論，取樣及估計，還有虛無假設檢定。理解理論概念之後，接著學習統計方法：包括列聯表分析、相關、t檢定、迴歸、變異數分析以及因素分析。最後一章將介紹貝氏統計。\n\n\n\n引用建議(英): Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15"
  },
  {
    "objectID": "16-Bayesian-statistics.html#probabilistic-reasoning-by-rational-agents",
    "href": "16-Bayesian-statistics.html#probabilistic-reasoning-by-rational-agents",
    "title": "16  貝氏統計",
    "section": "16.1 Probabilistic reasoning by rational agents",
    "text": "16.1 Probabilistic reasoning by rational agents\nFrom a Bayesian perspective statistical inference is all about belief revision. I start out with a set of candidate hypotheses h about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.\n\nI’m carrying an umbrella. Do you think it will rain?\n\nIn this problem I have presented you with a single piece of data (d = I’m carrying the umbrella), and I’m asking you to tell me your belief or hypothesis about whether it’s raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem?\n\n16.1.1 Priors: what you believed before\nThe first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia2 and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see Table 16.1). If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you I’m betting that this really is what you believe) then what I have written here is your prior distribution, written \\(P(h)\\).\n\n\n\n\nTable 16.1:  How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall \n\nHypothesisDegree of Belief\n\nRainy day0.15\n\nDry day0.85\n\n\n\n\n\n\n\n16.1.2 Likelihoods: theories about the data\nTo solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,3 and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write this out as in Table 16.2.\n\n\n\n\nTable 16.2:  How likely am I to be carrying an umbrella on rainy and dry days \n\nDataData\n\nHypothesisUmbrellaNo umbrella\n\nRainy day0.300.70\n\nDry day0.050.95\n\n\n\n\n\nIt’s important to remember that each cell in this table describes your beliefs about what data d will be observed, given the truth of a particular hypothesis \\(h\\). This “conditional probability” is written \\(P(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. In Bayesian statistics, this is referred to as the likelihood of the data \\(d\\) given the hypothesis \\(h\\).4\n\n\n16.1.3 The joint probability of data and hypothesis\nAt this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it so I’m going to try to build it up from more basic ideas.\nLet’s start out with one of the rules of probability theory. I listed it way back in Table 7.1, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I’m carrying an umbrella (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(P(d,h)\\), and you can calculate it by multiplying the prior \\(P(h)\\) by the likelihood \\(P(d|h)\\). Mathematically, we say that\n\\[P(d,h)=P(d|h)P(h)\\]\nSo, what is the probability that today is a rainy day and I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is \\(30\\%\\). So the probability that both of these things are true is calculated by multiplying the two\n\\[\n\\begin{split}\nP(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\n& = 0.30 \\times 0.15 \\\\\n& = 0.045\n\\end{split}\n\\]\nIn other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with Table 16.3.\n\n\n\n\nTable 16.3:  Four possibilities combining rain (or not) and umbrella carrying (or not) \n\nUmbrellaNo-umbrella\n\nRainy0.0450.105\n\nDry0.04250.807\n\n\n\n\n\nThis table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us Table 16.4.\n\n\n\n\nTable 16.4:  Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals \n\nUmbrellaNo-umbrellaTotal\n\nRainy0.0450.1050.15\n\nDry0.04250.8070.85\n\nTotal0.08750.9121\n\n\n\n\n\nThis is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course, as that’s our prior.5 The important thing isn’t the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.\nNow, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (Table 16.5):\n\n\n\n\nTable 16.5:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities \n\nUmbrellaNo-umbrella\n\nRainyP(Umbrella, Rainy)P(No-umbrella, Rainy)P(Rainy)\n\nDryP(Umbrella, Dry)P(No-umbrella, Dry)P(Dry)\n\nP(Umbrella)P(No-umbrella)\n\n\n\n\n\nFinally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let \\(d_1\\) refer to the possibility that you observe me carrying an umbrella, and \\(d_2\\) refers to you observing me not carrying one. Similarly, \\(h_1\\) is your hypothesis that today is rainy, and \\(h_2\\) is the hypothesis that it is not. Using this notation, the table looks like Table 16.6.\n\n\n\n\nTable 16.6:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities \n\n\\( d_1 \\)\\( d_2 \\)\n\n\\( h_1 \\)\\(P(h_1, d_1)\\)\\(P(h_1, d_2)\\)\\( P(h_1) \\)\n\n\\( h_2 \\)\\(P(h_2, d_1)\\)\\(P(h_2, d_2)\\)\\( P(h_2) \\)\n\n\\( P(d_1) \\)\\( P(d_2) \\)\n\n\n\n\n\n\n\n16.1.4 Updating beliefs using Bayes’ rule\nThe table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now know that I have an umbrella.6 To reflect this new knowledge, our revised table must have the following numbers. (see Table 16.7).\n\n\n\n\nTable 16.7:  Revising beliefs given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0\n\nDry0\n\nTotal10\n\n\n\n\n\nIn other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that \\(P(umbrella) = 1\\).\nWhat two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is slightly more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like Table 16.8, perhaps?\n\n\n\n\nTable 16.8:  Revising probabilities given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0.5140\n\nDry0.4860\n\nTotal10\n\n\n\n\n\nWhat this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The posterior probability of rain \\(P(h\\|d)\\) given that I am carrying an umbrella is 51.4%\nHow did I calculate these numbers? You can probably guess. To work out that there was a \\(0.514\\) probability of “rain”, all I did was take the \\(0.045\\) probability of “rain and umbrella” and divide it by the \\(0.0875\\) chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data \\(P(d, h)\\) by the marginal probability of the data \\(P(d)\\), and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: 7\n\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]\nHowever, remember what I said at the start of the last section, namely that the joint probability \\(P(d, h)\\) is calculated by multiplying the prior Pphq by the likelihood \\(P(d|h)\\). In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:\n\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule."
  },
  {
    "objectID": "16-Bayesian-statistics.html#bayesian-hypothesis-tests",
    "href": "16-Bayesian-statistics.html#bayesian-hypothesis-tests",
    "title": "16  貝氏統計",
    "section": "16.2 Bayesian hypothesis tests",
    "text": "16.2 Bayesian hypothesis tests\nIn Chapter 9 I described the orthodox approach to hypothesis testing. It took an entire chapter to describe, because null hypothesis testing is a very elaborate contraption that people find very hard to make sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let’s pick a setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to compare, a null hypothesis \\(h_0\\) and an alternative hypothesis \\(h_1\\). Prior to running the experiment we have some beliefs \\(P(h)\\) about which hypotheses are true. We run an experiment and obtain data d. Unlike frequentist statistics, Bayesian statistics does allow us to talk about the probability that the null hypothesis is true. Better yet, it allows us to calculate the posterior probability of the null hypothesis, using Bayes’ rule:\n\\[P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}\\]\nThis formula tells us exactly how much belief we should have in the null hypothesis after having observed the data d. Similarly, we can work out how much belief to place in the alternative hypothesis using essentially the same equation. All we do is change the subscript\n\\[P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}\\]\nIt’s all so simple that I feel like an idiot even bothering to write these equations down, since all I’m doing is copying Bayes rule from the previous section.8\n\n16.2.1 The Bayes factor\nIn practice, most Bayesian data analysts tend not to talk in terms of the raw posterior probabilities \\(P(h_0|d)\\) and \\(P(h_1|d)\\). Instead, we tend to talk in terms of the posterior odds ratio. Think of it like betting. Suppose, for instance, the posterior probability of the null hypothesis is 25%, and the posterior probability of the alternative is 75%. The alternative hypothesis is three times as probable as the null, so we say that the odds are 3:1 in favour of the alternative. Mathematically, all we have to do to calculate the posterior odds is divide one posterior probability by the other\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3\\]\nOr, to write the same thing in terms of the equations above\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}\\]\nActually, this equation is worth expanding on. There are three different terms here that you should know. On the left hand side, we have the posterior odds, which tells you what you believe about the relative plausibilty of the null hypothesis and the alternative hypothesis after seeing the data. On the right hand side, we have the prior odds, which indicates what you thought before seeing the data. In the middle, we have the Bayes factor, which describes the amount of evidence provided by the data. (Table 16.9).\n\n\n\n\nTable 16.9:  Posterior odds given the Bsyes factor and prior odds \n\n\\(\\frac{P(h_1|d)}{h_0|d}\\)\\(=\\)\\(\\frac{P(d|h_1)}{d|h_0}\\)\\(\\times \\)\\(\\frac{P(h_1)}{h_0}\\)\n\n\\(\\Uparrow\\)\\(\\Uparrow\\)\\(\\Uparrow\\)\n\nPosterior oddsBayes factorPrior odds\n\n\n\n\n\nThe Bayes factor (sometimes abbreviated as BF) has a special place in Bayesian hypothesis testing, because it serves a similar role to the p-value in orthodox hypothesis testing. The Bayes factor quantifies the strength of evidence provided by the data, and as such it is the Bayes factor that people tend to report when running a Bayesian hypothesis test. The reason for reporting Bayes factors rather than posterior odds is that different researchers will have different priors. Some people might have a strong bias to believe the null hypothesis is true, others might have a strong bias to believe it is false. Because of this, the polite thing for an applied researcher to do is report the Bayes factor. That way, anyone reading the paper can multiply the Bayes factor by their own personal prior odds, and they can work out for themselves what the posterior odds would be. In any case, by convention we like to pretend that we give equal consideration to both the null hypothesis and the alternative, in which case the prior odds equals 1, and the posterior odds becomes the same as the Bayes factor\n\n\n16.2.2 Interpreting Bayes factors\nOne of the really nice things about the Bayes factor is the numbers are inherently meaningful. If you run an experiment and you compute a Bayes factor of 4, it means that the evidence provided by your data corresponds to betting odds of 4:1 in favour of the alternative. However, there have been some attempts to quantify the standards of evidence that would be considered meaningful in a scientific context. The two most widely used are from Jeffreys (1961) and Kass & Raftery (1995). Of the two, I tend to prefer the Kass & Raftery (1995) table because it’s a bit more conservative. So here it is (Table 16.10).\n\n\n\n\nTable 16.10:  Bayes factors and strength of evidence \n\nBayes factorInterpretation\n\n1 - 3Negligible evidence\n\n3 - 20Positive evidence\n\n20 - 150Strong evidence\n\n> 150Very strong evidence\n\n\n\n\n\nAnd to be perfectly honest, I think that even the Kass & Raftery (1995) standards are being a bit charitable. If it were up to me, I’d have called the “positive evidence” category “weak evidence”. To me, anything in the range 3:1 to 20:1 is “weak” or “modest” evidence at best. But there are no hard and fast rules here. What counts as strong or weak evidence depends entirely on how conservative you are and upon the standards that your community insists upon before it is willing to label a finding as “true”.\nIn any case, note that all the numbers listed above make sense if the Bayes factor is greater than 1 (i.e., the evidence favours the alternative hypothesis). However, one big practical advantage of the Bayesian approach relative to the orthodox approach is that it also allows you to quantify evidence for the null. When that happens, the Bayes factor will be less than 1. You can choose to report a Bayes factor less than 1, but to be honest I find it confusing. For example, suppose that the likelihood of the data under the null hypothesis \\(P(d|h_0)\\) is equal to 0.2, and the corresponding likelihood \\(P(d|h_1)\\) under the alternative hypothesis is 0.1. Using the equations given above, Bayes factor here would be\n\\[BF=\\frac{P(d|h_1)}{P(d|h_0)}=\\frac{0.1}{0.2}=0.5\\]\nRead literally, this result tells is that the evidence in favour of the alternative is 0.5 to 1. I find this hard to understand. To me, it makes a lot more sense to turn the equation “upside down”, and report the amount op evidence in favour of the null. In other words, what we calculate is this\n\\[BF^{'}=\\frac{P(d|h_0)}{P(d|h_1)}=\\frac{0.2}{0.1}=2\\]\nAnd what we would report is a Bayes factor of 2:1 in favour of the null. Much easier to understand, and you can interpret this using the table above."
  },
  {
    "objectID": "16-Bayesian-statistics.html#why-be-a-bayesian",
    "href": "16-Bayesian-statistics.html#why-be-a-bayesian",
    "title": "16  貝氏統計",
    "section": "16.3 Why be a Bayesian",
    "text": "16.3 Why be a Bayesian\nUp to this point I’ve focused exclusively on the logic underpinning Bayesian statistics. We’ve talked about the idea of “probability as a degree of belief”, and what it implies about how a rational agent should reason about the world. The question that you have to answer for yourself is this: how do you want to do your statistics? Do you want to be an orthodox statistician, relying on sampling distributions and p-values to guide your decisions? Or do you want to be a Bayesian, relying on things like prior beliefs, Bayes factors and the rules for rational belief revision? And to be perfectly honest, I can’t answer this question for you. Ultimately it depends on what you think is right. It’s your call and your call alone. That being said, I can talk a little about why I prefer the Bayesian approach.\n\n16.3.1 Statistics that mean what you think they mean\n\nYou keep using that word. I do not think it means what you think it means\n– Inigo Montoya, The Princess Bride 9\n\nTo me, one of the biggest advantages to the Bayesian approach is that it answers the right questions. Within the Bayesian framework, it is perfectly sensible and allowable to refer to “the probability that a hypothesis is true”. You can even try to calculate this probability. Ultimately, isn’t that what you want your statistical tests to tell you? To an actual human being, this would seem to be the whole point of doing statistics, i.e., to determine what is true and what isn’t. Any time that you aren’t exactly sure about what the truth is, you should use the language of probability theory to say things like “there is an 80% chance that Theory A is true, but a 20% chance that Theory B is true instead”.\nThis seems so obvious to a human, yet it is explicitly forbidden within the orthodox framework. To a frequentist, such statements are a nonsense because “the theory is true” is not a repeatable event. A theory is true or it is not, and no probabilistic statements are allowed, no matter how much you might want to make them. There’s a reason why, back in Section 9.5, I repeatedly warned you not to interpret the p-value as the probability that the null hypothesis is true. There’s a reason why almost every textbook on statstics is forced to repeat that warning. It’s because people desperately want that to be the correct interpretation. Frequentist dogma notwithstanding, a lifetime of experience of teaching undergraduates and of doing data analysis on a daily basis suggests to me that most actual humans think that “the probability that the hypothesis is true” is not only meaningful, it’s the thing we care most about. It’s such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a p-value this way. For example, here is a quote from an official Newspoll report in 2013, explaining how to interpret their (frequentist) data analysis:10\n\nThroughout the report, where relevant, statistically significant changes have been noted. All significance tests have been based on the 95 percent level of confidence. This means that if a change is noted as being statistically significant, there is a 95 percent probability that a real change has occurred, and is not simply due to chance variation. (emphasis added)\n\nNope! That’s not what p < .05 means. That’s not what 95% confidence means to a frequentist statistician. The bolded section is just plain wrong. Orthodox methods cannot tell you that “there is a 95% chance that a real change has occurred”, because this is not the kind of event to which frequentist probabilities may be assigned. To an ideological frequentist, this sentence should be meaningless. Even if you’re a more pragmatic frequentist, it’s still the wrong definition of a p-value. It is simply not an allowed or correct thing to say if you want to rely on orthodox statistical tools.\nOn the other hand, let’s suppose you are a Bayesian. Although the bolded passage is the wrong definition of a p-value, it’s pretty much exactly what a Bayesian means when they say that the posterior probability of the alternative hypothesis is greater than 95%. And here’s the thing. If the Bayesian posterior is actually the thing you want to report, why are you even trying to use orthodox methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.\nSpeaking for myself, I found this to be the most liberating thing about switching to the Bayesian view. Once you’ve made the jump, you no longer have to wrap your head around counter-intuitive definitions of p-values. You don’t have to bother remembering why you can’t say that you’re 95% confident that the true mean lies within some interval. All you have to do is be honest about what you believed before you ran the study and then report what you learned from doing it. Sounds nice, doesn’t it? To me, this is the big promise of the Bayesian approach. You do the analysis you really want to do, and express what you really believe the data are telling you.\n\n\n16.3.2 Evidentiary standards you can believe\n\nIf \\(p\\) is below .02 it is strongly indicated that the \\(null\\) hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that smaller values of \\(p\\) indicate a real discrepancy.\n– Sir Ronald Fisher (Fisher, 1925)\n\nConsider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of p-values, it’s Fisher. In this passage, taken from his classic guide Statistical Methods for Research Workers, he’s pretty clear about what it means to reject a null hypothesis at p < .05. In his opinion, if we take p < .05 to mean there is “a real effect”, then “we shall not often be astray”. This view is hardly unusual. In my experience, most practitioners express views very similar to Fisher’s. In essence, the p < .05 convention is assumed to represent a fairly stringent evidential standard.\nWell, how true is that? One way to approach this question is to try to convert p-values to Bayes factors, and see how the two compare. It’s not an easy thing to do because a p-value is a fundamentally different kind of calculation to a Bayes factor, and they don’t measure the same thing. However, there have been some attempts to work out the relationship between the two, and it’s somewhat surprising. For example, Johnson (2013) presents a pretty compelling case that (for t-tests at least) the p < .05 threshold corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If that’s right, then Fisher’s claim is a bit of a stretch. Let’s suppose that the null hypothesis is true about half the time (i.e., the prior probability of \\(H_0\\) is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at p < .05. Using the data from Johnson (2013), we see that if you reject the null at p ă .05, you’ll be correct about 80% of the time. I don’t know about you but, in my opinion, an evidential standard that ensures you’ll be wrong on 20% of your decisions isn’t good enough. The fact remains that, quite contrary to Fisher’s claim, if you reject at p < .05 you shall quite often go astray. It’s not a very stringent evidential threshold at all.\n\n\n16.3.3 The p-value is a lie.\n\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\n– Portal11\n\nOkay, at this point you might be thinking that the real problem is not with orthodox statistics, just the p < .05 standard. In one sense, that’s true. The recommendation that Johnson (2013) gives is not that “everyone must be a Bayesian now”. Instead, the suggestion is that it would be wiser to shift the conventional standard to something like a p < .01 level. That’s not an unreasonable view to take, but in my view the problem is a little more severe than that. In my opinion, there’s a fairly big problem built into the way most (but not all) orthodox hypothesis tests are constructed. They are grossly naive about how humans actually do research, and because of this most p-values are wrong.\nSounds like an absurd claim, right? Well, consider the following scenario. You’ve come up with a really exciting research hypothesis and you design a study to test it. You’re very diligent, so you run a power analysis to work out what your sample size should be, and you run the study. You run your hypothesis test and out pops a p-value of 0.072. Really bloody annoying, right?\nWhat should you do? Here are some possibilities:\n\nYou conclude that there is no effect and try to publish it as a null result\nYou guess that there might be an effect and try to publish it as a “borderline significant” result\nYou give up and try a new study\nYou collect some more data to see if the p value goes up or (preferably!) drops below the “magic” criterion of p < .05\n\nWhich would you choose? Before reading any further, I urge you to take some time to think about it. Be honest with yourself. But don’t stress about it too much, because you’re screwed no matter what you choose. Based on my own experiences as an author, reviewer and editor, as well as stories I’ve heard from others, here’s what will happen in each case:\n\nLet’s start with option 1. If you try to publish it as a null result, the paper will struggle to be published. Some reviewers will think that p = .072 is not really a null result. They’ll argue it’s borderline significant. Other reviewers will agree it’s a null result but will claim that even though some null results are publishable, yours isn’t. One or two reviewers might even be on your side, but you’ll be fighting an uphill battle to get it through.\nOkay, let’s think about option number 2. Suppose you try to publish it as a borderline significant result. Some reviewers will claim that it’s a null result and should not be published. Others will claim that the evidence is ambiguous, and that you should collect more data until you get a clear significant result. Again, the publication process does not favour you.\nGiven the difficulties in publishing an “ambiguous” result like p = .072, option number 3 might seem tempting: give up and do something else. But that’s a recipe for career suicide. If you give up and try a new project every time you find yourself faced with ambiguity, your work will never be published. And if you’re in academia without a publication record you can lose your job. So that option is out.\nIt looks like you’re stuck with option 4. You don’t have conclusive results, so you decide to collect some more data and re-run the analysis. Seems sensible, but unfortunately for you, if you do this all of your p-values are now incorrect. All of them. Not just the p-values that you calculated for this study. All of them. All the p-values you calculated in the past and all the p-values you will calculate in the future. Fortunately, no-one will notice. You’ll get published, and you’ll have lied.\n\nWait, what? How can that last part be true? I mean, it sounds like a perfectly reasonable strategy doesn’t it? You collected some data, the results weren’t conclusive, so now what you want to do is collect more data until the the results are conclusive. What’s wrong with that?\nHonestly, there’s nothing wrong with it. It’s a reasonable, sensible and rational thing to do. In real life, this is exactly what every researcher does. Unfortunately, the theory of null hypothesis testing as I described it in Chapter 9 forbids you from doing this.12 The reason is that the theory assumes that the experiment is finished and all the data are in. And because it assumes the experiment is over, it only considers two possible decisions. If you’re using the conventional p < .05 threshold, those decisions are shown oin Table 16.11.\n\n\n\n\nTable 16.11:  Conventional Null hypothesis signicance testing (NHST) with p < .05) \n\nOutcomeAction\n\np less than .05Reject the null\n\np greater than .05Retain the null\n\n\n\n\n\nWhat you’re doing is adding a third possible action to the decision making problem. Specifically, what you’re doing is using the p-value itself as a reason to justify continuing the experiment. And as a consequence you’ve transformed the decision-making procedure into one that looks more like Table 16.12.\n\n\n\n\nTable 16.12:  Carrying on data collecting based on p-values obtained in preliminary testing \n\nOutcomeAction\n\np less than .05Stop the experiment and reject the null\n\np between .05 and .1Continue the experiment\n\np greater than .1Stop the experiment and retain the null\n\n\n\n\n\nThe “basic” theory of null hypothesis testing isn’t built to handle this sort of thing, not in the form I described in Chapter 9. If you’re the kind of person who would choose to “collect more data” in real life, it implies that you are not making decisions in accordance with the rules of null hypothesis testing. Even if you happen to arrive at the same decision as the hypothesis test, you aren’t following the decision process it implies, and it’s this failure to follow the process that is causing the problem.13 Your p-values are a lie.\nWorse yet, they’re a lie in a dangerous way, because they’re all too small. To give you a sense of just how bad it can be, consider the following (worst case) scenario. Imagine you’re a really super-enthusiastic researcher on a tight budget who didn’t pay any attention to my warnings above. You design a study comparing two groups. You desperately want to see a significant result at the \\(p < .05\\) level, but you really don’t want to collect any more data than you have to (because it’s expensive). In order to cut costs you start collecting data but every time a new observation arrives you run a t-test on your data. If the t-tests says \\(p < .05\\) then you stop the experiment and report a significant result. If not, you keep collecting data. You keep doing this until you reach your pre-defined spending limit for this experiment. Let’s say that limit kicks in at \\(N = 1000\\) observations. As it turns out, the truth of the matter is that there is no real effect to be found: the null hypothesis is true. So, what’s the chance that you’ll make it to the end of the experiment and (correctly) conclude that there is no effect? In an ideal world, the answer here should be 95%. After all, the whole point of the \\(p < .05\\) criterion is to control the Type I error rate at 5%, so what we’d hope is that there’s only a 5% chance of falsely rejecting the null hypothesis in this situation. However, there’s no guarantee that will be true. You’re breaking the rules. Because you’re running tests repeatedly, “peeking” at your data to see if you’ve gotten a significant result, all bets are off.\nSo how bad is it? The answer is shown as the solid black line in Figure 16.1, and it’s astoundingly bad. If you peek at your data after every single observation, there is a 49% chance that you will make a Type I error. That’s, um, quite a bit bigger than the 5% that it’s supposed to be. By way of comparison, imagine that you had used the following strategy. Start collecting data. Every single time an observation arrives, run Bayesian t-tests and look at the Bayes factor. I’ll assume that Johnson (2013) is right, and I’ll treat a Bayes factor of 3:1 as roughly equivalent to a p-value of .05.14 This time around, our trigger happy researcher uses the following procedure. If the Bayes factor is 3:1 or more in favour of the null, stop the experiment and retain the null. If it is 3:1 or more in favour of the alternative, stop the experiment and reject the null. Otherwise continue testing. Now, just like last time, let’s assume that the null hypothesis is true. What happens? As it happens, I ran the simulations for this scenario too, and the results are shown as the dashed line in Figure 16.1. It turns out that the Type I error rate is much much lower than the 49% rate that we were getting by using the orthodox t-test.\n\n\n\n\n\nFigure 16.1: How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is very wrong\n\n\n\n\nIn some ways, this is remarkable. The entire point of orthodox null hypothesis testing is to control the Type I error rate. Bayesian methods aren’t actually designed to do this at all. Yet, as it turns out, when faced with a “trigger happy” researcher who keeps running hypothesis tests as the data come in, the Bayesian approach is much more effective. Even the 3:1 standard, which most Bayesians would consider unacceptably lax, is much safer than the p < .05 rule.\n\n\n16.3.4 Is it really this bad?\nThe example I gave in the previous section is a pretty extreme situation. In real life, people don’t run hypothesis tests every time a new observation arrives. So it’s not fair to say that the p < .05 threshold “really” corresponds to a 49% Type I error rate (i.e., \\(p = .49\\)). But the fact remains that if you want your p-values to be honest then you either have to switch to a completely different way of doing hypothesis tests or enforce a strict rule of no peeking. You are not allowed to use the data to decide when to terminate the experiment. You are not allowed to look at a “borderline” p-value and decide to collect more data. You aren’t even allowed to change your data analyis strategy after looking at data. You are strictly required to follow these rules, otherwise the p-values you calculate will be nonsense.\nAnd yes, these rules are surprisingly strict. As a class exercise a couple of years back, I asked students to think about this scenario. Suppose you started running your study with the intention of collecting \\(N = 80\\) people. When the study starts out you follow the rules, refusing to look at the data or run any tests. But when you reach \\(N = 50\\) your willpower gives in… and you take a peek. Guess what? You’ve got a significant result! Now, sure, you know you said that you’d keep running the study out to a sample size of \\(N = 80\\), but it seems sort of pointless now, right? The result is significant with a sample size of \\(N = 50\\), so wouldn’t it be wasteful and inefficient to keep collecting data? Aren’t you tempted to stop? Just a little? Well, keep in mind that if you do, your Type I error rate at \\(p < .05\\) just ballooned out to 8%. When you report \\(p < .05\\) in your paper, what you’re really saying is \\(p < .08\\). That’s how bad the consequences of “just one peek” can be.\nNow consider this. The scientific literature is filled with t-tests, ANOVAs, regressions and chi-square tests. When I wrote this book I didn’t pick these tests arbitrarily. The reason why these four tools appear in most introductory statistics texts is that these are the bread and butter tools of science. None of these tools include a correction to deal with “data peeking”: they all assume that you’re not doing it. But how realistic is that assumption? In real life, how many people do you think have “peeked” at their data before the experiment was finished and adapted their subsequent behaviour after seeing what the data looked like? Except when the sampling procedure is fixed by an external constraint, I’m guessing the answer is “most people have done it”. If that has happened, you can infer that the reported p-values are wrong. Worse yet, because we don’t know what decision process they actually followed, we have no way to know what the p-values should have been. You can’t compute a p-value when you don’t know the decision making procedure that the researcher used. And so the reported p-value remains a lie.\nGiven all of the above, what is the take home message? It’s not that Bayesian methods are foolproof. If a researcher is determined to cheat, they can always do so. Bayes’ rule cannot stop people from lying, nor can it stop them from rigging an experiment. That’s not my point here. My point is the same one I made at the very beginning of the book in Section 1.1: the reason why we run statistical tests is to protect us from ourselves. And the reason why “data peeking” is such a concern is that it’s so tempting, even for honest researchers. A theory for statistical inference has to acknowledge this. Yes, you might try to defend p-values by saying that it’s the fault of the researcher for not using them properly, but to my mind that misses the point. A theory of statistical inference that is so completely naive about humans that it doesn’t even consider the possibility that the researcher might look at their own data isn’t a theory worth having. In essence, my point is this:\n\nGood laws have their origins in bad morals.\n– Ambrosius Macrobius 15\n\nGood rules for statistical testing have to acknowledge human frailty. None of us are without sin. None of us are beyond temptation. A good system for statistical inference should still work even when it is used by actual humans. Orthodox null hypothesis testing does not.16"
  },
  {
    "objectID": "16-Bayesian-statistics.html#bayesian-t-tests",
    "href": "16-Bayesian-statistics.html#bayesian-t-tests",
    "title": "16  貝氏統計",
    "section": "16.4 Bayesian t-tests",
    "text": "16.4 Bayesian t-tests\nAn important type of statistical inference problem discussed in this book is comparing two means, discussed in some detail in Chapter 11 on t-tests. If you can remember back that far, you’ll recall that there are several versions of the t-test. I’ll talk a little about Bayesian versions of the independent samples t-tests and the paired samples t-test in this section.\n\n16.4.1 Independent samples t-test\nThe most common type of t-test is the independent samples t-test, and it arises when you have data as in the harpo.csv data set that we used in Chapter 11 on t-tests. In this data set, we have two groups of students, those who received lessons from Anastasia and those who took their classes with Bernadette. The question we want to answer is whether there’s any difference in the grades received by these two groups of students. Back in Chapter 11 I suggested you could analyse this kind of data using the Independent Samples t-test in jamovi, which gave us the results in Figure 16.2. As we obtain a p-value less than 0.05, we reject the null hypothesis.\n\n\n\n\n\nFigure 16.2: Independent Samples t-test result in jamovi\n\n\n\n\nWhat does the Bayesian version of the t-test look like? We can get the Bayes factor analysis by selecting the ‘Bayes factor’ checkbox under the ‘Tests’ option, and accepting the suggested default value for the ‘Prior’. This gives the results shown in the table in Figure 16.3. What we get in this table is a Bayes factor statistic of 1.75, meaning that the evidence provided by these data are about 1.8:1 in favour of the alternative hypothesis.\nBefore moving on, it’s worth highlighting the difference between the orthodox test results and the Bayesian one. According to the orthodox test, we obtained a significant result, though only barely. Nevertheless, many people would happily accept p = .043 as reasonably strong evidence for an effect. In contrast, notice that the Bayesian test doesn’t even reach 2:1 odds in favour of an effect, and would be considered very weak evidence at best. In my experience that’s a pretty typical outcome. Bayesian methods usually require more evidence before rejecting the null.\n\n\n\n\n\nFigure 16.3: Bayes factors analysis alongside Independent Samples t-Test\n\n\n\n\n\n\n16.4.2 Paired samples t-test\nBack in Section 11.5 I discussed the chico.csv data set in which student grades were measured on two tests, and we were interested in finding out whether grades went up from test 1 to test 2. Because every student did both tests, the tool we used to analyse the data was a paired samples t-test. Figure 16.4 shows the jamovi results table for the conventional paired t-test alongside the Bayes factor analysis. At this point, I hope you can read this output without any difficulty. The data provide evidence of about 6000:1 in favour of the alternative. We could probably reject the null with some confidence!\n\n\n\n\n\nFigure 16.4: Paired samples T-Test and Bayes Factor result in jamovi"
  },
  {
    "objectID": "16-Bayesian-statistics.html#summary",
    "href": "16-Bayesian-statistics.html#summary",
    "title": "16  貝氏統計",
    "section": "16.5 Summary",
    "text": "16.5 Summary\nThe first half of this chapter was focused primarily on the theoretical underpinnings of Bayesian statistics. I introduced the mathematics for how Bayesian inference works in the section on Probabilistic reasoning by rational agents, and gave a very basic overview of Bayesian hypothesis tests]. Finally, I devoted some space to talking about why I think Bayesian methods are worth using.\nThen I gave a practical example, with Bayesian t-tests. If you’re interested in learning more about the Bayesian approach, there are many good books you could look into. John Kruschke’s book Doing Bayesian Data Analysis is a pretty good place to start (Kruschke, 2011) and is a nice mix of theory and practice. His approach is a little different to the “Bayes factor” approach that I’ve discussed here, so you won’t be covering the same ground. If you’re a cognitive psychologist, you might want to check out Lee & Wagenmakers (2014). I picked these two because I think they’re especially useful for people in my discipline, but there’s a lot of good books out there, so look around!\n\n\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nJeffreys, H. (1961). The theory of probability (3rd ed.). Oxford.\n\n\nJohnson, V. E. (2013). Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 48, 19313–19317.\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773–795.\n\n\nKruschke, J. K. (2011). Doing Bayesian data analysis: A tutorial with R and BUGS. Academic Press.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press."
  },
  {
    "objectID": "Preface.html#history-and-license",
    "href": "Preface.html#history-and-license",
    "title": "Preface",
    "section": "History and License",
    "text": "History and License\nThis book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "Preface.html#preface-to-version-0.75",
    "href": "Preface.html#preface-to-version-0.75",
    "title": "前言",
    "section": "Preface to Version 0.75",
    "text": "Preface to Version 0.75\nIn this version we have updated the figures, images and text to maintain compatibility with latest versions of jamovi (2.2); many thanks to Peter Fisk for his help with this. Also tweaked and corrected are a few sections where improvements have been suggested by readers. This has mainly included fixing typos but also in places correcting conceptual detail, for example we have updated the information on kurtosis to reflect that it isn’t really about distribution “pointiness” and instead kurtosis is about whether data distributions have thin or fat tails. Thanks to all the readers who made suggestions, either through contacting me by email, or raising an issue on github.\nDavid Foxcroft\nFebruary 9th, 2022"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.70",
    "href": "Preface.html#preface-to-version-0.70",
    "title": "前言",
    "section": "Preface to Version 0.70",
    "text": "Preface to Version 0.70\nThis update from version 0.65 introduces some new analyses. In the ANOVA chapters we have added sections on repeated measures ANOVA and analysis of covariance (ANCOVA). In a new chapter we have introduced Factor Analysis and related techniques. Hopefully the style of this new material is consistent with the rest of the book, though eagle-eyed readers might spot a bit more of an emphasis on conceptual and practical explanations, and a bit less algebra. I’m not sure this is a good thing, and might add the algebra in a bit later. But it reflects both my approach to understanding and teaching statistics, and also some feedback I have received from students on a course I teach. In line with this, I have also been through the rest of the book and tried to separate out some of the algebra by putting it into a box or frame. It’s not that this stuff is not important or useful, but for some students they may wish to skip over it and therefore the boxing of these parts should help some readers.\nWith this version I am very grateful to comments and feedback received from my students and colleagues, notably Wakefield Morys-Carter, and also to numerous people all over the world who have sent in small suggestions and corrections - much appreciated, and keep them coming! One pretty neat new feature is that the example data files for the book can now be loaded into jamovi as an add-on module - thanks to Jonathon Love for helping with that.\nDavid Foxcroft\nFebruary 1st, 2019"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.65",
    "href": "Preface.html#preface-to-version-0.65",
    "title": "前言",
    "section": "Preface to Version 0.65",
    "text": "Preface to Version 0.65\nIn this adaptation of the excellent ‘Learning statistics with R’, by Danielle Navarro, we have replaced the statistical software used for the analyses and examples with jamovi. Although R is a powerful statistical programming language, it is not the first choice for every instructor and student at the beginning of their statistical learning. Some instructors and students tend to prefer the point-and-click style of software, and that’s where jamovi comes in. jamovi is software that aims to simplify two aspects of using R. It offers a point-and-click graphical user interface (GUI), and it also provides functions that combine the capabilities of many others, bringing a more SPSS- or SAS-like method of programming to R. Importantly, jamovi will always be free and open - that’s one of its core values - because jamovi is made by the scientific community, for the scientific community.\nWith this version I am very grateful for the help of others who have read through drafts and provided excellent suggestions and corrections, particularly Dr David Emery and Kirsty Walter.\nDavid Foxcroft\nJuly 1st, 2018"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.6",
    "href": "Preface.html#preface-to-version-0.6",
    "title": "前言",
    "section": "Preface to Version 0.6",
    "text": "Preface to Version 0.6\nThe book hasn’t changed much since 2015 when I released Version 0.5 – it’s probably fair to say that I’ve changed more than it has. I moved from Adelaide to Sydney in 2016 and my teaching profile at UNSW is different to what it was at Adelaide, and I haven’t really had a chance to work on it since arriving here! It’s a little strange looking back at this actually. A few quick comments…\n\nWeirdly, the book consistently misgenders me, but I suppose I have only myself to blame for that one :-) There’s now a brief footnote on page 12 that mentions this issue; in real life I’ve been working through a gender affirmation process for the last two years and mostly go by she/her pronouns. I am, however, just as lazy as I ever was so I haven’t bothered updating the text in the book.\nFor Version 0.6 I haven’t changed much I’ve made a few minor changes when people have pointed out typos or other errors. In particular it’s worth noting the issue associated with the etaSquared function in the lsr package (which isn’t really being maintained any more) in Section 14.4. The function works fine for the simple examples in the book, but there are definitely bugs in there that I haven’t found time to check! So please take care with that one.\nThe biggest change really is the licensing! I’ve released it under a Creative Commons licence (CC BY-SA 4.0, specifically), and placed all the source files to the associated GitHub repository, if anyone wants to adapt it.\n\nMaybe someone would like to write a version that makes use of the tidyverse… I hear that’s become rather important to R these days :-)\nBest,\nDanielle Navarro"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.5",
    "href": "Preface.html#preface-to-version-0.5",
    "title": "前言",
    "section": "Preface to Version 0.5",
    "text": "Preface to Version 0.5\nAnother year, another update. This time around, the update has focused almost entirely on the theory sections of the book. Chapters 9, 10 and 11 have been rewritten, hopefully for the better. Along the same lines, Chapter 17 is entirely new, and focuses on Bayesian statistics. I think the changes have improved the book a great deal. I’ve always felt uncomfortable about the fact that all the inferential statistics in the book are presented from an orthodox perspective, even though I almost always present Bayesian data analyses in my own work. Now that I’ve managed to squeeze Bayesian methods into the book somewhere, I’m starting to feel better about the book as a whole. I wanted to get a few other things done in this update, but as usual I’m running into teaching deadlines, so the update has to go out the way it is!\nDanielle Navarro\nFebruary 16, 2015"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.4",
    "href": "Preface.html#preface-to-version-0.4",
    "title": "前言",
    "section": "Preface to Version 0.4",
    "text": "Preface to Version 0.4\nA year has gone by since I wrote the last preface. The book has changed in a few important ways: Chapters 3 and 4 do a better job of documenting some of the time saving features of Rstudio, Chapters 12 and 13 now make use of new functions in the lsr package for running chi-square tests and t tests, and the discussion of correlations has been adapted to refer to the new functions in the lsr package. The soft copy of 0.4 now has better internal referencing (i.e., actual hyperlinks between sections), though that was introduced in 0.3.1. There’s a few tweaks here and there, and many typo corrections (thank you to everyone who pointed out typos!), but overall 0.4 isn’t massively different from 0.3.\nI wish I’d had more time over the last 12 months to add more content. The absence of any discussion of repeated measures ANOVA and mixed models more generally really does annoy me. My excuse for this lack of progress is that my second child was born at the start of 2013, and so I spent most of last year just trying to keep my head above water. As a consequence, unpaid side projects like this book got sidelined in favour of things that actually pay my salary! Things are a little calmer now, so with any luck version 0.5 will be a bigger step forward.\nOne thing that has surprised me is the number of downloads the book gets. I finally got some basic tracking information from the website a couple of months ago, and (after excluding obvious robots) the book has been averaging about 90 downloads per day. That’s encouraging: there’s at least a few people who find the book useful!\nDanielle Navarro\nFebruary 4, 2014"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.3",
    "href": "Preface.html#preface-to-version-0.3",
    "title": "前言",
    "section": "Preface to Version 0.3",
    "text": "Preface to Version 0.3\nThere’s a part of me that really doesn’t want to publish this book. It’s not finished.\nAnd when I say that, I mean it. The referencing is spotty at best, the chapter summaries are just lists of section titles, there’s no index, there are no exercises for the reader, the organisation is suboptimal, and the coverage of topics is just not comprehensive enough for my liking. Additionally, there are sections with content that I’m not happy with, figures that really need to be redrawn, and I’ve had almost no time to hunt down inconsistencies, typos, or errors. In other words, this book is not finished. If I didn’t have a looming teaching deadline and a baby due in a few weeks, I really wouldn’t be making this available at all.\nWhat this means is that if you are an academic looking for teaching materials, a Ph.D. student looking to learn R, or just a member of the general public interested in statistics, I would advise you to be cautious. What you’re looking at is a first draft, and it may not serve your purposes. If we were living in the days when publishing was expensive and the internet wasn’t around, I would never consider releasing a book in this form. The thought of someone shelling out $80 for this (which is what a commercial publisher told me it would retail for when they offered to distribute it) makes me feel more than a little uncomfortable. However, it’s the 21st century, so I can post the pdf on my website for free, and I can distribute hard copies via a print-on-demand service for less than half what a textbook publisher would charge. And so my guilt is assuaged, and I’m willing to share! With that in mind, you can obtain free soft copies and cheap hard copies online, from the following webpages:\nSoft copy: www.compcogscisydney.com/learning-statistics-with-r.html\nHard copy: www.lulu.com/content/13570633\n[Ed: these links are defunct, try this instead: learningstatisticswithr.com]\nEven so, the warning still stands: what you are looking at is Version 0.3 of a work in progress. If and when it hits Version 1.0, I would be willing to stand behind the work and say, yes, this is a textbook that I would encourage other people to use. At that point, I’ll probably start shamelessly flogging the thing on the internet and generally acting like a tool. But until that day comes, I’d like it to be made clear that I’m really ambivalent about the work as it stands.\nAll of the above being said, there is one group of people that I can enthusiastically endorse this book to: the psychology students taking our undergraduate research methods classes (DRIP and DRIP:A) in 2013. For you, this book is ideal, because it was written to accompany your stats lectures. If a problem arises due to a shortcoming of these notes, I can and will adapt content on the fly to fix that problem. Effectively, you’ve got a textbook written specifically for your classes, distributed for free (electronic copy) or at near-cost prices (hard copy). Better yet, the notes have been tested: Version 0.1 of these notes was used in the 2011 class, Version 0.2 was used in the 2012 class, and now you’re looking at the new and improved Version 0.3. I’m not saying these notes are titanium plated awesomeness on a stick – though if you wanted to say so on the student evaluation forms, then you’re totally welcome to – because they’re not. But I am saying that they’ve been tried out in previous years and they seem to work okay. Besides, there’s a group of us around to troubleshoot if any problems come up, and you can guarantee that at least one of your lecturers has read the whole thing cover to cover!\nOkay, with all that out of the way, I should say something about what the book aims to be. At its core, it is an introductory statistics textbook pitched primarily at psychology students. As such, it covers the standard topics that you’d expect of such a book: study design, descriptive statistics, the theory of hypothesis testing, t-tests, χ 2 tests, ANOVA and regression. However, there are also several chapters devoted to the R statistical package, including a chapter on data manipulation and another one on scripts and programming. Moreover, when you look at the content presented in the book, you’ll notice a lot of topics that are traditionally swept under the carpet when teaching statistics to psychology students. The Bayesian/frequentist divide is openly disussed in the probability chapter, and the disagreement between Neyman and Fisher about hypothesis testing makes an appearance. The difference between probability and density is discussed. A detailed treatment of Type I, II and III sums of squares for unbalanced factorial ANOVA is provided. And if you have a look in the Epilogue, it should be clear that my intention is to add a lot more advanced content.\nMy reasons for pursuing this approach are pretty simple: the students can handle it, and they even seem to enjoy it. Over the last few years I’ve been pleasantly surprised at just how little difficulty I’ve had in getting undergraduate psych students to learn R. It’s certainly not easy for them, and I’ve found I need to be a little charitable in setting marking standards, but they do eventually get there. Similarly, they don’t seem to have a lot of problems tolerating ambiguity and complexity in presentation of statistical ideas, as long as they are assured that the assessment standards will be set in a fashion that is appropriate for them. So if the students can handle it, why not teach it? The potential gains are pretty enticing. If they learn R, the students get access to CRAN, which is perhaps the largest and most comprehensive library of statistical tools in existence. And if they learn about probability theory in detail, it’s easier for them to switch from orthodox null hypothesis testing to Bayesian methods if they want to. Better yet, they learn data analysis skills that they can take to an employer without being dependent on expensive and proprietary software.\nSadly, this book isn’t the silver bullet that makes all this possible. It’s a work in progress, and maybe when it is finished it will be a useful tool. One among many, I would think. There are a number of other books that try to provide a basic introduction to statistics using R, and I’m not arrogant enough to believe that mine is better. Still, I rather like the book, and maybe other people will find it useful, incomplete though it is.\nDanielle Navarro\nJanuary 13, 2013"
  },
  {
    "objectID": "Preface.html#沿革與版權說明",
    "href": "Preface.html#沿革與版權說明",
    "title": "前言",
    "section": "沿革與版權說明",
    "text": "沿革與版權說明\nThis book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#on-the-psychology-of-statistics",
    "href": "01-Why-do-we-learn-statistics.html#on-the-psychology-of-statistics",
    "title": "1  Why do we learn statistics",
    "section": "1.1 On the psychology of statistics",
    "text": "1.1 On the psychology of statistics\nTo the surprise of many students, statistics is a fairly significant part of a psychological education. To the surprise of no-one, statistics is very rarely the favourite part of one’s psychological education. After all, if you really loved the idea of doing statistics, you’d probably be enrolled in a statistics class right now, not a psychology class. So, not surprisingly, there’s a pretty large proportion of the student base that isn’t happy about the fact that psychology has so much statistics in it. In view of this, I thought that the right place to start might be to answer some of the more common questions that people have about stats.\nA big part of this issue at hand relates to the very idea of statistics. What is it? What’s it there for? And why are scientists so bloody obsessed with it? These are all good questions, when you think about it. So let’s start with the last one. As a group, scientists seem to be bizarrely fixated on running statistical tests on everything. In fact, we use statistics so often that we sometimes forget to explain to people why we do. It’s a kind of article of faith among scientists – and especially social scientists – that your findings can’t be trusted until you’ve done some stats. Undergraduate students might be forgiven for thinking that we’re all completely mad, because no-one takes the time to answer one very simple question:\nWhy do you do statistics? Why don’t scientists just use common sense?\nIt’s a naive question in some ways, but most good questions are. There’s a lot of good answers to it,2 but for my money, the best answer is a really simple one: we don’t trust ourselves enough. We worry that we’re human, and susceptible to all of the biases, temptations and frailties that humans suffer from. Much of statistics is basically a safeguard. Using “common sense” to evaluate evidence means trusting gut instincts, relying on verbal arguments and on using the raw power of human reason to come up with the right answer. Most scientists don’t think this approach is likely to work.\nIn fact, come to think of it, this sounds a lot like a psychological question to me, and since I do work in a psychology department, it seems like a good idea to dig a little deeper here. Is it really plausible to think that this “common sense” approach is very trustworthy? Verbal arguments have to be constructed in language, and all languages have biases – some things are harder to say than others, and not necessarily because they’re false (e.g., quantum electrodynamics is a good theory, but hard to explain in words). The instincts of our “gut” aren’t designed to solve scientific problems, they’re designed to handle day to day inferences – and given that biological evolution is slower than cultural change, we should say that they’re designed to solve the day to day problems for a different world than the one we live in. Most fundamentally, reasoning sensibly requires people to engage in “induction”, making wise guesses and going beyond the immediate evidence of the senses to make generalisations about the world. If you think that you can do that without being influenced by various distractors, well, I have a bridge in London I’d like to sell you. Heck, as the next section shows, we can’t even solve “deductive” problems (ones where no guessing is required) without being influenced by our pre-existing biases.\n\n1.1.1 The curse of belief bias\nPeople are mostly pretty smart. We’re certainly smarter than the other species that we share the planet with (though many people might disagree). Our minds are quite amazing things, and we seem to be capable of the most incredible feats of thought and reason. That doesn’t make us perfect though. And among the many things that psychologists have shown over the years is that we really do find it hard to be neutral, to evaluate evidence impartially and without being swayed by pre-existing biases. A good example of this is the belief bias effect in logical reasoning: if you ask people to decide whether a particular argument is logically valid (i.e., conclusion would be true if the premises were true), we tend to be influenced by the believability of the conclusion, even when we shouldn’t. For instance, here’s a valid argument where the conclusion is believable:\n\nAll cigarettes are expensive (Premise 1)\nSome addictive things are inexpensive (Premise 2)\nTherefore, some addictive things are not cigarettes (Conclusion)\n\nAnd here’s a valid argument where the conclusion is not believable:\n\nAll addictive things are expensive (Premise 1)\nSome cigarettes are inexpensive (Premise 2)\nTherefore, some cigarettes are not addictive (Conclusion)\n\nThe logical structure of argument #2 is identical to the structure of argument #1, and they’re both valid. However, in the second argument, there are good reasons to think that premise 1 is incorrect, and as a result it’s probably the case that the conclusion is also incorrect. But that’s entirely irrelevant to the topic at hand; an argument is deductively valid if the conclusion is a logical consequence of the premises. That is, a valid argument doesn’t have to involve true statements.\nOn the other hand, here’s an invalid argument that has a believable conclusion:\n\nAll addictive things are expensive (Premise 1)\nSome cigarettes are inexpensive (Premise 2)\nTherefore, some addictive things are not cigarettes (Conclusion)\n\nAnd finally, an invalid argument with an unbelievable conclusion:\n\nAll cigarettes are expensive (Premise 1)\nSome addictive things are inexpensive (Premise 2)\nTherefore, some cigarettes are not addictive (Conclusion)\n\nNow, suppose that people really are perfectly able to set aside their pre-existing biases about what is true and what isn’t, and purely evaluate an argument on its logical merits. We’d expect 100% of people to say that the valid arguments are valid, and 0% of people to say that the invalid arguments are valid. So if you ran an experiment looking at this, you’d expect to see data as in Table 1.1.\n\n\n\n\nTable 1.1:  Validity of arguments \n\nconclusion feels trueconclusion feels false\n\nargument is valid100\\(\\%\\) say \"valid\"100\\(\\%\\) say \"valid\"\n\nargument is invalid0\\(\\%\\) say \"valid\"0\\(\\%\\) say \"valid\"\n\n\n\n\n\nIf the psychological data looked like this (or even a good approximation to this), we might feel safe in just trusting our gut instincts. That is, it’d be perfectly okay just to let scientists evaluate data based on their common sense, and not bother with all this murky statistics stuff. However, you guys have taken psych classes, and by now you probably know where this is going.\nIn a classic study, Evans et al. (1983) ran an experiment looking at exactly this. What they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of the data, everything went the way you’d hope (Table 1.2).\n\n\n\n\nTable 1.2:  Pre-existing biases and argument validity \n\nconclusion feels trueconclusion feels false\n\nargument is valid92\\(\\%\\) say \"valid\"\n\nargument is invalid8\\(\\%\\) say \"valid\"\n\n\n\n\n\nNot perfect, but that’s pretty good. But look what happens when our intuitive feelings about the truth of the conclusion run against the logical structure of the argument (Table 1.3):\n\n\n\n\nTable 1.3:  Intuition and argument validity \n\nconclusion feels trueconclusion feels false\n\nargument is valid92\\(\\%\\) say \"valid\"46\\(\\%\\) say \"valid\"\n\nargument is invalid92\\(\\%\\) say \"valid\"8\\(\\%\\) say \"valid\"\n\n\n\n\n\nOh dear, that’s not as good. Apparently, when people are presented with a strong argument that contradicts our pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument (people only did so 46% of the time). Even worse, when people are presented with a weak argument that agrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that one wrong 92% of the time!).3\nIf you think about it, it’s not as if these data are horribly damning. Overall, people did do better than chance at compensating for their prior biases, since about 60% of people’s judgements were correct (you’d expect 50% by chance). Even so, if you were a professional “evaluator of evidence”, and someone came along and offered you a magic tool that improves your chances of making the right decision from 60% to (say) 95%, you’d probably jump at it, right? Of course you would. Thankfully, we actually do have a tool that can do this. But it’s not magic, it’s statistics. So that’s reason #1 why scientists love statistics. It’s just too easy for us to “believe what we want to believe”. So instead, if we want to “believe in the data”, we’re going to need a bit of help to keep our personal biases under control. That’s what statistics does, it helps keep us honest."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox",
    "href": "01-Why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox",
    "title": "1  Why do we learn statistics",
    "section": "1.2 The cautionary tale of Simpson’s paradox",
    "text": "1.2 The cautionary tale of Simpson’s paradox\nThe following is a true story (I think!). In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions (Table 1.4).\n\n\n\n\nTable 1.4:  Berkeley students by gender \n\nNumber of applicantsPercent admitted\n\nMales844244\\(\\%\\)\n\nFemales432135\\(\\%\\)\n\n\n\n\n\nGiven this, they were worried about being sued!4 Given that there were nearly 13,000 applicants, a difference of 9% in admission rates between males and females is just way too big to be a coincidence. Pretty compelling data, right? And if I were to say to you that these data actually reflect a weak bias in favour of women (sort of!), you’d probably think that I was either crazy or sexist.\nOddly, it’s actually sort of true. When people started looking more carefully at the admissions data they told a rather different story (Bickel et al., 1975). Specifically, when they looked at it on a department by department basis, it turned out that most of the departments actually had a slightly higher success rate for female applicants than for male applicants. Table 1.5 shows the admission figures for the six largest departments (with the names of the departments removed for privacy reasons):\n\n\n\n\nTable 1.5:  Berkeley students by gender for six largest Departments \n\nMales\n\nDepartmentApplicantsPercent admittedApplicantsPercent admitted\n\nA82562\\(\\%\\)10882\\(\\%\\)\n\nB56063\\(\\%\\)2568\\(\\%\\)\n\nC32537\\(\\%\\)59334\\(\\%\\)\n\nD41733\\(\\%\\)37535\\(\\%\\)\n\nE19128\\(\\%\\)39324\\(\\%\\)\n\nF2726\\(\\%\\)3417\\(\\%\\)\n\n\n\n\n\nRemarkably, most departments had a higher rate of admissions for females than for males! Yet the overall rate of admission across the university for females was lower than for males. How can this be? How can both of these statements be true at the same time?\nHere’s what’s going on. Firstly, notice that the departments are not equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get A>B>D>C>F>E (the “easy” departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C>E>D>F>A>B. In other words, what these data seem to be suggesting is that the female applicants tended to apply to “harder” departments. And in fact, if we look at Figure Figure 1.1 we see that this trend is systematic, and quite striking. This effect is known as Simpson’s paradox. It’s not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it’s real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are lots of subtle, counter-intuitive traps lying in wait for the unwary. That’s reason #2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.\nBefore leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves part of the problem. Remember that we started all this with the concern that Berkeley’s admissions processes might be unfairly biased against female applicants. When we looked at the “aggregated” data, it did seem like the university was discriminating against women, but when we “disaggregate” and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments. From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can’t dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce.\n\n\n\n\n\nFigure 1.1: The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from Bickel et al. (1975). Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot departments with fewer than 40 applicants\n\n\n\n\nThat was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.That was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.\nIn short there are a lot of critical questions that you can’t answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a tool to help you learn about your data. No more and no less. It’s a powerful tool to that end, but there’s no substitute for careful thought."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#statistics-in-psychology",
    "href": "01-Why-do-we-learn-statistics.html#statistics-in-psychology",
    "title": "1  Why do we learn statistics",
    "section": "1.3 Statistics in psychology",
    "text": "1.3 Statistics in psychology\nI hope that the discussion above helped explain why science in general is so focused on statistics. But I’m guessing that you have a lot more questions about what role statistics plays in psychology, and specifically why psychology classes always devote so many lectures to stats. So here’s my attempt to answer a few of them…\nWhy does psychology have so much statistics?\nTo be perfectly honest, there’s a few different reasons, some of which are better than others. The most important reason is that psychology is a statistical science. What I mean by that is that the “things” that we study are people. Real, complicated, gloriously messy, infuriatingly perverse people. The “things” of physics include objects like electrons, and while there are all sorts of complexities that arise in physics, electrons don’t have minds of their own. They don’t have opinions, they don’t differ from each other in weird and arbitrary ways, they don’t get bored in the middle of an experiment, and they don’t get angry at the experimenter and then deliberately try to sabotage the data set (not that I’ve ever done that!). At a fundamental level psychology is harder than physics.5 Basically, we teach statistics to you as psychologists because you need to be better at stats than physicists. There’s actually a saying used sometimes in physics, to the effect that “if your experiment needs statistics, you should have done a better experiment”. They have the luxury of being able to say that because their objects of study are pathetically simple in comparison to the vast mess that confronts social scientists. And it’s not just psychology. Most social sciences are desperately reliant on statistics. Not because we’re bad experimenters, but because we’ve picked a harder problem to solve. We teach you stats because you really, really need it.\nCan’t someone else do the statistics?\nTo some extent, but not completely. It’s true that you don’t need to become a fully trained statistician just to do psychology, but you do need to reach a certain level of statistical competence. In my view, there’s three reasons that every psychological researcher ought to be able to do basic statistics:\n\nFirstly, there’s the fundamental reason: statistics is deeply intertwined with research design. If you want to be good at designing psychological studies, you need to at the very least understand the basics of stats.\nSecondly, if you want to be good at the psychological side of the research, then you need to be able to understand the psychological literature, right? But almost every paper in the psychological literature reports the results of statistical analyses. So if you really want to understand the psychology, you need to be able to understand what other people did with their data. And that means understanding a certain amount of statistics.\nThirdly, there’s a big practical problem with being dependent on other people to do all your statistics: statistical analysis is expensive. If you ever get bored and want to look up how much the Australian government charges for university fees, you’ll notice something interesting: statistics is designated as a “national priority” category, and so the fees are much, much lower than for any other area of study. This is because there’s a massive shortage of statisticians out there. So, from your perspective as a psychological researcher, the laws of supply and demand aren’t exactly on your side here! As a result, in almost any real life situation where you want to do psychological research, the cruel facts will be that you don’t have enough money to afford a statistician. So the economics of the situation mean that you have to be pretty self-sufficient.\n\nNote that a lot of these reasons generalise beyond researchers. If you want to be a practicing psychologist and stay on top of the field, it helps to be able to read the scientific literature, which relies pretty heavily on statistics.\nI don’t care about jobs, research, or clinical work. Do I need statistics?\nOkay, now you’re just messing with me. Still, I think it should matter to you too. Statistics should matter to you in the same way that statistics should matter to everyone. We live in the 21st century, and data are everywhere. Frankly, given the world in which we live these days, a basic knowledge of statistics is pretty damn close to a survival tool! Which is the topic of the next section."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#statistics-in-everyday-life",
    "href": "01-Why-do-we-learn-statistics.html#statistics-in-everyday-life",
    "title": "1  Why do we learn statistics",
    "section": "1.4 Statistics in everyday life",
    "text": "1.4 Statistics in everyday life\n\n“We are drowning in information,\nbut we are starved for knowledge”\n- Various authors, original probably John Naisbitt\n\nWhen I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC news website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that I would call a statistical topic and 6 of those made a mistake. The most common error, if you’re curious, was failing to report baseline data (e.g., the article mentions that 5% of people in situation X have some characteristic Y, but doesn’t say how common the characteristic is for everyone else!). The point I’m trying to make here isn’t that journalists are bad at statistics (though they almost always are), it’s that a basic knowledge of statistics is very helpful for trying to figure out when someone else is either making a mistake or even lying to you. In fact, one of the biggest things that a knowledge of statistics does to you is cause you to get angry at the newspaper or the internet on a far more frequent basis. You can find a good example of this in Section 4.1.5 in Chapter 4. In later versions of this book I’ll try to include more anecdotes along those lines."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics",
    "href": "01-Why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics",
    "title": "1  Why do we learn statistics",
    "section": "1.5 There’s more to research methods than statistics",
    "text": "1.5 There’s more to research methods than statistics\nSo far, most of what I’ve talked about is statistics, and so you’d be forgiven for thinking that statistics is all I care about in life. To be fair, you wouldn’t be far wrong, but research methodology is a broader concept than statistics. So most research methods courses will cover a lot of topics that relate much more to the pragmatics of research design, and in particular the issues that you encounter when trying to do research with humans. However, about 99% of student fears relate to the statistics part of the course, so I’ve focused on the stats in this discussion, and hopefully I’ve convinced you that statistics matters, and more importantly, that it’s not to be feared. That being said, it’s pretty typical for introductory research methods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite the contrary, in fact. Introductory classes focus a lot on the statistics because you almost always find yourself needing statistics before you need the other research methods training. Why? Because almost all of your assignments in other classes will rely on statistical training, to a much greater extent than they rely on other methodological tools. It’s not common for undergraduate assignments to require you to design your own study from the ground up (in which case you would need to know a lot about research design), but it is common for assignments to ask you to analyse and interpret data that were collected in a study that someone else designed (in which case you need statistics). In that sense, from the perspective of allowing you to do well in all your other classes, the statistics is more urgent.\nBut note that “urgent” is different from “important” – they both matter. I really do want to stress that research design is just as important as data analysis, and this book does spend a fair amount of time on it. However, while statistics has a kind of universality, and provides a set of core tools that are useful for most types of psychological research, the research methods side isn’t quite so universal. There are some general principles that everyone should think about, but a lot of research design is very idiosyncratic, and is specific to the area of research that you want to engage in. To the extent that it’s the details that matter, those details don’t usually show up in an introductory stats and research methods class.\n\n\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187, 398–404.\n\n\nEvans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On the conflict between logic and belief in syllogistic reasoning. Memory and Cognition, 11, 295–306."
  },
  {
    "objectID": "Epilogue.html#the-undiscovered-statistics",
    "href": "Epilogue.html#the-undiscovered-statistics",
    "title": "後記",
    "section": "The undiscovered statistics",
    "text": "The undiscovered statistics\nFirst, I’m going to talk a bit about some of the content that I wish I’d had the chance to cram into this version of the book, just so that you can get a sense of what other ideas are out there in the world of statistics. I think this would be important even if this book were getting close to a final product. One thing that students often fail to realise is that their introductory statistics classes are just that, an introduction. If you want to go out into the wider world and do real data analysis, you have to learn a whole lot of new tools that extend the content of your undergraduate lectures in all sorts of different ways. Don’t assume that something can’t be done just because it wasn’t covered in undergrad. Don’t assume that something is the right thing to do just because it was covered in an undergrad class. To stop you from falling victim to that trap, I think it’s useful to give a bit of an overview of some of the other ideas out there\n\nOmissions within the topics covered\nEven within the topics that I have covered in the book, there are a lot of omissions that I’d like to redress in future version of the book. Just sticking to things that are purely about statistics (rather than things associated with jamovi), the following is a representative but not exhaustive list of topics that I’d like to expand on at some time:\n\nOther types of correlations. In Chapter 10 I talked about two types of correlation: Pearson and Spearman. Both of these methods of assessing correlation are applicable to the case where you have two continuous variables and want to assess the relationship between them. What about the case where your variables are both nominal scale? Or when one is nominal scale and the other is continuous? There are actually methods for computing correlations in such cases (e.g., polychoric correlation), and it would be good to see these included.\nMore detail on effect sizes. In general, I think the treatment of effect sizes throughout the book is a little more cursory than it should be. In almost every instance, I’ve tended just to pick one measure of effect size (usually the most popular one) and describe that. However, for almost all tests and models there are multiple ways of thinking about effect size, and I’d like to go into more detail in the future.\nDealing with violated assumptions. In a number of places in the book I’ve talked about some things you can do when you find that the assumptions of your test (or model) are violated, but I think that I ought to say more about this. In particular, I think it would have been nice to talk in a lot more detail about how you can tranform variables to fix problems. I talked a bit about this in Chapter 6, but the discussion isn’t detailed enough I think.\nInteraction terms for regression. In Chapter 13 I talked about the fact that you can have interaction terms in an ANOVA, and I also pointed out that ANOVA can be interpreted as a kind of linear regression model. Yet, when talking about regression in Chapter 10 I made no mention of interactions at all. However, there’s nothing stopping you from including interaction terms in a regression model. It’s just a little more complicated to figure out what an “interaction” actually means when you’re talking about the interaction between two continuous predictors, and it can be done in more than one way. Even so, I would have liked to talk a little about this.\nMethod of planned comparison. As I mentioned this in Chapter 13, it’s not always appropriate to be using a post hoc correction like Tukey’s HSD when doing an ANOVA, especially when you had a very clear (and limited) set of comparisons that you cared about ahead of time. I would like to talk more about this in the future.\nMultiple comparison methods. Even within the context of talking about post hoc tests and multiple comparisons, I would have liked to talk about the methods in more detail, and talk about what other methods exist besides the few options I mentioned.\n\n\n\nStatistical models missing from the book\nStatistics is a huge field. The core tools that I’ve described in this book (chi-square tests, t-tests, regression and ANOVA) are basic tools that are widely used in everyday data analysis, and they form the core of most introductory stats books. However, there are a lot of other tools out there. There are so very many data analysis situations that these tools don’t cover, and it would be great to give you a sense of just how much more there is, for example:\n\nNonlinear regression. When discussing regression in Chapter 10, we saw that regression assumes that the relationship between predictors and outcomes is linear. On the other hand, when we talked about the simpler problem of correlation in Chapter 4, we saw that there exist tools (e.g., Spearman correlations) that are able to assess non-linear relationships between variables. There are a number of tools in statistics that can be used to do non-linear regression. For instance, some non-linear regression models assume that the relationship between predictors and outcomes is monotonic (e.g., isotonic regression), while others assume that it is smooth but not necessarily monotonic (e.g., Lowess regression), while others assume that the relationship is of a known form that happens to be nonlinear (e.g., polynomial regression).\nLogistic regression. Yet another variation on regression occurs when the outcome variable is binary, but the predictors are continuous. For instance, suppose you’re investigating social media, and you want to know if it’s possible to predict whether or not someone is on Twitter as a function of their income, their age, and a range of other variables. This is basically a regression model, but you can’t use regular linear regression because the outcome variable is binary (you’re either on Twitter or you’re not). Because the outcome variable is binary, there’s no way that the residuals could possibly be normally distributed. There are a number of tools that statisticians can apply to this situation, the most prominent of which is logistic regression.\nThe General Linear Model (GLM). The GLM is actually a family of models that includes logistic regression, linear regression, (some) nonlinear regression, ANOVA and many others. The basic idea in the GLM is essentially the same idea that underpins linear models, but it allows for the idea that your data might not be normally distributed, and allows for nonlinear relationships between predictors and outcomes. There are a lot of very handy analyses that you can run that fall within the GLM, so it’s a very useful thing to know about.\nSurvival analysis. In Chapter 2 I talked about “differential attrition”, the tendency for people to leave the study in a non-random fashion. Back then, I was talking about it as a potential methodological concern, but there are a lot of situations in which differential attrition is actually the thing you’re interested in. Suppose, for instance, you’re interested in finding out how long people play different kinds of computer games in a single session. Do people tend to play RTS (real time strategy) games for longer stretches than FPS (first person shooter) games? You might design your study like this. People come into the lab, and they can play for as long or as little as they like. Once they’re finished, you record the time they spent playing. However, due to ethical restrictions, let’s suppose that you cannot allow them to keep playing longer than two hours. A lot of people will stop playing before the two hour limit, so you know exactly how long they played. But some people will run into the two hour limit, and so you don’t know how long they would have kept playing if you’d been able to continue the study. As a consequence, your data are systematically censored: you’re missing all of the very long times. How do you analyse this data sensibly? This is the problem that survival analysis solves. It is specifically designed to handle this situation, where you’re systematically missing one “side” of the data because the study ended. It’s very widely used in health research, and in that context it is often literally used to analyse survival. For instance, you may be tracking people with a particular type of cancer, some who have received treatment A and others who have received treatment B, but you only have funding to track them for 5 years. At the end of the study period some people are alive, others are not. In this context, survival analysis is useful for determining which treatment is more effective, and telling you about the risk of death that people face over time.\nMixed models. Repeated measures ANOVA is often used in situations where you have observations clustered within experimental units. A good example of this is when you track individual people across multiple time points. Let’s say you’re tracking happiness over time, for two people. Aaron’s happiness starts at 10, then drops to 8, and then to 6. Belinda’s happiness starts at 6, then rises to 8 and then to 10. Both of these two people have the same “overall” level of happiness (the average across the three time points is 8), so a repeated measures ANOVA analysis would treat Aaron and Belinda the same way. But that’s clearly wrong. Aaron’s happiness is decreasing, whereas Belinda’s is increasing. If you want to optimally analyse data from an experiment where people can change over time, then you need a more powerful tool than repeated measures ANOVA. The tools that people use to solve this problem are called “mixed” models, because they are designed to learn about individual experimental units (e.g. happiness of individual people over time) as well as overall effects (e.g. the effect of money on happiness over time). Repeated measures ANOVA is perhaps the simplest example of a mixed model, but there’s a lot you can do with mixed models that you can’t do with repeated measures ANOVA.\nMultidimensional scaling. Factor analysis is an example of an “unsupervised learning” model. What this means is that, unlike most of the “supervised learning” tools I’ve mentioned, you can’t divide up your variables into predictors and outcomes. Regression is supervised learning whereas factor analysis is unsupervised learning. It’s not the only type of unsupervised learning model however. For example, in factor analysis one is concerned with the analysis of correlations between variables. However, there are many situations where you’re actually interested in analysing similarities or dissimilarities between objects, items or people. There are a number of tools that you can use in this situation, the best known of which is multidimensional scaling (MDS). In MDS, the idea is to find a “geometric” representation of your items. Each item is “plotted” as a point in some space, and the distance between two points is a measure of how dissimilar those items are.\nClustering. Another example of an unsupervised learning model is clustering (also referred to as classification), in which you want to organise all of your items into meaningful groups, such that similar items are assigned to the same groups. A lot of clustering is unsupervised, meaning that you don’t know anything about what the groups are, you just have to guess. There are other “supervised clustering” situations where you need to predict group memberships on the basis of other variables, and those group memberships are actually observables. Logistic regression is a good example of a tool that works this way. However, when you don’t actually know the group memberships, you have to use different tools (e.g., k-means clustering). There are even situations where you want to do something called “semi-supervised clustering”, in which you know the group memberships for some items but not others. As you can probably guess, clustering is a pretty big topic, and a pretty useful thing to know about.\nCausal models. One thing that I haven’t talked about much in this book is how you can use statistical modelling to learn about the causal relationships between variables. For instance, consider the following three variables which might be of interest when thinking about how someone died in a firing squad. We might want to measure whether or not an execution order was given (variable A), whether or not a marksman fired their gun (variable B), and whether or not the person got hit with a bullet (variable C). These three variables are all correlated with one another (e.g., there is a correlation between guns being fired and people getting hit with bullets), but we actually want to make stronger statements about them than merely talking about correlations. We want to talk about causation. We want to be able to say that the execution order (A) causes the marksman to fire (B) which causes someone to get shot (C). We can express this by a directed arrow notation: we write it as \\(A \\rightarrow B \\rightarrow C\\). This “causal chain” is a fundamentally different explanation for events than one in which the marksman fires first, which causes the shooting \\(B \\rightarrow C\\), and then causes the executioner to “retroactively” issue the execution order, \\(B \\rightarrow A\\). This “common effect” model says that A and C are both caused by B. You can see why these are different. In the first causal model, if we had managed to stop the executioner from issuing the order (intervening to change A), then no shooting would have happened. In the second model, the shooting would have happened any way because the marksman was not following the execution order. There is a big literature in statistics on trying to understand the causal relationships between variables, and a number of different tools exist to help you test different causal stories about your data. The most widely used of these tools (in psychology at least) is structural equations modelling (SEM), and at some point I’d like to extend the book to talk about it.\n\nOf course, even this listing is incomplete. I haven’t mentioned time series analysis, item response theory, market basket analysis, classification and regression trees, or any of a huge range of other topics. However, the list that I’ve given above is essentially my wish list for this book. Sure, it would double the length of the book, but it would mean that the scope has become broad enough to cover most things that applied researchers in psychology would need to use.\n\n\nOther ways of doing inference\nA different sense in which this book is incomplete is that it focuses pretty heavily on a very narrow and old-fashioned view of how inferential statistics should be done. In Chapter 8 I talked a little bit about the idea of unbiased estimators, sampling distributions and so on. In Chapter 9 I talked about the theory of null hypothesis significance testing and p-values. These ideas have been around since the early 20th century, and the tools that I’ve talked about in the book rely very heavily on the theoretical ideas from that time. I’ve felt obligated to stick to those topics because the vast majority of data analysis in science is also reliant on those ideas. However, the theory of statistics is not restricted to those topics and, whilst everyone should know about them because of their practical importance, in many respects those ideas do not represent best practice for contemporary data analysis. One of the things that I’m especially happy with is that I’ve been able to go a little beyond this. Chapter 16 now presents the Bayesian perspective in a reasonable amount of detail, but the book overall is still pretty heavily weighted towards the frequentist orthodoxy. Additionally, there are a number of other approaches to inference that are worth mentioning:\n\nBootstrapping. Throughout the book, whenever I’ve introduced a hypothesis test, I’ve had a strong tendency just to make assertions like “the sampling distribution for BLAH is a t-distribution” or something like that. In some cases, I’ve actually attempted to justify this assertion. For example, when talking about \\(\\chi^2\\) tests in Chapter 14 I made reference to the known relationship between normal distributions and \\(\\chi^2\\) distributions (see Chapter 7) to explain how we end up assuming that the sampling distribution of the goodness-of-fit statistic is \\(\\chi^2\\) . However, it’s also the case that a lot of these sampling distributions are, well, wrong. The \\(\\chi^2\\) test is a good example. It is based on an assumption about the distribution of your data, an assumption which is known to be wrong for small sample sizes! Back in the early 20th century, there wasn’t much you could do about this situation. Statisticians had developed mathematical results that said that “under assumptions BLAH about the data, the sampling distribution is approximately BLAH”, and that was about the best you could do. A lot of times they didn’t even have that. There are lots of data analysis situations for which no-one has found a mathematical solution for the sampling distributions that you need. And so up until the late 20th century, the corresponding tests didn’t exist or didn’t work. However, computers have changed all that now. There are lots of fancy tricks, and some not-so-fancy, that you can use to get around it. The simplest of these is bootstrapping, and in it’s simplest form it’s incredibly simple. What you do is simulate the results of your experiment lots and lots of times, under the twin assumptions that (a) the null hypothesis is true and (b) the unknown population distribution actually looks pretty similar to your raw data. In other words, instead of assuming that the data are (for instance) normally distributed, just assume that the population looks the same as your sample, and then use computers to simulate the sampling distribution for your test statistic if that assumption holds. Despite relying on a somewhat dubious assumption (i.e., the population distribution is the same as the sample!) bootstrapping is quick and easy method that works remarkably well in practice for lots of data analysis problems.\nCross validation. One question that pops up in my stats classes every now and then, usually by a student trying to be provocative, is “Why do we care about inferential statistics at all? Why not just describe your sample?” The answer to the question is usually something like this, “Because our true interest as scientists is not the specific sample that we have observed in the past, we want to make predictions about data we might observe in the future”. A lot of the issues in statistical inference arise because of the fact that we always expect the future to be similar to but a bit different from the past. Or, more generally, new data won’t be quite the same as old data. What we do, in a lot of situations, is try to derive mathematical rules that help us to draw the inferences that are most likely to be correct for new data, rather than to pick the statements that best describe old data. For instance, given two models A and B, and a data set \\(X\\) you collected today, try to pick the model that will best describe a new data set \\(Y\\) that you’re going to collect tomorrow. Sometimes it’s convenient to simulate the process, and that’s what cross-validation does. What you do is divide your data set into two subsets, \\(X1\\) and \\(X2\\). Use the subset \\(X1\\) to train the model (e.g., estimate regression coefficients, let’s say), but then assess the model performance on the other one \\(X2\\). This gives you a measure of how well the model generalises from an old data set to a new one, and is often a better measure of how good your model is than if you just fit it to the full data set \\(X\\).\nRobust statistics. Life is messy, and nothing really works the way it’s supposed to. This is just as true for statistics as it is for anything else, and when trying to analyse data we’re often stuck with all sorts of problems in which the data are just messier than they’re supposed to be. Variables that are supposed to be normally distributed are not actually normally distributed, relationships that are supposed to be linear are not actually linear, and some of the observations in your data set are almost certainly junk (i.e., not measuring what they’re supposed to). All of this messiness is ignored in most of the statistical theory I developed in this book. However, ignoring a problem doesn’t always solve it. Sometimes, it’s actually okay to ignore the mess, because some types of statistical tools are “robust”, i.e., if the data don’t satisfy your theoretical assumptions they nevertheless still work pretty well. Other types of statistical tools are not robust, and even minor deviations from the theoretical assumptions cause them to break. Robust statistics is a branch of stats concerned with this question, and they talk about things like the “breakdown point” of a statistic. That is, how messy does your data have to be before the statistic cannot be trusted? I touched on this in places. The mean is not a robust estimator of the central tendency of a variable, but the median is. For instance, suppose I told you that the ages of my five best friends are 34, 39, 31, 43 and 4003 years. How old do you think they are on average? That is, what is the true population mean here? If you use the sample mean as your estimator of the population mean, you get an answer of 830 years. If you use the sample median as the estimator of the population mean, you get an answer of 39 years. Notice that, even though you’re “technically” doing the wrong thing in the second case (using the median to estimate the mean!) you’re actually getting a better answer. The problem here is that one of the observations is clearly, obviously, a lie. I don’t have a friend aged 4003 years. It’s probably a typo, I probably meant to type 43. But what if I had typed 53 instead of 43, or 34 instead of 43? Could you be sure if this was a typo or not? Sometimes the errors in the data are subtle, so you can’t detect them just by eyeballing the sample, but they’re still errors that contaminate your data, and they still affect your conclusions. Robust statistics is concerned with how you can make safe inferences even when faced with contamination that you don’t know about. It’s pretty cool stuff.\n\n\n\nMiscellaneous topics\n\nSuppose you’re doing a survey, and you’re interested in exercise and weight. You send data to four people. Adam says he exercises a lot and is not overweight. Briony says she exercises a lot and is not overweight. Carol says she does not exercise and is overweight. Tim says he does not exercise and refuses to answer the question about his weight. Elaine does not return the survey. You now have a missing data problem. There is one entire survey missing, and one question missing from another one, What do you do about it? Ignoring missing data is not, in general, a safe thing to do. Let’s think about Tim’s survey here. Firstly, notice that, on the basis of his other responses, he appear to be more similar to Carol (neither of us exercise) than to Adam or Briony. So if you were forced to guess his weight, you’d guess that he is closer to her than to them. Maybe you’d make some correction for the fact that Adam and Tim are males and Briony and Carol are females. The statistical name for this kind of guessing is “imputation”. Doing imputation safely is hard, but it’s important, especially when the missing data are missing in a systematic way. Because of the fact that people who are overweight are often pressured to feel poorly about their weight (often thanks to public health campaigns), we actually have reason to suspect that the people who are not responding are more likely to be overweight than the people who do respond. Imputing a weight to Tim means that the number of overweight people in the sample will probably rise from 1 out of 3 (if we ignore Tim), to 2 out of 4 (if we impute Tim’s weight). Clearly this matters. But doing it sensibly is more complicated than it sounds. Earlier, I suggested you should treat Tim like Carol, since they gave the same answer to the exercise question. But that’s not quite right. There is a systematic difference between them. She answered the question, and Tim didn’t. Given the social pressures faced by overweight people, isn’t it likely that Tim is more overweight than Carol? And of course this is still ignoring the fact that it’s not sensible to impute a single weight to Tim, as if you actually knew his weight. Instead, what you need to do it is impute a range of plausible guesses (referred to as multiple imputation), in order to capture the fact that you’re more uncertain about Tim’s weight than you are about Carol’s. And let’s not get started on the problem posed by the fact that Elaine didn’t send in the survey. As you can probably guess, dealing with missing data is an increasingly important topic. In fact, I’ve been told that a lot of journals in some fields will not accept studies that have missing data unless some kind of sensible multiple imputation scheme is followed.\nPower analysis. In Chapter 9 I discussed the concept of power (i.e., how likely are you to be able to detect an effect if it actually exists) and referred to power analysis, a collection of tools that are useful for assessing how much power your study has. Power analysis can be useful for planning a study (e.g., figuring out how large a sample you’re likely to need), but it also serves a useful role in analysing data that you already collected. For instance, suppose you get a significant result, and you have an estimate of your effect size. You can use this information to estimate how much power your study actually had. This is kind of useful, especially if your effect size is not large. For instance, suppose you reject the null hypothesis at \\(p< .05\\), but you use power analysis to figure out that your estimated power was only .08. The significant result means that, if the null hypothesis was in fact true, there was a 5% chance of getting data like this. But the low power means that, even if the null hypothesis is false and the effect size was really as small as it looks, there was only an 8% chance of getting data like you did. This suggests that you need to be pretty cautious, because luck seems to have played a big part in your results, one way or the other!\nData analysis using theory-inspired models. In a few places in this book I’ve mentioned response time (RT) data, where you record how long it takes someone to do something (e.g., make a simple decision). I’ve mentioned that RT data are almost invariably non-normal, and positively skewed. Additionally, there’s a thing known as the speed / accuracy trade-off: if you try to make decisions too quickly (low RT) then you’re likely to make poorer decisions (lower accuracy). So if you measure both the accuracy of a participant’s decisions and their RT, you’ll probably find that speed and accuracy are related. There’s more to the story than this, of course, because some people make better decisions than others regardless of how fast they’re going. Moreover, speed depends on both cognitive processes (i.e., time spent thinking) but also physiological ones (e.g., how fast can you move your muscles). It’s starting to sound like analysing this data will be a complicated process. And indeed it is, but one of the things that you find when you dig into the psychological literature is that there already exist mathematical models (called “sequential sampling models”) that describe how people make simple decisions, and these models take into account a lot of the factors I mentioned above. You won’t find any of these theoretically-inspired models in a standard statistics textbook. Standard stats textbooks describe standard tools, tools that could meaningfully be applied in lots of different disciplines, not just psychology. ANOVA is an example of a standard tool that is just as applicable to psychology as to pharmacology. Sequential sampling models are not, they are psychology-specific, more or less. This doesn’t make them less powerful tools. In fact, if you’re analysing data where people have to make choices quickly you should really be using sequential sampling models to analyse the data. Using ANOVA or regression or whatever won’t work as well, because the theoretical assumptions that underpin them are not well-matched to your data. In contrast, sequential sampling models were explicitly designed to analyse this specific type of data, and their theoretical assumptions are extremely well-matched to the data."
  },
  {
    "objectID": "Epilogue.html#learning-the-basics-and-learning-them-in-jamovi",
    "href": "Epilogue.html#learning-the-basics-and-learning-them-in-jamovi",
    "title": "後記",
    "section": "Learning the basics, and learning them in jamovi",
    "text": "Learning the basics, and learning them in jamovi\nOkay, that was a long list. And even that listing is massively incomplete. There really are a lot of big ideas in statistics that I haven’t covered in this book. It can seem pretty depressing to finish an almost 500-page textbook only to be told that this is only the beginning, especially when you start to suspect that half of the stuff you’ve been taught is wrong. For instance, there are a lot of people in the field who would strongly argue against the use of the classical ANOVA model, yet I’ve devoted two whole chapters to it! Standard ANOVA can be attacked from a Bayesian perspective, or from a robust statistics perspective, or even from a “it’s just plain wrong” perspective (people very frequently use ANOVA when they should actually be using mixed models). So why learn it at all?\nAs I see it, there are two key arguments. Firstly, there’s the pure pragmatism argument. Rightly or wrongly, ANOVA is widely used. If you want to understand the scientific literature, you need to understand ANOVA. And secondly, there’s the “incremental knowledge” argument. In the same way that it was handy to have seen one-way ANOVA before trying to learn factorial ANOVA, understanding ANOVA is helpful for understanding more advanced tools, because a lot of those tools extend on or modify the basic ANOVA setup in some way. For instance, although mixed models are way more useful than ANOVA and regression, I’ve never heard of anyone learning how mixed models work without first having worked through ANOVA and regression. You have to learn to crawl before you can climb a mountain.\nActually, I want to push this point a bit further. One thing that I’ve done a lot of in this book is talk about fundamentals. I spent a lot of time on probability theory. I talked about the theory of estimation and hypothesis tests in more detail than I needed to. Why did I do all this? Looking back, you might ask whether I really needed to spend all that time talking about what a probability distribution is, or why there was even a section on probability density. If the goal of the book was to teach you how to run a t-test or an ANOVA, was all that really necessary? Was this all just a huge waste of everyone’s time???\nThe answer, I hope you’ll agree, is no. The goal of an introductory stats is not to teach ANOVA. It’s not to teach t-tests, or regressions, or histograms, or p-values. The goal is to start you on the path towards becoming a skilled data analyst. And in order for you to become a skilled data analyst, you need to be able to do more than ANOVA, more than t-tests, regressions and histograms. You need to be able to think properly about data. You need to be able to learn the more advanced statistical models that I talked about in the last section, and to understand the theory upon which they are based. And you need to have access to software that will let you use those advanced tools. And this is where, in my opinion at least, all that extra time I’ve spent on the fundamentals pays off. If you understand probability theory, you’ll find it much easier to switch from frequentist analyses to Bayesian ones.\nIn short, I think that the big payoff for learning statistics this way is extensibility. For a book that only covers the very basics of data analysis, this book has a massive overhead in terms of learning probability theory and so on. There’s a whole lot of other things that it pushes you to learn besides the specific analyses that the book covers. So if your goal had been to learn how to run an ANOVA in the minimum possible time, well, this book wasn’t a good choice. But as I say, I don’t think that is your goal. I think you want to learn how to do data analysis. And if that really is your goal, you want to make sure that the skills you learn in your introductory stats class are naturally and cleanly extensible to the more complicated models that you need in real world data analysis. You want to make sure that you learn to use the same tools that real data analysts use, so that you can learn to do what they do. And so yeah, okay, you’re a beginner right now (or you were when you started this book), but that doesn’t mean you should be given a dumbed-down story, a story in which I don’t tell you about probability density, or a story where I don’t tell you about the nightmare that is factorial ANOVA with unbalanced designs. And it doesn’t mean that you should be given baby toys instead of proper data analysis tools. Beginners aren’t dumb, they just lack knowledge. What you need is not to have the complexities of real world data analysis hidden from from you. What you need are the skills and tools that will let you handle those complexities when they inevitably ambush you in the real world.\nAnd what I hope is that this book, or the finished book that this will one day turn into, is able to help you with that.\nAuthor’s note – I’ve mentioned it before, but I’ll quickly mention it again. The book’s reference list is appallingly incomplete. Please don’t assume that these are the only sources I’ve relied upon. The final version of this book will have a lot more references. And if you see anything clever sounding in this book that doesn’t seem to have a reference, I can absolutely promise you that the idea was someone else’s. This is an introductory textbook: none of the ideas are original. I’ll take responsibility for all the errors, but I can’t take credit for any of the good stuff. Everything smart in this book came from someone else, and they all deserve proper attribution for their excellent work. I just haven’t had the chance to give it to them yet."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Introduction-to-psychological-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Introduction-to-psychological-measurement",
    "title": "2  研究設計入門",
    "section": "2.1 認識心理測量",
    "text": "2.1 認識心理測量\n首先請同學理解資料分析就是一種測量。在心理學系要學的就是如何測量人類行為及心智活動。那麼到底”測量”是什麼？\n\n\n2.1.1 心理測量面面觀\n測量並不是一個實在的概念，而是泛指賦予可觀察的屬性數值、標記、或者某種清晰定義之類的表述。以下都是心理測量的實際例子：\n\n我的年齡是33歲。\n我不喜歡 小魚乾。\n我的染色體性別是男性。\n我的自我認同性別是女性。\n\n以上例子裡的粗體字是“可被測量的事物或 屬性”，斜體字就是“測量值”2。我們可以再對每個例子做些深入探討，了解每種測量的細節：\n\n我的年齡(以年計)可以是0,1,2,3…。年齡的上限是多少沒人知道，不過你可以假定我能活到150歲，雖然還沒有人類活到這麼老。\n若你問我喜歡小魚乾嗎？我可以回答我喜歡，我不喜歡，我沒有喜不喜歡，我有時候喜歡。\n我的染色體性別在多數情況下是男性(\\(XY\\))或女性(\\(XX\\))，不過凡事總有例外。我可能有克林那費爾特症候群(\\(XXY\\))，使得我有男性和女性性徵。當然還有其他性染色體變異的可能。\n我的自我認同性別也許傾向男性或女性，而這和我的染色體性別無關。我也可以認為自已不是二元性別，也可以宣稱是跨性別。\n\n如你所見，有的例子像是能間單測量數值的項目，例如年齡，但是有些研究主題的特殊考量會讓測量變得複雜。因為年齡是大家都能理解的例子，這裡先用年齡做個說明。讀到這裡的同學可能會想，用“年度”做為測量年齡的數值不就好了嗎？若是你加入發展心理學的研究，“年度”可能不是精確的測量單位，可能還有加上“月份”，例如找到兒童參與者2歲11個月大，可縮寫為”2:11”。如果研究對象是新生兒，可能要紀錄出生到現在的“天數”，甚至要用“小時”紀錄。總而言之，確認在研究主題之下，什麼是有意義的測量數值是重要的第一課。\n再想得更仔細的話，我們會發覺“年齡”並不是全然精確的概念。在一般情況，我們講“年齡”有隱含“從出生到現在的時間長度”的意思。但是並不是每次提到“年齡”就是這樣的意思。設想在一個研究新生兒控制眼球運動的實驗室，受測幼兒的”出生時間”不一定是實驗人員記錄“年齡”的有意義參照點。如果今天找到兩位“出生兩小時”的新生兒，一位是比預產期提前三週出生的早產兒，另一位比預產期晚一週出生，那麼研究人員能紀錄這兩位新生兒“年齡相同”嗎？從日常社會互動的情境來說，每個人都是以出生時間做為計算年齡的參照點，因為任何人都能接受每個人出生後都是獨立生活的個體。然而在科學研究的情境要考慮的就不只如此。將人類當成一種生物的話，最能獲得充分分析的年齡紀錄，應該從個體受孕那一刻起，紀錄成長到成熟的狀況。所以“年齡”的測量定義有兩種：從受孕時間起算的年齡，以及從出生時間起算的年齡。研究對象是成人的話或許不必計較兩種定義的區別，若研究對象是新生兒也許要考量那種定義才符合研究目的。\n由測量定義問題再討論下去，就是方法學的課題了。有關人類年齡的”測量方法“，同學們能想到或找到幾種有用的調查方法呢？以下是一些你可以參考的做法：\n\n直接問受測對象”你現在幾歲？“ 這種自陳式報告是快速、便宜又簡單的調查方法。不過必須確認受測對象已經成長到有能力回答調查問題，並且不會給不實回答。\n詢問可靠的消息來源，像是受測幼童的父母親“你們的孩子幾歲？”這種調查方法實施快速，特別是受測者的父母親就在旁邊的情境。不過如果要調查的是”受孕年齡“，大多數父母親都不清楚是什麼時候開始懷孕的。這時也許要詢問更可靠的來源，像是他們的婦產科主治醫師。\n尋找官方紀錄，像是個人的出生與死亡證明。這種方式耗時又勞力，不過如果研究對象是已往生的人是種可靠的來源。\n\n\n\n\n2.1.2 操作型定義: 定義研究的測量方法\n以上討論這麼多，是要引導同學理解操作型定義的概念。用更清楚簡潔的話來說，操作型定義是將有意義但有些模糊的概念，轉換為精確測量程序的設定過程。完整的操作型定義要在過程中考慮這些條件：\n\n清楚定義研究要測量的項目。以“年齡”為例，研究要測量的是“胎兒出生起算的時間長度”還是“母體受孕起算的時間長度”。\n確認用什麼方法進行測量。同樣以“年齡”為例，研穵 要用自陳式報告、詢問受測對象雙親、還是查閱官方紀錄？如果決定用自陳式報告，要如何設定問題內容？\n定義可做紀錄的測量數值範圍。請留意數值不一定要是數字！雖然年齡的測量數值無疑問題數字，依然要考慮什麼單位的數字是需要被紀錄的。是年份？月份？還是小時？像性別一類數值不是數字的測量方法，同樣也要考慮數值的範圍。像是請受測對象透過自陳報告回報性別，要設計什麼樣的選項讓人選擇？只有“男性”和“女性”兩個選項夠嗎？需不需要增加“其他”這一項？或者改用開放式回答，讓受測對象自已寫下個人認同的性別？如果確定使用開放式回答，要如何解讀各式各樣的答案？\n\n設定操作型定義有各種方法，沒有一種“唯一正解”。任何研究都是根據測量目的，透過設定操作型定義的過程，將“年齡”與”性別”等不具形式的概念，轉化為可用數值形式表達的測量值。每個科學領域都對研究對象的測量方式有基本共識。因此，要理解如何設定操作型定義，是因研究主題而異。畢竟有許多主題存在大量差異化的個別研究，有些主題則有一致的研究模式，無法用統一的方法設定操作型定義。\n進入下一節之前，讓我們整理一些之後的單元會經常遇到，與測量有關的專用名詞，這些名詞彼此之間有許多關聯：\n\n理論建構(A theoretical construct) 研究者想要測量的目標，像是”年齡”、“性別”、或”選項”。理論建構不能直接觀察記錄，都是模糊抽象的概念。\n測量程序(measure)3 測量程序是進行觀察紀錄的方法或工具。像是問卷的題目、行為觀察、腦部活動掃瞄都是一種測量程序。\n操作型定義(operationalisation) 測量程序與理論建構之間的邏輯關連條件，或是將理論建構轉換為測量程序的過程。\n變項(variable) 執行測量程序的最終成品就是“資料”，一個變項就是”資料”的集合4。\n\n在統計實務，即使是訓練過的科學家也不大會去管這些名詞的差異，但是對於正在學習的同學們，搞清楚這些名詞的涵義會很有幫助。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Scales-of-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Scales-of-measurement",
    "title": "2  研究設計入門",
    "section": "2.2 測量尺度",
    "text": "2.2 測量尺度\n前一節最後提到心理測量程序的產物就是變項。一份資料檔通常不只有一個變項，而且各種變項有本質的區別，所以這一節要好好認識幾種變項型態。同學們將認識各種測量尺度以及學會判斷資料的變項型態。\n\n\n2.2.1 名義尺度\n名義尺度變項又稱類別變項，命名理由是代表資料的每個數值之間沒有任何明確關係。名義尺度變項的資料之間，沒有那一個數值比較”大”或比較”好”，因此也無法計算這筆變項的平均值。最典型的例子是”瞳孔顏色”。人類瞳孔有藍色、綠色、棕色等各種顏色。沒有人能說瞳孔有所謂的”平均顏色”。人類性別也是典型的名義尺度變項：男性或女性沒有那一種比較好或比較不好，當然也沒有所謂的”平均性別”。總而言之，以名義尺度紀錄的資料，不同數值之間只有各自不一樣的意思而已。\n讓我們再用一個例子更深入了解名義尺度。假如今天要研究民眾如何通勤上班，我們可以設定資料變項記錄每個人是用什麼交通方式去上班。“通勤方式”這個變項可能有好幾種數值：像是“火車”，“公車”，“汽車”，“自行車”。假設調查100位民眾，得到了這四種回答，完成的紀錄就如表2-1。\n\n\n\n\n\n表2-1: 調查100位民眾本日通勤方式\n\n\n\n\n通勤方式\n人數\n\n\n\n\n(1)火車\n12\n\n\n(2)公車\n30\n\n\n(3)汽車\n48\n\n\n(4)自行車\n10\n\n\n\n那麼這裡可以找出平均的通勤方式嗎？很明顯答案不只一個，或者說這是個蠢問題。也許你會說汽車是最多人上班的交通工具，也可以說火車最沒有民眾使用，但都不能涵括調查結果。同學們也可以看一看表2-2，無論如何變動記錄表裡的項目順序，都無法讓這筆資料透露任何意義。\n\n\n\n\n\n表2-2: 調查100位民眾本日通勤方式，依上段描述更改呈現順序\n\n\n\n\n通勤方式\n人數\n\n\n\n\n(3)汽車\n48\n\n\n(1)火車\n12\n\n\n(4)自行車\n10\n\n\n(2)公車\n30\n\n\n\n\n…很明顯沒有什麼改變。\n\n\n2.2.2 次序尺度\n次序尺度變項比起名義尺度變項能呈現一點有結構的資訊，不過沒有很多。次序尺度變項使用自然有意義的方式排序資料數值，但是除了排序我們就無法再做什麼了。最典型的例子是“賽跑選手抵達終點的名次”。先抵達終點的選手毫無疑問比第二名選手快，不過紀錄看不出快多少。紀錄一場比賽的選手名次後，我們會知道第一名比第二名快，第二名比第三名快，但是從資料看不出第一名和第二名的差距，與第二名和第三名的差距有什麼差異。\n以下是個比較有心理學意義的例子。假如我想調查民眾對於氣候變遷的態度，我設計了以下幾個項目，請受訪者選出其中一個最接近個人看法的項目。\n\n氣溫確實有上升，而且是人類活動造成的\n氣溫確實有上升，但是原因不明\n氣溫確實有上升，但是與人類活動無關\n氣溫沒有變化\n\n以上四個項目的順序是根據當前已知的科學研究資訊排列，第1項看法最符合現在的研究所知，第2與第3項是尚有研究證據支持的看法，第4項看法與所有研究證據相左。若是受測者們對於已知科學證據有充分了解，以上四個項目的排序就是符合多數人的看法。如果我將選項用以下段落的方式排序，則是不符合多數人的看法。\n\n氣溫確實有上升，但是與人類活動無關\n氣溫確實有上升，而且是人類活動造成的\n氣溫沒有變化\n氣溫確實有上升，但是原因不明\n\n…這個例子說明心理測量的“結構”是資料呈現出多數人的合理回應。\n假設我成功收集了100位民眾的回應，調查結果總計如表2-3。\n\n\n\n\n\n表2-3: 氣候變遷態度調查結果\n\n\n\n\n調查項目回應\n人數\n\n\n\n\n(1)氣溫確實有上升，而且是人類活動造成的\n51\n\n\n(2)氣溫確實有上升，但是原因不明\n20\n\n\n(3)氣溫確實有上升，但是與人類活動無關\n10\n\n\n(4)氣溫沒有變化\n19\n\n\n\n資料分析能獲得幾種合理解釋，其中一種是最多民眾回應代表理性意見的(1),(2),(3)，或者說100位受訪中的81人至少有理解最新科學研究。另一種合理解釋是至少一半受訪者傾向不同意氣候變遷是現實的看法，因為100位受訪者中的49人選擇不同於主流科學觀點的(2),(3), (4)。然而也可能很難合理解釋為何有九成受訪者選擇(1),(2),(4)，因為這三項的排序不符合預期的順序結構，無法說明最多人選擇的三項有什麼意義。\n以上的說明是向同學們示範，次序尺度變項資料要符合預期中的”順序結構“，才會有合理的分析，而且我們不能計算數值之間的平均。如果我們用回應人數計算加權平均，雖然可以得到平均值1.97，但是這個數值對於解釋調查結果沒有任何幫助。請同學想想這個數字能不能做為報告的結論。\n\n\n\n2.2.3 等距尺度\n不同於名義尺度與次序尺度變項，等距尺度和比例尺度變項以可數的數字表數值，能獲得有意義的資訊。等距尺度變項的數值之間差異是可以推論的，但是變項數值沒有“自然的”零點。攝氏溫度是一個說明等距尺度變項的好例子。例如是，昨天氣溫是 15\\(^{\\circ}\\)，今天氣溫是 18\\(^{\\circ}\\)，所以兩日的溫差是3\\(^{\\circ}\\)。更重要的是，這個3\\(^{\\circ}\\)與氣溫7\\(^{\\circ}\\) 與 10\\(^{\\circ}\\)的差異是完全相等。簡言之，用等距尺度變項數值的做加法或減法是有意義的5。\n然而，攝氏零度並不是“量不到溫度”的意思，最早攝氏零度是根據“觀察到水開始結冰”而指定的數值。這造成溫度的數值無法相乘或相除：要說20\\(^{\\circ}\\) 比10\\(^{\\circ}\\) 熱兩倍是荒誔的，也不可能說20\\(^{\\circ}\\) 是-10\\(^{\\circ}\\) 的負兩倍。\n我們再來舉心理學的例子吧。假如我想要了解各位同學在大學四年間學習態度的變化，最好的方式是每學年或學期開始，就請同手們做一次態度調查。這樣的記錄就是等距尺度變項資料。如果我手上的紀錄有一位學生是2013年的，另一位學生的紀錄是2018年的，向你們報告說2018年學生接受調查的時間是2013年的”1.0025倍“，這樣說奇不奇怪呢？\n\n\n\n\n2.2.4 比例尺度\n最後一種變項型態是有零點的比例尺度，也就是說變項數值是可以相乘且相除的。有個不錯的心理學研究常用的比例尺度變項是反應時間(RT)。許多心理學作業都會紀錄參與者花了多少時間解決問題或給出回答，因為時間長度是作業難度的指標。假如今天有項作業，參與者A花了2.3秒回答，另一位參與者B花了3.1秒。就像等距尺度變項，比例尺度數值的相加與相減是有意義的，也就是我們可以說受試者B比起受試者A多花了3.1 - 2.3 = 0.8秒。同學也要留意反應時間數值的乘法與除法是有意義的：我們可以說受試者B比受試者A花了 3.1/2.3 = 1.35倍的時間完成回答。能做完整四則運算的原因是，反應時間有真正的”零點”~零秒就是沒有反應記錄。\n\n\n\n2.2.5 連續與間斷變項\n還有另一套變項型態是同學們需要知道的：連續變項與間斷變項。不論你要處理的測量尺度是什麼，也會具備切換為連續變項與間斷變項的條件 (見表2-4)。我將連續與間斷變項的差異整理一下：\n\n連續變項(continuous variable)的任何兩個數值之間，都能存在另一個數值，因此是連續的。\n間斷變項(discrete variable)的數值當然不是連續的。任何兩個相鄰的間斷變項數值，不可能存在其他數值。\n\n\n\n\n\n\n表2-4: 測量尺度與間斷/連續變項的關聯性。細格內的符號是jamovi的變項標示符號。6\n\n\n\n\n\n\n\n\n\n\n連續變項\n間斷變項\n\n\n\n\n名義尺度\n\n,\n\n\n次序尺度\n\n\n\n\n等距尺度\n\n,\n\n\n比例尺度\n\n,\n\n\n\n雖然表2-5看起來有點抽象，透過一些例子就能理解如何切換。同樣用解釋比例尺度的反應時間為例，現在除了有參與者A用了2.3秒，與參與者B用了3.1秒的資料，還有一位參與者C用了3.0秒鐘，剛好記錄在前兩位之間。當然，若再有一位參與者D的記錄是3.031秒，就是在B與C之間有一筆資料。雖然真正的實驗不一定會測量得如此準確，這只是示範連續變項的主要特性是在已經存在的任何兩筆資料數值之間，都能增加一個新的資料數值。\n只要變項無法在任何兩個資料數值之間增加資料，就只能是間斷變項。像是名義尺度變項永遠都只能是間斷變項。如同火車走的鐵路與自行車道之間不可能有“切換機制”，名義數值2與3之間不可能增加一個數值2.3，所以名義尺度變項資料只能當成間斷變項資料處理。類別尺度變項也是只能切換成間斷變項，雖然”第2名”確實是在”第1名”與”第3名”之間，“第1名”與”第2名”之間沒有空間給其他數值。至於等距尺度與比例尺度變項，可以切換為間斷變項，也可以切換為連續變項。前面提到的反應時間(比例尺度)與攝氏溫度(等距尺度)都是可以切換為連續變項。不過如果是各位同學的入學年份，雖然是等距尺度變項，卻只能切換為間斷變項，像是2022年與2023年之間不能放入其他年份。還有假如今天同學們做了一份都是是非題的測驗，雖然每一題分數是等距尺度變項資料，也是只能切換為間斷變項，因為沒有5/10正確或6/10錯誤之類的回答。表2-4總結四種測量尺度變項在jamovi介面標記符號，以及能否切換為間斷變項或連續變項7。這一節特別解釋各種測量尺度與間斷/連續的切換關係，出於原作者的兩個理由：首先是有些統計教科書混淆了測量尺度與間斷/連續變項的定義，其次是經常聽到很多人，包括資深研究人員與統計學教師，提到「間斷變項」都會直覺認定是「名義尺度變項」，理解清楚的話就會知道這樣認定會誤讀資料8。\n\n\n\n2.2.6 複雜的現實\n好啦，測量尺度與資料變項之間的切換規律或許會讓一些讀者感到震驚，不過真實世界遠比這條小規律複雜許多。其實現實生活中，只有非常少的可測量指標充分符合這條規律，所以同學們必須留意，不要把切換規律硬套在任何你遇到的測量尺度。切換規律只是指引而已，提示統計使用者在實務中如何找出處置資料的最佳方式而已9。\n讓我們用一個心理學家滿常用的心理測量工具～李克特量表(Likert scale)來說明現實世界有多複雜。許多調查問卷都會使用李克特量表收集受測者的回應，讀者與同學們也許已經填過好多份使用李克特量表的問卷，搞不好曾經在某份自行設計的問卷使用過李克特量表。以下是一條假想的問卷問題：\n請從以下五個選項，挑出最符合您對於「所有韓星都很潮」10這句描述的看法？\n\n完全不同意\n部分不同意\n即非同意也非不同意\n部分同意\n完全同意\n\n這是典型的五點式李克特量表，依數值大小順序排列同意程度，讓受測者選一個數字，通常每個數字旁邊都有文字說明。不過也不必所有數字旁邊都要放文字說明，所以問卷選項也能寫成這個樣子：\n\n完全不同意\n\n\n\n完全同意\n\n李克特量表是非常好用，但是用途有限的工具。怎麼說？請同學想想用這個問題收集到的回應是什麼樣的資料變項？很明顯應該是間斷變項，因為沒有人能給2.5這種答案。這筆資料也顯然不是名義尺度，因為選項是有順序的；也顯然不是比例尺度，因為沒有自然零點。\n那麼這筆資料是次序尺度還是等距尺度？有一種說法是我們無法確定”部分同意”和”完全同意”之間的數值差異，與”部分同意”和”即非同意也非不同意”之間的數值差異是相等的。其實拿日常生活的任何事物做成調查問題，不必有數學知識，任何人都能同意李克特量表的任何一對相鄰數字之間差異是不相等的。所以說我們不應該將用李克特量表收集的資料當成次序尺度變項。另一種說法是假設受測者填答時，會以為選項數字1到5是均等平分的量尺，心理預設五個選項之間的差異如同標示在選項前的數字一樣。經常使用到今天，多數研究者都將李克特量表測得的資料當成等距尺度11。但是嚴格來說又不能算是等距尺度，所以在實務上心理學研究者常將李克特量表當成準等距尺度。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Assessing-the-reliability-of-a-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Assessing-the-reliability-of-a-measurement",
    "title": "2  研究設計入門",
    "section": "2.3 測量的信度",
    "text": "2.3 測量的信度\nAt this point we’ve thought a little bit about how to operationalise a theoretical construct and thereby create a psychological measure. And we’ve seen that by applying psychological measures we end up with variables, which can come in many different types. At this point, we should start discussing the obvious question: is the measurement any good? We’ll do this in terms of two related ideas: reliability and validity. Put simply, the reliability of a measure tells you how precisely you are measuring something, whereas the validity of a measure tells you how accurate the measure is. In this section I’ll talk about reliability; we’ll talk about validity in in the section on [Assessing the validity of a study].\nReliability is actually a very simple concept. It refers to the repeatability or consistency of your measurement. The measurement of my weight by means of a “bathroom scale” is very reliable. If I step on and off the scales over and over again, it’ll keep giving me the same answer. Measuring my intelligence by means of “asking my mum” is very unreliable. Some days she tells me I’m a bit thick, and other days she tells me I’m a complete idiot. Notice that this concept of reliability is different to the question of whether the measurements are correct (the correctness of a measurement relates to it’s validity). If I’m holding a sack of potatos when I step on and off the bathroom scales the measurement will still be reliable: it will always give me the same answer. However, this highly reliable answer doesn’t match up to my true weight at all, therefore it’s wrong. In technical terms, this is a reliable but invalid measurement. Similarly, whilst my mum’s estimate of my intelligence is a bit unreliable, she might be right. Maybe I’m just not too bright, and so while her estimate of my intelligence fluctuates pretty wildly from day to day, it’s basically right. That would be an unreliable but valid measure. Of course, if my mum’s estimates are too unreliable it’s going to be very hard to figure out which one of her many claims about my intelligence is actually the right one. To some extent, then, a very unreliable measure tends to end up being invalid for practical purposes; so much so that many people would say that reliability is necessary (but not sufficient) to ensure validity.\nOkay, now that we’re clear on the distinction between reliability and validity, let’s have a think about the different ways in which we might measure reliability:\n\nTest-retest reliability. This relates to consistency over time. If we repeat the measurement at a later date do we get a the same answer?\nInter-rater reliability. This relates to consistency across people. If someone else repeats the measurement (e.g., someone else rates my intelligence) will they produce the same answer?\nParallel forms reliability. This relates to consistency across theoretically-equivalent measurements. If I use a different set of bathroom scales to measure my weight does it give the same answer?\nInternal consistency reliability. If a measurement is constructed from lots of different parts that perform similar functions (e.g., a personality questionnaire result is added up across several questions) do the individual parts tend to give similar answers. We’ll look at this particular form of reliability later in the book, in Section 15.5.\n\nNot all measurements need to possess all forms of reliability. For instance, educational assessment can be thought of as a form of measurement. One of the subjects that I teach, Computational Cognitive Science, has an assessment structure that has a research component and an exam component (plus other things). The exam component is intended to measure something different from the research component, so the assessment as a whole has low internal consistency. However, within the exam there are several questions that are intended to (approximately) measure the same things, and those tend to produce similar outcomes. So the exam on its own has a fairly high internal consistency. Which is as it should be. You should only demand reliability in those situations where you want to be measuring the same thing!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#the-role-of-variables-predictors-and-outcomes",
    "href": "02-A-brief-introduction-to-research-design.html#the-role-of-variables-predictors-and-outcomes",
    "title": "2  A brief introduction to research design",
    "section": "2.4 The “role” of variables: predictors and outcomes",
    "text": "2.4 The “role” of variables: predictors and outcomes\nI’ve got one last piece of terminology that I need to explain to you before moving away from variables. Normally, when we do some research we end up with lots of different variables. Then, when we analyse our data, we usually try to explain some of the variables in terms of some of the other variables. It’s important to keep the two roles “thing doing the explaining” and “thing being explained” distinct. So let’s be clear about this now. First, we might as well get used to the idea of using mathematical symbols to describe variables, since it’s going to happen over and over again. Let’s denote the “to be explained” variable \\(Y\\), and denote the variables “doing the explaining” as \\(X_1 , X_2\\), etc.\nWhen we are doing an analysis we have different names for \\(X\\) and \\(Y\\), since they play different roles in the analysis. The classical names for these roles are independent variable (IV) and dependent variable (DV). The IV is the variable that you use to do the explaining (i.e., \\(X\\)) and the DV is the variable being explained (i.e.,$Y $). The logic behind these names goes like this: if there really is a relationship between \\(X\\) and \\(Y\\) then we can say that \\(Y\\)depends on \\(X\\), and if we have designed our study “properly” then \\(X\\) isn’t dependent on anything else. However, I personally find those names horrible. They’re hard to remember and they’re highly misleading because (a) the IV is never actually “independent of everything else”, and (b) if there’s no relationship then the DV doesn’t actually depend on the IV. And in fact, because I’m not the only person who thinks that IV and DV are just awful names, there are a number of alternatives that I find more appealing. The terms that I’ll use in this book are predictors and outcomes. The idea here is that what you’re trying to do is use \\(X\\) (the predictors) to make guesses about \\(Y\\) (the outcomes).4 This is summarised in Table 2.5.\n\n\n\n\nTable 2.5:  Variable distinctions \n\nrole of the variableclassical namemodern name\n\n\"to be explained\"dependent variable (DV)outcome\n\n\"to do the explaining\"independent variable (IV)predictor"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#experimental-and-non-experimental-research",
    "href": "02-A-brief-introduction-to-research-design.html#experimental-and-non-experimental-research",
    "title": "2  A brief introduction to research design",
    "section": "2.5 Experimental and non-experimental research",
    "text": "2.5 Experimental and non-experimental research\nOne of the big distinctions that you should be aware of is the distinction between “experimental research” and “non-experimental research”. When we make this distinction, what we’re really talking about is the degree of control that the researcher exercises over the people and events in the study.\n\n2.5.1 Experimental research\nThe key feature of experimental research is that the researcher controls all aspects of the study, especially what participants experience during the study. In particular, the researcher manipulates or varies the predictor variables (IVs) but allows the outcome variable (DV) to vary naturally. The idea here is to deliberately vary the predictors (IVs) to see if they have any causal effects on the outcomes. Moreover, in order to ensure that there’s no possibility that something other than the predictor variables is causing the outcomes, everything else is kept constant or is in some other way “balanced”, to ensure that they have no effect on the results. In practice, it’s almost impossible to think of everything else that might have an influence on the outcome of an experiment, much less keep it constant. The standard solution to this is randomisation. That is, we randomly assign people to different groups, and then give each group a different treatment (i.e., assign them different values of the predictor variables). We’ll talk more about randomisation later, but for now it’s enough to say that what randomisation does is minimise (but not eliminate) the possibility that there are any systematic difference between groups.\nLet’s consider a very simple, completely unrealistic and grossly unethical example. Suppose you wanted to find out if smoking causes lung cancer. One way to do this would be to find people who smoke and people who don’t smoke and look to see if smokers have a higher rate of lung cancer. This is not a proper experiment, since the researcher doesn’t have a lot of control over who is and isn’t a smoker. And this really matters. For instance, it might be that people who choose to smoke cigarettes also tend to have poor diets, or maybe they tend to work in asbestos mines, or whatever. The point here is that the groups (smokers and non-smokers) actually differ on lots of things, not just smoking. So it might be that the higher incidence of lung cancer among smokers is caused by something else, and not by smoking per se. In technical terms these other things (e.g. diet) are called “confounders”, and we’ll talk about those in just a moment.\nIn the meantime, let’s consider what a proper experiment might look like. Recall that our concern was that smokers and non-smokers might differ in lots of ways. The solution, as long as you have no ethics, is to control who smokes and who doesn’t. Specifically, if we randomly divide young non-smokers into two groups and force half of them to become smokers, then it’s very unlikely that the groups will differ in any respect other than the fact that half of them smoke. That way, if our smoking group gets cancer at a higher rate than the non-smoking group, we can feel pretty confident that (a) smoking does cause cancer and (b) we’re murderers.\n\n\n2.5.2 Non-experimental research\nNon-experimental research is a broad term that covers “any study in which the researcher doesn’t have as much control as they do in an experiment”. Obviously, control is something that scientists like to have, but as the previous example illustrates there are lots of situations in which you can’t or shouldn’t try to obtain that control. Since it’s grossly unethical (and almost certainly criminal) to force people to smoke in order to find out if they get cancer, this is a good example of a situation in which you really shouldn’t try to obtain experimental control. But there are other reasons too. Even leaving aside the ethical issues, our “smoking experiment” does have a few other issues. For instance, when I suggested that we “force” half of the people to become smokers, I was talking about starting with a sample of non-smokers, and then forcing them to become smokers. While this sounds like the kind of solid, evil experimental design that a mad scientist would love, it might not be a very sound way of investigating the effect in the real world. For instance, suppose that smoking only causes lung cancer when people have poor diets, and suppose also that people who normally smoke do tend to have poor diets. However, since the “smokers” in our experiment aren’t “natural” smokers (i.e., we forced non-smokers to become smokers, but they didn’t take on all of the other normal, real life characteristics that smokers might tend to possess) they probably have better diets. As such, in this silly example they wouldn’t get lung cancer and our experiment will fail, because it violates the structure of the “natural” world (the technical name for this is an “artefactual” result).\nOne distinction worth making between two types of non-experimental research is the difference between quasi-experimental research and case studies. The example I discussed earlier, in which we wanted to examine incidence of lung cancer among smokers and non-smokers without trying to control who smokes and who doesn’t, is a quasi-experimental design. That is, it’s the same as an experiment but we don’t control the predictors (IVs). We can still use statistics to analyse the results, but we have to be a lot more careful and circumspect.\nThe alternative approach, case studies, aims to provide a very detailed description of one or a few instances. In general, you can’t use statistics to analyse the results of case studies and it’s usually very hard to draw any general conclusions about “people in general” from a few isolated examples. However, case studies are very useful in some situations. Firstly, there are situations where you don’t have any alternative. Neuropsychology has this issue a lot. Sometimes, you just can’t find a lot of people with brain damage in a specific brain area, so the only thing you can do is describe those cases that you do have in as much detail and with as much care as you can. However, there’s also some genuine advantages to case studies. Because you don’t have as many people to study you have the ability to invest lots of time and effort trying to understand the specific factors at play in each case. This is a very valuable thing to do. As a consequence, case studies can complement the more statistically-oriented approaches that you see in experimental and quasi-experimental designs. We won’t talk much about case studies in this book, but they are nevertheless very valuable tools!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#assessing-the-validity-of-a-study",
    "href": "02-A-brief-introduction-to-research-design.html#assessing-the-validity-of-a-study",
    "title": "2  A brief introduction to research design",
    "section": "2.6 Assessing the validity of a study",
    "text": "2.6 Assessing the validity of a study\nMore than any other thing, a scientist wants their research to be “valid”. The conceptual idea behind validity is very simple. Can you trust the results of your study? If not, the study is invalid. However, whilst it’s easy to state, in practice it’s much harder to check validity than it is to check reliability. And in all honesty, there’s no precise, clearly agreed upon notion of what validity actually is. In fact, there are lots of different kinds of validity, each of which raises it’s own issues. And not all forms of validity are relevant to all studies. I’m going to talk about five different types of validity:\n\nInternal validity\nExternal validity\nConstruct validity\nFace validity\nEcological validity\n\nFirst, a quick guide as to what matters here. (1) Internal and external validity are the most important, since they tie directly to the fundamental question of whether your study really works. (2) Construct validity asks whether you’re measuring what you think you are. (3) Face validity isn’t terribly important except insofar as you care about “appearances”. (4) Ecological validity is a special case of face validity that corresponds to a kind of appearance that you might care about a lot.\n\n2.6.1 Internal validity\nInternal validity refers to the extent to which you are able draw the correct conclusions about the causal relationships between variables. It’s called “internal” because it refers to the relationships between things “inside” the study. Let’s illustrate the concept with a simple example. Suppose you’re interested in finding out whether a university education makes you write better. To do so, you get a group of first year students, ask them to write a 1000 word essay, and count the number of spelling and grammatical errors they make. Then you find some third-year students, who obviously have had more of a university education than the first-years, and repeat the exercise. And let’s suppose it turns out that the third-year students produce fewer errors. And so you conclude that a university education improves writing skills. Right? Except that the big problem with this experiment is that the third-year students are older and they’ve had more experience with writing things. So it’s hard to know for sure what the causal relationship is. Do older people write better? Or people who have had more writing experience? Or people who have had more education? Which of the above is the true cause of the superior performance of the third-years? Age? Experience? Education? You can’t tell. This is an example of a failure of internal validity, because your study doesn’t properly tease apart the causal relationships between the different variables.\n\n\n2.6.2 External validity\nExternal validity relates to the generalisability or applicability of your findings. That is, to what extent do you expect to see the same pattern of results in “real life” as you saw in your study. To put it a bit more precisely, any study that you do in psychology will involve a fairly specific set of questions or tasks, will occur in a specific environment, and will involve participants that are drawn from a particular subgroup (disappointingly often it is college students!). So, if it turns out that the results don’t actually generalise or apply to people and situations beyond the ones that you studied, then what you’ve got is a lack of external validity.\nThe classic example of this issue is the fact that a very large proportion of studies in psychology will use undergraduate psychology students as the participants. Obviously, however, the researchers don’t care only about psychology students. They care about people in general. Given that, a study that uses only psychology students as participants always carries a risk of lacking external validity. That is, if there’s something “special” about psychology students that makes them different to the general population in some relevant respect, then we may start worrying about a lack of external validity.\nThat said, it is absolutely critical to realise that a study that uses only psychology students does not necessarily have a problem with external validity. I’ll talk about this again later, but it’s such a common mistake that I’m going to mention it here. The external validity of a study is threatened by the choice of population if (a) the population from which you sample your participants is very narrow (e.g., psychology students), and (b) the narrow population that you sampled from is systematically different from the general population in some respect that is relevant to the psychological phenomenon that you intend to study. The italicised part is the bit that lots of people forget. It is true that psychology undergraduates differ from the general population in lots of ways, and so a study that uses only psychology students may have problems with external validity. However, if those differences aren’t very relevant to the phenomenon that you’re studying, then there’s nothing to worry about. To make this a bit more concrete here are two extreme examples:\n\nYou want to measure “attitudes of the general public towards psychotherapy”, but all of your participants are psychology students. This study would almost certainly have a problem with external validity.\nYou want to measure the effectiveness of a visual illusion, and your participants are all psychology students. This study is unlikely to have a problem with external validity\n\nHaving just spent the last couple of paragraphs focusing on the choice of participants, since that’s a big issue that everyone tends to worry most about, it’s worth remembering that external validity is a broader concept. The following are also examples of things that might pose a threat to external validity, depending on what kind of study you’re doing:\n\nPeople might answer a “psychology questionnaire” in a manner that doesn’t reflect what they would do in real life.\nYour lab experiment on (say) “human learning” has a different structure to the learning problems people face in real life.\n\n\n\n2.6.3 Construct validity\nConstruct validity is basically a question of whether you’re measuring what you want to be measuring. A measurement has good construct validity if it is actually measuring the correct theoretical construct, and bad construct validity if it doesn’t. To give a very simple (if ridiculous) example, suppose I’m trying to investigate the rates with which university students cheat on their exams. And the way I attempt to measure it is by asking the cheating students to stand up in the lecture theatre so that I can count them. When I do this with a class of 300 students 0 people claim to be cheaters. So I therefore conclude that the proportion of cheaters in my class is 0%. Clearly this is a bit ridiculous. But the point here is not that this is a very deep methodological example, but rather to explain what construct validity is. The problem with my measure is that while I’m trying to measure “the proportion of people who cheat” what I’m actually measuring is “the proportion of people stupid enough to own up to cheating, or bloody minded enough to pretend that they do”. Obviously, these aren’t the same thing! So my study has gone wrong, because my measurement has very poor construct validity.\n\n\n2.6.4 Face validity\nFace validity simply refers to whether or not a measure “looks like” it’s doing what it’s supposed to, nothing more. If I design a test of intelligence, and people look at it and they say “no, that test doesn’t measure intelligence”, then the measure lacks face validity. It’s as simple as that. Obviously, face validity isn’t very important from a pure scientific perspective. After all, what we care about is whether or not the measure actually does what it’s supposed to do, not whether it looks like it does what it’s supposed to do. As a consequence, we generally don’t care very much about face validity. That said, the concept of face validity serves three useful pragmatic purposes:\n\nSometimes, an experienced scientist will have a “hunch” that a particular measure won’t work. While these sorts of hunches have no strict evidentiary value, it’s often worth paying attention to them. Because often times people have knowledge that they can’t quite verbalise, so there might be something to worry about even if you can’t quite say why. In other words, when someone you trust criticises the face validity of your study, it’s worth taking the time to think more carefully about your design to see if you can think of reasons why it might go awry. Mind you, if you don’t find any reason for concern, then you should probably not worry. After all, face validity really doesn’t matter very much.\nOften (very often), completely uninformed people will also have a “hunch” that your research is crap. And they’ll criticise it on the internet or something. On close inspection you may notice that these criticisms are actually focused entirely on how the study “looks”, but not on anything deeper. The concept of face validity is useful for gently explaining to people that they need to substantiate their arguments further.\nExpanding on the last point, if the beliefs of untrained people are critical (e.g., this is often the case for applied research where you actually want to convince policy makers of something or other) then you have to care about face validity. Simply because, whether you like it or not, a lot of people will use face validity as a proxy for real validity. If you want the government to change a law on scientific psychological grounds, then it won’t matter how good your studies “really” are. If they lack face validity you’ll find that politicians ignore you. Of course, it’s somewhat unfair that policy often depends more on appearance than fact, but that’s how things go.\n\n\n\n2.6.5 Ecological validity\nEcological validity is a different notion of validity, which is similar to external validity, but less important. The idea is that, in order to be ecologically valid, the entire set up of the study should closely approximate the real world scenario that is being investigated. In a sense, ecological validity is a kind of face validity. It relates mostly to whether the study “looks” right, but with a bit more rigour to it. To be ecologically valid the study has to look right in a fairly specific way. The idea behind it is the intuition that a study that is ecologically valid is more likely to be externally valid. It’s no guarantee, of course. But the nice thing about ecological validity is that it’s much easier to check whether a study is ecologically valid than it is to check whether a study is externally valid. A simple example would be eyewitness identification studies. Most of these studies tend to be done in a university setting, often with a fairly simple array of faces to look at, rather than a line up. The length of time between seeing the “criminal” and being asked to identify the suspect in the “line up” is usually shorter. The “crime” isn’t real so there’s no chance of the witness being scared, and there are no police officers present so there’s not as much chance of feeling pressured. These things all mean that the study definitely lacks ecological validity. They might (but might not) mean that it also lacks external validity."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#confounds-artefacts-and-other-threats-to-validity",
    "href": "02-A-brief-introduction-to-research-design.html#confounds-artefacts-and-other-threats-to-validity",
    "title": "2  簡易入門研究設計",
    "section": "2.7 Confounds, artefacts and other threats to validity",
    "text": "2.7 Confounds, artefacts and other threats to validity\nIf we look at the issue of validity in the most general fashion the two biggest worries that we have are confounders and artefacts. These two terms are defined in the following way:\n\nConfounder: A confounder is an additional, often unmeasured variable5 that turns out to be related to both the predictors and the outcome. The existence of confounders threatens the internal validity of the study because you can’t tell whether the predictor causes the outcome, or if the confounding variable causes it.\nArtefact: A result is said to be “artefactual” if it only holds in the special situation that you happened to test in your study. The possibility that your result is an artefact describes a threat to your external validity, because it raises the possibility that you can’t generalise or apply your results to the actual population that you care about.\n\nAs a general rule confounders are a bigger concern for non-experimental studies, precisely because they’re not proper experiments. By definition, you’re leaving lots of things uncontrolled, so there’s a lot of scope for confounders being present in your study. Experimental research tends to be much less vulnerable to confounders. The more control you have over what happens during the study, the more you can prevent confounders from affecting the results. With random allocation, for example, confounders are distributed randomly, and evenly, between different groups.\nHowever, there are always swings and roundabouts and when we start thinking about artefacts rather than confounders the shoe is very firmly on the other foot. For the most part, artefactual results tend to be a concern for experimental studies than for non-experimental studies. To see this, it helps to realise that the reason that a lot of studies are non-experimental is precisely because what the researcher is trying to do is examine human behaviour in a more naturalistic context. By working in a more real-world context you lose experimental control (making yourself vulnerable to confounders), but because you tend to be studying human psychology “in the wild” you reduce the chances of getting an artefactual result. Or, to put it another way, when you take psychology out of the wild and bring it into the lab (which we usually have to do to gain our experimental control), you always run the risk of accidentally studying something different to what you wanted to study.\nBe warned though. The above is a rough guide only. It’s absolutely possible to have confounders in an experiment, and to get artefactual results with non-experimental studies. This can happen for all sorts of reasons, not least of which is experimenter or researcher error. In practice, it’s really hard to think everything through ahead of time and even very good researchers make mistakes.\nAlthough there’s a sense in which almost any threat to validity can be characterised as a confounder or an artefact, they’re pretty vague concepts. So let’s have a look at some of the most common examples.\n\n2.7.1 History effects\nHistory effects refer to the possibility that specific events may occur during the study that might influence the outcome measure. For instance, something might happen in between a pretest and a post-test. Or in-between testing participant 23 and participant 24. Alternatively, it might be that you’re looking at a paper from an older study that was perfectly valid for its time, but the world has changed enough since then that the conclusions are no longer trustworthy. Examples of things that would count as history effects are:\n\nYou’re interested in how people think about risk and uncertainty. You started your data collection in December 2010. But finding participants and collecting data takes time, so you’re still finding new people in February 2011. Unfortunately for you (and even more unfortunately for others), the Queensland floods occurred in January 2011 causing billions of dollars of damage and killing many people. Not surprisingly, the people tested in February 2011 express quite different beliefs about handling risk than the people tested in December 2010. Which (if any) of these reflects the “true” beliefs of participants? I think the answer is probably both. The Queensland floods genuinely changed the beliefs of the Australian public, though possibly only temporarily. The key thing here is that the “history” of the people tested in February is quite different to people tested in December.\nYou’re testing the psychological effects of a new anti-anxiety drug. So what you do is measure anxiety before administering the drug (e.g., by self-report, and taking physiological measures). Then you administer the drug, and afterwards you take the same measures. In the middle however, because your lab is in Los Angeles, there’s an earthquake which increases the anxiety of the participants.\n\n\n\n2.7.2 Maturation effects\nAs with history effects, maturational effects are fundamentally about change over time. However, maturation effects aren’t in response to specific events. Rather, they relate to how people change on their own over time. We get older, we get tired, we get bored, etc. Some examples of maturation effects are:\n\nWhen doing developmental psychology research you need to be aware that children grow up quite rapidly. So, suppose that you want to find out whether some educational trick helps with vocabulary size among 3 year olds. One thing that you need to be aware of is that the vocabulary size of children that age is growing at an incredible rate (multiple words per day) all on its own. If you design your study without taking this maturational effect into account, then you won’t be able to tell if your educational trick works.\nWhen running a very long experiment in the lab (say, something that goes for 3 hours) it’s very likely that people will begin to get bored and tired, and that this maturational effect will cause performance to decline regardless of anything else going on in the experiment\n\n\n\n2.7.3 Repeated testing effects\nAn important type of history effect is the effect of repeated testing. Suppose I want to take two measurements of some psychological construct (e.g., anxiety). One thing I might be worried about is if the first measurement has an effect on the second measurement. In other words, this is a history effect in which the “event” that influences the second measurement is the first measurement itself! This is not at all uncommon. Examples of this include:\n\nLearning and practice: e.g., “intelligence” at time 2 might appear to go up relative to time 1 because participants learned the general rules of how to solve “intelligence-test-style” questions during the first testing session.\nFamiliarity with the testing situation: e.g., if people are nervous at time 1, this might make performance go down. But after sitting through the first testing situation they might calm down a lot precisely because they’ve seen what the testing looks like.\nAuxiliary changes caused by testing: e.g., if a questionnaire assessing mood is boring then mood rating at measurement time 2 is more likely to be “bored” precisely because of the boring measurement made at time 1.\n\n\n\n2.7.4 Selection bias\nSelection bias is a pretty broad term. Suppose that you’re running an experiment with two groups of participants where each group gets a different “treatment”, and you want to see if the different treatments lead to different outcomes. However, suppose that, despite your best efforts, you’ve ended up with a gender imbalance across groups (say, group A has 80% females and group B has 50% females). It might sound like this could never happen but, trust me, it can. This is an example of a selection bias, in which the people “selected into” the two groups have different characteristics. If any of those characteristics turns out to be relevant (say, your treatment works better on females than males) then you’re in a lot of trouble.\n\n\n2.7.5 Differential attrition\nWhen thinking about the effects of attrition, it is sometimes helpful to distinguish between two different types. The first is homogeneous attrition, in which the attrition effect is the same for all groups, treatments or conditions. In the example I gave above, the attrition would be homogeneous if (and only if) the easily bored participants are dropping out of all of the conditions in my experiment at about the same rate. In general, the main effect of homogeneous attrition is likely to be that it makes your sample unrepresentative. As such, the biggest worry that you’ll have is that the generalisability of the results decreases. In other words, you lose external validity.\nThe second type of attrition is heterogeneous attrition, in which the attrition effect is different for different groups. More often called differential attrition, this is a kind of selection bias that is caused by the study itself. Suppose that, for the first time ever in the history of psychology, I manage to find the perfectly balanced and representative sample of people. I start running “Dani’s incredibly long and tedious experiment” on my perfect sample but then, because my study is incredibly long and tedious, lots of people start dropping out. I can’t stop this. Participants absolutely have the right to stop doing any experiment, any time, for whatever reason they feel like, and as researchers we are morally (and professionally) obliged to remind people that they do have this right. So, suppose that “Dani’s incredibly long and tedious experiment” has a very high drop out rate. What do you suppose the odds are that this drop out is random? Answer: zero. Almost certainly the people who remain are more conscientious, more tolerant of boredom, etc., than those that leave. To the extent that (say) conscientiousness is relevant to the psychological phenomenon that I care about, this attrition can decrease the validity of my results.\nHere’s another example. Suppose I design my experiment with two conditions. In the “treatment” condition, the experimenter insults the participant and then gives them a questionnaire designed to measure obedience. In the “control” condition, the experimenter engages in a bit of pointless chitchat and then gives them the questionnaire. Leaving aside the questionable scientific merits and dubious ethics of such a study, let’s have a think about what might go wrong here. As a general rule, when someone insults me to my face I tend to get much less co-operative. So, there’s a pretty good chance that a lot more people are going to drop out of the treatment condition than the control condition. And this drop out isn’t going to be random. The people most likely to drop out would probably be the people who don’t care all that much about the importance of obediently sitting through the experiment. Since the most bloody minded and disobedient people all left the treatment group but not the control group, we’ve introduced a confound: the people who actually took the questionnaire in the treatment group were already more likely to be dutiful and obedient than the people in the control group. In short, in this study insulting people doesn’t make them more obedient. It makes the more disobedient people leave the experiment! The internal validity of this experiment is completely shot.\n\n\n2.7.6 Non-response bias\nNon-response bias is closely related to selection bias and to differential attrition. The simplest version of the problem goes like this. You mail out a survey to 1000 people but only 300 of them reply. The 300 people who replied are almost certainly not a random subsample. People who respond to surveys are systematically different to people who don’t. This introduces a problem when trying to generalise from those 300 people who replied to the population at large, since you now have a very non-random sample. The issue of non-response bias is more general than this, though. Among the (say) 300 people that did respond to the survey, you might find that not everyone answers every question. If (say) 80 people chose not to answer one of your questions, does this introduce problems? As always, the answer is maybe. If the question that wasn’t answered was on the last page of the questionnaire, and those 80 surveys were returned with the last page missing, there’s a good chance that the missing data isn’t a big deal; probably the pages just fell off. However, if the question that 80 people didn’t answer was the most confrontational or invasive personal question in the questionnaire, then almost certainly you’ve got a problem. In essence, what you’re dealing with here is what’s called the problem of missing data. If the data that is missing was “lost” randomly, then it’s not a big problem. If it’s missing systematically, then it can be a big problem.\n\n\n2.7.7 Regression to the mean\nRegression to the mean refers to any situation where you select data based on an extreme value on some measure. Because the variable has natural variation it almost certainly means that when you take a subsequent measurement the later measurement will be less extreme than the first one, purely by chance.\nHere’s an example. Suppose I’m interested in whether a psychology education has an adverse effect on very smart kids. To do this, I find the 20 psychology I students with the best high school grades and look at how well they’re doing at university. It turns out that they’re doing a lot better than average, but they’re not topping the class at university even though they did top their classes at high school. What’s going on? The natural first thought is that this must mean that the psychology classes must be having an adverse effect on those students. However, while that might very well be the explanation, it’s more likely that what you’re seeing is an example of “regression to the mean”. To see how it works, let’s take a moment to think about what is required to get the best mark in a class, regardless of whether that class be at high school or at university. When you’ve got a big class there are going to be lots of very smart people enrolled. To get the best mark you have to be very smart, work very hard, and be a bit lucky. The exam has to ask just the right questions for your idiosyncratic skills, and you have to avoid making any dumb mistakes (we all do that sometimes) when answering them. And that’s the thing, whilst intelligence and hard work are transferable from one class to the next, luck isn’t. The people who got lucky in high school won’t be the same as the people who get lucky at university. That’s the very definition of “luck”. The consequence of this is that when you select people at the very extreme values of one measurement (the top 20 students), you’re selecting for hard work, skill and luck. But because the luck doesn’t transfer to the second measurement (only the skill and work), these people will all be expected to drop a little bit when you measure them a second time (at university). So their scores fall back a little bit, back towards everyone else. This is regression to the mean.\nRegression to the mean is surprisingly common. For instance, if two very tall people have kids their children will tend to be taller than average but not as tall as the parents. The reverse happens with very short parents. Two very short parents will tend to have short children, but nevertheless those kids will tend to be taller than the parents. It can also be extremely subtle. For instance, there have been studies done that suggested that people learn better from negative feedback than from positive feedback. However, the way that people tried to show this was to give people positive reinforcement whenever they did good, and negative reinforcement when they did bad. And what you see is that after the positive reinforcement people tended to do worse, but after the negative reinforcement they tended to do better. But notice that there’s a selection bias here! When people do very well, you’re selecting for “high” values, and so you should expect, because of regression to the mean, that performance on the next trial should be worse regardless of whether reinforcement is given. Similarly, after a bad trial, people will tend to improve all on their own. The apparent superiority of negative feedback is an artefact caused by regression to the mean (see Kahneman & Tversky (1973), for discussion).\n\n\n2.7.8 Experimenter bias\nExperimenter bias can come in multiple forms. The basic idea is that the experimenter, despite the best of intentions, can accidentally end up influencing the results of the experiment by subtly communicating the “right answer” or the “desired behaviour” to the participants. Typically, this occurs because the experimenter has special knowledge that the participant does not, for example the right answer to the questions being asked or knowledge of the expected pattern of performance for the condition that the participant is in. The classic example of this happening is the case study of “Clever Hans”, which dates back to 1907 (Pfungst, 1911). Clever Hans was a horse that apparently was able to read and count and perform other human like feats of intelligence. After Clever Hans became famous, psychologists started examining his behaviour more closely. It turned out that, not surprisingly, Hans didn’t know how to do maths. Rather, Hans was responding to the human observers around him, because the humans did know how to count and the horse had learned to change its behaviour when people changed theirs.\nThe general solution to the problem of experimenter bias is to engage in double blind studies, where neither the experimenter nor the participant knows which condition the participant is in or knows what the desired behaviour is. This provides a very good solution to the problem, but it’s important to recognise that it’s not quite ideal, and hard to pull off perfectly. For instance, the obvious way that I could try to construct a double blind study is to have one of my Ph.D. students (one who doesn’t know anything about the experiment) run the study. That feels like it should be enough. The only person (me) who knows all the details (e.g., correct answers to the questions, assignments of participants to conditions) has no interaction with the participants, and the person who does all the talking to people (the Ph.D. student) doesn’t know anything. Except for the reality that the last part is very unlikely to be true. In order for the Ph.D. student to run the study effectively they need to have been briefed by me, the researcher. And, as it happens, the Ph.D. student also knows me and knows a bit about my general beliefs about people and psychology (e.g., I tend to think humans are much smarter than psychologists give them credit for). As a result of all this, it’s almost impossible for the experimenter to avoid knowing a little bit about what expectations I have. And even a little bit of knowledge can have an effect. Suppose the experimenter accidentally conveys the fact that the participants are expected to do well in this task. Well, there’s a thing called the “Pygmalion effect”, where if you expect great things of people they’ll tend to rise to the occasion. But if you expect them to fail then they’ll do that too. In other words, the expectations become a self-fulfilling prophesy.\n\n\n2.7.9 Demand effects and reactivity\nWhen talking about experimenter bias, the worry is that the experimenter’s knowledge or desires for the experiment are communicated to the participants, and that these can change people’s behaviour (Rosenthal, 1966). However, even if you manage to stop this from happening, it’s almost impossible to stop people from knowing that they’re part of a psychological study. And the mere fact of knowing that someone is watching or studying you can have a pretty big effect on behaviour. This is generally referred to as reactivity or demand effects. The basic idea is captured by the Hawthorne effect: people alter their performance because of the attention that the study focuses on them. The effect takes its name from a study that took place in the “Hawthorne Works” factory outside of Chicago (see Adair (1984)). This study, from the 1920s, looked at the effects of factory lighting on worker productivity. But, importantly, change in worker behaviour occurred because the workers knew they were being studied, rather than any effect of factory lighting.\nTo get a bit more specific about some of the ways in which the mere fact of being in a study can change how people behave, it helps to think like a social psychologist and look at some of the roles that people might adopt during an experiment but might not adopt if the corresponding events were occurring in the real world:\n\nThe good participant tries to be too helpful to the researcher. He or she seeks to figure out the experimenter’s hypotheses and confirm them.\nThe negative participant does the exact opposite of the good participant. He or she seeks to break or destroy the study or the hypothesis in some way.\nThe faithful participant is unnaturally obedient. He or she seeks to follow instructions perfectly, regardless of what might have happened in a more realistic setting.\nThe apprehensive participant gets nervous about being tested or studied, so much so that his or her behaviour becomes highly unnatural, or overly socially desirable.\n\n\n\n2.7.10 Placebo effects\nThe placebo effect is a specific type of demand effect that we worry a lot about. It refers to the situation where the mere fact of being treated causes an improvement in outcomes. The classic example comes from clinical trials. If you give people a completely chemically inert drug and tell them that it’s a cure for a disease, they will tend to get better faster than people who aren’t treated at all. In other words, it is people’s belief that they are being treated that causes the improved outcomes, not the drug.\nHowever, the current consensus in medicine is that true placebo effects are quite rare and most of what was previously considered placebo effect is in fact some combination of natural healing (some people just get better on their own), regression to the mean and other quirks of study design. Of interest to psychology is that the strongest evidence for at least some placebo effect is in self-reported outcomes, most notably in treatment of pain (Hróbjartsson & Gøtzsche, 2010).\n\n\n2.7.11 Situation, measurement and sub-population effects\nIn some respects, these terms are a catch-all term for “all other threats to external validity”. They refer to the fact that the choice of sub-population from which you draw your participants, the location, timing and manner in which you run your study (including who collects the data) and the tools that you use to make your measurements might all be influencing the results. Specifically, the worry is that these things might be influencing the results in such a way that the results won’t generalise to a wider array of people, places and measures.\n\n\n2.7.12 Fraud, deception and self-deception\n\nIt is difficult to get a man to understand something, when his salary depends on his not understanding it.\n- Upton Sinclair\n\nThere’s one final thing I feel I should mention. While reading what the textbooks often have to say about assessing the validity of a study I couldn’t help but notice that they seem to make the assumption that the researcher is honest. I find this hilarious. While the vast majority of scientists are honest, in my experience at least, some are not.6 Not only that, as I mentioned earlier, scientists are not immune to belief bias. It’s easy for a researcher to end up deceiving themselves into believing the wrong thing, and this can lead them to conduct subtly flawed research and then hide those flaws when they write it up. So you need to consider not only the (probably unlikely) possibility of outright fraud, but also the (probably quite common) possibility that the research is unintentionally “slanted”. I opened a few standard textbooks and didn’t find much of a discussion of this problem, so here’s my own attempt to list a few ways in which these issues can arise:\n\nData fabrication. Sometimes, people just make up the data. This is occasionally done with “good” intentions. For instance, the researcher believes that the fabricated data do reflect the truth, and may actually reflect “slightly cleaned up” versions of actual data. On other occasions, the fraud is deliberate and malicious. Some high-profile examples where data fabrication has been alleged or shown include Cyril Burt (a psychologist who is thought to have fabricated some of his data), Andrew Wakefield (who has been accused of fabricating his data connecting the MMR vaccine to autism) and Hwang Woo-suk (who falsified a lot of his data on stem cell research).\nHoaxes. Hoaxes share a lot of similarities with data fabrication, but they differ in the intended purpose. A hoax is often a joke, and many of them are intended to be (eventually) discovered. Often, the point of a hoax is to discredit someone or some field. There’s quite a few well known scientific hoaxes that have occurred over the years (e.g., Piltdown man) and some were deliberate attempts to discredit particular fields of research (e.g., the Sokal affair).\nData misrepresentation. While fraud gets most of the headlines, it’s much more common in my experience to see data being misrepresented. When I say this I’m not referring to newspapers getting it wrong (which they do, almost always). I’m referring to the fact that often the data don’t actually say what the researchers think they say. My guess is that, almost always, this isn’t the result of deliberate dishonesty but instead is due to a lack of sophistication in the data analyses. For instance, think back to the example of Simpson’s paradox that I discussed in the beginning of this book. It’s very common to see people present “aggregated” data of some kind and sometimes, when you dig deeper and find the raw data yourself you find that the aggregated data tell a different story to the disaggregated data. Alternatively, you might find that some aspect of the data is being hidden, because it tells an inconvenient story (e.g., the researcher might choose not to refer to a particular variable). There’s a lot of variants on this, many of which are very hard to detect.\nStudy “misdesign”. Okay, this one is subtle. Basically, the issue here is that a researcher designs a study that has built-in flaws and those flaws are never reported in the paper. The data that are reported are completely real and are correctly analysed, but they are produced by a study that is actually quite wrongly put together. The researcher really wants to find a particular effect and so the study is set up in such a way as to make it “easy” to (artefactually) observe that effect. One sneaky way to do this, in case you’re feeling like dabbling in a bit of fraud yourself, is to design an experiment in which it’s obvious to the participants what they’re “supposed” to be doing, and then let reactivity work its magic for you. If you want you can add all the trappings of double blind experimentation but it won’t make a difference since the study materials themselves are subtly telling people what you want them to do. When you write up the results the fraud won’t be obvious to the reader. What’s obvious to the participant when they’re in the experimental context isn’t always obvious to the person reading the paper. Of course, the way I’ve described this makes it sound like it’s always fraud. Probably there are cases where this is done deliberately, but in my experience the bigger concern has been with unintentional misdesign. The researcher believes and so the study just happens to end up with a built in flaw, and that flaw then magically erases itself when the study is written up for publication.\nData mining & post hoc hypothesising. Another way in which the authors of a study can more or less misrepresent the data is by engaging in what’s referred to as “data mining” (see Gelman and Loken 2014, for a broader discussion of this as part of the “garden of forking paths” in statistical analysis). As we’ll discuss later, if you keep trying to analyse your data in lots of different ways, you’ll eventually find something that “looks” like a real effect but isn’t. This is referred to as “data mining”. It used to be quite rare because data analysis used to take weeks, but now that everyone has very powerful statistical software on their computers it’s becoming very common. Data mining per se isn’t “wrong”, but the more that you do it the bigger the risk you’re taking. The thing that is wrong, and I suspect is very common, is unacknowledged data mining. That is, the researcher runs every possible analysis known to humanity, finds the one that works, and then pretends that this was the only analysis that they ever conducted. Worse yet, they often “invent” a hypothesis after looking at the data to cover up the data mining. To be clear. It’s not wrong to change your beliefs after looking at the data, and to reanalyse your data using your new “post hoc” hypotheses. What is wrong (and I suspect common) is failing to acknowledge that you’ve done. If you acknowledge that you did it then other researchers are able to take your behaviour into account. If you don’t, then they can’t. And that makes your behaviour deceptive. Bad!\nPublication bias & self-censoring. Finally, a pervasive bias is “non-reporting” of negative results. This is almost impossible to prevent. Journals don’t publish every article that is submitted to them. They prefer to publish articles that find “something”. So, if 20 people run an experiment looking at whether reading Finnegans Wake causes insanity in humans, and 19 of them find that it doesn’t, which one do you think is going to get published? Obviously, it’s the one study that did find that Finnegans Wake causes insanity.7 This is an example of a publication bias. Since no-one ever published the 19 studies that didn’t find an effect, a naive reader would never know that they existed. Worse yet, most researchers “internalise” this bias and end up self-censoring their research. Knowing that negative results aren’t going to be accepted for publication, they never even try to report them. As a friend of mine says “for every experiment that you get published, you also have 10 failures”. And she’s right. The catch is, while some (maybe most) of those studies are failures for boring reasons (e.g. you stuffed something up) others might be genuine “null” results that you ought to acknowledge when you write up the “good” experiment. And telling which is which is often hard to do. A good place to start is a paper by Ioannidis (2005) with the depressing title “Why most published research findings are false”. I’d also suggest taking a look at work by Kühberger et al. (2014) presenting statistical evidence that this actually happens in psychology.\n\nThere’s probably a lot more issues like this to think about, but that’ll do to start with. What I really want to point out is the blindingly obvious truth that real world science is conducted by actual humans, and only the most gullible of people automatically assumes that everyone else is honest and impartial. Actual scientists aren’t usually that naive, but for some reason the world likes to pretend that we are, and the textbooks we usually write seem to reinforce that stereotype."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#summary",
    "href": "02-A-brief-introduction-to-research-design.html#summary",
    "title": "2  A brief introduction to research design",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nThis chapter isn’t really meant to provide a comprehensive discussion of psychological research methods. It would require another volume just as long as this one to do justice to the topic. However, in real life statistics and study design are so tightly intertwined that it’s very handy to discuss some of the key topics. In this chapter, I’ve briefly discussed the following topics:\n\nIntroduction to psychological measurement. What does it mean to operationalise a theoretical construct? What does it mean to have variables and take measurements?\nScales of measurement and types of variables. Remember that there are two different distinctions here. There’s the difference between discrete and continuous data, and there’s the difference between the four different scale types (nominal, ordinal, interval and ratio).\nAssessing the reliability of a measurement. If I measure the “same” thing twice, should I expect to see the same result? Only if my measure is reliable. But what does it mean to talk about doing the “same” thing? Well, that’s why we have different types of reliability. Make sure you remember what they are.\nThe “role” of variables: predictors and outcomes. What roles do variables play in an analysis? Can you remember the difference between predictors and outcomes? Dependent and independent variables? Etc.\nExperimental and non-experimental research designs. What makes an experiment an experiment? Is it a nice white lab coat, or does it have something to do with researcher control over variables?\nAssessing the validity of a study. Does your study measure what you want it to? How might things go wrong? And is it my imagination, or was that a very long list of possible ways in which things can go wrong?\n\nAll this should make clear to you that study design is a critical part of research methodology. I built this chapter from the classic little book by Campbell & Stanley (1963), but there are of course a large number of textbooks out there on research design. Spend a few minutes with your favourite search engine and you’ll find dozens.\n\n\n\n\nAdair, G. (1984). The hawthorne effect: A reconsideration of the methodological artifact. Journal of Applied Psychology, 69, 334–345.\n\n\nCampbell, D. T., & Stanley, J. C. (1963). Experimental and quasi-experimental designs for research. Houghton Mifflin.\n\n\nHróbjartsson, A., & Gøtzsche, P. (2010). Placebo interventions for all clinical conditions. Cochrane Database of Systematic Reviews, 1. https://doi.org//10.1002/14651858.CD003974.pub3\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Med, 2(8), 697–701.\n\n\nKahneman, D., & Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80, 237–251.\n\n\nKühberger, A., Fritz, A., & Scherndl, T. (2014). Publication bias in psychology: A diagnosis based on the correlation between effect size and sample size. Public Library of Science One, 9, 1–8.\n\n\nPfungst, O. (1911). Clever hans (the horse of mr. Von osten): A contribution to experimental animal and human psychology (C. L. Rahn, Trans.). Henry Holt.\n\n\nRosenthal, R. (1966). Experimenter effects in behavioral research. Appleton.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677–680."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#首先談談統計的心理學",
    "href": "01-Why-do-we-learn-statistics.html#首先談談統計的心理學",
    "title": "1  為什麼要學習統計",
    "section": "1.1 首先談談統計的心理學",
    "text": "1.1 首先談談統計的心理學\n先講許多第一次來上課的同學驚呆的現實，統計學在心理學課程佔有相當的份量。另外一個不令人意外的現實，是統計學很少被上過心理學課程的同學推薦。畢竟正常來說，如果是喜歡統計的同學，應該是去修統計系的課，而不是來上心理學系的課。不需要太正式的調查，正在上心理學課程的同學，有一大部分想必很不高興要學有這麼多統計。為了幫助同學渡過這段不適應，我想這本書先來談談一般人對於統計學的一些常見疑問。\n\n這個問題的很大一部分與統計學的本質有關。統計學是什麼？統計學是做什麼用的？為什麼科學家如此迷戀統計？只要你能提出這些問題，都是值得好好回答的問題。我們先從最後一個問題開始談吧。科學家集體似乎想對每件事情進行統計分析，旁人看來近乎固執。科學家確實經常使用統計學，有時候甚至忘記向一般人解釋為什麽這樣做。這是科學家之間的一種信仰，尤其是社會科學家。任何科學家沒有完成統計之前，他的發現是不會被其他科學家信任的。(新鮮人)大學生看了，可能會認為這群人都是瘋子，因為沒有科學家會花時間向一般人回答這個非常簡單的問題：\n\n為什麼當心理學家要會使用統計？為什麼科學家不能憑生活常識做研究？\n\n在某些方面，這是一個天真的問題，但大多數好問題都是這樣開始的。對此問題已經有許多有意思的回答2，不過我心中最好的回答是一個大家都懂的人性現實：人類無法充分信任自己的判斷。正是因為了解人類本身，很容易受到各種偏見、誘惑和軟弱而影響個人的判斷。在很多情況，統計學提供一種基本保障。使用”生活常識”評估證據意味著信任直覺，依靠言語推論以及使用人類原生的推理能力找出正確答案。但是大多數科學家認為這種方法不太可靠。\n\n其實好好地想一想，這很像是一個心理學家會研究的問題，既然我在心理學系工作，這似乎是個值得深入研究的好題目。真的有道理認為”靠常識”做出的研究是值得信賴的嗎？言語推論是用語言構建的，所有語言都帶有偏見 - 像是有些事情特別難說，並不一定是因為它們是錯的（例如，量子力學是一個很好的理論，但很難用言語解釋）。因為人類的”直覺”本能並不是用來解決科學問題，而是用來應付日常推論問題 - 由於生物演化速度比文化演化遲緩，我們應該說直覺是為了解決不同於我們的生活經驗的日常問題而設計的產物。科學研究的根本是理性推理，需要我們進行”歸納”，做出明智的猜测，超越用感官接收的證據，概括這個世界的樣貌。如果你認為有本事不受各種外界干擾進行理性推理，那麼我有份年薪百萬，請你去柬埔寨打工的機會想介紹給你3。哎呀，正如下一節我要說的，我们也無法解决不需要猜測的”演繹”問題，也就是那些不會受到預先存在的偏見所影響的問題。\n\n\n1.1.1 信念偏誤的詛咒\n大多數人類都很聰明。我們肯定比地球上其他物種要聰明得多（盡管可能很多人不這麽認為）。人類的思維能力是非常神奇的產物，我們似乎有能力思考和推理任何不可思議的事情。但是這並不意味著人類完美無缺。心理學家累積多年的研究已經表明，人類確實很難保持中立，公正地評估證據，而不會受到預期偏見的影響。有個很好的例子是邏輯推理中的信念偏差效應：如果你要求我判斷一個特定的論點是否在邏輯上是正確的（也就是說，如果前提是真實的，結論就是真實的），我常常會受到結論的可信度影響，即使我明知不該如此。以下是一段結論可信的有效邏輯論證：\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n再看這一段結論不可信的有效邏輯論證：\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n兩段論證的結構都是相同而且有效4。然而第二段的前提1有理由相信並不正確，所以有人會認為結論也不正確。其實無論前提的內容如何，論證的演繹有效性僅取決於前提與結論的結構。也就是說，有效論證不必然要包括真實的敘述。\n\n另一方面，無效的論證也能有讓人相信為真的結論，就像下一段論證：\n\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n\n最後來看以下這段無效演繹且結論不可信的論證：\n\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n假設人類真的完全能夠放下對於敘述真實性的預期偏見，僅憑邏輯有效性評估論點是否合理。那麼我們可以設計實驗，測試看看是否所有人都會認為有效論證是正確的，沒有人會說無效論證是正確的。我把這樣的假設製成表1.1 。\n\n\n\n\n  表1.1 判斷人類能拋開偏見進行有效論證的假設結果\n    \n         \n        結論應該為真\n        結論應該為假\n    \n    \n        論證有效\n        100\\(\\%\\) 認為“有效”\n        100\\(\\%\\) 認為“有效”\n    \n    \n        論證無效\n        0\\(\\%\\) 認為“有效”\n        0\\(\\%\\) 認為“有效”\n    \n\n假如心理學研究結果就像表內的數值（或者只是接近這樣的數值），我們可能覺得完全靠人類的直覺就能做出結論。只要是像這樣的研究結果，科學家完全可以根據他們的常識評估結果數據，不用花時間處理那堆讓很多人看不懂什麼意思的統計分析。然而，你們已經至少修過心理學概論，對於這套實驗的真正結果應該略知一二。\nEvans et al. (1983) 做了一系列探討人類如何進行邏輯推論的經典實驗。他們發現只有結論敘述符合多數人的預期偏誤(也就是信念)，實驗結果才會接近人類能做有效推論的假設(表1.2)。\n\n\n\n  表1.2 預期偏誤與論證有效性的實驗結果\n    \n         \n        結論應該為真\n        結論應該為假\n    \n    \n        論證有效\n        92\\(\\%\\) 認為“有效”\n         \n    \n    \n        論證無效\n         \n        8\\(\\%\\) 認為“有效”\n    \n\n\n雖然不夠完美，但是這樣的結果也算不錯了。不過看看另外兩個與一般人的直覺完全相反實驗情況，與表1.1的完美假設完全不同(表1.3)。\n\n\n\n  表1.3 直覺判斷與論證有效性的實驗結果\n    \n         \n        結論應該為真\n        結論應該為假\n    \n    \n        論證有效\n        92\\(\\%\\) 認為“有效”\n        46\\(\\%\\) 認為“有效”\n    \n    \n        論證無效\n        92\\(\\%\\) 認為“有效”\n        8\\(\\%\\) 認為“有效”\n    \n\n\n哎呀，這不是好解釋的結果。實驗結果顯示似乎向一般人講述一個與既有信念互相矛盾但有邏輯效力的論點時，人們很難相信這是一個強而有力的論點（只有 46% 的人會相信）。更糟的是，向一般人講述一個與既有偏見相符但沒有邏輯效力的的論點時，幾乎沒有人能看出這個論點無效（高達 92% 的人判斷錯誤！）。5\n如果仔細想想，這並不是很糟糕的結果。總體來看，一般人的表現比隨機亂猜好，大約有60％的人做出了正確的判斷（隨機亂猜應該是50％）。即使如此，如果你是一名專業的“證據鑑識人員”，有人送你一個可以提高正確決策的機率的神奇工具，比如說從 60% 到95% ，你應會欣然接受吧？幸好我們有一種工具可以做到這一點。這種工具不是魔法，而是統計學。這就是為什麼科學家喜歡使用統計的最主要原因。我們很容易「信任我們想要相信的」。所以如果我們要做到「信任資料」，就需要一些工具幫助我們控制個人偏見。而這就是統計學的用途，它能幫助我們保持推論的誠實。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#檢核辛普森悖論",
    "href": "01-Why-do-we-learn-statistics.html#檢核辛普森悖論",
    "title": "1  為什麼要學習統計",
    "section": "1.2 檢核辛普森悖論",
    "text": "1.2 檢核辛普森悖論\n接著來談一則真實故事(我想應該是真的)。1973年，美國加州大學柏克萊分校高層擔憂該年研究所新生報考及錄取的狀況。更明白的說，他們覺得被錄取的研究生呈現性別不平等的狀況(見表1.4)。\n\n\n  表1.4 柏克萊男女新生報考人數及錄取比例\n    \n         \n        申請者人數\n        通過比例\n    \n    \n        男性\n        8442\n        44\\(\\%\\)\n    \n    \n        女性\n        4321\n        35\\(\\%\\)\n    \n\n\n當年的柏克萊校方擔心被考生告上法院！因為將近13,000名學生申請入學，男女之間錄取率相差9％，這樣的差距實在太大了，不可能是巧合。而且人數如此龐大，可說是鐵一般的事實。但是如果我對你說，這些數據實際上反映了對女性考生些微的偏袒，你可能會認為我搞錯了或者有性別歧視。\n\n\n然而，實情卻有點出人意料。仔細檢視錄取資料時，有人發現了另一個版本的故事(Bickel et al., 1975)。具體地說，當校方按照學系逐一計算錄取率時，發現多數學系的女性錄取率實際上略高於男性。表1.5 顯示了六個考生最多的學系錄取情形（為了保障隱私，以下將學系名稱省略）：\n\n\n  表1.5 1973年伯克萊大學六個學系錄取學生的性別分佈\n    \n         \n         男性\n         女性\n    \n    \n        學系\n        申請者人數\n        通過比例\n        申請者人數\n        通過比例\n    \n    \n        A\n        825\n        62\\(\\%\\)\n        108\n        82\\(\\%\\)\n    \n    \n        B\n        560\n        63\\(\\%\\)\n        25\n        68\\(\\%\\)\n    \n    \n        C\n        325\n        37\\(\\%\\)\n        593\n        34\\(\\%\\)\n    \n    \n        D\n        417\n        33\\(\\%\\)\n        375\n        35\\(\\%\\)\n    \n    \n        E\n        191\n        28\\(\\%\\)\n        393\n        24\\(\\%\\)\n    \n    \n        F\n        272\n        6\\(\\%\\)\n        341\n        7\\(\\%\\)\n    \n\n\nRemarkably, most departments had a higher rate of admissions for females than for males! Yet the overall rate of admission across the university for females was lower than for males. How can this be? How can both of these statements be true at the same time?\nHere’s what’s going on. Firstly, notice that the departments are not equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get A>B>D>C>F>E (the “easy” departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C>E>D>F>A>B. In other words, what these data seem to be suggesting is that the female applicants tended to apply to “harder” departments. And in fact, if we look at Figure Figure 1.1 we see that this trend is systematic, and quite striking. This effect is known as Simpson’s paradox. It’s not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it’s real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are lots of subtle, counter-intuitive traps lying in wait for the unwary. That’s reason #2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.\nBefore leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves part of the problem. Remember that we started all this with the concern that Berkeley’s admissions processes might be unfairly biased against female applicants. When we looked at the “aggregated” data, it did seem like the university was discriminating against women, but when we “disaggregate” and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments. From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can’t dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce.\n\n\n\n\n\nFigure 1.1: 1973年柏克萊大學招生數據，取自 Bickel et al. (1975) 的圖1。圖中85個點分別代表至少接受一位女性申請入學的學系，依不分性別錄取率與女性申請入學比例繪圖。圓圈代表申請者超過40人的學系；圓圈面積代表該系申請人數佔全校申請人數的比例。十字代表申請者未達40人的學系。\n\n\n\n\nThat was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.That was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.\nIn short there are a lot of critical questions that you can’t answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a tool to help you learn about your data. No more and no less. It’s a powerful tool to that end, but there’s no substitute for careful thought."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#心理學中的統計",
    "href": "01-Why-do-we-learn-statistics.html#心理學中的統計",
    "title": "1  為什麼要學習統計",
    "section": "1.3 心理學中的統計",
    "text": "1.3 心理學中的統計\n希望前面的討論能夠清楚解釋，為何各種科學領域都要使用統計。不過你應該還是會質疑學習心理學為何要學習統計，也會納悶為何許多心理學課程內容都與統計有關。以下是我常聽到來自學生的質疑，以及我的回應…\n\n為什麼學習心理學要學這麼多統計？\n\n坦白說，有好幾個原因，其中有些原因比較值得談。最重要的原因是心理學是一門統計科學。我的意思是，我們研究的「對象」是人–真實的人是複雜的、美妙的、混亂的、還有偏激的人總令人憤怒。物理學研究的「對象」包括電子等物體，雖然物理學問題也有各種複雜的情況，但是電子沒有自己的思想。它們沒有個人意見，電子之間沒有奇怪而且任意的差異，在實驗中不會感到無聊，不會對實驗者發脾氣，然後故意搞爛實驗（我沒有做過這樣的事情唷！）。從基本面來說，心理學比物理學更難研究6。所以說，我們以心理學家的身份教你統計學，是因為你需要比物理學家更會掌握統計學。物理學裡有句老生常談，意思是”如果你的實驗要用到統計學，那應該要設計一個更好的實驗”。他們確實有底氣這樣說，因為物理學研究對象的混亂程度與社會科學家面對的相比，是令人妒恨的簡單。而且不只是心理學。大多數社會科學都非常依賴統計學。不是因為心理學家是糟糕的實驗設計者，而是因為心理學面對的多數問題很難只靠實驗設計解決。學習統計學是因為你真的真的需要它。\n\n不能將統計外包給別人做嗎？\n\n在某些情況是可以這樣，但不能全部都交給別人做。你確實不必要成為一名受過完整訓練的統計學家，也可以研究心理學，但是你需要具備一定的統計能力。在我看來，有三個原因，每個心理學研究者都應該具備基本統計能力：\n\n\n首先是最基本的原因：統計學與研究設計密不可分。如果你想成為設計心理學研究的專家，你至少需要了解統計學的基礎。(譯註：第2章就是談研究設計)\n其次，如果你想成為研究心理學的專家，那麼你就需要有能夠讀懂心理學文獻的能力，對吧？但是幾乎每篇心理學論文都有報告統計分析結果。如果你真的想徹底搞懂心理學，你就需要理解報告作者對資料做了什麼分析。也就是說你需要了解一定程度的統計學。\n第三，依靠別人做統計學有一個很實際問題：統計分析是件價格昂貴的工作。如果你曾經無聊到去查詢澳洲政府制定的大學學費標準，會發現一件有趣的事情：統計學被指定為”國家優先”項目，因此學費比任何其他學科都低得多。這是因為社會各界都需要統計學專家的協助。從心理學研究者的立場來看，我們面對的是供給遠少於需求的賣方市場！幾乎任何一個心理學研究室都能看到相同的殘酷現實，就是沒有足夠經費聘請統計專家。因此，經濟現實逼迫心理學家必須自立自強。\n\n除此之外，這些原因不只適用於從事研究的人員。如果你想成為一名應用取向的心理學家，為了掌握最新的研究進展，能夠獨立閱讀充滿各種統計報告的科學文獻也有助職涯發展。\n\n我不打算從事與心理學有關的工作、研究或臨床實務。我還需要學嗎？\n\n好吧，你快要難倒我了。總而言之，我相信統計學對你還是很重要的。對包括你在內的所有現代人類來說，統計學是很重要的基本知識。在21世紀的今天，隨處都是資料。老實說，今天要維持自已活得像現代人，基本的統計知識已經是必備生存工具了！下一節我繼續說給你聽。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#日常生活中的統計",
    "href": "01-Why-do-we-learn-statistics.html#日常生活中的統計",
    "title": "1  為什麼要學習統計",
    "section": "1.4 日常生活中的統計",
    "text": "1.4 日常生活中的統計\n\n“We are drowning in information,\nbut we are starved for knowledge”\n- Various authors, original probably John Naisbitt\n\n當我撰寫統計學講義時，我從 ABC 新聞網站找了 20 篇當時最新的新聞報導。我判斷其中 8 篇文字內容有提到統計資訊，不過有 6 篇報導內容有誤。若你想知道有什麼錯誤，最常見的錯誤是沒有報告基礎數據（例如，報導提到某個情況能觀察到 5％的人具有某種特徵 ，但沒有說明找了多少人總計出這個百分比！）。我想說的不是記者的統計素養很糟糕（雖然多數記者的統計知識真的很糟糕），而是基本的統計知識真的非常有幫助，可以幫助你瞭解別人表達的錯誤，還有是不是隨便拿些數據製造謠言(譯注：本書翻譯工程始於2022年末，有經歷疫情的同學應該對這段時間的各種不實疫情報導有感受。)。事實上，充實統計知識後，為個人帶來的最大一種改變，就是讓你更常對報紙和網路的資訊感到憤怒。之後在 Chapter 4 的 Section 4.1.5 ，你會看到實際例子。本書的後續更新版本，會繼續收集更多類似的錯誤報導。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#統計以外的研究方法",
    "href": "01-Why-do-we-learn-statistics.html#統計以外的研究方法",
    "title": "1  為什麼要學習統計",
    "section": "1.5 統計以外的研究方法",
    "text": "1.5 統計以外的研究方法\n到這裡為止，討論的研究方法大部分都是與統計學有關，你有理由相信這門課只關心和統計學有關的研究方法主題。坦白說，這麼認為並非完全錯誤，但是研究方法是比統計學更廣泛的課題。大多數研究方法課程都會涵蓋研究設計相關的實用主題，特別是進行與人類有關的研究會遇到的問題。但是，約有 99% 的學生害怕課程中統計的部分。這本書之所以著重統計學，是希望我能讓你相信統計學很重要，更重要的是，統計並不可怕。這也就是為什麼，大多數初級研究方法課程都會提到統計學。但是不是因為教師都是壞蛋，其實恰恰相反。各種入門課程之所以重視統計學，是因為同學們需要在學習各種研究方法之前，要充分了解統計學。為什麼？因為你在任何課程的所有作業都要靠統計方法才能完成，比起其他方法學工具，作業中的統計使用頻率經常居冠。心理系的作業通常不需要你從頭開始設計自己的研究（若要從頭開始，就需要了解很多關於研究設計的知識），而是要你分析和解釋別人設計的研究所收集的數據（這是你需要統計學的狀況）。就作業安排的意義來說，為了幫助你在其他課程學得好，統計學是需要優先學習的課程。\n但是要注意，“優先”與”重要”是不同的概念 - 雖然兩者都是關鍵。我想強調的是研究設計和數據分析同樣重要，本書確實花了相當多篇幅在這些主題。然而，雖然統計學具有某種普遍性，提供了一套對許多類型的心理學研究都有用的核心工具，但是不是所有研究方法都會運用統計。有些研究設計的一般原則是每個研究者都應該注意，但是許多研究設計非常特殊，並且只有在特定研究領域才會使用。基礎統計和研究方法課程考慮到細節的重要性，不一定會介紹這些特殊的研究設計。\n\n\n\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187, 398–404.\n\n\nEvans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On the conflict between logic and belief in syllogistic reasoning. Memory and Cognition, 11, 295–306."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html",
    "href": "01-Why-do-we-learn-statistics.html",
    "title": "1  為什麼要學習統計",
    "section": "",
    "text": "\\[ \\]"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html",
    "href": "02-A-brief-introduction-to-research-design.html",
    "title": "2  簡易入門研究設計",
    "section": "",
    "text": "“To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.”\n– Sir Ronald Fisher1\nIn this chapter, we’re going to start thinking about the basic ideas that go into designing a study, collecting data, checking whether your data collection works, and so on. It won’t give you enough information to allow you to design studies of your own, but it will give you a lot of the basic tools that you need to assess the studies done by other people. However, since the focus of this book is much more on data analysis than on data collection, I’m only giving a very brief overview. Note that this chapter is “special” in two ways. Firstly, it’s much more psychology specific than the later chapters. Secondly, it focuses much more heavily on the scientific problem of research methodology, and much less on the statistical problem of data analysis. Nevertheless, the two problems are related to one another, so it’s traditional for stats textbooks to discuss the problem in a little detail. This chapter relies heavily on Campbell & Stanley (1963) and Stevens (1946) for the discussion of scales of measurement."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#變項的角色-預測變項與結果變項",
    "href": "02-A-brief-introduction-to-research-design.html#變項的角色-預測變項與結果變項",
    "title": "2  研究設計入門",
    "section": "2.4 變項的”角色”: 預測變項與結果變項",
    "text": "2.4 變項的”角色”: 預測變項與結果變項\nI’ve got one last piece of terminology that I need to explain to you before moving away from variables. Normally, when we do some research we end up with lots of different variables. Then, when we analyse our data, we usually try to explain some of the variables in terms of some of the other variables. It’s important to keep the two roles “thing doing the explaining” and “thing being explained” distinct. So let’s be clear about this now. First, we might as well get used to the idea of using mathematical symbols to describe variables, since it’s going to happen over and over again. Let’s denote the “to be explained” variable \\(Y\\), and denote the variables “doing the explaining” as \\(X_1 , X_2\\), etc.\nWhen we are doing an analysis we have different names for \\(X\\) and \\(Y\\), since they play different roles in the analysis. The classical names for these roles are independent variable (IV) and dependent variable (DV). The IV is the variable that you use to do the explaining (i.e., \\(X\\)) and the DV is the variable being explained (i.e.,$Y $). The logic behind these names goes like this: if there really is a relationship between \\(X\\) and \\(Y\\) then we can say that \\(Y\\)depends on \\(X\\), and if we have designed our study “properly” then \\(X\\) isn’t dependent on anything else. However, I personally find those names horrible. They’re hard to remember and they’re highly misleading because (a) the IV is never actually “independent of everything else”, and (b) if there’s no relationship then the DV doesn’t actually depend on the IV. And in fact, because I’m not the only person who thinks that IV and DV are just awful names, there are a number of alternatives that I find more appealing. The terms that I’ll use in this book are predictors and outcomes. The idea here is that what you’re trying to do is use \\(X\\) (the predictors) to make guesses about \\(Y\\) (the outcomes).12 This is summarised in Table 2.1.\n\n\n\n\nTable 2.1:  Variable distinctions \n\nrole of the variableclassical namemodern name\n\n\"to be explained\"dependent variable (DV)outcome\n\n\"to do the explaining\"independent variable (IV)predictor"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#實驗與非實驗研究",
    "href": "02-A-brief-introduction-to-research-design.html#實驗與非實驗研究",
    "title": "2  研究設計入門",
    "section": "2.5 實驗與非實驗研究",
    "text": "2.5 實驗與非實驗研究\nOne of the big distinctions that you should be aware of is the distinction between “experimental research” and “non-experimental research”. When we make this distinction, what we’re really talking about is the degree of control that the researcher exercises over the people and events in the study.\n\n2.5.1 實驗研究\nThe key feature of experimental research is that the researcher controls all aspects of the study, especially what participants experience during the study. In particular, the researcher manipulates or varies the predictor variables (IVs) but allows the outcome variable (DV) to vary naturally. The idea here is to deliberately vary the predictors (IVs) to see if they have any causal effects on the outcomes. Moreover, in order to ensure that there’s no possibility that something other than the predictor variables is causing the outcomes, everything else is kept constant or is in some other way “balanced”, to ensure that they have no effect on the results. In practice, it’s almost impossible to think of everything else that might have an influence on the outcome of an experiment, much less keep it constant. The standard solution to this is randomisation. That is, we randomly assign people to different groups, and then give each group a different treatment (i.e., assign them different values of the predictor variables). We’ll talk more about randomisation later, but for now it’s enough to say that what randomisation does is minimise (but not eliminate) the possibility that there are any systematic difference between groups.\nLet’s consider a very simple, completely unrealistic and grossly unethical example. Suppose you wanted to find out if smoking causes lung cancer. One way to do this would be to find people who smoke and people who don’t smoke and look to see if smokers have a higher rate of lung cancer. This is not a proper experiment, since the researcher doesn’t have a lot of control over who is and isn’t a smoker. And this really matters. For instance, it might be that people who choose to smoke cigarettes also tend to have poor diets, or maybe they tend to work in asbestos mines, or whatever. The point here is that the groups (smokers and non-smokers) actually differ on lots of things, not just smoking. So it might be that the higher incidence of lung cancer among smokers is caused by something else, and not by smoking per se. In technical terms these other things (e.g. diet) are called “confounders”, and we’ll talk about those in just a moment.\nIn the meantime, let’s consider what a proper experiment might look like. Recall that our concern was that smokers and non-smokers might differ in lots of ways. The solution, as long as you have no ethics, is to control who smokes and who doesn’t. Specifically, if we randomly divide young non-smokers into two groups and force half of them to become smokers, then it’s very unlikely that the groups will differ in any respect other than the fact that half of them smoke. That way, if our smoking group gets cancer at a higher rate than the non-smoking group, we can feel pretty confident that (a) smoking does cause cancer and (b) we’re murderers.\n\n\n2.5.2 非實驗研究\nNon-experimental research is a broad term that covers “any study in which the researcher doesn’t have as much control as they do in an experiment”. Obviously, control is something that scientists like to have, but as the previous example illustrates there are lots of situations in which you can’t or shouldn’t try to obtain that control. Since it’s grossly unethical (and almost certainly criminal) to force people to smoke in order to find out if they get cancer, this is a good example of a situation in which you really shouldn’t try to obtain experimental control. But there are other reasons too. Even leaving aside the ethical issues, our “smoking experiment” does have a few other issues. For instance, when I suggested that we “force” half of the people to become smokers, I was talking about starting with a sample of non-smokers, and then forcing them to become smokers. While this sounds like the kind of solid, evil experimental design that a mad scientist would love, it might not be a very sound way of investigating the effect in the real world. For instance, suppose that smoking only causes lung cancer when people have poor diets, and suppose also that people who normally smoke do tend to have poor diets. However, since the “smokers” in our experiment aren’t “natural” smokers (i.e., we forced non-smokers to become smokers, but they didn’t take on all of the other normal, real life characteristics that smokers might tend to possess) they probably have better diets. As such, in this silly example they wouldn’t get lung cancer and our experiment will fail, because it violates the structure of the “natural” world (the technical name for this is an “artefactual” result).\nOne distinction worth making between two types of non-experimental research is the difference between quasi-experimental research and case studies. The example I discussed earlier, in which we wanted to examine incidence of lung cancer among smokers and non-smokers without trying to control who smokes and who doesn’t, is a quasi-experimental design. That is, it’s the same as an experiment but we don’t control the predictors (IVs). We can still use statistics to analyse the results, but we have to be a lot more careful and circumspect.\nThe alternative approach, case studies, aims to provide a very detailed description of one or a few instances. In general, you can’t use statistics to analyse the results of case studies and it’s usually very hard to draw any general conclusions about “people in general” from a few isolated examples. However, case studies are very useful in some situations. Firstly, there are situations where you don’t have any alternative. Neuropsychology has this issue a lot. Sometimes, you just can’t find a lot of people with brain damage in a specific brain area, so the only thing you can do is describe those cases that you do have in as much detail and with as much care as you can. However, there’s also some genuine advantages to case studies. Because you don’t have as many people to study you have the ability to invest lots of time and effort trying to understand the specific factors at play in each case. This is a very valuable thing to do. As a consequence, case studies can complement the more statistically-oriented approaches that you see in experimental and quasi-experimental designs. We won’t talk much about case studies in this book, but they are nevertheless very valuable tools!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#研究的效度",
    "href": "02-A-brief-introduction-to-research-design.html#研究的效度",
    "title": "2  研究設計入門",
    "section": "2.6 研究的效度",
    "text": "2.6 研究的效度\nMore than any other thing, a scientist wants their research to be “valid”. The conceptual idea behind validity is very simple. Can you trust the results of your study? If not, the study is invalid. However, whilst it’s easy to state, in practice it’s much harder to check validity than it is to check reliability. And in all honesty, there’s no precise, clearly agreed upon notion of what validity actually is. In fact, there are lots of different kinds of validity, each of which raises it’s own issues. And not all forms of validity are relevant to all studies. I’m going to talk about five different types of validity:\n\nInternal validity\nExternal validity\nConstruct validity\nFace validity\nEcological validity\n\nFirst, a quick guide as to what matters here. (1) Internal and external validity are the most important, since they tie directly to the fundamental question of whether your study really works. (2) Construct validity asks whether you’re measuring what you think you are. (3) Face validity isn’t terribly important except insofar as you care about “appearances”. (4) Ecological validity is a special case of face validity that corresponds to a kind of appearance that you might care about a lot.\n\n2.6.1 內在效度\nInternal validity refers to the extent to which you are able draw the correct conclusions about the causal relationships between variables. It’s called “internal” because it refers to the relationships between things “inside” the study. Let’s illustrate the concept with a simple example. Suppose you’re interested in finding out whether a university education makes you write better. To do so, you get a group of first year students, ask them to write a 1000 word essay, and count the number of spelling and grammatical errors they make. Then you find some third-year students, who obviously have had more of a university education than the first-years, and repeat the exercise. And let’s suppose it turns out that the third-year students produce fewer errors. And so you conclude that a university education improves writing skills. Right? Except that the big problem with this experiment is that the third-year students are older and they’ve had more experience with writing things. So it’s hard to know for sure what the causal relationship is. Do older people write better? Or people who have had more writing experience? Or people who have had more education? Which of the above is the true cause of the superior performance of the third-years? Age? Experience? Education? You can’t tell. This is an example of a failure of internal validity, because your study doesn’t properly tease apart the causal relationships between the different variables.\n\n\n2.6.2 外在效度\nExternal validity relates to the generalisability or applicability of your findings. That is, to what extent do you expect to see the same pattern of results in “real life” as you saw in your study. To put it a bit more precisely, any study that you do in psychology will involve a fairly specific set of questions or tasks, will occur in a specific environment, and will involve participants that are drawn from a particular subgroup (disappointingly often it is college students!). So, if it turns out that the results don’t actually generalise or apply to people and situations beyond the ones that you studied, then what you’ve got is a lack of external validity.\nThe classic example of this issue is the fact that a very large proportion of studies in psychology will use undergraduate psychology students as the participants. Obviously, however, the researchers don’t care only about psychology students. They care about people in general. Given that, a study that uses only psychology students as participants always carries a risk of lacking external validity. That is, if there’s something “special” about psychology students that makes them different to the general population in some relevant respect, then we may start worrying about a lack of external validity.\nThat said, it is absolutely critical to realise that a study that uses only psychology students does not necessarily have a problem with external validity. I’ll talk about this again later, but it’s such a common mistake that I’m going to mention it here. The external validity of a study is threatened by the choice of population if (a) the population from which you sample your participants is very narrow (e.g., psychology students), and (b) the narrow population that you sampled from is systematically different from the general population in some respect that is relevant to the psychological phenomenon that you intend to study. The italicised part is the bit that lots of people forget. It is true that psychology undergraduates differ from the general population in lots of ways, and so a study that uses only psychology students may have problems with external validity. However, if those differences aren’t very relevant to the phenomenon that you’re studying, then there’s nothing to worry about. To make this a bit more concrete here are two extreme examples:\n\nYou want to measure “attitudes of the general public towards psychotherapy”, but all of your participants are psychology students. This study would almost certainly have a problem with external validity.\nYou want to measure the effectiveness of a visual illusion, and your participants are all psychology students. This study is unlikely to have a problem with external validity\n\nHaving just spent the last couple of paragraphs focusing on the choice of participants, since that’s a big issue that everyone tends to worry most about, it’s worth remembering that external validity is a broader concept. The following are also examples of things that might pose a threat to external validity, depending on what kind of study you’re doing:\n\nPeople might answer a “psychology questionnaire” in a manner that doesn’t reflect what they would do in real life.\nYour lab experiment on (say) “human learning” has a different structure to the learning problems people face in real life.\n\n\n\n2.6.3 建構效度\nConstruct validity is basically a question of whether you’re measuring what you want to be measuring. A measurement has good construct validity if it is actually measuring the correct theoretical construct, and bad construct validity if it doesn’t. To give a very simple (if ridiculous) example, suppose I’m trying to investigate the rates with which university students cheat on their exams. And the way I attempt to measure it is by asking the cheating students to stand up in the lecture theatre so that I can count them. When I do this with a class of 300 students 0 people claim to be cheaters. So I therefore conclude that the proportion of cheaters in my class is 0%. Clearly this is a bit ridiculous. But the point here is not that this is a very deep methodological example, but rather to explain what construct validity is. The problem with my measure is that while I’m trying to measure “the proportion of people who cheat” what I’m actually measuring is “the proportion of people stupid enough to own up to cheating, or bloody minded enough to pretend that they do”. Obviously, these aren’t the same thing! So my study has gone wrong, because my measurement has very poor construct validity.\n\n\n2.6.4 表面效度\nFace validity simply refers to whether or not a measure “looks like” it’s doing what it’s supposed to, nothing more. If I design a test of intelligence, and people look at it and they say “no, that test doesn’t measure intelligence”, then the measure lacks face validity. It’s as simple as that. Obviously, face validity isn’t very important from a pure scientific perspective. After all, what we care about is whether or not the measure actually does what it’s supposed to do, not whether it looks like it does what it’s supposed to do. As a consequence, we generally don’t care very much about face validity. That said, the concept of face validity serves three useful pragmatic purposes:\n\nSometimes, an experienced scientist will have a “hunch” that a particular measure won’t work. While these sorts of hunches have no strict evidentiary value, it’s often worth paying attention to them. Because often times people have knowledge that they can’t quite verbalise, so there might be something to worry about even if you can’t quite say why. In other words, when someone you trust criticises the face validity of your study, it’s worth taking the time to think more carefully about your design to see if you can think of reasons why it might go awry. Mind you, if you don’t find any reason for concern, then you should probably not worry. After all, face validity really doesn’t matter very much.\nOften (very often), completely uninformed people will also have a “hunch” that your research is crap. And they’ll criticise it on the internet or something. On close inspection you may notice that these criticisms are actually focused entirely on how the study “looks”, but not on anything deeper. The concept of face validity is useful for gently explaining to people that they need to substantiate their arguments further.\nExpanding on the last point, if the beliefs of untrained people are critical (e.g., this is often the case for applied research where you actually want to convince policy makers of something or other) then you have to care about face validity. Simply because, whether you like it or not, a lot of people will use face validity as a proxy for real validity. If you want the government to change a law on scientific psychological grounds, then it won’t matter how good your studies “really” are. If they lack face validity you’ll find that politicians ignore you. Of course, it’s somewhat unfair that policy often depends more on appearance than fact, but that’s how things go.\n\n\n\n2.6.5 生態效度\nEcological validity is a different notion of validity, which is similar to external validity, but less important. The idea is that, in order to be ecologically valid, the entire set up of the study should closely approximate the real world scenario that is being investigated. In a sense, ecological validity is a kind of face validity. It relates mostly to whether the study “looks” right, but with a bit more rigour to it. To be ecologically valid the study has to look right in a fairly specific way. The idea behind it is the intuition that a study that is ecologically valid is more likely to be externally valid. It’s no guarantee, of course. But the nice thing about ecological validity is that it’s much easier to check whether a study is ecologically valid than it is to check whether a study is externally valid. A simple example would be eyewitness identification studies. Most of these studies tend to be done in a university setting, often with a fairly simple array of faces to look at, rather than a line up. The length of time between seeing the “criminal” and being asked to identify the suspect in the “line up” is usually shorter. The “crime” isn’t real so there’s no chance of the witness being scared, and there are no police officers present so there’s not as much chance of feeling pressured. These things all mean that the study definitely lacks ecological validity. They might (but might not) mean that it also lacks external validity."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#本章小結",
    "href": "02-A-brief-introduction-to-research-design.html#本章小結",
    "title": "2  研究設計入門",
    "section": "2.8 本章小結",
    "text": "2.8 本章小結\nThis chapter isn’t really meant to provide a comprehensive discussion of psychological research methods. It would require another volume just as long as this one to do justice to the topic. However, in real life statistics and study design are so tightly intertwined that it’s very handy to discuss some of the key topics. In this chapter, I’ve briefly discussed the following topics:\n\n[Introduction to psychological measurement]. What does it mean to operationalise a theoretical construct? What does it mean to have variables and take measurements?\n[Scales of measurement] and types of variables. Remember that there are two different distinctions here. There’s the difference between discrete and continuous data, and there’s the difference between the four different scale types (nominal, ordinal, interval and ratio).\n[Assessing the reliability of a measurement]. If I measure the “same” thing twice, should I expect to see the same result? Only if my measure is reliable. But what does it mean to talk about doing the “same” thing? Well, that’s why we have different types of reliability. Make sure you remember what they are.\n[The “role” of variables: predictors and outcomes]. What roles do variables play in an analysis? Can you remember the difference between predictors and outcomes? Dependent and independent variables? Etc.\n[Experimental and non-experimental research] designs. What makes an experiment an experiment? Is it a nice white lab coat, or does it have something to do with researcher control over variables?\n[Assessing the validity of a study]. Does your study measure what you want it to? How might things go wrong? And is it my imagination, or was that a very long list of possible ways in which things can go wrong?\n\nAll this should make clear to you that study design is a critical part of research methodology. I built this chapter from the classic little book by Campbell & Stanley (1963), but there are of course a large number of textbooks out there on research design. Spend a few minutes with your favourite search engine and you’ll find dozens.\n\n\n\n\nAdair, G. (1984). The hawthorne effect: A reconsideration of the methodological artifact. Journal of Applied Psychology, 69, 334–345.\n\n\nCampbell, D. T., & Stanley, J. C. (1963). Experimental and quasi-experimental designs for research. Houghton Mifflin.\n\n\nHróbjartsson, A., & Gøtzsche, P. (2010). Placebo interventions for all clinical conditions. Cochrane Database of Systematic Reviews, 1. https://doi.org//10.1002/14651858.CD003974.pub3\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Med, 2(8), 697–701.\n\n\nKahneman, D., & Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80, 237–251.\n\n\nKühberger, A., Fritz, A., & Scherndl, T. (2014). Publication bias in psychology: A diagnosis based on the correlation between effect size and sample size. Public Library of Science One, 9, 1–8.\n\n\nPfungst, O. (1911). Clever hans (the horse of mr. Von osten): A contribution to experimental animal and human psychology (C. L. Rahn, Trans.). Henry Holt.\n\n\nRosenthal, R. (1966). Experimenter effects in behavioral research. Appleton.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677–680."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html",
    "href": "03-Getting-started-with-jamovi.html",
    "title": "3  Getting started with jamovi",
    "section": "",
    "text": "Robots are nice to work with.\n– Roger Zelazny1\nIn this chapter I’ll discuss how to get started in jamovi. I’ll briefly talk about how to download and install jamovi, but most of the chapter will be focused on getting you started with finding your way around the jamovi GUI. Our goal in this chapter is not to learn any statistical concepts: we’re just trying to learn the basics of how jamovi works and get comfortable interacting with the system. To do this we’ll spend a bit of time looking at datasets and variables. In doing so, you’ll get a bit of a feel for what it’s like to work in jamovi.\nHowever, before going into any of the specifics, it’s worth talking a little about why you might want to use jamovi at all. Given that you’re reading this you’ve probably got your own reasons. However, if those reasons are “because that’s what my stats class uses”, it might be worth explaining a little why your lecturer has chosen to use jamovi for the class. Of course, I don’t really know why other people choose jamovi so I’m really talking about why I use it.\nThose are the main reasons I use jamovi. It’s not without its flaws, though. It’s relatively new2 so there is not a huge set of textbooks and other resources to support it, and it has a few annoying quirks that we’re all pretty much stuck with, but on the whole I think the strengths outweigh the weakness; more so than any other option I’ve encountered so far."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#installing-jamovi",
    "href": "03-Getting-started-with-jamovi.html#installing-jamovi",
    "title": "3  Getting started with jamovi",
    "section": "3.1 Installing jamovi",
    "text": "3.1 Installing jamovi\nOkay, enough with the sales pitch. Let’s get started. Just as with any piece of software, jamovi needs to be installed on a “computer”, which is a magical box that does cool things and delivers free ponies. Or something along those lines; I may be confusing computers with the iPad marketing campaigns. Anyway, jamovi is freely distributed online and you can download it from the jamovi homepage, which is: https://www.jamovi.org/\nAt the top of the page, under the heading “Download”, you’ll see separate links for Windows users, Mac users, and Linux users. If you follow the relevant link you’ll see that the online instructions are pretty self-explanatory. As of this writing, the current version of jamovi is 2.3, but they usually issue updates every few months, so you’ll probably have a newer version.3\n\n3.1.1 Starting up jamovi\nOne way or another, regardless of what operating system you’re using, it’s time to open jamovi and get started. When first starting jamovi you will be presented with a user interface which looks something like Figure 3.1.\n\n\n\n\n\nFigure 3.1: jamovi starts up!\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions and this can be dragged to the left or the right to change their sizes.\nIt is possible to simply begin typing values into the jamovi spreadsheet as you would in any other spreadsheet software. Alternatively, existing data sets in the CSV (.csv) file format can be opened in jamovi. Additionally, you can easily import SPSS, SAS, Stata and JASP files directly into jamovi. To open a file select the File tab (three horizontal lines signify this tab) at the top left hand corner, select ‘Open’ and then choose from the files listed on ‘Browse’ depending on whether you want to open an example or a file stored on your computer."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#analyses",
    "href": "03-Getting-started-with-jamovi.html#analyses",
    "title": "3  Getting started with jamovi",
    "section": "3.2 Analyses",
    "text": "3.2 Analyses\nAnalyses can be selected from the analysis ribbon or menu along the top. Selecting an analysis will present an ‘options panel’ for that particular analysis, allowing you to assign different variables to different parts of the analysis, and select different options. At the same time, the results for the analysis will appear in the right ‘Results panel’ and will update in real-time as you make changes to the options.\nWhen you have the analysis set up correctly you can dismiss the analysis options by clicking the arrow to the top right of the optional panel. If you wish to return to these options, you can click on the results that were produced. In this way, you can return to any analysis that you (or say, a colleague) created earlier.\nIf you decide you no longer need a particular analysis, you can remove it with the results context menu. Right-clicking on the analysis results will bring up a menu and by selecting ‘Analysis’ and then ‘Remove’ the analysis can be removed. But more on this later. First, let’s take a more detailed look at the spreadsheet view."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#the-spreadsheet",
    "href": "03-Getting-started-with-jamovi.html#the-spreadsheet",
    "title": "3  Getting started with jamovi",
    "section": "3.3 The spreadsheet",
    "text": "3.3 The spreadsheet\nIn jamovi data is represented in a spreadsheet with each column representing a ‘variable’ and each row representing a ‘case’ or ‘participant’.\n\n3.3.1 Variables\nThe most commonly used variables in jamovi are ‘Data Variables’, these variables simply contain data either loaded from a data file, or ‘typed in’ by the user. Data variables can be one of several measurement levels (Figure 3.2).\n\n\n\n\n\nFigure 3.2: measurement levels\n\n\n\n\nThese levels are designated by the symbol in the header of the variable’s column. The ID variable type is unique to jamovi. It’s intended for variables that contain identifiers that you would almost never want to analyse. For example, a persons name, or a participant ID. Specifying an ID variable type can improve performance when interacting with very large data sets.\nNominal variables are for categorical variables which are text labels, for example a column called Gender with the values Male and Female would be nominal. So would a person’s name. Nominal variable values can also have a numeric value. These variables are used most often when importing data which codes values with numbers rather than text. For example, a column in a dataset may contain the values 1 for males, and 2 for females. It is possible to add nice ‘human-readable’ labels to these values with the variable editor (more on this later).\nOrdinal variables are like Nominal variables, except the values have a specific order. An example is a Likert scale with 3 being ‘strongly agree’ and -3 being ‘strongly disagree’.\nContinuous variables are variables which exist on a continuous scale. Examples might be height or weight. This is also referred to as ‘Interval’ or ‘Ratio scale’.\nIn addition, you can also specify different data types: variables have a data type of either ‘Text’, ‘Integer’ or ‘Decimal’.\nWhen starting with a blank spreadsheet and typing values in the variable type will change automatically depending on the data you enter. This is a good way to get a feel for which variable types go with which sorts of data. Similarly, when opening a data file jamovi will try and guess the variable type from the data in each column. In both cases this automatic approach may not be correct, and it may be necessary to manually specify the variable type with the variable editor.\nThe variable editor can be opened by selecting ‘Setup’ from the data tab or by double-clicking on the variable column header. The variable editor allows you to change the name of the variable and, for data variables, the variable type, the order of the levels, and the label displayed for each level. Changes can be applied by clicking the ‘tick’ to the top right. The variable editor can be dismissed by clicking the ‘Hide’ arrow.\nNew variables can be inserted or appended to the data set using the ‘add’ button from the data ribbon. The ‘add’ button also allows the addition of computed variables.\n\n\n3.3.2 Computed variables\nComputed Variables are those which take their value by performing a computation on other variables. Computed Variables can be used for a range of purposes, including log transforms, z-scores, sum-scores, negative scoring and means.\nComputed variables can be added to the data set with the ‘add’ button available on the data tab. This will produce a formula box where you can specify the formula. The usual arithmetic operators are available. Some examples of formulas are:\nA + B\nLOG10(len)\nMEAN(A, B)\n(len - VMEAN(len)) / VSTDEV(len)\nIn order, these are the sum of A and B, a log (base 10) transform of len, the mean of A and B, and the z-score of the variable len4. Figure 3.3 shows the jamovi screen for the new variable computed as the z-score of len (from the ‘Tooth Growth’ example data set).\n\n\n\n\n\nFigure 3.3: A newly computed variable, the z-score of ‘dose’\n\n\n\n\n\n3.3.2.1 V-functions\nSeveral functions are already available in jamovi and available from the drop down box labelled fx. A number of functions appear in pairs, one prefixed with a V and the other not. V functions perform their calculation on a variable as a whole, where as non-V functions perform their calculation row by row. For example, MEAN(A, B) will produce the mean of A and B for each row. Where as VMEAN(A) gives the mean of all the values in A.\n\n\n\n3.3.3 Copy and Paste\njamovi produces nice American Psychological Association (APA) formatted tables and attractive plots. It is often useful to be able to copy and paste these, perhaps into a Word document, or into an email to a colleague. To copy results right click on the object of interest and from the menu select exactly what you want to copy. The menu allows you to choose to copy only the image or the entire analysis. Selecting “copy” copies the content to the clipboard and this can be pasted into other programs in the usual way. You can practice this later on when we do some analyses.\n\n\n3.3.4 Syntax mode\njamovi also provides an “R Syntax Mode”. In this mode jamovi produces equivalent R code for each analysis. To change to syntax mode, select the Application menu to the top right of jamovi (a button with three vertical dots) and click the “Syntax mode” checkbox there. You can turn off syntax mode by clicking this a second time.\nIn syntax mode analyses continue to operate as before but now they produce R syntax, and ‘ascii output’ like an R session. Like all results objects in jamovi, you can right click on these items (including the R syntax) and copy and paste them, for example into an R session. At present, the provided R syntax does not include the data import step and so this must be performed manually in R. There are many resources explaining how to import data into R and if you are interested we recommend you take a look at these; just search on the interweb."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#loading-data-in-jamovi",
    "href": "03-Getting-started-with-jamovi.html#loading-data-in-jamovi",
    "title": "3  Getting started with jamovi",
    "section": "3.4 Loading data in jamovi",
    "text": "3.4 Loading data in jamovi\nThere are several different types of files that are likely to be relevant to us when doing data analysis. There are two in particular that are especially important from the perspective of this book:\n\njamovi files are those with a .omv file extension. This is the standard kind of file that jamovi uses to store data, and variables and analyses.\nComma separated value (csv) files are those with a .csv file extension. These are just regular old text files and they can be opened with many different software programs. It’s quite typical for people to store data in csv files, precisely because they’re so simple.\n\nThere are also several other kinds of data file that you might want to import into jamovi. For instance, you might want to open Microsoft Excel spreadsheets (.xls files), or data files that have been saved in the native file formats for other statistics software, such as SPSS or SAS. Whichever file formats you are using, it’s a good idea to create a folder or folders especially for your jamovi data sets and analyses and to make sure you keep these backed up regularly.\n\n3.4.1 Importing data from csv files\nOne quite commonly used data format is the humble “comma separated value” file, also called a csv file, and usually bearing the file extension .csv. csv files are just plain old-fashioned text files and what they store is basically just a table of data. This is illustrated in Figure 3.4, which shows a file called booksales.csv that I’ve created. As you can see, each row represents the book sales data for one month. The first row doesn’t contain actual data though, it has the names of the variables.\n\n\n\n\n\nFigure 3.4: The booksales.csv data file. On the left I have opened the file using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas\n\n\n\n\nIt’s easy to open csv files in jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. An example of what it looks like on a Mac is shown in Figure 3.5. I’m assuming that you’re familiar with your own computer, so you should have no problem finding the csv file that you want to import! Find the one you want, then click on the “Open” button.\n\n\n\n\n\nFigure 3.5: A dialog box on a Mac asking you to select the csv file jamovi should try to import. Mac users will recognise this immediately, it’s the usual way in which a Mac asks you to find a file. Windows users won’t see this, instead they’ll see the usual explorer window that Windows always gives you when it wants you to select a file\n\n\n\n\nThere are a few things that you can check to make sure that the data gets imported correctly:\n\nHeading. Does the first row of the file contain the names for each variable - a ‘header’ row? The booksales.csv file has a header, so that’s a yes.\nDecimal. What character is used to specify the decimal point? In English speaking countries this is almost always a period (i.e., .). That’s not universally true though, many European countries use a comma.\nQuote. What character is used to denote a block of text? That’s usually going to be a double quote mark (“). It is for the booksales.csv file."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#importing-unusual-data-files",
    "href": "03-Getting-started-with-jamovi.html#importing-unusual-data-files",
    "title": "3  Getting started with jamovi",
    "section": "3.5 Importing unusual data files",
    "text": "3.5 Importing unusual data files\nThroughout this book I’ve assumed that your data are stored as a jamovi .omv file or as a “properly” formatted csv file. However, in real life that’s not a terribly plausible assumption to make so I’d better talk about some of the other possibilities that you might run into.\n\n3.5.1 Loading data from text files\nThe first thing I should point out is that if your data are saved as a text file but aren’t quite in the proper csv format then there’s still a pretty good chance that jamovi will be able to open it. You just need to try it and see if it works. Sometimes though you will need to change some of the formatting. The ones that I’ve often found myself needing to change are:\n\nheader. A lot of the time when you’re storing data as a csv file the first row actually contains the column names and not data. If that’s not true then it’s a good idea to open up the csv file in a spreadsheet programme such as Open Office and add the header row manually.\nsep. As the name “comma separated value” indicates, the values in a row of a csv file are usually separated by commas. This isn’t universal, however. In Europe the decimal point is typically written as , instead of . and as a consequence it would be somewhat awkward to use , as the separator. Therefore it is not unusual to use ; instead of , as the separator. At other times, I’ve seen a TAB character used.\nquote. It’s conventional in csv files to include a quoting character for textual data. As you can see by looking at the booksales.csv file, this is usually a double quote character, “. But sometimes there is no quoting character at all, or you might see a single quote mark ’ used instead.\nskip. It’s actually very common to receive CSV files in which the first few rows have nothing to do with the actual data. Instead, they provide a human readable summary of where the data came from, or maybe they include some technical info that doesn’t relate to the data.\nmissing values. Often you’ll get given data with missing values. For one reason or another, some entries in the table are missing. The data file needs to include a “special” value to indicate that the entry is missing. By default jamovi assumes that this value is 995, for both numeric and text data, so you should make sure that, where necessary, all missing values in the csv file are replaced with 99 (or -9999; whichever you choose) before opening / importing the file into jamovi. Once you have opened / imported the file into jamovi all the missing values are converted to blank or greyed out cells in the jamovi spreadsheet view. You can also change the missing value for each variable as an option in the Data - Setup view.\n\n\n\n3.5.2 Loading data from SPSS (and other statistics packages)\nThe commands listed above are the main ones we’ll need for data files in this book. But in real life we have many more possibilities. For example, you might want to read data files in from other statistics programs. Since SPSS is probably the most widely used statistics package in psychology, it’s worth mentioning that jamovi can also import SPSS data files (file extension .sav). Just follow the instructions above for how to open a csv file, but this time navigate to the .sav file you want to import. For SPSS files, jamovi will regard all values as missing if they are regarded as “system missing” files in SPSS. The ‘Default missings’ value does not seem to work as expected when importing SPSS files, so be aware of this - you might need another step: import the SPSS file into jamovi, then export as a csv file before re-opening in jamovi.6\nAnd that’s pretty much it, at least as far as SPSS goes. As far as other statistical software goes, jamovi can also directly open / import SAS and STATA files.\n\n\n3.5.3 Loading Excel files\nA different problem is posed by Excel files. Despite years of yelling at people for sending data to me encoded in a proprietary data format, I get sent a lot of Excel files. The way to handle Excel files is to open them up first in Excel or another spreadsheet programme that can handle Excel files, and then export the data as a csv file before opening / importing the csv file into jamovi."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#sec-Changing-data-from-one-level-to-another",
    "href": "03-Getting-started-with-jamovi.html#sec-Changing-data-from-one-level-to-another",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.6 更動資料變項性質",
    "text": "3.6 更動資料變項性質\n為了方便排序數值，或是匯入的資料型態不正確，你會想要手動改變變項級別。像是匯入後的數字被判定為名義變項；日期被當成一般文字；受測者編號被當成連續變項。這些時候你會想把變項型態壓製成真正的樣子。\n這時請擅用”測量級別設定面板” (參考 Figure 3.2 )，改變“Measurement Type”的選項，調整變項為你要的尺度。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#installing-add-on-modules-into-jamovi",
    "href": "03-Getting-started-with-jamovi.html#installing-add-on-modules-into-jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.7 Installing add-on modules into jamovi",
    "text": "3.7 Installing add-on modules into jamovi\nA really great feature of jamovi is the ability to install add-on modules from the jamovi library. These add-on modules have been developed by the jamovi community, i.e., jamovi users and developers who have created special software add-ons that do other, usually more advanced, analyses that go beyond the capabilities of the base jamovi program.\nTo install add-on modules, just click on the large \\(+\\) in the top right of the jamovi window, select “jamovi-library” and then browse through the various add-on modules that are available. Choose the one(s) you want, and then install them, as in Figure 3.6. It’s that easy. The newly installed modules can then be accessed from the “Analyses” button bar. Try it…useful add-on modules to install include “scatr” (added under “Descriptives”) and \\(R_j\\).\n\n\n\n\n\nFigure 3.6: Installing add-on modules in jamovi"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#quitting-jamovi",
    "href": "03-Getting-started-with-jamovi.html#quitting-jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.8 Quitting jamovi",
    "text": "3.8 Quitting jamovi\nThere’s one last thing I should cover in this chapter: how to quit jamovi. It’s not hard, just close the program the same way you would any other program. However, what you might want to do before you quit is save your work! There are two parts to this: saving any changes to the data set, and saving the analyses that you ran.\nIt is good practice to save any changes to the data set as a new data set. That way you can always go back to the original data. To save any changes in jamovi, select ‘Export’…‘Data’ from the main jamovi menu (button with three horizontal bars in the top left) and create a new file name for the changed data set.\nAlternatively, you can save both the changed data and any analyses you have undertaken by saving as a jamovi file. To do this, from the main jamovi menu select ‘Save as’ and type in a file name for this ‘jamovi file (.omv)’. Remember to save the file in a location where you can find it again later. I usually create a new folder for specific data sets and analyses."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#summary",
    "href": "03-Getting-started-with-jamovi.html#summary",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nEvery book that tries to teach a new statistical software program to novices has to cover roughly the same topics, and in roughly the same order. Ours is no exception, and so in the grand tradition of doing it just the same way everyone else did it, this chapter covered the following topics:\n\n[Installing jamovi]. We downloaded and installed jamovi, and started it up.\n[Analyses]. We very briefly oriented to the part of jamovi where analyses are done and results appear, but then deferred this until later in the book.\n[The spreadsheet]. We spent more time looking at the spreadsheet part of jamovi, and considered different variable types, and how to compute new variables.\n[Loading data in jamovi]. We also saw how to load data files in jamovi.\n[Importing unusual data files]. Then we figured out how to open other data files, from different file types.\nChanging data from one level to another. And saw that sometimes we need to coerce data from one type to another.\nInstalling add-on modules into jamovi. Installing add-on modules from the jamovi community really extends jamovi capabilities.\nQuitting jamovi. Finally, we looked at good practice in terms of saving your data set and analyses when you have finished and are about to quit jamovi.\n\nWe still haven’t arrived at anything that resembles data analysis. Maybe the next Chapter will get us a bit closer!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#混淆人為結果以及各種降低效度的因素",
    "href": "02-A-brief-introduction-to-research-design.html#混淆人為結果以及各種降低效度的因素",
    "title": "2  研究設計入門",
    "section": "2.7 混淆、人為結果、以及各種降低效度的因素",
    "text": "2.7 混淆、人為結果、以及各種降低效度的因素\nIf we look at the issue of validity in the most general fashion the two biggest worries that we have are confounders and artefacts. These two terms are defined in the following way:\n\nConfounder: A confounder is an additional, often unmeasured variable13 that turns out to be related to both the predictors and the outcome. The existence of confounders threatens the internal validity of the study because you can’t tell whether the predictor causes the outcome, or if the confounding variable causes it.\nArtefact: A result is said to be “artefactual” if it only holds in the special situation that you happened to test in your study. The possibility that your result is an artefact describes a threat to your external validity, because it raises the possibility that you can’t generalise or apply your results to the actual population that you care about.\n\nAs a general rule confounders are a bigger concern for non-experimental studies, precisely because they’re not proper experiments. By definition, you’re leaving lots of things uncontrolled, so there’s a lot of scope for confounders being present in your study. Experimental research tends to be much less vulnerable to confounders. The more control you have over what happens during the study, the more you can prevent confounders from affecting the results. With random allocation, for example, confounders are distributed randomly, and evenly, between different groups.\nHowever, there are always swings and roundabouts and when we start thinking about artefacts rather than confounders the shoe is very firmly on the other foot. For the most part, artefactual results tend to be a concern for experimental studies than for non-experimental studies. To see this, it helps to realise that the reason that a lot of studies are non-experimental is precisely because what the researcher is trying to do is examine human behaviour in a more naturalistic context. By working in a more real-world context you lose experimental control (making yourself vulnerable to confounders), but because you tend to be studying human psychology “in the wild” you reduce the chances of getting an artefactual result. Or, to put it another way, when you take psychology out of the wild and bring it into the lab (which we usually have to do to gain our experimental control), you always run the risk of accidentally studying something different to what you wanted to study.\nBe warned though. The above is a rough guide only. It’s absolutely possible to have confounders in an experiment, and to get artefactual results with non-experimental studies. This can happen for all sorts of reasons, not least of which is experimenter or researcher error. In practice, it’s really hard to think everything through ahead of time and even very good researchers make mistakes.\nAlthough there’s a sense in which almost any threat to validity can be characterised as a confounder or an artefact, they’re pretty vague concepts. So let’s have a look at some of the most common examples.\n\n2.7.1 歷史效應\nHistory effects refer to the possibility that specific events may occur during the study that might influence the outcome measure. For instance, something might happen in between a pretest and a post-test. Or in-between testing participant 23 and participant 24. Alternatively, it might be that you’re looking at a paper from an older study that was perfectly valid for its time, but the world has changed enough since then that the conclusions are no longer trustworthy. Examples of things that would count as history effects are:\n\nYou’re interested in how people think about risk and uncertainty. You started your data collection in December 2010. But finding participants and collecting data takes time, so you’re still finding new people in February 2011. Unfortunately for you (and even more unfortunately for others), the Queensland floods occurred in January 2011 causing billions of dollars of damage and killing many people. Not surprisingly, the people tested in February 2011 express quite different beliefs about handling risk than the people tested in December 2010. Which (if any) of these reflects the “true” beliefs of participants? I think the answer is probably both. The Queensland floods genuinely changed the beliefs of the Australian public, though possibly only temporarily. The key thing here is that the “history” of the people tested in February is quite different to people tested in December.\nYou’re testing the psychological effects of a new anti-anxiety drug. So what you do is measure anxiety before administering the drug (e.g., by self-report, and taking physiological measures). Then you administer the drug, and afterwards you take the same measures. In the middle however, because your lab is in Los Angeles, there’s an earthquake which increases the anxiety of the participants.\n\n\n\n2.7.2 發展成熟效應\nAs with history effects, maturational effects are fundamentally about change over time. However, maturation effects aren’t in response to specific events. Rather, they relate to how people change on their own over time. We get older, we get tired, we get bored, etc. Some examples of maturation effects are:\n\nWhen doing developmental psychology research you need to be aware that children grow up quite rapidly. So, suppose that you want to find out whether some educational trick helps with vocabulary size among 3 year olds. One thing that you need to be aware of is that the vocabulary size of children that age is growing at an incredible rate (multiple words per day) all on its own. If you design your study without taking this maturational effect into account, then you won’t be able to tell if your educational trick works.\nWhen running a very long experiment in the lab (say, something that goes for 3 hours) it’s very likely that people will begin to get bored and tired, and that this maturational effect will cause performance to decline regardless of anything else going on in the experiment\n\n\n\n2.7.3 重覆測試效應\nAn important type of history effect is the effect of repeated testing. Suppose I want to take two measurements of some psychological construct (e.g., anxiety). One thing I might be worried about is if the first measurement has an effect on the second measurement. In other words, this is a history effect in which the “event” that influences the second measurement is the first measurement itself! This is not at all uncommon. Examples of this include:\n\nLearning and practice: e.g., “intelligence” at time 2 might appear to go up relative to time 1 because participants learned the general rules of how to solve “intelligence-test-style” questions during the first testing session.\nFamiliarity with the testing situation: e.g., if people are nervous at time 1, this might make performance go down. But after sitting through the first testing situation they might calm down a lot precisely because they’ve seen what the testing looks like.\nAuxiliary changes caused by testing: e.g., if a questionnaire assessing mood is boring then mood rating at measurement time 2 is more likely to be “bored” precisely because of the boring measurement made at time 1.\n\n\n\n2.7.4 選擇偏誤\nSelection bias is a pretty broad term. Suppose that you’re running an experiment with two groups of participants where each group gets a different “treatment”, and you want to see if the different treatments lead to different outcomes. However, suppose that, despite your best efforts, you’ve ended up with a gender imbalance across groups (say, group A has 80% females and group B has 50% females). It might sound like this could never happen but, trust me, it can. This is an example of a selection bias, in which the people “selected into” the two groups have different characteristics. If any of those characteristics turns out to be relevant (say, your treatment works better on females than males) then you’re in a lot of trouble.\n\n\n2.7.5 個體特質差異\nWhen thinking about the effects of attrition, it is sometimes helpful to distinguish between two different types. The first is homogeneous attrition, in which the attrition effect is the same for all groups, treatments or conditions. In the example I gave above, the attrition would be homogeneous if (and only if) the easily bored participants are dropping out of all of the conditions in my experiment at about the same rate. In general, the main effect of homogeneous attrition is likely to be that it makes your sample unrepresentative. As such, the biggest worry that you’ll have is that the generalisability of the results decreases. In other words, you lose external validity.\nThe second type of attrition is heterogeneous attrition, in which the attrition effect is different for different groups. More often called differential attrition, this is a kind of selection bias that is caused by the study itself. Suppose that, for the first time ever in the history of psychology, I manage to find the perfectly balanced and representative sample of people. I start running “Dani’s incredibly long and tedious experiment” on my perfect sample but then, because my study is incredibly long and tedious, lots of people start dropping out. I can’t stop this. Participants absolutely have the right to stop doing any experiment, any time, for whatever reason they feel like, and as researchers we are morally (and professionally) obliged to remind people that they do have this right. So, suppose that “Dani’s incredibly long and tedious experiment” has a very high drop out rate. What do you suppose the odds are that this drop out is random? Answer: zero. Almost certainly the people who remain are more conscientious, more tolerant of boredom, etc., than those that leave. To the extent that (say) conscientiousness is relevant to the psychological phenomenon that I care about, this attrition can decrease the validity of my results.\nHere’s another example. Suppose I design my experiment with two conditions. In the “treatment” condition, the experimenter insults the participant and then gives them a questionnaire designed to measure obedience. In the “control” condition, the experimenter engages in a bit of pointless chitchat and then gives them the questionnaire. Leaving aside the questionable scientific merits and dubious ethics of such a study, let’s have a think about what might go wrong here. As a general rule, when someone insults me to my face I tend to get much less co-operative. So, there’s a pretty good chance that a lot more people are going to drop out of the treatment condition than the control condition. And this drop out isn’t going to be random. The people most likely to drop out would probably be the people who don’t care all that much about the importance of obediently sitting through the experiment. Since the most bloody minded and disobedient people all left the treatment group but not the control group, we’ve introduced a confound: the people who actually took the questionnaire in the treatment group were already more likely to be dutiful and obedient than the people in the control group. In short, in this study insulting people doesn’t make them more obedient. It makes the more disobedient people leave the experiment! The internal validity of this experiment is completely shot.\n\n\n2.7.6 無回應偏誤\nNon-response bias is closely related to selection bias and to differential attrition. The simplest version of the problem goes like this. You mail out a survey to 1000 people but only 300 of them reply. The 300 people who replied are almost certainly not a random subsample. People who respond to surveys are systematically different to people who don’t. This introduces a problem when trying to generalise from those 300 people who replied to the population at large, since you now have a very non-random sample. The issue of non-response bias is more general than this, though. Among the (say) 300 people that did respond to the survey, you might find that not everyone answers every question. If (say) 80 people chose not to answer one of your questions, does this introduce problems? As always, the answer is maybe. If the question that wasn’t answered was on the last page of the questionnaire, and those 80 surveys were returned with the last page missing, there’s a good chance that the missing data isn’t a big deal; probably the pages just fell off. However, if the question that 80 people didn’t answer was the most confrontational or invasive personal question in the questionnaire, then almost certainly you’ve got a problem. In essence, what you’re dealing with here is what’s called the problem of missing data. If the data that is missing was “lost” randomly, then it’s not a big problem. If it’s missing systematically, then it can be a big problem.\n\n\n2.7.7 趨向平均數的迴歸\nRegression to the mean refers to any situation where you select data based on an extreme value on some measure. Because the variable has natural variation it almost certainly means that when you take a subsequent measurement the later measurement will be less extreme than the first one, purely by chance.\nHere’s an example. Suppose I’m interested in whether a psychology education has an adverse effect on very smart kids. To do this, I find the 20 psychology I students with the best high school grades and look at how well they’re doing at university. It turns out that they’re doing a lot better than average, but they’re not topping the class at university even though they did top their classes at high school. What’s going on? The natural first thought is that this must mean that the psychology classes must be having an adverse effect on those students. However, while that might very well be the explanation, it’s more likely that what you’re seeing is an example of “regression to the mean”. To see how it works, let’s take a moment to think about what is required to get the best mark in a class, regardless of whether that class be at high school or at university. When you’ve got a big class there are going to be lots of very smart people enrolled. To get the best mark you have to be very smart, work very hard, and be a bit lucky. The exam has to ask just the right questions for your idiosyncratic skills, and you have to avoid making any dumb mistakes (we all do that sometimes) when answering them. And that’s the thing, whilst intelligence and hard work are transferable from one class to the next, luck isn’t. The people who got lucky in high school won’t be the same as the people who get lucky at university. That’s the very definition of “luck”. The consequence of this is that when you select people at the very extreme values of one measurement (the top 20 students), you’re selecting for hard work, skill and luck. But because the luck doesn’t transfer to the second measurement (only the skill and work), these people will all be expected to drop a little bit when you measure them a second time (at university). So their scores fall back a little bit, back towards everyone else. This is regression to the mean.\nRegression to the mean is surprisingly common. For instance, if two very tall people have kids their children will tend to be taller than average but not as tall as the parents. The reverse happens with very short parents. Two very short parents will tend to have short children, but nevertheless those kids will tend to be taller than the parents. It can also be extremely subtle. For instance, there have been studies done that suggested that people learn better from negative feedback than from positive feedback. However, the way that people tried to show this was to give people positive reinforcement whenever they did good, and negative reinforcement when they did bad. And what you see is that after the positive reinforcement people tended to do worse, but after the negative reinforcement they tended to do better. But notice that there’s a selection bias here! When people do very well, you’re selecting for “high” values, and so you should expect, because of regression to the mean, that performance on the next trial should be worse regardless of whether reinforcement is given. Similarly, after a bad trial, people will tend to improve all on their own. The apparent superiority of negative feedback is an artefact caused by regression to the mean (see Kahneman & Tversky (1973), for discussion).\n\n\n2.7.8 實驗者偏誤\nExperimenter bias can come in multiple forms. The basic idea is that the experimenter, despite the best of intentions, can accidentally end up influencing the results of the experiment by subtly communicating the “right answer” or the “desired behaviour” to the participants. Typically, this occurs because the experimenter has special knowledge that the participant does not, for example the right answer to the questions being asked or knowledge of the expected pattern of performance for the condition that the participant is in. The classic example of this happening is the case study of “Clever Hans”, which dates back to 1907 (Pfungst, 1911). Clever Hans was a horse that apparently was able to read and count and perform other human like feats of intelligence. After Clever Hans became famous, psychologists started examining his behaviour more closely. It turned out that, not surprisingly, Hans didn’t know how to do maths. Rather, Hans was responding to the human observers around him, because the humans did know how to count and the horse had learned to change its behaviour when people changed theirs.\nThe general solution to the problem of experimenter bias is to engage in double blind studies, where neither the experimenter nor the participant knows which condition the participant is in or knows what the desired behaviour is. This provides a very good solution to the problem, but it’s important to recognise that it’s not quite ideal, and hard to pull off perfectly. For instance, the obvious way that I could try to construct a double blind study is to have one of my Ph.D. students (one who doesn’t know anything about the experiment) run the study. That feels like it should be enough. The only person (me) who knows all the details (e.g., correct answers to the questions, assignments of participants to conditions) has no interaction with the participants, and the person who does all the talking to people (the Ph.D. student) doesn’t know anything. Except for the reality that the last part is very unlikely to be true. In order for the Ph.D. student to run the study effectively they need to have been briefed by me, the researcher. And, as it happens, the Ph.D. student also knows me and knows a bit about my general beliefs about people and psychology (e.g., I tend to think humans are much smarter than psychologists give them credit for). As a result of all this, it’s almost impossible for the experimenter to avoid knowing a little bit about what expectations I have. And even a little bit of knowledge can have an effect. Suppose the experimenter accidentally conveys the fact that the participants are expected to do well in this task. Well, there’s a thing called the “Pygmalion effect”, where if you expect great things of people they’ll tend to rise to the occasion. But if you expect them to fail then they’ll do that too. In other words, the expectations become a self-fulfilling prophesy.\n\n\n2.7.9 需求效應與反應性\nWhen talking about experimenter bias, the worry is that the experimenter’s knowledge or desires for the experiment are communicated to the participants, and that these can change people’s behaviour (Rosenthal, 1966). However, even if you manage to stop this from happening, it’s almost impossible to stop people from knowing that they’re part of a psychological study. And the mere fact of knowing that someone is watching or studying you can have a pretty big effect on behaviour. This is generally referred to as reactivity or demand effects. The basic idea is captured by the Hawthorne effect: people alter their performance because of the attention that the study focuses on them. The effect takes its name from a study that took place in the “Hawthorne Works” factory outside of Chicago (see Adair (1984)). This study, from the 1920s, looked at the effects of factory lighting on worker productivity. But, importantly, change in worker behaviour occurred because the workers knew they were being studied, rather than any effect of factory lighting.\nTo get a bit more specific about some of the ways in which the mere fact of being in a study can change how people behave, it helps to think like a social psychologist and look at some of the roles that people might adopt during an experiment but might not adopt if the corresponding events were occurring in the real world:\n\nThe good participant tries to be too helpful to the researcher. He or she seeks to figure out the experimenter’s hypotheses and confirm them.\nThe negative participant does the exact opposite of the good participant. He or she seeks to break or destroy the study or the hypothesis in some way.\nThe faithful participant is unnaturally obedient. He or she seeks to follow instructions perfectly, regardless of what might have happened in a more realistic setting.\nThe apprehensive participant gets nervous about being tested or studied, so much so that his or her behaviour becomes highly unnatural, or overly socially desirable.\n\n\n\n2.7.10 安慰劑效應\nThe placebo effect is a specific type of demand effect that we worry a lot about. It refers to the situation where the mere fact of being treated causes an improvement in outcomes. The classic example comes from clinical trials. If you give people a completely chemically inert drug and tell them that it’s a cure for a disease, they will tend to get better faster than people who aren’t treated at all. In other words, it is people’s belief that they are being treated that causes the improved outcomes, not the drug.\nHowever, the current consensus in medicine is that true placebo effects are quite rare and most of what was previously considered placebo effect is in fact some combination of natural healing (some people just get better on their own), regression to the mean and other quirks of study design. Of interest to psychology is that the strongest evidence for at least some placebo effect is in self-reported outcomes, most notably in treatment of pain (Hróbjartsson & Gøtzsche, 2010).\n\n\n2.7.11 情境、測量、及小群體效應\nIn some respects, these terms are a catch-all term for “all other threats to external validity”. They refer to the fact that the choice of sub-population from which you draw your participants, the location, timing and manner in which you run your study (including who collects the data) and the tools that you use to make your measurements might all be influencing the results. Specifically, the worry is that these things might be influencing the results in such a way that the results won’t generalise to a wider array of people, places and measures.\n\n\n2.7.12 詐欺、欺暪與自我欺暪\n\nIt is difficult to get a man to understand something, when his salary depends on his not understanding it.\n- Upton Sinclair\n\nThere’s one final thing I feel I should mention. While reading what the textbooks often have to say about assessing the validity of a study I couldn’t help but notice that they seem to make the assumption that the researcher is honest. I find this hilarious. While the vast majority of scientists are honest, in my experience at least, some are not.14 Not only that, as I mentioned earlier, scientists are not immune to belief bias. It’s easy for a researcher to end up deceiving themselves into believing the wrong thing, and this can lead them to conduct subtly flawed research and then hide those flaws when they write it up. So you need to consider not only the (probably unlikely) possibility of outright fraud, but also the (probably quite common) possibility that the research is unintentionally “slanted”. I opened a few standard textbooks and didn’t find much of a discussion of this problem, so here’s my own attempt to list a few ways in which these issues can arise:\n\n資料造假(Data fabrication). Sometimes, people just make up the data. This is occasionally done with “good” intentions. For instance, the researcher believes that the fabricated data do reflect the truth, and may actually reflect “slightly cleaned up” versions of actual data. On other occasions, the fraud is deliberate and malicious. Some high-profile examples where data fabrication has been alleged or shown include Cyril Burt (a psychologist who is thought to have fabricated some of his data), Andrew Wakefield (who has been accused of fabricating his data connecting the MMR vaccine to autism) and Hwang Woo-suk (who falsified a lot of his data on stem cell research).\n惡作劇(Hoaxes). Hoaxes share a lot of similarities with data fabrication, but they differ in the intended purpose. A hoax is often a joke, and many of them are intended to be (eventually) discovered. Often, the point of a hoax is to discredit someone or some field. There’s quite a few well known scientific hoaxes that have occurred over the years (e.g., Piltdown man) and some were deliberate attempts to discredit particular fields of research (e.g., the Sokal affair).\n資料不實(Data misrepresentation). While fraud gets most of the headlines, it’s much more common in my experience to see data being misrepresented. When I say this I’m not referring to newspapers getting it wrong (which they do, almost always). I’m referring to the fact that often the data don’t actually say what the researchers think they say. My guess is that, almost always, this isn’t the result of deliberate dishonesty but instead is due to a lack of sophistication in the data analyses. For instance, think back to the example of Simpson’s paradox that I discussed in the beginning of this book. It’s very common to see people present “aggregated” data of some kind and sometimes, when you dig deeper and find the raw data yourself you find that the aggregated data tell a different story to the disaggregated data. Alternatively, you might find that some aspect of the data is being hidden, because it tells an inconvenient story (e.g., the researcher might choose not to refer to a particular variable). There’s a lot of variants on this, many of which are very hard to detect.\n“不良”研究設計(Study “misdesign”). Okay, this one is subtle. Basically, the issue here is that a researcher designs a study that has built-in flaws and those flaws are never reported in the paper. The data that are reported are completely real and are correctly analysed, but they are produced by a study that is actually quite wrongly put together. The researcher really wants to find a particular effect and so the study is set up in such a way as to make it “easy” to (artefactually) observe that effect. One sneaky way to do this, in case you’re feeling like dabbling in a bit of fraud yourself, is to design an experiment in which it’s obvious to the participants what they’re “supposed” to be doing, and then let reactivity work its magic for you. If you want you can add all the trappings of double blind experimentation but it won’t make a difference since the study materials themselves are subtly telling people what you want them to do. When you write up the results the fraud won’t be obvious to the reader. What’s obvious to the participant when they’re in the experimental context isn’t always obvious to the person reading the paper. Of course, the way I’ve described this makes it sound like it’s always fraud. Probably there are cases where this is done deliberately, but in my experience the bigger concern has been with unintentional misdesign. The researcher believes and so the study just happens to end up with a built in flaw, and that flaw then magically erases itself when the study is written up for publication.\n資料探勘與後設檢定(Data mining & post hoc hypothesising). Another way in which the authors of a study can more or less misrepresent the data is by engaging in what’s referred to as “data mining” (see Gelman and Loken 2014, for a broader discussion of this as part of the “garden of forking paths” in statistical analysis). As we’ll discuss later, if you keep trying to analyse your data in lots of different ways, you’ll eventually find something that “looks” like a real effect but isn’t. This is referred to as “data mining”. It used to be quite rare because data analysis used to take weeks, but now that everyone has very powerful statistical software on their computers it’s becoming very common. Data mining per se isn’t “wrong”, but the more that you do it the bigger the risk you’re taking. The thing that is wrong, and I suspect is very common, is unacknowledged data mining. That is, the researcher runs every possible analysis known to humanity, finds the one that works, and then pretends that this was the only analysis that they ever conducted. Worse yet, they often “invent” a hypothesis after looking at the data to cover up the data mining. To be clear. It’s not wrong to change your beliefs after looking at the data, and to reanalyse your data using your new “post hoc” hypotheses. What is wrong (and I suspect common) is failing to acknowledge that you’ve done. If you acknowledge that you did it then other researchers are able to take your behaviour into account. If you don’t, then they can’t. And that makes your behaviour deceptive. Bad!\n發表偏誤與自我審查(Publication bias & self-censoring). Finally, a pervasive bias is “non-reporting” of negative results. This is almost impossible to prevent. Journals don’t publish every article that is submitted to them. They prefer to publish articles that find “something”. So, if 20 people run an experiment looking at whether reading Finnegans Wake causes insanity in humans, and 19 of them find that it doesn’t, which one do you think is going to get published? Obviously, it’s the one study that did find that Finnegans Wake causes insanity.15 This is an example of a publication bias. Since no-one ever published the 19 studies that didn’t find an effect, a naive reader would never know that they existed. Worse yet, most researchers “internalise” this bias and end up self-censoring their research. Knowing that negative results aren’t going to be accepted for publication, they never even try to report them. As a friend of mine says “for every experiment that you get published, you also have 10 failures”. And she’s right. The catch is, while some (maybe most) of those studies are failures for boring reasons (e.g. you stuffed something up) others might be genuine “null” results that you ought to acknowledge when you write up the “good” experiment. And telling which is which is often hard to do. A good place to start is a paper by Ioannidis (2005) with the depressing title “Why most published research findings are false”. I’d also suggest taking a look at work by Kühberger et al. (2014) presenting statistical evidence that this actually happens in psychology.\n\nThere’s probably a lot more issues like this to think about, but that’ll do to start with. What I really want to point out is the blindingly obvious truth that real world science is conducted by actual humans, and only the most gullible of people automatically assumes that everyone else is honest and impartial. Actual scientists aren’t usually that naive, but for some reason the world likes to pretend that we are, and the textbooks we usually write seem to reinforce that stereotype."
  },
  {
    "objectID": "04-Descriptive-statistics.html#measures-of-central-tendency",
    "href": "04-Descriptive-statistics.html#measures-of-central-tendency",
    "title": "4  Descriptive statistics",
    "section": "4.1 Measures of central tendency",
    "text": "4.1 Measures of central tendency\nDrawing pictures of the data, as I did in Figure 4.2, is an excellent way to convey the “gist” of what the data is trying to tell you. It’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about where the “average” or “middle” of your data lies. The three most commonly used measures are the mean, median and mode. I’ll explain each of these in turn, and then discuss when each of them is useful.\n\n4.1.1 The mean\nThe mean of a set of observations is just a normal, old-fashioned average. Add all of the values up, and then divide by the total number of values. The first five AFL winning margins were 56, 31, 56, 8 and 32, so the mean of these observations is just:\n\\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone. Averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in jamovi.\nThe first piece of notation to introduce is \\(N\\), which we’ll use to refer to the number of observations that we’re averaging (in this case \\(N = 5\\)). Next, we need to attach a label to the observations themselves. It’s traditional to use X for this, and to use subscripts to indicate which observation we’re actually talking about. That is, we’ll use \\(X_1\\) to refer to the first observation, \\(X_2\\) to refer to the second observation, and so on all the way up to \\(X_N\\) for the last one. Or, to say the same thing in a slightly more abstract way, we use \\(X_i\\) to refer to the i-th observation. Just to make sure we’re clear on the notation, Table 4.1 lists the 5 observations in the afl.margins variable, along with the mathematical symbol used to refer to it and the actual value that the observation corresponds to.\n\n\n\n\nTable 4.1:  Observations in the afl.margins variable \n\nthe observationits symbolthe observed value\n\nwinning margin, game 1\\( X_1 \\)56 points\n\nwinning margin, game 2\\( X_2 \\)31 points\n\nwinning margin, game 3\\( X_3 \\)56 points\n\nwinning margin, game 4\\( X_4 \\)8 points\n\nwinning margin, game 5\\( X_5 \\)32 points\n\n\n\n\n\n[Additional technical detail2]\n\n\n4.1.2 Calculating the mean in jamovi\nOkay, that’s the maths. So how do we get the magic computing box to do the work for us? When the number of observations starts to become large it’s much easier to do these sorts of calculations using a computer. To calculate the mean using all the data we can use jamovi. The first step is to click on the ‘Exploration’ button and then click ‘Descriptives’. Then you can highlight the afl.margins variable and click the ‘right arrow’ to move it across into the ‘Variables box’. As soon as you do that a Table appears on the right hand side of the screen containing default ‘Descriptives’ information; see Figure 4.3.\n\n\n\n\n\nFigure 4.3: Default descriptives for the AFL 2010 winning margin data (the afl.margins variable)\n\n\n\n\nAs you can see in Figure 4.3, the mean value for the afl.margins variable is 35.30. Other information presented includes the total number of observations (N=176), the number of missing values (none), and the Median, Minimum and Maximum values for the variable.\n\n\n4.1.3 The median\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the first 5 AFL winning margins: \\(56\\), \\(31\\), \\(56\\), \\(8\\) and \\(32\\). To figure out the median we sort these numbers into ascending order:\n8, 31, 32, 56, 56\nFrom inspection, it’s obvious that the median value of these 5 observations is 32 since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we are interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now\n8, 31, 32, 56, 56\nand there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life we use a computer to do the heavy lifting for us, and jamovi has provided us with a Median value of 30.50 for the afl.margins variable (Figure 4.3).\n\n\n4.1.4 Mean or median? What’s the difference?\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure 4.4. The mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary then it’s probably best to use the Mode instead.\nIf your data are ordinal scale you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger) but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data). But it’s very sensitive to extreme, outlying values.\n\nLet’s expand on that last part a little. One consequence is that there are systematic differences between the mean and the median when the histogram is asymmetric (Skew and kurtosis). This is illustrated in Figure 4.4. Notice that the median (right hand side) is located closer to the “body” of the histogram, whereas the mean (left hand side) gets dragged towards the “tail” (where the extreme values are). To give a concrete example, suppose Bob (income $50,000), Kate (income $60,000) and Jane (income $65,000) are sitting at a table. The average income at the table is $58,333 and the median income is $60,000. Then Bill sits down with them (income $100,000,000). The average income has now jumped to $25,043,750 but the median rises only to $62,500. If you’re interested in looking at the overall income at the table the mean might be the right answer. But if you’re interested in what counts as a typical income at the table the median would be a better choice here.\n\n\n\n\n\nFigure 4.4: An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ‘centre of gravity’ of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger\n\n\n\n\n\n\n4.1.5 A real life example\nTo try to get a sense of why you need to pay attention to the differences between the mean and the median let’s consider a real life example. Since I tend to mock journalists for their poor scientific and statistical knowledge, I should give credit where credit is due. This is an excellent article on the ABC news website3 from 24 September, 2010:\n\nSenior Commonwealth Bank executives have travelled the world in the past couple of weeks with a presentation showing how Australian house prices, and the key price to income ratios, compare favourably with similar countries. “Housing affordability has actually been going sideways for the last five to six years,” said Craig James, the chief economist of the bank’s trading arm, CommSec.\n\nThis probably comes as a huge surprise to anyone with a mortgage, or who wants a mortgage, or pays rent, or isn’t completely oblivious to what’s been going on in the Australian housing market over the last several years. Back to the article:\n\nCBA has waged its war against what it believes are housing doomsayers with graphs, numbers and international comparisons. In its presentation, the bank rejects arguments that Australia’s housing is relatively expensive compared to incomes. It says Australia’s house price to household income ratio of 5.6 in the major cities, and 4.3 nationwide, is comparable to many other developed nations. It says San Francisco and New York have ratios of 7, Auckland’s is 6.7, and Vancouver comes in at 9.3.\n\nMore excellent news! Except, the article goes on to make the observation that:\n\nMany analysts say that has led the bank to use misleading figures and comparisons. If you go to page four of CBA’s presentation and read the source information at the bottom of the graph and table, you would notice there is an additional source on the international comparison – Demographia. However, if the Commonwealth Bank had also used Demographia’s analysis of Australia’s house price to income ratio, it would have come up with a figure closer to 9 rather than 5.6 or 4.3\n\nThat’s, um, a rather serious discrepancy. One group of people say 9, another says 4-5. Should we just split the difference and say the truth lies somewhere in between? Absolutely not! This is a situation where there is a right answer and a wrong answer. Demographia is correct, and the Commonwealth Bank is wrong. As the article points out:\n\n[An] obvious problem with the Commonwealth Bank’s domestic price to income figures is they compare average incomes with median house prices (unlike the Demographia figures that compare median incomes to median prices). The median is the mid-point, effectively cutting out the highs and lows, and that means the average is generally higher when it comes to incomes and asset prices, because it includes the earnings of Australia’s wealthiest people. To put it another way: the Commonwealth Bank’s figures count Ralph Norris’ multi-million dollar pay packet on the income side, but not his (no doubt) very expensive house in the property price figures, thus understating the house price to income ratio for middle-income Australians.\n\nCouldn’t have put it better myself. The way that Demographia calculated the ratio is the right thing to do. The way that the Bank did it is incorrect. As for why an extremely quantitatively sophisticated organisation such as a major bank made such an elementary mistake, well… I can’t say for sure since I have no special insight into their thinking. But the article itself does happen to mention the following facts, which may or may not be relevant:\n\n[As] Australia’s largest home lender, the Commonwealth Bank has one of the biggest vested interests in house prices rising. It effectively owns a massive swathe of Australian housing as security for its home loans as well as many small business loans.\n\nMy, my.\n\n\n4.1.6 Mode\nThe mode of a sample is very simple. It is the value that occurs most frequently. We can illustrate the mode using a different AFL variable: who has played in the most finals? Open the aflsmall finalists file and take a look at the afl.finalists variable, see Figure 4.5. This variable contains the names of all 400 teams that played in all 200 finals matches played during the period 1987 to 2010.\nWhat we could do is read through all 400 entries and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use jamovi to do this for us. Under ‘Exploration’ - ‘Descriptives’ click the small check box labelled ‘Frequency tables’ and you should get something like Figure 4.6.\nNow that we have our frequency table we can just look at it and see that, over the 24 years for which we have data, Geelong has played in more finals than any other team. Thus, the mode of the afl.finalists data is “Geelong”. We can see that Geelong (39 finals) played in more finals than any other team during the 1987-2010 period. It’s also worth noting that in the ‘Descriptives’ Table no results are calculated for Mean, Median, Minimum or Maximum. This is because the afl.finalists variable is a nominal text variable so it makes no sense to calculate these values.\n\n\n\n\n\nFigure 4.5: A screenshot of jamovi showing the variables stored in the aflsmall finalists.csv file\n\n\n\n\n\n\n\n\n\nFigure 4.6: A screenshot of jamovi showing the frequency table for the afl.finalists variable\n\n\n\n\nOne last point to make regarding the mode. Whilst the mode is most often calculated when you have nominal data, because means and medians are useless for those sorts of variables, there are some situations in which you really do want to know the mode of an ordinal, interval or ratio scale variable. For instance, let’s go back to our afl.margins variable. This variable is clearly ratio scale (if it’s not clear to you, it may help to re-read Section 2.2), and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario: a friend of yours is offering a bet and they pick a football game at random. Without knowing who is playing you have to guess the exact winning margin. If you guess correctly you win $50. If you don’t you lose $1. There are no consolation prizes for “almost” getting the right answer. You have to guess exactly the right margin. For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. To calculate the mode for the afl.margins variable in jamovi, go back to that data set and on the ‘Exploration’ - ‘Descriptives’ screen you will see you can expand the section marked ‘Statistics’. Click on the checkbox marked ‘Mode’ and you will see the modal value presented in the ‘Descriptives’ Table, as in Figure 4.7. So the 2010 data suggest you should bet on a 3 point margin.\n\n\n\n\n\nFigure 4.7: A screenshot of jamovi showing the modal value for the afl.margins variable"
  },
  {
    "objectID": "04-Descriptive-statistics.html#sec-Measures-of-variability",
    "href": "04-Descriptive-statistics.html#sec-Measures-of-variability",
    "title": "4  描述統計",
    "section": "4.2 變異量數",
    "text": "4.2 變異量數\n至此有關集中量數的說明先告一段落。除了資料的“中心點”或“次數最多”的部分，描述統計也要說明資料的變異程度(variability)。像是資料有多“離散”？測量到的數值離平均值或中位數有多遠？這一節的只有談等距或等比尺度資料的變異量數，所以繼續使用afl.margins資料示範，還有說明各種變異量數的優缺點。\n\n\n4.2.1 全距\n計算變項資料的全距非常容易，就是最大值減去最小值。以AFL勝隊得分的例子來說，最大值是116，最小值是0。儘管全距是所有變異量數裡計算方法最簡單，卻也是最不可靠的。還記得平均值的說明曾提到，統計量數最好要穩定(robust)。如果變項資料裡有一兩個極端數值，會影響全距的大小。就像以下數列有個非常極端的數值：\n-100, 2, 3, 4, 5, 6, 7, 8, 9, 10\n很明顯這筆資料的全距完全被極端值主宰：沒有去掉極端值的全距是110，但是移除之後變成8。\n\n\n\n4.2.2 四分位數間距\n四分位數間距(interquartile range)類似全距，不過是改成計算資料內位於25%與75%兩個百分位數之間的差異。若是同學還不知道什麼是百分位數(percentile)，可以想成一筆資料x的10%百分位數，是資料內小於這個數值的比例佔10%。其實前面我們就已經學過這個概念，因為中位數就是50%百分位數！jamovi的預設描述統計功能可以直接算出一筆資料的25%、50%、75%三個百分位數。請在jamovi描述統計選單內勾選“Quartiles”的選項試試看。\nFigure 4.8 展示的50%百分位數與中位數毫不意外是相同的。同時也能得到2010年AFL勝隊得分的IQR是37.5 (50.50 - 12.75 = 37.75)。理解IQR的簡單方式是：這個數值代表一筆資料“中間一半”跨越的範圍。小於25%百分位數的四分之一數值，以及大於75%百分位數的四分之一數值都在這“中間一半”之外。只有IQR內的數值在這段範圍之內。\n\n\n\n\n\n\nFigure 4.8: jamovi計算資料變項afl.margins的三個百分位數。\n\n\n\n\n\n\n4.2.3 平均絕對差\n全距與IQR都是利用資料內的百分位數，估計資料的變異程度。不過這不是估計變異程度的唯一方法。另一種方法是先決定一個有意義的參考點(像是平均值或中位數)，再計算整筆資料與參考點的“典型”差異值。那麼，什麼是“典型”差異值？通常是總和所有資料與參考點的差異值。實務上有兩種變異量數可用：“平均絕對差”(mean absolute deviation，平均值是參考點)與“中位絕對差”(median absolute deviation，中位數是參考點)。就原作者研讀過的書藉，“中位絕對差”是較理想的變異程度評估指標，不過心理學領域不常見到。\n經過前一段的提要，我們來看看如何計算平均絕對差。同樣透過手算AFL勝隊得分來學習，就用前五場得分紀錄56, 31, 56, 8, 32來示範計算步驟。之前我們已經算出平均值是36.6 (記為 \\(\\bar{X} = 36.6\\) )。每個數值減去平均，可以得到個別的差異值( 記為 \\(X_i - \\bar{X}\\) )，接著取每個差異值的絕對值( 記為 \\(\\mid X_i - \\bar{X} \\mid\\) )。建議同學按步驟照著表4-2 依序紀錄計算結果4。\n\n\n\n\n\n表4-2: 使用離均差絕對值計算資料變異程度\n\n\n\n\n口語\n符號\n數值\n離均差\n離均差絕對值\n\n\n\n\n符號\n\\(i\\)\n\\(X_i\\)\n\\(X_i - \\bar{X}\\)\n\\(|X_i - \\bar{X}|\\)\n\n\n\n1\n56\n19.4\n19.4\n\n\n\n2\n31\n-5.6\n5.6\n\n\n\n3\n56\n19.4\n19.4\n\n\n\n4\n8\n-28.6\n28.6\n\n\n\n5\n32\n-4.6\n4.6\n\n\n\n現在只要計算所有差異值的絕對值，就是平均絕對差。\n\n\\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\]\n至此我們算出5個數值的平均絕對差是15.52。\n[其他技術細節5]\n\n\n\n4.2.4 變異數\n雖然平均絕對差有其用處，但是並非評估變異程度的最好指標。從數學原理的角度來看，差異值平方比差異值絕對值是更合適的指標。這裡介紹的變異數(variance)會省去許多統計細節6，主要著墨討論心理學研究累積的巨大缺陷。首先我們定義一筆資料(\\(X\\))的變異數記號是 Var(\\(X\\))，通常也被寫成 \\(s^2\\)，其中原因之後會說明。\n[更多技術細節7]\n\n有了基本概念，就用實際資料熟悉變異數的運算。同樣用AFL前五場比賽紀錄練習，請同學用前一節計算平均絕對差異步驟，完成表4-3的手算步驟8。\n\n\n\n\n\n表4-3: 使用離均差平方計算資料變異程度\n\n\n\n\n口語\n符號\n數值\n離均差\n離均差平方\n\n\n\n\n符號\n\\(i\\)\n\\(X_i\\)\n\\(X_i - \\bar{X}\\)\n\\((X_i - \\bar{X})^2\\)\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n最後一欄是五筆資料的差異值平方，接著依公式用手或計算機計算平均，我們會得到變異數是\\(324.64\\)。好像大功告成了是不是？那這樣的變異數代表什麼意思？進一步解釋前，請用jamovi計算看看，建議重新匯入afl.margins資料變項，並且設定只計算前五項紀錄，然後開啟描述統計模組選單，這次要增加勾選”Variance”計算變異數(見 Figure 4.9 )，然後你會發現jamovi的計算結果是\\(405.80\\)，難到表4-3* 的計算步驟是錯誤的？\n\n\n\n\n\n\nFigure 4.9: 使用jamovi計算資料變項afl.margins前五筆的變異數。\n\n\n\n\n其實按公式筆算是正確的，jamovi的計算也沒問題9。解釋jamovi用什麼公式計算變異數很簡單，但是為什麼jamovi要用這個公式就需要花點功夫說明。我們先了解jamovi用什麼公式。前面提到的變異數公式是差異值平方總和除以資料個數\\(N\\)，不過jamovi計算的變異數是除以\\(N - 1\\)。\n[更多技術細節10]\n那麼為什麼jamovi是用 \\(N - 1\\)而非 \\(N\\)？畢竟變異數是差異值平方的平均不是嗎？為何不是用資料數值個數\\(N\\)做為分母呢？這樣想雖然沒有錯，不過在 Chapter 8 我們會討論“描述樣本”與“根據樣本推測母群性質”兩種統計操作的區別。平均數的計算方法在“描述樣本”與“推測母群性質”都是一樣的。但是對於變異數、標準差、還有許多估計變異程度的量數，“樣本“與”母群”的計算方法是不一樣的。正在學習的同學們從這裡開始，必須察覺多數統計實務計算的變異數其實是樣本變異數。樣本只是揭露真實世界的一部分而己。這一章學習的描述統計方法其實是“整理樣本的方法”，真實目的是要要估計“母群參數”。不過這樣的轉念對很多同學來說可能要求太高了，我們先接受jamovi計算的統計量數都是正確的，到了 Chapter 8 我們會再回來討論這些問題。\n好啦，這一節讀起來有些像是燒腦的偵探小說。先請同學們擱置具體理由，接受只要是用jamovi計算的變異數都是用\\(N-1\\)除以平方差總和的結果。不過還有一件事需要討論：怎麼解讀變異數的意思？描述統計的功能畢竟是用數字說明資料裡的故事，而且變異數就明明白白的顯示在報表上。但是目前還不能直接用變異數解釋資料的變異程度，原因在於變異數是總計差異值的平方，而任何數值平方之後，就失去測量尺度的數值差異。就拿表4-3紀錄的第一場比賽分數，差異值平方是\\(324.64\\)來說，這個數字無法說明這筆資料與平均值的差異究竟有多大，因為平方後的數值喪失了原來測量尺度的意義。另外，你也可以想想看，你知道了全班同學的身高平方總和有什麼意義？\n\n\n\n\n4.2.5 標準差\n若是同學學到了這裡，應該都能接受變異數的數學特性，不過要撰寫人類看得到的報告，我們還是要呈現能具體描述變異程度的量數。那要怎麼改造變異數呢？方法很簡單，就是將變異數開根號取得標準差(standard deviation)。統計學術語還有“差異平均值之平方根”(mean squared deviation)，或者簡稱RMSD。標準差的數值能以資料尺度解釋變異程度。以AFL資料的實務報告來說，通常會寫“標準差18.01”而不是”變異數324.68“。專業報告規範會提示樣本資料的標準差寫法，像是用小寫s，sd，還有“std dev”等。\n[更多技術細節11]\n不過在前一節變異數的介紹中，我們已經知道jamovi使用的樣本變異數計算公式是除以 \\(N - 1\\)，所以樣本標準差同樣是來自這個公式計算結果的開根號。\n[更多技術細節12]\n使用標準差解讀變異程度的數學原理很複雜，在此先學一套簡單的規則：一筆資料裡與平均值相差一個標準差的資料大約佔68%；相差兩個標準差的資料大約佔95%；相差三個標準差的資料大約佔99.7%。只要資料繪制出的直方圖接近對稱或”鐘形曲線“，大致能用這套規則解讀資料的變異程度。但是從AFL資料的直方圖( Figure 4.2 )來看，似乎不大適用這套規則。根據 Figure 4.10 的解說，實際上只有65.3%的得分紀錄，落在AFL資料平均值的前後一個標準差之內。\n\n\n\n4.2.6 應該用那種變異量數?\n這一節學到了幾種變異量數：全距，四分位數間距，平均絕對差，變異數，以及標準差。各種量數優缺點不同，以下做個小結：\n\n全距 呈現資料的完整變異範圍。極端值無可避免地會決定全距數值大小，除非我們在意資料裡的極端值，報告全距才有用處。\n四分位數間距 呈現資料中間一半涵蓋的範圍。數值穩定且與中位數互補。多數研究者愛用。\n平均絕對差 呈現所有資料遠離平均值的平均距離。容易解讀但是有些這一節未談到的問題，讓這項量數比較不如標準差受歡迎。研究文獻中不常見到。\n變異數 呈現所有資料遠離平均值的平均距離的平方。在數學意義上能優雅解釋資料與平均值的變異，但是因為尺度單位改變，導致不易解讀。除非有數學工具不然不常在報告裡呈現，不過許多統計軟體的後台程序都會計算變異數。\n標準差 變異數的開根號。具備同樣的數學特性，但是保留測量尺度所以容易解讀。平均值做為集中量數的報告時常搭配呈現，是最常在研究文獻裡看到的變異數數。\n\n簡而言之，四分位數間距與標準差是最容易解讀而且最常見的變異量數。不過其他量數也有適用的時機。在此介紹以備同學們說不定那一天會真的用到。\n\n\n\n\n\n\n\nFigure 4.10: 直方圖著色範圍與AFL勝隊得分平均值相差一個標準差，範圍面積約佔65.3%，與正文內提到的68%有些微差異。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#skew-and-kurtosis",
    "href": "04-Descriptive-statistics.html#skew-and-kurtosis",
    "title": "4  Descriptive statistics",
    "section": "4.3 Skew and kurtosis",
    "text": "4.3 Skew and kurtosis\nThere are two more descriptive statistics that you will sometimes see reported in the psychological literature: skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit, but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n\n\n\nFigure 4.11: An illustration of skewness. On the left we have a negatively skewed data set, in the middle we have a data set with no skew, and on the right we have a positively skewed data set\n\n\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry and the easiest way to explain it is by drawing some pictures. As Figure 4.11 illustrates, if the data tend to have a lot of extreme small values (i.e., the lower tail is “longer” than the upper tail) and not so many extremely large values (left panel) then we say that the data are negatively skewed. On the other hand, if there are more extremely large values than extremely small ones (right panel) we say that the data are positively skewed. That’s the qualitative idea behind skewness. If there are relatively more values that are far greater than the mean, the distribution is positively skewed or right skewed, with a tail stretching to the right. Negative or left skew is the opposite. A symmetric distribution has a skewness of 0. The skewness value for a positively skewed distribution is positive, and a negative value for a negatively skewed distribution.\n[Additional technical detail11]\nPerhaps more helpfully, you can use jamovi to calculate skewness: it’s a check box in the ‘Statistics’ options under ‘Exploration’ - ‘Descriptives’. For the afl.margins variable, the skewness figure is \\(0.780\\). If you divide the skewness estimate by the Std. error for skewness you have an indication of how skewed the data is. Especially in small samples (N \\(<\\) 50), one rule of thumb suggests that a value of 2 or less can mean that the data is not very skewed, and a value of over 2 that there is sufficient skew in the data to possibly limit its use in some statistical analyses. Though there is no clear agreement on this interpretation. That said, this does indicate that the AFL winning margins data is somewhat skewed (\\(\\frac{0.780}{0.183} = 4.262\\)).\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of how thin or fat the tails of a distribution are, as illustrated in Figure 4.12. By convention, we say that the “normal curve” (black lines) has zero kurtosis, so the degree of kurtosis is assessed relative to this curve.\n\n\n\n\n\nFigure 4.12: An illustration of kurtosis. On the left, we have a ‘platykurtic’ distribution (kurtosis = -.95) meaning that the distribution has ‘thin’ or flat tails. In the middle we have a ‘mesokurtic’ distribution (kurtosis is almost exactly 0) which means that the tails are neither thin or fat. Finally, on the right, we have a ‘leptokurtic’ distribution (kurtosis = 2.12) indicating that the distribution has ‘fat’ tails. Note that kurtosis is measured with respect to a normal curve (black line)\n\n\n\n\nIn this Figure, the data on the left have a pretty flat distribution, with thin tails, so the kurtosis is negative and we call the data platykurtic. The data on the right have a distribution with fat tails, so the kurtosis is positive and we say that the data is leptokurtic. But the data in the middle have neither think or fat tails, so we say that it is mesokurtic and has kurtosis zero. This is summarised in Table 4.4:\n\n\n\n\nTable 4.4:  Thin to fat tails to illustrate kurtosis \n\nEnglishinformal termkurtosis value\n\n\"tails too thin\"platykurticnegative\n\n\"tails neither thin or fat\"mesokurticzero\n\n\"tails too fat\"leptokurticpositive\n\n\n\n\n\n[Additional technical detail12]\nMore to the point, jamovi has a check box for kurtosis just below the check box for skewness, and this gives a value for kurtosis of \\(0.101\\) with a standard error of \\(0.364\\). This means that the AFL winning margins data has only a small kurtosis, which is ok."
  },
  {
    "objectID": "04-Descriptive-statistics.html#descriptive-statistics-separately-for-each-group",
    "href": "04-Descriptive-statistics.html#descriptive-statistics-separately-for-each-group",
    "title": "4  Descriptive statistics",
    "section": "4.4 Descriptive statistics separately for each group",
    "text": "4.4 Descriptive statistics separately for each group\nIt is very commonly the case that you find yourself needing to look at descriptive statistics broken down by some grouping variable. This is pretty easy to do in jamovi. For instance, let’s say I want to look at the descriptive statistics for some clinical trial data, broken down separately by therapy type. This is a new data set, one that you’ve never seen before. The data is stored in the clinicaltrial.csv file and we’ll use it a lot later on in Chapter 13 (you can find a complete description of the data at the start of that chapter). Let’s load it and see what we’ve got (Figure 4.13):\nEvidently there were three drugs: a placebo, something called “anxifree” and something called “joyzepam”, and there were 6 people administered each drug. There were 9 people treated using cognitive behavioural therapy (CBT) and 9 people who received no psychological treatment. And we can see from looking at the ‘Descriptives’ of the mood.gain variable that most people did show a mood gain (\\(mean = 0.88\\)), though without knowing what the scale is here it’s hard to say much more than that. Still, that’s not too bad. Overall I feel that I learned something from that.\nWe can also go ahead and look at some other descriptive statistics, and this time separately for each type of therapy. In jamovi, check Std. deviation, Skewness and Kurtosis in the ‘Statistics’ options. At the same time, transfer the therapy variable into the ‘Split by’ box, and you should get something like Figure 4.14.\n\n\n\n\n\nFigure 4.13: A screenshot of jamovi showing the variables stored in the clinicaltrial.csv file\n\n\n\n\nWhat if you have multiple grouping variables? Suppose you want to look at the average mood gain separately for all possible combinations of drug and therapy. It is possible to do this by adding another variable, drug, into the ‘Split by’ box. Easy peasy, though sometimes if you split too much there isn’t enough data in each breakdown combination to make meaningful calculations. In this case jamovi tells you this by stating something like NaN or Inf. 13\n\n\n\n\n\nFigure 4.14: A screenshot of jamovi showing Descriptives split by therapy type"
  },
  {
    "objectID": "04-Descriptive-statistics.html#sec-Standard-scores",
    "href": "04-Descriptive-statistics.html#sec-Standard-scores",
    "title": "4  描述統計",
    "section": "4.5 標準分數",
    "text": "4.5 標準分數\n想像同學們今天一起做了一份“壞脾氣量表”，量表一供有50項二選一單選題，答“是”就得1分。假設這項量表已經有非常龐大的常模樣本，常模資料分佈相當接近常態分佈，平均得分為17分，標準差是5。今天某位同學的得分是35分，那麼他的壞脾氣程度有多少？也許有人會說可以用 \\(\\frac{35}{50}\\) 或者 70% 代表這位同學的壞脾氣程度。但是想一想用這些數值代表一個人的壞脾氣都有點奇怪。只要編製測驗的人改一下題目敘述，受測者的答題反應就會改變，全部樣本的平均分數也會變高或變低。因此70%只是代表這位同學在某版本的壞脾氣量表得到的分數，所以得分比例並不能提供什麼有用的資訊。\n比較這位同學的分數和其他人的分數，是個簡單的描述壞脾氣程度的方法。如果編製者的常模是收集了一百萬份樣本，只有159人的壞脾氣得分高於35分，也就是說這位同學的壞脾氣排序是前0.016%。這麼一來就比較能解釋資料了。轉換原始分數為可相對比較的分數，這種程序稱為“標準化”(standardisation)。將原始分數轉換為相對百分比相當簡單，但是百分比有個限制：如果常模樣本只有1000人，平均值是16，標準差是5，那麼35分是所有樣本得分的第一高分，無法說明這位同學的脾氣有多壞。\n不過還不用絕望。我們可以將原始評量得分轉換為標準分數(standard score)，也是一般報告稱呼的z分數。標準分數的定義是原始分數與平均分數的差異相當於多少標準差。這句話可以寫成像以下的數學公式：\n\\[\n\\text{標準分數} = \\frac{\\text{原始分數} - 平均數}{\\text{標準差}}\n\\]\n[更多技術細節16]\n現在我們可以將這位同學的評量得分轉換為標準化壞脾氣分數。\n\\[ z =\\frac{35 - 17}{5} = 3.6 \\] 要解釋這個分數的意義，請回憶 Section 4.2.5 提到的簡易判斷法則：在常態分佈的資料中，比平均值大於3個標準差的數值，大約佔99.7%。所以3.6代表這位同學真的是位壞脾氣的人物。如果有興趣詳細探究，3.6代表在樣本中高於99.98%的受測者。\n除了能了解原始分數在龐大樣本裡的相對高低，標準分數還有另一種有用的功能：可以比較同一位受測者的不同份量表分數。想像有這位同學後來做了另一份”外向度量表“，一共有24題。外向度量表的常模樣本平均值是13，樣本標準差是4，而這位同學的原始外向度得分是2。這個分數無法直接與他的壞脾氣量表得分35分，硬要比的話就變成拿蘋果和橘子比較了。\n如果用標準分數就不同了。我們已經知道他的壞脾氣標準分數\\((z = \\frac{(35-17)}{5}=3.6)\\) 還有外向度標準分數 \\((z = \\frac{(2-13)}{4}=-2.75)\\)。兩項標準分數是指向原始數值在各自常模樣本的相對位置，才可以互相比較17。這位同學明顯比大多數人不外向(\\(z = -2.75\\))，脾氣也比大多數人壞 (\\(z=3.6\\))。而且標準分數的絕對值說明，這位同學給人壞脾氣的印象(\\(3.6\\))，明顯高於他不夠外向(\\(2.75\\))。我們能這樣斷言，是因為z分數是指出資料在變項代表的群體之間相對位置，如此才能比較不同變項的標準分數。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#summary",
    "href": "04-Descriptive-statistics.html#summary",
    "title": "4  Descriptive statistics",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nCalculating some basic descriptive statistics is one of the very first things you do when analysing real data, and descriptive statistics are much simpler to understand than inferential statistics, so like every other statistics textbook I’ve started with descriptives. In this chapter, we talked about the following topics:\n\nMeasures of central tendency. Broadly speaking, central tendency measures tell you where the data are. There’s three measures that are typically reported in the literature: the mean, median and mode.\nMeasures of variability. In contrast, measures of variability tell you about how “spread out” the data are. The key measures are: range, standard deviation, and interquartile range.\nSkew and kurtosis. We also looked at assymetry in a variable’s distribution (skew) and thin or fat tailed distributions (kurtosis).\nDescriptive statistics separately for each group. Since this book focuses on doing data analysis in jamovi, we spent a bit of time talking about how descriptive statistics are computed for different subgroups.\nStandard scores. The z-score is a slightly unusual beast. It’s not quite a descriptive statistic, and not quite an inference. Make sure you understand this section. It’ll come up again later.\n\nIn the next Chapter we’ll move on to a discussion of how to draw pictures! Everyone loves a pretty picture, right? But before we do, I want to end on an important point. A traditional first course in statistics spends only a small proportion of the class on descriptive statistics, maybe one or two lectures at most. The vast majority of the lecturer’s time is spent on inferential statistics because that’s where all the hard stuff is. That makes sense, but it hides the practical everyday importance of choosing good descriptives. With that in mind…"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#安裝jamovi",
    "href": "03-Getting-started-with-jamovi.html#安裝jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.1 安裝jamovi",
    "text": "3.1 安裝jamovi\n好啦，推銷結束。開始上課吧。就像任何應用軟體，jamovi需要安裝能執行各種軟體工具還有免費遊戲的”電腦”。這樣說好像”電腦”和各家平板沒什麼分別，不過如果你有使用套裝軟體的經驗，你就會懂我的意思。無論如何，jamovi可從網路免費取得，現誰你可以從jamovi官網https://www.jamovi.org/下載安裝程式2。\n\n首先在官網首頁按下”jamovi Desktop”的大標題按鈕，讀者會看到給Windows, Mac, 還有Linux作業系統使用者的各種安裝程式超連結。只要點選符合同學用的作業系統規格，就能按照視窗指示完成下載安裝程序。翻譯這本電子書時，jamovi版本已經來到2.3.21，請留意每隔幾個月會有新版本上架，有需要的話得要安裝新版本。3\n\n\n\n3.1.1 啟動jamovi\n不論讀者的作業系統是什麼，安裝成功就可以啟動jamovi開始第一次接觸。首次啟動的jamovi視窗介面應該像 Figure 3.1。\n\n\n\n\n\n\nFigure 3.1: 啟動jamovi!\n\n\n\n\n視窗左半區塊很像Excel試算表(本書名稱”試算表介面”），右半區塊是顯示統計測試結果的地方（以下稱”報表介面”）。中間邊界線區隔區塊範圍，使用者可以水平拖曳改變左右區塊的面積。\n\n使用者可以在試算表區域的任意格子內輸入任何值，就像使用試算表軟體一樣。還有，已存在電腦裡的資料檔案是CSV (.csv)的話，可以直接載入jamovi。另外，jamovi可以直接載入SPSS, SAS, Stata, 還有JASP等軟體的資料格式檔案。要開啟檔案請先選擇管理面板的File(按左上角三條橫線開啟)，再選擇’Open’，然後從’Browse’視窗選擇你要載入的檔案，這種操作可以開啟範例檔案或已存在的資料檔案。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#分析模組",
    "href": "03-Getting-started-with-jamovi.html#分析模組",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.2 分析模組",
    "text": "3.2 分析模組\n如 Figure 3.1 界面上方的一排圖示是可以選擇使用的分析方法模組。選好要使用的分析方法，就會出現該模組的“選項視窗”。“選項視窗”可以指定要分析的變項，選擇分析方法的設定。任何在“選項視窗”的有效動作，會在報表介面顯示分析結果，每次增加或修改設定都會即時更新報表介面內容。\n\n只要你認為報表介面顯示的結果已經足夠，按下“選項視窗”右上角巨大的右箭頭，就能關閉“選項視窗”。若是想再重新調整選項，只要在報表介面任何一處點一下，就可以重新開啟“選項視窗”。這樣的操作讓你或你的同伴回顧報表結果如何生成。\n\n若是報表介面內的某項分析結果不再需要了，你也可以透過副選單移除這項分析結果。以滑鼠右鍵點擊要移除的結果，就會跳出副選單。選擇副選單的Analysis，再選擇Remove就能移除結果。這些就是jamovi的基本操作，接著我們會看到更多。首先來了解試算表介面有什麼功能可以操作。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#資料試算表",
    "href": "03-Getting-started-with-jamovi.html#資料試算表",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.3 資料試算表",
    "text": "3.3 資料試算表\n載入jamovi的資料是以試算表的模樣呈現在試算表介面，每一欄代表“變項”，每一列代表一筆“個案”或“受測者”。\n\n\n3.3.1 變項\njamovi試算表介面最常見的變項是資料變項(Data Variables)4，純粹展示從資料檔匯入的數值，或者使用者自行輸入的數值。每個資料變項能設定為各種測量級別(見 Figure 3.2 )\n\n\n\n\n\n\nFigure 3.2: 測量級別設定面板\n\n\n\n\n變項標題旁的圖示提示使用者每個變項的測量級別。ID是jamovi特有的變項型態。這個型態用來表示不會放進分析模組的變項。像是參與者姓名或者編號。在處理規模巨大的資料集時，指定某些變項為ID能提供運算效率。\n\n名義變項(Nominal)是表示數值是文字標籤的類別變項，像是性別的標籤是”男”,“女”,“跨性別”等名義數值。人名也是一樣。名義變項數值也可以用數字表現。某些資料集的名義變項會用數字而不是文字做為標籤，像是男生表示為1，女生表示為2。稍後介紹的“變項編輯器”，可以將數字轉換為“人類可讀的”標籤。\n\n次序變項(Ordinal)很像名義變項，除了數值之間有順序性。例如李克特七點量表的數值3代表”非常同意”，數值-3代表”非常不同意”。\n\n連續變項(Continuous)表示來自連續量表的數值，像是身高或體重。又稱”等距尺度“(Interval scale)或”等比尺度“(Ratio scale)。\n\n除此之外，使用者還可以設定資料型態：包括”文字”(Text)，“整數”(Integer)，“十進制小數”(Decimal)。\n\n在空白試算表內任意輸入任何數值，jamovi會根據你輸入的資料自動判斷變項的資料型態。你可以借助這套功能掌握什麼樣的資料適用何種變項尺度。開啟一份資料檔時，jamovi也會自動判斷每一欄的資料型態。不過自動判斷的資料型態不一定是正確的，有必要的話你需要使用變項編輯器手動設定正確的資料型態。\n\n在資料面板(Data)點選Setup圖示或雙擊變項標題都能開啟變項編輯器。在變項編輯器的介面，你可以更改變項名稱、更動變項型態、標籤順序、還有標籤文字。任何變動馬上會在試算表介面看到改變的效果。要關閉變項編輯器，只要點選右上角的向上箭頭。\n\n要增加或插入新變項，只要點選資料面板上方的Add圖示。不只是增加資料變項，也能增加計算變項(computed variables)\n\n\n\n3.3.2 自訂計算變項\n計算變項存儲其他變項的計算結果。可存儲的數值相當多樣，像是log轉換，z分數，變項數值總和，正負轉換與跨變項平均值。\n\n在資料面板上方按下Add圖示，就能在已匯入的資料變項之間插入計算變項。接著會出現方程式編輯區讓你設定計算公式。常用的四則運算符號都能輸入，以下是一些可輸入的公式：\n\nA + B\nLOG10(len)\nMEAN(A, B)\n(len - VMEAN(len)) / VSTDEV(len)\n以上公式依序是變項A與變項B的總和，變項len的log轉換(底數為10)，變項A與變項B的平均值，以及變項len的z分數5。 Figure 3.3 示範建立新變項存儲變項len的z分數(這份資料來自Tooth Growh練習資料集)。\n\n\n\n\n\n\nFigure 3.3: 設定計算變項zscore-len存放變項len的z分數。\n\n\n\n\n\n3.3.2.1 垂直計算函式\nFigure 3.3 的面板裡有個可開啟捲動清單的 \\(f_x\\) 圖示，能選擇已內建的函式。其中有些以V開頭的函式是垂直計算函式。相對於沒有V開頭的同名函式，垂直計算函式用於計算資料變項的總計數值。例如函式MEAN(A, B)是計算變項A與變項B同一列的平均值，而函式VMEAN(A)製造的計算變項內容都是變項A的平均值。\n\n\n\n\n3.3.3 複製貼上\njamovi產生的報表內表格及統計圖符合美國心理學會(APA)建議的學術發表格式。使用者可以使用內建的複製貼上功能，將圖表加到WORD文件、或者email裡給同事。直要對想複製的圖表以滑鼠右鍵點選，開啟副選單即可選擇複製功能。透過副選單能指定要複製報表裡的某張圖或整個分析表格。jamovi複製的操作與其他應用程式的操作完全相同。在稍後的分析示範裡你可以練習看看。\n\n\n\n\n3.3.4 程式碼模式\njamovi提供”R程式碼模式”(R Syntax Mode)。開啟此模式能檢視分析模組後台的R程式碼。從jamovi主畫面右上方開啟系統設定面板(圖示是三個垂直小點)，勾選”Syntax Mode)旁邊的核取方塊就能開啟或關閉。\n“R程式碼模式”開啟時運作方式沒有不同，除了每份報表會增加R程式碼，還有表格會以ASCII碼的模式輸出，如同R語言的終端介面。如果你有支援編譯R語言的軟體，可以直接將R程式碼複製到編譯介面測試看看。至此還沒有正式載入任何資料，所以你看到的R程式碼複製到編譯介面，並不會產生任何結果。如果你有興趣測試的話，網路上有許多學習資源，等著你去發掘。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#載入資料檔案",
    "href": "03-Getting-started-with-jamovi.html#載入資料檔案",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.4 載入資料檔案",
    "text": "3.4 載入資料檔案\n在實際的資料分析場景裡，我們通常要處理好幾份不同格式的檔案。本書示範的資料分析所處理的檔案主要有兩種格式：\n\njamovi專案檔(.omv) 這是jamovi儲存資料，變項及分析結果的標準檔案格式。\n逼號分隔值文字檔(.csv) 歷史最久的標準文字檔案格式，可以處理文字檔案的軟體幾乎都可以開啟。由於格式簡單，專業統計分析人員首選以csv格式儲存資料。\n\njamovi也能載入其他格式的資料檔案。最新版本支援開啟微軟Excel試算表檔案(.xls)，還有市場佔有率高的商用套裝統計軟體，如SPSS、SAS的資料格式檔案。無論你要處理的資料是用那種格式儲存，強烈建議先新增一個或一群資料夾，分別儲存資料檔案或jamovi檔，以便定期備份。\n\n\n3.4.1 匯入csv格式資料檔\n在現代資料分析軟體發展史裡，到處都能見到csv檔的身影。csv檔可儲存任何表格化的資料，如同 Figure 3.4 展示的本章示範資料檔 booksales.csv 。每一列記錄每個月書籍銷售資料，不過第一列不是資料，而是變項名稱。\n\n\n\n\n\n\n\nFigure 3.4: booksales.csv的資料內容。畫面左方是以試算表軟體OpenOffice開啟的視窗，可見到清楚的表格標記。畫面右方是以純文字編輯器開啟的視窗，可見到逗號標記。試算表裡的每一格在csv檔是以逗號間隔。\n\n\n\n\njamovi能輕鬆開啟csv檔。按下主畫面左上角三條平行線的圖示鈕開啟面板，點選Open，再點選This Device，就能從個人電腦資料夾裡選擇要開啟的csv檔6。 Figure 3.5 是以Mac作業系統展示瀏覽和選取檔案的畫面。我相信讀者應該熟悉自己使用的設備，應該能找到自己想匯入的csv檔。請放手試試看吧。\n\n\n\n\n\n\nFigure 3.5: Mac作業系統的檔案瀏覽及對話視窗，相信Mac使用者都很熟悉。Windows系統有類似的檔案總管，選取檔案的操作大同小異。\n\n\n\n\n這裡有些標記提示，能幫助你開啟的csv資料檔案能順利進行分析：\n\n變項名稱。確認csv檔第一列是各變項的名稱。如同 booksales.csv 的示範。\n小數點。確認數字資料的小數點是以英文句號(.)表示。在英語系國家是慣例，但是部分如東歐國家是以英文逼號(,)表示小數點。\n引號。變項名稱與文字資料都會放在兩個引號(“)之間，jamovi會以引號自動判斷變項資料的型態。請參考 booksales.csv 的示範。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#匯入不常見格式的資料檔",
    "href": "03-Getting-started-with-jamovi.html#匯入不常見格式的資料檔",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.5 匯入不常見格式的資料檔",
    "text": "3.5 匯入不常見格式的資料檔\nThroughout this book I’ve assumed that your data are stored as a jamovi .omv file or as a “properly” formatted csv file. However, in real life that’s not a terribly plausible assumption to make so I’d better talk about some of the other possibilities that you might run into.\n\n3.5.1 資料檔是純文字格式\nThe first thing I should point out is that if your data are saved as a text file but aren’t quite in the proper csv format then there’s still a pretty good chance that jamovi will be able to open it. You just need to try it and see if it works. Sometimes though you will need to change some of the formatting. The ones that I’ve often found myself needing to change are:\n\nheader. A lot of the time when you’re storing data as a csv file the first row actually contains the column names and not data. If that’s not true then it’s a good idea to open up the csv file in a spreadsheet programme such as Open Office and add the header row manually.\nsep. As the name “comma separated value” indicates, the values in a row of a csv file are usually separated by commas. This isn’t universal, however. In Europe the decimal point is typically written as , instead of . and as a consequence it would be somewhat awkward to use , as the separator. Therefore it is not unusual to use ; instead of , as the separator. At other times, I’ve seen a TAB character used.\nquote. It’s conventional in csv files to include a quoting character for textual data. As you can see by looking at the booksales.csv file, this is usually a double quote character, “. But sometimes there is no quoting character at all, or you might see a single quote mark ’ used instead.\nskip. It’s actually very common to receive CSV files in which the first few rows have nothing to do with the actual data. Instead, they provide a human readable summary of where the data came from, or maybe they include some technical info that doesn’t relate to the data.\nmissing values. Often you’ll get given data with missing values. For one reason or another, some entries in the table are missing. The data file needs to include a “special” value to indicate that the entry is missing. By default jamovi assumes that this value is 995, for both numeric and text data, so you should make sure that, where necessary, all missing values in the csv file are replaced with 99 (or -9999; whichever you choose) before opening / importing the file into jamovi. Once you have opened / imported the file into jamovi all the missing values are converted to blank or greyed out cells in the jamovi spreadsheet view. You can also change the missing value for each variable as an option in the Data - Setup view.\n\n\n\n3.5.2 套裝軟體專用格式(如SPSS)\nThe commands listed above are the main ones we’ll need for data files in this book. But in real life we have many more possibilities. For example, you might want to read data files in from other statistics programs. Since SPSS is probably the most widely used statistics package in psychology, it’s worth mentioning that jamovi can also import SPSS data files (file extension .sav). Just follow the instructions above for how to open a csv file, but this time navigate to the .sav file you want to import. For SPSS files, jamovi will regard all values as missing if they are regarded as “system missing” files in SPSS. The ‘Default missings’ value does not seem to work as expected when importing SPSS files, so be aware of this - you might need another step: import the SPSS file into jamovi, then export as a csv file before re-opening in jamovi.6\nAnd that’s pretty much it, at least as far as SPSS goes. As far as other statistical software goes, jamovi can also directly open / import SAS and STATA files.\n\n\n3.5.3 微軟Excel資料格式\nA different problem is posed by Excel files. Despite years of yelling at people for sending data to me encoded in a proprietary data format, I get sent a lot of Excel files. The way to handle Excel files is to open them up first in Excel or another spreadsheet programme that can handle Excel files, and then export the data as a csv file before opening / importing the csv file into jamovi."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#安裝jamovi擴充模組",
    "href": "03-Getting-started-with-jamovi.html#安裝jamovi擴充模組",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.7 安裝jamovi擴充模組",
    "text": "3.7 安裝jamovi擴充模組\njamovi的最大賣點是使用者能依實際需要，自行從擴充模組庫安排需要的模組。模組都是由jamovi社區成員熱情開發貢獻。當你有需要進行較複雜的統計分析，可以找找模組庫裡有沒有符合目的的模組。\n安裝模組的操作很簡單，只要按一下主畫面右上方的大十字，就能開啟如 Figure 3.6 的清單視窗。接著如同逛購物網站一樣，找到想用的模組，按下”Install”按鈕，等幾秒後，主畫面”Analysis”面板上方顯示模組圖示，就能開始使用。推薦安裝的模組有”scatr”(安裝後加強”Descriptives”模組功能)，“lsj-data”(本書示範檔案集合，安裝後開啟檔案”Open”選單增加”Data Library”，以及\\(R_j\\)(R程式碼編譯介面)。10\n\n\n\n\n\n\nFigure 3.6: jamovi擴充模組庫管理面板"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#關閉jamovi",
    "href": "03-Getting-started-with-jamovi.html#關閉jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.8 關閉jamovi",
    "text": "3.8 關閉jamovi\n本章結束前來談談如何正確關閉jamovi。操作不難，就像關閉任何應用程式一樣。但是，關閉前一定要記得存檔！統計軟體的存檔其實分成兩種：儲存更新過的資料檔；儲存資料分析結果。\n更新的資料檔最好另存新檔，保留原始檔案能避免悲劇發生。步驟是從檔案管理面板(主畫面左上角三條橫線)開啟，選擇Export，再選擇檔案格式(建議csv)，取好新檔案名稱後，按Enter鍵或點按右上方的”Export”都能完成另存新檔。\n另外，儲存為jamovi專案檔(.omv)就會包括資料及分析結果兩部分。步驟是從檔案管理面板(主畫面左上角三條橫線)開啟，選擇Save as，取好檔案名稱後，按Enter鍵或點按右上方的”Save”都能完成存檔。請記得要為存檔的資料夾找個容易找到的地方，特定的檔案用特定資料夾區分。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#本章小節",
    "href": "03-Getting-started-with-jamovi.html#本章小節",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 本章小節",
    "text": "3.9 本章小節\n到這裡還沒有真的開始處理資料，下一章才是真正的開始。\nEvery book that tries to teach a new statistical software program to novices has to cover roughly the same topics, and in roughly the same order. Ours is no exception, and so in the grand tradition of doing it just the same way everyone else did it, this chapter covered the following topics:\n\n安裝jamovi. We downloaded and installed jamovi, and started it up.\n分析模組. We very briefly oriented to the part of jamovi where analyses are done and results appear, but then deferred this until later in the book.\n資料試算表 We spent more time looking at the spreadsheet part of jamovi, and considered different variable types, and how to compute new variables.\n[Loading data in jamovi]. We also saw how to load data files in jamovi.\n[Importing unusual data files]. Then we figured out how to open other data files, from different file types.\n[Changing data from one level to another]. And saw that sometimes we need to coerce data from one type to another.\n[Installing add-on modules into jamovi]. Installing add-on modules from the jamovi community really extends jamovi capabilities.\n[Quitting jamovi]. Finally, we looked at good practice in terms of saving your data set and analyses when you have finished and are about to quit jamovi.\n\nWe still haven’t arrived at anything that resembles data analysis. Maybe the next Chapter will get us a bit closer!"
  },
  {
    "objectID": "05-Drawing-graphs.html#sec-Histograms",
    "href": "05-Drawing-graphs.html#sec-Histograms",
    "title": "5  繪製統計圖",
    "section": "5.1 直方圖",
    "text": "5.1 直方圖\n直方圖(Histograms)是製作方式最簡單且易懂的資料視覺化工具，主要用途是概覽等距尺度或比例尺度資料變項(例如 Chapter 4 示範的afl.margins資料集)的分佈趨勢。同學們可能在某些課程或網路媒體見過用直方圖解釋的研究證據，現在要學會如何製作直方圖。製作方式是先設定變項數值分成數個間值(bins)，接著計算每個間值之間有多少資料落入。每個間值之間的資料個數稱為次數(frequency)或密度(density)，並繪製對應高度的長條就完成了。 Chapter 4 的 Figure 4.2 展示的直方圖最左邊的長條對應勝隊得分小於10分的場次總數，一共有33場。這份直方圖是用R語言套件製作的，超出本書的學習範圍，所以在此說明如何使用jamovi繪製接近該作品的直方圖。請開啟描述統計模組選單(Exploration-Descriptives)，將變項放到”Variable”視窗後展開最下方plot次選單，勾選histogram就會看到如 Figure 5.1 的畫面。jamovi預設的直方圖y軸是密度，x軸是變項名稱。間值是jamovi自動指定，y值並不是顯示各間值之間的資料數目。雖然如此，這份直方圖已充分展示這筆資料變項的分佈形狀，你覺得這筆資料是常態分佈還是有偏態或峰度呢？分析實務對資料的第一印象就是從繪製直方圖(Histograms)開始。\n\n\n\n\n\n\nFigure 5.2: 使用jamovi繪製直方圖示範畫面\n\n\n\n\n值得一提jamovi有繪製”密度”曲線的功能。只要開啟”Histogram”選項之下的”Density“方塊，並取消勾選”Histogram”，就可以得到 Figure 5.3 的作品。只要資料數值是連續的，密度曲線能將分佈趨勢視覺化。密度曲線是直方圖的變形，jamvoi採用kernel smoothing演算法繪製，原理是將資料中的雜訊平滑化，使得曲線看起來平滑。使用密度曲線判斷資料變項分佈趨勢，比直方圖更佳，因為曲線不受間值設定影響。如果這份資料只用4個間值繪製直方圖，看起來絕對和用20個間值繪製的直方圖不一樣。\n\n\n\n\n\n\nFigure 5.3: 用Jamovi繪製資料變項afl.margins的密度曲線。\n\n\n\n\n雖然這裡示範的直方圖和密度曲線還需要一些加工才能用在報告，至少已經為完善描述統計工作給出清楚方向。直方圖或密度曲線的最大用處是顯示資料的變異趨勢，讓分析者掌握資料特性。直方圖的劣勢是無法有效簡化資料訊息，本書稍後會展示將20多個直方圖塞在一張圖裡有多讓人眼花燎亂。最後請注意，直方圖不能將名義尺度資料視覺化。"
  },
  {
    "objectID": "05-Drawing-graphs.html#boxplots",
    "href": "05-Drawing-graphs.html#boxplots",
    "title": "5  繪製統計圖",
    "section": "5.2 Boxplots",
    "text": "5.2 Boxplots\nAnother alternative to histograms is a boxplot, sometimes called a “box and whiskers” plot. Like histograms they’re most suited to interval or ratio scale data. The idea behind a boxplot is to provide a simple visual depiction of the median, the interquartile range, and the range of the data. And because they do so in a fairly compact way boxplots have become a very popular statistical graphic, especially during the exploratory stage of data analysis when you’re trying to understand the data yourself. Let’s have a look at how they work, again using the afl.margins data as our example.\n\n\n\n\n\nFigure 5.4: A box plot of the afl.margins variable plotted in jamovi\n\n\n\n\nThe easiest way to describe what a boxplot looks like is just to draw one. Click on the ‘Box plot’ check box and you will get the plot shown on the lower right of Figure 5.4. jamovi has drawn the most basic boxplot possible. When you look at this plot this is how you should interpret it: the thick line in the middle of the box is the median; the box itself spans the range from the 25th percentile to the 75th percentile; and the “whiskers” go out to the most extreme data point that doesn’t exceed a certain bound. By default, this value is 1.5 times the interquartile range (IQR), calculated as 25th percentile - (1.5*IQR) for the lower boundary, and 75th percentile + (1.5*IQR) for the upper boundary. Any observation whose value falls outside this range is plotted as a circle or dot instead of being covered by the whiskers, and is commonly referred to as an outlier. For our AFL margins data there are two observations that fall outside this range, and these observations are plotted as dots (the upper boundary is 107, and looking over the data column in the spreadsheet there are two observations with values higher than this, 108 and 116, so these are the dots).\n\n5.2.1 Violin plots\n\n\n\n\n\nFigure 5.5: A violin plot of the afl.margins variable plotted in jamovi, also showing a box plot and data points\n\n\n\n\nA variation to the traditional box plot is the violin plot. Violin plots are similar to box plots except that they also show the kernel probability density of the data at different values. Typically, violin plots will include a marker for the median of the data and a box indicating the interquartile range, as in standard box plots. In jamovi you can achieve this sort of functionality by checking both the ‘Violin’ and the ‘Box plot’ check boxes. See Figure 5.5, which also has the ‘Data’ check box turned on to show the actual data points on the plot. This does tend to make the graph a bit too busy though, in my opinion. Clarity is simplicity, so in practice it might be better to just use a simple box plot.\n\n\n5.2.2 Drawing multiple boxplots\nOne last thing. What if you want to draw multiple boxplots at once? Suppose, for instance, I wanted separate boxplots showing the AFL margins not just for 2010 but for every year between 1987 and 2010. To do that the first thing we’ll have to do is find the data. These are stored in the aflmarginbyyear.csv file. So let’s load it into jamovi and see what is in it. You will see that it is a pretty big data set. It contains 4296 games and the variables that we’re interested in. What we want to do is have jamovi draw boxplots for the margin variable, but plotted separately for each year. The way to do this is to move the year variable across into the ‘Split by’ box, as in Figure 5.6.\n\n\n\n\n\nFigure 5.6: jamovi screen shot showing the ‘Split by’ window\n\n\n\n\nThe result is shown in Figure 5.7. This version of the box plot, split by year, gives a sense of why it’s sometimes useful to choose box plots instead of histograms. It’s possible to get a good sense of what the data look like from year to year without getting overwhelmed with too much detail. Now imagine what would have happened if I’d tried to cram 24 histograms into this space: no chance at all that the reader is going to learn anything useful.\n\n\n\n\n\nFigure 5.7: Multiple boxplots plotted in jamovi, for the margin by year variables\n\n\n\n\n\n\n5.2.3 Using box plots to detect outliers\nBecause the boxplot automatically separates out those observations that lie outside a certain range, depicting them with a dot in jamovi, people often use them as an informal method for detecting outliers: observations that are “suspiciously” distant from the rest of the data. Here’s an example. Suppose that I’d drawn the boxplot for the AFL margins data and it came up looking like Figure 5.8. It’s pretty clear that something funny is going on with two of the observations. Apparently, there were two games in which the margin was over 300 points! That doesn’t sound right to me. Now that I’ve become suspicious it’s time to look a bit more closely at the data. In jamovi you can quickly find out which of these observations are suspicious and then you can go back to the raw data to see if there has been a mistake in data entry. One way to do this is to tell jamovi to label the outliers, by checking the box next to the Box plot check box. This adds a row number label next to the outlier in the boxplot, so you can go look at that row and find the extreme value. Another, more flexible way, is to set up a filter so that only those observations with values over a certain threshold are included. In our example, the threshold is over 300, so that is the filter we will create. First, click on the ‘Filters’ button at the top of the jamovi window, and then type ‘margin > 300’ into the filter field, as in Figure 5.9.\n\n\n\n\n\nFigure 5.8: A boxplot showing two very suspicious outliers!\n\n\n\n\nThis filter creates a new column in the spreadsheet view where only those observations that pass the filter are included. One neat way to quickly identify which observations these are is to tell jamovi to produce a ‘Frequency table’ (in the ‘Exploration’ - ‘Descriptives’ window) for the ID variable (which must be a nominal variable otherwise the Frequency table is not produced). In Figure 5.10 you can see that the ID values for the observations where the margin was over 300 are 14 and 134. These are suspicious cases, or observations, where you should go back to the original data source to find out what is going on.\n\n\n\n\n\nFigure 5.9: The jamovi filter screen\n\n\n\n\n\n\n\n\n\nFigure 5.10: Frequency table for ID showing the ID numbers for the two suspicious outliers, 14 and 134\n\n\n\n\nUsually you find that someone has just typed in the wrong number. Whilst this might seem like a silly example, I should stress that this kind of thing actually happens a lot. Real world data sets are often riddled with stupid errors, especially when someone had to type something into a computer at some point. In fact, there’s actually a name for this phase of data analysis and in practice it can take up a huge chunk of our time: data cleaning. It involves searching for typing mistakes (“typos”), missing data and all sorts of other obnoxious errors in raw data files.\nFor less extreme values, even if they are flagged in a a boxplot as outliers, the decision about whether to include outliers or exclude them in any analysis depends heavily on why you think the data look they way they do and what you want to use the data for. You really need to exercise good judgement here. If the outlier looks legitimate to you, then keep it. In any case, I’ll return to the topic again in Section 12.10 in Chapter 12."
  },
  {
    "objectID": "05-Drawing-graphs.html#sec-Bar-graphs",
    "href": "05-Drawing-graphs.html#sec-Bar-graphs",
    "title": "5  繪製統計圖",
    "section": "5.3 柱狀圖",
    "text": "5.3 柱狀圖\n柱狀圖(bar graph)也是同學們常見到的資料視覺化作品，主要用來展示名義尺度變項的分佈趨勢。我們用 Section 4.1.6 這一節示範眾數的資料變項afl.finalists，來示範如何繪製。只要將出現在變項裡的隊名擺在x軸，再將每一隊打入季後賽的次數繪製成對應高度的柱子就完成了。這筆資料有很多隊伍，我們只抓四支隊伍做個示範，他們是Brisbane, Carlton, Fremantle 以及 Richmond。請同學先點選主介面左下角的漏斗圖示，開啟jamovi的Filters功能選單，複製下列視窗內的所有文字與符號(點一下視窗右側的圖示即可複製)，再貼到Filters選單對話視窗的”=“之後，關閉選單即可生效4。\n\nafl.finalists == 'Brisbane' or afl.finalists == 'Carlton' or afl.finalists == 'Fremantle' or afl.finalists == 'Richmond'\n\n在試算表介面你會看到有的數值被反白，代表該項資料已經被過濾了。接著照舊開啟描述統計模組選單，這次勾選Bar plot選項(請記得變項要放到Variables視窗)，就會看到如 Figure 5.11 的柱狀圖成品。\n\n\n\n\n\n\nFigure 5.11: 只抓四支AFL球隊資料繪製的柱狀圖示範畫面。"
  },
  {
    "objectID": "05-Drawing-graphs.html#saving-image-files-using-jamovi",
    "href": "05-Drawing-graphs.html#saving-image-files-using-jamovi",
    "title": "5  繪製統計圖",
    "section": "5.4 Saving image files using jamovi",
    "text": "5.4 Saving image files using jamovi\nHold on, you might be thinking. What’s the good of being able to draw pretty pictures in jamovi if I can’t save them and send them to friends to brag about how awesome my data is? How do I save the picture? Simples. Just right click on the plot image and export it to a file, either as ‘png’, ‘eps’, ‘svg’ or ‘pdf’. These formats all produce nice images that you can then send to your friends, or include in your assignments or papers."
  },
  {
    "objectID": "05-Drawing-graphs.html#summary",
    "href": "05-Drawing-graphs.html#summary",
    "title": "5  繪製統計圖",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nPerhaps I’m a simple minded person, but I love pictures. Every time I write a new scientific paper one of the first things I do is sit down and think about what the pictures will be. In my head an article is really just a sequence of pictures linked together by a story. All the rest of it is just window dressing. What I’m really trying to say here is that the human visual system is a very powerful data analysis tool. Give it the right kind of information and it will supply a human reader with a massive amount of knowledge very quickly. Not for nothing do we have the saying “a picture is worth a thousand words”. With that in mind, I think that this is one of the most important chapters in the book. The topics covered were:\n\nCommon plots. Much of the chapter was focused on standard graphs that statisticians like to produce: Histograms, Boxplots and Bar graphs\nSaving image files using jamovi. Importantly, we also covered how to export your pictures.\n\nOne final thing to point out. Whilst jamovi produces some really neat default graphics, editing the plots is currently not possible. For more advanced graphics and plotting capability the packages available in R are much more powerful. One of the most popular graphics systems is provided by the ggplot2 package (see https://ggplot2.tidyverse.org/), which is loosely based on “The grammar of graphics” (Wilkinson et al., 2006). It’s not for novices. You need to have a pretty good grasp of R before you can start using it, and even then it takes a while to really get the hang of it. But when you’re ready it’s worth taking the time to teach yourself, because it’s a much more powerful and cleaner system.\n\n\n\n\n\nWilkinson, L., Wills, D., Rope, D., Norton, A., & Dubbs, R. (2006). The grammar of graphics. Springer."
  },
  {
    "objectID": "04-Descriptive-statistics.html#集中量數",
    "href": "04-Descriptive-statistics.html#集中量數",
    "title": "4  描述統計",
    "section": "4.1 集中量數",
    "text": "4.1 集中量數\n如同 Figure 4.2 展示的統計繪圖，是一種向人展現資料“要點”的絕佳方式。統計圖在精鍊資料為簡化的”總成”統計資訊相同有用。在許多實務場合的統計工作，首先要做的總成統計是計算集中量數。 也就是要呈現資料裡的“平均值”或“中位數”。以下依序介紹最常見的平均值、中位數、以及眾數，還有說明這些量數的用途。\n\n\n4.1.1 平均值\n一組資料的平均值通常指算術平均值。計算方法是將所有數值加起來，除以數值的數目。以下是拿AFL資料前五場比賽勝隊得分56, 31, 56, 8, 32，代入公式計算的平均值：\n\n\\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\]\n會用這本書學習統計的同學應該對以上定義耳熟能詳。平均值(也稱平均數)在許多日常場合都會用到。儘管很多同學都熟悉如何計算，在此我們用這套公式學習一些統計學家常用的數學記號，這樣能讓我們了解如何使用jamovi進行算術運算。\n\n首先來認識數學記號\\(N\\)(大寫N)，用於表示要計算平均值的觀察值個數(以上的例子 \\(N = 5\\))。接著是表示一組觀察值的記號，慣例用大寫X表示，再加上底標數字記號就代表個別觀察值。也就是說，\\(X_1\\)是第一筆觀察值，\\(X_2\\)是第二筆觀察值，以此類推\\(X_N\\)是最後一筆觀察值。觀察值記號用比較抽象簡化的方式使用，\\(X_i\\)是資料裡的第i筆觀察值。為了幫同學了解符號的用法，我們將afl.margins的前五場比賽紀錄，連同數學記號整理在表4-1。\n\n\n\n\n\n表4-1: afl.margins的觀察資料資訊。\n\n\n\n\n觀察值資訊\n數學記號\n觀察值數值\n\n\n\n\n勝隊得分。第1場\n\\(X_1\\)\n56分\n\n\n勝隊得分。第2場\n\\(X_2\\)\n31分\n\n\n勝隊得分。第3場\n\\(X_3\\)\n56分\n\n\n勝隊得分。第4場\n\\(X_4\\)\n8分\n\n\n勝隊得分。第5場\n\\(X_5\\)\n32分\n\n\n\n[更多技術細節2]\n\n\n\n4.1.2 平均值計算示範\n好啦，以上是純數學的說明，那要如何使用jamovi幫我們完成計算工作？特別是資料變項有成千上百個觀察值，還是使用電腦計算平均值比較簡單。第一步是點選Analysis面板上的’Exploration’按鈕，再點選’Descriptives’，就會出現如同 Figure 4.3 的選單畫面。要完成如同畫面中顯示的結果，只要點選左邊方框裡的afl.margins，將變項移動到”Variables”方框裡，馬上就能在報表介面看到顯示統計量數的表格。請同學自行試看看能否得到一樣的結果。\n\n\n\n\n\n\n\nFigure 4.3: 2010年AFL例行賽勝隊得分的預設描述統計報告。\n\n\n\n\nFigure 4.3 的報表顯示變項afl.margins的平均值是35.30。其他一起呈現的資訊有比賽場次數目(N = 176)，遺漏值數目(無)，以及中位數、最小值、最大值。\n\n\n\n4.1.3 中位數\n第二種常見的集中量數是中位數，計算方式甚至比平均值更簡單。中位數就是一組觀察值排序在中間的數值。以下同樣用AFL前五場比賽紀錄：\\(56\\), \\(31\\), \\(56\\), \\(8\\), \\(32\\)說明如何計算中位數。首先將所由數值由小到大升冪排序：\n\n8, 31, 32, 56, 56\n表面看來，以上五個數值的中位數是32，因為這個數值剛好排在中央位(以粗體字標示)。但是如果要整理的是前六場比賽紀錄的話呢？由於第6場比賽的勝隊得分是14，排序的數列如下：\n\n8, 14, 31, 32, 56, 56\n位在中間的數值有兩個，31與32。中位數就變成兩個數值的平均，也就是31.5。就像前面的例子，到這裡我們都可以用手計算。但是在真正的統計實務裡，我們不可能排序所有數值再找出中位數，還是要使用電腦完成繁複的工作。就像 Figure 4.3 的示範，jamovi已經算出afl.margins變項所有數值的中位數是30.50。\n\n\n\n4.1.4 應該要計算平均值還是中位數?\n只知道怎麼計算平均值與中位數還不能下課。我們還要知道這兩種量數指出了一組資料的什麼特徵，這也能讓我們曉得各自的正確使用時機。請看 Figure 4.4 的圖解。平均值顯示一筆資料的“重心”；中位數則是一筆資料的“中心點”。這也就是說，要用那種量數描述資料特徵，要看資料的種類，以及解釋這筆資料的目的是什麼。以下是簡要原則：\n\n名義尺度(nomral scale)資料不應使用平均值或中位數。因為兩種量數的計算要有意義，都有資料數值有大小順序的條件。如果數值之間的大小順序不明確，最好使用眾數。\n次序尺度(ordinal scale)資料應使用中位數而非平均值。中位數只會表示資料的順序資訊，不會指出數值間的差異比例。收集次序尺度資料之前已經知道不需測量觀察值之間的差異。只有測量尺度包括數值之間的差異時，才能使用平均值表現資料特徵。\n等距(interval)與等比(ratio)尺度資料都能使用平均值表達資料特徵。實際使用那一種尺度，全賴實務目的。平均值能掌握這類尺度測量的觀察值所有資訊特徵。不過請切記，平均值對於位於極端、遠離重心的數值非常敏感。\n\n關於最後一條，我們再多談一些。如 Figure 4.4 的圖解，資料直方圖呈現非對稱時，平均值與中位數必定是不一樣的值(見偏態與峰度)。圖中右邊的直方圖內的中位數接近資料密集的“軀幹”，左邊的直方圖內中位數則落在資料稀少的”尾巴“。用實際的例子來說，有三位朋友同桌聚會，Bob年收入有50,000澳幣、Kate年收入有60,000澳幣、Jane年收入有65,000澳幣。三人的收入平均值是58,333澳幣，中位數是60,000。接著年收入高達100,000,000澳幣的Bill加入他們，平均值馬上提高到25,043,750澳幣，不過中位數只有升到62,500澳幣。你也許會覺得這一桌朋友的總收入應該能用平均值總結，但是考慮到只有四人，中位數也許是更好的選擇。\n\n\n\n\n\n\n\nFigure 4.4: 圖解使用平均值與中位數解釋資料特徵的差異。平均值是一筆資料的“重心”。如果構成資料直方圖的成份是有重量的積木，平均值是能保持積木平衡不動的支點。中位數則是所有資料平分切開，代表其中一半剛好小於另一半的中間數值。\n\n\n\n\n\n\n4.1.5 真實案例\n為了弄清楚為什麼你需要注意平均值和中位數的區別，讓我們看一個現實生活中的例子。雖然我喜歡看媒體記者因為缺乏科學和統計知識而鬧的笑話，有好報導還是應該給予讚揚。請看2010年9月24日澳洲廣播公司新聞網站的這篇優秀報導3：\n\n在過去的幾個星期，澳大利亞聯邦銀行的資深高層到世界各地旅行，準備了一份簡報說明與發達程度接近的國家相比，澳大利亞房價及主要價格收入比率比較有利。『實際上過去五、六年，澳大利亞的住宅負擔能力出現偏差』聯邦銀行的首席經濟學家Craig James說道。\n\n這對於任何有按時付房貸，想要申請房貸，支付租金，或對於過去幾年澳大利亞房地產市場發生的事情全然不知情的民眾來說是重大消息。讓我們繼續看下去：\n\n澳大利亞聯邦銀行以圖表、數字還有比較各國狀況，反擊房屋末日論。銀行官員用這份簡報裡的數據，否定與澳大利亞一般家庭收入比較，住宅負擔相對昂貴的論點。簡報裡提到澳洲主要城市的房屋價格與家庭收入的比值為5.6，全國平均為4.3，與其他已開發國家的城市相比，美國舊金山和紐約的比值都是7，紐西蘭奧克蘭6.7，加拿大溫哥華9.3。\n\n聽起來很棒! 不過這篇報導接下來評論了簡報內容：\n\n許多分析家指出聯邦銀行誤用了圖表及數字，做出的結論是錯誤的。看一下簡報第四頁的表格，表格註腳提到其他國家的數值是來自一個網站——國際住宅負擔調查報告。然而，如果聯邦銀行也引用該網站對澳大利亞房價與收入比值的分析，數字應該是9，而不是5.6或4.3。\n\n嗯，出現一個相當嚴重的歧異。一群專家說是9，另一群專家說是4或5。我們能說雙方都有理，宣稱真實數字就在兩者之間？絕對不行！房價和收入的調查結果只會有一個。正如報導接著指出，國際住宅負擔調查報告的數字是正確的，澳洲聯邦銀行提供的數字是錯誤的：\n\n澳洲聯邦銀行提供的國內房價與收入數字存在一個明顯的問題，他們比較的是收入平均值與房價中位數（國際住宅負擔調查報告比較的是收入中位數與房價中位數）。中位數是數據資料中間的點，能有效剔除了最高數值和最低數值，這意味著當涉及到收入和資產價格時，平均值會被拉高，因為整體收入數據包括澳大利亞最有錢的人。換句話說：聯邦銀行計算的收入數據包括執行長拉爾夫-諾里斯（Ralph Norris）的數百萬美元年薪，但是沒有把他的豪宅算在房產價格數據裡，因此低估了澳大利亞中產受薪階級的房價與收入比。\n\n這段報導還是我自己(原作者)來寫好了。“(簡單說，)國際住宅負擔調查報告的比值計算方式是正確的，但是聯邦銀行的算法是錯的。”至於為什麽經常處理複雜數據的大銀行會犯這種低級錯誤，嗯…我沒什麼話可說，因為我對這間機構沒什麼了解。不過這篇報導剛好提到一個事實，可能與銀行出這種包有關，也可能無關:\n\n作為澳大利亞最大的房屋貸款機構，房價上漲會讓聯邦銀行有最大獲益。全澳大利亞大多數購屋族以及小型企業都是向聯邦銀行貸款。\n\n同學們，這就是社會現實呀！\n\n\n\n\n4.1.6 眾數\n從一組樣本找出眾數非常容易，就是個數最多的數值是也。我們用另一個AFL資料變項說明：那個球隊在歷年季後賽出場次數最多？請參考 Figure 4.5 開啟AFL Finallist這個檔案，看一下變項afl.finalists。這個變項的數值是包括1987到2010年200場季後賽的400支球隊隊名。\n大家可以直接掃過全部400項，然後計數每支球隊的出場次數，如此能製造一張次數表(frequency table)。不過這樣的工作相當耗神且無聊，還是交給電腦吧。同樣也是點選開啟”Exploration”的“Descriptives”選單，這次要勾選”Frequency tables”這個方塊。如同 Figure 4.6 的示範。\n從次數表可以看到，在這24年的季後賽，Geelong隊的出場次數遠多其他球隊，所以變項afl.finalists的眾數是Geelong。眾數說明Geelong(39次)是1987到2010季後賽出場最多的球隊，也能看到”Descriptives”表格並沒有呈現平均值、中位數、最小值、還有最大值。這是因為變項afl.finalists是名義尺度資料，無法計算這些量數。\n\n\n\n\n\n\n\nFigure 4.5: aflsmall finalists.csv匯入jamovi的變項顯示畫面。\n\n\n\n\n\n\n\n\n\nFigure 4.6: 變項afl.finalists的次數表在jamovi的輸出畫面。\n\n\n\n\n關於眾數還有一點要了解。儘管眾數常用來計算名義尺度資料的集中量數，是因為這類資料無法計算平均值和中位數，還是有些情況是不論資料尺度是什麼，我們都想知道眾數。我們回頭看前面的示範資料變項afl.margins，這是等比尺度變項，通常我們用平均值或中位數代表這筆資料的集中量數。不過想想這個情況：某天你的朋友想買運動彩卷賭下一場比賽的勝隊是幾分。若是不知道下一場是那隊出賽，只能隨便猜任何一個贏球分數。如果賭對了就能拿到50澳幣，賭輸了買彩卷的一塊就沒了。彩卷規則沒有所謂的“幾乎猜中”，只能賭勝隊實際會贏幾分。這時平均值或中位數都不能幫你朋友做決定，只有眾數才有參考價值。這時可以回到afl.margins的jamovi檔案，我們在”Exploration”的“Descriptives”選單勾選”Mode”，就能在描述統計報表看到眾數是3，如同 Figure 4.7 的示範。\n\n\n\n\n\n\n\nFigure 4.7: 計算變項afl.margins眾數的jamovi示範畫面。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#偏態與峰度",
    "href": "04-Descriptive-statistics.html#偏態與峰度",
    "title": "4  描述統計",
    "section": "4.3 偏態與峰度",
    "text": "4.3 偏態與峰度\n有些心理學文獻還會報告兩項描述統計項目：偏態(skew)與峰度(kurtosis)。實務上任何一項出現在報告裡的機會，都不比以上討論過的集中量數與變異量數。偏態稍微重要，所以會在某些領域的文獻經常看到(例如反應時間的測量)，峰度則相當罕見。\n\n\n\n\n\n\nFigure 4.11: 圖解資料分佈的偏態類型。左圖是負偏態，中間是無偏態，右圖是正偏態。\n\n\n\n\n在此我們多認識一些偏態在解讀資料的意義。偏態是解釋資料不對稱程度的最常用指標。以 Figure 4.11 的展示來看，負偏態是多數資料的數值偏高所造成；而正偏態則是多數資料的數值偏低才會出現的現象。因此一筆資料大於平均值的數值佔多數，分析者會自然想到資料分佈是正偏態，偏態指數會是正值；相反的情況就會是負偏態，偏態指數會是負值。資料是對稱分佈的偏態指數則為0。\n[更多技術細節13]\n同學們也可以使用jamovi計算偏態，同樣在描述統計模組選單就有”skweness”可以勾選。資料變項afl.margins的偏態指數是0.780。使用偏態指數除以標準誤(Std. error)得到的數值，可以表示一筆資料的“偏態程度”。有個簡易判斷法則(特別是個數少於50的資料)：這個數值小於或等於2低表示偏態並不明顯，大於2就表示偏態程度會影響統計分析的結果。雖然這樣的判斷法則還不是學界共識，不過AFL的這筆資料偏態程度是 \\(\\frac{0.780}{0.183} = 4.262\\) 。\n峰度的報告雖然相當罕見，不過還是值得了解一下。峰度指標是描述資料分佈曲線的寬窄程度，如同 Figure 4.12 的圖解，常態分佈曲線的峰度指數是0，峰度指數是正是負顯示曲線是低闊峰(左圖)或高狹峰(右圖)。\n\n\n\n\n\n\nFigure 4.12: 圖解資料分佈的峰度。左圖的低闊峰分佈呈現資料往兩側分佈(峰度指數= -.95)；中間的常峰態分佈呈現資料分佈貼合常態分佈曲線(峰度指數= 0)；右圖的高狹峰分佈呈現資料往中央集中(峰度指數= 2.12)。峰度指數是根據各資料點偏離圖中黑色曲線的程度估算。\n\n\n\n\nFigure 4.12 的左圖是平坦的資料分佈，稱為”低闊峰”是因為資料往雙側分佈而造成，峰度指數是負值。右圖的資料分佈兩側很寬，稱為”高狹峰”是因為資料往雙側分佈而造成，峰度指數是正值。中間的資料分佈不窄也不寬，所以稱為”常峰態”，峰度指數是0。 表4-4 整理三種峰度的稱呼與及指數數值說明：\n\n\n\n\n\n表4-4: 峰度的稱呼與指數數值說明\n\n\n\n\n口語\n術語\n峰度指數\n\n\n\n\n分佈尾端太廋\n低闊峰(platykurtic)\n指數為負值\n\n\n分佈尾端均勻\n常態峰(mesokurtic)\n指數為零\n\n\n分佈尾端太肥\n高狹峰(leptokurtic)\n指數為正值\n\n\n\n[更多技術細節14]\n最後不免俗地提醒同學，jamovi的描述統計模組選單有計算峰度“kurtosis”的選項，只要開啟就會在報表看到AFL資料變項的峰度是0.101，除以標準誤的話是0.364，嚴格來說不算是”高狹峰”。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#分組描述統計",
    "href": "04-Descriptive-statistics.html#分組描述統計",
    "title": "4  描述統計",
    "section": "4.4 分組描述統計",
    "text": "4.4 分組描述統計\n往後許多實例要根據某個分組變項，對其他變項進行分組描述統計。使用jamovi分組描述統計只是小菜一碟。例如根據臨床治療方式，對收集到的臨床試驗資料進行分組描述統計。在這一節我們用另一批資料學習。請開啟學習資料庫裡的”Clinical Trial”資料集，之後在 Chapter 12 會詳細這一批資料的來歷。開啟後的試算表介面如同 Figure 4.13 :\n先看一下名義變項drug的數值：有安慰劑(placebo)，還有”anxifree”和“joyzepam”，三種藥各有六個受測者。另一個名義變項therapy代表治療方法，其中有九位受測者接受認知行為治療(CBT)，另外九位並未接受任何心理治療。比照之前的操作，在描述統計模組選單裡，單獨將連續尺度變項mood.gain放到Variables框裡，就會看到報表介面出現這個變項的平均值0.88。雖然我們還不知道這個變項的測量尺度是什麼意義，至少看起來能做些統計工作。\n接著可以試試看分組描述統計，請將therapy放到”Split by”，並且勾選Std. deviation, Skewness, Kurtosis等項目，看看會不會出現如同 Figure 4.14 的結果。\n\n\n\n\n\n\nFigure 4.13: 匯入 clinicaltrial.csv 的jamovi試算表介面擷圖。\n\n\n\n\n如果用不只一種分組變項會看到什麼結果呢？試試組合drug與theropy兩個變項的分組描述統計結果會是什麼。只要將drug放到”Split by”就知道了。分組描述統計的操作很容易，不過如果資料不多，分析結果很難說有什麼意義。有時候因為資料太少，報表會出現”NaN”或“Inf”等記號15。\n\n\n\n\n\n\nFigure 4.14: 使用jamovi以治療方式進行分組描述統計的擷圖。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#本章小結",
    "href": "04-Descriptive-statistics.html#本章小結",
    "title": "4  描述統計",
    "section": "4.6 本章小結",
    "text": "4.6 本章小結\n任何資料分析作業都是從最基本的描述統計開始，而且比起推論統計學習門檻較低。因此本書比照其他教科書由描述統計開始。這一章的各主題有以下重點：\n\n集中量數 集中量數顯示資料在幾何空間的所在位置。各種論文通常會報告平均值、中位數或眾數其中一種集中量數。\n變異量數 變異量數顯示資料在幾何空間的離散範圍。常用的變異量數有全距、四分位數間距、平均絕對差、變異數、標準差。\n偏態與峰度 分析作業也會關切變項資料分配的對稱程度(偏態)，以及寬窄程度(峰度)。\n分組描述統計 分組統計是jamovi的基本功能，值得學習如何設定分組用變項。\n標準分數 z分數放在這一章有些微妙，因為能用於描述統計，也能用於推論統計。請務必學好這一節的示範重點，之後的章節還會用到。\n\n下一章我們將學習如何繪製統計圖，似乎是輕鬆有趣的單元。結束前我想提醒一點，傳統統計課給描述統計的講授時間比例，大約只有推論統計的1/8或1/9，這是因為後者內容比較複雜且難學。不過這樣的安排很容易讓同學輕忽安排易讀的描述統計，也是重要的實務工作。請切記…"
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "前言",
    "section": "",
    "text": "This book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "04-Descriptive-statistics.html",
    "href": "04-Descriptive-statistics.html",
    "title": "4  描述統計",
    "section": "",
    "text": "Any time that you get a new data set to look at one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what descriptive statistics (as opposed to inferential statistics) is all about. In fact, to many people the term “statistics” is synonymous with descriptive statistics. It is this topic that we’ll consider in this chapter, but before going into any details, let’s take a moment to get a sense of why we need descriptive statistics. To do this, let’s open the aflsmall_margins file and see what variables are stored in the file, see Figure 4.1.\nIn fact, there is just one variable here, afl.margins. We’ll focus a bit on this variable in this chapter, so I’d better tell you what it is. Unlike most of the data sets in this book, this is actually real data, relating to the Australian Football League (AFL).1 The afl.margins variable contains the winning margin (number of points) for all 176 home and away games played during the 2010 season.\nThis output doesn’t make it easy to get a sense of what the data are actually saying. Just “looking at the data” isn’t a terribly effective way of understanding data. In order to get some idea about what the data are actually saying we need to calculate some descriptive statistics (this chapter) and draw some nice pictures (Chapter 5). Since the descriptive statistics are the easier of the two topics I’ll start with those, but nevertheless I’ll show you a histogram of the afl.margins data since it should help you get a sense of what the data we’re trying to describe actually look like, see Figure 4.2. We’ll talk a lot more about how to draw histograms in Section 5.1 in the next chapter. For now, it’s enough to look at the histogram and note that it provides a fairly interpretable representation of the afl.margins data."
  },
  {
    "objectID": "05-Drawing-graphs.html",
    "href": "05-Drawing-graphs.html",
    "title": "5  繪製統計圖",
    "section": "",
    "text": "Above all else show the data.\n– Edward Tufte1\nVisualising data is one of the most important tasks facing the data analyst. It’s important for two distinct but closely related reasons. Firstly, there’s the matter of drawing “presentation graphics”, displaying your data in a clean, visually appealing fashion makes it easier for your reader to understand what you’re trying to tell them. Equally important, perhaps even more important, is the fact that drawing graphs helps you to understand the data. To that end, it’s important to draw “exploratory graphics” that help you learn about the data as you go about analysing it. These points might seem pretty obvious but I cannot count the number of times I’ve seen people forget them.\nTo give a sense of the importance of this chapter, I want to start with a classic illustration of just how powerful a good graph can be. To that end, Figure 5.1 shows a redrawing of one of the most famous data visualisations of all time. This is John Snow’s 1854 map of cholera deaths. The map is elegant in its simplicity. In the background we have a street map which helps orient the viewer. Over the top we see a large number of small dots, each one representing the location of a cholera case. The larger symbols show the location of water pumps, labelled by name. Even the most casual inspection of the graph makes it very clear that the source of the outbreak is almost certainly the Broad Street pump. Upon viewing this graph Dr Snow arranged to have the handle removed from the pump and ended the outbreak that had killed over 500 people. Such is the power of a good data visualisation.\nThe goals in this chapter are twofold. First, to discuss several fairly standard graphs that we use a lot when analysing and presenting data, and second to show you how to create these graphs in jamovi. The graphs themselves tend to be pretty straightforward, so in one respect this chapter is pretty simple. Where people usually struggle is learning how to produce graphs, and especially learning how to produce good graphs. Fortunately, learning how to draw graphs in jamovi is reasonably simple as long as you’re not too picky about what your graph looks like. What I mean when I say this is that jamovi has a lot of very good default graphs, or plots, that most of the time produce a clean, high-quality graphic. However, on those occasions when you do want to do something non-standard, or if you need to make highly specific changes to the figure, then the graphics functionality in jamovi is not yet capable of supporting advanced work or detail editing."
  },
  {
    "objectID": "06-Pragmatic-matters.html",
    "href": "06-Pragmatic-matters.html",
    "title": "6  實務課題",
    "section": "",
    "text": "The garden of life never seems to confine itself to the plots philosophers have laid out for its convenience. Maybe a few more tractors would do the trick.\n– Roger Zelazny1\nThis is a somewhat strange chapter, even by my standards. My goal in this chapter is to talk a bit more honestly about the realities of working with data than you’ll see anywhere else in the book. The problem with real world data sets is that they are messy. Very often the data file that you start out with doesn’t have the variables stored in the right format for the analysis you want to do. Sometimes there might be a lot of missing values in your data set. Sometimes you only want to analyse a subset of the data. Et cetera. In other words, there’s a lot of data manipulation that you need to do just to get the variables in your data set into the format that you need it. The purpose of this chapter is to provide a basic introduction to these pragmatic topics. Although the chapter is motivated by the kinds of practical issues that arise when manipulating real data, I’ll stick with the practice that I’ve adopted through most of the book and rely on very small, toy data sets that illustrate the underlying issue. Because this chapter is essentially a collection of techniques and doesn’t tell a single coherent story, it may be useful to start with a list of topics:\nAs you can see, the list of topics that the chapter covers is pretty broad, and there’s a lot of content there. Even though this is one of the longest and hardest chapters in the book, I’m really only scratching the surface of several fairly different and important topics. My advice, as usual, is to read through the chapter once and try to follow as much of it as you can. Don’t worry too much if you can’t grasp it all at once, especially the later sections. The rest of the book is only lightly reliant on this chapter so you can get away with just understanding the basics. However, what you’ll probably find is that later on you’ll need to flick back to this chapter in order to understand some of the concepts that I refer to here."
  },
  {
    "objectID": "06-Pragmatic-matters.html#sec-Tabulating-and-cross-tabulating-data",
    "href": "06-Pragmatic-matters.html#sec-Tabulating-and-cross-tabulating-data",
    "title": "6  實務課題",
    "section": "6.1 製作次數表及列聯表",
    "text": "6.1 製作次數表及列聯表\n幾乎每項統計分析實務都要建立次數表(frequency table)或資料變項列聯表(cross tabulation)。這一節示範如何運用jamovi完成。\n\n\n\n6.1.1 製表示範\n此處以原作者某天晚上照顧剛出生的小孩的時候，偶然看了一集深夜節目，隨手紀錄節目中出現的角色及台詞，整理成”說話者(speaker)“與”台詞(utterance)“兩個變項。這份記錄存於jamovi示範檔案Night Garden，開啟後兩個變項的內容如下：\n\n資料變項’說話者’: upsy-daisy upsy-daisy upsy-daisy upsy-daisy tombliboo tombliboo makka-pakka makka-pakka makka-pakka makka-pakka 資料變項’台詞’: pip pip onk onk ee oo pip pip onk onk\n那一晚原作者的腦袋發生了什麽事？讓我們將每個角色的說話次數做成表格吧，只要用將資料變項’speaker’放到jamovi述統計模組選單的Variables視窗，再勾選啟動’Frequency tables’，報表介面就會出現像是 Table 6.1 的次數表。\n\n\n\n\n\n\nTable 6.1:  資料變項’speaker’的次數表 \n \n  \n    角色名字 \n    發言次數 \n    發言百分比 \n    累積百分比 \n  \n \n\n  \n    makka-pakka \n    4 \n    40 \n    40 \n  \n  \n    tombliboo \n    2 \n    20 \n    60 \n  \n  \n    upsy-daisy \n    4 \n    40 \n    100 \n  \n\n\n\n\n\n\n次數表表格第一行是speaker資料變項的各種基本統計資料的欄位名稱。像是”Levels”這欄列出紀錄在變項裡的所有角色名字，“Counts”這欄之中的數字是每位角色說了幾次台詞。\njamovi的”Frequency tables”功能只能製作一個變項的次數表。若是要製作能展示兩個變項的列聯表，像是計算每個角色’speaker’講了各種台詞’utterance’幾次，就要使用Frequencies模組之中的Contigency Tables-Independent Samples功能。功能選單如同 Figure 6.1 ，請按照畫面範，製造兩個變項的列聯表。\n\n\n\n\n\n\n\nFigure 6.1: 資料變項speaker 與 utterances的列聯表示範畫面。\n\n\n\n\n各位先不必管報表最下面的”\\(\\chi^2\\) 檢定”，這是 Chapter 10 的學習項目。解讀列聯表的關鍵是表格中每個數字是計數，像是第一列第二欄的”2”是指Makka-Pakka這個角色說了”onk”這個台詞2次。\n\n\n\n\n6.1.2 列聯表裡的百分比\nFigure 6.1 的列聯表呈現的是原始計數，也就是兩個變項內各層次資料組合的總次數。不過統計實務通常也要呈現次數百分比。在這個例子，只要從’Contingency Tables’功能選單下的’Cells’次選單勾選啟動Percentages之下的方塊。 Figure 6.2 示範啟動Row，自動計算每個原始計數在各列之內的百分比。\n\n\n\n\n\n\n\nFigure 6.2: 列聯表呈現資料變項speaker 與 utterances的次數及列內百分比。\n\n\n\n\n列內百分比讓我們知道每位角色說的台詞次數百分比，所以Makka-Pakka講的台詞有50%是”pip”，另外的50%是”onk”。若是改成計算每欄之內的百分比(取消Row並改成Column)，列聯表就會變成如 Figure 6.3 的樣子。這樣的列聯表告訴我們那個台詞被那些角色講了幾次。像是”ee” 100% 是Tombliboo講的。\n\n\n\n\n\n\n\nFigure 6.3: 列聯表呈現資料變項speaker 與 utterances的次數及欄內百分比。"
  },
  {
    "objectID": "06-Pragmatic-matters.html#logical-expressions-in-jamovi",
    "href": "06-Pragmatic-matters.html#logical-expressions-in-jamovi",
    "title": "6  實務課題",
    "section": "6.2 Logical expressions in jamovi",
    "text": "6.2 Logical expressions in jamovi\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 Assessing mathematical truths\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 Logical operations\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 Applying logical operation to text\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable",
    "href": "06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable",
    "title": "6  實務課題",
    "section": "6.3 資料變項的轉換與編碼",
    "text": "6.3 資料變項的轉換與編碼\nIt’s not uncommon in real world data analysis to find that one of your variables isn’t quite equivalent to the variable that you really want. For instance, it’s often convenient to take a continuous-valued variable (e.g., age) and break it up into a smallish number of categories (e.g., younger, middle, older). At other times, you may need to convert a numeric variable into a different numeric variable (e.g., you may want to analyse at the absolute value of the original variable). In this section I’ll describe a few key ways you can do these things in jamovi.\n\n6.3.1 轉換資料數值\nThe first trick to discuss is the idea of transforming a variable. Taken literally, anything you do to a variable is a transformation, but in practice what it usually means is that you apply a relatively simple mathematical function to the original variable in order to create a new variable that either (a) provides a better way of describing the thing you’re actually interested in, or (b) is more closely in agreement with the assumptions of the statistical tests you want to do. Since, at this stage, I haven’t talked about statistical tests or their assumptions, I’ll show you an example based on the first case.\nSuppose I’ve run a short study in which I ask 10 people a single question: On a scale of 1 (strongly disagree) to 7 (strongly agree), to what extent do you agree with the proposition that “Dinosaurs are awesome”?\nNow let’s load and look at the data. The data file likert.omv contains a single variable that contains raw Likert-scale responses for these 10 people. However, if you think about it, this isn’t the best way to represent these responses. Because of the fairly symmetric way that we set up the response scale, there’s a sense in which the midpoint of the scale should have been coded as 0 (no opinion), and the two endpoints should be `3 (strongly agree) and ´3 (strongly disagree). By recoding the data in this way it’s a bit more reflective of how we really think about the responses. The recoding here is pretty straightforward, we just subtract 4 from the raw scores. In jamovi you can do this by computing a new variable: click on the ‘Data’ - ‘Compute’ button and you will see that a new variable has been added to the spreadsheet. Let’s call this new variable likert.centred (go ahead and type that in) and then add the following in the formula box, like in Figure 6.4: ‘likert.raw - 4’\n\n\n\n\n\nFigure 6.4: Creating a new computed variable in jamovi\n\n\n\n\nOne reason why it might be useful to have the data in this format is that there are a lot of situations where you might prefer to analyse the strength of the opinion separately from the direction of the opinion. We can do two different transformations on this likert.centred variable in order to distinguish between these two different concepts. First, to compute an opinion.strength variable, we want to take the absolute value of the centred data (using the ‘ABS’ function).6 In jamovi, create another new variable using the ‘Compute’ button. Name the variable opinion.strength and this time click on the fx button next to the ‘Formula’ box. This shows the different ‘Functions’ and ‘Variables’ that you can add to the ‘Formula’ box, so double click on ‘ABS’ and then double click on “likert.centred’ and you will see that the ‘Formula’ box is populated with ABS(likert.centred) and a new variable has been created in the spreadsheet view, as in Figure 6.5.\n\n\n\n\n\nFigure 6.5: Using the \\(f_x\\) button to select functions and variables\n\n\n\n\nSecond, to compute a variable that contains only the direction of the opinion and ignores the strength, we want to calculate the ‘sign’ of the variable. In jamovi we can use the IF function to do this. Create another new variable using the ‘Compute’ button, name this one opinion.sign, and then type the following into the function box:\nIF(likert.centred \\(==\\) 0, 0, likert.centred / opinion.strength) When done, you’ll see that all negative numbers from the likert.centred variable are converted to -1, all positive numbers are converted to 1 and zero stays as 0, like so:\n-1 1 -1 0 0 0 -1 1 1 1\nLet’s break down what this ‘IF’ command is doing. In jamovi there are three parts to an ‘IF’ statement, written as ‘IF(expression, value, else)’. The first part, ‘expression’ can be a logical or mathematical statement. In our example, we have specified ‘likert.centred \\(==\\) 0’, which is TRUE for values where likert.centred is zero. The next part, ‘value’, is the new value where the expression in part one is TRUE. In our example, we have said that for all those values where likert.centred is zero, keep them zero. In the next part, ‘else’, we can enter another logical or mathematical statement to be used if part one evaluates to FALSE, i.e. where likert.centred is not zero. In our example we have divided likert.centred by opinion.strength to give ‘-1’ or ‘+1’ depending of the sign of the original value in likert.centred.7\nAnd we’re done. We now have three shiny new variables, all of which are useful transformations of the original likert.raw data.\n\n\n6.3.2 轉換連續變項為間斷變項\nOne pragmatic task that comes up quite often is the problem of collapsing a variable into a smaller number of discrete levels or categories. For instance, suppose I’m interested in looking at the age distribution of people at a social gathering:\n60,58,24,26,34,42,31,30,33,2,9\nIn some situations it can be quite helpful to group these into a smallish number of categories. For example, we could group the data into three broad categories: young (0-20), adult (21-40) and older (41-60). This is a quite coarse-grained classification, and the labels that I’ve attached only make sense in the context of this data set (e.g., viewed more generally, a 42 year old wouldn’t consider themselves as “older”). We can slice this variable up quite easily using the jamovi ‘IF’ function that we have already used. This time we have to specify nested ‘IF’ statements, meaning simply that IF the first logical expression is TRUE, insert a first value, but IF a second logical expression is TRUE, insert a second value, but IF a third logical expression is TRUE, then insert a third value. This can be written as:\nIF(Age >= 0 and Age <= 20, 1, IF(Age >= 21 and Age <= 40, 2, IF(Age >= 41 and Age <= 60, 3 )))\nNote that there are three left parentheses used during the nesting, so the whole statement has to end with three right parentheses otherwise you will get an error message. The jamovi screen shot for this data manipulation, along with an accompanying frequency table, is shown in Figure 6.6.\n\n\n\n\n\nFigure 6.6: Collapsing a variable into a smaller number of discrete levels using the jamovi ‘IF’ function\n\n\n\n\nIt’s important to take the time to figure out whether or not the resulting categories make any sense at all in terms of your research project. If they don’t make any sense to you as meaningful categories, then any data analysis that uses those categories is likely to be just as meaningless. More generally, in practice I’ve noticed that people have a very strong desire to carve their (continuous and messy) data into a few (discrete and simple) categories, and then run analyses using the categorised data instead of the original data.8 I wouldn’t go so far as to say that this is an inherently bad idea, but it does have some fairly serious drawbacks at times, so I would advise some caution if you are thinking about doing it.\n\n\n6.3.3 設計用途多重的轉換功能\nSometimes you want to apply the same transformation to more than one variable, for example when you have multiple questionnaire items that all need to be recalculated or recoded in the same way. And one of the neat features in jamovi is that you can create a transformation, using the ‘Data’ - ‘Transform’ button, that can then be saved and applied to multiple variables. Let’s go back to the first example above, using the data file likert.omv that contains a single variable with raw Likert-scale responses for 10 people. To create a transformation that you can save and then apply across multiple variables (assuming you had more variables like this in your data file), first in the spreadsheet editor select (i.e., click) the variable you want to use to initially create the transformation. In our example this is likert.raw. Next click the ‘Transform’ button in the jamovi ‘Data’ ribbon, and you’ll see something like Figure 6.7.\nGive your new variable a name, let’s call it opinion.strength and then click on the ‘using transform’ selection box and select ‘Create New Transform…’. This is where you will create, and name, the transformation that can be re-applied to as many variables as you like. The transformation is automatically named for us as ‘Transform 1’ (imaginative, huh. You can change this if you like). Then type the expression “ABS($source - 4)” into the function text box, as in Figure 6.8, press Enter or Return on your keyboard and, hey presto, you have created a new transformation and applied it to the likert.raw variable! Good, eh. Note that instead of using the variable label in the expression, we have instead used ‘$source’. This is so that we can then use the same transformation with as many different variables as we like - jamovi requires you to use ‘$source’ to refer to the source variable you are transforming. Your transformation has also been saved and can be re-used any time you like (providing you save the dataset as an ‘.omv’ file, otherwise you’ll lose it!).\nYou can also create a transformation with the second example we looked at, the age distribution of people at a social gathering. Go on, you know you want to! Remember that we collapsed this variable into three groups: younger, adult and older. This time we will achieve the same thing, but using the jamovi ‘Transform’ - ‘Add condition’ button. With this data set (go back to it or create it again if you didn’t save it) set up a new variable transformation. Call the transformed variable AgeCats and the transformation you will create Agegroupings. Then click on the big “\\(+\\)” sign next to the function box. This is the ‘Add condition’ button and I’ve stuck a big red arrow onto Figure 6.9 so you can see exactly where this is. Re-create the transformation shown in Figure 6.9 and when you have done, you will see the new values appear in the spreadsheet window. What’s more, the Age groupings transformation has been saved and can be re-applied any time you like. Ok, so I know that it’s unlikely you will have more than one ‘Age’ variable, but you get the idea now of how to set up transformations in jamovi, so you can follow this idea with other sorts of variables. A typical scenario for this is when you have a questionnaire scale with, say, 20 items (variables) and each item was originally scored from 1 to 6 but, for some reason or quirk of the data you decide to recode all the items as 1 to 3. You can easily do this in jamovi by creating and then re-applying your transformation for each variable that you want to recode.\n\n\n\n\n\nFigure 6.7: Creating a new variable transformation using the jamovi ‘Transform’ command\n\n\n\n\n\n\n\n\n\nFigure 6.8: Specifying a transformation in jamovi, to be saved as the imaginatively named ‘Transform 1’\n\n\n\n\n\n\n\n\n\nFigure 6.9: jamovi transformation into three age categories, using the ‘Add condition’ button\n\n\n\n\n\n\n\n\nTable 6.5:  Some mathematical operators \n\nfunctionexample input(answer)\n\nsquare rootSQRT(x)SQRT(25)5\n\nabsolute valueABS(x)ABS(-23)23\n\nlogarithm (base 10)LOG10(x)LOG10(1000)3\n\nlogarithm (base e)LN(x)LN(1000)6.91\n\nexponentiationEXP(x)EXP(6.908)1e+03\n\nbox-coxBOXCOX(x, lamda)BOXCOX(6.908, 3)110"
  },
  {
    "objectID": "06-Pragmatic-matters.html#a-few-more-mathematical-functions-and-operations",
    "href": "06-Pragmatic-matters.html#a-few-more-mathematical-functions-and-operations",
    "title": "6  實務課題",
    "section": "6.4 A few more mathematical functions and operations",
    "text": "6.4 A few more mathematical functions and operations\nIn the section on Transforming and recoding a variable I discussed the ideas behind variable transformations and showed that a lot of the transformations that you might want to apply to your data are based on fairly simple mathematical functions and operations. In this section I want to return to that discussion and mention several other mathematical functions and arithmetic operations that are actually quite useful for a lot of real world data analysis. Table 6.5 gives a brief overview of the various mathematical functions I want to talk about here, or later.9 Obviously this doesn’t even come close to cataloguing the range of possibilities available, but it does cover a range of functions that are used regularly in data analysis and that are available in jamovi.\n\n6.4.1 Logarithms and exponentials\nAs I’ve mentioned earlier, jamovi has an useful range of mathematical functions built into it and there really wouldn’t be much point in trying to describe or even list all of them. For the most part, I’ve focused only on those functions that are strictly necessary for this book. However I do want to make an exception for logarithms and exponentials. Although they aren’t needed anywhere else in this book, they are everywhere in statistics more broadly. And not only that, there are a lot of situations in which it is convenient to analyse the logarithm of a variable (i.e., to take a “log-transform” of the variable). I suspect that many (maybe most) readers of this book will have encountered logarithms and exponentials before, but from past experience I know that there’s a substantial proportion of students who take a social science statistics class who haven’t touched logarithms since high school, and would appreciate a bit of a refresher.\nIn order to understand logarithms and exponentials, the easiest thing to do is to actually calculate them and see how they relate to other simple calculations. There are three jamovi functions in particular that I want to talk about, namely LN(), LOG10() and EXP(). To start with, let’s consider LOG10(), which is known as the “logarithm in base 10”. The trick to understanding a logarithm is to understand that it’s basically the “opposite” of taking a power. Specifically, the logarithm in base 10 is closely related to the powers of 10. So let’s start by noting that 10-cubed is 1000. Mathematically, we would write this:\n\\[10^3=1000\\]\nThe trick to understanding a logarithm is to recognise that the statement that “10 to the power of 3 is equal to 1000” is equivalent to the statement that “the logarithm (in base 10) of 1000 is equal to 3”. Mathematically, we write this as follows,\n\\[log_{10}(1000)=3\\]\nOkay, since the LOG10() function is related to the powers of 10, you might expect that there are other logarithms (in bases other than 10) that are related to other powers too. And of course that’s true: there’s not really anything mathematically special about the number 10. You and I happen to find it useful because decimal numbers are built around the number 10, but the big bad world of mathematics scoffs at our decimal numbers. Sadly, the universe doesn’t actually care how we write down numbers. Anyway, the consequence of this cosmic indifference is that there’s nothing particularly special about calculating logarithms in base 10. You could, for instance, calculate your logarithms in base 2. Alternatively, a third type of logarithm, and one we see a lot more of in statistics than either base 10 or base 2, is called the natural logarithm, and corresponds to the logarithm in base e. Since you might one day run into it, I’d better explain what e is. The number e, known as Euler’s number, is one of those annoying “irrational” numbers whose decimal expansion is infinitely long, and is considered one of the most important numbers in mathematics. The first few digits of e are:\n\\[e = 2.718282 \\]\nThere are quite a few situation in statistics that require us to calculate powers of \\(e\\), though none of them appear in this book. Raising e to the power \\(x\\) is called the exponential of \\(x\\), and so it’s very common to see \\(e^x\\) written as exppxq. And so it’s no surprise that jamovi has a function that calculates exponentials, called EXP(). Because the number e crops up so often in statistics, the natural logarithm (i.e., logarithm in base e) also tends to turn up. Mathematicians often write it as \\(log_e(x)\\) or \\(ln(x)\\). In fact, jamovi works the same way: the LN() function corresponds to the natural logarithm.\nAnd with that, I think we’ve had quite enough exponentials and logarithms for this book!"
  },
  {
    "objectID": "06-Pragmatic-matters.html#extracting-a-subset-of-the-data",
    "href": "06-Pragmatic-matters.html#extracting-a-subset-of-the-data",
    "title": "6  實務課題",
    "section": "6.5 Extracting a subset of the data",
    "text": "6.5 Extracting a subset of the data\nOne very important kind of data handling is being able to extract a particular subset of the data. For instance, you might be interested only in analysing the data from one experimental condition, or you may want to look closely at the data from people over 50 years in age. To do this, the first step is getting jamovi to filter the subset of the data corresponding to the observations that you’re interested in.\nThis section returns to the nightgarden.csv data set. If you’re reading this whole chapter in one sitting, then you should already have this data set loaded into a jamovi window. For this section, let’s focus on the two variables speaker and utterance (see [Tabulating and cross-tabulating data]) if you’ve forgotten what those variables look like). Suppose that what I want to do is pull out only those utterances that were made by Makka-Pakka. To that end, we need to specify a filter in jamovi. First open up a filter window by clicking on ‘Filters’ on the main jamovi ‘Data’ toolbar. Then, in the ‘Filter 1’ text box, next to the ‘=’ sign, type the following:\nspeaker == ‘makka-pakka’\n\n\n\n\n\nFigure 6.10: Creating a subset of the nightgarden data using the jamovi ‘Filters’ option\n\n\n\n\nWhen you have done this, you will see that a new column has been added to the spreadsheet window (see Figure 6.10), labelled ‘Filter 1’, with the cases where speaker is not ‘makka-pakka’ greyed-out (i.e., filtered out) and, conversely, the cases where speaker is ‘makka-pakka’ have a green check mark indicating they are filtered in. You can test this by running ‘Exploration’ - ‘Descriptives’ - ‘Frequency tables’ for the speaker variable and seeing what that shows. Go on, try it!\nFollowing on from this simple example, you can also build up more complex filters using logical expressions in jamovi. For instance, suppose I wanted to keep only those cases when the utterance is either “pip” or “oo”. In this case in the ‘Filter 1’ text box, next to the ‘=’ sign, you would type the following:\nutterance == ‘pip’ or utterance == ‘oo’"
  },
  {
    "objectID": "06-Pragmatic-matters.html#summary",
    "href": "06-Pragmatic-matters.html#summary",
    "title": "6  Pragmatic matters",
    "section": "6.6 Summary",
    "text": "6.6 Summary\nObviously, there’s no real coherence to this chapter. It’s just a grab bag of topics and tricks that can be handy to know about, so the best wrap up I can give here is just to repeat this list:\n\nTabulating and cross-tabulating data\nLogical expressions in jamovi\nTransforming and recoding a variable\nA few more mathematical functions and operations\nExtracting a subset of the data"
  },
  {
    "objectID": "05-Drawing-graphs.html#箱型圖",
    "href": "05-Drawing-graphs.html#箱型圖",
    "title": "5  繪製統計圖",
    "section": "5.2 箱型圖",
    "text": "5.2 箱型圖\n箱型圖(boxplot)是與直方圖功能相同，適用於等距尺度或比例尺度變項的另一種資料視覺化方法，又被稱為“盒鬚圖”。箱形圖的基本概念是將變項資料的中位數、四分位數間距、以及全距用視覺標記呈現。如此簡潔的構成讓箱形圖變成最常使用的統計圖，特別是用在初步探索資料趨勢的時候。以下同樣使用afl.margins資料集做為示範。\n\n\n\n\n\n\nFigure 5.4: 使用jamovi繪製afl.margins變項的箱形圖。\n\n\n\n\n了解如何用箱形圖解釋資料的最簡單方式，就是親手繪製一份。與繪製直方圖一樣的步驟，只是改成勾選”Box plot”，就會在jamovi報表介面得到如 Figure 5.4 的成品。看著圖中的特徵，你能辨識出重要訊息：箱子中大的粗線是中位數；箱子的上下邊界距離是25%到75%的四分位數間距；箱子之外的”觸鬚”長度，只要不會超過“限制邊界”，就能延伸到資料的最小值及最大值。預設的限制邊界是四分位數間距的1.5倍，也就是說觸鬚向下延伸只能到25%的四分位數 減去1.5倍的四分位數間距，以及向上延伸最多到75%的四分位數加上1.5倍的四分位數間距。任何落在觸鬚或限制邊界之外的數值，一般稱為極端值(outlier)。afl.margins這筆變項有兩個極端值，因為上邊界值是107，試算表裡可以找出是第108行及第116行的數值。\n\n\n5.2.1 小提琴圖\n\n\n\n\n\nFigure 5.5: 使用jamovi繪製afl.margins變項的小提琴圖，同時繪製資料點與箱形圖。\n\n\n\n\n小提琴圖(violin plot)是傳統箱形圖的變形。小提琴圖類似箱形圖，曲線代表每個數值在整筆資料的機率密度。通常小提琴圖都要同時呈現箱形圖的主要視覺標記，包括標記中位數的粗線，以及代表四分位數間距的箱子。用jamovi完成 Figure 5.5 作品的繪製方法是只要將”Box plot”選項之下的”Violin”與”Data”都一起勾選。因為統計圖儘可能簡潔易懂，小提琴圖有太多視覺元素，原作者喜歡使用箱形圖比小提琴圖多一些。\n\n\n\n5.2.2 使用多重箱形圖\n最後，如果我們需要繪製不只一個箱形圖要怎麼做呢？像是我們要將每一年AFL勝隊得分的資料都會成一份箱形圖。首先把能達到這個目的的資料匯入jamovi，請開啟示範資料庫的”AFL Margins By Year”這份檔案。這份檔案一共有4296場比賽紀錄，還有一個變項儲存年份。要用jamovi繪製每一年勝隊得分margins的箱形圖，請依照 Figure 5.6 的示範，把變項year放到”Split by”視窗裡。\n\n\n\n\n\n\nFigure 5.6: 使用”Split by”視窗的示範畫面。\n\n\n\n\n繪製成品如 Figure 5.7 。由於每一年都有一份箱形圖，比直方圖更能讓我們看到逐年趨勢，而且不會被年份的連續性干擾解讀2。如果這裡改成繪製24份直方圖，各位可以自行試試看容不容易解讀。\n\n\n\n\n\n\nFigure 5.7: 使用jamovi繪製多重箱形圖，每份箱形圖代表各年份的勝隊得分分佈。\n\n\n\n\n\n\n5.2.3 使用箱形圖辨認極端值\njamovi繪製箱形圖會自動標記超出限制邊界的資料點，實務上通常用繪製箱形圖偵測資料裡的極端值：泛指離多數資料遠得”可疑”的數值。我們用 Figure 5.8 的AFL勝隊得分箱形圖裡的兩個資料點來說明為何應該懷疑極端值的存在：因為這兩場勝隊得分超過300分！實在太不尋常了3。接著我們試著使用jamovi的功能，仔細檢視這兩場紀錄。只要勾選”Box Plot”選項時，一併啟動之下的”Label outliers”，jamovi就會在箱形圖標示極端值資料的在第幾列，讓我們能回到試算表介面看個仔細。另一種方式是使用jmaovi的過濾器(Filter)功能：按下主介面左下角的漏斗圖示，就會開啟如 Figure 5.9 Filter選單，接著比照該圖示範，在視窗裡輸入或複製貼上 ‘margin > 300’。\n\n\n\n\n\n\nFigure 5.8: 這份箱形圖顯示兩筆非常可疑的極端值！\n\n\n\n\n過濾器啟動後，試算面介面最左邊會多一個欄位，欄位下的細格會標記有那些資料通過稍早設定的條件。這種方法能快速辨識極端值資料在那裡。更進一步還可以開啟jamovi描述統計選單的”Frequency table”，如同 Figure 5.10 的示範。報表顯示超過300分的比賽紀錄是第14與第134場。這些資料探索能讓我們決定，是不是要回去看看資料檔案裡，這些不尋常的極端值到底是怎麼回事。\n\n\n\n\n\n\n\nFigure 5.9: jamovi Filter視窗操作示範畫面\n\n\n\n\n\n\n\n\n\nFigure 5.10: 以次數表展示兩筆極端值ID ~ 176與202。\n\n\n\n\n通常造常極端值的問題是登打紀錄的人手誤。雖然這是個不細心而犯的錯，但是這種事情在統計實務層出不窮。真實世界的各種資料充滿這類錯誤，尤其是透過人工輸入電腦的資料。其實實務上有個專有名詞叫「資料清理」：泛指在正式進行資料分析前，找出錯誤或可疑數值的一切工作。找出並清理原始資料裡一切輸入錯誤、遺漏值、或者各種希奇古怪問題的工作都是資料清理的項目。\n至於比較沒那麼極端，但是在箱形圖裡被標為極端值的資料，要不要納入分析或排除這些數值，全看你要如何看待這筆資料，以及利用這筆資料的想法。上統計課就是自我訓練判斷如何運用這類資料的能力。若是你認為這些極端值應該納入分析，就保留它們。到了 Section 10.10 我們會繼續學習更多判斷極端值要不要保留的策略。"
  },
  {
    "objectID": "05-Drawing-graphs.html#匯出統計圖",
    "href": "05-Drawing-graphs.html#匯出統計圖",
    "title": "5  繪製統計圖",
    "section": "5.4 匯出統計圖",
    "text": "5.4 匯出統計圖\n快要下課了，不過認真的同學應該會想問：如果我很滿意用jamovi畫出來的統計圖，想要存起來分享給朋友看的話，難道他們也要有jamovi才看得到嗎？可以只將統計圖單獨存檔嗎？很簡單，在你想存檔的統計圖上按一下滑鼠右鍵，就會出現匯出存檔選單，你可以選擇要存檔的格式，目前有’png’，‘eps’，‘svg’，’pdf’等可以選擇。只要存成你覺得合適的圖檔格式，就可以用電子郵件或社交軟體分享給朋友，或者將圖檔放到你的作業報告裡。"
  },
  {
    "objectID": "05-Drawing-graphs.html#本章小結",
    "href": "05-Drawing-graphs.html#本章小結",
    "title": "5  繪製統計圖",
    "section": "5.5 本章小結",
    "text": "5.5 本章小結\n本書原作者自敘個人寫作學術報告的習慣，都是從思考要放什麼圖開始。因為每個人都對搭配有敘事順序的圖畫故事有興趣，想清楚要放什麼圖，報告的其他部分都是點綴。因為人類天生會用眼睛探索世界的傾向，統計圖也是一種分析資料的工具。佈局精密的統計圖能幫助讀者從海量資訊裡立刻看到關鍵，也就是許多人都聽過的「一圖抵萬言」。希望讀過並操作過本章範例的同學，能將這個想法記在心裡。本章涵蓋的主題有：\n\n常用統計圖. 本章重點介紹與示範最基本的統計圖，包括直方圖, 箱型圖 以及 柱狀圖\n匯出統計圖 完成統計圖後，別忘了匯出到你的正式報告裡。\n\n最後提醒一點。jamovi(還有大多數套裝軟體)輸出的統計圖都是最基本的樣式，不一定符合報告呈現的需要。如果有心製作報告需要的統計圖，建議學習使用R語言及製作統計圖的套件。最多R語言使用者喜歡用ggplot2，有許多參考資源可以拿來自我學習及參照(例如 Wilkinson et al., 2006)。不過如果你還是統計初學者，你需要先花些時間掌握R語法。本書不會談到R語言，當你認為有必要使用時，可以運用本書的範例，做為學習R的入門資源(請記得jamovi有R程式碼模式)。\n\n\n\n\n\n\nWilkinson, L., Wills, D., Rope, D., Norton, A., & Dubbs, R. (2006). The grammar of graphics. Springer."
  },
  {
    "objectID": "07-Introduction-to-probability.html",
    "href": "07-Introduction-to-probability.html",
    "title": "7  Introduction to probability",
    "section": "",
    "text": "[God] has afforded us only the twilight … of Probability.\n– John Locke\nUp to this point in the book we’ve discussed some of the key ideas in experimental design, and we’ve talked a little about how you can summarise a data set. To a lot of people this is all there is to statistics: collecting all the numbers, calculating averages, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics and one of the least powerful. The bigger and more useful part of statistics is that it provides information that lets you make inferences about data.\nOnce you start thinking about statistics in these terms, that statistics is there to help us draw inferences from data, you start seeing examples of it everywhere. For instance, here’s a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):\nThis kind of remark is entirely unremarkable in the papers or in everyday life, but let’s have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I’m too lazy to track down the original survey so let’s just imagine that they called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those claimed that they intended to vote for the Australian Labor Party (ALP). For the 2010 Federal election the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?\nThe answer to the question is pretty obvious. If I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the only 230 people out of the entire voting public who actually intend to vote ALP. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it’s a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.\nInferential statistics provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of probability theory. And it is to probability theory that we must now turn. This discussion of probability theory is basically background detail. There’s not a lot of statistics per se in this chapter, and you don’t need to understand this material in as much depth as the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it’s worth covering some of the basics."
  },
  {
    "objectID": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "href": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "title": "7  Introduction to probability",
    "section": "7.1 How are probability and statistics different?",
    "text": "7.1 How are probability and statistics different?\nBefore we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:\n\nWhat are the chances of a fair coin coming up heads 10 times in a row?\nIf I roll a six sided dice twice, how likely is it that I’ll roll two sixes?\nHow likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?\nWhat are the chances that I’ll win the lottery?\n\nNotice that all of these questions have something in common. In each case the “truth of the world” is known and my question relates to the “what kind of events” will happen. In the first question I know that the coin is fair so there’s a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:\n\\[P(head)=0.5\\]\nwhich you can read as “the probability of heads is 0.5”. As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing. In probability theory the model is known but the data are not.\nSo that’s probability. What about statistics? Statistical questions work the other way around. In statistics we do not know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:\n\nIf my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?\nIf five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?\nIf the lottery commissioner’s spouse wins the lottery how likely is it that the lottery was rigged?\n\nThis time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:\nH H H H H H H H H H H\nand what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as \\(P(heads)\\ne{0.5}\\). In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works."
  },
  {
    "objectID": "07-Introduction-to-probability.html#what-does-probability-mean",
    "href": "07-Introduction-to-probability.html#what-does-probability-mean",
    "title": "7  機率入門",
    "section": "7.2 What does probability mean?",
    "text": "7.2 What does probability mean?\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n7.2.1 The frequentist view\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the frequentist view and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 7.1:  Coin flips and proportion of heads \n\nnumber of flips12345678910\n\nnumber of heads0123444567\n\nproportion00.50.670.750.80.670.570.630.670.7\n\nnumber of flips11121314151617181920\n\nnumber of heads88910101010101011\n\nproportion0.730.670.690.710.670.630.590.560.530.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 7.1. As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.1 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 8.5).\n\n\n\n\n\nFigure 7.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n7.2.2 The Bayesian view\nThe Bayesian view of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n7.2.3 What’s the difference? And who is right?\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in Chapter 16."
  },
  {
    "objectID": "07-Introduction-to-probability.html#basic-probability-theory",
    "href": "07-Introduction-to-probability.html#basic-probability-theory",
    "title": "7  機率入門",
    "section": "7.3 Basic probability theory",
    "text": "7.3 Basic probability theory\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n7.3.1 Introducing probability distributions\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 7.2 shows an example of a probability distribution.\n\n\n\n\nTable 7.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see Section 5.3) to visualise this distribution, as shown in Figure 7.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 7.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 7.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\nTable 7.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "title": "7  機率入門",
    "section": "7.4 二項分佈",
    "text": "7.4 二項分佈\nAs you might imagine, probability distributions vary enormously and there’s an enormous range of distributions out there. However, they aren’t all equally important. In fact, the vast majority of the content in this book relies on one of five distributions: the binomial distribution, the normal distribution, the t distribution, the \\(\\chi^2\\) (“chi-square”) distribution and the F distribution. Given this, what I’ll do over the next few sections is provide a brief introduction to all five of these, paying special attention to the binomial and the normal. I’ll start with the binomial distribution since it’s the simplest of the five.\n\n7.4.1 二項分佈入門\nThe theory of probability originated in the attempt to describe how games of chance work, so it seems fitting that our discussion of the binomial distribution should involve a discussion of rolling dice and flipping coins. Let’s imagine a simple “experiment”. In my hot little hand I’m holding 20 identical six-sided dice. On one face of each die there’s a picture of a skull, the other five faces are all blank. If I proceed to roll all 20 dice, what’s the probability that I’ll get exactly 4 skulls? Assuming that the dice are fair, we know that the chance of any one die coming up skulls is 1 in 6. To say this another way, the skull probability for a single die is approximately .167. This is enough information to answer our question, so let’s have a look at how it’s done.\nAs usual, we’ll want to introduce some names and some notation. We’ll let \\(N\\) denote the number of dice rolls in our experiment, which is often referred to as the size parameter of our binomial distribution. We’ll also use \\(\\theta\\) to refer to the the probability that a single die comes up skulls, a quantity that is usually called the success probability of the binomial.8 Finally, we’ll use \\(X\\) to refer to the results of our experiment, namely the number of skulls I get when I roll the dice. Since the actual value of \\(X\\) is due to chance we refer to it as a random variable. In any case, now that we have all this terminology and notation we can use it to state the problem a little more precisely. The quantity that we want to calculate is the probability that \\(X = 4\\) given that we know that \\(\\theta = .167\\) and \\(N = 20\\). The general “form” of the thing I’m interested in calculating could be written as\n\\[P(X|\\theta,N)\\]\nand we’re interested in the special case where \\(X = 4\\), \\(\\theta = .167\\) and \\(N = 20\\).\n[Additional technical detail 9]\nYeah, yeah. I know what you’re thinking: notation, notation, notation. Really, who cares? Very few readers of this book are here for the notation, so I should probably move on and talk about how to use the binomial distribution. I’ve included the formula for the binomial distribution in Table 7.2, since some readers may want to play with it themselves, but since most people probably don’t care that much and because we don’t need the formula in this book, I won’t talk about it in any detail. Instead, I just want to show you what the binomial distribution looks like.\nTo that end, Figure 7.3 plots the binomial probabilities for all possible values of \\(X\\) for our dice rolling experiment, from \\(X = 0\\) (no skulls) all the way up to \\(X = 20\\) (all skulls). Note that this is basically a bar chart, and is no different to the “trousers probability” plot I drew in Figure 7.2. On the horizontal axis we have all the possible events, and on the vertical axis we can read off the probability of each of those events. So, the probability of rolling \\(4\\) skulls out of \\(20\\) is about \\(0.20\\) (the actual answer is \\(0.2022036\\), as we’ll see in a moment). In other words, you’d expect that to happen about 20% of the times you repeated this experiment.\nTo give you a feel for how the binomial distribution changes when we alter the values of \\(theta\\) and \\(N\\), let’s suppose that instead of rolling dice I’m actually flipping coins. This time around, my experiment involves flipping a fair coin repeatedly and the outcome that I’m interested in is the number of heads that I observe. In this scenario, the success probability is now \\(\\theta = \\frac{1}{2}\\). Suppose I were to flip the coin \\(N = 20\\) times. In this example, I’ve changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as Figure 7.4 shows, the main effect of this is to shift the whole distribution, as you’d expect. Okay, what if we flipped a coin \\(N = 100\\) times? Well, in that case we get Figure 7.4 (b). The distribution stays roughly in the middle but there’s a bit more variability in the possible outcomes.\n\n\n\n\n\nFigure 7.3: The binomial distribution with size parameter of \\(N = 20\\) and an underlying success probability of \\(\\theta = \\frac{1}{6}\\). Each vertical bar depicts the probability of one specific outcome (i.e., one possible value of X). Because this is a probability distribution, each of the probabilities must be a number between 0 and 1, and the heights of the bars must sum to 1 as well\n\n\n\n\nI were to flip the coin \\(N = 20\\) times. In this example, I’ve changed the success probability but kept the size of the experiment the same. What does this do to our binomial distribution? Well, as Figure 7.4 (a) shows, the main effect of this is to shift the whole distribution, as you’d expect. Okay, what if we flipped a coin \\(N = 100\\) times? Well, in that case we get Figure 7.4 (a). The distribution stays roughly in the middle but there’s a bit more variability in the possible outcomes."
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "title": "7  機率入門",
    "section": "7.5 常態分佈",
    "text": "7.5 常態分佈\nWhile the binomial distribution is conceptually the simplest distribution to understand, it’s not the most important one. That particular honour goes to the normal distribution, also referred to as “the bell curve” or a “Gaussian distribution”. A normal distribution is described using two parameters: the mean of the distribution µ and the standard deviation of the distribution \\(\\sigma\\).\n\n\n\n\n\nFigure 7.4: Two binomial distributions, involving a scenario in which I’m flipping a fair coin, so the underlying success probability is \\(\\theta = \\frac{1}{2}\\). In panel (a), we assume I’m flipping the coin \\(N = 20\\) times. In panel (b) we assume that the coin is flipped \\(N = 100\\) times\n\n\n\n\n\n\n\n\n\nFigure 7.5: The normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). The x-axis corresponds to the value of some variable, and the y-axis tells us something about how likely we are to observe that value. However, notice that the y-axis is labelled Probability Density and not Probability. There is a subtle and somewhat frustrating characteristic of continuous distributions that makes the \\(y\\) axis behave a bit oddly - the height of the curve here isn’t actually the probability of observing a particular x value. On the other hand, it is true that the heights of the curve tells you which \\(x\\) values are more likely (the higher ones!). (see [Probability density] section for all the annoying details)\n\n\n\n\n[Additional technical detail 10]\nLet’s try to get a sense for what it means for a variable to be normally distributed. To that end, have a look at Figure 7.5 which plots a normal distribution with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). You can see where the name “bell curve” comes from; it looks a bit like a bell. Notice that, unlike the plots that I drew to illustrate the binomial distribution, the picture of the normal distribution in Figure 7.5 shows a smooth curve instead of “histogram-like” bars. This isn’t an arbitrary choice, the normal distribution is continuous whereas the binomial is discrete. For instance, in the die rolling example from the last section it was possible to get 3 skulls or 4 skulls, but impossible to get 3.9 skulls. The figures that I drew in the previous section reflected this fact. In Figure 7.3, for instance, there’s a bar located at \\(X = 3\\) and another one at \\(X = 4\\) but there’s nothing in between. Continuous quantities don’t have this constraint. For instance, suppose we’re talking about the weather. The temperature on a pleasant Spring day could be 23 degrees, 24 degrees, 23.9 degrees, or anything in between since temperature is a continuous variable. And so a normal distribution might be quite appropriate for describing Spring temperatures11\n\n\n\n\n\nFigure 7.6: An illustration of what happens when you change the mean of a normal distribution. The solid line depicts a normal distribution with a mean of \\(\\mu = 4\\). The dashed line shows a normal distribution with a mean of \\(\\mu = 7\\). In both cases, the standard deviation is \\(\\sigma = 1\\). Not surprisingly, the two distributions have the same shape, but the dashed line is shifted to the right\n\n\n\n\nWith this in mind, let’s see if we can’t get an intuition for how the normal distribution works. First, let’s have a look at what happens when we play around with the parameters of the distribution. To that end, Figure 7.6 plots normal distributions that have different means but have the same standard deviation. As you might expect, all of these distributions have the same “width”. The only difference between them is that they’ve been shifted to the left or to the right. In every other respect they’re identical. In contrast, if we increase the standard deviation while keeping the mean constant, the peak of the distribution stays in the same place but the distribution gets wider, as you can see in Figure 7.7. Notice, though, that when we widen the distribution the height of the peak shrinks. This has to happen, in the same way that the heights of the bars that we used to draw a discrete binomial distribution have to sum to 1, the total area under the curve for the normal distribution must equal 1. Before moving on, I want to point out one important characteristic of the normal distribution. Irrespective of what the actual mean and standard deviation are, \\(68.3\\%\\) of the area falls within 1 standard deviation of the mean. Similarly, \\(95.4\\%\\) of the distribution falls within 2 standard deviations of the mean, and \\((99.7\\%)\\) of the distribution is within 3 standard deviations. This idea is illustrated in Figure 7.8; see also Figure 7.9.\n\n\n\n\n\nFigure 7.7: An illustration of what happens when you change the the standard deviation of a normal distribution. Both distributions plotted in this figure have a mean of \\(\\mu = 5\\), but they have different standard deviations. The solid line plots a distribution with standard deviation \\(\\sigma = 1\\), and the dashed line shows a distribution with standard deviation \\(\\sigma = 2\\). As a consequence, both distributions are ‘centred’ on the same spot, but the dashed line is wider than the solid one\n\n\n\n\n\n\n\n\n\nFigure 7.8: The area under the curve tells you the probability that an observation falls within a particular range. The solid lines plot normal distributions with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma = 1\\). The shaded areas illustrate ‘areas under the curve’ for two important cases. In panel (a), we can see that there is a 68.3% chance that an observation will fall within one standard deviation of the mean. In panel (b), we see that there is a 95.4% chance that an observation will fall within two standard deviations of the mean\n\n\n\n\n\n\n\n\n\nFigure 7.9: Two more examples of the ‘area under the curve idea’. There is a 15.9% chance that an observation is one standard deviation below the mean or smaller (panel (a)), and a 34.1% chance that the observation is somewhere between one standard deviation below the mean and the mean (panel (b)). Notice that if you add these two numbers together you get 15.9% + 34.1% = 50%. For normally distributed data, there is a 50% chance that an observation falls below the mean. And of course that also implies that there is a 50% chance that it falls above the mean\n\n\n\n\n\n7.5.1 機率密度\nThere’s something I’ve been trying to hide throughout my discussion of the normal distribution, something that some introductory textbooks omit completely. They might be right to do so. This “thing” that I’m hiding is weird and counter-intuitive even by the admittedly distorted standards that apply in statistics. Fortunately, it’s not something that you need to understand at a deep level in order to do basic statistics. Rather, it’s something that starts to become important later on when you move beyond the basics. So, if it doesn’t make complete sense, don’t worry too much, but try to make sure that you follow the gist of it.\nThroughout my discussion of the normal distribution there’s been one or two things that don’t quite make sense. Perhaps you noticed that the y-axis in these figures is labelled “Probability Density” rather than density. Maybe you noticed that I used \\(P(X)\\) instead of \\(P(X)\\) when giving the formula for the normal distribution.\nAs it turns out, what is presented here isn’t actually a probability, it’s something else. To understand what that something is you have to spend a little time thinking about what it really means to say that \\(X\\) is a continuous variable. Let’s say we’re talking about the temperature outside. The thermometer tells me it’s \\(23\\) degrees, but I know that’s not really true. It’s not exactly \\(23\\) degrees. Maybe it’s \\(23.1\\) degrees, I think to myself. But I know that that’s not really true either because it might actually be \\(23.09\\) degrees. But I know that… well, you get the idea. The tricky thing with genuinely continuous quantities is that you never really know exactly what they are.\nNow think about what this implies when we talk about probabilities. Suppose that tomorrow’s maximum temperature is sampled from a normal distribution with mean \\(23\\) and standard deviation 1. What’s the probability that the temperature will be exactly \\(23\\) degrees? The answer is “zero”, or possibly “a number so close to zero that it might as well be zero”. Why is this? It’s like trying to throw a dart at an infinitely small dart board. No matter how good your aim, you’ll never hit it. In real life you’ll never get a value of exactly \\(23\\). It’ll always be something like \\(23.1\\) or \\(22.99998\\) or suchlike. In other words, it’s completely meaningless to talk about the probability that the temperature is exactly \\(23\\) degrees. However, in everyday language if I told you that it was \\(23\\) degrees outside and it turned out to be \\(22.9998\\) degrees you probably wouldn’t call me a liar. Because in everyday language “\\(23\\) degrees” usually means something like “somewhere between \\(22.5\\) and \\(23.5\\) degrees”. And while it doesn’t feel very meaningful to ask about the probability that the temperature is exactly \\(23\\) degrees, it does seem sensible to ask about the probability that the temperature lies between \\(22.5\\) and \\(23.5\\), or between \\(20\\) and \\(30\\), or any other range of temperatures.\nThe point of this discussion is to make clear that when we’re talking about continuous distributions it’s not meaningful to talk about the probability of a specific value. However, what we can talk about is the probability that the value lies within a particular range of values. To find out the probability associated with a particular range what you need to do is calculate the “area under the curve”. We’ve seen this concept already, in Figure 7.8 the shaded areas shown depict genuine probabilities (e.g., in Figure 7.8) it shows the probability of observing a value that falls within 1 standard deviation of the mean).\nOkay, so that explains part of the story. I’ve explained a little bit about how continuous probability distributions should be interpreted (i.e., area under the curve is the key thing). But what does the formula for ppxq that I described earlier actually mean? Obviously, \\(P(x)\\) doesn’t describe a probability, but what is it? The name for this quantity \\(P(x)\\) is a probability density, and in terms of the plots we’ve been drawing it corresponds to the height of the curve. The densities themselves aren’t meaningful in and of themselves, but they’re “rigged” to ensure that the area under the curve is always interpretable as genuine probabilities. To be honest, that’s about as much as you really need to know for now.12"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "href": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "title": "7  機率入門",
    "section": "7.6 其他常見機率分佈",
    "text": "7.6 其他常見機率分佈\nThe normal distribution is the distribution that statistics makes most use of (for reasons to be discussed shortly), and the binomial distribution is a very useful one for lots of purposes. But the world of statistics is filled with probability distributions, some of which we’ll run into in passing. In particular, the three that will appear in this book are the t distribution, the \\(\\chi^2\\) distribution and the F distribution. I won’t give formulas for any of these, or talk about them in too much detail, but I will show you some pictures: Figure 7.10, Figure 7.11 and Figure 7.12.\n\n\n\n\n\nFigure 7.10: A \\(t\\) distribution with 3 degrees of freedom (solid line). It looks similar to a normal distribution, but it’s not quite the same. For comparison purposes I’ve plotted a standard normal distribution as the dashed line\n\n\n\n\n\n\n\n\n\nFigure 7.11: \\(\\chi^2\\) distribution with 3 degrees of freedom. Notice that the observed values must always be greater than zero, and that the distribution is pretty skewed. These are the key features of a chi-square distribution\n\n\n\n\n\n\n\n\n\nFigure 7.12: An \\(F\\) distribution with 3 and 5 degrees of freedom. Qualitatively speaking, it looks pretty similar to a chi-square distribution, but they’re not quite the same in general\n\n\n\n\n\nThe \\(t\\) distribution is a continuous distribution that looks very similar to a normal distribution, see Figure 7.10. Note that the “tails” of the t distribution are “heavier” (i.e., extend further outwards) than the tails of the normal distribution). That’s the important difference between the two. This distribution tends to arise in situations where you think that the data actually follow a normal distribution, but you don’t know the mean or standard deviation. We’ll run into this distribution again in Chapter 11.\nThe \\(\\chi^2\\) distribution is another distribution that turns up in lots of different places. The situation in which we’ll see it is when doing categorical data analysis in ?sec-Categorical-data-analysis, but it’s one of those things that actually pops up all over the place. When you dig into the maths (and who doesn’t love doing that?), it turns out that the main reason why the \\(\\chi^2\\) distribution turns up all over the place is that if you have a bunch of variables that are normally distributed, square their values and then add them up (a procedure referred to as taking a “sum of squares”), this sum has a \\(\\chi^2\\) distribution. You’d be amazed how often this fact turns out to be useful. Anyway, here’s what a \\(\\chi^2\\) distribution looks like: Figure 7.11.\nThe \\(F\\) distribution looks a bit like a \\(\\chi^2\\) distribution, and it arises whenever you need to compare two \\(\\chi^2\\) distributions to one another. Admittedly, this doesn’t exactly sound like something that any sane person would want to do, but it turns out to be very important in real world data analysis. Remember when I said that \\(\\chi^2\\) turns out to be the key distribution when we’re taking a “sum of squares”? Well, what that means is if you want to compare two different “sums of squares”, you’re probably talking about something that has an F distribution. Of course, as yet I still haven’t given you an example of anything that involves a sum of squares, but I will in ?sec-Comparing-several-means-one-way-ANOVA. And that’s where we’ll run into the F distribution. Oh, and there’s a picture in Figure 7.12.\n\nOkay, time to wrap this section up. We’ve seen three new distributions: \\(\\chi^2\\)), \\(t\\) and \\(F\\). They’re all continuous distributions, and they’re all closely related to the normal distribution. The main thing for our purposes is that you grasp the basic idea that these distributions are all deeply related to one another, and to the normal distribution. Later on in this book we’re going to run into data that are normally distributed, or at least assumed to be normally distributed. What I want you to understand right now is that, if you make the assumption that your data are normally distributed, you shouldn’t be surprised to see \\(\\chi^2\\), \\(t\\) and \\(F\\) distributions popping up all over the place when you start trying to do your data analysis."
  },
  {
    "objectID": "07-Introduction-to-probability.html#summary",
    "href": "07-Introduction-to-probability.html#summary",
    "title": "7  機率入門",
    "section": "7.7 Summary",
    "text": "7.7 Summary\nIn this chapter we’ve talked about probability. We’ve talked about what probability means and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:\n\nProbability theory versus statistics: [How are probability and statistics different?]\nThe frequentist view versus The Bayesian view of probability\nBasic probability theory\nThe binomial distribution, The normal distribution, and Other useful distributions\n\nAs you’d expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic. I’ve described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called “Statistical Distributions” (Evans et al., 2011) that lists a lot more than that. Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.\nPicking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in Chapter 11 we’re going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.\n\n\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical Transactions of the Royal Society A, 222, 309–368.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A methodological paradox. Philosophy of Science, 34, 103–115."
  },
  {
    "objectID": "06-Pragmatic-matters.html#本章小結",
    "href": "06-Pragmatic-matters.html#本章小結",
    "title": "6  實務課題",
    "section": "6.6 本章小結",
    "text": "6.6 本章小結\n這一章是原作者分享實務經驗會用到的各種不起眼，但是會影響描述統計品質的小技巧或注意項目。最後再次回顧本章的課題\n\n製作次數表及列聯表\n邏輯運算\n資料變項的轉換與編碼\n數學函式及運算子\n篩選部分資料"
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯表達",
    "href": "06-Pragmatic-matters.html#邏輯表達",
    "title": "6  實務課題",
    "section": "6.2 邏輯表達",
    "text": "6.2 邏輯表達\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 判斷算式真假值\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 邏輯運算子\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 在文字內嵌入邏輯運算子\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#機率和統計有什麼不一樣",
    "href": "07-Introduction-to-probability.html#機率和統計有什麼不一樣",
    "title": "7  機率入門",
    "section": "7.1 機率和統計有什麼不一樣？",
    "text": "7.1 機率和統計有什麼不一樣？\n\nBefore we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:2\n\nWhat are the chances of a fair coin coming up heads 10 times in a row?\nIf I roll a six sided dice twice, how likely is it that I’ll roll two sixes?\nHow likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?\nWhat are the chances that I’ll win the lottery?\n\nNotice that all of these questions have something in common. In each case the “truth of the world” is known and my question relates to the “what kind of events” will happen. In the first question I know that the coin is fair so there’s a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:\n\\[P(head)=0.5\\]\nwhich you can read as “the probability of heads is 0.5”. As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing. In probability theory the model is known but the data are not.\nSo that’s probability. What about statistics? Statistical questions work the other way around. In statistics we do not know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:\n\nIf my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?\nIf five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?\nIf the lottery commissioner’s spouse wins the lottery how likely is it that the lottery was rigged?\n\nThis time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:\nH H H H H H H H H H H\nand what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as \\(P(heads)\\ne{0.5}\\). In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.1 Samples, populations and sampling",
    "text": "8.1 Samples, populations and sampling\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 Defining a population\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 Simple random samples\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 Most samples are not simple random samples\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 How much does it matter if you don’t have a simple random sample?\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 Population parameters and sample statistics\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about Estimating population parameters using your sample statistics and also Estimating a confidence interval but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.2 The law of large numbers",
    "text": "8.2 The law of large numbers\nIn the previous section I showed you the results of one fictitious IQ experiment with a sample size of N = 100. The results were somewhat encouraging as the true population mean is 100 and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it? The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using jamovi. The IQsim.omv file is a jamovi data file. In this file I have generated 10,000 random numbers sampled from a normal distribution for a population with mean = 100 and sd = 15. This was done by computing a new variable using the = NORM(100,15) function. A histogram and density plot shows that this larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard deviation is 14.90. These values are now very close to the true population. See Figure 8.5.\n\n\n\n\n\nFigure 8.5: A random sample drawn from a normal distribution using jamovi\n\n\n\n\nI feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli, one of the founders of probability theory, formalised this idea back in 1713 he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:\n\nFor even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal (Stigler, 1986, p. 65).\n\nOkay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct. It really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the law of large numbers. The law of large numbers is a mathematical law that applies to many different sample statistics but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as \\(N \\longrightarrow \\infty\\)), the sample mean approaches the population mean \\(\\bar{X} \\longrightarrow \\mu\\))3\nI don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.3 Sampling distributions and the central limit theorem",
    "text": "8.3 Sampling distributions and the central limit theorem\nThe law of large numbers is a very powerful tool but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee”. In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life.\n\n[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again. (Keynes, 1923, p. 80).\n\nAs in economics, so too in psychology and statistics. It is not enough to know that we will eventually arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of \\(N = 100\\). In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!\n\n8.3.1 Sampling distribution of the mean\nWith this in mind, let’s abandon the idea that our studies will have sample sizes of 10,000 and consider instead a very modest experiment indeed. This time around we’ll sample \\(N = 5\\) people and measure their IQ scores. As before, I can simulate this experiment in jamovi = NORM(100,15) function, but I only need 5 participant IDs this time, not 10,000. These are the five numbers that jamovi generated:\n90 82 94 99 110\nThe mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to replicate the experiment. That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates these five numbers:\n78 88 111 111 117\nThis time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in Table 8.1, and as you can see the sample mean varies from one replication to the next.\n\n\n\n\nTable 8.1:  Ten replications of the IQ experiment, each with a sample size of ( N = 5 ) \n\nPerson 1Person 2Person 3Person 4Person 5Sample Mean\n\nRep. 19082949911095.0\n\nRep. 27888111111117101.0\n\nRep. 3111122919886101.6\n\nRep. 4989611999107103.8\n\nRep. 510511310310398104.4\n\nRep. 68189938511492.4\n\nRep. 71009310898133106.4\n\nRep. 810710010511785102.8\n\nRep. 98611910873116100.4\n\nRep. 109512611212076105.8\n\n\n\n\n\nNow suppose that I decided to keep going in this fashion, replicating this “five IQ scores” experiment over and over again. Every time I replicate the experiment I write down the sample mean. Over time, I’d be amassing a new data set, in which every experiment generates a single data point. The first 10 observations from my data set are the sample means listed in Table 8.1, so my data set starts out like this:\n95.0 101.0 101.6 103.8 104.4 …\nWhat if I continued like this for 10,000 replications, and then drew a histogram. Well that’s exactly what I did, and you can see the results in Figure 8.6. As this picture illustrates, the average of 5 IQ scores is usually between 90 and 110. But more importantly, what it highlights is that if we replicate an experiment over and over again, what we end up with is a distribution of sample means! (Table 8.1)) This distribution has a special name in statistics, it’s called the sampling distribution of the mean.\n\n\n\n\n\nFigure 8.6: The sampling distribution of the mean for the ‘five IQ scores experiment’. If you sample 5 people at random and calculate their average IQ you’ll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores\n\n\n\n\nSampling distributions are another important theoretical idea in statistics, and they’re crucial for understanding the behaviour of small samples. For instance, when I ran the very first “five IQ scores” experiment, the sample mean turned out to be 95. What the sampling distribution in Figure 8.6 tells us, though, is that the “five IQ scores” experiment is not very accurate. If I repeat the experiment, the sampling distribution tells me that I can expect to see a sample mean anywhere between 80 and 120.\n\n\n8.3.2 Sampling distributions exist for any sample statistic!\nOne thing to keep in mind when thinking about sampling distributions is that any sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time I replicated the “five IQ scores” experiment I wrote down the largest IQ score in the experiment. This would give me a data set that started out like this:\n110 117 122 119 113 …\nDoing this over and over again would give me a very different sampling distribution, namely the sampling distribution of the maximum. The sampling distribution of the maximum of 5 IQ scores is shown in Figure 8.7. Not surprisingly, if you pick 5 people at random and then find the person with the highest IQ score, they’re going to have an above average IQ. Most of the time you’ll end up with someone whose IQ is measured in the 100 to 140 range.\n\n\n\n\n\nFigure 8.7: The sampling distribution of the maximum for the ‘five IQ scores experiment’. If you sample 5 people at random and select the one with the highest IQ score you’ll probably see someone with an IQ between 100 and 140\n\n\n\n\n\n\n\n\n\nFigure 8.8: An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel (a), each data set contained only a single observation, so the mean of each sample is just one person’s IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than any one person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel (c)), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean\n\n\n\n\n\n\n8.3.3 The central limit theorem\nAt this point I hope you have a pretty good sense of what sampling distributions are, and in particular what the sampling distribution of the mean is. In this section I want to talk about how the sampling distribution of the mean changes as a function of sample size. Intuitively, you already know part of the answer. If you only have a few observations, the sample mean is likely to be quite inaccurate. If you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you’ll probably get the same answer you got last time, so the sampling distribution will be very narrow. You can see this visually in Figure 8.8, showing that the bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the standard error. The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the picture, as the sample size \\(N\\) increases, the SEM decreases.\nOkay, so that’s one part of the story. However, there’s something I’ve been glossing over so far. All my examples up to this point have been based on the “IQ scores” experiments, and because IQ scores are roughly normally distributed I’ve assumed that the population distribution is normal. What if it isn’t normal? What happens to the sampling distribution of the mean? The remarkable thing is this, no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this I ran some simulations. To do this, I started with the “ramped” distribution shown in the histogram in Figure 8.9. As you can see by comparing the triangular shaped histogram to the bell curve plotted by the black line, the population distribution doesn’t look very much like a normal distribution at all. Next, I simulated the results of a large number of experiments. In each experiment I took \\(N = 2\\) samples from this distribution, and then calculated the sample mean. Figure 8.9 (b) plots the histogram of these sample means (i.e., the sampling distribution of the mean for \\(N = 2\\)). This time, the histogram produces a \\(\\chi^2\\)-shaped distribution. It’s still not normal, but it’s a lot closer to the black line than the population distribution in Figure 8.9 (a). When I increase the sample size to \\(N = 4\\), the sampling distribution of the mean is very close to normal (Figure 8.9 (c)), and by the time we reach a sample size of N = 8 it’s almost perfectly normal. In other words, as long as your sample size isn’t tiny, the sampling distribution of the mean will be approximately normal no matter what your population distribution looks like!\n\n\n\n\n\nFigure 8.9: A demonstration of the central limit theorem. In panel (a), we have a non-normal population distribution, and panels (b)-(d) show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel (a). As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations\n\n\n\n\nOn the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean.\n\nThe mean of the sampling distribution is the same as the mean of the population\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases\nThe shape of the sampling distribution becomes normal as the sample size increases\n\nAs it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the central limit theorem. Among other things, the central limit theorem tells us that if the population distribution has mean µ and standard deviation σ, then the sampling distribution of the mean also has mean µ and the standard error of the mean is\n\\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]\nBecause we divide the population standard deviation σ by the square root of the sample size N, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.4\nThis result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.4 Estimating population parameters",
    "text": "8.4 Estimating population parameters\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.5 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 Estimating the population mean\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 Estimating the population standard deviation\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.6 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on Estimating population parameters, the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in Estimating population parameters. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).7\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "title": "8  運用樣本估計未知量數",
    "section": "8.5 母群參數的區間估計",
    "text": "8.5 母群參數的區間估計\n\n學習統計的最大收穫是永遠不再說「我肯定這就是答案」\n– 來源不明9\n\n這一章到目前為止，已經概述了統計學家常取用的基本取樣理論，用來根樣本資料猜測母群參數。正如己經解說的各節主題，我們需要取樣理論的原因之一是每個資料集都會有一些不確定性，因此估計值永遠不會百分之百準確。這一節我們要學的是量化估計不確定程度的方法，因為只報告像是大學心理學專業學生平均智商為115這要的點估計值並不足夠，我們還要能用數值表達猜測程度。例如：有95％ 的可能性，真實平均值介於109和121之間。這種表達方式被稱做平均值的信賴區間。\n充分理解取樣分佈，計算平均值的信賴區間其實相當簡單。這裡解說計算方法：假設母群平均值為 \\(\\mu\\)，標準差為 \\(\\sigma\\)。我剛完成了一項有 N 名參與者的研究，這些參與者的智力測驗分數的平均值是 \\(\\bar{X}\\)。根據中央極限定理，我們可以知道平均值的取樣分佈接近常態分佈。經由 Section 7.5 關於常態分佈的學習，我們也知道一個服從常態分佈的取樣分佈，在平均值周圍相距約兩個標準差之內，有95％左右的量數落在此範圍內。\n更精確的正確答案是：以一個符合常態分佈的取樣分佈估計母群平均值，估計值有95% 的機率落在真正母群平均值的1.96個標準差內。接下來，請記住取樣分配的標準差正式名稱為標準誤(standar error)，因此平均值的標準誤差可簡寫為SEM(Standard Error of Measurement, 測量標準誤)。當我們合成這些數字時，就能得知有95%的可能性確定由樣本資料計算得到的樣本平均值 \\(\\bar{X}\\) ，與母群平均值差距在1.96倍的標準誤差之內。\n當然，1.96並不是什麼神奇數字。如果你想要計算一個95％的信賴區間，這就是經常需要使用的乘數。若是我想要70％的置信區間，我會使用1.04作為神奇數字而不是1.96。\n[更多技術細節10]\n\n\n8.5.1 解讀信賴區間\n學習信賴區間最困難的地方是理解其意義。很多學生第一次接觸信賴區間時，總是憑直覺說“真實平均值有95％的機率落在信賴區間內”。這種說法很簡單，似乎捕捉到“我有95％的自信”的常識想法。不幸的是，這並不完全正確。直覺定義非常依賴個人對母群平均值的個人價值觀。我之所以會說我有95％的自信，是因為這樣的說法是基於我的個人信仰。在日常生活這樣講沒什麼問題，但是回顧一下 如何解讀機率？你會注意到談論個人信念和自信心的統計觀點是貝氏統計的中心思想。然而，信賴區間不是基於貝氏觀點開發出來的工具。就像這一章各節介紹的其他估計方法，信賴區間是次數主義學派開發的工具；如果要使用次數主義學派的方法，用貝氏觀點解釋就不對盤了！\n好吧，如果這樣子不是正確回答，那麼要怎麼回答呢？記得討論次數主義如何解讀機率的時候，唯一“陳述機率”的合法方式是列舉一系列可觀察的機率事件，再計算各種事件的出現次數。由此看來，解釋95％信賴區間必須根據次數。具體而言，如果我們反覆執行同樣的實驗程序，並計算每次實驗結果的95％信賴區間，大約會有95％的實驗結果計算的區間會包含真正的平均值。更進一步的說法是，使用這樣的實驗程序得到的所有資料樣本信賴區間，應該會有95％包括真正的母群平均值 。Figure 8.12 用模擬實驗結果的視覺化說明這樣的概念，兩幅圖各顯示50個信賴區間，分別表示“10位受測者的智力測驗分數”（圖a）和“25位受測者的智力測驗分數”（圖b）。有點幸運的是，在這100次模擬實驗結果裡，恰好有95次包含了真實平均值。\n這與貝氏觀點的關鍵差異在於，貝氏認為我們能用一個機率值，表達我們看不到的母群平均值有多麼不確定，但是次數主義學派不允許這樣解讀機率，因為沒有人能“復刻”母群！次數主義學派主張母群平均值是固定的，不能母群做任何機率式的陳述。不過，資料樣本的信賴區間是可重覆取得及計算的，所以我們可以不斷重做實驗。\n因此，次數主義學派允許使用信賴區間（一種隨機變數），表達包括真實平均值的機率，但不允許估計真實母群平均值（不可重覆觀察的事件）落在置信區間內的概率。我知道這似乎有點拘泥於細節，但這確實很重要。它之所以重要是因為解釋方式的差異導致了數學方法的差異。貝氏統計有一種替代信賴區間的方法，稱為可信區間(Credible intervals)。在大多數情況下，可信區間與信賴區間的數值範圍非常接近，但在其他情況下可能會截然不同。在本書@sec-Bayesian-statistics，我們將學習更多貝氏統計觀點。\n\n\n\n\n\n\n\nFigure 8.12: 95％信賴區間視覺化。圖（a）顯示50次模擬實驗的結果，每次實驗測量10人的智力測驗分數。一個點表示一次實驗的樣本平均值，一個線條表示95％信賴區間。其中有47個信賴區間包含期望的平均值（即100），標示星號的三個區間則沒有包含。圖（b）展示另一項類似的模擬實驗，每次實驗測量25人的智力測驗分數。\n\n\n\n\n\n\n8.5.2 計算信賴區間\njamovi 的描述統計模組內建計算信賴區間的簡單設定。開啟’Descriptives’面板的’Statistics’子選單，最下面有兩個勾選框，分別是’Std. Error of Mean’（平均值標準誤差）和’Confidence interval for the mean’（平均值信賴區間），同學可以使用這些功能計算每個平均值的95％信賴區間（預設值）。例如，載入 IQsim.omv 檔案並勾選’Confidence interval for the mean’，就可以得到與智力測驗模擬實驗平均分數的信賴區間：下限95％CI = 99.39 和上限95％CI = 99.97。也就是大樣本數據（N=10,000）的模擬結果顯示，平均智商得分為99.68，95％ CI 為99.39至99.97。\n假如要使用 jamovi 繪製信賴區間的統計圖，可以指定平均值作為箱形圖選項之一。此外，在學習特定統計方法時（例如 Chapter 12 ），我們還可以將信賴區間作為分析報表的一部分。這很酷，稍後我們會學習如何做到。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "title": "8  從樣本估計未知量數",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:\n\nBasic ideas about [Samples, populations and sampling]\nStatistical theory of sampling: [The law of large numbers] and [Sampling distributions and the central limit theorem]\n[Estimating population parameters]. Means and standard deviations\n[Estimating a confidence interval]\n\nAs always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.\n\n\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan; Company.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard University Press."
  },
  {
    "objectID": "07-Introduction-to-probability.html#如何解讀機率",
    "href": "07-Introduction-to-probability.html#如何解讀機率",
    "title": "7  機率入門",
    "section": "7.2 如何解讀機率？",
    "text": "7.2 如何解讀機率？\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n7.2.1 次數主義觀點\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the 次數主義觀點(frequentist view) and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 7.1:  Coin flips and proportion of heads \n\nnumber of flips12345678910\n\nnumber of heads0123444567\n\nproportion00.50.670.750.80.670.570.630.670.7\n\nnumber of flips11121314151617181920\n\nnumber of heads88910101010101011\n\nproportion0.730.670.690.710.670.630.590.560.530.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 7.1. As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.1 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 8.5).\n\n\n\n\n\nFigure 7.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n7.2.2 貝氏觀點\n貝氏觀點(The Bayesian view) of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n7.2.3 觀點之間的差異是什麼？何者正確？\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in Chapter 16."
  },
  {
    "objectID": "07-Introduction-to-probability.html#基本機率理論",
    "href": "07-Introduction-to-probability.html#基本機率理論",
    "title": "7  機率入門",
    "section": "7.3 基本機率理論",
    "text": "7.3 基本機率理論\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n7.3.1 機率分佈入門\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 7.2 shows an example of a probability distribution.\n\n\n\n\nTable 7.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see Section 5.3) to visualise this distribution, as shown in Figure 7.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 7.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 7.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\nTable 7.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#本章小結",
    "href": "07-Introduction-to-probability.html#本章小結",
    "title": "7  機率入門",
    "section": "7.7 本章小結",
    "text": "7.7 本章小結\nIn this chapter we’ve talked about probability. We’ve talked about what probability means and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:\n\nProbability theory versus statistics: [How are probability and statistics different?]\n[The frequentist view] versus [The Bayesian view] of probability\n[Basic probability theory]\n[The binomial distribution], [The normal distribution], and [Other useful distributions]\n\nAs you’d expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic. I’ve described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called “Statistical Distributions” (Evans et al., 2011) that lists a lot more than that. Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.\nPicking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in Chapter 11 we’re going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.\n\n\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical Transactions of the Royal Society A, 222, 309–368.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A methodological paradox. Philosophy of Science, 34, 103–115."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群取樣",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群取樣",
    "title": "8  從樣本估計未知量數",
    "section": "8.1 樣本、母群、取樣",
    "text": "8.1 樣本、母群、取樣\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 定義何謂母群\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 簡單隨機樣本\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 母群參數與樣本統計\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about [Estimating population parameters] using your sample statistics and also [Estimating a confidence interval] but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#大數法則",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#大數法則",
    "title": "8  運用樣本估計未知量數",
    "section": "8.2 大數法則",
    "text": "8.2 大數法則\n在前一節中，我們展示了一個樣本數本數是 N = 100 的虛構智力測驗實驗結果。這個結果有些令人振奮，因為真實母群的平均值是 100，而樣本平均值 98.5 是一個相當合理的近似值。在許多科學研究中，這種精確度是可以接受的，但在其他情況下，我們希望更加精確。如果我們希望樣本統計量數更接近母群參數，可以怎麼做呢？顯然需要收集更多的資料。假定我們進行了一個更大型的實驗，這次測量了 10,000 人的智商。同學們可以使用 jamovi 模擬這個實驗的結果。在示範檔案庫裡的IQsim.omv ，我們生成了 10,000 個從平均值為 100、標準差為 15 的常態分佈中隨機取樣的數字。這是通過使用計算變項函式 = NORM(100, 15) 生成的。參見圖 Figure 8.5 的直方圖和密度圖，模擬結果顯示這個更大的樣本比較小的樣本更近似真實母群分佈。這也反映在樣本統計量數。更大樣本的平均智力分數是 99.68，標準差為 14.90。這些值現在非常接近真實母群。\n\n\n\n\n\n\n\nFigure 8.5: 使用jamovi產生由符合常態分佈的母群隨機取樣之結果。\n\n\n\n\n希望同學能從這個示範得到一點啟示，雖然有點不好意思，因為大樣本能提供更有品質的訊息，是顯而易見的，似乎不需要特別說明。其實這個觀點非常直觀，以至於機率理論的創始人之一雅各布·伯努利在1713年發表大數法則的論文時，曾經用尖酸刻薄的語氣描述這個人人都有的直覺：\n\n即使是最蠢的人，也憑藉本能的直覺，不靠他人教導(相當了不起)就能獨自明白：觀察的次數越多，結果就越不容易偏離目標。 (Stigler, 1986, p. 第65頁)。\n\n嗯，這段話聽起來有點自大（而且還有點性別歧視），但他的主要觀點是正確的。事實上，更多數據確實會產生更好的結果。問題是，為什麼會這樣呢？不出所料，所有人類都會直覺地認為的這樣的看法是正確的，統計學家稱之為大數法則。大數法則是一條適用於許多不同樣本統計量的數學法則，但最簡單的想法就是關於平均數的法則。樣本平均值是一個最明顯的例子，因為它是算述平均的產物（因為算述平均就是一個平均值）。大數法則應用於算述平均的意思是，隨著樣本數增加，算述平均的結果越趨近於真實的母群平均值。或者更精確一點地說，當樣本數“趨近”於無窮大（寫為\\(N \\longrightarrow \\infty\\)）時，樣本平均值趨近於母群平均值（\\(\\bar{X} \\longrightarrow \\mu\\)）4。\n我並不打算向同學示範如何證明大數法則，不過它是統計理論中最重要的工具之一。大數法則可以用來證明，收集越多的資料，最終將接近真相的數學工具。對於任何特定的資料集，所計算出來的樣本統計量數都可能是錯誤的，但是大數法則告訴我們，只要繼續收集更多的數據，這些樣本統計量數將趨近於真實的母群參數。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本分佈與中央極限定理",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本分佈與中央極限定理",
    "title": "8  運用樣本估計未知量數",
    "section": "8.3 樣本分佈與中央極限定理",
    "text": "8.3 樣本分佈與中央極限定理\n大數法則是非常強大的工具，但它並不能回答所有現實生活的問題。它給我們的只是一個“長期保證”。長期而言，如果我們能夠收集無限量的資料，那麼大數法則保證我們的樣本統計數據是正確的。但是正如經濟學大師凱因斯的著名論點，長期保證在我們的現實生活中幾乎沒有用處。\n\n長期保證對當前問題的指引是一個誤導，(因為問題解決時)，我們(可能)都已經死去。如果經濟學家只能告訴我們，當暴風雨過去後，海洋會再次變平靜，那麼他們的任務就太容易也太無用了，特別是在風雨飄搖的時刻。 (Keynes, 1923, p. 80)\n\n就像經濟學一樣，心理學和統計學也是如此。僅僅知道最終計算出的樣本平均值會趨近於母體平均值是不夠的。一個無限大的資料集終將揭露實際的母群平均值，只是精神上的慰藉，但是如果實際的資料樣本數只有 \\(N = 100\\) ，那麼這樣的知識在現實生活中是不夠用的。在心理學的實際研究場景，我們通常要從一個樣本數更小的資料計算樣本平均值，描述這群樣本的行為！\n\n\n8.3.1 平均值的樣本分佈\n考慮到現實情況，讓我們不得不放棄這樣的想法，也就是不要期待我們的研究會收集到一萬人，改成設計一個規模非常小的實驗。這次只找 \\(N=5\\) 個人，測量他們的智力分數。和前面的示範一樣，我們可以在 jamovi 中使用 NORM(100,15) 函式模擬這個實驗，但是這一次我只需要5個參與者，而不是10,000個。以下是 jamovi 隨機生成的五個數值(應該和你的操作結果不同)：\n\n90 82 94 99 110\n這批樣本的平均智力分數恰好是95。不出所料，比起前面示範的實驗結果不準確得多。現在想像一下，我決定重現(replicate)這項實驗。也就是說，我會盡可能地重做原來的實驗程序，隨機選擇另外5個人並測量他們的智力分數。同樣地，Jamovi可以模擬這個過程的結果，生成以下5個數字(也應該和你的操作結果不同)：\n\n78 88 111 111 117\n這次樣本的平均智力分數是101。如果我重做這個實驗10次，可能得到的結果如 表8-1 ，可以看到每次實驗的樣本平均值都不一樣。\n\n表8-1：重現智力測驗實驗十次的結果，每次樣本數都是 \\(( N = 5 )\\) 。\n\n\n\n\n\n\n\n\n\n\n\n\n\n參與者 1\n參與者 2\n參與者 3\n參與者 4\n參與者 5\n樣本平均值\n\n\n\n\n實驗批次 1\n90\n82\n94\n99\n110\n95.0\n\n\n實驗批次 2\n78\n88\n111\n111\n117\n101.0\n\n\n實驗批次 3\n111\n122\n91\n98\n86\n101.6\n\n\n實驗批次 4\n98\n96\n119\n99\n107\n103.8\n\n\n實驗批次 5\n105\n113\n103\n103\n98\n104.4\n\n\n實驗批次 6\n81\n89\n93\n85\n114\n92.4\n\n\n實驗批次 7\n100\n93\n108\n98\n133\n106.4\n\n\n實驗批次 8\n107\n100\n105\n117\n85\n102.8\n\n\n實驗批次 9\n86\n119\n108\n73\n116\n100.4\n\n\n實驗批次 10\n95\n126\n112\n120\n76\n105.8\n\n\n\n假如現在我決定繼續以這種方式，繼續重做這個「五個智力分數」的實驗。每次完成實驗，我都會記錄下樣本平均值。隨著實驗的進行，我會累積一個新的資料集，在這個資料集中，每次實驗都有產生一個資料點。我的資料集的前10個觀察值就是 表8-1 列出的樣本平均值，因此我的資料集前幾個數值是這樣的：\n\n95.0 101.0 101.6 103.8 104.4 …\n假如我繼續重做實驗10,000次，然後用每次實驗的樣本平均值繪製直方圖，那會怎麼樣呢？這正是你在@fig-fig8-6看到的結果。如這張圖所示，5個智力分數的平均值大部分落在90到110之間。更重要的是：如果我們一遍又一遍地重做一個實驗，我們最終得到的是一個樣本平均值的分佈！（見表8-1）。統計學的正式名稱叫做樣本平均值的取樣分佈(sampling distribution of the mean)。\n\n\n\n\n\n\nFigure 8.6: 「五個智力分數實驗」的樣本平均值分配。假如你隨機找五個人並計算他們的平均智力測驗分數，幾乎會得到一個介於80和120之間的數字，即使很多人測得的智力分數是高於120或低於80。為了比較，黑線表示智力分數的母群分佈。\n\n\n\n\n取樣分佈是另一個重要的統計理論概念，對於理解小樣本的行為非常關鍵。例如，當我進行第一個“五個智力分數”的實驗時，樣本平均值是95。但是 Figure 8.6 的取樣分佈告訴我們，“五個智力分數”的實驗不是很準確。如果我重複進行實驗，取樣分佈告訴我，我可以預期大部分的樣本平均值會是80到120之間。\n\n\n\n8.3.2 任何數值皆有樣本分佈！\n針對取樣分佈的概念，需要注意的一點是，任何你可能想要計算的樣本統計量數都有其對應的取樣分佈。例如，假設每次我重複進行 “五個智力分數” 的實驗時，我都記錄其中最高的智力分數。這會產生一個資料集，一開始的數值可能如下：\n\n110 117 122 119 113 …\n反覆進行這個實驗過程會給我一個非常不同的取樣分佈，也就是最大值的取樣分佈。五個智力分數的最大值的取樣分佈顯示在 Figure 8.7 。不出所料，如果你隨機選擇五個人，然後找出智力分數最高的人，他們的智力分數很有可能會高於平均水準。大多數情況下，你會得到智力分數在100到140之間的結果。\n\n\n\n\n\n\nFigure 8.7: 「五個智力分數實驗」最大值的取樣分佈。如果您隨機選取5個人，然後紀錄最高的智力分數，您可能會看到多數智力分數落在100到140之間。\n\n\n\n\n\n\n8.3.3 中央極限定理\n\n\n\n\n\nFigure 8.8: 這是一個說明樣本數如何影響樣本平均數的取樣分佈的例子。每張圖的直方圖是由10,000組智力分數樣本平值構成。圖中的直方圖顯示了這些平均值的分佈（即平均值的取樣分佈）。每個單獨的智力分數都是從平均值為100，標準差為15的常態分佈選機選取的，這在圖中以實線表示。圖(a)的每個資料集僅包含一個觀察值，因此每個樣本的平均值就是一個人的智力分數。因此，平均數的取樣分佈當然與智力分數的母群分佈相同。然而，當我們增加樣本數到2，任何一個樣本的平均值都比任何一個人的智力分數更接近母體平均值，因此直方圖（即取樣分佈）比母群分佈更窄。當樣本數提高到10（圖(c)），可以看到樣本平均值的分佈會緊密地聚集在真實的母體平均值周圍。\n\n\n\n\n至此希望同學對取樣分佈有充分的理解，特別是有關平均值的取樣分佈。在這一節，我想談談平均值的取樣分佈如何隨著樣本數而改變。以直觀來說，你應該已經知道一部分的答案。如果你只有幾個觀察值，樣本平均值可能相當不準確。假如你持續重做一個小樣本實驗並計算平均值，你會得到一個非常不同的答案。換句話說，取樣分佈的變異範圍非常寬。如果你持續重做一個大樣本實驗並計算樣本平均值，你有可能會得到和上次實驗一樣的結果，因此取樣分佈的變異範圍會非常窄。你可以在 Figure 8.8 看到這個直觀敘述的效果，由左到右的圖表顯示樣本數越大，取樣分佈越窄。我們可以通過計算取樣分佈的標準差來量化這種變化，統計學名詞為標準誤差(standard error)。統計學報告裡的標準誤差通常寫成SE，由於最常報告的標準誤差是樣本平均值的，因此我們通常使用縮寫SEM(standard error of the sample mean)。從 Figure 8.8 可以看出，隨著樣本數 \\(N\\) 的增加，SEM會減少。\n好的，走到這一節，我有一個到目前為止一直省略的部分。至此示範的模擬實驗都是基於”智力測驗分數”，是因為智力測驗分數大致呈現常態分佈，所以我假設母群的分佈也是常態。如果母群的分佈不是常態分佈，那麼樣本平均數的取樣分佈會變成什麼樣子？令初次學習的同學驚訝的是，無論母群分佈是什麼形狀，當樣本數 \\(N\\) 增加時，樣本平均數的取樣分佈會越來越像是常態分佈。為了讓同學了解，我進行了一些模擬。我們首先從 Figure 8.9 (a)，像“斜坡”的母群分佈開始。比較看起來像三角形的直方圖和黑色的鐘形曲線，同學可以看出母群分佈看起來根本不像常態分佈。接下來，我做了大量模擬的實驗。在每次實驗，我從母群隨機選取 \\(N=2\\) 個樣本，然後計算樣本平均值。Figure 8.9 (b) 繪製了這些樣本平均值的直方圖（即 \\(N=2\\) 時的樣本平均數的取樣分佈）。累積的直方圖近似 \\(\\chi^2\\) 分佈。雖然不是常態分佈，但是比起 Figure 8.9（a）的母群分佈更接近黑線。當我將樣本大小增加到 \\(N=4\\) 時，樣本平均數的取樣分佈就非常接近常態分佈（Figure 8.9（c）），當樣本數達到 \\(N=8\\) 時，它幾乎完全等於常態分佈。換句話說，只要你的樣本數不是太小，無論你的母群分佈長什麼樣子，樣本平均值的取樣分佈都會近似於常態分佈！\n\n\n\n\n\n\nFigure 8.9: 中央極限定理的視覺化示範。圖(a)是一個非常態分佈的母群分佈，而圖 (b) - (d) 展示從圖 (a) 取得樣本數分別為 2、4 和 8 的樣本平均值的取樣分佈。正如您所看到的，即使原始母群分佈不是常態分佈，隨著樣本數增加，樣本平均值的取樣分佈會趨近常態分佈。\n\n\n\n\n根據以上視覺化展示，關於樣本平均值的取樣分佈，我們可以得出以下結論：\n\n取樣分佈的平均值與母群的平均值相同。\n隨著樣本數的增加，取樣分佈的標準差（即標準誤）越來越小。\n隨著樣本數的增加，取樣分佈的形狀變得越來越接近常態分佈。\n\n中央極限定理（Central Limit Theorem）是統計學中的一個著名定理，除了證明上述所有說法都是正確的。中央極限定理也告訴我們，假設母群的平均值為\\(\\mu\\)，標準差為\\(\\sigma\\)，那麼樣本平均值的取樣分佈的平均值也是\\(\\mu\\)，而平均值的標準誤則是：\n\\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]\n因為是母群標準差\\(\\sigma\\)除以樣本大小 N 的平方根，因此SEM會隨著樣本數的增加而變小。中央極限定理還告訴我們，樣本平均值的取樣分佈形狀會變成常態分佈。5\n中央極限定理在處理各種問題都很有用。它告訴我們為什麼大樣本實驗比小樣本實驗更可靠，而且因為有標準誤的明確公式，所以它還告訴我們大樣本實驗的可靠性有多高。它也解釋了為什麼常態分佈是正常的。在真正的實驗中，我們想要測量的許多事物實際上是綜合各種不同數值指標的平均值（例如，智力測驗所測量的“普遍”智力是很多“具體”技能和能力的平均值），遇到這種情況時，各種指標的平均值應該遵循常態分佈。因為中央極限定理，常態分佈在各種真實數據隨處可見。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#估計母群參數",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#估計母群參數",
    "title": "8  從樣本估計未知量數",
    "section": "8.4 估計母群參數",
    "text": "8.4 估計母群參數\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.5 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 母群平均值\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 母群標準差\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.6 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on [Estimating population parameters], the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in [Estimating population parameters]. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).7\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "href": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "title": "9  Hypothesis testing",
    "section": "9.1 A menagerie of hypotheses",
    "text": "9.1 A menagerie of hypotheses\nEventually we all succumb to madness. For me, that day will arrive once I’m finally promoted to full professor. Safely ensconced in my ivory tower, happily protected by tenure, I will finally be able to take leave of my senses (so to speak) and indulge in that most thoroughly unproductive line of psychological research, the search for extrasensory perception (ESP).3\nLet’s suppose that this glorious day has come. My first study is a simple one in which I seek to test whether clairvoyance exists. Each participant sits down at a table and is shown a card by an experimenter. The card is black on one side and white on the other. The experimenter takes the card away and places it on a table in an adjacent room. The card is placed black side up or white side up completely at random, with the randomisation occurring only after the experimenter has left the room with the participant. A second experimenter comes in and asks the participant which side of the card is now facing upwards. It’s purely a one-shot experiment. Each person sees only one card and gives only one answer, and at no stage is the participant actually in contact with someone who knows the right answer. My data set, therefore, is very simple. I have asked the question of N people and some number \\(X\\) of these people have given the correct response. To make things concrete, let’s suppose that I have tested \\(N = 100\\) people and \\(X = 62\\) of these got the answer right. A surprisingly large number, sure, but is it large enough for me to feel safe in claiming I’ve found evidence for ESP? This is the situation where hypothesis testing comes in useful. However, before we talk about how to test hypotheses, we need to be clear about what we mean by hypotheses.\n\n9.1.1 Research hypotheses versus statistical hypotheses\nThe first distinction that you need to keep clear in your mind is between research hypotheses and statistical hypotheses. In my ESP study my overall scientific goal is to demonstrate that clairvoyance exists. In this situation I have a clear research goal: I am hoping to discover evidence for ESP. In other situations I might actually be a lot more neutral than that, so I might say that my research goal is to determine whether or not clairvoyance exists. Regardless of how I want to portray myself, the basic point that I’m trying to convey here is that a research hypothesis involves making a substantive, testable scientific claim. If you are a psychologist then your research hypotheses are fundamentally about psychological constructs. Any of the following would count as research hypotheses:\n\nListening to music reduces your ability to pay attention to other things. This is a claim about the causal relationship between two psychologically meaningful concepts (listening to music and paying attention to things), so it’s a perfectly reasonable research hypothesis.\nIntelligence is related to personality. Like the last one, this is a relational claim about two psychological constructs (intelligence and personality), but the claim is weaker: correlational not causal\nIntelligence is speed of information processing. This hypothesis has a quite different character. It’s not actually a relational claim at all. It’s an ontological claim about the fundamental character of intelligence (and I’m pretty sure this one actually. It’s usually easier to think about how to construct experiments to test research hypotheses of the form “does \\(X\\) affect \\(Y\\)?” than it is to address claims like “what is \\(X\\)?” And in practice what usually happens is that you find ways of testing relational claims that follow from your ontological ones. For instance, if I believe that intelligence is speed of information processing in the brain, my experiments will often involve looking for relationships between measures of intelligence and measures of speed. As a consequence most everyday research questions do tend to be relational in nature, but they’re almost always motivated by deeper ontological questions about the state of nature.\n\nNotice that in practice, my research hypotheses could overlap a lot. My ultimate goal in the ESP experiment might be to test an ontological claim like “ESP exists”, but I might operationally restrict myself to a narrower hypothesis like “Some people can ‘see’ objects in a clairvoyant fashion”. That said, there are some things that really don’t count as proper research hypotheses in any meaningful sense:\n\nLove is a battlefield. This is too vague to be testable. Whilst it’s okay for a research hypothesis to have a degree of vagueness to it, it has to be possible to operationalise your theoretical ideas. Maybe I’m just not creative enough to see it, but I can’t see how this can be converted into any concrete research design. If that’s true then this isn’t a scientific research hypothesis, it’s a pop song. That doesn’t mean it’s not interesting. A lot of deep questions that humans have fall into this category. Maybe one day science will be able to construct testable theories of love, or to test to see if God exists, and so on. But right now we can’t, and I wouldn’t bet on ever seeing a satisfying scientific approach to either.\nThe first rule of tautology club is the first rule of tautology club. This is not a substantive claim of any kind. It’s true by definition. No conceivable state of nature could possibly be inconsistent with this claim. We say that this is an unfalsifiable hypothesis, and as such it is outside the domain of science. Whatever else you do in science your claims must have the possibility of being wrong.\nMore people in my experiment will say “yes” than “no”. This one fails as a research hypothesis because it’s a claim about the data set, not about the psychology (unless of course your actual research question is whether people have some kind of “yes” bias!). Actually, this hypothesis is starting to sound more like a statistical hypothesis than a research hypothesis.\n\nAs you can see, research hypotheses can be somewhat messy at times and ultimately they are scientific claims. Statistical hypotheses are neither of these two things. Statistical hypotheses must be mathematically precise and they must correspond to specific claims about the characteristics of the data generating mechanism (i.e., the “population”). Even so, the intent is that statistical hypotheses bear a clear relationship to the substantive research hypotheses that you care about! For instance, in my ESP study my research hypothesis is that some people are able to see through walls or whatever. What I want to do is to “map” this onto a statement about how the data were generated. So let’s think about what that statement would be. The quantity that I’m interested in within the experiment is \\(P(correct)\\), the true-but-unknown probability with which the participants in my experiment answer the question correctly. Let’s use the Greek letter \\(\\theta\\) (theta) to refer to this probability. Here are four different statistical hypotheses:\n\nIf ESP doesn’t exist and if my experiment is well designed then my participants are just guessing. So I should expect them to get it right half of the time and so my statistical hypothesis is that the true probability of choosing correctly is \\(\\theta=0.5\\) .\nAlternatively, suppose ESP does exist and participants can see the card. If that’s true people will perform better than chance and the statistical hypothesis is that \\(\\theta > 0.5\\).\nA third possibility is that ESP does exist, but the colours are all reversed and people don’t realise it (okay, that’s wacky, but you never know). If that’s how it works then you’d expect people’s performance to be below chance. This would correspond to a statistical hypothesis that \\(\\theta < 0.5\\).\nFinally, suppose ESP exists but I have no idea whether people are seeing the right colour or the wrong one. In that case the only claim I could make about the data would be that the probability of making the correct answer is not equal to 0.5. This corresponds to the statistical hypothesis that \\(\\theta \\neq 0.5\\).\n\nAll of these are legitimate examples of a statistical hypothesis because they are statements about a population parameter and are meaningfully related to my experiment.\nWhat this discussion makes clear, I hope, is that when attempting to construct a statistical hypothesis test the researcher actually has two quite distinct hypotheses to consider. First, he or she has a research hypothesis (a claim about psychology), and this then corresponds to a statistical hypothesis (a claim about the data generating population). In my ESP example these might be as shown in Table 9.1.\n\n\n\n\nTable 9.1:  Research and statistical hypotheses \n\nDani's research hypothesis:\"ESP exists\"\n\nDani's statistical hypothesis:\\( \\theta \\neq 0.5 \\)\n\n\n\n\n\nAnd a key thing to recognise is this. A statistical hypothesis test is a test of the statistical hypothesis, not the research hypothesis. If your study is badly designed then the link between your research hypothesis and your statistical hypothesis is broken. To give a silly example, suppose that my ESP study was conducted in a situation where the participant can actually see the card reflected in a window. If that happens I would be able to find very strong evidence that \\(\\theta \\neq 0.5\\), but this would tell us nothing about whether “ESP exists”.\n\n\n9.1.2 Null hypotheses and alternative hypotheses\nSo far, so good. I have a research hypothesis that corresponds to what I want to believe about the world, and I can map it onto a statistical hypothesis that corresponds to what I want to believe about how the data were generated. It’s at this point that things get somewhat counter-intuitive for a lot of people. Because what I’m about to do is invent a new statistical hypothesis (the “null” hypothesis, \\(H_0\\) ) that corresponds to the exact opposite of what I want to believe, and then focus exclusively on that almost to the neglect of the thing I’m actually interested in (which is now called the “alternative” hypothesis, H1). In our ESP example, the null hypothesis is that \\(\\theta = 0.5\\), since that’s what we’d expect if ESP didn’t exist. My hope, of course, is that ESP is totally real and so the alternative to this null hypothesis is \\(\\theta \\neq 0.5\\). In essence, what we’re doing here is dividing up the possible values of \\(\\theta\\) into two groups: those values that I really hope aren’t true (the null), and those values that I’d be happy with if they turn out to be right (the alternative). Having done so, the important thing to recognise is that the goal of a hypothesis test is not to show that the alternative hypothesis is (probably) true. The goal is to show that the null hypothesis is (probably) false. Most people find this pretty weird.\nThe best way to think about it, in my experience, is to imagine that a hypothesis test is a criminal trial4, the trial of the null hypothesis. The null hypothesis is the defendant, the researcher is the prosecutor, and the statistical test itself is the judge. Just like a criminal trial, there is a presumption of innocence. The null hypothesis is deemed to be true unless you, the researcher, can prove beyond a reasonable doubt that it is false. You are free to design your experiment however you like (within reason, obviously!) and your goal when doing so is to maximise the chance that the data will yield a conviction for the crime of being false. The catch is that the statistical test sets the rules of the trial and those rules are designed to protect the null hypothesis, specifically to ensure that if the null hypothesis is actually true the chances of a false conviction are guaranteed to be low. This is pretty important. After all, the null hypothesis doesn’t get a lawyer, and given that the researcher is trying desperately to prove it to be false someone has to protect it."
  },
  {
    "objectID": "09-Hypothesis-testing.html#two-types-of-errors",
    "href": "09-Hypothesis-testing.html#two-types-of-errors",
    "title": "9  Hypothesis testing",
    "section": "9.2 Two types of errors",
    "text": "9.2 Two types of errors\nBefore going into details about how a statistical test is constructed it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky. For instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence for a conclusion that the coin is biased, but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we always have to accept that there’s a chance that we made a mistake. As a consequence the goal behind statistical hypothesis testing is not to eliminate errors, but to minimise them.\nAt this point, we need to be a bit more precise about what we mean by “errors”. First, let’s state the obvious. It is either the case that the null hypothesis is true or that it is false, and our test will either retain the null hypothesis or reject it.5 So, as Table 9.2 illustrates, after we run the test and make our choice one of four things might have happened:\n\n\n\n\nTable 9.2:  Null hypothesis statistical testing (NHST) \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\nAs a consequence there are actually two different types of error here. If we reject a null hypothesis that is actually true then we have made a type I error. On the other hand, if we retain the null hypothesis when it is in fact false then we have made a type II error.\nRemember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidential rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant, as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way. Punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same. The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted \\(\\alpha\\), is called the significance level of the test. And I’ll say it again, because it is so central to the whole set-up, a hypothesis test is said to have significance level \\(\\alpha\\) if the type I error rate is no larger than \\(\\alpha\\).\nSo, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by \\(\\beta\\). However, it’s much more common to refer to the power of the test, that is the probability with which we reject a null hypothesis when it really is false, which is \\(1 - \\beta\\). To help keep this straight, here’s the same table again but with the relevant numbers added (Table 9.3):\n\n\n\n\nTable 9.3:  Null hypothesis statistical testing (NHST) - additional detail \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\nA “powerful” hypothesis test is one that has a small value of \\(\\beta\\), while still keeping \\(\\alpha\\) fixed at some (small) desired level. By convention, scientists make use of three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) and \\(.001\\). Notice the asymmetry here; the tests are designed to ensure that the \\(\\alpha\\) level is kept small but there’s no corresponding guarantee regarding \\(\\beta\\). We’d certainly like the type II error rate to be small and we try to design tests that keep it small, but this is typically secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one”. To be honest, I don’t know that I agree with this philosophy. There are situations where I think it makes sense, and situations where I think it doesn’t, but that’s neither here nor there. It’s how the tests are built."
  },
  {
    "objectID": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "href": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "title": "9  Hypothesis testing",
    "section": "9.3 Test statistics and sampling distributions",
    "text": "9.3 Test statistics and sampling distributions\nAt this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the form of the data is that \\(X\\) out of \\(N\\) people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true, that ESP doesn’t exist and the true probability that anyone picks the correct colour is exactly \\(\\theta = 0.5\\). What would we expect the data to look like? Well, obviously we’d expect the proportion of people who make the correct response to be pretty close to \\(50\\%\\). Or, to phrase this in more mathematical terms, we’d say that \\(\\frac{X}{N}\\) is approximately \\(0.5\\). Of course, we wouldn’t expect this fraction to be exactly \\(0.5\\). If, for example, we tested \\(N = 100\\) people and \\(X = 53\\) of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if \\(X = 99\\) of our participants got the question right then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only \\(X = 3\\) people got the answer right we’d be similarly confident that the null was wrong. Let’s be a little more technical about this. We have a quantity \\(X\\) that we can calculate by looking at our data. After looking at the value of \\(X\\) we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a test statistic.\nHaving chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so we need to determine what the sampling distribution of the test statistic would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section 8.3.1. Why do we need this? Because this distribution tells us exactly what values of X our null hypothesis would lead us to expect. And, therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.\nHow do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter \\(\\theta\\) is just the overall probability that people respond correctly when asked the question, and our test statistic \\(X\\) is the count of the number of people who did so out of a sample size of N. We’ve seen a distribution like this before, in Section 7.4, and that’s exactly what the binomial distribution describes! So, to use the notation and terminology that I introduced in that section, we would say that the null hypothesis predicts that \\(X\\) is binomially distributed, which is written\n\\[X \\sim Binomial(\\theta,N)\\]\nSince the null hypothesis states that \\(\\theta = 0.5\\) and our experiment has \\(N = 100\\) people, we have the sampling distribution we need. This sampling distribution is plotted in Figure 9.1. No surprises really, the null hypothesis says that \\(X = 50\\) is the most likely outcome, and it says that we’re almost certain to see somewhere between \\(40\\) and \\(60\\) correct responses.\n\n\n\n\n\nFigure 9.1: The sampling distribution for our test statistic \\(X\\) when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is \\(\\theta = .5\\), the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60"
  },
  {
    "objectID": "09-Hypothesis-testing.html#making-decisions",
    "href": "09-Hypothesis-testing.html#making-decisions",
    "title": "9  Hypothesis testing",
    "section": "9.4 Making decisions",
    "text": "9.4 Making decisions\nOkay, we’re very close to being finished. We’ve constructed a test statistic \\((X)\\) and we chose this test statistic in such a way that we’re pretty confident that if \\(X\\) is close to \\(\\frac{N}{2}\\) then we should retain the null, and if not we should reject it. The question that remains is this. Exactly which values of the test statistic should we associate with the null hypothesis, and exactly which values go with the alternative hypothesis? In my ESP study, for example, I’ve observed a value of \\(X = 62\\). What decision should I make? Should I choose to believe the null hypothesis or the alternative hypothesis?\n\n9.4.1 Critical regions and critical values\nTo answer this question we need to introduce the concept of a critical region for the test statistic X. The critical region of the test corresponds to those values of X that would lead us to reject null hypothesis (which is why the critical region is also sometimes called the rejection region). How do we find this critical region? Well, let’s consider what we know:\n\n\\(X\\) should be very big or very small in order to reject the null hypothesis\nIf the null hypothesis is true, the sampling distribution of \\(X\\) is \\(Binomial(0.5, N)\\)\nIf \\(\\alpha = .05\\), the critical region must cover 5% of this sampling distribution.\n\nIt’s important to make sure you understand this last point. The critical region corresponds to those values of \\(X\\) for which we would reject the null hypothesis, and the sampling distribution in question describes the probability that we would obtain a particular value of \\(X\\) if the null hypothesis were actually true. Now, let’s suppose that we chose a critical region that covers \\(20\\%\\) of the sampling distribution, and suppose that the null hypothesis is actually true. What would be the probability of incorrectly rejecting the null? The answer is of course \\(20\\%\\). And, therefore, we would have built a test that had an α level of \\(0.2\\). If we want \\(\\alpha = .05\\), the critical region is only allowed to cover 5% of the sampling distribution of our test statistic.\nAs it turns out those three things uniquely solve the problem. Our critical region consists of the most extreme values, known as the tails of the distribution. This is illustrated in Figure 9.2. If we want \\(\\alpha = .05\\) then our critical regions correspond to \\(X \\leq 40\\) and \\(X \\geq 60\\).6 That is, if the number of people saying “true” is between 41 and 59, then we should retain the null hypothesis. If the number is between \\(0\\) to \\(40\\), or between \\(60\\) to \\(100\\), then we should reject the null hypothesis. The numbers \\(40\\) and \\(60\\) are often referred to as the critical values since they define the edges of the critical region\n\n\n\n\n\nFigure 9.2: The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of \\(\\alpha = .05\\). The plot shows the sampling distribution of \\(X\\) under the null hypothesis (i.e., same as Figure 9.1) . The grey bars correspond to those values of \\(X\\) for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of \\(X\\) for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both \\(\\theta < .5\\) and \\(\\theta > .5\\), the critical region covers both tails of the distribution. To ensure an \\(\\alpha\\) level of \\(.05\\), we need to ensure that each of the two regions encompasses \\(2.5\\%\\) of the sampling distribution\n\n\n\n\nAt this point, our hypothesis test is essentially complete:\n\nWe choose an α level (e.g., \\(\\alpha = .05\\));\nCome up with some test statistic (e.g., \\(X\\)) that does a good job (in some meaningful sense) of comparing \\(H_0\\) to \\(H_1\\);\nFigure out the sampling distribution of the test statistic on the assumption that the null hypothesis is true (in this case, binomial); and then\nCalculate the critical region that produces an appropriate α level (0-40 and 60-100).\n\nAll that we have to do now is calculate the value of the test statistic for the real data (e.g., X = 62) and then compare it to the critical values to make our decision. Since 62 is greater than the critical value of 60 we would reject the null hypothesis. Or, to phrase it slightly differently, we say that the test has produced a statistically significant result.\n\n\n9.4.2 A note on statistical “significance”\n\nLike other occult techniques of divination, the statistical method has a private jargon deliberately contrived to obscure its methods from non-practitioners.\n– Attributed to G. O. Ashley 7\n\nA very brief digression is in order at this point, regarding the word “significant”. The concept of statistical significance is actually a very simple one, but has a very unfortunate name. If the data allow us to reject the null hypothesis, we say that “the result is statistically significant”, which is often shortened to “the result is significant”. This terminology is rather old and dates back to a time when “significant” just meant something like “indicated”, rather than its modern meaning which is much closer to “important”. As a result, a lot of modern readers get very confused when they start learning statistics because they think that a “significant result” must be an important one. It doesn’t mean that at all. All that “statistically significant” means is that the data allowed us to reject a null hypothesis. Whether or not the result is actually important in the real world is a very different question, and depends on all sorts of other things.\n\n\n9.4.3 The difference between one sided and two sided tests\nThere’s one more thing I want to point out about the hypothesis test that I’ve just constructed. If we take a moment to think about the statistical hypotheses I’ve been using, \\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] we notice that the alternative hypothesis covers both the possibility that \\(\\theta < .5\\) and the possibility that \\(\\theta \\> .5.\\) This makes sense if I really think that ESP could produce either better-than chance performance or worse-than-chance performance (and there are some people who think that). In statistical language this is an example of a two-sided test. It’s called this because the alternative hypothesis covers the area on both “sides” of the null hypothesis, and as a consequence the critical region of the test covers both tails of the sampling distribution (2.5% on either side if α = .05), as illustrated earlier in Figure 9.2. However, that’s not the only possibility. I might only be willing to believe in ESP if it produces better than chance performance. If so, then my alternative hypothesis would only covers the possibility that \\(\\theta > .5\\), and as a consequence the null hypothesis now becomes \\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] When this happens, we have what’s called a one-sided test and the critical region only covers one tail of the sampling distribution. This is illustrated in Figure 9.3.\n\n\n\n\n\nFigure 9.3: The critical region for a one sided test. In this case, the alternative hypothesis is that \\(\\theta \\geq .5\\) so we would only reject the null hypothesis for large values of \\(X\\). As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper \\(5\\%\\) of the distribution. Contrast this to the two-sided version in Figure 9.2"
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "href": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "title": "9  假設檢定",
    "section": "9.5 統計檢定的p值",
    "text": "9.5 統計檢定的p值\n在某種意義上，我們已經完成假設檢定程序。我們已經建立一個統計檢定值，設定好如果虛無假設符合事實的取樣分佈，並為檢定結果決策設定棄卻域。然而，其實我還省略了一個最重要的數值 – p值。p 值有兩種不同的解釋版本，一種是由羅納德·費雪爵士(Sir Ronald Fisher)提出，另一種是由傑茲·尼曼 (Jerzy Neyman) 提出。兩種版本都是統計學家接受的解釋方法，雖然彼此反映非常不同的假設檢定思維途徑。大多數統計學教課書只會講費雪的版本，但我認為這有點可惜。我認為尼曼的版本更簡潔，更能反映虛無假設檢定的邏輯。當然，也許讀者會有不同意見，以下兩種版本都會介紹。我先從尼曼的版本開始說。\n\n\n9.5.1 運用p值做決策的簡單理由\n前面描述的假設檢定程序有一個問題，就是並沒有區分“剛好顯著”和“非常顯著”的結果。像是在我的ESP研究案例裡，所獲得的資料只是剛好落在棄卻域的邊緣，讓我我確實得到了一個顯著結果，但這個結論其實非常微妙。若是我另外進行了一項研究，\\(N=100\\) 參與者中有 \\(X=97\\) 人回答正確，顯然這個結果也是顯著的，但是顯著性的程度要大得多，沒有任何模糊空間。前面描述的程序都沒有區分這兩種情況，若是我採用慣例做法，只以 \\(\\alpha=.05\\) 做為我可以接受的型一錯誤率，那麼這兩個結果都是顯著的。\n這裡就是p值派上用場的地方了。為了容易理解其中的原理，讓我們想像對同一組資料做了好幾次假設檢定，但是每次設定不一樣的顯著水準 \\(\\alpha\\) 。對我所得到的 ESP 資料( \\(X=62\\) )進行好幾次檢定後，得到的結論大致如 Table 9.4 。\n\n\n\n\n\n\nTable 9.4:  以不同顯著水準執行假設檢定的結果 \n \n  \n    α值 \n    X0.05 \n    X0.04 \n    X0.03 \n    X0.02 \n    X0.01 \n  \n \n\n  \n    是否拒絕虛無假設 \n    是 \n    是 \n    是 \n    否 \n    否 \n  \n\n\n\n\n\n\n對我的ESP資料（100次觀察中有62次成功）進行幾次檢定後，以\\(\\alpha=.03\\)及以上的顯著水準決策，都是會拒絕虛無假設。以\\(\\alpha=.02\\)及以下的水準，都是是會保留虛無假設。因此，在\\(.02\\)和\\(.03\\)之間必定有一個最小的\\(\\alpha\\)值，讓我們可以拒絕虛無假設，這個值就是p值。最後我得到這筆ESP資料的p值為\\(.021\\)。簡而言之，p值被定義為如果你想要拒絕虛無假設的話，你必須願意容忍的最小型一錯誤率(\\(\\alpha\\))。\n如果p值顯示的決策錯誤率是大到你無法接受，那麼你必須保留虛無假設。如果你對等於p值的決策錯誤率感到滿意，那麼你可以拒絕虛無假設並支持偏好的對立假設。\n總而言之，p值是對以所有可能的顯著水準\\(\\alpha\\)，所執行的假設檢定結果總結。因此p值有”軟化”決策難度的效果。檢定結果的p值比 \\(\\alpha\\) 大的話，我們會拒絕虛無假設；而檢定結果的p值比 \\(\\alpha\\) 小的話，我們會保留虛無假設。由於我的 ESP 實驗結果是 \\(X = 62\\)，因此 p = .021，要宣稱人類有ESP的話，我必須容忍 \\(2.1%\\) 的型一錯誤率。另一方面，若是我的實驗結果是 \\(X = 97\\)，那麼p值會是多少？這次縮小為 \\(p = 1.36 \\times 10^{-25}\\) 8，這是一個非常、非常微小的型一錯誤率。對於第二個實驗結果，我會更有信心地拒絕虛無假設，因為我只需要 “願意” 容忍大約十分之一兆兆兆兆的型一錯誤率，我的結論會是正確的。\n\n\n\n\n9.5.2 獲得極端資料的機率\n第二種p值的定義來自羅納德·費雪爵士，大多數入門統計學的教科書採用這個定義解釋p值。留意一下設定棄卻域時，是不是對應到取樣分佈的尾部，也就是分佈所涵蓋的量數之極端值？這並不是巧合，幾乎所有「好的」檢定都有這種特徵（所謂「好」是指型二錯誤率 \\(\\beta\\) 被最小化）。這是因為好的棄卻域幾乎總是對應到虛無假設成立時最不可能觀察到的統計檢定值。如果這條規則成立，那麼我們可以定義p值是我們會觀察到這個檢定統計值，其極端程度至少達到我們實際上得到該統計值的機率。換句話說，如果根據虛無假設解釋資料的正確性非常的低，那麼虛無假設應該是錯誤的。\n\n\n\n9.5.3 常見的錯誤解讀\n好了，我們看到了兩種相當不同但是統計學家公認合理的解釋p值的定義，一種基於尼曼對假設檢定的想法，另一種基於費雪的可能性思考。不幸的是，還有第三種解釋，有些學生在初次學習統計學會遇到某些講師這樣介紹，但是完全錯誤的解讀。這種錯誤的定義方法是把p值稱為“虛無假設為真的機率”。這是一種直觀上很容易吸收的想法，但是有兩個關鍵錯誤。首先，虛無假設檢定是一種次數主義工具，而次數主義方法對機率的基本立場是，不允許研究者給虛無假設設定機率分佈。根據這種觀點，虛無假設要麼是真的，要麼不是真的，不可能存在“有\\(5%\\)的機率是真的”這種說法。其次，即使是貝氏方法這一派，雖然會允許研究者給假設設定機率分佈，p值也不會對應虛無假設為真的機率，這派主張與計算p值的數學原理是完全不一樣的。總之，儘管這種定義方式非常直覺，但是沒有任何公認的學術方法能充分證明p值是某種假設為真的機率。請同學千萬要謹慎分辨。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "href": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "title": "9  Hypothesis testing",
    "section": "9.6 Reporting the results of a hypothesis test",
    "text": "9.6 Reporting the results of a hypothesis test\nWhen writing up the results of a hypothesis test there’s usually several pieces of information that you need to report, but it varies a fair bit from test to test. Throughout the rest of the book I’ll spend a little time talking about how to report the results of different tests (see Section 10.1.9 for a particularly detailed example, so that you can get a feel for how it’s usually done. However, regardless of what test you’re doing, the one thing that you always have to do is say something about the \\(p\\) value and whether or not the outcome was significant.\nThe fact that you have to do this is unsurprising, it’s the whole point of doing the test. What might be surprising is the fact that there is some contention over exactly how you’re supposed to do it. Leaving aside those people who completely disagree with the entire framework underpinning null hypothesis testing, there’s a certain amount of tension that exists regarding whether or not to report the exact \\(p\\) value that you obtained, or if you should state only that \\(p < \\alpha\\) for a significance level that you chose in advance (e.g., \\(p < .05\\)).\n\n9.6.1 The issue\nTo see why this is an issue, the key thing to recognise is that p values are terribly convenient. In practice, the fact that we can compute a p value means that we don’t actually have to specify any \\(\\alpha\\) level at all in order to run the test. Instead, what you can do is calculate your p value and interpret it directly. If you get \\(p = .062\\), then it means that you’d have to be willing to tolerate a Type I error rate of \\(6.2\\%\\) to justify rejecting the null. If you personally find \\(6.2\\%\\) intolerable then you retain the null. Therefore, the argument goes, why don’t we just report the actual \\(p\\) value and let the reader make up their own minds about what an acceptable Type I error rate is? This approach has the big advantage of “softening” the decision making process. In fact, if you accept the Neyman definition of the p value, that’s the whole point of the p value. We no longer have a fixed significance level of \\(\\alpha = .05\\) as a bright line separating “accept” from “reject” decisions, and this removes the rather pathological problem of being forced to treat \\(p = .051\\) in a fundamentally different way to \\(p = .049\\).\nThis flexibility is both the advantage and the disadvantage to the \\(p\\) value. The reason why a lot of people don’t like the idea of reporting an exact \\(p\\) value is that it gives the researcher a bit too much freedom. In particular, it lets you change your mind about what error tolerance you’re willing to put up with after you look at the data. For instance, consider my ESP experiment. Suppose I ran my test and ended up with a \\(p\\) value of \\(.09\\). Should I accept or reject? Now, to be honest, I haven’t yet bothered to think about what level of Type I error I’m “really” willing to accept. I don’t have an opinion on that topic. But I do have an opinion about whether or not ESP exists, and I definitely have an opinion about whether my research should be published in a reputable scientific journal. And amazingly, now that I’ve looked at the data I’m starting to think that a \\(9\\%\\) error rate isn’t so bad, especially when compared to how annoying it would be to have to admit to the world that my experiment has failed. So, to avoid looking like I just made it up after the fact, I now say that my \\(\\alpha\\) is .1, with the argument that a \\(10\\%\\) type I error rate isn’t too bad and at that level my test is significant! I win.\nIn other words, the worry here is that I might have the best of intentions, and be the most honest of people, but the temptation to just “shade” things a little bit here and there is really, really strong. As anyone who has ever run an experiment can attest, it’s a long and difficult process and you often get very attached to your hypotheses. It’s hard to let go and admit the experiment didn’t find what you wanted it to find. And that’s the danger here. If we use the “raw” p-value, people will start interpreting the data in terms of what they want to believe, not what the data are actually saying and, if we allow that, why are we even bothering to do science at all? Why not let everyone believe whatever they like about anything, regardless of what the facts are? Okay, that’s a bit extreme, but that’s where the worry comes from. According to this view, you really must specify your \\(\\alpha\\) value in advance and then only report whether the test was significant or not. It’s the only way to keep ourselves honest\n\n\n\n\nTable 9.5:  Typical translations of p value levels \n\nUsual notationSignif. starsEnglish translationThe null is...\n\np > .05The test wasn't significantRetained\n\np < .05*The test was significant at \\( \\alpha \\) = .05 but not at \\( \\alpha \\) = .01 or \\( \\alpha \\) = .001.Rejected\n\np < .01**The test was significant at \\( \\alpha \\) = .05  and \\( \\alpha \\) = .01 but not at \\( \\alpha \\) = .001.Rejected\n\np < .001***The test was significant at all levelsRejected\n\n\n\n\n\n\n\n9.6.2 Two proposed solutions\nIn practice, it’s pretty rare for a researcher to specify a single α level ahead of time. Instead, the convention is that scientists rely on three standard significance levels: \\(.05\\), \\(.01\\) and \\(.001\\). When reporting your results, you indicate which (if any) of these significance levels allow you to reject the null hypothesis. This is summarised in Table 9.5. This allows us to soften the decision rule a little bit, since \\(p < .01\\) implies that the data meet a stronger evidential standard than \\(p < .05\\) would. Nevertheless, since these levels are fixed in advance by convention, it does prevent people choosing their α level after looking at the data\nNevertheless, quite a lot of people still prefer to report exact p values. To many people, the advantage of allowing the reader to make up their own mind about how to interpret p = .06 outweighs any disadvantages. In practice, however, even among those researchers who prefer exact p values it is quite common to just write \\(p < .001\\) instead of reporting an exact value for small p. This is in part because a lot of software doesn’t actually print out the p value when it’s that small (e.g., SPSS just writes \\(p = .000\\) whenever \\(p < .001\\)), and in part because a very small p value can be kind of misleading. The human mind sees a number like .0000000001 and it’s hard to suppress the gut feeling that the evidence in favour of the alternative hypothesis is a near certainty. In practice however, this is usually wrong. Life is a big, messy, complicated thing, and every statistical test ever invented relies on simplifications, approximations and assumptions. As a consequence, it’s probably not reasonable to walk away from any statistical analysis with a feeling of confidence stronger than \\(p < .001\\) implies. In other words, \\(p < .001\\) is really code for “as far as this test is concerned, the evidence is overwhelming.”\nIn light of all this, you might be wondering exactly what you should do. There’s a fair bit of contradictory advice on the topic, with some people arguing that you should report the exact p value, and other people arguing that you should use the tiered approach illustrated in Table 9.1. As a result, the best advice I can give is to suggest that you look at papers/reports written in your field and see what the convention seems to be. If there doesn’t seem to be any consistent pattern, then use whichever method you prefer."
  },
  {
    "objectID": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "href": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "title": "9  Hypothesis testing",
    "section": "9.7 Running the hypothesis test in practice",
    "text": "9.7 Running the hypothesis test in practice\nAt this point some of you might be wondering if this is a “real” hypothesis test, or just a toy example that I made up. It’s real. In the previous discussion I built the test from first principles, thinking that it was the simplest possible problem that you might ever encounter in real life. However, this test already exists. It’s called the binomial test, and it’s implemented by jamovi as one of the statistical analyses available when you hit the ‘Frequencies’ button. To test the null hypothesis that the response probability is one-half \\(p = .5\\),9 and using data in which \\(x =62\\) of \\(n = 100\\) people made the correct response, available in the binomialtest.omv data file, we get the results shown in Figure 9.4.\n\n\n\n\n\nFigure 9.4: Binomial test analysis and results in jamovi\n\n\n\n\nRight now, this output looks pretty unfamiliar to you, but you can see that it’s telling you more or less the right things. Specifically, the p-value of \\(0.02\\) is less than the usual choice of \\(\\alpha = .05\\), so you can reject the null. We’ll talk a lot more about how to read this sort of output as we go along, and after a while you’ll hopefully find it quite easy to read and understand."
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "href": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "title": "9  假設檢定",
    "section": "9.8 效果量、樣本量、考驗力",
    "text": "9.8 效果量、樣本量、考驗力\n在前面的章節，我(原作者)一再強調統計假設檢定背後的主要設計原則，那就是要儘可能有效控制型一錯誤率。當我們設定 \\(\\alpha = .05\\) 時，就要嘗試確保虛無假設主張成立的話，最多只有 \\(5%\\) 的機率被錯誤拒絕。然而，這並不是說我們不用關心型二錯誤。從研究者進行統計實務的角度來看，當虛無假設實際不成立，卻不能拒絕所造成的錯誤非常令人困擾。考慮這一點，假設檢定的次要目標就是要儘可能讓型二錯誤率 \\(\\beta\\) 維持在最低水準，儘管我們通常不會用「最小化型二錯誤」來形容這個目標。相反的，我們常用「最大化考驗力」這樣的描述。因為考驗力的定義是 \\(1-\\beta\\)，所以兩者意思是相同的。\n\n\n9.8.1 圖解考驗力\n\n\n\n\n\nFigure 9.5: 對立假設主張母群參數 \\(\\theta=0.55\\) 的取樣分佈。圖中藍色區域代表可合理拒絕虛無假設的棄卻域範圍。\n\n\n\n\n讓我們花一點時間來好好理解什麼是型二錯誤。當對立假設成立，但我們仍然無法拒絕虛無假設時，做出的決策錯誤就是型二錯誤(再看一下 Table 9.2 )。較理想的情況是我們可以設定一個代表型二錯誤率的數值 \\(\\beta\\)，就像我們可以將 \\(\\alpha = .05\\) 設定為型一錯誤率一樣。不幸的是，實際的做法非常棘手。仔細看一下ESP 研究案例的對立假設主張，其實對應許多可能的 \\(\\theta\\) 值，只要是除了 0.5 之外，每個 \\(\\theta\\) 值都可以支持對立假設。若是某位參與者 選擇正確答案的真實機率為 55%（也就是 \\(\\theta = .55\\)）。若是如此， \\(X\\) 的取樣分佈就不是虛無假設所預測的分佈，因為 \\(X\\) 的最有可能的數值是 \\(100\\) 次中有 \\(55\\) 次。不僅如此，整個取樣分佈也偏移了，如同 Figure 9.5 的展示。當然，棄絕域並不會改變。根據定義，棄卻域是根據虛無假設的主張所設定的。從 Figure 9.5 看到的是，當虛無假設不成立， \\(\\theta = .55\\) 的取樣分佈內有很高的比例屬於棄卻域。 的確，當虛無假設主張不成立時，拒絕虛無假設的機率相對更大！然而，\\(\\theta = .55\\) 並不是唯一符合對立假設的可能值。現在讓我們看看 \\(\\theta\\) 的真實數值是 \\(0.7\\)，取樣分佈會變成什麼樣子？如同 Figure 9.6 的展示，當 \\(\\theta = 0.7\\) 時，幾乎整個取樣分佈都落在棄卻域裡。因此，如果 \\(\\theta = 0.7\\)，我們正確拒絕虛無假設的機率（即檢定力）比 \\(\\theta = 0.55\\) 要大得多。簡而言之，雖然 \\(\\theta = .55\\) 和 \\(\\theta = .70\\) 都是支持對立假設的實驗結果，但型二錯誤率是不相等的。\n\n\n\n\n\n\nFigure 9.6: 對立假設主張母群參數 \\(\\theta=0.70\\) 的取樣分佈，幾乎整個分佈都在棄絕域內。\n\n\n\n\n這樣討論下來，我們知道假設檢定的考驗力（即\\(1-\\beta\\)）取決於 \\(\\theta\\) 的真實數值。為了清楚說明這一點，我已經算出所有 \\(\\theta\\) 值可拒絕虛無假設的預期機率，繪製出 Figure 9.7 。圖中的曲線通常稱為假設檢定的考驗力函數。這是一個能評估檢定方法品質的好工具，因為函數列出所有可能的 \\(\\theta\\) 值可達到的考驗力（\\(1-\\beta\\)）。正如圖中顯示，當 \\(\\theta\\) 的真實值越接近 \\(0.5\\) 時，檢定的考驗力會急劇降低，遠離 \\(0.5\\) 的話，考驗力就越大。\n\n\n\n\n\n\nFigure 9.7: 如果真正的\\(\\theta\\)值不同於虛無假設所主 張的值（\\(\\theta=0.5\\)），我們能拒絕虛無假設的機率，也就是檢定的考驗力。可以明顯看出，當\\(\\theta\\)與\\(\\theta=0.5\\)差異越大，假設檢定的考驗力更高（正確拒絕虛無假設的機率更高）。需要注意的是，當\\(\\theta\\)確實等於\\(0.5\\)時（以黑點表示），虛無假設實際上是成立的，此時拒絕虛無假設所犯的錯誤就是型一錯誤。\n\n\n\n\n\n\n9.8.2 估計考驗力的功用\n\nSince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned with mice when there are tigers abroad\n- George Box (Box 1976, p. 792)\n\nFigure 9.7 展示了我們能執行假設檢定的最基本條件 。如果實際狀態和虛無假設預測的狀態非常不同，那麼考驗力會非常高；但是如果實際狀態和虛無假設預測的非常相似（但不完全相同），那麼考驗力會非常低。所以，為了能讓科學家們能方便估計考驗力，統計學家發展出一些方法量化實際狀態與虛無假設之間的“相似程度”。這種統計方法就是效果量(effect size)的測量（例如 Cohen (1988); Ellis (2010)）。效果量在不同的研究中的定義可能略有不同（因此，本節僅討論一般性的定義），但是各種定義嘗試捕捉的質性想法始終一致（參考 Table 9.6 ）。其中一種就像本章討論的ESP案例：真實的母群參數數值與虛無假設的參數數值之間差異有多大？如果讓 \\(\\theta_0 = 0.5\\) 表示虛無假設的參數值， \\(\\theta\\) 表示母群參數的真實數值，那麼測量效果量的方式就是計算兩種數值之間的差異（即\\(\\theta - \\theta_0\\)），或者數值差異的絕對值，\\(abs(\\theta - \\theta_0)\\)。\n\n\n\n\n\nTable 9.6:  關於統計顯著性和效果量之間關係的簡單解讀指南。如果你沒有得到顯著的結果，基本上效果量多少就沒什麼意義，因為你沒有表明真的有效果的任何證據。另一方面，如果你得到一個顯著的效果，但是效果量很小，那麼你的研究結果（雖然是真實的）可能並不太值得注意。不過，由於這只是非常粗略的指南，結果有沒有意思主要取決於研究的具體內容。在某些情況下，即使效果量很小，仍可能具有重大的實用意義。因此，不要太過認真地看待這份指南，只是讓初學者有一個粗略的概念而已。 \n \n  \n    X. \n    效果量大 \n    效果量小 \n  \n \n\n  \n    有顯著結果 \n    確實發現差異，且有實用意義 \n    確實發現差異，但未必有實用意義 \n  \n  \n    無顯著結果 \n    未發現差異 \n    未發現差異 \n  \n\n\n\n\n\n\n為什麼要計算效果量？假設你已經進行實驗，收集完資料，進行假設檢定而得到了一個顯著性的結果。那麼你宣稱得到了一個明顯的效果，研究就結束嗎？這不就是假設檢定的功能嗎？當然不完全是。同學們現在應該會同意，進行假設檢定的目的是試圖證明虛無假設是錯誤的，但這並不是我們做研究唯一關注的事情。如果虛無假設主張\\(\\theta=0.5\\)，然後我們證實這是錯誤的，我們只是講完了一半的故事。拒絕虛無假設表示我們相信\\(\\theta \\neq 0.5\\)，但是\\(\\theta=0.51\\)和\\(\\theta=0.8\\)之間存在很大的差異。如果實驗結果是\\(\\theta=0.8\\)，那麼虛無假設不僅僅是錯了，還錯得非常離譜。另一方面，若是我們成功地拒絕虛無假設，但是\\(\\theta\\)的真實數值似乎只有0.51（這只有在規模非常大的研究中才可能發生）。這個情況的虛無假設當然是錯的，但是我們並不確定這個結果是否值得注意，因為效果量非常地小。以我創造的ESP研究案例來說，我們可能仍然感到興趣，因為證實人類真的有超能力是相當酷的事10，但是在大多數研究的狀況，即使結果顯示有\\(1%\\)的顯著差異，就算是真的有差異，通常也不是值得注意的發現。例如，假如我們今天研究了男女高中生的考試成績差異，結果發現女學生的平均成績比男生高\\(1%\\)。若是我有來自成千上萬名學生的資料，那麼這種差異幾乎肯定是有統計顯著性，但是無論p值有多小，這種差異都沒什麼意思。沒有人敢根據如此微小的差異，宣稱一個國家出現了青少年教育危機吧？正是因為這個原因，越來越多的研究者使用某種效果量的標準尺度，與假設檢定的結果並列報告。假設檢定只有告訴我們，應不應該相信所觀察到的效果是不是真實的（即不僅僅是偶然的），而效果量則透露是不是應該關心這種效果。\n\n\n\n9.8.3 增加研究考驗力的方案\n毫不意外地，現代科學家非常關心如何最大幅度提高實驗的考驗力。我們都希望實驗能夠成功，因此期望虛無假設不成立的話，能夠最大幅度地增加拒絕的機會（當然，我們通常希望虛無假設是錯誤的！）。正如前一節討論的，影響考驗力的因素之一是效果量。因此，增加考驗力的首選方案就是增加效果量。這表示在研究實務，我們所設計的研究能使效果量放到最大。以ESP研究為例，我也許認為超感官能力在安靜、昏暗的房間裡，且要儘量減少干擾，才能得到最佳實驗效果。因此，我會嘗試在這樣的環境裡進行實驗。如果我的實驗方式確實能增強人類的ESP能力，那麼\\(\\theta\\)的真實數值就會增加11，就能提高效果量。簡而言之，巧妙的實驗設計是提高考驗力的一種方式，因為實驗設計可以改變效果量。\n不幸的是，即使擁有最佳的實驗設計，大部分的研究只能得到微小效果。例如，超感官知覺也許確實存在，但是即使在最佳的設計條件測試，效果量也非常微弱。遇到這種情況，增加樣本量是增加考驗力的最佳方案。一般來說，收集越多觀察值，就越有可能區分兩種假設。比如說我找了10位參與者進行超感官知覺實驗，其中7人正確答出隱藏卡片的顏色，這樣的結果可能不會令人印象深刻。但是如果我找了10,000位參與者進行實驗，其中7,000人猜對了，這樣的結果更有可能讓人覺得是件大事。換句話說，樣本量增加，考驗力也會增加。Figure 9.8 展示了兩者的關係，對於參數真實數值 \\(\\theta = 0.7\\) ，該圖顯示從1到100的所有樣本量\\(N\\)的檢定力，其中虛無假設預測 \\(\\theta_0 = 0.5\\) 。\n\n\n\n\n\n\n\nFigure 9.8: ESP研究的二項式檢定考驗力與樣本量 \\(N\\) 函式關係視覺化。此圖的繪製條件還有 \\(\\theta\\) 的真實數值為0.7，但是虛無假設主張 \\(\\theta = 0.5\\) 。總體來說，樣本數 \\(N\\) 越大，考驗力越高。(註：曲線裡的鋸齒是因為 \\(\\theta\\) ， \\(\\alpha\\) ，以及二項分佈是間斷機率分佈的本質等條件，交互運作而造成的，不過整體來說不影響我想表達的目的。)\n\n\n\n\n因為考驗力很重要，規劃一個實驗的執行細節時，知道可能有多少考驗力會非常有用。當然，因為無法確定真正的效果量有多少，我們無法得知考驗力會有多高，但是有時候我們可以評估考驗力應該要有多大。能做到的話，我們就可以評估需要多大的樣本量！這種方案叫做考驗力分析，做得好的話，對於設計實驗會非常有用。有用的考驗力分析結果包括能成功執行實驗所需要的時間或金錢是否足夠等資訊。現在越來越多的研究者同意，考驗力分析應該成為實驗設計的必要部分，因此值得好好了解如何執行。然而，本書沒有打算介紹考驗力分析。有部分理由是講出來很無聊，還有部分是實質性的理由。無聊的理由是原作者還沒有時間去寫關於考驗力分析的介紹。實質理由是原作者對於考驗力分析的方法還有些懷疑的地方。作為一位研究者，我幾乎沒做過考驗力分析。我認為可能是因為 (a) 我的實驗經常有些非常規條件 ，不曉得如何正確地定義效果量，還有 (b) 我對研究結果的效果量實在是一無所知，不知道如何解釋分析結果。此外，我常與一位擔任統計諮詢師廣泛交流（其實就是我老婆），她實際接過的案子裡，只有要撰寫申請補助經費的計畫書的客戶才會詢問怎麼做考驗力分析。換句話說，在現實的研究場景，科學家們似乎只有被特別要求時才需要進行考驗力分析，而且考驗力分析並不是日常科研工作的一部分。簡而言之，我同意考驗力是一個重要的概念，但是考驗力分析並不像人們所說的那麼有用，除非在少數情況。像是（a）已經有方法能計算你要執行的實驗設計考驗力，（b）你對要測量的效果量可能大小有很好的想法12。也許其他研究者比我幸運，但我個人從未遇到過(a)和(b)都成立的情況。未來我的想法也許會改變，這本書的更新版本就會有章節介紹詳細考驗力分析，但是以目前來說，這是我編寫這個主題最心安理得的處理方式。13"
  },
  {
    "objectID": "09-Hypothesis-testing.html#some-issues-to-consider",
    "href": "09-Hypothesis-testing.html#some-issues-to-consider",
    "title": "9  Hypothesis testing",
    "section": "9.9 Some issues to consider",
    "text": "9.9 Some issues to consider\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n9.9.1 Neyman versus Fisher\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually13 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n9.9.2 Bayesians versus frequentists\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 7) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in Chapter 16, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n9.9.3 Traps\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,14 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#summary",
    "href": "09-Hypothesis-testing.html#summary",
    "title": "9  Hypothesis testing",
    "section": "9.10 Summary",
    "text": "9.10 Summary\nNull hypothesis testing is one of the most ubiquitous elements to statistical theory. The vast majority of scientific papers report the results of some hypothesis test or another. As a consequence it is almost impossible to get by in science without having at least a cursory understanding of what a p-value means, making this one of the most important chapters in the book. As usual, I’ll end the chapter with a quick recap of the key ideas that we’ve talked about:\n\nA menagerie of hypotheses. Research hypotheses and statistical hypotheses. Null and alternative hypotheses.\nTwo types of errors. Type I and Type II.\nTest statistics and sampling distributions.\nHypothesis testing for Making decisions\nThe p value of a test. p-values as “soft” decisions\nReporting the results of a hypothesis test\nRunning the hypothesis test in practice\nEffect size, sample size and power\nSome issues to consider regarding hypothesis testing\n\nLater in the book, in Chapter 16, I’ll revisit the theory of null hypothesis tests from a Bayesian perspective and introduce a number of new tools that you can use if you aren’t particularly fond of the orthodox approach. But for now, though, we’re done with the abstract statistical theory, and we can start discussing specific data analysis tools.\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results. Cambridge University Press.\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60, 328–331.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation of classical statistics. Springer."
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯表達功能",
    "href": "06-Pragmatic-matters.html#邏輯表達功能",
    "title": "6  實務課題",
    "section": "6.2 邏輯表達功能",
    "text": "6.2 邏輯表達功能\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 判斷算式真假值\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 邏輯運算子\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 在報告中表達邏輯運算子\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#其他常用數學函式及運算子",
    "href": "06-Pragmatic-matters.html#其他常用數學函式及運算子",
    "title": "6  實務課題",
    "section": "6.4 其他常用數學函式及運算子",
    "text": "6.4 其他常用數學函式及運算子\nIn the section on [Transforming and recoding a variable] I discussed the ideas behind variable transformations and showed that a lot of the transformations that you might want to apply to your data are based on fairly simple mathematical functions and operations. In this section I want to return to that discussion and mention several other mathematical functions and arithmetic operations that are actually quite useful for a lot of real world data analysis. Table 6.5 gives a brief overview of the various mathematical functions I want to talk about here, or later.9 Obviously this doesn’t even come close to cataloguing the range of possibilities available, but it does cover a range of functions that are used regularly in data analysis and that are available in jamovi.\n\n6.4.1 對數與指數\nAs I’ve mentioned earlier, jamovi has an useful range of mathematical functions built into it and there really wouldn’t be much point in trying to describe or even list all of them. For the most part, I’ve focused only on those functions that are strictly necessary for this book. However I do want to make an exception for logarithms and exponentials. Although they aren’t needed anywhere else in this book, they are everywhere in statistics more broadly. And not only that, there are a lot of situations in which it is convenient to analyse the logarithm of a variable (i.e., to take a “log-transform” of the variable). I suspect that many (maybe most) readers of this book will have encountered logarithms and exponentials before, but from past experience I know that there’s a substantial proportion of students who take a social science statistics class who haven’t touched logarithms since high school, and would appreciate a bit of a refresher.\nIn order to understand logarithms and exponentials, the easiest thing to do is to actually calculate them and see how they relate to other simple calculations. There are three jamovi functions in particular that I want to talk about, namely LN(), LOG10() and EXP(). To start with, let’s consider LOG10(), which is known as the “logarithm in base 10”. The trick to understanding a logarithm is to understand that it’s basically the “opposite” of taking a power. Specifically, the logarithm in base 10 is closely related to the powers of 10. So let’s start by noting that 10-cubed is 1000. Mathematically, we would write this:\n\\[10^3=1000\\]\nThe trick to understanding a logarithm is to recognise that the statement that “10 to the power of 3 is equal to 1000” is equivalent to the statement that “the logarithm (in base 10) of 1000 is equal to 3”. Mathematically, we write this as follows,\n\\[log_{10}(1000)=3\\]\nOkay, since the LOG10() function is related to the powers of 10, you might expect that there are other logarithms (in bases other than 10) that are related to other powers too. And of course that’s true: there’s not really anything mathematically special about the number 10. You and I happen to find it useful because decimal numbers are built around the number 10, but the big bad world of mathematics scoffs at our decimal numbers. Sadly, the universe doesn’t actually care how we write down numbers. Anyway, the consequence of this cosmic indifference is that there’s nothing particularly special about calculating logarithms in base 10. You could, for instance, calculate your logarithms in base 2. Alternatively, a third type of logarithm, and one we see a lot more of in statistics than either base 10 or base 2, is called the natural logarithm, and corresponds to the logarithm in base e. Since you might one day run into it, I’d better explain what e is. The number e, known as Euler’s number, is one of those annoying “irrational” numbers whose decimal expansion is infinitely long, and is considered one of the most important numbers in mathematics. The first few digits of e are:\n\\[e = 2.718282 \\]\nThere are quite a few situation in statistics that require us to calculate powers of \\(e\\), though none of them appear in this book. Raising e to the power \\(x\\) is called the exponential of \\(x\\), and so it’s very common to see \\(e^x\\) written as exppxq. And so it’s no surprise that jamovi has a function that calculates exponentials, called EXP(). Because the number e crops up so often in statistics, the natural logarithm (i.e., logarithm in base e) also tends to turn up. Mathematicians often write it as \\(log_e(x)\\) or \\(ln(x)\\). In fact, jamovi works the same way: the LN() function corresponds to the natural logarithm.\nAnd with that, I think we’ve had quite enough exponentials and logarithms for this book!"
  },
  {
    "objectID": "06-Pragmatic-matters.html#篩選部分資料",
    "href": "06-Pragmatic-matters.html#篩選部分資料",
    "title": "6  實務課題",
    "section": "6.5 篩選部分資料",
    "text": "6.5 篩選部分資料\nOne very important kind of data handling is being able to extract a particular subset of the data. For instance, you might be interested only in analysing the data from one experimental condition, or you may want to look closely at the data from people over 50 years in age. To do this, the first step is getting jamovi to filter the subset of the data corresponding to the observations that you’re interested in.\nThis section returns to the nightgarden.csv data set. If you’re reading this whole chapter in one sitting, then you should already have this data set loaded into a jamovi window. For this section, let’s focus on the two variables speaker and utterance (see [Tabulating and cross-tabulating data]) if you’ve forgotten what those variables look like). Suppose that what I want to do is pull out only those utterances that were made by Makka-Pakka. To that end, we need to specify a filter in jamovi. First open up a filter window by clicking on ‘Filters’ on the main jamovi ‘Data’ toolbar. Then, in the ‘Filter 1’ text box, next to the ‘=’ sign, type the following:\nspeaker == ‘makka-pakka’\n\n\n\n\n\nFigure 6.10: Creating a subset of the nightgarden data using the jamovi ‘Filters’ option\n\n\n\n\nWhen you have done this, you will see that a new column has been added to the spreadsheet window (see Figure 6.10), labelled ‘Filter 1’, with the cases where speaker is not ‘makka-pakka’ greyed-out (i.e., filtered out) and, conversely, the cases where speaker is ‘makka-pakka’ have a green check mark indicating they are filtered in. You can test this by running ‘Exploration’ - ‘Descriptives’ - ‘Frequency tables’ for the speaker variable and seeing what that shows. Go on, try it!\nFollowing on from this simple example, you can also build up more complex filters using logical expressions in jamovi. For instance, suppose I wanted to keep only those cases when the utterance is either “pip” or “oo”. In this case in the ‘Filter 1’ text box, next to the ‘=’ sign, you would type the following:\nutterance == ‘pip’ or utterance == ‘oo’"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html",
    "title": "8  從樣本估計未知量數",
    "section": "",
    "text": "At the start of the last chapter I highlighted the critical distinction between descriptive statistics and inferential statistics. As discussed in Chapter 4, the role of descriptive statistics is to concisely summarise what we do know. In contrast, the purpose of inferential statistics is to “learn what we do not know from what we do”. Now that we have a foundation in probability theory we are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two “big ideas”: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I’m going to witter on about sampling theory first because estimation theory doesn’t make sense until you understand sampling. As a consequence, this chapter divides naturally into two parts, the first three sections are focused on sampling theory, and the last two sections make use of sampling theory to discuss how statisticians think about estimation."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#母群參數的點估計",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#母群參數的點估計",
    "title": "8  運用樣本估計未知量數",
    "section": "8.4 母群參數的點估計",
    "text": "8.4 母群參數的點估計\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.6 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 母群平均值\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 母群標準差\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.7 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on [Estimating population parameters], the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in [Estimating population parameters]. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).8\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#本章小結",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#本章小結",
    "title": "8  運用樣本估計未知量數",
    "section": "8.6 本章小結",
    "text": "8.6 本章小結\n這一章涵蓋兩大主題。前半部談取樣理論，後半部談運用取樣理論估計母群參數的方法。兩大主題分為幾個小主題：\n\n有關樣本、母群、取樣的基本概念\n取樣的機率理論：大數法則、樣本分佈與中央極限定理\n母群參數的點估計討論平均值及標準差的機率意義。\n母群參數的區間估計\n\n還有許多取樣與估計的主題尚未談到，本章對於心理學及統計的初學者應該是能夠理解且能吸收的。而且應用取向的心理學者不大需要深入了解統計理論。唯一你可能需要了解但本章未探討的問題，是分析的資料不是來自隨機樣本的狀況。其實已經有許多處理非隨機樣本的統計理論，不過已經超出本書範圍了。\n\n\n\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan; Company.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard University Press."
  },
  {
    "objectID": "09-Hypothesis-testing.html",
    "href": "09-Hypothesis-testing.html",
    "title": "9  假設檢定",
    "section": "",
    "text": "The process of induction is the process of assuming the simplest law that can be made to harmonize with our experience. This process, however, has no logical foundation but only a psychological one. It is clear that there are no grounds for believing that the simplest course of events will really happen. It is an hypothesis that the sun will rise tomorrow: and this means that we do not know whether it will rise.\n– Ludwig Wittgenstein 1\nIn the last chapter I discussed the ideas behind estimation, which is one of the two “big ideas” in inferential statistics. It’s now time to turn our attention to the other big idea, which is hypothesis testing. In its most abstract form, hypothesis testing is really a very simple idea. The researcher has some theory about the world and wants to determine whether or not the data actually support that theory. However, the details are messy and most people find the theory of hypothesis testing to be the most frustrating part of statistics. The structure of the chapter is as follows. First, I’ll describe how hypothesis testing works in a fair amount of detail, using a simple running example to show you how a hypothesis test is “built”. I’ll try to avoid being too dogmatic while doing so, and focus instead on the underlying logic of the testing procedure.2 Afterwards, I’ll spend a bit of time talking about the various dogmas, rules and heresies that surround the theory of hypothesis testing."
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設大觀園",
    "href": "09-Hypothesis-testing.html#假設大觀園",
    "title": "9  假設檢定",
    "section": "9.1 假設大觀園",
    "text": "9.1 假設大觀園\n我們從一個原作者虛構的狂想開始談吧：我認為最終，人類都會屈服於瘋狂。我真正想做的研究，將在我升任正教授的時候開始。那一天到來時，我在象牙塔中會受到終身職的保障，我總算能夠拋棄理智，完全投入那個最沒有成效的心理研究領域：證實人類有超感官知覺（ESP）。3\n假如這一天終於來臨了，我的第一個研究是一項簡單的測試透視力實驗。每個參與者坐在桌子前，由實驗者向他展示一張卡片。這張卡片的一面是黑色的，另一面是白色的。實驗者把卡片拿走後，將它放在隔壁房間的桌子上。在實驗者與參與者離開房間後，卡片將被第二位實驗者隨機地翻到黑面或白面朝上，然後第二位實驗者進入房間，詢問參與者現在卡片的哪一面朝上。這只是一次性的實驗。每個人只會看到一張卡片，只回答一個問題，而參與者在任何時候都沒有與知道正確答案的人接觸。因此，我的資料紀錄非常簡單：我問了N個人的問題，其中有\\(X\\)個人給出了正確的答案。為了具體描述，假如我測試了100個人，其中有62個人回答正確。這會是一個令人驚訝的大數字，但是這個數字是否足夠讓我聲稱發現了ESP的證據呢？這就是檢驗假設有無效用的情況。然而，在我們談論如何檢驗假設之前，我們需要明白假設是什麼。\n\n\n\n9.1.1 研究假設還是統計假設\n首先需要清楚區別的是研究假設和統計假設之間的差別。在我設想的ESP研究中，我的整體科學目標是證實人類有透視能力。在這樣的場景，我有一個清晰的研究目標：我希望發現ESP的證據。在其他場景，我的想法可能會比較中立，也就是我可能會說我的研究目標是確定人類是否有透視能力。無論如何描述我的目標，我想傳達給各位同學的基本觀點是，制定研究假設是提出一個實際的、可測試的科學主張。如果你是一名心理學家，那麼你的研究假設基本上是關於心理學構念的。以下任何一種案例都可說是研究假設：\n\n聆聽音樂會降低你對其他事物的注意力能力。這是一種關於兩個有心理學意義的構念之間有因果關係的主張（聆聽音樂和注意力），因此這是一個非常合理的研究假設。\n智力與個性有關。和前一個一樣，這個假設主張兩個有心理學意義的構念之間存在關係性（智力和個性），但這個主張立埸比較弱：探討相關性而不是因果關係。\n智力是訊息處理速度。這個假設與前面兩個很不一樣。實際上這並不是一個因果關係或關聯性的假設，而是關於智力基本特性的本體論主張（我相當確定是這樣的）。通常來說，設計實驗測試像是“\\(X\\)是否影響\\(Y\\)？”，比回答“\\(X\\)是什麼？”這樣的問題要容易得多。在實際情況通常是你會找到方法，測試基本特性所形成的關聯性假設。例如，如果我相信智力的本質是大腦中訊息處理速度，我就會設計實驗探討智力和訊息處理之間的關係。因此，大多數日常生活中想到的研究問題雖然都與本質有關，但是通常是基於好奇關於自然界本體論問題的更深層動機。\n\n請注意在真實的實驗室，我會設定幾個互相重疊的研究假設。儘管我設計ESP實驗的終極目標是測試“人類有ESP”這樣的本體論主張，但是實際操作會限制自己只測試目標更狹窄的假設，像是“某些人可以用透視‘看見’物體”。話雖如此，有一些看似目標明確的主張，在任何意義上都不算是合適的研究假設：\n\n愛情就像戰場。這個假設過於模糊，無法進行測試。雖然研究假設可以有一定程度的模糊性，但是必須能夠將理論觀念具體化。也許我不夠有創造力，想不到能將這個假設轉化為具體研究設計的方式。如果真的有辦法，那麼應該不是科學的研究假設，而是一首流行歌曲。這並不是說這樣的假設不有趣，而是要指出許多人能想到的深刻問題都是屬於這種類別。也許有一天科學家能夠構建關於愛情的可測試理論，或者測試上帝是否存在等等。但是現在的我們還做不到，我不會指望看到一個令人滿意的科學方法來解決這些問題。\n套套邏輯俱樂部的第一條規則就是套套邏輯俱樂部的第一條規則。這是不具備任何實質意義的主張，儘管形式上是符合邏輯的。因為在任何自然狀態都不能提出與此主張相反的看法，我們會說這是一個不可證偽的假設，因此這樣的主張不屬於科學研究的領域。在科學研究，無論你想研究的問題是什麼，你提出的主張都必須有可能是錯誤的。\n在我的實驗裡，較多的參與者會說‘是’，而不是‘否’。這並不是一個有意義的研究假設，因為重點的是資料本身而非心理學問題（當然，除非真正要研究的問題是關於多數人是不是有回答“是”的偏好！）。實際上，這個假設看起來更像是一個統計假設而非研究假設。\n\n正如同學所見，有的研究假設的主張可能會有些混亂，不過都是各樣科學主張的一種。統計假設則不是一種主張。統計假設必須具有數學精確性，並且必須對資料的生成機制（也就是“母群”）的特徵提出具體的條件。即便如此，統計假設的內在意圖必須有與真正的研究假設有一個明確的關係！例如，在我的ESP研究中，我的研究假設是有些人能夠透視牆壁看到隔壁的物體。我要做的是將這樣的研究假設對應到產生資料的方法陳述。因此，現在我們考慮一下要如何表達這樣的陳述。我感興趣的實驗數值是 \\(P(correct)\\)，也就是實驗參與者正確回答問題的理論上為真但未知之機率。讓我們使用希臘字母\\(\\theta\\)（theta）來表示這項機率。以下是四種不同的統計假設：\n\n如果ESP不存在，而我的實驗設計沒有偏誤，那麼參與者的回答只是猜測。因此，我應該期望有一半參與者的回答是正確的，所以我的統計假設是，回答正確的理論機率是 \\(\\theta=0.5\\)。\n或者，假設ESP存在並且參與者真的能夠看到卡片。如果實驗結果真的是這樣，參與者回答的正確率會高於只是猜測，所以統計假設是 \\(\\theta > 0.5\\)。\n第三種可能是ESP確實存在，但是參與者並沒有意識到透視看到的物體顏色是相反的（好吧，這有些荒謬，但我們永遠無法知道）。如果是這樣的實驗結果，我會期望參與者回答的正確率會低於只是猜測。所以統計假設是 \\(\\theta < 0.5\\)。\n最後，如果人類確實有ESP，但是我不知道參與者是否看到了正確的顏色。在這種情況下，我只能期望參與者回答的正確率不等於0.5。所以統計假設是 \\(\\theta \\neq 0.5\\)。\n\n以上例子都是合乎科學研究目標的統計假設，因為每條陳述都有定義母群參數，並且緊密扣連我的實驗目的。\n我希望這些例子可以讓同學清楚了解，當研究者要構建一個統計假設檢定程序時，實際上要考慮兩種不同層次的假設。首先，研究者要有一個研究假設（關於心理學的主張），能對應到一個統計假設（關於數據生成母群的主張）。以我的ESP實驗來說，會像 Table 9.1 的表達。\n\n\n\n\n\n\nTable 9.1:  原作者狂想的研究假設與統計假設 \n \n  \n    研究假設 \n    人類有超感官知覺 \n  \n \n\n  \n    統計假設 \n     \n\n  \n\n\n\n\n\n\n小結一下兩種假設的差別。統計假設檢定的測試對象是統計假設，而非研究假設。假如你的研究設計不良，會造成研究假設和統計假設之間的斷裂。舉個有點荒謬的狀況，要是我的ESP研究是在參與者可以從窗戶反光看到卡片的環境裡進行，那麼我肯定能得到非常強的證據證明 \\(\\theta \\neq 0.5\\)，但是這並不能告訴我們”人類真的有ESP”。\n\n\n\n9.1.2 虛無假設與對立假設\n到目前為止還算順利。我有一個研究假設，對應我想相信的世界，還有映射到一個對應於資料生成方式的統計假設。接下來我要創造一個新的統計假設（“虛無假設”，\\(H_0\\)），這對很多人來說有些違反直覺。因為”虛無假設”對應與我想相信的事情完全相反，然後專注於驗證這條統計假設，並且忽略實際關心的事情（現在被稱為”對立假設”，\\(H_1\\)）。在我的ESP研究裡，虛無假設是 \\(\\theta = 0.5\\)，因為如果人類沒有ESP，我會期望看到這個結果。當然，我期望ESP是真的，所以這個虛無假設對立的假設就是 \\(\\theta \\neq 0.5\\)。實際上，我們是在將 \\(\\theta\\) 涵括的可能數值分成兩類：我真心期望不是真的那些數值（虛無假設），以及如果ESP被證實是存在的，我會很高興的那些數值（對立假設）。完成這些設定之後，需要意識到的關鍵是，假設檢定的真正目標不是證實對立假設（可能）是真的，而是證實虛無假設（可能）是假的。很多初學的同學會覺得這樣的邏輯很奇怪。\n就我的學習經驗，最好的比喻是把假設檢定當作刑事法庭審判4。虛無假設就是被告，研究者就像檢察官，統計檢定程序是法官。像真正的刑事審判程序一樣，一開始我們要以無罪推定原則看待虛無假設，也就是說它的主張應被認為是真實的，直到位研究者能夠明確證實它的主張是錯的。研究者可以憑自由意志設計實驗（當然也要合理），目的就是要用可能性最大的資料證實虛無假設是錯的。然而，統計檢定(法官)設定了審判的規則，而這些規則是為了保護虛無假設而設計的，這些規則是要確保如果虛無假設的主張確實是真的，讓法官誤判的機會保持在很低的水準。這非常重要，畢竟虛無假設沒有律師幫忙辨護，而研究者卻在拼命地嘗試證實它的主張是錯的，所以必須有一方提供虛無假設一些保護。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#兩種決策失誤型態",
    "href": "09-Hypothesis-testing.html#兩種決策失誤型態",
    "title": "9  假設檢定",
    "section": "9.2 兩種決策失誤型態",
    "text": "9.2 兩種決策失誤型態\nBefore going into details about how a statistical test is constructed it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky. For instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence for a conclusion that the coin is biased, but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we always have to accept that there’s a chance that we made a mistake. As a consequence the goal behind statistical hypothesis testing is not to eliminate errors, but to minimise them.\nAt this point, we need to be a bit more precise about what we mean by “errors”. First, let’s state the obvious. It is either the case that the null hypothesis is true or that it is false, and our test will either retain the null hypothesis or reject it.5 So, as Table 9.2 illustrates, after we run the test and make our choice one of four things might have happened:\n\n\n\n\nTable 9.2:  Null hypothesis statistical testing (NHST) \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\nAs a consequence there are actually two different types of error here. If we reject a null hypothesis that is actually true then we have made a type I error. On the other hand, if we retain the null hypothesis when it is in fact false then we have made a type II error.\nRemember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidential rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant, as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way. Punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same. The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted \\(\\alpha\\), is called the significance level of the test. And I’ll say it again, because it is so central to the whole set-up, a hypothesis test is said to have significance level \\(\\alpha\\) if the type I error rate is no larger than \\(\\alpha\\).\nSo, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by \\(\\beta\\). However, it’s much more common to refer to the power of the test, that is the probability with which we reject a null hypothesis when it really is false, which is \\(1 - \\beta\\). To help keep this straight, here’s the same table again but with the relevant numbers added (Table 9.3):\n\n\n\n\nTable 9.3:  Null hypothesis statistical testing (NHST) - additional detail \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\nA “powerful” hypothesis test is one that has a small value of \\(\\beta\\), while still keeping \\(\\alpha\\) fixed at some (small) desired level. By convention, scientists make use of three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) and \\(.001\\). Notice the asymmetry here; the tests are designed to ensure that the \\(\\alpha\\) level is kept small but there’s no corresponding guarantee regarding \\(\\beta\\). We’d certainly like the type II error rate to be small and we try to design tests that keep it small, but this is typically secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one”. To be honest, I don’t know that I agree with this philosophy. There are situations where I think it makes sense, and situations where I think it doesn’t, but that’s neither here nor there. It’s how the tests are built."
  },
  {
    "objectID": "09-Hypothesis-testing.html#運用樣本分佈檢測統計值",
    "href": "09-Hypothesis-testing.html#運用樣本分佈檢測統計值",
    "title": "9  假設檢定",
    "section": "9.3 運用樣本分佈檢測統計值",
    "text": "9.3 運用樣本分佈檢測統計值\n現在我們需要開始討論如何建立一個假設檢定的具體步驟。為此，讓我們回到剛才的靈異感知（ESP）的例子。暫且忽略我們實際獲得的資料，只談談實驗的結構。無論數據是什麼，資料的形式都是 \\(N\\) 個人當中有 \\(X\\) 個人正確地辨認出了隱藏卡牌的顏色。此外，假設虛無假設（null hypothesis）確實是真的，也就是說靈異感知不存在，每個人正確辨認出卡牌顏色的真實機率 \\(\\theta\\) 等於 \\(0.5\\)。在這種情況下，我們預期的資料是什麼樣子呢？很明顯，我們期望作出正確反應的人的比例接近 \\(50%\\)。換言之，在更加數學化的語言中，我們可以說 \\(\\frac{X}{N}\\) 大約等於 \\(0.5\\)。當然，我們不會期望這個比例完全等於 \\(0.5\\)。例如，如果我們測試了 \\(N=100\\) 個人，其中有 \\(X=53\\) 人回答正確，我們可能不得不承認這個數據與虛無假設是相當一致的。另一方面，如果有 \\(X=99\\) 個參與者回答正確，我們會非常有信心地認為虛無假設是錯的。同樣地，如果只有 \\(X=3\\) 人回答正確，我們也會非常有信心地認為虛無假設是錯的。現在讓我們更加技術地描述這一點。我們有一個數量 \\(X\\)，可以通過觀察資料計算出來。觀察了 \\(X\\) 值之後，我們就必須決定是相信虛無假設還是拒絕虛無假設，接受另一種假設。我們用來指導我們作出這個決定的量被稱為檢定統計量。\n在選擇了一個統計值後，下一步是明確地說明哪些統計值將導致我們拒絕虛無假設，哪些統計值將導致我們接受它。為了做到這一點，我們需要確定當虛無假設實際成立時，該統計值的樣本分佈是什麼（我們在@sec-Sampling-distribution-of-the-mean中討論過樣本分佈）。為什麼我們需要這個？因為這個分佈告訴我們，如果虛無假設是正確的，我們將期望哪些X的值。因此，我們可以使用這個分佈作為評估虛無假設與我們的資料是否一致的工具。\n如何確定統計值的樣本分佈？對於很多假設檢定來說，這個步驟通常相當複雜，甚至有些假設檢定我自己都不是很懂，稍後在本書中你會看到我對某些測試有些含糊其辭。但有時，這個步驟是非常簡單的。幸運的是，在ESP的例子中，我們可以提供其中最簡單的案例。我們的母群參數 \\(\\theta\\) 就是人們回答問題時的整體機率，而統計值 \\(X\\) 則是在樣本大小 \\(N\\) 中回答正確的人數。我們之前在「二項分佈」一節中已經見過這樣的分佈，那正是二項分佈所描述的內容！因此，為了使用在那個節中介紹的符號和術語，我們會說虛無假設預測 \\(X\\) 的分佈是二項式分佈，寫作\nAt this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the form of the data is that \\(X\\) out of \\(N\\) people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true, that ESP doesn’t exist and the true probability that anyone picks the correct colour is exactly \\(\\theta = 0.5\\). What would we expect the data to look like? Well, obviously we’d expect the proportion of people who make the correct response to be pretty close to \\(50\\%\\). Or, to phrase this in more mathematical terms, we’d say that \\(\\frac{X}{N}\\) is approximately \\(0.5\\). Of course, we wouldn’t expect this fraction to be exactly \\(0.5\\). If, for example, we tested \\(N = 100\\) people and \\(X = 53\\) of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if \\(X = 99\\) of our participants got the question right then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only \\(X = 3\\) people got the answer right we’d be similarly confident that the null was wrong. Let’s be a little more technical about this. We have a quantity \\(X\\) that we can calculate by looking at our data. After looking at the value of \\(X\\) we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a test statistic.\nHaving chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so we need to determine what the sampling distribution of the test statistic would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section 8.3.1. Why do we need this? Because this distribution tells us exactly what values of X our null hypothesis would lead us to expect. And, therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.\nHow do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter \\(\\theta\\) is just the overall probability that people respond correctly when asked the question, and our test statistic \\(X\\) is the count of the number of people who did so out of a sample size of N. We’ve seen a distribution like this before, in Section 7.4, and that’s exactly what the binomial distribution describes! So, to use the notation and terminology that I introduced in that section, we would say that the null hypothesis predicts that \\(X\\) is binomially distributed, which is written\n\\[X \\sim Binomial(\\theta,N)\\]\n因為虛無假設聲明 \\(\\theta = 0.5\\)，而我們的實驗有 \\(N=100\\) 人，所以我們已經擁有所需的樣本分佈。這個樣本分佈在@fig-fig9-1中繪製。沒有什麼驚喜，虛無假設說 \\(X=50\\) 是最有可能的結果，並且它說我們幾乎一定會看到 \\(40\\) 到 \\(60\\) 個正確的回答。\nSince the null hypothesis states that \\(\\theta = 0.5\\) and our experiment has \\(N = 100\\) people, we have the sampling distribution we need. This sampling distribution is plotted in Figure 9.1. No surprises really, the null hypothesis says that \\(X = 50\\) is the most likely outcome, and it says that we’re almost certain to see somewhere between \\(40\\) and \\(60\\) correct responses.\n\n\n\n\n\nFigure 9.1: 這是當虛無假設為真時，我們測試統計值 \\(X\\) 的樣本分佈。對於 ESP 的情境，這是一個二項式分佈。毫不意外的，因為虛無假設說明正確回答的機率是 \\(\\theta = 0.5\\)，所以樣本分佈顯示 50 (在 100 次測試中) 正確回答的比數最有可能發生。大部分的機率分佈在 40 到 60 之間。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#統計推論的決策要素",
    "href": "09-Hypothesis-testing.html#統計推論的決策要素",
    "title": "9  假設檢定",
    "section": "9.4 統計推論的決策要素",
    "text": "9.4 統計推論的決策要素\n我們已經非常接近最後一步了。前一步設定了一個統計檢定值 \\((X)\\)，並且選擇了我們相當有信心的檢定值數值。如果 \\(X\\) 接近 \\(\\frac{N}{2}\\)，我們應該保留虛無假設，否則我們就應該拒絕虛無假設。剩下的問題是什麼呢？確切地說，我們應該設定那些統計檢定值是對應虛無假設，那些統計檢定值對應對立假設？以ESP研究案為例，假如我觀察到一個值 \\(X=62\\)。我應該做出什麼決策呢？我應該相信虛無假設還是對立假設？\n\n\n9.4.1 棄卻域與臨界值\n要回答這個問題，我需要向各位介紹統計檢定值 \\(X\\) 的棄卻域（critical region）。檢定的棄卻域對應那些會讓我們拒絕虛無假設的 \\(X\\) 數值集合（這就是為什麼棄卻域有時也被稱為拒絕域）。我們如何找到這個棄卻域呢？嗯，讓我們想一想已知的條件：\n\n為了拒絕虛無假設，\\(X\\) 應該非常大或非常小\n如果虛無假設為真，\\(X\\) 的取樣分佈會是 \\(Binomial(0.5, N)\\)\n如果 \\(\\alpha = .05\\)，則棄卻域必須包含這個取樣分佈的 5%。\n\n最後一點非常重要。棄卻域的範圍是指那些會導致我們拒絕虛無假設的 \\(X\\) 數值範圍，而這個範圍是經由取樣分佈代換後的機率質量所決定的。如果我們選擇了一個其涵蓋 \\(20%\\) 機率質量的棄卻域，且虛無假設是符合事實的，那麼拒絕虛無假設的錯誤機率就是 \\(20%\\)。換言之，我們完成了一個顯著水準為 \\(0.2\\) 的檢驗。如果我們要求顯著水準是 \\(\\alpha = .05\\)，那麼棄卻域只能涵蓋統計檢定量取樣分佈的 \\(5%\\) 機率質量。\n\n我們在此總結解決完成假設檢定程序的三個要點。我們的棄卻域包括了機率分佈的最極端數值，也就是機率分佈的尾部。 Figure 9.2 展示了這個概念的視覺化。如果我們希望 \\(\\alpha = .05\\)，那麼對應的棄卻域是 \\(X \\leq 40\\) 和 \\(X \\geq 60\\)。6也就是說，如果回答正確的人數在 41 到 59 之間，那麼我們應該保留虛無假設。如果回答正確的人數在 0 到 40 或 60 到 100 之間，那麼我們就應該拒絕虛無假設。數字 40 和 60 通常被稱為臨界值(critical values)，因為這些數值定義了棄卻域的邊界。\n\n\n\n\n\n\nFigure 9.2: 這張圖與 Figure 9.1 的虛無假設 \\(X\\) 取樣分佈一樣，進一步展示ESP研究的假設檢定的棄卻域，假設檢定的顯著水準為\\(\\alpha = .05\\) 。灰色的柱子表示我們會保留虛無假設的 \\(X\\) 數值集合。深藍色的柱子表示棄卻域，也就是我們會拒絕虛無假設的 \\(X\\) 值。由於對立假設的主張是雙側的（即允許 \\(\\theta < .5\\) 和 \\(\\theta > .5\\)），因此棄卻域涵蓋分佈的兩個尾部。為確保 \\(\\alpha\\) 水準為 \\(.05\\)，我們需要確保左右區域各涵蓋了取樣分佈的 \\(2.5%\\)。\n\n\n\n\n最後，總結一下完成假設檢驗的主要步驟：\n\n選擇一個顯著水準 (例如，\\(\\alpha = .05\\))；\n選擇一個適當的統計檢定值 (例如，\\(X\\))，並且設定有比較意義的\\(H_0\\)和\\(H_1\\);\n假設虛無假設是符合事實的，找出該統計檢定值的取樣分佈（在ESP案例為二項分佈）；\n計算會產生符合 \\(\\alpha\\) 的棄卻域（0-40和60-100）。\n\n現在我們所要做的就是用實際資料計算統計檢定值（例如 \\(X=62\\)），然後比較檢定值與臨界值做出決策。由於 \\(62\\) 大於臨界值 \\(60\\)，我們可以拒絕虛無假設。也可以說，我們根據檢定結果得到一個在統計顯著的結論。\n\n\n\n9.4.2 小心使用統計“顯著”\n\n統計學和其他占卜術一樣，擁有一套專門術語，故意設計成讓非專業人員無法從字面理解術語的意思。 – G. O. Ashley 7\n\n在此需要講個關於 “significant”(常見中文說法“顯著”) 這個詞怎麼來的題外話。“significant” 在統計學中的概念其實很簡單，但這樣的命名並不夠好。如果實際資料能讓我們拒絕虛無假設，我們會說 “the result is statistically significant”(常見中文說法”結果有統計顯著性”)，通常簡單寫成 “the result is significant”(常見中文說法”有顯著結果”)。這個英文詞彙其實由來已久，來源可以追溯到 “significant” 的意思只是表達 “indicated”(已確認)的時代，並沒有現代英語的 “重要” 之類的含義。因此，今天許多讀者在開始學習統計學時會感到非常困惑，因為他們認為 “significant result” 必定是一個重要的結果。實際上，這並不是最早統計學家開始使用這個詞的意思。所有用”statistically significant”表達的主張， 只是表示資料允許我們拒絕一個虛無假設。至於結果是不是真的重要，則是另一個完全不同的問題，並且有其他各種因素的影響。\n\n\n\n9.4.3 單側與雙側檢定的不同\n還有一件事情要提醒各位同學，就是虛無假設與對立假設的設定方式。假如前面我所使用的統計假設是：\\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] 我們會發現對立假設涵蓋了 \\(\\theta < .5\\) 和 \\(\\theta > .5\\) 這兩種可能的數值集合。這代表我認為超感官知覺可能造成優於純粹猜測的表現，也可能產生比純粹猜測還差的表現（有些人就是會這麼認為），那麼這樣的設定是有意義的。在統計學的語彙庫，這稱為雙側檢定(two-sided test)。這是因為對立假設涵蓋了 無假設兩側的數值集合，因此檢定的棄卻域覆蓋取樣分佈的兩側尾部（如果 \\(\\alpha = .05\\)，則每側尾部佔取樣分佈的2.5％），如同 Figure 9.2 的展示。不過，這不是唯一的可能結果。如果我只在乎超感官知覺能夠產生優於純粹猜測的表現時，才願意相信這是事實，那麼對立假設就只會涵蓋 \\(\\theta > .5\\) 的數值集合。因此，虛無假設和對立假設就會變成\\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] 這種檢定條件 就是所謂的單側檢定(one-sided test)，此時檢定的棄卻域只有覆蓋取樣分佈的右側尾部，如同 Figure 9.3 的展示。\n\n\n\n\n\n\nFigure 9.3: 單側檢定的臨界區間。此狀況的對立假設是\\(\\theta \\geq .5\\)，當\\(X\\)的值很大，我們才能拒絕虛無假設。因此，臨界區間僅覆蓋取樣分佈的較大數值，具體來說是分佈的右側的 \\(5%\\) 。比較一下 Figure 9.2 的雙側檢定狀況。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設檢定的報告格式",
    "href": "09-Hypothesis-testing.html#假設檢定的報告格式",
    "title": "9  假設檢定",
    "section": "9.6 假設檢定的報告格式",
    "text": "9.6 假設檢定的報告格式\n在假設檢定結果的報告，通常需要報告幾項資訊，各種檢定方法的報告格式也各有特別之處。本書的後半部將介紹如何報告各種的檢定結果（特別詳細的例子請參考 Section 14.1.9 ），讓同學對常用檢定方法的報告格式有所了解。不過，無論是做那一種檢定，都是要報告p值和結果是否有統計顯著性。\n報告裡要說明p值是否表示有顯著性，這一點並不令人意外，因為這正是執行假設檢定程序的目的。會讓人意外的是，學術界對於要如何報告假設檢定結果一直存在各種爭議。先不管完全反對在報告中呈現虛無假設檢定結果的聲音，報告確切的p值和僅聲明\\(p < \\alpha\\)（例如，\\(p < .05\\)），這兩種報告規範一直互相拉鋸。\n\n\n9.6.1 一些爭議\n要了解為什麼有這些爭議，首先要認識報告p值的便利性。在統計實務，只要能夠計算出p值，即使事先沒有指定任何顯著水準 \\(\\alpha\\) ，任何人都可以進行檢定。也就是說，人人可以直接計算p值並直接做解釋。如果今天得到p = .062，這代表著研究者 必須願意容忍 \\(6.2%\\) 的型一錯誤率，才能證實虛無假設不符合事實。如果研究者無法接受 \\(6.2%\\) 的型一錯誤率，那麼就要接受虛無假設。因此爭議在於，為什麼不報告實際的p值，讓讀者自己決定什麼樣的型一錯誤率是可以接受的？這種報告方式的優勢在於 “軟化” 決策責任。若是你傾向接受尼曼定義p值的方法，這就是p值的充分意義。我們不必再依賴一個固定的顯著水準，像是 \\(\\alpha = .05\\) ， 做為 “接受” 或 “否決” 之間的明確界線，如此就消除了用 p = .051 和 p = .049等邊緣數值，做為決策依據的詭異問題。\n這種解讀的彈性是p值的優勢和劣勢。許多人不喜歡報告精確的p值原因是，研究人員因此有太多解釋的彈性空間。特別是每次查看資料後，p值能讓研究者改變願意容忍多少錯誤率的想法。以我虛構的 ESP 實驗為例，若是我進行了一次實驗，得到了一個\\(0.09\\)的p值。我應該要接受還是拒絕虛無假設？說實話，我甚至還沒有想到我“真正”願意接受的型一錯誤率。我對方法學的問題沒有意見，但是我對 ESP 是否存在肯定有預先立場，而且我肯定會在乎我的研究是否能夠在一個有影響力的科學期刊上發表。最匪夷所思的是，在我看了初期資料後，開始覺得 \\(9%\\) 的錯誤率還算可以，特別是與向全世界承認我的實驗失敗的煩惱相比，似乎是幸運的。因此，為了避免看起來像是事後編造，我就在報告裡說我的 \\(\\alpha\\) 是 \\(0.1\\)，理由是型一錯誤率 \\(10%\\) 並不算太糟糕，而且以這個條件來說，我的實驗結果是顯著的！我贏了。\n換句話說，這個故事情節透露，即使研究者的意圖是好的，態度也夠誠實，有時也很難抵擋在某些方面“淡化”研究難度的誘惑。任何曾經做過實驗的人都知道，這是一個漫長而且困難的過程，你常常會對你的假設情有獨鍾，很難讓人放下，並承認實驗沒有找到你想找到的結果，而這就是決策風險所在。如果我們報告“原始的”p值，每個人會朝他們想要相信的方向來解讀數據，而不是從數據看出事實，如果我們允許這樣做，那為什麼科學家們還要繼續進行科學研究呢？為什麼不讓每個人在任何事情上相信他們喜歡的立場，而不去管事實是什麼？好吧，這麼說有點極端了，但這就是爭議的來源。如果同學同意這樣的觀點，就必須事先指定顯著水準\\(\\alpha\\)，然後只報告檢定結果是否顯著。這是保持誠實的唯一途徑。\n\n\n\n\n\n\nTable 9.5:  p值水準的一般報告方法 \n\nUsual notationSignif. starsEnglish translationThe null is...\n\np > .05The test wasn't significantRetained\n\np < .05*The test was significant at \\( \\alpha \\) = .05 but not at \\( \\alpha \\) = .01 or \\( \\alpha \\) = .001.Rejected\n\np < .01**The test was significant at \\( \\alpha \\) = .05  and \\( \\alpha \\) = .01 but not at \\( \\alpha \\) = .001.Rejected\n\np < .001***The test was significant at all levelsRejected\n\n\n\n\n\n\n\n9.6.2 兩種實務建議\n在許多領域的統計實務，研究人員很少只指定一種顯著水準 \\(\\alpha\\) 。很多科學家會根據三種顯著水準做為決策標準：\\(.05\\)、\\(.01\\) 和 \\(.001\\)。結果報告裡要明確指出根據那些顯著水準，能拒絕虛無假設。陳述檢定結果的報告範例整理在 Table 9.5 。這樣可以稍微放開決策規則，因為以符合 \\(p < .01\\) 的資料做為證據，強度明顯大過 \\(p < .05\\) 的資料。除此之外，由於這些顯著水準是事先按慣例設定的，可以避免研究人員在查看數據後，才選擇顯著水準 \\(\\alpha\\) 。\n儘管如此，許多研究者仍然喜歡報告精確的p值。對許多人來說，讓讀者自行決定如何解讀\\(p=0.06\\)的好處多過壞處。然而，在許多實務場合，即使是那些偏好報告精確p值的研究人員，通常也只會寫\\(p<0.001\\)，而不是報告超微小p值的確切數值。這部分是因為許多軟體在p值太小時並不會印出完整數值（例如，SPSS算出\\(p<0.001\\)的話，只會印出\\(p=0.000\\)），另一方面，非常小的p值可能會誤導讀者 。人類大腦看到像\\(0.0000000001\\)這樣的數字時，很難壓抑直覺認為支持對立假設的結果是不可動搖的鐵證。然而，在真實世界，這樣的想法通常是錯誤的。生命是複雜的，每一個被發明出來的統計檢驗方法都是根據簡化的、逼近的和假設的模型。因此，很難抗拒從任何統計分析得到的\\(p<0.001\\)強烈信心感。換句話說，\\(p<0.001\\)實際只代表“就這個檢驗方法而言，證據是壓倒性的”。\n有鑑於此，同學可能好奇應該要怎麼做。有許多處理這個問題的建議，存在相當多相互矛盾的地方。有些人認為應該報告精確的p值，也有人認為檢定報告應該如同 Table 9.1 ，明確寫出研究假設與統計假設。對此，我能給同學們的最好建議是，大量閱讀你的主修領域已經發表的文獻報告，看看有什麼慣例。如果看不出有什麼一致的報告模式，那麼就使用你偏好的任何方法。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設檢定實作須知",
    "href": "09-Hypothesis-testing.html#假設檢定實作須知",
    "title": "9  假設檢定",
    "section": "9.7 假設檢定實作須知",
    "text": "9.7 假設檢定實作須知\n介紹到這裡，可能有些同學會想知道ESP研究是一個“真正”的假設檢定，還是只為了教學而編造的虛構範例，其實我們真的可以跑檢定程序。在前面的介紹，我(原作者)根據 Section 9.4 介紹的決策要素設定這個檢定程序，因為我認為這是大家在現實世界裡會遇到的最簡單的研究問題。不過，統計學家早就發明對應這種問題的檢定方法，正式名稱是二項式檢定，並且能用 jamovi 的內建模組”Frequencies”執行，在”Analyses”面板開啟模組選單，選擇“2 Outcomes”就能執行。若是設定虛無假設是回答機率為一半，即 \\(p = 0.5\\)，9。從示範資料庫裡開啟binomialtest.omv這個檔案，檢驗\\(n=100\\)位參與者裡 \\(x=62\\) 位回答正確的資料能拒絕或接受虛無假設，檢定結果如同 Figure 9.4 。\n\n\n\n\n\n\nFigure 9.4: 使用jamovi執行二項式檢定\n\n\n\n\n報表內容對初學的同學來說可能還很陌生，但你應該可以看出一些前面談過，要在報告裡呈現的資訊。最具體的一項是p = 0.02 小於常用的顯著水準 \\(\\alpha = 0.05\\)，因此我們可以拒絕虛無假設。隨著後續章節的學習，同學們會逐漸了解如何閱讀報表內容，希望學過之後，大家會發現報表內容的呈現方式，在閱讀和理解報告的好處。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#值得多了解的觀點",
    "href": "09-Hypothesis-testing.html#值得多了解的觀點",
    "title": "9  假設檢定",
    "section": "9.9 值得多了解的觀點",
    "text": "9.9 值得多了解的觀點\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n9.9.1 尼曼與費雪\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually13 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n9.9.2 貝氏統計與次數統計\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 7) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in Chapter 16, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n9.9.3 決策陷阱\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,14 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#本章小結",
    "href": "09-Hypothesis-testing.html#本章小結",
    "title": "9  假設檢定",
    "section": "9.10 本章小結",
    "text": "9.10 本章小結\n虛無假設檢定是統計理論最普遍的一種應用，科學研究報告結果都會呈現某種假設的檢定。現代科學家幾乎都多少要了解p值的意義，否則不能被認為是合格的科學研究者，所以這一章是本書最重要的部分。以下幫助讀者及學生快速回顧本章重點：\n\n假設的層次 研究假設與統計假設。虛無假設與對立假設。\n兩種決策失誤 型一與型二錯誤\n運用取樣分佈檢測統計值.\n統計推論的決策要素\n統計檢定的p值 為何用p值做決策是”模擬兩可”。\n假設檢定的報告格式\n假設檢定實作須知\n效果量、樣本量、考驗力\n一些值得繼續學習的主題\n\n到了本書最後一章 Chapter 16 ，我們會從貝氏統計觀點回顧統計理論與虛無假設檢定，還有介紹幾套非次數主義取向的統計工具。不過我們先暫別抽象的統計理論，接下來的第五部分將學習實用的統計分析方法。\n\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results. Cambridge University Press.\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60, 328–331.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation of classical statistics. Springer."
  },
  {
    "objectID": "10-Categorical-data-analysis.html",
    "href": "10-Categorical-data-analysis.html",
    "title": "10  類別資料分析",
    "section": "",
    "text": "Now that we’ve covered the basic theory behind hypothesis testing it’s time to start looking at specific tests that are commonly used in psychology. So where should we start? Not every textbook agrees on where to start, but I’m going to start with “\\(\\chi^2\\) tests” (this chapter, pronounced “chi-square”1 and “t-tests” in Chapter 11). Both of these tools are very frequently used in scientific practice, and whilst they’re not as powerful as “regression” and “analysis of variance” which we cover in later chapters, they’re much easier to understand.\nThe term “categorical data” is just another name for “nominal scale data”. It’s nothing that we haven’t already discussed, it’s just that in the context of data analysis people tend to use the term “categorical data” rather than “nominal scale data”. I don’t know why. In any case, categorical data analysis refers to a collection of tools that you can use when your data are nominal scale. However, there are a lot of different tools that can be used for categorical data analysis, and this chapter covers only a few of the more common ones."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-chi2-chi-square-goodness-of-fit-test",
    "href": "10-Categorical-data-analysis.html#the-chi2-chi-square-goodness-of-fit-test",
    "title": "10  Categorical data analysis",
    "section": "10.1 The \\(\\chi^2\\) (chi-square) goodness-of-fit test",
    "text": "10.1 The \\(\\chi^2\\) (chi-square) goodness-of-fit test\nThe \\(\\chi^2\\) goodness-of-fit test is one of the oldest hypothesis tests around. It was invented by Karl Pearson around the turn of the century (Pearson, 1900), with some corrections made later by Sir Ronald Fisher (Fisher, 1922). It tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution. For example, suppose a group of patients has been undergoing an experimental treatment and have had their health assessed to see whether their condition has improved, stayed the same or worsened. A goodness-of-fit test could be used to determine whether the numbers in each category - improved, no change, worsened - match the numbers that would be expected given the standard treatment option. Let’s think about this some more, with some psychology.\n\n10.1.1 The cards data\nOver the years there have been many studies showing that humans find it difficult to simulate randomness. Try as we might to “act” random, we think in terms of patterns and structure and so, when asked to “do something at random”, what people actually do is anything but random. As a consequence, the study of human randomness (or non-randomness, as the case may be) opens up a lot of deep psychological questions about how we think about the world. With this in mind, let’s consider a very simple study. Suppose I asked people to imagine a shuffled deck of cards, and mentally pick one card from this imaginary deck “at random”. After they’ve chosen one card I ask them to mentally select a second one. For both choices what we’re going to look at is the suit (hearts, clubs, spades or diamonds) that people chose. After asking, say, \\(N = 200\\) people to do this, I’d like to look at the data and figure out whether or not the cards that people pretended to select were really random. The data are contained in the randomness.csv file in which, when you open it up in jamovi and take a look at the spreadsheet view, you will see three variables. These are: an id variable that assigns a unique identifier to each participant, and the two variables choice_1 and choice_2 that indicate the card suits that people chose.\nFor the moment, let’s just focus on the first choice that people made. We’ll use the Frequency tables option under ‘Exploration’ - ‘Descriptives’ to count the number of times that we observed people choosing each suit. This is what we get (Table 10.1):\n\n\n\n\nTable 10.1:  Number of times each suit was chosen \n\nclubsdiamondsheartsspades\n\n35516450\n\n\n\n\n\nThat little frequency table is quite helpful. Looking at it, there’s a bit of a hint that people might be more likely to select hearts than clubs, but it’s not completely obvious just from looking at it whether that’s really true, or if this is just due to chance. So we’ll probably have to do some kind of statistical analysis to find out, which is what I’m going to talk about in the next section.\nExcellent. From this point on, we’ll treat this table as the data that we’re looking to analyse. However, since I’m going to have to talk about this data in mathematical terms (sorry!) it might be a good idea to be clear about what the notation is. In mathematical notation, we shorten the human-readable word “observed” to the letter \\(O\\), and we use subscripts to denote the position of the observation. So the second observation in our table is written as \\(O_2\\) in maths. The relationship between the English descriptions and the mathematical symbols are illustrated in Table 10.2.\n\n\n\n\nTable 10.2:  Relationship between English descriptions and mathematical symbols \n\nlabelindex, imath. symbolthe value\n\nclubs, \\( \\clubsuit \\)1\\( O_1 \\)35\n\ndiamonds, \\( \\diamondsuit \\)2\\( O_2 \\)51\n\nhearts, \\( \\heartsuit \\)3\\( O_3 \\)64\n\nspades, \\( \\spadesuit \\)4\\( O_4 \\)50\n\n\n\n\n\nHopefully that’s pretty clear. It’s also worth noting that mathematicians prefer to talk about general rather than specific things, so you’ll also see the notation \\(O_i\\), which refers to the number of observations that fall within the i-th category (where i could be 1, 2, 3 or 4). Finally, if we want to refer to the set of all observed frequencies, statisticians group all observed values into a vector 2, which I’ll refer to as \\(O\\).\n\\[O = (O_1, O_2, O_3, O_4)\\]\nAgain, this is nothing new or interesting. It’s just notation. If I say that \\(O = (35, 51, 64, 50)\\) all I’m doing is describing the table of observed frequencies (i.e., observed), but I’m referring to it using mathematical notation.\n\n\n10.1.2 The null hypothesis and the alternative hypothesis\nAs the last section indicated, our research hypothesis is that “people don’t choose cards randomly”. What we’re going to want to do now is translate this into some statistical hypotheses and then construct a statistical test of those hypotheses. The test that I’m going to describe to you is Pearson’s \\(\\chi^2\\) (chi-square) goodness-of-fit test, and as is so often the case we have to begin by carefully constructing our null hypothesis. In this case, it’s pretty easy. First, let’s state the null hypothesis in words:\n\\[H_0: \\text{ All four suits are chosen with equal probability}\\]\nNow, because this is statistics, we have to be able to say the same thing in a mathematical way. To do this, let’s use the notation \\(P_j\\) to refer to the true probability that the j-th suit is chosen. If the null hypothesis is true, then each of the four suits has a 25% chance of being selected. In other words, our null hypothesis claims that \\(P_1 = .25\\), \\(P_2 = .25\\), \\(P3 = .25\\) and finally that \\(P_4 = .25\\) . However, in the same way that we can group our observed frequencies into a vector O that summarises the entire data set, we can use P to refer to the probabilities that correspond to our null hypothesis. So if I let the vector \\(P = (P_1, P_2, P_3, P_4)\\) refer to the collection of probabilities that describe our null hypothesis, then we have:\n\\[H_0: P =(.25, .25, .25, .25)\\]\nIn this particular instance, our null hypothesis corresponds to a vector of probabilities P in which all of the probabilities are equal to one another. But this doesn’t have to be the case. For instance, if the experimental task was for people to imagine they were drawing from a deck that had twice as many clubs as any other suit, then the null hypothesis would correspond to something like \\(P = (.4, .2, .2, .2)\\). As long as the probabilities are all positive numbers, and they all sum to 1, then it’s a perfectly legitimate choice for the null hypothesis. However, the most common use of the goodness-of-fit test is to test a null hypothesis that all of the categories are equally likely, so we’ll stick to that for our example.\nWhat about our alternative hypothesis, \\(H_1\\)? All we’re really interested in is demonstrating that the probabilities involved aren’t all identical (that is, people’s choices weren’t completely random). As a consequence, the “human friendly” versions of our hypotheses look like this:\n\\(H_0: \\text{ All four suits are chosen with equal probability}\\)\n\\(H_1: \\text{ At least one of the suit-choice probabilities isn’t 0.25}\\)\n…and the “mathematician friendly” version is:\n\\(H_0: P= (.25, .25, .25, .25)\\)\n\\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n10.1.3 The “goodness-of-fit” test statistic\nAt this point, we have our observed frequencies O and a collection of probabilities P corresponding to the null hypothesis that we want to test. What we now want to do is construct a test of the null hypothesis. As always, if we want to test \\(H_0\\) against \\(H_1\\), we’re going to need a test statistic. The basic trick that a goodness-of-fit test uses is to construct a test statistic that measures how “close” the data are to the null hypothesis. If the data don’t resemble what you’d “expect” to see if the null hypothesis were true, then it probably isn’t true. Okay, if the null hypothesis were true, what would we expect to see? Or, to use the correct terminology, what are the expected frequencies. There are \\(N = 200\\) observations, and (if the null is true) the probability of any one of them choosing a heart is \\(P_3 = .25\\), so I guess we’re expecting \\(200 \\times .25 = 50\\) hearts, right? Or, more specifically, if we let Ei refer to “the number of category i responses that we’re expecting if the null is true”, then\n\\[E_i=N \\times P_i\\]\nThis is pretty easy to calculate.If there are 200 observations that can fall into four categories, and we think that all four categories are equally likely, then on average we’d expect to see 50 observations in each category, right?\nNow, how do we translate this into a test statistic? Clearly, what we want to do is compare the expected number of observations in each category (\\(E_i\\)) with the observed number of observations in that category (\\(O_i\\)). And on the basis of this comparison we ought to be able to come up with a good test statistic. To start with, let’s calculate the difference between what the null hypothesis expected us to find and what we actually did find. That is, we calculate the “observed minus expected” difference score, \\(O_i - E_i\\) . This is illustrated in Table 10.3.\n\n\n\n\nTable 10.3:  Expected and observed frequencies \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)50505050\n\nobserved frequency \\( O_i\\)35516450\n\ndifference score \\( O_i-E_i\\)-151140\n\n\n\n\n\nSo, based on our calculations, it’s clear that people chose more hearts and fewer clubs than the null hypothesis predicted. However, a moment’s thought suggests that these raw differences aren’t quite what we’re looking for. Intuitively, it feels like it’s just as bad when the null hypothesis predicts too few observations (which is what happened with hearts) as it is when it predicts too many (which is what happened with clubs). So it’s a bit weird that we have a negative number for clubs and a positive number for hearts. One easy way to fix this is to square everything, so that we now calculate the squared differences, \\((E_i - O_i)^2\\) . As before, we can do this by hand (Table 10.4).\n\n\n\n\nTable 10.4:  Squaring the difference scores \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n22511960\n\n\n\n\n\nNow we’re making progress. What we’ve got now is a collection of numbers that are big whenever the null hypothesis makes a bad prediction (clubs and hearts), but are small whenever it makes a good one (diamonds and spades). Next, for some technical reasons that I’ll explain in a moment, let’s also divide all these numbers by the expected frequency Ei , so we’re actually calculating \\(\\frac{(E_i-O_i)^2}{E_i}\\) . Since \\(E_i = 50\\) for all categories in our example, it’s not a very interesting calculation, but let’s do it anyway (Table 10.5).\n\n\n\n\nTable 10.5:  Dividing the squared difference scores by the expected frequency to provide an ‘error’ score \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n4.500.023.920.00\n\n\n\n\n\nIn effect, what we’ve got here are four different “error” scores, each one telling us how big a “mistake” the null hypothesis made when we tried to use it to predict our observed frequencies. So, in order to convert this into a useful test statistic, one thing we could do is just add these numbers up. The result is called the goodness-of-fit statistic, conventionally referred to either as \\(\\chi^2\\) (chi-square) or GOF. We can calculate it as in Table 10.6.\n\\[\\sum( (observed - expected)^2 / expected )\\]\nThis gives us a value of 8.44.\n[Additional technical detail 3]\nAs we’ve seen from our calculations, in our cards data set we’ve got a value of \\(\\chi^2\\) = 8.44. So now the question becomes is this a big enough value to reject the null?\n\n\n10.1.4 The sampling distribution of the GOF statistic\nTo determine whether or not a particular value of \\(\\chi^2\\) is large enough to justify rejecting the null hypothesis, we’re going to need to figure out what the sampling distribution for \\(\\chi^2\\) would be if the null hypothesis were true. So that’s what I’m going to do in this section. I’ll show you in a fair amount of detail how this sampling distribution is constructed, and then, in the next section, use it to build up a hypothesis test. If you want to cut to the chase and are willing to take it on faith that the sampling distribution is a \\(\\chi^2\\) (chi-square) distribution with \\(k - 1\\) degrees of freedom, you can skip the rest of this section. However, if you want to understand why the goodness-of-fit test works the way it does, read on.\nOkay, let’s suppose that the null hypothesis is actually true. If so, then the true probability that an observation falls in the i-th category is \\(P_i\\) . After all, that’s pretty much the definition of our null hypothesis. Let’s think about what this actually means. This is kind of like saying that “nature” makes the decision about whether or not the observation ends up in category i by flipping a weighted coin (i.e., one where the probability of getting a head is \\(P_j\\) ). And therefore we can think of our observed frequency \\(O_i\\) by imagining that nature flipped N of these coins (one for each observation in the data set), and exactly \\(O_i\\) of them came up heads. Obviously, this is a pretty weird way to think about the experiment. But what it does (I hope) is remind you that we’ve actually seen this scenario before. It’s exactly the same set up that gave rise to Section 7.4 in Chapter 7. In other words, if the null hypothesis is true, then it follows that our observed frequencies were generated by sampling from a binomial distribution:\n\\[O_i \\sim Binomial(P_i,N) \\]\nNow, if you remember from our discussion of Section 8.3.3 the binomial distribution starts to look pretty much identical to the normal distribution, especially when \\(N\\) is large and when \\(P_i\\) isn’t too close to 0 or 1. In other words as long as \\(N^P_i\\) is large enough. Or, to put it another way, when the expected frequency Ei is large enough then the theoretical distribution of \\(O_i\\) is approximately normal. Better yet, if \\(O_i\\) is normally distributed, then so is \\((O_i-E_i)/\\sqrt{(E_i)}\\) . Since \\(E_i\\) is a fixed value, subtracting off Ei and dividing by ? Ei changes the mean and standard deviation of the normal distribution but that’s all it does. Okay, so now let’s have a look at what our goodness-of-fit statistic actually is. What we’re doing is taking a bunch of things that are normally-distributed, squaring them, and adding them up. Wait. We’ve seen that before too! As we discussed in the section on Section 7.6, when you take a bunch of things that have a standard normal distribution (i.e., mean 0 and standard deviation 1), square them and then add them up, the resulting quantity has a chi-square distribution. So now we know that the null hypothesis predicts that the sampling distribution of the goodness-of-fit statistic is a chi-square distribution. Cool.\nThere’s one last detail to talk about, namely the degrees of freedom. If you remember back to Section 7.6, I said that if the number of things you’re adding up is k, then the degrees of freedom for the resulting chi-square distribution is k. Yet, what I said at the start of this section is that the actual degrees of freedom for the chi-square goodness-of-fit test is \\(k - 1\\). What’s up with that? The answer here is that what we’re supposed to be looking at is the number of genuinely independent things that are getting added together. And, as I’ll go on to talk about in the next section, even though there are k things that we’re adding only \\(k - 1\\) of them are truly independent, and so the degrees of freedom is actually only \\(k - 1\\). That’s the topic of the next section4.\n\n\n10.1.5 Degrees of freedom\n\n\n\n\n\nFigure 10.1: \\(\\chi^2\\) (chi-square) distributions with different values for the ‘degrees of freedom’\n\n\n\n\nWhen I introduced the chi-square distribution in Section 7.6, I was a bit vague about what “degrees of freedom” actually means. Obviously, it matters. Looking at Figure 10.1, you can see that if we change the degrees of freedom then the chi-square distribution changes shape quite substantially. But what exactly is it? Again, when I introduced the distribution and explained its relationship to the normal distribution, I did offer an answer: it’s the number of “normally distributed variables” that I’m squaring and adding together. But, for most people, that’s kind of abstract and not entirely helpful. What we really need to do is try to understand degrees of freedom in terms of our data. So here goes.\nThe basic idea behind degrees of freedom is quite simple. You calculate it by counting up the number of distinct “quantities” that are used to describe your data and then subtracting off all of the “constraints” that those data must satisfy.5 This is a bit vague, so let’s use our cards data as a concrete example. We describe our data using four numbers, \\(O1, O2, O3\\) and O4 corresponding to the observed frequencies of the four different categories (hearts, clubs, diamonds, spades). These four numbers are the random outcomes of our experiment. But my experiment actually has a fixed constraint built into it: the sample size \\(N\\). 6 That is, if we know\nhow many people chose hearts, how many chose diamonds and how many chose clubs, then we’d be able to figure out exactly how many chose spades. In other words, although our data are described using four numbers, they only actually correspond to \\(4 - 1 = 3\\) degrees of freedom. A slightly different way of thinking about it is to notice that there are four probabilities that we’re interested in (again, corresponding to the four different categories), but these probabilities must sum to one, which imposes a constraint. Therefore the degrees of freedom is \\(4 - 1 = 3\\). Regardless of whether you want to think about it in terms of the observed frequencies or in terms of the probabilities, the answer is the same. In general, when running the \\(\\chi^2\\)(chi-square) goodness-of-fit test for an experiment involving \\(k\\) groups, then the degrees of freedom will be \\(k - 1\\).\n\n\n10.1.6 Testing the null hypothesis\nThe final step in the process of constructing our hypothesis test is to figure out what the rejection region is. That is, what values of \\(\\chi^2\\) would lead us to reject the null hypothesis. As we saw earlier, large values of \\(\\chi^2\\) imply that the null hypothesis has done a poor job of predicting the data from our experiment, whereas small values of \\(\\chi^2\\) imply that it’s actually done pretty well. Therefore, a pretty sensible strategy would be to say there is some critical value such that if \\(\\chi^2\\) is bigger than the critical value we reject the null, but if \\(\\chi^2\\) is smaller than this value we retain the null. In other words, to use the language we introduced in Chapter 9 the chi-square goodness-of-fit test is always a one-sided test. Right, so all we have to do is figure out what this critical value is. And it’s pretty straightforward. If we want our test to have significance level of \\(\\alpha = .05\\) (that is, we are willing to tolerate a Type I error rate of \\(5%\\)), then we have to choose our critical value so that there is only a 5% chance that \\(\\chi^2\\) could get to be that big if the null hypothesis is true. This is illustrated in Figure 10.2.\n\n\n\n\n\nFigure 10.2: Illustration of how the hypothesis testing works for the \\(\\chi^2\\) (chi-square) goodness of-fit test\n\n\n\n\nAh but, I hear you ask, how do I find the critical value of a chi-square distribution with \\(k-1\\) degrees of freedom? Many many years ago when I first took a psychology statistics class we used to look up these critical values in a book of critical value tables, like the one in Figure 10.3. Looking at this Figure, we can see that the critical value for a \\(\\chi^2\\) distribution with 3 degrees of freedom, and p=0.05 is 7.815.\n\n\n\n\n\nFigure 10.3: Table of critical values for the chi-square distribution\n\n\n\n\nSo, if our calculated \\(\\chi^2\\) statistic is bigger than the critical value of \\(7.815\\), then we can reject the null hypothesis (remember that the null hypothesis, \\(H_0\\), is that all four suits are chosen with equal probability). Since we actually already calculated that before (i.e., \\(\\chi^2\\) = 8.44) we can reject the null hypothesis. And that’s it, basically. You now know “Pearson’s \\(\\chi^2\\) test for the goodness-of-fit”. Lucky you.\n\n\n10.1.7 Doing the test in jamovi\nNot surprisingly, jamovi provides an analysis that will do these calculations for you. Let’s use the Randomness.omv file. From the main ‘Analyses’ toolbar select ‘Frequencies’ - ‘One Sample Proportion Tests’ - ‘\\(N\\) Outcomes’. Then in the analysis window that appears move the variable you want to analyse (choice 1 across into the ‘Variable’ box. Also, click on the ‘Expected counts’ check box so that these are shown on the results table. When you have done all this, you should see the analysis results in jamovi as in Figure 10.4. No surprise then that jamovi provides the same expected counts and statistics that we calculated by hand above, with a \\(\\chi^2\\) value of \\((8.44\\) with \\(3\\) d.f. and \\(p=0.038\\). Note that we don’t need to look up a critical p-value threshold value any more, as jamovi gives us the actual p-value of the calculated \\(\\chi^2\\) for \\(3\\) d.f.\n\n\n\n\n\nFigure 10.4: A \\(\\chi^2\\) One Sample Proportion Test in jamovi, with table showing both observed and expected frequencies and proportions\n\n\n\n\n\n\n10.1.8 Specifying a different null hypothesis\nAt this point you might be wondering what to do if you want to run a goodness-of-fit test but your null hypothesis is not that all categories are equally likely. For instance, let’s suppose that someone had made the theoretical prediction that people should choose red cards \\(60\\%\\) of the time, and black cards \\(40\\%\\) of the time (I’ve no idea why you’d predict that), but had no other preferences. If that were the case, the null hypothesis would be to expect \\(30\\%\\) of the choices to be hearts, \\(30\\%\\) to be diamonds, \\(20\\%\\) to be spades and \\(20\\%\\) to be clubs. In other words we would expect hearts and diamonds to appear 1.5 times more often than spades and clubs (the ratio \\(30\\%\\) : \\(20\\%\\) is the same as 1.5 : 1). This seems like a silly theory to me, and it’s pretty easy to test this explicitly specified null hypothesis with the data in our jamovi analysis. In the analysis window (labelled ‘Proportion Test (N Outcomes)’ in Figure 10.4 you can expand the options for ‘Expected Proportions’. When you do this, there are options for entering different ratio values for the variable you have selected, in our case this is choice 1. Change the ratio to reflect the new null hypothesis, as in Figure 10.5, and see how the results change.\n\n\n\n\n\nFigure 10.5: Changing the expected proportions in the \\(\\\\chi^2\\) One Sample Proportion Test in jamovi\n\n\n\n\nThe expected counts are now shown in Table 10.6.\n\n\n\n\nTable 10.6:  Expected counts for a different null hypothesis \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)40606040\n\n\n\n\n\nand the \\(\\chi^2\\) statistic is 4.74, 3 d.f., \\(p = 0.182\\). Now, the results of our updated hypotheses and the expected frequencies are different from what they were last time. As a consequence our \\(\\chi^2\\) test statistic is different, and our p-value is different too. Annoyingly, the p-value is \\(.182\\), so we can’t reject the null hypothesis (look back at Section 9.5 to remind yourself why). Sadly, despite the fact that the null hypothesis corresponds to a very silly theory, these data don’t provide enough evidence against it.\n\n\n10.1.9 How to report the results of the test\nSo now you know how the test works, and you know how to do the test using a wonderful jamovi flavoured magic computing box. The next thing you need to know is how to write up the results. After all, there’s no point in designing and running an experiment and then analysing the data if you don’t tell anyone about it! So let’s now talk about what you need to do when reporting your analysis. Let’s stick with our card-suits example. If I wanted to write this result up for a paper or something, then the conventional way to report this would be to write something like this:\n\nOf the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness-of-fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were significant (\\(\\chi^2(3) = 8.44, p< .05)\\), suggesting that people did not select suits purely at random.\n\nThis is pretty straightforward and hopefully it seems pretty unremarkable. That said, there’s a few things that you should note about this description:\n\nThe statistical test is preceded by the descriptive statistics. That is, I told the reader something about what the data look like before going on to do the test. In general, this is good practice. Always remember that your reader doesn’t know your data anywhere near as well as you do. So, unless you describe it to them properly, the statistical tests won’t make any sense to them and they’ll get frustrated and cry.\nThe description tells you what the null hypothesis being tested is. To be honest, writers don’t always do this but it’s often a good idea in those situations where some ambiguity exists, or when you can’t rely on your readership being intimately familiar with the statistical tools that you’re using. Quite often the reader might not know (or remember) all the details of the test that your using, so it’s a kind of politeness to “remind” them! As far as the goodness-of-fit test goes, you can usually rely on a scientific audience knowing how it works (since it’s covered in most intro stats classes). However, it’s still a good idea to be explicit about stating the null hypothesis (briefly!) because the null hypothesis can be different depending on what you’re using the test for. For instance, in the cards example my null hypothesis was that all the four suit probabilities were identical (i.e., \\(P1 = P2 = P3 = P4 = 0.25\\)), but there’s nothing special about that hypothesis. I could just as easily have tested the null hypothesis that \\(P_1 = 0.7\\) and \\(P2 = P3 = P4 = 0.1\\) using a goodness-of-fit test. So it’s helpful to the reader if you explain to them what your null hypothesis was. Also, notice that I described the null hypothesis in words, not in maths. That’s perfectly acceptable. You can describe it in maths if you like, but since most readers find words easier to read than symbols, most writers tend to describe the null using words if they can.\nA “stat block” is included. When reporting the results of the test itself, I didn’t just say that the result was significant, I included a “stat block” (i.e., the dense mathematical looking part in the parentheses) which reports all the “key” statistical information. For the chi-square goodness-of-fit test, the information that gets reported is the test statistic (that the goodness-of-fit statistic was 8.44), the information about the distribution used in the test (\\(\\chi^2\\) with 3 degrees of freedom which is usually shortened to \\(\\chi^2\\)(3)), and then the information about whether the result was significant (in this case \\(p< .05\\)). The particular information that needs to go into the stat block is different for every test, and so each time I introduce a new test I’ll show you what the stat block should look like.7 However the general principle is that you should always provide enough information so that the reader could check the test results themselves if they really wanted to.\nThe results are interpreted. In addition to indicating that the result was significant, I provided an interpretation of the result (i.e., that people didn’t choose randomly). This is also a kindness to the reader, because it tells them something about what they should believe about what’s going on in your data. If you don’t include something like this, it’s really hard for your reader to understand what’s going on.8\n\nAs with everything else, your overriding concern should be that you explain things to your reader. Always remember that the point of reporting your results is to communicate to another human being. I cannot tell you just how many times I’ve seen the results section of a report or a thesis or even a scientific article that is just gibberish, because the writer has focused solely on making sure they’ve included all the numbers and forgotten to actually communicate with the human reader.\n\nSatan delights equally in statistics and in quoting scripture9 – H.G. Wells"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-chi2-test-of-independence-or-association",
    "href": "10-Categorical-data-analysis.html#the-chi2-test-of-independence-or-association",
    "title": "10  Categorical data analysis",
    "section": "10.2 The \\(\\chi^2\\) test of independence (or association)",
    "text": "10.2 The \\(\\chi^2\\) test of independence (or association)\n\nGUARDBOT 1: Halt!\nGUARDBOT 2: Be you robot or human?\nLEELA: Robot…we be.\nFRY: Uh, yup! Just two robots out roboting it up! Eh?\nGUARDBOT 1: Administer the test.\nGUARDBOT 2: Which of the following would you most prefer? A: A puppy, B: A pretty flower from your sweetie, or C: A large properly-formatted data file?\nGUARDBOT 1: Choose!\nFuturama, “Fear of a Bot Planet”\n\nThe other day I was watching an animated documentary examining the quaint customs of the natives of the planet Chapek 9. Apparently, in order to gain access to their capital city a visitor must prove that they’re a robot, not a human. In order to determine whether or not a visitor is human, the natives ask whether the visitor prefers puppies, flowers, or large, properly formatted data files. “Pretty clever,” I thought to myself “but what if humans and robots have the same preferences? That probably wouldn’t be a very good test then, would it?” As it happens, I got my hands on the testing data that the civil authorities of Chapek 9 used to check this. It turns out that what they did was very simple. They found a bunch of robots and a bunch of humans and asked them what they preferred. I saved their data in a file called chapek9.omv, which we can now load into jamovi. As well as the ID variable that identifies individual people, there are two nominal text variables, species and choice. In total there are 180 entries in the data set, one for each person (counting both robots and humans as “people”) who was asked to make a choice. Specifically, there are 93 humans and 87 robots, and overwhelmingly the preferred choice is the data file. You can check this yourself by asking jamovi for Frequency Tables, under the ‘Exploration’ - ‘Descriptives’ button. However, this summary does not address the question we’re interested in. To do that, we need a more detailed description of the data. What we want to do is look at the choices broken down by species. That is, we need to cross-tabulate the data (see Section 6.1). In jamovi we do this using the ‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’ analysis, and we should get a table something like Table 10.7.\n\n\n\n\nTable 10.7:  Cross-tabulating the data \n\nRobotHumanTotal\n\nPuppy131528\n\nFlower301343\n\nData4465109\n\nTotal8793180\n\n\n\n\n\nFrom this, it’s quite clear that the vast majority of the humans chose the data file, whereas the robots tended to be a lot more even in their preferences. Leaving aside the question of why the humans might be more likely to choose the data file for the moment (which does seem quite odd, admittedly), our first order of business is to determine if the discrepancy between human choices and robot choices in the data set is statistically significant.\n\n10.2.1 Constructing our hypothesis test\nHow do we analyse this data? Specifically, since my research hypothesis is that “humans and robots answer the question in different ways”, how can I construct a test of the null hypothesis that “humans and robots answer the question the same way”? As before, we begin by establishing some notation to describe the data (Table 10.8).\n\n\n\n\nTable 10.8:  Notation to describe the data \n\nRobotHumanTotal\n\nPuppy\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nFlower\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nData\\(O_{31}\\)\\(O_{32}\\)\\(R_{3}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)N\n\n\n\n\n\nIn this notation we say that \\(O_{ij}\\) is a count (observed frequency) of the number of respondents that are of species j (robots or human) who gave answer i (puppy, flower or data) when asked to make a choice. The total number of observations is written \\(N\\), as usual. Finally, I’ve used \\(R_i\\) to denote the row totals (e.g., \\(R_1\\) is the total number of people who chose the flower), and \\(C_j\\) to denote the column totals (e.g., \\(C_1\\) is the total number of robots).10\nSo now let’s think about what the null hypothesis says. If robots and humans are responding in the same way to the question, it means that the probability that “a robot says puppy” is the same as the probability that “a human says puppy”, and so on for the other two possibilities. So, if we use \\(P_{ij}\\) to denote “the probability that a member of species j gives response i” then our null hypothesis is that:\n\\[\n\\begin{aligned}\nH_0 &: \\text{All of the following are true:} \\\\\n&P_{11} = P_{12}\\text{ (same probability of saying “puppy”),} \\\\\n&P_{21} = P_{22}\\text{ (same probability of saying “flower”), and} \\\\\n&P_{31} = P_{32}\\text{ (same probability of saying “data”).}\n\\end{aligned}\n\\]\nAnd actually, since the null hypothesis is claiming that the true choice probabilities don’t depend on the species of the person making the choice, we can let Pi refer to this probability, e.g., P1 is the true probability of choosing the puppy.\nNext, in much the same way that we did with the goodness-of-fit test, what we need to do is calculate the expected frequencies. That is, for each of the observed counts \\(O_{ij}\\) , we need to figure out what the null hypothesis would tell us to expect. Let’s denote this expected frequency by \\(E_{ij}\\). This time, it’s a little bit trickier. If there are a total of \\(C_j\\) people that belong to species \\(j\\), and the true probability of anyone (regardless of species) choosing option \\(i\\) is \\(P_i\\) , then the expected frequency is just:\n\\[E_{ij}=C_j \\times P_i\\]\nNow, this is all very well and good, but we have a problem. Unlike the situation we had with the goodness-of-fit test, the null hypothesis doesn’t actually specify a particular value for Pi .\nIt’s something we have to estimate (see Chapter 8) from the data! Fortunately, this is pretty easy to do. If 28 out of 180 people selected the flowers, then a natural estimate for the probability of choosing flowers is \\(\\frac{28}{180}\\), which is approximately \\(.16\\). If we phrase this in mathematical terms, what we’re saying is that our estimate for the probability of choosing option i is just the row total divided by the total sample size:\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\nTherefore, our expected frequency can be written as the product (i.e. multiplication) of the row total and the column total, divided by the total number of observations:11\n\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]\n[Additional technical detail 12]\nAs before, large values of \\(X^2\\) indicate that the null hypothesis provides a poor description of the data, whereas small values of \\(X^2\\) suggest that it does a good job of accounting for the data. Therefore, just like last time, we want to reject the null hypothesis if \\(X^2\\) is too large.\nNot surprisingly, this statistic is \\(\\chi^2\\) distributed. All we need to do is figure out how many degrees of freedom are involved, which actually isn’t too hard. As I mentioned before, you can (usually) think of the degrees of freedom as being equal to the number of data points that you’re analysing, minus the number of constraints. A contingency table with r rows and c columns contains a total of \\(r^{c}\\) observed frequencies, so that’s the total number of observations. What about the constraints? Here, it’s slightly trickier. The answer is always the same\n\\[df=(r-1)(c-1)\\]\nbut the explanation for why the degrees of freedom takes this value is different depending on the experimental design. For the sake of argument, let’s suppose that we had honestly intended to survey exactly 87 robots and 93 humans (column totals fixed by the experimenter), but left the row totals free to vary (row totals are random variables). Let’s think about the constraints that apply here. Well, since we deliberately fixed the column totals by Act of Experimenter, we have \\(c\\) constraints right there. But, there’s actually more to it than that. Remember how our null hypothesis had some free parameters (i.e., we had to estimate the Pi values)? Those matter too. I won’t explain why in this book, but every free parameter in the null hypothesis is rather like an additional constraint. So, how many of those are there? Well, since these probabilities have to sum to 1, there’s only \\(r - 1\\) of these. So our total degrees of freedom is:\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\nAlternatively, suppose that the only thing that the experimenter fixed was the total sample size N. That is, we quizzed the first 180 people that we saw and it just turned out that 87 were robots and 93 were humans. This time around our reasoning would be slightly different, but would still lead us to the same answer. Our null hypothesis still has \\(r - 1\\) free parameters corresponding to the choice probabilities, but it now also has \\(c - 1\\) free parameters corresponding to the species probabilities, because we’d also have to estimate the probability that a randomly sampled person turns out to be a robot.13 Finally, since we did actually fix the total number of observations N, that’s one more constraint. So, now we have rc observations, and \\((c-1)+(r-1)+1\\) constraints. What does that give?\n\\[\\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\n\\] Amazing.\n\n\n10.2.2 Doing the test in jamovi\nOkay, now that we know how the test works let’s have a look at how it’s done in jamovi. As tempting as it is to lead you through the tedious calculations so that you’re forced to learn it the long way, I figure there’s no point. I already showed you how to do it the long way for the goodness-of-fit test in the last section, and since the test of independence isn’t conceptually any different, you won’t learn anything new by doing it the long way. So instead I’ll go straight to showing you the easy way. After you have run the test in jamovi (‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’), all you have to do is look underneath the contingency table in the jamovi results window and there is the \\(\\chi^2\\) statistic for you. This shows a \\(\\chi^2\\) statistic value of 10.72, with 2 d.f. and p-value = 0.005.\nThat was easy, wasn’t it! You can also ask jamovi to show you the expected counts - just click on the check box for ‘Counts’ - ‘Expected’ in the ‘Cells’ options and the expected counts will appear in the contingency table. And whilst you are doing that, an effect size measure would be helpful. We’ll choose Cramér’s \\(V\\), and you can specify this from a check box in the ‘Statistics’ options, and it gives a value for Cramér’s \\(V\\) of \\(0.24\\). See Figure 10.6. We will talk about this some more in just a moment.\n\n\n\n\n\nFigure 10.6: Independent samples \\(\\chi^2\\) test in jamovi using the Chapek 9 data\n\n\n\n\nThis output gives us enough information to write up the result:\n\nPearson’s \\(\\chi^2\\) revealed a significant association between species and choice (\\(\\chi^2(2) = 10.7, p< .01)\\). Robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data.\n\nNotice that, once again, I provided a little bit of interpretation to help the human reader understand what’s going on with the data. Later on in my discussion section I’d provide a bit more context. To illustrate the difference, here’s what I’d probably say later on:\n\nThe fact that humans appeared to have a stronger preference for raw data files than robots is somewhat counter-intuitive. However, in context it makes some sense, as the civil authority on Chapek 9 has an unfortunate tendency to kill and dissect humans when they are identified. As such it seems most likely that the human participants did not respond honestly to the question, so as to avoid potentially undesirable consequences. This should be considered to be a substantial methodological weakness.\n\nThis could be classified as a rather extreme example of a reactivity effect, I suppose. Obviously, in this case the problem is severe enough that the study is more or less worthless as a tool for understanding the difference preferences among humans and robots. However, I hope this illustrates the difference between getting a statistically significant result (our null hypothesis is rejected in favour of the alternative), and finding something of scientific value (the data tell us nothing of interest about our research hypothesis due to a big methodological flaw)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-continuity-correction",
    "href": "10-Categorical-data-analysis.html#the-continuity-correction",
    "title": "10  Categorical data analysis",
    "section": "10.3 The continuity correction",
    "text": "10.3 The continuity correction\nOkay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the Yates correction. Remember what I pointed out earlier: the \\(\\chi^2\\) test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large \\(N\\). One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a \\(2 \\times 2\\) contingency table). The main reason for this is that the true sampling distribution for the \\(X^{2}\\) statistic is actually discrete (because you’re dealing with categorical data!) but the \\(\\chi^2\\) distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when \\(df = 1\\), the goodness-of-fit statistic tends to be “too big”, meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).\nAs far as I can tell from reading Yates’ paper14, the correction is basically a hack. It’s not derived from any principled theory. Rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the ‘Statistics’ options, where it is called ‘\\(\\chi^2\\) continuity correction’."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#effect-size",
    "href": "10-Categorical-data-analysis.html#effect-size",
    "title": "10  Categorical data analysis",
    "section": "10.4 Effect size",
    "text": "10.4 Effect size\nAs we discussed earlier in Section 9.8, it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?\nThere are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them but will instead focus on the most commonly reported measures of effect size.\nBy default, the two measures that people tend to report most frequently are the \\(\\phi\\) statistic and the somewhat superior version, known as Cramér’s \\(V\\) .\n[Additional technical detail 15]\nAnd you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly. With Cramér’s \\(V\\), you know that the value really does range from 0 (no association at all) to 1 (perfect association)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#assumptions-of-the-tests",
    "href": "10-Categorical-data-analysis.html#assumptions-of-the-tests",
    "title": "10  Categorical data analysis",
    "section": "10.5 Assumptions of the test(s)",
    "text": "10.5 Assumptions of the test(s)\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:\n\nExpected frequencies are sufficiently large. Remember how in the previous section we saw that the \\(\\chi^2\\) sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter 7 this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran (1954)) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative (Larntz, 1978).\nData are independent of one another. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that everyone selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic” (Tversky & Kahneman, 1974). This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.\n\nIf you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-fisher-exact-test",
    "href": "10-Categorical-data-analysis.html#the-fisher-exact-test",
    "title": "10  Categorical data analysis",
    "section": "10.6 The Fisher exact test",
    "text": "10.6 The Fisher exact test\nWhat should you do if your cell counts are too small, but you’d still like to test the null hypothesis that the two variables are independent? One answer would be “collect more data”, but that’s far too glib There are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, Fisher (1922) kindly provided the right answer to the question. To illustrate the basic idea let’s suppose that we’re analysing data from a field experiment looking at the emotional status of people who have been accused of Witchcraft, some of whom are currently being burned at the stake.16 Unfortunately for the scientist (but rather fortunately for the general populace), it’s actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. A contingency table of the salem.csv data illustrates the point (Table 10.9).\n\n\n\n\nTable 10.9:  Contingency table of the salem.csv data \n\nhappyFALSETRUE\n\non.fireFALSE310\n\nTRUE30\n\n\n\n\n\nLooking at this data, you’d be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. So, speaking as someone who doesn’t want to be set on fire, I’d really like to be able to get a better answer than this. This is where Fisher’s exact test (Fisher, 1922) comes in very handy.\nThe Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn’t have a test statistic, but it calculates the p-value “directly”. I’ll explain the basics of how the test works for a \\(2 \\times 2\\) contingency table. As before, let’s have some notation (Table 10.10).\n\n\n\n\nTable 10.10:  Notation for the Fisher exact test \n\nHappySadTotal\n\nSet on fire\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nNot set on fire\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)\\(N\\)\n\n\n\n\n\nIn order to construct the test Fisher treats both the row and column totals \\((R_1, R_2, C_1 \\text{ and } C_2)\\) as known, fixed quantities and then calculates the probability that we would have obtained the observed frequencies that we did \\((O_{11}, O_{12}, O_{21} \\text{ and } O_{22})\\) given those totals. In the notation that we developed in Chapter 7 this is written:\n\\[P(O_{11}, O_{12}, O_{21}, O_{22}  \\text{ | } R_1, R_2, C_1, C_2)\\] and as you might imagine, it’s a slightly tricky exercise to figure out what this probability is. But it turns out that this probability is described by a distribution known as the hypergeometric distribution. What we have to do to calculate our p-value is calculate the probability of observing this particular table or a table that is “more extreme”. 17 Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it’s pretty easy as long as the tables aren’t too big and the sample size isn’t too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more “extreme” than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the p-value.\nYou can specify this test in jamovi from a check box in the ‘Statistics’ options of the ‘Contingency Tables’ analysis. When you do this with the data from the salem.csv file, the Fisher exact test statistic is shown in the results. The main thing we’re interested in here is the p-value, which in this case is small enough (p = .036) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire. See Figure 10.7.\n\n\n\n\n\nFigure 10.7: Fisher exact test analysis in jamovi"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-The-McNemar-test",
    "href": "10-Categorical-data-analysis.html#sec-The-McNemar-test",
    "title": "10  類別資料分析",
    "section": "10.7 麥內瑪檢定",
    "text": "10.7 麥內瑪檢定\nSuppose you’ve been hired to work for the Australian Generic Political Party (AGPP), and part of your job is to find out how effective the AGPP political advertisements are. So you decide to put together a sample of \\(N = 100\\) people and ask them to watch the AGPP ads. Before they see anything, you ask them if they intend to vote for the AGPP, and then after showing the ads you ask them again to see if anyone has changed their minds. Obviously, if you’re any good at your job, you’d also do a whole lot of other things too, but let’s consider just this one simple experiment. One way to describe your data is via the contingency table shown in Table 10.11.\n\n\n\n\nTable 10.11:  Contingency table with data on AGPP political advertisements \n\nBeforeAfterTotal\n\nYes301040\n\nNo7090160\n\nTotal100100200\n\n\n\n\n\nAt first pass, you might think that this situation lends itself to the Pearson \\(\\chi^2\\) test of independence (as per [The \\(\\chi^2\\) test of independence (or association)]). However, a little bit of thought reveals that we’ve got a problem. We have 100 participants but 200 observations. This is because each person has provided us with an answer in both the before column and the after column. What this means is that the 200 observations aren’t independent of each other. If voter A says “yes” the first time and voter B says “no”, then you’d expect that voter A is more likely to say “yes” the second time than voter B! The consequence of this is that the usual \\(\\chi^2\\) test won’t give trustworthy answers due to the violation of the independence assumption. Now, if this were a really uncommon situation, I wouldn’t be bothering to waste your time talking about it. But it’s not uncommon at all. This is a standard repeated measures design, and none of the tests we’ve considered so far can handle it. Eek.\nThe solution to the problem was published by McNemar (1947). The trick is to start by tabulating your data in a slightly different way (Table 10.12).\n\n\n\n\nTable 10.12:  Tabulate the data in a different way when you have repeated measures data \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes5510\n\nAfter: No256590\n\nTotal3070100\n\n\n\n\n\nNext, let’s think about what our null hypothesis is: it’s that the “before” test and the “after” test have the same proportion of people saying “Yes, I will vote for AGPP”. Because of the way that we have rewritten the data, it means that we’re now testing the hypothesis that the row totals and column totals come from the same distribution. Thus, the null hypothesis in McNemar’s test is that we have “marginal homogeneity”. That is, the row totals and column totals have the same distribution: \\(P_a + P_b = P_a + P_c\\) and similarly that \\(P_c + P_d = P_b + P_d\\). Notice that this means that the null hypothesis actually simplifies to Pb = Pc. In other words, as far as the McNemar test is concerned, it’s only the off-diagonal entries in this table (i.e., b and c) that matter! After noticing this, the 麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity) is no different to a usual \\(\\chi^2\\) test. After applying the Yates correction, our test statistic becomes:\n\\[\\chi^2=\\frac{(|b-c|-0.5)^2}{b+c}\\] or, to revert to the notation that we used earlier in this chapter:\n\\[\\chi^2=\\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}\\] and this statistic has a \\(\\chi^2\\) distribution (approximately) with df = 1. However, remember that just like the other \\(\\chi^2\\) tests it’s only an approximation, so you need to have reasonably large expected cell counts for it to work.\n\n10.7.1 實作麥內瑪檢定\nNow that you know what the McNemar test is all about, lets actually run one. The agpp.csv file contains the raw data that I discussed previously. The agpp data set contains three variables, an id variable that labels each participant in the data set (we’ll see why that’s useful in a moment), a response_before variable that records the person’s answer when they were asked the question the first time, and a response_after variable that shows the answer that they gave when asked the same question a second time. Notice that each participant appears only once in this data set. Go to the ‘Analyses’ - ‘Frequencies’ - ‘Contingency Tables’ - ‘Paired Samples’ analysis in jamovi, and move response_before into the ‘Rows’ box, and response_after into the ‘Columns’ box. You will then get a contingency table in the results window, with the statistic for the McNemar test just below it, see Figure 10.8.\n\n\n\n\n\nFigure 10.8: McNemar test output in jamovi\n\n\n\n\nAnd we’re done. We’ve just run a McNemar’s test to determine if people were just as likely to vote AGPP after the ads as they were before hand. The test was significant (\\(\\chi^2(1)= 12.03, p< .001)\\), suggesting that they were not. And, in fact it looks like the ads had a negative effect: people were less likely to vote AGPP after seeing the ads. Which makes a lot of sense when you consider the quality of a typical political advertisement.\n\n\n10.7.2 與獨立性檢定有可分別?\nLet’s go all the way back to the beginning of the chapter and look at the cards data set again. If you recall, the actual experimental design that I described involved people making two choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice (Table 10.13).\n\n\n\n\nTable 10.13:  Cross-tabulating first against second choice with the Randomness.omv (cards) data \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes\\(a \\)\\(b \\)\\(a + b \\)\n\nAfter: No\\(c  \\)\\(d  \\)\\(c + d  \\)\n\nTotal\\(a+c  \\)\\(b+d  \\)\\(n  \\)\n\n\n\n\n\nSuppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we’re trying to do is see if there’s some relationship between the rows and columns of this table.\nAlternatively, suppose I wanted to know if on average, the frequencies of suit choices were different the second time than the first time. In that situation, what I’m really trying to see is if the row totals are different from the column totals. That’s when you use the McNemar test.\nThe different statistics produced by these different analyses are shown in Figure 10.9. Notice that the results are different! These aren’t the same test.\n\n\n\n\n\nFigure 10.9: Independent vs. Paired (McNemar) with the Randomness.omv (cards) data"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#whats-the-difference-between-mcnemar-and-independence",
    "href": "10-Categorical-data-analysis.html#whats-the-difference-between-mcnemar-and-independence",
    "title": "10  Categorical data analysis",
    "section": "10.8 What’s the difference between McNemar and independence?",
    "text": "10.8 What’s the difference between McNemar and independence?\nLet’s go all the way back to the beginning of the chapter and look at the cards data set again. If you recall, the actual experimental design that I described involved people making two choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice (Table 10.13).\n\n\n\n\nTable 10.13:  Cross-tabulating first against second choice with the Randomness.omv (cards) data \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes\\(a \\)\\(b \\)\\(a + b \\)\n\nAfter: No\\(c  \\)\\(d  \\)\\(c + d  \\)\n\nTotal\\(a+c  \\)\\(b+d  \\)\\(n  \\)\n\n\n\n\n\nSuppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we’re trying to do is see if there’s some relationship between the rows and columns of this table.\nAlternatively, suppose I wanted to know if on average, the frequencies of suit choices were different the second time than the first time. In that situation, what I’m really trying to see is if the row totals are different from the column totals. That’s when you use the McNemar test.\nThe different statistics produced by these different analyses are shown in Figure 10.9. Notice that the results are different! These aren’t the same test.\n\n\n\n\n\nFigure 10.9: Independent vs. Paired (McNemar) with the Randomness.omv (cards) data"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#summary",
    "href": "10-Categorical-data-analysis.html#summary",
    "title": "10  類別資料分析",
    "section": "10.8 Summary",
    "text": "10.8 Summary\nThe key ideas discussed in this chapter are:\n\n[The \\(\\chi^2\\) (chi-square) goodness-of-fit test] is used when you have a table of observed frequencies of different categories, and the null hypothesis gives you a set of “known” probabilities to compare them to.\n[The \\(\\chi^2\\) test of independence (or association)] is used when you have a contingency table (cross-tabulation) of two categorical variables. The null hypothesis is that there is no relationship or association between the variables.\n[Effect size] for a contingency table can be measured in several ways. In particular we noted the Cramér’s \\(V\\) statistic.\nBoth versions of the Pearson test rely on two assumptions: that the expected frequencies are sufficiently large, and that the observations are independent ([Assumptions of the test(s)]. [The Fisher exact test] can be used when the expected frequencies are small. [The McNemar test] can be used for some kinds of violations of independence.\n\nIf you’re interested in learning more about categorical data analysis a good first choice would be Agresti (1996) which, as the title suggests, provides an Introduction to Categorical Data Analysis. If the introductory book isn’t enough for you (or can’t solve the problem you’re working on) you could consider Agresti (2002), Categorical Data Analysis. The latter is a more advanced text, so it’s probably not wise to jump straight from this book to that one.\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nCramer, H. (1946). Mathematical methods of statistics. Princeton University Press.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "11-Comparing-two-means.html",
    "href": "11-Comparing-two-means.html",
    "title": "11  比較兩組平均值",
    "section": "",
    "text": "In Chapter 10 we covered the situation when your outcome variable is nominal scale and your predictor variable is also nominal scale. Lots of real world situations have that character, and so you’ll find that chi-square tests in particular are quite widely used. However, you’re much more likely to find yourself in a situation where your outcome variable is interval scale or higher, and what you’re interested in is whether the average value of the outcome variable is higher in one group or another. For instance, a psychologist might want to know if anxiety levels are higher among parents than non-parents, or if working memory capacity is reduced by listening to music (relative to not listening to music). In a medical context we might want to know if a new drug increases or decreases blood pressure. An agricultural scientist might want to know whether adding phosphorus to Australian native plants will kill them.1 In all these situations our outcome variable is a fairly continuous, interval or ratio scale variable, and our predictor is a binary “grouping” variable. In other words, we want to compare the means of the two groups.\nThe standard answer to the problem of comparing means is to use a t-test, of which there are several varieties depending on exactly what question you want to solve. As a consequence, the majority of this chapter focuses on different types of t-test: one sample t-tests, independent samples t-tests and paired samples t-tests. We’ll then talk about one sided tests and, after that, we’ll talk a bit about Cohen’s d, which is the standard measure of effect size for a t-test. The later sections of the chapter focus on the assumptions of the t-tests, and possible remedies if they are violated. However, before discussing any of these useful things, we’ll start with a discussion of the z-test."
  },
  {
    "objectID": "11-Comparing-two-means.html#the-one-sample-z-test",
    "href": "11-Comparing-two-means.html#the-one-sample-z-test",
    "title": "11  Comparing two means",
    "section": "11.1 The one-sample z-test",
    "text": "11.1 The one-sample z-test\nIn this section I’ll describe one of the most useless tests in all of statistics: the z-test. Seriously – this test is almost never used in real life. Its only real purpose is that, when teaching statistics, it’s a very convenient stepping stone along the way towards the t-test, which is probably the most (over)used tool in all statistics.\n\n11.1.1 The inference problem that the test addresses\nTo introduce the idea behind the z-test, let’s use a simple example. A friend of mine, Dr Zeppo, grades his introductory statistics class on a curve. Let’s suppose that the average grade in his class is \\(67.5\\), and the standard deviation is \\(9.5\\). Of his many hundreds of students, it turns out that 20 of them also take psychology classes. Out of curiosity, I find myself wondering if the psychology students tend to get the same grades as everyone else (i.e., mean \\(67.5\\)) or do they tend to score higher or lower? He emails me the zeppo.csv file, which I use to look at the grades of those students, in the jamovi spreadsheet view,and then calculate the mean in ‘Exploration’ - ‘Descriptives’ 2. The mean value is \\(72.3\\).\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\nHmm. It might be that the psychology students are scoring a bit higher than normal. That sample mean of \\(\\bar{X} = 72.3\\) is a fair bit higher than the hypothesised population mean of \\(\\mu = 67.5\\) but, on the other hand, a sample size of \\(N = 20\\) isn’t all that big. Maybe it’s pure chance.\nTo answer the question, it helps to be able to write down what it is that I think I know. Firstly, I know that the sample mean is \\(\\bar{X} = 72.3\\). If I’m willing to assume that the psychology students have the same standard deviation as the rest of the class then I can say that the population standard deviation is \\(\\sigma = 9.5\\). I’ll also assume that since Dr Zeppo is grading to a curve, the psychology student grades are normally distributed.\nNext, it helps to be clear about what I want to learn from the data. In this case my research hypothesis relates to the population mean \\(\\mu\\) for the psychology student grades, which is unknown. Specifically, I want to know if \\(\\mu = 67.5\\) or not. Given that this is what I know, can we devise a hypothesis test to solve our problem? The data, along with the hypothesised distribution from which they are thought to arise, are shown in Figure 11.1. Not entirely obvious what the right answer is, is it? For this, we are going to need some statistics.\n\n\n\n\n\nFigure 11.1: The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated\n\n\n\n\n\n\n11.1.2 Constructing the hypothesis test\nThe first step in constructing a hypothesis test is to be clear about what the null and alternative hypotheses are. This isn’t too hard to do. Our null hypothesis, \\(H_0\\), is that the true population mean \\(\\mu\\) for psychology student grades is \\(67.5\\%\\), and our alternative hypothesis is that the population mean isn’t \\(67.5\\%\\). If we write this in mathematical notation, these hypotheses become:\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\nthough to be honest this notation doesn’t add much to our understanding of the problem, it’s just a compact way of writing down what we’re trying to learn from the data. The null hypotheses \\(H_0\\) and the alternative hypothesis \\(H_1\\) for our test are both illustrated in Figure 11.2. In addition to providing us with these hypotheses, the scenario outlined above provides us with a fair amount of background knowledge that might be useful. Specifically, there are two special pieces of information that we can add:\n\nThe psychology grades are normally distributed.\nThe true standard deviation of these scores \\(\\sigma\\) is known to be 9.5.\n\nFor the moment, we’ll act as if these are absolutely trustworthy facts. In real life, this kind of absolutely trustworthy background knowledge doesn’t exist, and so if we want to rely on these facts we’ll just have make the assumption that these things are true. However, since these assumptions may or may not be warranted, we might need to check them. For now though, we’ll keep things simple.\n\n\n\n\n\nFigure 11.2: Graphical illustration of the null and alternate hypotheses assumed by the one sample \\(z\\)-test (the two sided version, that is). The null and alternate hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value \\(\\$sigma_0\\)). The null hypothesis (left) is that the population mean \\(\\mu\\) is equal to some specified value \\(\\mu_0\\). The alternative hypothesis (right) is that the population mean differs from this value, \\(\\mu \\neq \\mu_0\\)\n\n\n\n\nThe next step is to figure out what we would be a good choice for a diagnostic test statistic, something that would help us discriminate between \\(H_0\\) and \\(H_1\\). Given that the hypotheses all refer to the population mean \\(\\mu\\), you’d feel pretty confident that the sample mean \\(\\bar{X}\\) would be a pretty useful place to start. What we could do is look at the difference between the sample mean \\(\\bar{X}\\) and the value that the null hypothesis predicts for the population mean. In our example that would mean we calculate \\(\\bar{X} - 67.5\\). More generally, if we let \\(\\mu_0\\) refer to the value that the null hypothesis claims is our population mean, then we’d want to calculate\n\\[\\bar{X}-\\mu_0\\]\nIf this quantity equals or is very close to 0, things are looking good for the null hypothesis. If this quantity is a long way away from 0, then it’s looking less likely that the null hypothesis is worth retaining. But how far away from zero should it be for us to reject H0?\nTo figure that out we need to be a bit more sneaky, and we’ll need to rely on those two pieces of background knowledge that I wrote down previously; namely that the raw data are normally distributed and that we know the value of the population standard deviation \\(\\sigma\\). If the null hypothesis is actually true, and the true mean is \\(\\mu_0\\), then these facts together mean that we know the complete population distribution of the data: a normal distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma\\).3\nOkay, if that’s true, then what can we say about the distribution of \\(\\bar{X}\\)? Well, as we discussed earlier (see Section 8.3.3), the sampling distribution of the mean \\(\\bar{X}\\) is also normal, and has mean \\(\\mu\\). But the standard deviation of this sampling distribution \\(\\\\{se(\\bar{X})\\\\}\\), which is called the standard error of the mean, is 4\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\nNow comes the trick. What we can do is convert the sample mean \\(\\bar{X}\\) into a standard score (see Section 4.5). This is conventionally written as z, but for now I’m going to refer to it as \\(z_{\\bar{X}}\\). The reason for using this expanded notation is to help you remember that we’re calculating a standardised version of a sample mean, not a standardised version of a single observation, which is what a z-score usually refers to). When we do so the z-score for our sample mean is\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\] or, equivalently \\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\nThis z-score is our test statistic. The nice thing about using this as our test statistic is that like all z-scores, it has a standard normal distribution:5\nIn other words, regardless of what scale the original data are on, the z-statistic itself always has the same interpretation: it’s equal to the number of standard errors that separate the observed sample mean \\(\\bar{X}\\) from the population mean \\(\\mu_0\\) predicted by the null hypothesis. Better yet, regardless of what the population parameters for the raw scores actually are, the 5% critical regions for the z-test are always the same, as illustrated in Figure 11.3. And what this meant, way back in the days where people did all their statistics by hand, is that someone could publish a table like Table 11.1. This, in turn, meant that researchers could calculate their z-statistic by hand and then look up the critical value in a text book.\n\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\nTable 11.1:  Critical values for different alpha levels \n\ncritical z value\n\ndesired \\(\\alpha\\) leveltwo-sided testone-sided test\n\n.11.6448541.281552\n\n.051.9599641.644854\n\n.012.5758292.326348\n\n.0013.2905273.090232\n\n\n\n\n\n\n\n11.1.3 A worked example, by hand\nNow, as I mentioned earlier, the z-test is almost never used in practice. It’s so rarely used in real life that the basic installation of jamovi doesn’t have a built in function for it. However, the test is so incredibly simple that it’s really easy to do one manually. Let’s go back to the data from Dr Zeppo’s class. Having loaded the grades data, the first thing I need to do is calculate the sample mean, which I’ve already done (\\(72.3\\)). We already have the known population standard deviation (\\(\\sigma = 9.5\\)), and the value of the population mean that the null hypothesis specifies (\\(\\mu_0 = 67.5\\)), and we know the sample size (\\(N=20\\)).\n\n\n\n\n\nFigure 11.3: Rejection regions for the two-sided z-test (panel (a)) and the one-sided z-test (panel (b))\n\n\n\n\nNext, let’s calculate the (true) standard error of the mean (easily done with a calculator):\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\nAnd finally, we calculate our z-score:\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\nAt this point, we would traditionally look up the value \\(2.26\\) in our table of critical values. Our original hypothesis was two-sided (we didn’t really have any theory about whether psych students would be better or worse at statistics than other students) so our hypothesis test is two-sided (or two-tailed) also. Looking at the little table that I showed earlier, we can see that \\(2.26\\) is bigger than the critical value of \\(1.96\\) that would be required to be significant at \\(\\alpha = .05\\), but smaller than the value of \\(2.58\\) that would be required to be significant at a level of \\(\\alpha = .01\\). Therefore, we can conclude that we have a significant effect, which we might write up by saying something like this:\n\nWith a mean grade of \\(73.2\\) in the sample of psychology students, and assuming a true population standard deviation of \\(9.5\\), we can conclude that the psychology students have significantly different statistics scores to the class average (\\(z = 2.26, N = 20, p<.05\\)).\n\n\n\n11.1.4 Assumptions of the z-test\nAs I’ve said before, all statistical tests make assumptions. Some tests make reasonable assumptions, while other tests do not. The test I’ve just described, the one sample z-test, makes three basic assumptions. These are:\n\nNormality. As usually described, the z-test assumes that the true population distribution is normal.6 This is often a pretty reasonable assumption, and it’s also an assumption that we can check if we feel worried about it (see Section on Checking the normality of a sample).\nIndependence. The second assumption of the test is that the observations in your data set are not correlated with each other, or related to each other in some funny way. This isn’t as easy to check statistically, it relies a bit on good experimental design. An obvious (and stupid) example of something that violates this assumption is a data set where you “copy” the same observation over and over again in your data file so that you end up with a massive “sample size”, which consists of only one genuine observation. More realistically, you have to ask yourself if it’s really plausible to imagine that each observation is a completely random sample from the population that you’re interested in. In practice this assumption is never met, but we try our best to design studies that minimise the problems of correlated data.\nKnown standard deviation. The third assumption of the z-test is that the true standard deviation of the population is known to the researcher. This is just stupid. In no real world data analysis problem do you know the standard deviation σ of some population but are completely ignorant about the mean \\(\\mu\\). In other words, this assumption is always wrong.\n\nIn view of the stupidity of assuming that \\(\\alpha\\) is known, let’s see if we can live without it. This takes us out of the dreary domain of the z-test, and into the magical kingdom of the t-test, with unicorns and fairies and leprechauns!"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-The-one-sample-t-test",
    "href": "11-Comparing-two-means.html#sec-The-one-sample-t-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.2 單一樣本t檢定",
    "text": "11.2 單一樣本t檢定\n經過一番思考，我認為不能安全地假設心理學生的成績標準差與 Zeppo 博士班上其他學生的一樣。畢竟，如果我假設它們的平均值不同，為什麼我要相信它們的標準差絕對相同？基於這個原因，我真的應該停止假設我知道 \\(\\sigma\\) 的真實值。這違反了我的 z-test 的假設，因此從某種意義上說，我回到了起點。但我還是有一些選擇。畢竟，我還有我的原始數據，而這些原始數據可以給我一個對人口標準差的估計，它是 9.52。換句話說，雖然我不能說我知道 \\(\\sigma = 9.5\\)，但我可以說 \\(\\hat{\\sigma}\\) = 9.52。\n好的，這很不錯。你可能會想到的明顯做法是，使用 \\(9.52\\) 的估計標準差來進行 z 檢定，而不是依賴於我對真實標準差為 \\(9.5\\) 的假設。你可能不會感到意外，這種方法仍然會給我們一個顯著的結果。這種方法很接近，但不是完全正確的。因為現在我們依賴於對人群標準差的估計，所以我們需要對我們對真實人群標準差的不確定性進行一些調整。也許我們的數據只是一個偶然事件……也許真實人群標準差是 \\(11\\)。但如果這確實是真的，我們進行 z 檢定時假設 \\(\\sigma=11\\)，那麼結果將變得不顯著。這是一個問題，我們需要解決。\n\n\n\n\n\n\nFigure 11.4: 一樣從圖形上來解釋單樣本t檢驗所假設的虛無假設和對立假設（兩側）。需要注意的是，它和z檢驗有相似之處（見 Figure 11.2 ）。虛無假設是，母體平均值\\(\\mu\\)等於某個特定值\\(\\mu_0\\)，對立假設是不等於\\(\\mu_0\\)。與z檢驗一樣，我們假設數據服從正態分佈，但不假設母體標準差\\(\\sigma\\)事先已知。\n\n\n\n\n\n11.2.1 深入認識單一樣本t檢定\n這種模稜兩可的情況非常煩人，而這在 1908 年被 William Sealy Gosset 所解決 (Student, 1908)。當時他正在干吉尼斯啤酒廠擔任化學家（參見 Box (1987)）。由於吉尼斯公司不贊成其員工發表統計分析（顯然他們認為這是商業機密），所以他用化名“學生”發表了這項工作。直到今天，t 檢定的全名實際上是 Student’s t-test。Gosset 所發現的關鍵是，我們應該如何容納我們不完全確定真實標準差的事實。答案是它微妙地改變了抽樣分布。8在 t 檢定中，我們的檢定統計量，現在稱為 t 統計量，以我上面提到的方式計算。如果我們的虛無假設是真實均值為 \\(\\mu\\)，但我們的樣本平均值是 \\(\\bar{X}\\)，而我們對人口標準差的估計是 \\(\\hat{\\sigma}\\)，那麼我們的 t 統計量是：\n\n\\[\nt=\\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}\n\\]\n\n\n\n\n\nFigure 11.5: 具有2個自由度（左）和10個自由度（右）的t分布，標準正態分布（即，均值為0，標準差為1）以虛線繪製，以進行比較。請注意，t分布的尾部比正態分布重（高峰度），當自由度非常小時，這種效應會被極大地夸大，但對於較大的值可以忽略不計。換句話說，對於大的自由度，t分布基本上與正態分布相同。\n\n\n\n\n等式中唯一變化的是我們使用估計值 \\(\\hat{\\sigma}\\) 取代已知的真實值 \\(\\sigma\\)。而如果這個估計值是由 N 個觀察值所構成，那麼樣本分布就會變成一個有 \\(N-1\\) 個自由度的 t 分布。t 分布和正態分布非常相似，但有更重的尾部，正如 Section 7.6 中討論的，也在 Figure 11.5 中呈現。然而，注意到當自由度變大時，t 分布開始看起來和標準正態分布相同。這就是我們期望的：如果樣本大小為 \\(N = 70,000,000\\)，那麼你對標準差的「估計」就幾乎完美了，對吧？因此，你應該期望在大樣本時，t 檢驗的表現會和 z 檢驗完全一致。而這也確實如此！\n\n\n\n11.2.2 實作單一樣本t檢定\n正如你所預期的，t檢定的機制幾乎與z檢定的機制完全相同。因此，沒有必要繁瑣地展示如何使用低級命令進行計算。它與我們之前所做的計算幾乎相同，只是使用了估計的標準差，然後使用t分佈而不是正態分佈來測試假設。因此，我將直接跳到顯示如何實際進行t檢定。jamovi配備了一個專門用於t檢定的分析，非常靈活（它可以運行許多不同類型的t檢定）。它非常簡單易用；您需要做的只是指定“分析”-“T檢定”-“單樣本T檢定”，將您感興趣的變量（X）移動到“變量”框中，並在“假設”-“測試值”框中輸入零假設的平均值（‘67.5’）。很容易。見@fig-fig11-6，除了我們稍後會提到的其他事項之外，它還提供了t檢定統計量=2.25，自由度為19，相關的p值為\\(0.036\\)。\n\n\n\n\n\n\nFigure 11.6: jamovi執行單一樣本t檢定示範\n\n\n\n\n此外，報告了其他兩個可能會引起關注的指標：95％置信區間和效應大小的測量值（我們稍後會談論效應大小）。看起來相當簡單。現在我們該如何處理這個輸出呢？既然我們假裝真的在乎我們的例子，我們很高興地發現結果在統計上是顯著的（即p值低於0.05）。我們可以通過這樣的報告方式來表達結果：\n\n心理學生的平均成績為 \\(72.3\\)，略高於平均成績\\(67.5\\) (\\(t(19) = 2.25\\)，\\(p = .036\\))。平均分差為\\(4.80\\)，\\(95\\%\\)置信區間為\\(0.34\\)至\\(9.26\\)。\n\n…其中 \\(t(19)\\) 是一個簡寫符號，代表具有 \\(19\\) 自由度的 t 統計量。話雖如此，人們通常不會報告信賴區間，或者使用比我在此展示的更簡短的形式進行報告。例如，看到信賴區間被包含在報告均值差之後的統計資訊中也不是很少見，就像這樣：\n\\[t(19)=2.25, p = .036, CI_{95} = [0.34, 9.26]\\]\n在這半行內塞這麼多專業術語，你就知道這必定是非常聰明的。9\n\n\n\n11.2.3 單一樣本t檢定的適用條件\n好的，那麼單樣本 t 檢驗有什麼假設呢？因為 t 檢驗基本上就是一個移除了已知標準差假設的 z 檢驗，你不應該感到意外，看到它做出了與 z 檢驗相同的假設，減去了已知標準差的假設，那就是：\n\n正態性。我們仍然假設母體分布是正態的 10，如前所述，有標準的工具可以用來檢查這種假設是否符合（檢查樣本正態性），如果這種假設被違反，還有其他的測試方法可以替代（檢驗非正態數據）。\n獨立性。再次，我們必須假設樣本中的觀察值是相互獨立的產生的。詳細討論請參見之前有關 z 檢驗的討論（z 檢驗的假設）。\n\n總的來說，這兩個假設並不是非常不合理的，因此單樣本 t 檢定在實踐中被廣泛用於比較樣本平均數與假定的母體平均數。"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-student-test",
    "href": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-student-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.3 獨立樣本t檢定(學生t檢定)",
    "text": "11.3 獨立樣本t檢定(學生t檢定)\n儘管單樣本 t 檢驗有其用處，但它並不是 t 檢驗的最典型範例。11更常見的情況是當你有兩個不同的觀測組時。在心理學中，這往往對應於研究中的兩個不同條件，每個條件對應於一個不同的參與者組。對於研究中的每個人，您測量一些感興趣的結果變量，而您要問的研究問題是這兩組是否具有相同的母體平均值。這就是獨立樣本 t 檢驗所設計的情況。\n\n\n11.3.1 使用獨立t檢定的狀況\n假設我們有 33 名學生參加 Harpo 博士的統計學講座，而 Harpo 博士沒有按曲線分數。實際上，Harpo 博士的評分有點神秘，因此我們對整個課程的平均成績一無所知。該課程有兩名導師，Anastasia 和 Bernadette。Anastasia 的輔導課有 \\(N_1=15\\) 名學生，Bernadette 的輔導課有 \\(N_2=18\\) 名學生。我感興趣的研究問題是 Anastasia 和 Bernadette 誰是更好的導師，或者他們的教學效果沒有太大的差異。Harpo 博士將課程成績發送給我，存在 harpo.csv 文件中。像往常一樣，我會將文件加載到 jamovi 中，查看它包含哪些變量 - 有三個變量，ID、grade 和 tutor。grade 變量包含每個學生的成績，但它未按正確的測量層級屬性導入到 jamovi 中，因此我需要更改它以使其被視為連續變量（參見 Section 3.6）。tutor 變量是一個因子，表示每個學生的導師是 Anastasia 還是 Bernadette。\n我們可以使用 ‘Exploration’ - ‘descriptives’ 分析計算平均值和標準差，這裡是一個不錯的摘要表格（Table 11.2）。\n\n\n\n\n\n\nTable 11.2:  兩位統計助教帶的學生成績描述統計 \n\nmeanstd devN\n\nAnastasia's students74.539.0015\n\nBernadette's students69.065.7718\n\n\n\n\n\n為了讓你更清楚知道正在發生什麼，我在 jamovi 中繪製了盒形圖和小提琴圖，並在圖表中添加了平均分數的小實心方塊。這些圖表顯示了兩位助教的學生成績分佈 (Figure 11.7)。\n\n\n\n11.3.2 深入認識獨立t檢定\n獨立樣本 t 檢定有兩種不同的形式，學生 t 檢定和韋爾奇 t 檢定。原始的學生 t 檢定是兩者中比較簡單的一種，但比韋爾奇 t 檢定依賴更為嚴格的假設。假設你現在要進行雙側檢定，目標是確定兩個「獨立樣本」是否來自具有相同平均值（虛無假設）或不同平均值（對立假設）的母體。當我們說「獨立樣本」時，我們真正意思是這兩個樣本之間沒有特殊的關係。現在可能還不太明白，但等到我們談到成對樣本 t 檢定時，這一點就會更清楚了。現在，讓我們指出，如果我們有一個實驗設計，其中參與者是隨機分配到兩組中的一組，而我們想比較這兩組對某個結果度量的平均表現，那麼獨立樣本 t 檢定（而不是成對樣本 t 檢定）就是我們所需要的。\n\n\n\n\n\n\nFigure 11.7: 盒形圖和小提琴圖顯示Anastasia和Bernadette的班級學生成績分布。從視覺上看，這些圖表暗示著Anastasia的學生平均成績可能會更好，但它們也似乎更加變化不定。\n\n\n\n\n好的，讓我們讓 \\(\\mu_1\\) 代表第一組的真實母體平均值（例如 Anastasia 的學生），而 \\(\\mu_2\\) 則是第二組的真實母體平均值（例如 Bernadette 的學生）12，通常我們讓 \\(\\bar{X_1}\\) 和 \\(\\bar{X_2}\\) 代表這兩個組別的觀察樣本平均值。我們的虛無假設是兩個母體平均值是相同的（\\(\\mu_1 = \\mu_2\\)），而對立假設則是它們不相同（\\(\\mu_1 \\neq \\mu_2\\)）（Figure 11.8）。以數學術語來說，這是：\n\n\n\n\n\n\nFigure 11.8: 學生t檢定所假設的零假設和替代假設的圖形示意。零假設假設兩組的平均值 \\(\\mu\\) 相同，而替代假設則假設兩組的平均值 \\(\\mu_1\\) 和 \\(\\mu_2\\) 不同。請注意，假設母體分配為正態分配，且儘管替代假設允許兩組平均值不同，但假設它們具有相同的標準差。\n\n\n\n\n建立一個能夠處理這種情況的假設檢驗，我們開始注意到，如果零假設成立，那麼兩個母體平均數之間的差異就是 正好 為零，\\(\\mu_1-\\mu_2 = 0\\)。因此，診斷性的檢定統計量將基於兩個樣本平均數之間的差異。因為如果零假設成立，那麼我們預期 \\(\\bar{X}_1 - \\bar{X}_2\\) 要非常接近於零。然而，就像我們在單樣本測試中看到的那樣（即單樣本 z 檢定和單樣本 t 檢定），我們必須精確地確定這種差異應該接近於零。解決問題的方法幾乎相同。我們計算一個標準誤差估計值（SE），然後將平均值之間的差異除以這個估計值。因此，我們的 t 檢定統計量 將具有以下形式：\n\\[t=\\frac{\\bar{X_1}-\\bar{X_2}}{SE}\\]\n我們需要找出這個標準誤差估計值是什麼。這比我們之前介紹過的兩個檢定複雜得多，因此我們需要更仔細地研究它的運作方式。\n\n\n\n11.3.3 標準差的合併估計\n在原始的「Student t 檢定」中，我們假設兩個群體有相同的母體標準差。換句話說，無論母體平均數是否相同，我們假設母體標準差是相同的，\\(\\sigma_1 = \\sigma_2\\)。由於我們假設兩個標準差相同，因此我們省略下標，將它們都稱為 \\(\\sigma\\)。我們該如何估計這個值？當我們有兩個樣本時，該如何建構標準差的單一估計值？答案是，基本上我們將它們加以平均。好吧，有點。實際上，我們取一個加權平均的變異數估計值，它被用作我們的合併變異數的估計值。分配給每個樣本的權重等於該樣本中的觀察數量減去 1。\n[其他技術細節13]"
  },
  {
    "objectID": "11-Comparing-two-means.html#completing-the-test",
    "href": "11-Comparing-two-means.html#completing-the-test",
    "title": "11  Comparing two means",
    "section": "11.4 Completing the test",
    "text": "11.4 Completing the test\nRegardless of which way you want to think about it, we now have our pooled estimate of the standard deviation. From now on, I’ll drop the silly p subscript, and just refer to this estimate as \\(\\hat{\\sigma}\\). Great. Let’s now go back to thinking about the bloody hypothesis test, shall we? Our whole reason for calculating this pooled estimate was that we knew it would be helpful when calculating our standard error estimate. But standard error of what? In the one-sample t-test it was the standard error of the sample mean, \\(se(\\bar{X})\\), and since \\(se(\\bar{X}) = \\frac{\\sigma}{\\sqrt{N}}\\) that’s what the denominator of our t-statistic looked like. This time around, however, we have two sample means. And what we’re interested in, specifically, is the the difference between the two \\(\\bar{X}_1-\\bar{X}_2\\) As a consequence, the standard error that we need to divide by is in fact the standard error of the difference between means.\n[Additional technical detail 13]\nJust as we saw with our one-sample test, the sampling distribution of this t-statistic is a t-distribution (shocking, isn’t it?) as long as the null hypothesis is true and all of the assumptions of the test are met. The degrees of freedom, however, is slightly different. As usual, we can think of the degrees of freedom to be equal to the number of data points minus the number of constraints. In this case, we have N observations (\\(N_1\\) in sample 1, and \\(N_2\\) in sample 2), and 2 constraints (the sample means). So the total degrees of freedom for this test are \\(N - 2\\).\n\n11.4.1 Doing the test in jamovi\nNot surprisingly, you can run an independent samples t-test easily in jamovi. The outcome variable for our test is the student grade, and the groups are defined in terms of the tutor for each class. So you probably won’t be too surprised that all you have to do in jamovi is go to the relevant analysis (‘Analyses’ - ‘T-Tests’ - ‘Independent Samples T-Test’) and move the grade variable across to the ‘Dependent Variables’ box, and the tutor variable across into the ‘Grouping Variable’ box, as shown in Figure 11.10.\n\n\n\n\n\nFigure 11.10: Independent t-test in jamovi, with options checked for useful results\n\n\n\n\nThe output has a very familiar form. First, it tells you what test was run, and it tells you the name of the dependent variable that you used. It then reports the test results. Just like last time the test results consist of a t-statistic, the degrees of freedom, and the p-value. The final section reports two things: it gives you a confidence interval and an effect size. I’ll talk about effect sizes later. The confidence interval, however, I should talk about now.\nIt’s pretty important to be clear on what this confidence interval actually refers to. It is a confidence interval for the difference between the group means. In our example, Anastasia’s students had an average grade of \\(74.53\\), and Bernadette’s students had an average grade of \\(69.06\\), so the difference between the two sample means is \\(5.48\\). But of course the difference between population means might be bigger or smaller than this. The confidence interval reported in Figure 11.10 tells you that there’s a if we replicated this study again and again, then \\(95\\%\\) of the time the true difference in means would lie between \\(0.20\\) and \\(10.76\\). Look back at Section 8.5 for a reminder about what confidence intervals mean.\nIn any case, the difference between the two groups is significant (just barely), so we might write up the result using text like this:\n\nThe mean grade in Anastasia’s class was \\(74.5\\%\\) (std dev = \\(9.0\\)), whereas the mean in Bernadette’s class was \\(69.1\\%\\) (std dev = \\(5.8\\)). A Student’s independent samples t-test showed that this \\(5.4\\%\\) difference was significant \\((t(31) = 2.1, p<.05, CI_{95} = [0.2, 10.8], d = .74)\\), suggesting that a genuine difference in learning outcomes has occurred.\n\nNotice that I’ve included the confidence interval and the effect size in the stat block. People don’t always do this. At a bare minimum, you’d expect to see the t-statistic, the degrees of freedom and the p value. So you should include something like this at a minimum: \\(t(31) = 2.1, p< .05\\). If statisticians had their way, everyone would also report the confidence interval and probably the effect size measure too, because they are useful things to know. But real life doesn’t always work the way statisticians want it to so you should make a judgement based on whether you think it will help your readers and, if you’re writing a scientific paper, the editorial standard for the journal in question. Some journals expect you to report effect sizes, others don’t. Within some scientific communities it is standard practice to report confidence intervals, in others it is not. You’ll need to figure out what your audience expects. But, just for the sake of clarity, if you’re taking my class, my default position is that it’s usually worth including both the effect size and the confidence interval.\n\n\n11.4.2 Positive and negative t values\nBefore moving on to talk about the assumptions of the t-test, there’s one additional point I want to make about the use of t-tests in practice. The first one relates to the sign of the t-statistic (that is, whether it is a positive number or a negative one). One very common worry that students have when they start running their first t-test is that they often end up with negative values for the t-statistic and don’t know how to interpret it. In fact, it’s not at all uncommon for two people working independently to end up with results that are almost identical, except that one person has a negative t values and the other one has a positive t value. Assuming that you’re running a two-sided test then the p-values will be identical. On closer inspection, the students will notice that the confidence intervals also have the opposite signs. This is perfectly okay. Whenever this happens, what you’ll find is that the two versions of the results arise from slightly different ways of running the t-test. What’s happening here is very simple. The t-statistic that we calculate here is always of the form\n\\[t=\\frac{\\text{mean 1-mean 2}}{SE}\\]\nIf “mean 1” is larger than “mean 2” the t statistic will be positive, whereas if “mean 2” is larger then the t statistic will be negative. Similarly, the confidence interval that jamovi reports is the confidence interval for the difference “(mean 1) minus (mean 2)”, which will be the reverse of what you’d get if you were calculating the confidence interval for the difference “(mean 2) minus (mean 1)”.\nOkay, that’s pretty straightforward when you think about it, but now consider our t-test comparing Anastasia’s class to Bernadette’s class. Which one should we call “mean 1” and which one should we call “mean 2”. It’s arbitrary. However, you really do need to designate one of them as “mean 1” and the other one as “mean 2”. Not surprisingly, the way that jamovi handles this is also pretty arbitrary. In earlier versions of the book I used to try to explain it, but after a while I gave up, because it’s not really all that important and to be honest I can never remember myself. Whenever I get a significant t-test result, and I want to figure out which mean is the larger one, I don’t try to figure it out by looking at the t-statistic. Why would I bother doing that? It’s foolish. It’s easier just to look at the actual group means since the jamovi output actually shows them!\nHere’s the important thing. Because it really doesn’t matter what jamovi shows you, I usually try to report the t-statistic in such a way that the numbers match up with the text. Suppose that what I want to write in my report is: Anastasia’s class had higher grades than Bernadette’s class. The phrasing here implies that Anastasia’s group comes first, so it makes sense to report the t-statistic as if Anastasia’s class corresponded to group 1. If so, I would write Anastasia’s class had higher grades than Bernadette’s class \\((t(31) = 2.1, p = .04)\\).\n(I wouldn’t actually underline the word “higher” in real life, I’m just doing it to emphasise the point that “higher” corresponds to positive t values). On the other hand, suppose the phrasing I wanted to use has Bernadette’s class listed first. If so, it makes more sense to treat her class as group 1, and if so, the write up looks like this: Bernadette’s class had lower grades than Anastasia’s class \\((t(31) = -2.1, p = .04)\\).\nBecause I’m talking about one group having “lower” scores this time around, it is more sensible to use the negative form of the t-statistic. It just makes it read more cleanly.\nOne last thing: please note that you can’t do this for other types of test statistics. It works for t-tests, but it wouldn’t be meaningful for chi-square tests, F-tests or indeed for most of the tests I talk about in this book. So don’t over-generalise this advice! I’m really just talking about t-tests here and nothing else!\n\n\n11.4.3 Assumptions of the test\nAs always, our hypothesis test relies on some assumptions. So what are they? For the Student t-test there are three assumptions, some of which we saw previously in the context of the one sample t-test (see Assumptions of the one sample t-test):\n\nNormality. Like the one-sample t-test, it is assumed that the data are normally distributed. Specifically, we assume that both groups are normally distributed14. In the section on Checking the normality of a sample we’ll discuss how to test for normality, and in Testing non-normal data we’ll discuss possible solutions.\nIndependence. Once again, it is assumed that the observations are independently sampled. In the context of the Student test this has two aspects to it. Firstly, we assume that the observations within each sample are independent of one another (exactly the same as for the one-sample test). However, we also assume that there are no cross-sample dependencies. If, for instance, it turns out that you included some participants in both experimental conditions of your study (e.g., by accidentally allowing the same person to sign up to different conditions), then there are some cross sample dependencies that you’d need to take into account.\nHomogeneity of variance (also called “homoscedasticity”). The third assumption is that the population standard deviation is the same in both groups. You can test this assumption using the Levene test, which I’ll talk about later on in the book (in Section 13.6.1). However, there’s a very simple remedy for this assumption if you are worried, which I’ll talk about in the next section."
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-welch-test",
    "href": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-welch-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.5 獨立樣本t檢定(Welch t檢定)",
    "text": "11.5 獨立樣本t檢定(Welch t檢定)\n在實際應用中，使用學生 t 檢定的最大問題是前一節所列的第三個假設。它假設兩組具有相同的標準差，但在現實生活中，這種情況很少發生。如果兩個樣本的平均值不同，為什麼我們期望它們具有相同的標準差呢？實際上，我們沒有理由期望這個假設成立。我們稍後會談到如何檢查這個假設，因為它在幾個不同的地方會出現，不僅僅是 t 檢定。但現在我會談論一下不依賴這個假設的 t 檢定 (Welch, 1947) 的不同形式。Figure 11.10 以圖形方式說明了 Welch t 檢定對數據的假設，以與 Figure 11.8 中的學生 t 檢定進行對比。我承認在談診斷之前談治療有點奇怪，但事實上 Welch 檢定可以在 jamovi 的「獨立樣本 t 檢定」選項中指定，所以這可能是談論它的最好地方。\n\n\n\n\n\n\nFigure 11.10: 圖形說明了 Welch t 檢定所假設的虛無假說和替代假說。和 Student t-test（見圖@fig-fig11-9）一樣，我們假設兩個樣本都是從一個正態分佈的母體中抽取的; 但是，替代假說不再需要兩個母體具有相等的變異數。\n\n\n\n\nWelch 檢定與 Student 檢定非常相似。例如，在 Welch 檢定中，我們使用的 t 統計量的計算方式與 Student 檢定非常相似。也就是說，我們取樣本平均數之間的差異，然後除以其標準誤的估計：\n\\[t=\\frac{\\bar{X}_1-\\bar{X}_2}{SE(\\bar{X}_1-\\bar{X}_2)}\\]\n主要的差異在於標準誤的計算方式不同。如果兩個母體具有不同的標準差，那麼嘗試計算合併的標準差估計是完全荒謬的，因為你正在平均蘋果和橘子。16\n[其他技術細節 17]\n\\[SE(\\bar{X}_1-\\bar{X}_2)=\\sqrt{\\frac{\\hat{\\sigma}_1^2}{N_1}+\\frac{\\hat{\\sigma}_2^2}{N_2}}\\]\n為什麼是這樣計算的原因超出了本書的範圍。對我們而言重要的是，從 Welch t-test 得出的 t 統計量實際上與從 Student t-test 得出的 t 統計量略有不同。\nWelch 與 Student 之間的第二個差異在於，計算自由度的方式非常不同。在 Welch 測試中，「自由度」不再必須是一個整數，並且它不太符合「數據點數量減去約束數量」的啟發式方法，這是我到目前為止一直在使用的。\n\n\n11.5.1 jamovi實作\n如果你在上面的分析中勾選了 Welch test 的核取方塊，則會得到以下結果（見@fig-fig11-11）。\n\n\n\n\n\n\nFigure 11.11: jamovi並列學生t檢定與Welch t檢定結果。\n\n\n\n\n這個輸出結果的解釋應該很明顯。您可以像讀取學生t檢驗的結果一樣讀取Welch檢驗的輸出。你有你的描述統計數據，測試結果和其他信息。所以這一切都相當容易。\n除了，除了… 我們的結果不再顯著。 當我們執行Student t-test時，我們得到了一個顯著的效應，但在相同的數據集上進行的Welch測試並不顯著 \\((t(23.02) = 2.03, p = .054)\\)。 這是什麼意思？ 我們應該恐慌嗎？ 天空是否燃燒？ 可能不是。 一個測試顯著，而另一個測試不顯著本身並沒有什麼意義，特別是因為我有點編造了這些數據。 通常，不要試圖解釋或解釋\\(.049\\)和\\(.051\\)之間的p值差異。 如果這種情況在現實生活中發生，這些p值之間的差異幾乎肯定是由於偶然性引起的。 重要的是，您在選擇使用哪種測試時要仔細思考。 Student測試和Welch測試各有優勢和劣勢。 如果兩個母體確實具有相等的變異數，則Student測試比Welch測試略具備更高的功效（較低的II型錯誤率）。 但是，如果它們的變異數不同，則Student測試的假設就會被違反，您可能無法信任它。 您可能會得到更高的I型錯誤率。 因此，這是一個權衡。 但是，在現實生活中，我通常更喜歡使用Welch測試，因為幾乎沒有人認為母體變異數是相同的。\n\n\n\n11.5.2 Welch檢定的適用條件\nWelch檢定的假設與學生t檢定的假設非常相似（請參閱獨立樣本平均數檢定的適用條件），唯一不同的是Welch檢定不需要假定變異數相同。這只剩下正態性和獨立性的假設，這些假設的細節對於Welch檢定和學生t檢定來說是相同的。"
  },
  {
    "objectID": "11-Comparing-two-means.html#the-paired-samples-t-test",
    "href": "11-Comparing-two-means.html#the-paired-samples-t-test",
    "title": "11  Comparing two means",
    "section": "11.6 The paired-samples t-test",
    "text": "11.6 The paired-samples t-test\nRegardless of whether we’re talking about the Student test or the Welch test, an independent samples t-test is intended to be used in a situation where you have two samples that are, well, independent of one another. This situation arises naturally when participants are assigned randomly to one of two experimental conditions, but it provides a very poor approximation to other sorts of research designs. In particular, a repeated measures design, in which each participant is measured (with respect to the same outcome variable) in both experimental conditions, is not suited for analysis using independent samples t-tests. For example, we might be interested in whether listening to music reduces people’s working memory capacity. To that end, we could measure each person’s working memory capacity in two conditions: with music, and without music. In an experimental design such as this one, 17 each participant appears in both groups. This requires us to approach the problem in a different way, by using the paired samples t-test.\n\n11.6.1 The data\nThe data set that we’ll use this time comes from Dr Chico’s class.18 In her class students take two major tests, one early in the semester and one later in the semester. To hear her tell it, she runs a very hard class, one that most students find very challenging. But she argues that by setting hard assessments students are encouraged to work harder. Her theory is that the first test is a bit of a “wake up call” for students. When they realise how hard her class really is, they’ll work harder for the second test and get a better mark. Is she right? To test this, let’s import the chico.csv file into jamovi. This time jamovi does a good job during the import of attributing measurement levels correctly. The chico data set contains three variables: an id variable that identifies each student in the class, the grade_test1 variable that records the student grade for the first test, and the grade_test2 variable that has the grades for the second test.\nIf we look at the jamovi spreadsheet it does seem like the class is a hard one (most grades are between 50% and 60%), but it does look like there’s an improvement from the first test to the second one.\nIf we take a quick look at the descriptive statistics, in Figure 11.13, we see that this impression seems to be supported. Across all 20 students the mean grade for the first test is 57%, but this rises to 58% for the second test. Although, given that the standard deviations are 6.6% and 6.4% respectively, it’s starting to feel like maybe the improvement is just illusory; maybe just random variation. This impression is reinforced when you see the means and confidence intervals plotted in Figure 11.14 (a). If we were to rely on this plot alone, looking at how wide those confidence intervals are, we’d be tempted to think that the apparent improvement in student performance is pure chance.\n\n\n\n\n\nFigure 11.13: Descriptives for the two grade test variables in the chico data set\n\n\n\n\nNevertheless, this impression is wrong. To see why, take a look at the scatterplot of the grades for test 1 against the grades for test 2, shown in Figure 11.14 (b). In this plot each dot corresponds to the two grades for a given student. If their grade for test 1 (x co-ordinate) equals their grade for test 2 (y co-ordinate), then the dot falls on the line. Points falling above the line are the students that performed better on the second test. Critically, almost all of the data points fall above the diagonal line: almost all of the students do seem to have improved their grade, if only by a small amount. This suggests that we should be looking at the improvement made by each student from one test to the next and treating that as our raw data. To do this, we’ll need to create a new variable for the improvement that each student makes, and add it to the chico data set. The easiest way to do this is to compute a new variable, with the expression grade test2 - grade test1.\n\n\n\n\n\nFigure 11.14: Mean grade for test 1 and test 2, with associated 95% confidence intervals (panel (a)). Scatterplot showing the individual grades for test 1 and test 2 (panel (b)). Histogram showing the improvement made by each student in Dr Chico’s class (panel (c)). In panel (c), notice that almost the entire distribution is above zero - the vast majority of students did improve their performance from the first test to the second one\n\n\n\n\nOnce we have computed this new improvement variable we can draw a histogram showing the distribution of these improvement scores, shown in Figure 11.14 (c). When we look at the histogram, it’s very clear that there is a real improvement here. The vast majority of the students scored higher on test 2 than on test 1, reflected in the fact that almost the entire histogram is above zero.\n\n\n11.6.2 What is the paired samples t-test?\nIn light of the previous exploration, let’s think about how to construct an appropriate t test. One possibility would be to try to run an independent samples t-test using grade_test1 and grade_test2 as the variables of interest. However, this is clearly the wrong thing to do as the independent samples t-test assumes that there is no particular relationship between the two samples. Yet clearly that’s not true in this case because of the repeated measures structure in the data. To use the language that I introduced in the last section, if we were to try to do an independent samples t-test, we would be conflating the within subject differences (which is what we’re interested in testing) with the between subject variability (which we are not).\nThe solution to the problem is obvious, I hope, since we already did all the hard work in the previous section. Instead of running an independent samples t-test on grade_test1 and grade_test2, we run a one-sample t-test on the within-subject difference variable, improvement. To formalise this slightly, if \\(X_{i1}\\) is the score that the i-th participant obtained on the first variable, and \\(X_{i2}\\) is the score that the same person obtained on the second one, then the difference score is:\n\\[D_i=X_{i1}-X_{i2}\\]\nNotice that the difference scores is variable 1 minus variable 2 and not the other way around, so if we want improvement to correspond to a positive valued difference, we actually want “test 2” to be our “variable 1”. Equally, we would say that \\(\\mu_D = \\mu_1 - \\mu_2\\) is the population mean for this difference variable. So, to convert this to a hypothesis test, our null hypothesis is that this mean difference is zero and the alternative hypothesis is that it is not\n\\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]\nThis is assuming we’re talking about a two-sided test here. This is more or less identical to the way we described the hypotheses for the one-sample t-test. The only difference is that the specific value that the null hypothesis predicts is 0. And so our t-statistic is defined in more or less the same way too. If we let \\(\\bar{D}\\) denote the mean of the difference scores, then\n\\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\] which is \\[t=\\frac{\\bar{D}}{\\frac{\\hat{\\sigma}_D}{\\sqrt{N}}}\\]\nwhere \\(\\hat{\\sigma}_D\\) is the standard deviation of the difference scores. Since this is just an ordinary, one-sample t-test, with nothing special about it, the degrees of freedom are still \\(N - 1\\). And that’s it. The paired samples t-test really isn’t a new test at all. It’s a one-sample t-test, but applied to the difference between two variables. It’s actually very simple. The only reason it merits a discussion as long as the one we’ve just gone through is that you need to be able to recognise when a paired samples test is appropriate, and to understand why it’s better than an independent samples t test.\n\n\n11.6.3 Doing the test in jamovi\nHow do you do a paired samples t-test in jamovi? One possibility is to follow the process I outlined above. That is, create a “difference” variable and then run a one sample t-test on that. Since we’ve already created a variable called improvement, let’s do that and see what we get, Figure 11.15.\n\n\n\n\n\nFigure 11.15: Results showing a one sample t-test on paired difference scores\n\n\n\n\nThe output shown in Figure 11.15 is (obviously) formatted exactly the same was as it was the last time we used the one-sample t-Test analysis (Section 11.2), and it confirms our intuition. There’s an average improvement of \\(1.4\\%\\) from test 1 to test 2, and this is significantly different from \\(0\\) \\((t(19) = 6.48, p< .001)\\).\nHowever, suppose you’re lazy and you don’t want to go to all the effort of creating a new variable. Or perhaps you just want to keep the difference between one-sample and paired samples tests clear in your head. If so, you can use the jamovi ‘Paired Samples T-Test’ analysis, getting the results shown in Figure 11.16.\n\n\n\n\n\nFigure 11.16: Results showing a paired sample t-test. Compare with Figure 11.15\n\n\n\n\nThe numbers are identical to those that come from the one sample test, which of course they have to be given that the paired samples t-test is just a one sample test under the hood."
  },
  {
    "objectID": "11-Comparing-two-means.html#one-sided-tests",
    "href": "11-Comparing-two-means.html#one-sided-tests",
    "title": "11  Comparing two means",
    "section": "11.7 One-sided tests",
    "text": "11.7 One-sided tests\nWhen introducing the theory of null hypothesis tests, I mentioned that there are some situations when it’s appropriate to specify a one-sided test (see Section 9.4.3). So far all of the t-tests have been two-sided tests. For instance, when we specified a one sample t-test for the grades in Dr Zeppo’s class the null hypothesis was that the true mean was \\(67.5\\%\\). The alternative hypothesis was that the true mean was greater than or less than \\(67.5\\%\\). Suppose we were only interested in finding out if the true mean is greater than \\(67.5\\%\\), and have no interest whatsoever in testing to find out if the true mean is lower than \\(67.5\\%\\). If so, our null hypothesis would be that the true mean is \\(67.5\\%\\) or less, and the alternative hypothesis would be that the true mean is greater than \\(67.5\\%\\). In jamovi, for the ‘One Sample T-Test’ analysis, you can specify this by clicking on the ‘\\(>\\) Test Value’ option, under ‘Hypothesis’. When you have done this, you will get the results as shown in Figure 11.17.\n\n\n\n\n\nFigure 11.17: jamovi results showing a ‘One Sample T-Test’ where the actual hypothesis is one sided, i.e. that the true mean is greater than \\(67.5\\%\\)\n\n\n\n\nNotice that there are a few changes from the output that we saw last time. Most important is the fact that the actual hypothesis has changed, to reflect the different test. The second thing to note is that although the t-statistic and degrees of freedom have not changed, the p-value has. This is because the one-sided test has a different rejection region from the two-sided test. If you’ve forgotten why this is and what it means, you may find it helpful to read back over Chapter 9, and Section 9.4.3 in particular. The third thing to note is that the confidence interval is different too: it now reports a “one-sided” confidence interval rather than a two-sided one. In a two-sided confidence interval we’re trying to find numbers a and b such that we’re confident that, if we were to repeat the study many times, then \\(95\\%\\) of the time the mean would lie between a and b. In a one-sided confidence interval, we’re trying to find a single number a such that we’re confident that \\(95\\%\\) of the time the true mean would be greater than a (or less than a if you selected Measure 1 < Measure 2 in the ‘Hypothesis’ section).\nSo that’s how to do a one-sided one sample t-test. However, all versions of the t-test can be one-sided. For an independent samples t test, you could have a one-sided test if you’re only interested in testing to see if group A has higher scores than group B, but have no interest in finding out if group B has higher scores than group A. Let’s suppose that, for Dr Harpo’s class, you wanted to see if Anastasia’s students had higher grades than Bernadette’s. For this analysis, in the ‘Hypothesis’ options, specify that ‘Group 1 > Group2’. You should get the results shown in Figure 11.18.\n\n\n\n\n\nFigure 11.18: jamovi results showing an ‘Independent Samples t-Test’ where the actual hypothesis is one sided, i.e. that Anastasia’s students had higher grades than Bernadette’s\n\n\n\n\nAgain, the output changes in a predictable way. The definition of the alternative hypothesis has changed, the p-value has changed, and it now reports a one-sided confidence interval rather than a two-sided one.\nWhat about the paired samples t-test? Suppose we wanted to test the hypothesis that grades go up from test 1 to test 2 in Dr Zeppo’s class, and are not prepared to consider the idea that the grades go down. In jamovi you would do this by specifying, under the ‘Hypotheses’ option, that grade_test2 (‘Measure 1’ in jamovi, because we copied this first into the paired variables box) > grade test1 (‘Measure 2’ in jamovi). You should get the results shown in Figure 11.19.\n\n\n\n\n\nFigure 11.19: jamovi results showing a ‘Paired Samples T-Test’ where the actual hypothesis is one sided, i.e. that grade test2 (‘Measure 1’) \\(>\\) grade test1 (‘Measure 2’)\n\n\n\n\nYet again, the output changes in a predictable way. The hypothesis has changed, the p-value has changed, and the confidence interval is now one-sided."
  },
  {
    "objectID": "11-Comparing-two-means.html#effect-size",
    "href": "11-Comparing-two-means.html#effect-size",
    "title": "11  Comparing two means",
    "section": "11.8 Effect size",
    "text": "11.8 Effect size\nThe most commonly used measure of effect size for a t-test is Cohen’s d (Cohen, 1988). It’s a very simple measure in principle, with quite a few wrinkles when you start digging into the details. Cohen himself defined it primarily in the context of an independent samples t-test, specifically the Student test. In that context, a natural way of defining the effect size is to divide the difference between the means by an estimate of the standard deviation. In other words, we’re looking to calculate something along the lines of this:\n\\[d=\\frac{(\\text{mean 1})-(\\text{mean 2})}{\\text{std dev}}\\]\nand he suggested a rough guide for interpreting \\(d\\) in Table 11.3.\n\n\n\n\nTable 11.3:  A (very) rough guide to interpreting Cohen’s d. My personal recommendation is to not use these blindly. The d statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it’s generally a good idea to think about what that means in practical terms. In some contexts a ‘small’ effect could be of big practical importance. In other situations a ‘large’ effect may not be all that interesting \n\nd-valuerough interpretation\n\nabout 0.2\"small\" effect\n\nabout 0.5\"moderate\" effect\n\nabout 0.8\"large\" effect\n\n\n\n\n\nYou’d think that this would be pretty unambiguous, but it’s not. This is largely because Cohen wasn’t too specific on what he thought should be used as the measure of the standard deviation (in his defence he was trying to make a broader point in his book, not nitpick about tiny details). As discussed by McGrath & Meyer (2006), there are several different versions in common usage, and each author tends to adopt slightly different notation. For the sake of simplicity (as opposed to accuracy), I’ll use d to refer to any statistic that you calculate from the sample, and use \\(\\delta\\) to refer to a theoretical population effect. Obviously, that does mean that there are several different things all called d.\nMy suspicion is that the only time that you would want Cohen’s d is when you’re running a t-test, and jamovi has an option to calculate the effect size for all the different flavours of t-test it provides.\n\n11.8.1 Cohen’s d from one sample\nThe simplest situation to consider is the one corresponding to a one-sample t-test. In this case, this is the one sample mean \\(\\bar{X}\\) and one (hypothesised) population mean \\(\\mu_0\\) to compare it to. Not only that, there’s really only one sensible way to estimate the population standard deviation. We just use our usual estimate \\(\\hat{\\sigma}\\). Therefore, we end up with the following as the only way to calculate \\(d\\)\n\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]\nWhen we look back at the results in Figure 11.6, the effect size value is Cohen’s \\(d = 0.50\\). Overall, then, the psychology students in Dr Zeppo’s class are achieving grades (\\(mean = 72.3\\%\\)) that are about .5 standard deviations higher than the level that you’d expect (\\(67.5\\%\\)) if they were performing at the same level as other students. Judged against Cohen’s rough guide, this is a moderate effect size.\n\n\n11.8.2 Cohen’s d from a Student’s t test\nThe majority of discussions of Cohen’s \\(d\\) focus on a situation that is analogous to Student’s independent samples t test, and it’s in this context that the story becomes messier, since there are several different versions of \\(d\\) that you might want to use in this situation. To understand why there are multiple versions of \\(d\\), it helps to take the time to write down a formula that corresponds to the true population effect size \\(\\delta\\). It’s pretty straightforward,\n\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\]\nwhere, as usual, \\(\\mu_1\\) and \\(\\mu_2\\) are the population means corresponding to group 1 and group 2 respectively, and \\(\\sigma\\) is the standard deviation (the same for both populations). The obvious way to estimate \\(\\delta\\) is to do exactly the same thing that we did in the t-test itself, i.e., use the sample means as the top line and a pooled standard deviation estimate for the bottom line\n\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\nwhere \\(\\hat{\\sigma}_p\\) is the exact same pooled standard deviation measure that appears in the t-test. This is the most commonly used version of Cohen’s d when applied to the outcome of a Student t-test, and is the one provided in jamovi. It is sometimes referred to as Hedges’ \\(g\\) statistic (Hedges, 1981).\nHowever, there are other possibilities which I’ll briefly describe. Firstly, you may have reason to want to use only one of the two groups as the basis for calculating the standard deviation. This approach (often called Glass’ \\(\\triangle\\), pronounced delta) only makes most sense when you have good reason to treat one of the two groups as a purer reflection of “natural variation” than the other. This can happen if, for instance, one of the two groups is a control group. Secondly, recall that in the usual calculation of the pooled standard deviation we divide by \\(N - 2\\) to correct for the bias in the sample variance. In one version of Cohen’s d this correction is omitted, and instead we divide by \\(N\\). This version makes sense primarily when you’re trying to calculate the effect size in the sample rather than estimating an effect size in the population. Finally, there is a version called Hedge’s g, based on Hedges & Olkin (1985), who point out there is a small bias in the usual (pooled) estimation for Cohen’s d.19\nIn any case, ignoring all those variations that you could make use of if you wanted, let’s have a look at the default version in jamovi. In Figure 11.10 Cohen’s \\(d = 0.74\\), indicating that the grade scores for students in Anastasia’s class are, on average, \\(0.74\\) standard deviations higher than the grade scores for students in Bernadette’s class. For a Welch test, the estimated effect size is the same (Figure 11.12).\n\n\n11.8.3 Cohen’s d from a paired-samples test\nFinally, what should we do for a paired samples t-test? In this case, the answer depends on what it is you’re trying to do. jamovi assumes that you want to measure your effect sizes relative to the distribution of difference scores, and the measure of d that you calculate is:\n\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\nwhere \\(\\hat{\\sigma}_D\\) is the estimate of the standard deviation of the differences. In Figure 11.16 Cohen’s \\(d = 1.45\\), indicating that the time 2 grade scores are, on average, \\(1.45\\) standard deviations higher than the time 1 grade scores.\nThis is the version of Cohen’s \\(d\\) that gets reported by the jamovi ‘Paired Samples T-Test’ analysis. The only wrinkle is figuring out whether this is the measure you want or not. To the extent that you care about the practical consequences of your research, you often want to measure the effect size relative to the original variables, not the difference scores (e.g., the 1% improvement in Dr Chico’s class over time is pretty small when measured against the amount of between-student variation in grades), in which case you use the same versions of Cohen’s d that you would use for a Student or Welch test. It’s not so straightforward to do this in jamovi; essentially you have to change the structure of the data in the spreadsheet view so I won’t go into that here20, but the Cohen’s d for this perspective is quite different: it is \\(0.22\\) which is quite small when assessed on the scale of the original variables."
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-Checking-the-normality-of-a-sample",
    "href": "11-Comparing-two-means.html#sec-Checking-the-normality-of-a-sample",
    "title": "11  比較單一與兩組平均值",
    "section": "11.9 檢測樣本常態性",
    "text": "11.9 檢測樣本常態性\n迄今為止，在本章中討論的所有檢驗都假定數據呈正態分佈。這個假設通常是相當合理的，因為中心極限定理（見 Section 8.3.3 ）確實傾向於確保許多現實世界的數量呈正態分佈。任何時候你懷疑你的變量實際上是很多不同事物的平均值時，它很可能呈正態分佈，或者至少接近正態，以便你可以使用t檢驗。然而，生活中沒有保證，而且還有很多方法可以讓你得到非常非正態的變量。例如，任何時候你認為你的變量實際上是很多不同事物的最小值，它很可能會變得非常偏斜。在心理學中，反應時間（RT）數據就是一個很好的例子。如果你認為有很多事情可能觸發一個人類參與者的反應，那麼實際的反應將在這些觸發事件中的第一次發生時發生。22這意味著RT數據是系統性地非正態的。好吧，那麼，如果正態性是所有檢驗的假設，並且在現實世界的數據中大多數（至少近似地）得到滿足，那麼我們如何檢查樣本的正態性呢？在本節中，我將討論兩種方法：分位數對分位數圖(Q-Q plot)和Shapiro-Wilk檢驗。\n\n\n11.9.1 分位數對分位數圖\n檢查樣本是否違反正態性假設的一種方法是繪製“Q-Q圖”（分位數對分位數圖，Quantile-Quantile plot）。這可以讓您直觀地檢查是否有任何系統性違規行為。在QQ圖中，每個觀察值都作為一個單獨的點繪製。x坐標是觀察值在數據正態分佈（用樣本估計均值和方差）的情況下應該落入的理論分位數，y坐標是數據在樣本中的實際分位數。如果數據是正態的，點應該形成一條直線。例如，讓我們看看如果我們通過從正態分佈中抽樣生成數據，然後繪製QQ圖會發生什麼。結果顯示在@fig-fig11-20。\n\n\n\n\n\n\nFigure 11.20: 直方圖（圖板(a)）和正態QQ圖（圖板(b)）顯示 normal.data 是具有 \\(100\\) 個觀測值的正態分佈樣本。與這些數據相關的 Shapiro-Wilk 統計量是 \\(W = .99\\)，表示未檢測到顯著偏離正態性（\\(p = .54\\)）。\n\n\n\n\n如您所見，這些數據形成了一條非常直線；這並不奇怪，因為我們從正態分佈中抽樣得到它們！相比之下，請看@fig-fig11-21中顯示的兩個數據集。上面的面板顯示了一個高度偏斜的數據集的直方圖和QQ圖：QQ圖向上彎曲。下面的面板顯示了同樣的圖，但對於一個尾部較重（即高峰度）的數據集：在這種情況下，QQ圖在中間變平並在兩端急劇彎曲。\n\n\n\n\n\n\nFigure 11.21: 在第一行，顯示有偏的數據集的 \\(100\\) 個觀測值的直方圖和正態QQ圖。這裡的數據偏度為 \\(1.88\\)，並在向上彎曲的QQ圖中反映出來。因此，Shapiro-Wilk統計量為 \\(W = .80\\)，反映了與正態性的顯著偏差（\\(p < .001\\)）。底部的行顯示了同樣的圖，但對於尾部較重的數據集，同樣包含100個觀測值。在這種情況下，數據中的重尾產生了高峰度（\\(6.57\\)），並使QQ圖在中間變平，在兩側急劇彎曲。由此產生的Shapiro-Wilk統計量為 \\(W = .75\\)，同樣反映了顯著的非正態性（\\(p < .001\\)）。\n\n\n\n\n\n\n11.9.2 t檢定的分位數對分位數圖\n在之前的分析中，我們展示了如何在 jamovi 中進行獨立 t 檢驗（Figure 11.10）和配對樣本 t 檢驗（Figure 11.16）。對於這些分析，jamovi 提供了顯示差異得分的 QQ 圖選項（jamovi 稱之為「殘差」），這是檢查正態性假設的更好方法。當我們選擇這些分析的此選項時，我們將分別獲得 Figure 11.22 和 Figure 11.23 中顯示的 QQ 圖。我的解釋是，這兩個圖都表明差異得分基本上呈正態分佈，所以我們可以繼續了！\n\n\n\n\n\n\nFigure 11.22: 獨立t檢驗分析的QQ圖 參考 Figure 11.10\n\n\n\n\n\n\n\n\n\nFigure 11.23: 配對樣本t檢驗分析的QQ圖 參考 Figure 11.16\n\n\n\n\n\n\n11.9.3 Shapiro-Wilk檢定\nQQ圖提供了一種非正式檢查數據正態性的好方法，但有時您可能需要做更正式的檢查，Shapiro-Wilk檢定(Shapiro & Wilk, 1965)可能正是您正在尋找的。23如您所料，被檢驗的虛無假設是一組\\(N\\)個觀測值是正態分佈的。\n[其他技術細節 24]\n\n\n\n\n\n\nFigure 11.24: 在樣本大小為10、20和50的情況下，據擷取的Shapiro-Wilk \\(W\\)統計量分布，據此推出空假設是數據呈正態分布。注意，\\(W\\)值較小表示偏離正態分布。\n\n\n\n\n該檢驗統計量通常表示為\\(W\\)，其計算方法如下。首先，我們按照大小遞增的順序對觀測值進行排序，讓\\(\\bar{X_1}\\)成為樣本中最小的值，\\(X_2\\)成為第二小的值，依此類推。然後\\(W\\)的值由以下公式給出\\[W=\\frac{(\\sum_{i=1}^N a_iX_i)^2}{\\sum_{i=1}^N(X_i-\\bar{X})^2}\\]其中\\(\\bar{X}\\)是觀測值的平均值，\\(a_i\\)值是…咕嚕咕嚕…比較複雜，超出了入門級教材的範疇。\n\n\n\n11.9.4 檢測樣本常態性的範例\n與此同時，當數據變得非正態時，展示一下 QQ 圖和 Shapiro-Wilk 檢定的變化可能是值得的。為此，讓我們看一下我們的澳式足球聯賽（AFL）勝利幅度數據的分佈，如果您還記得@sec-描述性統計，它看起來根本不是來自正態分佈。以下是 QQ 圖發生的情況（Figure 11.25）。\n\n\n\n\n\n\nFigure 11.25: 展示 AFL 勝利幅度數據非正態性的 QQ 圖\n\n\n\n\n而當我們對 AFL margins 數據執行 Shapiro-Wilk 檢定時，我們得到了 Shapiro-Wilk 正態檢定統計量的值 \\(W = 0.94\\)，p值 = \\(9.481\\)x\\(10^{-07}\\)。顯然是一個顯著的效應！"
  },
  {
    "objectID": "11-Comparing-two-means.html#testing-non-normal-data",
    "href": "11-Comparing-two-means.html#testing-non-normal-data",
    "title": "11  Comparing two means",
    "section": "11.10 Testing non-normal data",
    "text": "11.10 Testing non-normal data\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 Two sample Mann-Whitney U test\nI’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.4).\n\n\n\n\nTable 11.4:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 24 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for the Mann-Whitney \\(U\\) test\n\n\n\n\n\n\n11.10.2 One sample Wilcoxon test\nWhat about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5.\n\n\n\n\nTable 11.5:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\npositive differences7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests"
  },
  {
    "objectID": "11-Comparing-two-means.html#summary",
    "href": "11-Comparing-two-means.html#summary",
    "title": "11  Comparing two means",
    "section": "11.11 Summary",
    "text": "11.11 Summary\n\nThe one-sample t-test is used to compare a single sample mean against a hypothesised value for the population mean.\nAn independent samples t-test is used to compare the means of two groups, and tests the null hypothesis that they have the same mean. It comes in two forms: The independent samples t-test (Student test) assumes that the groups have the same standard deviation, The independent samples t-test (Welch test) does not.\nThe paired-samples t-test is used when you have two scores from each person, and you want to test the null hypothesis that the two scores have the same mean. It is equivalent to taking the difference between the two scores for each person, and then running a one sample t-test on the difference scores.\nOne-sided tests are perfectly legitimate as long as they are pre-planned (like all tests!).\nEffect size calculations for the difference between means can be calculated via the Cohen’s d statistic.\nChecking the normality of a sample using QQ plots and the Shapiro-Wilk test.\nIf your data are non-normal, you can use Mann-Whitney or Wilcoxon tests instead of t-tests for Testing non-normal data.\n\n\n\n\n\nBox, J. F. (1987). Guinness, gosset, fisher, and small samples. Statistical Science, 2, 45–52.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nHedges, L. V. (1981). Distribution theory for glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6, 107–128.\n\n\nHedges, L. V., & Olkin, I. (1985). Statistical methods for meta-analysis. Academic Press.\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree: The case of \\(r\\) and \\(d\\). Psychological Methods, 11, 386–401.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52, 591–611.\n\n\nStudent, A. (1908). The probable error of a mean. Biometrika, 6, 1–2.\n\n\nWelch, B. L. (1947). The generalization of “Student’s” problem when several different population variances are involved. Biometrika, 34, 28–35."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方適合度檢定",
    "href": "10-Categorical-data-analysis.html#卡方適合度檢定",
    "title": "10  類別資料分析",
    "section": "10.1 卡方適合度檢定",
    "text": "10.1 卡方適合度檢定\nThe \\(\\chi^2\\) goodness-of-fit test is one of the oldest hypothesis tests around. It was invented by Karl Pearson around the turn of the century (Pearson, 1900), with some corrections made later by Sir Ronald Fisher (Fisher, 1922). It tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution. For example, suppose a group of patients has been undergoing an experimental treatment and have had their health assessed to see whether their condition has improved, stayed the same or worsened. A goodness-of-fit test could be used to determine whether the numbers in each category - improved, no change, worsened - match the numbers that would be expected given the standard treatment option. Let’s think about this some more, with some psychology.\n\n10.1.1 撲克牌花色T恤銷售數據\nOver the years there have been many studies showing that humans find it difficult to simulate randomness. Try as we might to “act” random, we think in terms of patterns and structure and so, when asked to “do something at random”, what people actually do is anything but random. As a consequence, the study of human randomness (or non-randomness, as the case may be) opens up a lot of deep psychological questions about how we think about the world. With this in mind, let’s consider a very simple study. Suppose I asked people to imagine a shuffled deck of cards, and mentally pick one card from this imaginary deck “at random”. After they’ve chosen one card I ask them to mentally select a second one. For both choices what we’re going to look at is the suit (hearts, clubs, spades or diamonds) that people chose. After asking, say, \\(N = 200\\) people to do this, I’d like to look at the data and figure out whether or not the cards that people pretended to select were really random. The data are contained in the randomness.csv file in which, when you open it up in jamovi and take a look at the spreadsheet view, you will see three variables. These are: an id variable that assigns a unique identifier to each participant, and the two variables choice_1 and choice_2 that indicate the card suits that people chose.\nFor the moment, let’s just focus on the first choice that people made. We’ll use the Frequency tables option under ‘Exploration’ - ‘Descriptives’ to count the number of times that we observed people choosing each suit. This is what we get (Table 10.1):\n\n\n\n\nTable 10.1:  Number of times each suit was chosen \n\nclubsdiamondsheartsspades\n\n35516450\n\n\n\n\n\nThat little frequency table is quite helpful. Looking at it, there’s a bit of a hint that people might be more likely to select hearts than clubs, but it’s not completely obvious just from looking at it whether that’s really true, or if this is just due to chance. So we’ll probably have to do some kind of statistical analysis to find out, which is what I’m going to talk about in the next section.\nExcellent. From this point on, we’ll treat this table as the data that we’re looking to analyse. However, since I’m going to have to talk about this data in mathematical terms (sorry!) it might be a good idea to be clear about what the notation is. In mathematical notation, we shorten the human-readable word “observed” to the letter \\(O\\), and we use subscripts to denote the position of the observation. So the second observation in our table is written as \\(O_2\\) in maths. The relationship between the English descriptions and the mathematical symbols are illustrated in Table 10.2.\n\n\n\n\nTable 10.2:  Relationship between English descriptions and mathematical symbols \n\nlabelindex, imath. symbolthe value\n\nclubs, \\( \\clubsuit \\)1\\( O_1 \\)35\n\ndiamonds, \\( \\diamondsuit \\)2\\( O_2 \\)51\n\nhearts, \\( \\heartsuit \\)3\\( O_3 \\)64\n\nspades, \\( \\spadesuit \\)4\\( O_4 \\)50\n\n\n\n\n\nHopefully that’s pretty clear. It’s also worth noting that mathematicians prefer to talk about general rather than specific things, so you’ll also see the notation \\(O_i\\), which refers to the number of observations that fall within the i-th category (where i could be 1, 2, 3 or 4). Finally, if we want to refer to the set of all observed frequencies, statisticians group all observed values into a vector 2, which I’ll refer to as \\(O\\).\n\\[O = (O_1, O_2, O_3, O_4)\\]\nAgain, this is nothing new or interesting. It’s just notation. If I say that \\(O = (35, 51, 64, 50)\\) all I’m doing is describing the table of observed frequencies (i.e., observed), but I’m referring to it using mathematical notation.\n\n\n10.1.2 虛無假設與對立假設\nAs the last section indicated, our research hypothesis is that “people don’t choose cards randomly”. What we’re going to want to do now is translate this into some statistical hypotheses and then construct a statistical test of those hypotheses. The test that I’m going to describe to you is Pearson’s \\(\\chi^2\\) (chi-square) goodness-of-fit test, and as is so often the case we have to begin by carefully constructing our null hypothesis. In this case, it’s pretty easy. First, let’s state the null hypothesis in words:\n\\[H_0: \\text{ All four suits are chosen with equal probability}\\]\nNow, because this is statistics, we have to be able to say the same thing in a mathematical way. To do this, let’s use the notation \\(P_j\\) to refer to the true probability that the j-th suit is chosen. If the null hypothesis is true, then each of the four suits has a 25% chance of being selected. In other words, our null hypothesis claims that \\(P_1 = .25\\), \\(P_2 = .25\\), \\(P3 = .25\\) and finally that \\(P_4 = .25\\) . However, in the same way that we can group our observed frequencies into a vector O that summarises the entire data set, we can use P to refer to the probabilities that correspond to our null hypothesis. So if I let the vector \\(P = (P_1, P_2, P_3, P_4)\\) refer to the collection of probabilities that describe our null hypothesis, then we have:\n\\[H_0: P =(.25, .25, .25, .25)\\]\nIn this particular instance, our null hypothesis corresponds to a vector of probabilities P in which all of the probabilities are equal to one another. But this doesn’t have to be the case. For instance, if the experimental task was for people to imagine they were drawing from a deck that had twice as many clubs as any other suit, then the null hypothesis would correspond to something like \\(P = (.4, .2, .2, .2)\\). As long as the probabilities are all positive numbers, and they all sum to 1, then it’s a perfectly legitimate choice for the null hypothesis. However, the most common use of the goodness-of-fit test is to test a null hypothesis that all of the categories are equally likely, so we’ll stick to that for our example.\nWhat about our alternative hypothesis, \\(H_1\\)? All we’re really interested in is demonstrating that the probabilities involved aren’t all identical (that is, people’s choices weren’t completely random). As a consequence, the “human friendly” versions of our hypotheses look like this:\n\\(H_0: \\text{ All four suits are chosen with equal probability}\\)\n\\(H_1: \\text{ At least one of the suit-choice probabilities isn’t 0.25}\\)\n…and the “mathematician friendly” version is:\n\\(H_0: P= (.25, .25, .25, .25)\\)\n\\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n10.1.3 適合度檢定統計程序\nAt this point, we have our observed frequencies O and a collection of probabilities P corresponding to the null hypothesis that we want to test. What we now want to do is construct a test of the null hypothesis. As always, if we want to test \\(H_0\\) against \\(H_1\\), we’re going to need a test statistic. The basic trick that a goodness-of-fit test uses is to construct a test statistic that measures how “close” the data are to the null hypothesis. If the data don’t resemble what you’d “expect” to see if the null hypothesis were true, then it probably isn’t true. Okay, if the null hypothesis were true, what would we expect to see? Or, to use the correct terminology, what are the expected frequencies. There are \\(N = 200\\) observations, and (if the null is true) the probability of any one of them choosing a heart is \\(P_3 = .25\\), so I guess we’re expecting \\(200 \\times .25 = 50\\) hearts, right? Or, more specifically, if we let Ei refer to “the number of category i responses that we’re expecting if the null is true”, then\n\\[E_i=N \\times P_i\\]\nThis is pretty easy to calculate.If there are 200 observations that can fall into four categories, and we think that all four categories are equally likely, then on average we’d expect to see 50 observations in each category, right?\nNow, how do we translate this into a test statistic? Clearly, what we want to do is compare the expected number of observations in each category (\\(E_i\\)) with the observed number of observations in that category (\\(O_i\\)). And on the basis of this comparison we ought to be able to come up with a good test statistic. To start with, let’s calculate the difference between what the null hypothesis expected us to find and what we actually did find. That is, we calculate the “observed minus expected” difference score, \\(O_i - E_i\\) . This is illustrated in Table 10.3.\n\n\n\n\nTable 10.3:  Expected and observed frequencies \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)50505050\n\nobserved frequency \\( O_i\\)35516450\n\ndifference score \\( O_i-E_i\\)-151140\n\n\n\n\n\nSo, based on our calculations, it’s clear that people chose more hearts and fewer clubs than the null hypothesis predicted. However, a moment’s thought suggests that these raw differences aren’t quite what we’re looking for. Intuitively, it feels like it’s just as bad when the null hypothesis predicts too few observations (which is what happened with hearts) as it is when it predicts too many (which is what happened with clubs). So it’s a bit weird that we have a negative number for clubs and a positive number for hearts. One easy way to fix this is to square everything, so that we now calculate the squared differences, \\((E_i - O_i)^2\\) . As before, we can do this by hand (Table 10.4).\n\n\n\n\nTable 10.4:  Squaring the difference scores \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n22511960\n\n\n\n\n\nNow we’re making progress. What we’ve got now is a collection of numbers that are big whenever the null hypothesis makes a bad prediction (clubs and hearts), but are small whenever it makes a good one (diamonds and spades). Next, for some technical reasons that I’ll explain in a moment, let’s also divide all these numbers by the expected frequency Ei , so we’re actually calculating \\(\\frac{(E_i-O_i)^2}{E_i}\\) . Since \\(E_i = 50\\) for all categories in our example, it’s not a very interesting calculation, but let’s do it anyway (Table 10.5).\n\n\n\n\nTable 10.5:  Dividing the squared difference scores by the expected frequency to provide an ‘error’ score \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n4.500.023.920.00\n\n\n\n\n\nIn effect, what we’ve got here are four different “error” scores, each one telling us how big a “mistake” the null hypothesis made when we tried to use it to predict our observed frequencies. So, in order to convert this into a useful test statistic, one thing we could do is just add these numbers up. The result is called the goodness-of-fit statistic, conventionally referred to either as \\(\\chi^2\\) (chi-square) or GOF. We can calculate it as in Table 10.6.\n\\[\\sum( (observed - expected)^2 / expected )\\]\nThis gives us a value of 8.44.\n[Additional technical detail 3]\nAs we’ve seen from our calculations, in our cards data set we’ve got a value of \\(\\chi^2\\) = 8.44. So now the question becomes is this a big enough value to reject the null?\n\n\n10.1.4 適合度檢定的樣本分佈\nTo determine whether or not a particular value of \\(\\chi^2\\) is large enough to justify rejecting the null hypothesis, we’re going to need to figure out what the sampling distribution for \\(\\chi^2\\) would be if the null hypothesis were true. So that’s what I’m going to do in this section. I’ll show you in a fair amount of detail how this sampling distribution is constructed, and then, in the next section, use it to build up a hypothesis test. If you want to cut to the chase and are willing to take it on faith that the sampling distribution is a \\(\\chi^2\\) (chi-square) distribution with \\(k - 1\\) degrees of freedom, you can skip the rest of this section. However, if you want to understand why the goodness-of-fit test works the way it does, read on.\nOkay, let’s suppose that the null hypothesis is actually true. If so, then the true probability that an observation falls in the i-th category is \\(P_i\\) . After all, that’s pretty much the definition of our null hypothesis. Let’s think about what this actually means. This is kind of like saying that “nature” makes the decision about whether or not the observation ends up in category i by flipping a weighted coin (i.e., one where the probability of getting a head is \\(P_j\\) ). And therefore we can think of our observed frequency \\(O_i\\) by imagining that nature flipped N of these coins (one for each observation in the data set), and exactly \\(O_i\\) of them came up heads. Obviously, this is a pretty weird way to think about the experiment. But what it does (I hope) is remind you that we’ve actually seen this scenario before. It’s exactly the same set up that gave rise to Section 7.4 in Chapter 7. In other words, if the null hypothesis is true, then it follows that our observed frequencies were generated by sampling from a binomial distribution:\n\\[O_i \\sim Binomial(P_i,N) \\]\nNow, if you remember from our discussion of Section 8.3.3 the binomial distribution starts to look pretty much identical to the normal distribution, especially when \\(N\\) is large and when \\(P_i\\) isn’t too close to 0 or 1. In other words as long as \\(N^P_i\\) is large enough. Or, to put it another way, when the expected frequency Ei is large enough then the theoretical distribution of \\(O_i\\) is approximately normal. Better yet, if \\(O_i\\) is normally distributed, then so is \\((O_i-E_i)/\\sqrt{(E_i)}\\) . Since \\(E_i\\) is a fixed value, subtracting off Ei and dividing by ? Ei changes the mean and standard deviation of the normal distribution but that’s all it does. Okay, so now let’s have a look at what our goodness-of-fit statistic actually is. What we’re doing is taking a bunch of things that are normally-distributed, squaring them, and adding them up. Wait. We’ve seen that before too! As we discussed in the section on Section 7.6, when you take a bunch of things that have a standard normal distribution (i.e., mean 0 and standard deviation 1), square them and then add them up, the resulting quantity has a chi-square distribution. So now we know that the null hypothesis predicts that the sampling distribution of the goodness-of-fit statistic is a chi-square distribution. Cool.\nThere’s one last detail to talk about, namely the degrees of freedom. If you remember back to Section 7.6, I said that if the number of things you’re adding up is k, then the degrees of freedom for the resulting chi-square distribution is k. Yet, what I said at the start of this section is that the actual degrees of freedom for the chi-square goodness-of-fit test is \\(k - 1\\). What’s up with that? The answer here is that what we’re supposed to be looking at is the number of genuinely independent things that are getting added together. And, as I’ll go on to talk about in the next section, even though there are k things that we’re adding only \\(k - 1\\) of them are truly independent, and so the degrees of freedom is actually only \\(k - 1\\). That’s the topic of the next section4.\n\n\n10.1.5 自由度\n\n\n\n\n\nFigure 10.1: \\(\\chi^2\\) (chi-square) distributions with different values for the ‘degrees of freedom’\n\n\n\n\nWhen I introduced the chi-square distribution in Section 7.6, I was a bit vague about what “degrees of freedom” actually means. Obviously, it matters. Looking at Figure 10.1, you can see that if we change the degrees of freedom then the chi-square distribution changes shape quite substantially. But what exactly is it? Again, when I introduced the distribution and explained its relationship to the normal distribution, I did offer an answer: it’s the number of “normally distributed variables” that I’m squaring and adding together. But, for most people, that’s kind of abstract and not entirely helpful. What we really need to do is try to understand degrees of freedom in terms of our data. So here goes.\nThe basic idea behind degrees of freedom is quite simple. You calculate it by counting up the number of distinct “quantities” that are used to describe your data and then subtracting off all of the “constraints” that those data must satisfy.5 This is a bit vague, so let’s use our cards data as a concrete example. We describe our data using four numbers, \\(O1, O2, O3\\) and O4 corresponding to the observed frequencies of the four different categories (hearts, clubs, diamonds, spades). These four numbers are the random outcomes of our experiment. But my experiment actually has a fixed constraint built into it: the sample size \\(N\\). 6 That is, if we know\nhow many people chose hearts, how many chose diamonds and how many chose clubs, then we’d be able to figure out exactly how many chose spades. In other words, although our data are described using four numbers, they only actually correspond to \\(4 - 1 = 3\\) degrees of freedom. A slightly different way of thinking about it is to notice that there are four probabilities that we’re interested in (again, corresponding to the four different categories), but these probabilities must sum to one, which imposes a constraint. Therefore the degrees of freedom is \\(4 - 1 = 3\\). Regardless of whether you want to think about it in terms of the observed frequencies or in terms of the probabilities, the answer is the same. In general, when running the \\(\\chi^2\\)(chi-square) goodness-of-fit test for an experiment involving \\(k\\) groups, then the degrees of freedom will be \\(k - 1\\).\n\n\n10.1.6 檢定虛無假設\nThe final step in the process of constructing our hypothesis test is to figure out what the rejection region is. That is, what values of \\(\\chi^2\\) would lead us to reject the null hypothesis. As we saw earlier, large values of \\(\\chi^2\\) imply that the null hypothesis has done a poor job of predicting the data from our experiment, whereas small values of \\(\\chi^2\\) imply that it’s actually done pretty well. Therefore, a pretty sensible strategy would be to say there is some critical value such that if \\(\\chi^2\\) is bigger than the critical value we reject the null, but if \\(\\chi^2\\) is smaller than this value we retain the null. In other words, to use the language we introduced in Chapter 9 the chi-square goodness-of-fit test is always a one-sided test. Right, so all we have to do is figure out what this critical value is. And it’s pretty straightforward. If we want our test to have significance level of \\(\\alpha = .05\\) (that is, we are willing to tolerate a Type I error rate of \\(5%\\)), then we have to choose our critical value so that there is only a 5% chance that \\(\\chi^2\\) could get to be that big if the null hypothesis is true. This is illustrated in Figure 10.2.\n\n\n\n\n\nFigure 10.2: Illustration of how the hypothesis testing works for the \\(\\chi^2\\) (chi-square) goodness of-fit test\n\n\n\n\nAh but, I hear you ask, how do I find the critical value of a chi-square distribution with \\(k-1\\) degrees of freedom? Many many years ago when I first took a psychology statistics class we used to look up these critical values in a book of critical value tables, like the one in Figure 10.3. Looking at this Figure, we can see that the critical value for a \\(\\chi^2\\) distribution with 3 degrees of freedom, and p=0.05 is 7.815.\n\n\n\n\n\nFigure 10.3: Table of critical values for the chi-square distribution\n\n\n\n\nSo, if our calculated \\(\\chi^2\\) statistic is bigger than the critical value of \\(7.815\\), then we can reject the null hypothesis (remember that the null hypothesis, \\(H_0\\), is that all four suits are chosen with equal probability). Since we actually already calculated that before (i.e., \\(\\chi^2\\) = 8.44) we can reject the null hypothesis. And that’s it, basically. You now know “Pearson’s \\(\\chi^2\\) test for the goodness-of-fit”. Lucky you.\n\n\n10.1.7 jamovi實作\nNot surprisingly, jamovi provides an analysis that will do these calculations for you. Let’s use the Randomness.omv file. From the main ‘Analyses’ toolbar select ‘Frequencies’ - ‘One Sample Proportion Tests’ - ‘\\(N\\) Outcomes’. Then in the analysis window that appears move the variable you want to analyse (choice 1 across into the ‘Variable’ box. Also, click on the ‘Expected counts’ check box so that these are shown on the results table. When you have done all this, you should see the analysis results in jamovi as in Figure 10.4. No surprise then that jamovi provides the same expected counts and statistics that we calculated by hand above, with a \\(\\chi^2\\) value of \\((8.44\\) with \\(3\\) d.f. and \\(p=0.038\\). Note that we don’t need to look up a critical p-value threshold value any more, as jamovi gives us the actual p-value of the calculated \\(\\chi^2\\) for \\(3\\) d.f.\n\n\n\n\n\nFigure 10.4: A \\(\\chi^2\\) One Sample Proportion Test in jamovi, with table showing both observed and expected frequencies and proportions\n\n\n\n\n\n\n10.1.8 另一種虛無假設\nAt this point you might be wondering what to do if you want to run a goodness-of-fit test but your null hypothesis is not that all categories are equally likely. For instance, let’s suppose that someone had made the theoretical prediction that people should choose red cards \\(60\\%\\) of the time, and black cards \\(40\\%\\) of the time (I’ve no idea why you’d predict that), but had no other preferences. If that were the case, the null hypothesis would be to expect \\(30\\%\\) of the choices to be hearts, \\(30\\%\\) to be diamonds, \\(20\\%\\) to be spades and \\(20\\%\\) to be clubs. In other words we would expect hearts and diamonds to appear 1.5 times more often than spades and clubs (the ratio \\(30\\%\\) : \\(20\\%\\) is the same as 1.5 : 1). This seems like a silly theory to me, and it’s pretty easy to test this explicitly specified null hypothesis with the data in our jamovi analysis. In the analysis window (labelled ‘Proportion Test (N Outcomes)’ in Figure 10.4 you can expand the options for ‘Expected Proportions’. When you do this, there are options for entering different ratio values for the variable you have selected, in our case this is choice 1. Change the ratio to reflect the new null hypothesis, as in Figure 10.5, and see how the results change.\n\n\n\n\n\nFigure 10.5: Changing the expected proportions in the \\(\\\\chi^2\\) One Sample Proportion Test in jamovi\n\n\n\n\nThe expected counts are now shown in Table 10.6.\n\n\n\n\nTable 10.6:  Expected counts for a different null hypothesis \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)40606040\n\n\n\n\n\nand the \\(\\chi^2\\) statistic is 4.74, 3 d.f., \\(p = 0.182\\). Now, the results of our updated hypotheses and the expected frequencies are different from what they were last time. As a consequence our \\(\\chi^2\\) test statistic is different, and our p-value is different too. Annoyingly, the p-value is \\(.182\\), so we can’t reject the null hypothesis (look back at Section 9.5 to remind yourself why). Sadly, despite the fact that the null hypothesis corresponds to a very silly theory, these data don’t provide enough evidence against it.\n\n\n10.1.9 適合度檢定的報告寫作\nSo now you know how the test works, and you know how to do the test using a wonderful jamovi flavoured magic computing box. The next thing you need to know is how to write up the results. After all, there’s no point in designing and running an experiment and then analysing the data if you don’t tell anyone about it! So let’s now talk about what you need to do when reporting your analysis. Let’s stick with our card-suits example. If I wanted to write this result up for a paper or something, then the conventional way to report this would be to write something like this:\n\nOf the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness-of-fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were significant (\\(\\chi^2(3) = 8.44, p< .05)\\), suggesting that people did not select suits purely at random.\n\nThis is pretty straightforward and hopefully it seems pretty unremarkable. That said, there’s a few things that you should note about this description:\n\nThe statistical test is preceded by the descriptive statistics. That is, I told the reader something about what the data look like before going on to do the test. In general, this is good practice. Always remember that your reader doesn’t know your data anywhere near as well as you do. So, unless you describe it to them properly, the statistical tests won’t make any sense to them and they’ll get frustrated and cry.\nThe description tells you what the null hypothesis being tested is. To be honest, writers don’t always do this but it’s often a good idea in those situations where some ambiguity exists, or when you can’t rely on your readership being intimately familiar with the statistical tools that you’re using. Quite often the reader might not know (or remember) all the details of the test that your using, so it’s a kind of politeness to “remind” them! As far as the goodness-of-fit test goes, you can usually rely on a scientific audience knowing how it works (since it’s covered in most intro stats classes). However, it’s still a good idea to be explicit about stating the null hypothesis (briefly!) because the null hypothesis can be different depending on what you’re using the test for. For instance, in the cards example my null hypothesis was that all the four suit probabilities were identical (i.e., \\(P1 = P2 = P3 = P4 = 0.25\\)), but there’s nothing special about that hypothesis. I could just as easily have tested the null hypothesis that \\(P_1 = 0.7\\) and \\(P2 = P3 = P4 = 0.1\\) using a goodness-of-fit test. So it’s helpful to the reader if you explain to them what your null hypothesis was. Also, notice that I described the null hypothesis in words, not in maths. That’s perfectly acceptable. You can describe it in maths if you like, but since most readers find words easier to read than symbols, most writers tend to describe the null using words if they can.\nA “stat block” is included. When reporting the results of the test itself, I didn’t just say that the result was significant, I included a “stat block” (i.e., the dense mathematical looking part in the parentheses) which reports all the “key” statistical information. For the chi-square goodness-of-fit test, the information that gets reported is the test statistic (that the goodness-of-fit statistic was 8.44), the information about the distribution used in the test (\\(\\chi^2\\) with 3 degrees of freedom which is usually shortened to \\(\\chi^2\\)(3)), and then the information about whether the result was significant (in this case \\(p< .05\\)). The particular information that needs to go into the stat block is different for every test, and so each time I introduce a new test I’ll show you what the stat block should look like.7 However the general principle is that you should always provide enough information so that the reader could check the test results themselves if they really wanted to.\nThe results are interpreted. In addition to indicating that the result was significant, I provided an interpretation of the result (i.e., that people didn’t choose randomly). This is also a kindness to the reader, because it tells them something about what they should believe about what’s going on in your data. If you don’t include something like this, it’s really hard for your reader to understand what’s going on.8\n\nAs with everything else, your overriding concern should be that you explain things to your reader. Always remember that the point of reporting your results is to communicate to another human being. I cannot tell you just how many times I’ve seen the results section of a report or a thesis or even a scientific article that is just gibberish, because the writer has focused solely on making sure they’ve included all the numbers and forgotten to actually communicate with the human reader.\n\nSatan delights equally in statistics and in quoting scripture9 – H.G. Wells"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方獨立性檢定",
    "href": "10-Categorical-data-analysis.html#卡方獨立性檢定",
    "title": "10  類別資料分析",
    "section": "10.2 卡方獨立性檢定",
    "text": "10.2 卡方獨立性檢定\n\nGUARDBOT 1: Halt!\nGUARDBOT 2: Be you robot or human?\nLEELA: Robot…we be.\nFRY: Uh, yup! Just two robots out roboting it up! Eh?\nGUARDBOT 1: Administer the test.\nGUARDBOT 2: Which of the following would you most prefer? A: A puppy, B: A pretty flower from your sweetie, or C: A large properly-formatted data file?\nGUARDBOT 1: Choose!\nFuturama, “Fear of a Bot Planet”\n\nThe other day I was watching an animated documentary examining the quaint customs of the natives of the planet Chapek 9. Apparently, in order to gain access to their capital city a visitor must prove that they’re a robot, not a human. In order to determine whether or not a visitor is human, the natives ask whether the visitor prefers puppies, flowers, or large, properly formatted data files. “Pretty clever,” I thought to myself “but what if humans and robots have the same preferences? That probably wouldn’t be a very good test then, would it?” As it happens, I got my hands on the testing data that the civil authorities of Chapek 9 used to check this. It turns out that what they did was very simple. They found a bunch of robots and a bunch of humans and asked them what they preferred. I saved their data in a file called chapek9.omv, which we can now load into jamovi. As well as the ID variable that identifies individual people, there are two nominal text variables, species and choice. In total there are 180 entries in the data set, one for each person (counting both robots and humans as “people”) who was asked to make a choice. Specifically, there are 93 humans and 87 robots, and overwhelmingly the preferred choice is the data file. You can check this yourself by asking jamovi for Frequency Tables, under the ‘Exploration’ - ‘Descriptives’ button. However, this summary does not address the question we’re interested in. To do that, we need a more detailed description of the data. What we want to do is look at the choices broken down by species. That is, we need to cross-tabulate the data (see Section 6.1). In jamovi we do this using the ‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’ analysis, and we should get a table something like Table 10.7.\n\n\n\n\nTable 10.7:  Cross-tabulating the data \n\nRobotHumanTotal\n\nPuppy131528\n\nFlower301343\n\nData4465109\n\nTotal8793180\n\n\n\n\n\nFrom this, it’s quite clear that the vast majority of the humans chose the data file, whereas the robots tended to be a lot more even in their preferences. Leaving aside the question of why the humans might be more likely to choose the data file for the moment (which does seem quite odd, admittedly), our first order of business is to determine if the discrepancy between human choices and robot choices in the data set is statistically significant.\n\n10.2.1 建立獨立性的假設檢定\nHow do we analyse this data? Specifically, since my research hypothesis is that “humans and robots answer the question in different ways”, how can I construct a test of the null hypothesis that “humans and robots answer the question the same way”? As before, we begin by establishing some notation to describe the data (Table 10.8).\n\n\n\n\nTable 10.8:  Notation to describe the data \n\nRobotHumanTotal\n\nPuppy\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nFlower\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nData\\(O_{31}\\)\\(O_{32}\\)\\(R_{3}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)N\n\n\n\n\n\nIn this notation we say that \\(O_{ij}\\) is a count (observed frequency) of the number of respondents that are of species j (robots or human) who gave answer i (puppy, flower or data) when asked to make a choice. The total number of observations is written \\(N\\), as usual. Finally, I’ve used \\(R_i\\) to denote the row totals (e.g., \\(R_1\\) is the total number of people who chose the flower), and \\(C_j\\) to denote the column totals (e.g., \\(C_1\\) is the total number of robots).10\nSo now let’s think about what the null hypothesis says. If robots and humans are responding in the same way to the question, it means that the probability that “a robot says puppy” is the same as the probability that “a human says puppy”, and so on for the other two possibilities. So, if we use \\(P_{ij}\\) to denote “the probability that a member of species j gives response i” then our null hypothesis is that:\n\\[\n\\begin{aligned}\nH_0 &: \\text{All of the following are true:} \\\\\n&P_{11} = P_{12}\\text{ (same probability of saying “puppy”),} \\\\\n&P_{21} = P_{22}\\text{ (same probability of saying “flower”), and} \\\\\n&P_{31} = P_{32}\\text{ (same probability of saying “data”).}\n\\end{aligned}\n\\]\nAnd actually, since the null hypothesis is claiming that the true choice probabilities don’t depend on the species of the person making the choice, we can let Pi refer to this probability, e.g., P1 is the true probability of choosing the puppy.\nNext, in much the same way that we did with the goodness-of-fit test, what we need to do is calculate the expected frequencies. That is, for each of the observed counts \\(O_{ij}\\) , we need to figure out what the null hypothesis would tell us to expect. Let’s denote this expected frequency by \\(E_{ij}\\). This time, it’s a little bit trickier. If there are a total of \\(C_j\\) people that belong to species \\(j\\), and the true probability of anyone (regardless of species) choosing option \\(i\\) is \\(P_i\\) , then the expected frequency is just:\n\\[E_{ij}=C_j \\times P_i\\]\nNow, this is all very well and good, but we have a problem. Unlike the situation we had with the goodness-of-fit test, the null hypothesis doesn’t actually specify a particular value for Pi .\nIt’s something we have to estimate (see Chapter 8) from the data! Fortunately, this is pretty easy to do. If 28 out of 180 people selected the flowers, then a natural estimate for the probability of choosing flowers is \\(\\frac{28}{180}\\), which is approximately \\(.16\\). If we phrase this in mathematical terms, what we’re saying is that our estimate for the probability of choosing option i is just the row total divided by the total sample size:\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\nTherefore, our expected frequency can be written as the product (i.e. multiplication) of the row total and the column total, divided by the total number of observations:11\n\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]\n[Additional technical detail 12]\nAs before, large values of \\(X^2\\) indicate that the null hypothesis provides a poor description of the data, whereas small values of \\(X^2\\) suggest that it does a good job of accounting for the data. Therefore, just like last time, we want to reject the null hypothesis if \\(X^2\\) is too large.\nNot surprisingly, this statistic is \\(\\chi^2\\) distributed. All we need to do is figure out how many degrees of freedom are involved, which actually isn’t too hard. As I mentioned before, you can (usually) think of the degrees of freedom as being equal to the number of data points that you’re analysing, minus the number of constraints. A contingency table with r rows and c columns contains a total of \\(r^{c}\\) observed frequencies, so that’s the total number of observations. What about the constraints? Here, it’s slightly trickier. The answer is always the same\n\\[df=(r-1)(c-1)\\]\nbut the explanation for why the degrees of freedom takes this value is different depending on the experimental design. For the sake of argument, let’s suppose that we had honestly intended to survey exactly 87 robots and 93 humans (column totals fixed by the experimenter), but left the row totals free to vary (row totals are random variables). Let’s think about the constraints that apply here. Well, since we deliberately fixed the column totals by Act of Experimenter, we have \\(c\\) constraints right there. But, there’s actually more to it than that. Remember how our null hypothesis had some free parameters (i.e., we had to estimate the Pi values)? Those matter too. I won’t explain why in this book, but every free parameter in the null hypothesis is rather like an additional constraint. So, how many of those are there? Well, since these probabilities have to sum to 1, there’s only \\(r - 1\\) of these. So our total degrees of freedom is:\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\nAlternatively, suppose that the only thing that the experimenter fixed was the total sample size N. That is, we quizzed the first 180 people that we saw and it just turned out that 87 were robots and 93 were humans. This time around our reasoning would be slightly different, but would still lead us to the same answer. Our null hypothesis still has \\(r - 1\\) free parameters corresponding to the choice probabilities, but it now also has \\(c - 1\\) free parameters corresponding to the species probabilities, because we’d also have to estimate the probability that a randomly sampled person turns out to be a robot.13 Finally, since we did actually fix the total number of observations N, that’s one more constraint. So, now we have rc observations, and \\((c-1)+(r-1)+1\\) constraints. What does that give?\n\\[\\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\n\\] Amazing.\n\n\n10.2.2 獨立性檢定實作\nOkay, now that we know how the test works let’s have a look at how it’s done in jamovi. As tempting as it is to lead you through the tedious calculations so that you’re forced to learn it the long way, I figure there’s no point. I already showed you how to do it the long way for the goodness-of-fit test in the last section, and since the test of independence isn’t conceptually any different, you won’t learn anything new by doing it the long way. So instead I’ll go straight to showing you the easy way. After you have run the test in jamovi (‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’), all you have to do is look underneath the contingency table in the jamovi results window and there is the \\(\\chi^2\\) statistic for you. This shows a \\(\\chi^2\\) statistic value of 10.72, with 2 d.f. and p-value = 0.005.\nThat was easy, wasn’t it! You can also ask jamovi to show you the expected counts - just click on the check box for ‘Counts’ - ‘Expected’ in the ‘Cells’ options and the expected counts will appear in the contingency table. And whilst you are doing that, an effect size measure would be helpful. We’ll choose Cramér’s \\(V\\), and you can specify this from a check box in the ‘Statistics’ options, and it gives a value for Cramér’s \\(V\\) of \\(0.24\\). See Figure 10.6. We will talk about this some more in just a moment.\n\n\n\n\n\nFigure 10.6: Independent samples \\(\\chi^2\\) test in jamovi using the Chapek 9 data\n\n\n\n\nThis output gives us enough information to write up the result:\n\nPearson’s \\(\\chi^2\\) revealed a significant association between species and choice (\\(\\chi^2(2) = 10.7, p< .01)\\). Robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data.\n\nNotice that, once again, I provided a little bit of interpretation to help the human reader understand what’s going on with the data. Later on in my discussion section I’d provide a bit more context. To illustrate the difference, here’s what I’d probably say later on:\n\nThe fact that humans appeared to have a stronger preference for raw data files than robots is somewhat counter-intuitive. However, in context it makes some sense, as the civil authority on Chapek 9 has an unfortunate tendency to kill and dissect humans when they are identified. As such it seems most likely that the human participants did not respond honestly to the question, so as to avoid potentially undesirable consequences. This should be considered to be a substantial methodological weakness.\n\nThis could be classified as a rather extreme example of a reactivity effect, I suppose. Obviously, in this case the problem is severe enough that the study is more or less worthless as a tool for understanding the difference preferences among humans and robots. However, I hope this illustrates the difference between getting a statistically significant result (our null hypothesis is rejected in favour of the alternative), and finding something of scientific value (the data tell us nothing of interest about our research hypothesis due to a big methodological flaw)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#連續性校正",
    "href": "10-Categorical-data-analysis.html#連續性校正",
    "title": "10  類別資料分析",
    "section": "10.3 連續性校正",
    "text": "10.3 連續性校正\nOkay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the Yates correction. Remember what I pointed out earlier: the \\(\\chi^2\\) test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large \\(N\\). One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a \\(2 \\times 2\\) contingency table). The main reason for this is that the true sampling distribution for the \\(X^{2}\\) statistic is actually discrete (because you’re dealing with categorical data!) but the \\(\\chi^2\\) distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when \\(df = 1\\), the goodness-of-fit statistic tends to be “too big”, meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).\nAs far as I can tell from reading Yates’ paper14, the correction is basically a hack. It’s not derived from any principled theory. Rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the ‘Statistics’ options, where it is called ‘\\(\\chi^2\\) continuity correction’."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#效果量",
    "href": "10-Categorical-data-analysis.html#效果量",
    "title": "10  類別資料分析",
    "section": "10.4 效果量",
    "text": "10.4 效果量\nAs we discussed earlier in Section 9.8, it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?\nThere are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them but will instead focus on the most commonly reported measures of effect size.\nBy default, the two measures that people tend to report most frequently are the \\(\\phi\\) statistic and the somewhat superior version, known as Cramér’s \\(V\\) .\n[Additional technical detail 15]\nAnd you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly. With Cramér’s \\(V\\), you know that the value really does range from 0 (no association at all) to 1 (perfect association)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#檢定方法的適用條件",
    "href": "10-Categorical-data-analysis.html#檢定方法的適用條件",
    "title": "10  類別資料分析",
    "section": "10.5 檢定方法的適用條件",
    "text": "10.5 檢定方法的適用條件\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:\n\nExpected frequencies are sufficiently large. Remember how in the previous section we saw that the \\(\\chi^2\\) sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter 7 this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran (1954)) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative (Larntz, 1978).\nData are independent of one another. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that everyone selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic” (Tversky & Kahneman, 1974). This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.\n\nIf you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-The-Fisher-test",
    "href": "10-Categorical-data-analysis.html#sec-The-Fisher-test",
    "title": "10  類別資料分析",
    "section": "10.6 費雪精確檢定",
    "text": "10.6 費雪精確檢定\nWhat should you do if your cell counts are too small, but you’d still like to test the null hypothesis that the two variables are independent? One answer would be “collect more data”, but that’s far too glib There are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, Fisher (1922) kindly provided the right answer to the question. To illustrate the basic idea let’s suppose that we’re analysing data from a field experiment looking at the emotional status of people who have been accused of Witchcraft, some of whom are currently being burned at the stake.16 Unfortunately for the scientist (but rather fortunately for the general populace), it’s actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. A contingency table of the salem.csv data illustrates the point (Table 10.9).\n\n\n\n\nTable 10.9:  Contingency table of the salem.csv data \n\nhappyFALSETRUE\n\non.fireFALSE310\n\nTRUE30\n\n\n\n\n\nLooking at this data, you’d be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. So, speaking as someone who doesn’t want to be set on fire, I’d really like to be able to get a better answer than this. This is where 費雪精確檢定(Fisher’s exact test) (Fisher, 1922) comes in very handy.\nThe Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn’t have a test statistic, but it calculates the p-value “directly”. I’ll explain the basics of how the test works for a \\(2 \\times 2\\) contingency table. As before, let’s have some notation (Table 10.10).\n\n\n\n\nTable 10.10:  Notation for the Fisher exact test \n\nHappySadTotal\n\nSet on fire\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nNot set on fire\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)\\(N\\)\n\n\n\n\n\nIn order to construct the test Fisher treats both the row and column totals \\((R_1, R_2, C_1 \\text{ and } C_2)\\) as known, fixed quantities and then calculates the probability that we would have obtained the observed frequencies that we did \\((O_{11}, O_{12}, O_{21} \\text{ and } O_{22})\\) given those totals. In the notation that we developed in Chapter 7 this is written:\n\\[P(O_{11}, O_{12}, O_{21}, O_{22}  \\text{ | } R_1, R_2, C_1, C_2)\\] and as you might imagine, it’s a slightly tricky exercise to figure out what this probability is. But it turns out that this probability is described by a distribution known as the hypergeometric distribution. What we have to do to calculate our p-value is calculate the probability of observing this particular table or a table that is “more extreme”. 17 Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it’s pretty easy as long as the tables aren’t too big and the sample size isn’t too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more “extreme” than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the p-value.\nYou can specify this test in jamovi from a check box in the ‘Statistics’ options of the ‘Contingency Tables’ analysis. When you do this with the data from the salem.csv file, the Fisher exact test statistic is shown in the results. The main thing we’re interested in here is the p-value, which in this case is small enough (p = .036) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire. See Figure 10.7.\n\n\n\n\n\nFigure 10.7: Fisher exact test analysis in jamovi"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的校正",
    "href": "10-Categorical-data-analysis.html#卡方檢定的校正",
    "title": "10  類別資料分析",
    "section": "10.3 卡方檢定的校正",
    "text": "10.3 卡方檢定的校正\nOkay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the Yates correction. Remember what I pointed out earlier: the \\(\\chi^2\\) test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large \\(N\\). One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a \\(2 \\times 2\\) contingency table). The main reason for this is that the true sampling distribution for the \\(X^{2}\\) statistic is actually discrete (because you’re dealing with categorical data!) but the \\(\\chi^2\\) distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when \\(df = 1\\), the goodness-of-fit statistic tends to be “too big”, meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).\nAs far as I can tell from reading Yates’ paper14, the correction is basically a hack. It’s not derived from any principled theory. Rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the ‘Statistics’ options, where it is called ‘\\(\\chi^2\\) continuity correction’."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的效果量",
    "href": "10-Categorical-data-analysis.html#卡方檢定的效果量",
    "title": "10  類別資料分析",
    "section": "10.4 卡方檢定的效果量",
    "text": "10.4 卡方檢定的效果量\nAs we discussed earlier in Section 9.8, it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?\nThere are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them but will instead focus on the most commonly reported measures of effect size.\nBy default, the two measures that people tend to report most frequently are the \\(\\phi\\) statistic and the somewhat superior version, known as Cramér’s \\(V\\) .\n[Additional technical detail 15]\nAnd you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly. With Cramér’s \\(V\\), you know that the value really does range from 0 (no association at all) to 1 (perfect association)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的適用條件",
    "href": "10-Categorical-data-analysis.html#卡方檢定的適用條件",
    "title": "10  類別資料分析",
    "section": "10.5 卡方檢定的適用條件",
    "text": "10.5 卡方檢定的適用條件\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:\n\nExpected frequencies are sufficiently large. Remember how in the previous section we saw that the \\(\\chi^2\\) sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter 7 this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran (1954)) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative (Larntz, 1978).\nData are independent of one another. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that everyone selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic” (Tversky & Kahneman, 1974). This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.\n\nIf you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#本章小結",
    "href": "10-Categorical-data-analysis.html#本章小結",
    "title": "10  類別資料分析",
    "section": "10.8 本章小結",
    "text": "10.8 本章小結\n本章的學習重點有：\n\n卡方適合度檢定用於你的表列資料是來自不同類別的觀察次數，虛無假設是可相互比較的已知機率。\n卡方獨立性或關聯性檢定用於你的資料是能化為列聯表的觀察次數。虛無假設是兩種變項之間沒有關聯性。\n列聯表的效果量有多種測量方法。在此介紹最常見的Cramér’s V。\n上述的卡方檢定法有兩種適用條件：期望值次數夠大，觀察值彼此獨立。如果期望值次數不夠大，可以使用費雪精確檢定；如果觀察值並非彼此獨立，可以使用麥內瑪檢定。\n\n如果想學習更多類別資料分析方法，推薦閱讀 Agresti (1996) 的專書”類別資料分析導論”。如果導論書無法滿足你的需要，或者未提供解決手上問題的方法，可以參考 Agresti (2002) 的進階書藉。因為是進階書，建議先充分理解導論再來學習進階教科書。\n\n\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nCramer, H. (1946). Mathematical methods of statistics. Princeton University Press.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "11-Comparing-two-means.html#單一樣本z檢定",
    "href": "11-Comparing-two-means.html#單一樣本z檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.1 單一樣本z檢定",
    "text": "11.1 單一樣本z檢定\nIn this section I’ll describe one of the most useless tests in all of statistics: the z-test. Seriously – this test is almost never used in real life. Its only real purpose is that, when teaching statistics, it’s a very convenient stepping stone along the way towards the t-test, which is probably the most (over)used tool in all statistics.\n\n11.1.1 使用z檢定前的注意事項\nTo introduce the idea behind the z-test, let’s use a simple example. A friend of mine, Dr Zeppo, grades his introductory statistics class on a curve. Let’s suppose that the average grade in his class is \\(67.5\\), and the standard deviation is \\(9.5\\). Of his many hundreds of students, it turns out that 20 of them also take psychology classes. Out of curiosity, I find myself wondering if the psychology students tend to get the same grades as everyone else (i.e., mean \\(67.5\\)) or do they tend to score higher or lower? He emails me the zeppo.csv file, which I use to look at the grades of those students, in the jamovi spreadsheet view,and then calculate the mean in ‘Exploration’ - ‘Descriptives’ 2. The mean value is \\(72.3\\).\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\nHmm. It might be that the psychology students are scoring a bit higher than normal. That sample mean of \\(\\bar{X} = 72.3\\) is a fair bit higher than the hypothesised population mean of \\(\\mu = 67.5\\) but, on the other hand, a sample size of \\(N = 20\\) isn’t all that big. Maybe it’s pure chance.\nTo answer the question, it helps to be able to write down what it is that I think I know. Firstly, I know that the sample mean is \\(\\bar{X} = 72.3\\). If I’m willing to assume that the psychology students have the same standard deviation as the rest of the class then I can say that the population standard deviation is \\(\\sigma = 9.5\\). I’ll also assume that since Dr Zeppo is grading to a curve, the psychology student grades are normally distributed.\nNext, it helps to be clear about what I want to learn from the data. In this case my research hypothesis relates to the population mean \\(\\mu\\) for the psychology student grades, which is unknown. Specifically, I want to know if \\(\\mu = 67.5\\) or not. Given that this is what I know, can we devise a hypothesis test to solve our problem? The data, along with the hypothesised distribution from which they are thought to arise, are shown in Figure 11.1 . Not entirely obvious what the right answer is, is it? For this, we are going to need some statistics.\n\n\n\n\n\nFigure 11.1: The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated\n\n\n\n\n\n\n11.1.2 建立z檢定的假設\nThe first step in constructing a hypothesis test is to be clear about what the null and alternative hypotheses are. This isn’t too hard to do. Our null hypothesis, \\(H_0\\), is that the true population mean \\(\\mu\\) for psychology student grades is \\(67.5\\%\\), and our alternative hypothesis is that the population mean isn’t \\(67.5\\%\\). If we write this in mathematical notation, these hypotheses become:\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\nthough to be honest this notation doesn’t add much to our understanding of the problem, it’s just a compact way of writing down what we’re trying to learn from the data. The null hypotheses \\(H_0\\) and the alternative hypothesis \\(H_1\\) for our test are both illustrated in Figure 11.2. In addition to providing us with these hypotheses, the scenario outlined above provides us with a fair amount of background knowledge that might be useful. Specifically, there are two special pieces of information that we can add:\n\nThe psychology grades are normally distributed.\nThe true standard deviation of these scores \\(\\sigma\\) is known to be 9.5.\n\nFor the moment, we’ll act as if these are absolutely trustworthy facts. In real life, this kind of absolutely trustworthy background knowledge doesn’t exist, and so if we want to rely on these facts we’ll just have make the assumption that these things are true. However, since these assumptions may or may not be warranted, we might need to check them. For now though, we’ll keep things simple.\n\n\n\n\n\nFigure 11.2: Graphical illustration of the null and alternate hypotheses assumed by the one sample \\(z\\)-test (the two sided version, that is). The null and alternate hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value \\(\\$sigma_0\\)). The null hypothesis (left) is that the population mean \\(\\mu\\) is equal to some specified value \\(\\mu_0\\). The alternative hypothesis (right) is that the population mean differs from this value, \\(\\mu \\neq \\mu_0\\)\n\n\n\n\nThe next step is to figure out what we would be a good choice for a diagnostic test statistic, something that would help us discriminate between \\(H_0\\) and \\(H_1\\). Given that the hypotheses all refer to the population mean \\(\\mu\\), you’d feel pretty confident that the sample mean \\(\\bar{X}\\) would be a pretty useful place to start. What we could do is look at the difference between the sample mean \\(\\bar{X}\\) and the value that the null hypothesis predicts for the population mean. In our example that would mean we calculate \\(\\bar{X} - 67.5\\). More generally, if we let \\(\\mu_0\\) refer to the value that the null hypothesis claims is our population mean, then we’d want to calculate\n\\[\\bar{X}-\\mu_0\\]\nIf this quantity equals or is very close to 0, things are looking good for the null hypothesis. If this quantity is a long way away from 0, then it’s looking less likely that the null hypothesis is worth retaining. But how far away from zero should it be for us to reject H0?\nTo figure that out we need to be a bit more sneaky, and we’ll need to rely on those two pieces of background knowledge that I wrote down previously; namely that the raw data are normally distributed and that we know the value of the population standard deviation \\(\\sigma\\). If the null hypothesis is actually true, and the true mean is \\(\\mu_0\\), then these facts together mean that we know the complete population distribution of the data: a normal distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma\\).3\nOkay, if that’s true, then what can we say about the distribution of \\(\\bar{X}\\)? Well, as we discussed earlier (see Section 8.3.3), the sampling distribution of the mean \\(\\bar{X}\\) is also normal, and has mean \\(\\mu\\). But the standard deviation of this sampling distribution \\(\\\\{se(\\bar{X})\\\\}\\), which is called the standard error of the mean, is 4\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\nNow comes the trick. What we can do is convert the sample mean \\(\\bar{X}\\) into a standard score (see Section 4.5). This is conventionally written as z, but for now I’m going to refer to it as \\(z_{\\bar{X}}\\). The reason for using this expanded notation is to help you remember that we’re calculating a standardised version of a sample mean, not a standardised version of a single observation, which is what a z-score usually refers to). When we do so the z-score for our sample mean is\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\] or, equivalently \\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\nThis z-score is our test statistic. The nice thing about using this as our test statistic is that like all z-scores, it has a standard normal distribution:5\nIn other words, regardless of what scale the original data are on, the z-statistic itself always has the same interpretation: it’s equal to the number of standard errors that separate the observed sample mean \\(\\bar{X}\\) from the population mean \\(\\mu_0\\) predicted by the null hypothesis. Better yet, regardless of what the population parameters for the raw scores actually are, the 5% critical regions for the z-test are always the same, as illustrated in Figure 11.3. And what this meant, way back in the days where people did all their statistics by hand, is that someone could publish a table like Table 11.1. This, in turn, meant that researchers could calculate their z-statistic by hand and then look up the critical value in a text book.\n\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\nTable 11.1:  Critical values for different alpha levels \n\ncritical z value\n\ndesired \\(\\alpha\\) leveltwo-sided testone-sided test\n\n.11.6448541.281552\n\n.051.9599641.644854\n\n.012.5758292.326348\n\n.0013.2905273.090232\n\n\n\n\n\n\n\n11.1.3 手作z檢定\nNow, as I mentioned earlier, the z-test is almost never used in practice. It’s so rarely used in real life that the basic installation of jamovi doesn’t have a built in function for it. However, the test is so incredibly simple that it’s really easy to do one manually. Let’s go back to the data from Dr Zeppo’s class. Having loaded the grades data, the first thing I need to do is calculate the sample mean, which I’ve already done (\\(72.3\\)). We already have the known population standard deviation (\\(\\sigma = 9.5\\)), and the value of the population mean that the null hypothesis specifies (\\(\\mu_0 = 67.5\\)), and we know the sample size (\\(N=20\\)).\n\n\n\n\n\nFigure 11.3: Rejection regions for the two-sided z-test (panel (a)) and the one-sided z-test (panel (b))\n\n\n\n\nNext, let’s calculate the (true) standard error of the mean (easily done with a calculator):\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\nAnd finally, we calculate our z-score:\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\nAt this point, we would traditionally look up the value \\(2.26\\) in our table of critical values. Our original hypothesis was two-sided (we didn’t really have any theory about whether psych students would be better or worse at statistics than other students) so our hypothesis test is two-sided (or two-tailed) also. Looking at the little table that I showed earlier, we can see that \\(2.26\\) is bigger than the critical value of \\(1.96\\) that would be required to be significant at \\(\\alpha = .05\\), but smaller than the value of \\(2.58\\) that would be required to be significant at a level of \\(\\alpha = .01\\). Therefore, we can conclude that we have a significant effect, which we might write up by saying something like this:\n\nWith a mean grade of \\(73.2\\) in the sample of psychology students, and assuming a true population standard deviation of \\(9.5\\), we can conclude that the psychology students have significantly different statistics scores to the class average (\\(z = 2.26, N = 20, p<.05\\)).\n\n\n\n11.1.4 z檢定的適用條件\nAs I’ve said before, all statistical tests make assumptions. Some tests make reasonable assumptions, while other tests do not. The test I’ve just described, the one sample z-test, makes three basic assumptions. These are:\n\nNormality. As usually described, the z-test assumes that the true population distribution is normal.6 This is often a pretty reasonable assumption, and it’s also an assumption that we can check if we feel worried about it (see Section on [Checking the normality of a sample]).\nIndependence. The second assumption of the test is that the observations in your data set are not correlated with each other, or related to each other in some funny way. This isn’t as easy to check statistically, it relies a bit on good experimental design. An obvious (and stupid) example of something that violates this assumption is a data set where you “copy” the same observation over and over again in your data file so that you end up with a massive “sample size”, which consists of only one genuine observation. More realistically, you have to ask yourself if it’s really plausible to imagine that each observation is a completely random sample from the population that you’re interested in. In practice this assumption is never met, but we try our best to design studies that minimise the problems of correlated data.\nKnown standard deviation. The third assumption of the z-test is that the true standard deviation of the population is known to the researcher. This is just stupid. In no real world data analysis problem do you know the standard deviation σ of some population but are completely ignorant about the mean \\(\\mu\\). In other words, this assumption is always wrong.\n\nIn view of the stupidity of assuming that \\(\\alpha\\) is known, let’s see if we can live without it. This takes us out of the dreary domain of the z-test, and into the magical kingdom of the t-test, with unicorns and fairies and leprechauns!"
  },
  {
    "objectID": "11-Comparing-two-means.html#平均數檢定的更多細節",
    "href": "11-Comparing-two-means.html#平均數檢定的更多細節",
    "title": "11  比較單一與兩組平均值",
    "section": "11.4 平均數檢定的更多細節",
    "text": "11.4 平均數檢定的更多細節\n無論你想以哪種方式思考，現在我們已經有了我們的樣本標準差的匯集估計值。從現在開始，我會忽略那個奇怪的p小標，只稱這個估計值為\\(\\hat{\\sigma}\\)。很好。現在讓我們回到考慮這個該死的假設檢驗，好嗎？我們計算這個匯集估計的整個原因是我們知道它將在計算標準誤時非常有用。但標準誤是什麼？在單樣本t檢驗中，它是樣本平均值的標準誤，\\(se(\\bar{X})\\)，因為\\(se(\\bar{X}) = \\frac{\\sigma}{\\sqrt{N}}\\)，這就是我們的t統計量的分母。然而，這一次，我們有兩個樣本均值。而我們特別感興趣的是兩者之間的差異\\(\\bar{X}_1-\\bar{X}_2\\)，因此，我們需要除以的標準誤實際上是均值差的標準誤。\n[其他技術細節14]\n正如我們在單樣本檢定中所看到的那樣，只要虛無假設為真且測試的所有假設都符合，這個 t 統計量的抽樣分布就是一個 t 分佈。然而，自由度略有不同。通常，我們可以把自由度視為數據點數減去約束數。在這個情況下，我們有 N 個觀察值（\\(N_1\\) 在樣本 1 中，\\(N_2\\) 在樣本 2 中），和 2 個約束（樣本均值）。因此，這個檢定的總自由度為 \\(N-2\\)。\n\n\n11.4.1 jamovi實作\n不出所料，您可以很容易地在 jamovi 中進行獨立樣本 t 檢定。我們測試的結果變量是學生成績，而組是根據每個班級的導師定義的。因此，您可能不會對 jamovi 中的相應分析（“分析”-“T 檢定”-“獨立樣本 T 檢定”）感到意外，只需將成績變量移動到“依變變量”框中，將導師變量移動到“分組變量”框中，如 Figure 11.9 所示。\n\n\n\n\n\n\nFigure 11.9: jamovi執行獨立樣本t檢定示範，請留意圖中勾選的項目。\n\n\n\n\n這裡的輸出形式非常熟悉。首先，它告訴您運行的是什麼測試，以及使用的依變量的名稱。然後報告測試結果。與上次一樣，測試結果包括 t 統計量、自由度和 p 值。最後一部分報告了兩件事：它給出了置信區間和效應大小。我會稍後談論效應大小。然而，現在我應該談論置信區間。\n很重要要清楚這個信賴區間到底是什麼意思，它是兩組平均數之間的差異的信賴區間。在這個例子中，Anastasia的學生平均分為74.53，而Bernadette的學生平均分為69.06，因此兩組平均數之間的差異是5.48。但是，兩個母體的平均數差異可能比這個還大或者還小。在@fig-fig11-10中報告的信賴區間告訴你，如果我們重複進行這個研究，那麼在95％的時間裡，真正的平均數差異會在0.20到10.76之間。回顧一下@sec-Estimating-a-confidence-interval了解信賴區間的意思。\n在任何情況下，兩組之間的差異是顯著的（僅僅）。因此，我們可以使用以下文本編寫結果:\n\nAnastasia 班級的平均成績為 \\(74.5\\%\\)（標準差為 \\(9.0\\)），而 Bernadette 班級的平均成績為 \\(69.1\\%\\)（標準差為 \\(5.8\\)）。 學生的獨立樣本 t 檢驗顯示此 \\(5.4\\%\\) 差異是顯著的 \\((t(31) = 2.1, p<.05, CI_{95} = [0.2, 10.8], d = .74)\\)，表明學習成果存在真正的差異。\n\n注意到我在統計區塊中包含了信心區間和效應大小。人們並不總是這樣做。至少，您應該期望看到t統計量、自由度和p值。所以您應該至少包含像這樣的內容：\\(t(31) = 2.1, p< .05\\)。如果統計學家得到了他們想要的結果，那麼每個人都會報告信心區間，可能還有效應大小測量，因為它們是有用的知識。但現實並不總是按照統計學家的期望運作，所以您應該根據您認為會幫助讀者的情況進行判斷，如果您正在撰寫科學論文，那麼應根據期刊的編輯標準進行決策。有些期刊希望您報告效應大小，而有些則不是。在某些科學社區中，報告信心區間是標準做法，在其他社區中則不是。您需要找出您的觀眾期望什麼。但是，僅僅為了清晰起見，如果您正在上我的課，我的默認立場是通常值得包括效應大小和信心區間。\n\n\n\n11.4.2 t統計值正負的意義\n在談到t-test的假設之前，我們先來討論t-test在實務上的使用上還有一個重要的觀點。第一個觀點是關於t統計量的符號（即它是正數還是負數）。當學生第一次進行t-test時，他們常常會擔心結果出現負值，不知道如何解釋。實際上，當兩個人獨立地進行實驗時，獲得的結果幾乎相同，但其中一個人的t值是負的，而另一個人的t值是正的，這種情況是很常見的。假設您正在進行雙側檢定，那麼p值是相同的。仔細檢查後，學生們會發現置信區間也是相反的。這是完全正常的。每當這種情況發生時，你會發現兩種不同的結果是由稍微不同的t-test方法引起的。這裡發生的事情非常簡單。我們在這裡計算的t統計量總是具有以下形式\n\\[t=\\frac{\\text{平均值1-平均值2}}{SE}\\]\n如果 “平均值 1” 大於 “平均值 2”，則 t 統計量將為正數，而如果 “平均值 2” 大於 “平均值 1”，則 t 統計量將為負數。同樣地，jamovi 報告的置信區間是 “(平均值 1) 減去 (平均值 2)” 差異的置信區間，這將是計算 “(平均值 2) 減去 (平均值 1)” 差異置信區間的相反。\n好的，當你考慮這件事時，其實很簡單，但現在考慮比較阿納斯塔西亞班和伯納德班的 t 檢定。我們應該把哪一個稱為 “mean 1”，哪一個稱為 “mean 2”。這是任意的。然而，你真的需要將其中之一指定為 “mean 1”，另一個為 “mean 2”。不出所料，jamovi 處理這個的方式也相當任意。在本書早期的版本中，我試圖解釋這一點，但過了一段時間後，我放棄了，因為這並不是真正重要的事情，老實說我自己也記不清楚。每當我得到一個顯著的 t 檢定結果，並且我想找出哪個均值較大，我不會嘗試通過查看 t 統計量來找出答案。我為什麼要這麼做？這是愚蠢的。最簡單的方法就是查看實際的組均值，因為 jamovi 的輸出實際上就顯示了它們！\n這是重要的一點。因為 jamovi 向您顯示的內容實際上並不重要，所以我通常會嘗試以使數字與文本相符的方式報告 t 統計量。假設我想在報告中寫道：「阿納斯塔西亞的班級的成績高於伯納德的班級。」這種措辭意味著阿納斯塔西亞的組排在第一位，因此把阿納斯塔西亞的班級對應到第一組是有道理的。如果是這樣，我會寫成 阿納斯塔西亞的班级成绩比伯纳德的班级更高\\((t(31) = 2.1，p = .04)\\)。\n（在現實中我不會真的用下劃線來強調 “higher”，我只是這樣做是為了強調 “higher” 與正的 t 值對應）。另一方面，假設我想使用的措辭是 Bernadette 的班級排在第一位，如果這樣的話，把她的班級作為第一組更有意義，這樣的話報告應該寫成：Bernadette 的班級的成績比 Anastasia 的班級更低 \\((t(31) = -2.1, p = .04)\\)。\n最後要注意的是，這種寫法只適用於t檢定，對於卡方檢定、F檢定或本書中提到的大多數檢定而言並無意義。因此不要過度將這個建議泛化！這裡我真的只是在談論t檢定，而不是其他任何東西！\n\n\n\n11.4.3 獨立樣本平均數檢定的適用條件\n和單樣本t檢定一樣，學生t檢定也有三個假設，其中一些在單一樣本t檢定的適用條件已經提到：\n\n常態性。和單樣本t檢定一樣，需要假設資料是常態分佈。具體來說，需要假設兩組數據都符合常態分佈。15在[檢查樣本正態性]一節中，我們將討論如何檢查正態性，而在[檢驗非正態數據]一節中，我們將討論可能的解決方案。\n獨立性。再次需要假設觀測值是獨立抽樣的。在學生t檢定的情況下，有兩個方面需要考慮。首先，需要假設每個樣本內的觀測值是相互獨立的（和單樣本t檢定一樣）。但是，我們還需要假設兩個樣本之間沒有相互依賴關係。例如，如果實驗中同一個人被不小心分配到不同的條件下（例如，同一個人不小心被允許報名參加不同的條件下的實驗），那麼就存在一些跨樣本的依賴關係需要考慮。\n方差的同質性（也稱為“等變異性”）。第三個假設是，兩組的母體標準差是相同的。可以使用Levene檢定來測試這個假設，我們稍後會在[檢查同質變異性的假設]中談到。然而，如果您擔心這個假設，有一個非常簡單的解決方法，我們將在下一節中討論。"
  },
  {
    "objectID": "11-Comparing-two-means.html#相依樣本t檢定",
    "href": "11-Comparing-two-means.html#相依樣本t檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.6 相依樣本t檢定",
    "text": "11.6 相依樣本t檢定\n不論是談論學生 t 檢驗或韋爾奇 t 檢驗，獨立樣本 t 檢驗都適用於具有兩個互相獨立樣本的情況。當參與者被隨機分配到其中一個實驗條件時，自然會出現這種情況，但這對於其他類型的研究設計提供了一個非常差的近似。特別是，在重複測量設計中，每個參與者都在兩個實驗條件下測量（對於相同的結果變量），並不適合使用獨立樣本 t 檢驗進行分析。例如，我們可能會對聆聽音樂是否降低人們的工作記憶容量感興趣。為此，我們可以在兩種情況下測量每個人的工作記憶容量：有音樂和無音樂。在這樣的實驗設計中，18每個參與者出現在 兩個 組中。這需要我們以不同的方式來解決問題，即使用配對樣本 t 檢驗。\n\n\n11.6.1 示範資料\n這次我們要使用的數據集來自Chico博士的班級。19 在她的課堂上，學生要參加兩次主要考試，一次在學期初，一次在學期後。據她所說，她開的課很難，大多數學生都覺得很有挑戰性。但她認為，通過設置困難的評估，學生會被鼓勵更加努力地學習。她的理論是，第一次考試對學生來說是一個“提醒”，當他們意識到她的課有多難時，他們會為第二次考試更加努力，取得更好的成績。她的觀點正確嗎？為了測試這個問題，讓我們將chico.csv文件導入到jamovi中。這次，jamovi在導入期間做了一個好工作，正確地分類了變量的測量水平。chico數據集包含三個變量：一個id變量，用於識別班級中的每個學生，grade_test1變量記錄第一次考試的學生成績，grade_test2變量則是第二次考試的成績。\n如果我們看一下 jamovi 的試算表，似乎這個班級很難（大多數的成績都在50%到60%之間），但從第一次測驗到第二次測驗似乎有進步的趨勢。\n如果我們快速查看一下描述性統計，在@fig-fig11-12中，我們可以看到這種印象似乎得到了支持。在所有20個學生中，第一次測驗的平均成績為57%，但第二次測驗的平均成績為58%。但是，考慮到標準差分別為6.6%和6.4%，這種進步感覺起來可能只是虛假的，也可能只是隨機變異。當你看到在@fig-fig11-13a中繪製的平均值和置信區間時，這種印象得到了加強。如果我們僅僅依靠這個圖表，看看這些置信區間有多寬，我們會認為學生表現的明顯改善純粹是偶然的。\n\n\n\n\n\n\nFigure 11.12: chico資料集中的兩次測驗成績資料及描述統計\n\n\n\n\n然而，這種印象是錯的。要知道原因，請看圖 Figure 11.13 (b) 中顯示的評分1和評分2的散佈圖。在這個圖中，每個點對應於一個給定學生的兩個成績。如果他們的評分1（x座標）等於他們的評分2（y座標），那麼該點就會落在直線上。在線上方的點是第二次測試表現更好的學生。重要的是，幾乎所有的數據點都在對角線以上：幾乎所有的學生似乎都有一些提高，即使只有一點點。這表明我們應該關注每個學生在一次測試和下一次測試中所取得的進步，並將其作為我們的原始數據。為此，我們需要創建一個新變量，用於表示每個學生所取得的進步，並將其添加到 chico 數據集中。最簡單的方法是計算一個新變量，使用表達式 grade test2 - grade test1。\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 11.13: 第一次測驗和第二次測驗的平均分數，以及相應的95％置信區間(a)。顯示第一次測驗和第二次測驗的個別分數的散點圖(b)。 \n\n\n我們一旦計算了這個新的改進變量，就可以繪製一個直方圖，顯示這些改進分數的分布，如@fig-fig11-14所示。當我們觀察這個直方圖時，很明顯這裡有真正的進步。絕大多數的學生在第二次測試中得分比第一次高，這反映在幾乎整個直方圖都在零以上。\n\n\n\n\nFigure 11.14: 柱狀圖顯示了 Chico 博士班上每位學生的成績提升。注意到幾乎整個分布都在 0 的上方——大多數學生在第二次考試中的表現確實有所提升。\n\n\n\n\n11.6.2 深入認識相依樣本t檢定\n根據先前的探索，讓我們思考如何建立一個適當的 t 檢定。一個可能的方法是嘗試使用grade_test1和grade_test2作為感興趣的變量進行獨立樣本 t 檢定。然而，這顯然是錯誤的，因為獨立樣本 t 檢定假定兩個樣本之間沒有特定的關係。然而，由於數據中的重複測量結構，這顯然是不正確的。使用我在上一節中介紹的語言，如果我們試圖進行獨立樣本 t 檢定，我們會混淆 樣本內 差異（這是我們想要測試的）和 樣本間 變異性（這是我們不想要的）。\n解決這個問題的方法很明顯，我希望，因為我們已經在前一節中完成了所有的艱苦工作。我們不是對grade_test1和grade_test2進行獨立樣本t檢驗，而是對內部差異變量improvement進行單樣本t檢驗。稍微形式化一下，如果\\(X_{i1}\\)是第i個參與者在第一個變量上獲得的分數，\\(X_{i2}\\)是同一個人在第二個變量上獲得的分數，那麼差異分數是：\n\\[D_i=X_{i1}-X_{i2}\\]\n請注意，差異分數是變量1減去變量2，而不是反過來，因此如果我們希望改進對應到一個正值的差異，我們實際上需要將「測試2」作為「變量1」。同樣，我們會說 \\(\\mu_D = \\mu_1 - \\mu_2\\) 是此差異變量的母體平均值。因此，為了將其轉換為假設檢驗，我們的虛無假設是此平均差異為零，而對立假設是它不是\n\\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]\n這假設我們進行的是雙邊檢定。這與我們描述一樣，進行一樣的假設檢定。唯一的不同之處在於零假設所預測的特定值為0。因此，我們的 t 統計量可以用類似的方式來定義。如果我們讓 \\(\\bar{D}\\) 代表差異得分的平均值，那麼\n\\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\]\n其中 \\(\\hat{\\sigma}_D\\) 是差異得分的標準差。因為這只是一個普通的單樣本 t 檢定，沒有什麼特別的地方，所以自由度仍然是 \\(N - 1\\)。這就是全部。實際上，配對樣本 t 檢定並不是一個新的檢定，它是一個應用在兩個變數之間差異上的單樣本 t 檢定。它實際上非常簡單。它需要進行長時間的討論的唯一原因是，您需要能夠辨認何時適用配對樣本 t 檢定，以及為什麼它比獨立樣本 t 檢定更好。\n\n\n\n11.6.3 實作相依樣本t檢定\n如何在jamovi中執行配對樣本t檢定？一種可能的方法是按照我上面概述的過程進行。即，創建一個“差異”變量，然後對其進行單樣本t檢定。因為我們已經創建了一個名為improvement的變量，讓我們這麼做並看看我們得到什麼，如@fig-fig11-15所示。\n\n\n\n\n\n\nFigure 11.15: 配對差異分數的單一樣本t檢定結果表。\n\n\n\n\n在@fig-fig11-15中顯示的輸出與上一次使用單樣本t檢定分析時（Section 11.2）完全相同的格式，並確認了我們的直覺。從第一次測試到第二次測試，平均改善了1.4％，並且這與0顯著不同\\((t(19)=6.48,p<.001)\\)。\n然而，假設您很懶，不想花費大量精力創建新變量。或者也許您只想保持單樣本和配對樣本測試之間的差異清晰。如果是這樣，您可以使用jamovi的“配對樣本t檢定”分析，獲得@fig-fig11-16中顯示的結果。\n\n\n\n\n\n\nFigure 11.16: 相依樣本t檢定結果表。請與 Figure 11.15 比較看看。\n\n\n\n\n結果跟進行單樣本t檢定時的結果相同，這當然是因為成對樣本t檢定在實質上就是對差異變數執行單樣本t檢定。"
  },
  {
    "objectID": "11-Comparing-two-means.html#單尾檢定",
    "href": "11-Comparing-two-means.html#單尾檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.7 單尾檢定",
    "text": "11.7 單尾檢定\n當介紹零假設檢定理論時，我提到有一些情況適合指定單邊檢定（請參閱 Section 9.4.3 ）。到目前為止，所有的t檢定都是雙邊檢定。例如，當我們為Zeppo博士課堂上的成績指定單一樣本t檢定時，零假設是真實均值為\\(67.5\\%\\)。對立假設是真實均值高於或低於\\(67.5\\%\\)。假設我們只想知道真實均值是否高於\\(67.5\\%\\)，並且完全沒有興趣測試真實均值是否低於\\(67.5\\%\\)。如果是這樣，我們的零假設將是真實均值為\\(67.5\\%\\)或更低，對立假設將是真實均值高於\\(67.5\\%\\)。在jamovi中，對於“單一樣本t檢定”分析，您可以在“假設”下點擊“> Test Value”選項進行指定。這樣做後，您將得到@fig-fig11-17中顯示的結果。\n\n\n\n\n\n\nFigure 11.17: jamovi生成的「單一樣本t檢定」結果，其中實際假設是單側的，即真實平均數大於 \\(67.5%\\)\n\n\n\n\n注意到輸出與上次看到的輸出有一些變化。最重要的是實際假設已更改，以反映不同的測試。第二件要注意的事情是儘管t統計量和自由度沒有改變，但p值已經改變。這是因為單側測試具有與雙側測試不同的拒絕區域。如果您忘記了這是為什麼以及它意味著什麼，您可以回顧一下@sec-Hypothesis-testing，特別是@sec-The-difference-between-one-sided-and-two-sided-tests。第三件要注意的是置信區間也不同：現在報告了一個“單側”的置信區間，而不是一個雙側的置信區間。在雙側置信區間中，我們試圖找到數字a和b，使得如果我們重複進行該研究多次，那麼有\\(95\\%\\)的機會均值會落在a和b之間。在單側置信區間中，我們試圖找到一個數字a，使得我們有信心有\\(95\\%\\)的機會真實均值會大於a（如果您在“假設”部分中選擇了測量1 < 測量2則為小於a）。\n所有版本的t檢定都可以是單側的。對於獨立樣本t檢定，如果您只想測試A組的得分是否比B組高，但對於是否要找出B組的得分是否高於A組沒有興趣，那麼您可以進行單側檢定。假設對於Harpo博士的課程，您想知道Anastasia的學生是否比Bernadette的學生成績更好。對於此分析，在“假設”選項中指定“Group 1 > Group2”。您應該可以得到如圖@fig-fig11-18所示的結果。\n\n\n\n\n\n\nFigure 11.18: jamovi生成的’獨立樣本t檢定’分析，實際假設為單尾檢定，即 Anastasia 的學生的成績高於 Bernadette 的學生\n\n\n\n\n再次強調，輸出結果有可預測的變化。替代假設的定義已改變，p值已變化，現在報告的是單邊信賴區間，而不是雙邊信賴區間。\n那麼，配對樣本t檢驗呢？假設我們想要測試 Dr Zeppo 課堂上考試成績是否從第一次測試到第二次測試有所提高，並且不考慮成績下降的可能性。在 jamovi 中，您可以通過在「假設」選項下指定 grade_test2 （在 jamovi 中為「測量1」，因為我們首先將其複製到了配對變量框中）> grade_test1 （在 jamovi 中為「測量2」）來實現這一點。您應該可以看到 Figure 11.19 中的結果。\n\n\n\n\n\n\n\nFigure 11.19: jamovi生成的’相依樣本t檢定’分析，實際假設為單尾檢定，即測驗1成績 > 測驗2成績。\n\n\n\n\n這次的輸出和之前一樣，以可預測的方式改變。假設已經改變，p值也改變，並且置信區間現在是單邊的。"
  },
  {
    "objectID": "11-Comparing-two-means.html#t檢定的效果量",
    "href": "11-Comparing-two-means.html#t檢定的效果量",
    "title": "11  比較單一與兩組平均值",
    "section": "11.8 t檢定的效果量",
    "text": "11.8 t檢定的效果量\n以下是您將翻譯成繁體中文的部分。\n對於t檢驗，最常用的效應量測量方法是科恩的d (Cohen, 1988)。從原理上看，它是一個非常簡單的方法，但當您深入研究細節時，會有相當多的變化。科恩本人主要在獨立樣本t檢驗的上下文中對其進行了定義，特別是學生檢驗。在該背景下，定義效應量的自然方法是將均值之間的差除以標準差的估計。換句話說，我們要計算的是類似於以下公式的東西：\n\\[d=\\frac{(\\text{平均值1})-(\\text{平均值2})}{\\text{標準差}}\\]\n他在 ?tbl-tab11-3中建議了一個解釋$d$的粗略指南。\n\n\n\n\n\nTable 11.3:  解釋科恩的d的（非常）粗略指南。我的個人建議是不要盲目地使用這些。d統計量本身具有自然的解釋。它將均值之間的差重新描述為將這些均值分開的標準差數。因此，通常最好考慮一下這在實際條件下意味著什麼。在某些情境下，一個「小」的效應可能具有很大的實際重要性。在其他情況下，一個「大」的效應可能並不是那麼有趣。 \n\nd-valuerough interpretation\n\nabout 0.2\"small\" effect\n\nabout 0.5\"moderate\" effect\n\nabout 0.8\"large\" effect\n\n\n\n\n\n以下是一些您將翻譯成繁體中文的單詞。\n您可能會認為這應該是非常明確的，但事實並非如此。這主要是因為科恩對他認為應該用作標準差測量的方法並未過多具體（在他的辯護中，他在書中試圖闡述一個更廣泛的觀點，而不是對微小細節吹毛求疵）。如@McGrath2006所討論，常用的有幾個不同版本，每位作者都傾向於採用略有不同的表示法。為了簡單起見（而非準確性），我將使用d來指代您從樣本中計算出的任何統計量，並使用\\(\\delta\\)來指代理論的群體效應。顯然，這確實意味著有幾個不同的東西都被稱為d。\n我的懷疑是，您需要科恩的d的唯一時刻是當您運行t檢驗時，jamovi提供了一個選項可以計算它提供的所有不同類型t檢驗的效應量。\n\n\n11.8.1 單一樣本的Cohen’s d\n最簡單的情況是與單樣本t檢驗相對應的情況。在這種情況下，這是一個樣本均值\\(\\bar{X}\\)和一個（假設的）群體均值\\(\\mu_0\\)進行比較。不僅如此，真正只有一種合理的方法來估計群體標準差。我們只需使用我們通常的估計\\(\\hat{\\sigma}\\)。因此，我們最終得出以下計算\\(d\\)的唯一方法\n\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]\n當我們回顧@fig-fig11-6中的結果時，效應量值是科恩的\\(d = 0.50\\)。因此，總的來說，Zeppo博士班上的心理學生取得的成績（\\(mean = 72.3\\%\\)）比您預期的水平（\\(67.5\\%\\)）高出約0.5個標準差，如果他們的表現與其他學生相同。根據科恩的粗略指南，這是一個中等效應量。\n\n\n\n11.8.2 獨立樣本的Cohen’s d\n大多數關於科恩的\\(d\\)的討論都集中在與Student獨立樣本t檢驗相似的情況上，正是在這種情境下，故事變得更加混亂，因為在這種情況下，您可能想要使用幾個不同版本的\\(d\\)。為了理解為什麼\\(d\\)有多個版本，抽點時間寫下與真實群體效應大小\\(\\delta\\)相對應的公式是有幫助的。這很簡單，\n\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\]\n其中，和往常一樣，\\(\\mu_1\\)和\\(\\mu_2\\)分別是與第1組和第2組相對應的群體均值，\\(\\sigma\\)是標準差（兩個群體都相同）。顯然，估計\\(\\delta\\)的方法是做我們在t檢驗本身中所做的完全相同的事情，即在分子使用樣本均值，並在分母使用池化標準差估計值\n\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\n其中\\(\\hat{\\sigma}_p\\)是t檢驗中出現的完全相同的池化標準差度量。這是應用於Student t檢驗結果時最常用的科恩的d版本，也是jamovi中提供的版本。有時它被稱為Hedges的\\(g\\)統計量(Hedges, 1981)。\n然而，還有其他可能性，我將簡要描述。首先，您可能有理由只想用兩個組中的一個作為計算標準差的基礎。這種方法（通常稱為Glass的\\(\\triangle\\)，讀作delta）在您有充分理由將兩個組中的一個視為比另一個更純粹地反映「自然變異」時，才最有意義。例如，如果兩個組中的一個是對照組，就可能發生這種情況。其次，回憶一下，在計算池化標準差的過程中，我們通常會除以\\(N - 2\\)以糾正樣本方差的偏差。在科恩的d的一個版本中，省略了這個糾正，而是除以\\(N\\)。當您試圖在樣本中計算效應量而不是估計群體中的效應量時，這個版本主要是有意義的。最後，有一個基於@Hedges1985的版本，叫做Hedge的g，他指出在科恩的d的常規（池化）估計中存在一個小的偏差。20\n\n\n\n11.8.3 相依樣本的Cohen’s d\n最後，對於配對樣本t檢驗，我們應該怎麼做？在這種情況下，答案取決於您試圖做什麼。jamovi假設您希望根據差異分數的分佈來衡量效應量，您計算的d度量為：\n\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\n其中\\(\\hat{\\sigma}_D\\)是差異的標準差估計。在@fig-fig11-16中，科恩的\\(d = 1.45\\)，表示時間2的分數平均比時間1的分數高出\\(1.45\\)個標準差。\n這是jamovi「配對樣本T檢驗」分析報告的科恩\\(d\\)版本。唯一的麻煩是弄清楚這是否是您想要的度量。在您關心研究的實際後果的程度上，您通常希望根據原始變量而不是差異分數（例如，與學生間的成績變異量相比，Chico博士班上隨著時間的1%改進相當小）來衡量效應量，在這種情況下，您使用的是與Student或Welch檢驗相同的科恩d版本。在jamovi中做到這一點並不那麼簡單；基本上您必須在試算表視圖中更改數據結構，所以我在這裡不會深入討論21，但是從這個角度看，科恩的d相當不同：它是\\(0.22\\)，在原始變量的尺度上評估相當小。"
  },
  {
    "objectID": "11-Comparing-two-means.html#非常態資料的檢定",
    "href": "11-Comparing-two-means.html#非常態資料的檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 非常態資料的檢定",
    "text": "11.10 非常態資料的檢定\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 獨立樣本的曼－惠特尼U檢定\n<<<<<<< HEAD I’ll start by describing the 曼－惠特尼U檢定(Mann-Whitney U test), since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group. ======= I’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group. >>>>>>> 49b1e97cffa47ef082c903513735779e8d1e5ed6\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.4).\n\n\n\n\nTable 11.4:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 24 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for the Mann-Whitney \\(U\\) test\n\n\n\n\n\n\n11.10.2 單一樣本的Wilcoxon檢定\n<<<<<<< HEAD What about the one sample Wilcoxon檢定(Wilcoxon test) (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5. ======= What about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5. >>>>>>> 49b1e97cffa47ef082c903513735779e8d1e5ed6\n\n\n\n\nTable 11.5:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\npositive differences7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests"
  },
  {
    "objectID": "11-Comparing-two-means.html#本章小結",
    "href": "11-Comparing-two-means.html#本章小結",
    "title": "11  比較單一與兩組平均值",
    "section": "11.11 本章小結",
    "text": "11.11 本章小結\n\n單一樣本z檢定 用來比對樣本平均值是否不同於母群平均值(各位可考慮跳過)。\n獨立樣本t檢定用於比較兩組平均值的差異，虛無假設設定兩組平均值相等。實務上會使用適用條件 是兩組變異相同的獨立樣本t檢定，或是兩組變異不同的獨立樣本t檢定(Welch t檢定)。\n相依樣本t檢定用於比較的資料來自同一個樣本的測量，虛無假設設定兩次測量的平均值相等。分析方法與適用條件與單一樣本t檢定相同。\n只有事前規劃差異比較方向，單尾檢定 才有真正的意義。\nt檢定的效果量以Cohen’ d的公式計算。\n我們能使用QQ圖及Shapiro-Wilk檢定檢測樣本常態性\n如果資料真的違反常態性，你可以使用等級資料的平均值檢定像是曼－惠特尼U檢定，以及Wilcoxon檢定。\n\n\n\n\n\n\nBox, J. F. (1987). Guinness, gosset, fisher, and small samples. Statistical Science, 2, 45–52.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nHedges, L. V. (1981). Distribution theory for glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6, 107–128.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52, 591–611.\n\n\nStudent, A. (1908). The probable error of a mean. Biometrika, 6, 1–2.\n\n\nWelch, B. L. (1947). The generalization of “Student’s” problem when several different population variances are involved. Biometrika, 34, 28–35."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html",
    "href": "12-Correlation-and-linear-regression.html",
    "title": "12  相闗與線性迴歸",
    "section": "",
    "text": "The goal in this chapter is to introduce correlation and linear regression. These are the standard tools that statisticians rely on when analysing the relationship between continuous predictors and continuous outcomes."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#correlations",
    "href": "12-Correlation-and-linear-regression.html#correlations",
    "title": "12  Correlation and linear regression",
    "section": "12.1 Correlations",
    "text": "12.1 Correlations\nIn this section we’ll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data (Table 12.1).\n\n12.1.1 The data\n\n\n\n\nTable 12.1:  Data for correlation analysis - descriptive statistics for the parenthood data \n\nvariableminmaxmeanmedianstd. devIQR\n\nDani's grumpiness419163.716210.0514\n\nDani's hours slept4.849.006.977.031.021.45\n\nDani's son's hours slept3.2512.078.057.952.073.21\n\n\n\n\n\nLet’s turn to a topic close to every parent’s heart: sleep. The data set we’ll use is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to \\(100\\) (grumpy as a very, very grumpy old man or woman). And lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for \\(100\\) days. And, being a nerd, I’ve saved the data as a file called parenthood.csv. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.1\nNext, I’ll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, Figure 12.1 plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.2 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s pretty standard.\n\n\n\n\n\nFigure 12.1: Histograms for the three interesting variables in the parenthood data set\n\n\n\n\n\n\n12.1.2 The strength and direction of a relationship\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dani.sleep and dani.grump (Figure 12.1), left) with that between baby.sleep and dani.grump (Figure 12.2), right). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\n\n\n\n\n\nFigure 12.2: Scatterplots showing the relationship between dani.sleep and dani.grump (left) and the relationship between baby.sleep and dani.grump (right)\n\n\n\n\nIn contrast, let’s consider the two scatterplots shown in Figure 12.3. If we compare the scatterplot of “baby.sleep v dani.grump” (left) to the scatterplot of “’baby.sleep v dani.sleep” (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps more then I get less grumpy (negative relationship, left hand side).\n\n\n\n\n\nFigure 12.3: Scatterplots showing the relationship between baby.sleep and dani.grump (left), as compared to the relationship between baby.sleep and dani.sleep (right)\n\n\n\n\n\n\n12.1.3 The correlation coefficient\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\) ), which we’ll define more precisely in the next section, is a measure that varies from -1 to 1. When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure 12.4, you can see several plots showing what different correlations look like.\n[Additional technical detail 3]\n\n\n\n\n\nFigure 12.4: Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are \\(0, .33, .66\\) and \\(1\\). In the right hand column, the correlations are \\(0, -.33, -.66\\) and \\(-1\\)\n\n\n\n\nBy standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in the section on Interpreting a correlation. But before I do, let’s look at how to calculate correlations in jamovi.\n\n\n12.1.4 Calculating correlations in jamovi\nCalculating correlations in jamovi can be done by clicking on the ‘Regression’ - ‘Correlation Matrix’ button. Transfer all four continuous variables across into the box on the right to get the output in Figure 12.5.\n\n\n\n\n\nFigure 12.5: A jamovi screenshot showing correlations between variables in the parenthood.csv file\n\n\n\n\n\n\n12.1.5 Interpreting a correlation\nNaturally, in real life you don’t see many correlations of \\(1\\). So how should you interpret a correlation of, say, r = \\(.4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table 12.2 is pretty typical.\n\n\n\n\nTable 12.2:  A rough guide to interpreting correlations. Note that I say a rough guide. There aren’t hard and fast rules for what counts as strong or weak relationships. It depends on the context. \n\nCorrelationStrengthDirection\n\n-1.0 to -0.9Very strongNegative\n\n-0.9 to -0.7StrongNegative\n\n-0.7 to -0.4ModerateNegative\n\n-0.4 to -0.2WeakNegative\n\n-0.2 to 0NegligibleNegative\n\n0 to 0.2NegligiblePositive\n\n0.2 to 0.4WeakPositive\n\n0.4 to 0.7ModeratePositive\n\n0.7 to 0.9StrongPositive\n\n0.9 to 1.0Very strongPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” (Anscombe, 1973), a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is \\(9\\) and the mean for \\(Y\\) is \\(7.5\\). The standard deviations for all \\(X\\) variables are almost identical, as are those for the Y variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since I happen to have saved it in a file called anscombe.csv.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 12.6, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is “always graph your raw data” (see Chapter 5).\n\n\n\n\n\nFigure 12.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of r = .816, but they are qualitatively different from one another\n\n\n\n\n\n\n12.1.6 Spearman’s rank correlations\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable Y , but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort (\\(X\\)) into learning a subject then you should expect a grade of \\(0\\%\\) (\\(Y\\)). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of \\(90\\%\\) than it takes to get a grade of \\(55\\%\\). What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure 12.7, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of \\(0.91\\). However, this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\n\n\n\n\n\nFigure 12.7: The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(\\rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved\n\n\n\n\nHow should we address this? Actually, it’s really easy. If we’re looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all \\(10\\) of our students in order of hours worked. That is, student \\(1\\) did the least work out of anyone (\\(2\\) hours) so they get the lowest rank (rank = \\(1\\)). Student \\(4\\) was the next laziest, putting in only \\(6\\) hours of work over the whole semester, so they get the next lowest rank (rank = \\(2\\)). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = \\(1\\)” to mean “top rank” rather than “bottom rank”. So be careful, you can rank “from smallest value to largest value” (i.e., small equals rank \\(1\\)) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, but as it’s really easy to forget which way you set things up you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward Table 12.3.\n\n\n\n\nTable 12.3:  Students ranked in terms of effort and reward \n\nrank (hours worked)rank (grade received)\n\nstudent 111\n\nstudent 21010\n\nstudent 366\n\nstudent 422\n\nstudent 533\n\nstudent 655\n\nstudent 744\n\nstudent 888\n\nstudent 977\n\nstudent 1099\n\n\n\n\n\nHmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation r. We can calculate Spearman’s \\(\\rho\\) using jamovi simply by clicking the ‘Spearman’ check box in the ‘Correlation Matrix’ screen."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#scatterplots",
    "href": "12-Correlation-and-linear-regression.html#scatterplots",
    "title": "12  Correlation and linear regression",
    "section": "12.2 Scatterplots",
    "text": "12.2 Scatterplots\nScatterplots are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on Correlations. It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in jamovi, using the same parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we’re after. The first way is to use the ‘Plot’ option under the ‘Regression’ - ‘Correlation Matrix’ button, giving us the output shown in Figure 12.8. Note that jamovi draws a line through the points, we’ll come onto this a bit later in the section on What is a linear regression model?. Plotting a scatterplot in this way also allow you to specify ‘Densities for variables’ and this option adds a density curve showing how the data in each variable is distributed.\n\n\n\n\n\nFigure 12.8: Scatterplot via the ‘Correlation Matrix’ command in jamovi\n\n\n\n\nThe second way do to it is to use one of the jamovi add-on modules. This module is called ‘scatr’ and you can install it by clicking on the large ‘\\(+\\)’ icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find ‘scatr’ and clicking ‘install’. When you have done this, you will find a new ‘Scatterplot’ command available under the ‘Exploration’ button. This plot is a bit different than the first way, see Figure 12.9, but the important information is the same.\n\n\n\n\n\nFigure 12.9: Scatterplot via the ‘scatr’ add-on module in - jamovi\n\n\n\n\n\n12.2.1 More elaborate options\nOften you will want to look at the relationships between several variables at once, using a scatterplot matrix (in jamovi via the ‘Correlation Matrix’ - ‘Plot’ command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in Figure 12.10.\n\n\n\n\n\nFigure 12.10: A matrix of scatterplots produced using jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#what-is-a-linear-regression-model",
    "href": "12-Correlation-and-linear-regression.html#what-is-a-linear-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.3 What is a linear regression model?",
    "text": "12.3 What is a linear regression model?\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see Correlations), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 12.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 12.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 12.11 (b).\n\n\n\n\n\nFigure 12.11: Panel (a) shows the sleep-grumpiness scatterplot from Figure 12.9 with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel (b) shows the same data, but with a very poor choice of regression line drawn over the top\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 12.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).4 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).5 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#estimating-a-linear-regression-model",
    "href": "12-Correlation-and-linear-regression.html#estimating-a-linear-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.4 Estimating a linear regression model",
    "text": "12.4 Estimating a linear regression model\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 12.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 12.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 12.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n12.4.1 Linear regression in jamovi\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 12.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 12.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 12.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n12.4.2 Interpreting the estimated model\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#multiple-linear-regression",
    "href": "12-Correlation-and-linear-regression.html#multiple-linear-regression",
    "title": "12  Correlation and linear regression",
    "section": "12.5 Multiple linear regression",
    "text": "12.5 Multiple linear regression\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n12.5.1 Doing it in jamovi\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 12.4.\n\n\n\n\nTable 12.4:  Adding multiple variables as predictors in a regression \n\n(Intercept)dani.sleepbaby.sleep\n\n125.97-8.950.01\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 12.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 12.14: A 3D visualisation of a multiple regression model. There are two predictors in the model, dani.sleep and baby.sleep and the outcome variable is dani.grump. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we’re trying to do is find a plane that is as close to all the blue dots as possible\n\n\n\n\n[Additional technical detail7]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#quantifying-the-fit-of-the-regression-model",
    "href": "12-Correlation-and-linear-regression.html#quantifying-the-fit-of-the-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.6 Quantifying the fit of the regression model",
    "text": "12.6 Quantifying the fit of the regression model\nSo we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction \\(\\hat{Y}_i\\) about what my mood is like, but my actual mood is \\(Y_i\\) . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n\n12.6.1 The \\(R^2\\) value\nOnce again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\nwhich we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\nWhile we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the \\(\\hat{Y}\\) values, and for the simple model that uses only a single predictor we would do the following:\n\ncreate a new column called ‘Y.pred’ using the formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’\ncalculate the SS(resid) by creating a new column called ‘(Y-Y.pred)^2’ using the formula ’ = (dani.grump - Y.pred)^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ’ sum( ( Y-Y.pred)^2 ) .\nAt the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ‘mean’ in its function).\nThen create a new column, called ’ (Y - mean(Y))^2 )’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ‘sum( (Y - mean(Y))^2 )’.\nCalculate R.squared by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.\n\nThis gives a value for \\(R^2\\) of ‘0.8161018’. The \\(R^2\\) value, sometimes called the coefficient of determination8 has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained \\(R^2 = .816\\) means that the predictor (my.sleep) explains \\(81.6\\%\\) of the variance in the outcome (my.grump).\nNaturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the \\(R^2\\) value for your regression model. As we’ll see later on in the section on Running the hypothesis tests in jamovi, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of \\(R^2\\) that I want to point out.\n\n\n12.6.2 The relationship between regression and correlation\nAt this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol \\(r\\) to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient \\(r\\) and the \\(R^2\\) value from linear regression? Of course there is: the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.\n\n\n12.6.3 The adjusted \\(R^2\\) value\nOne final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted \\(R^2\\)”. The motivation behind calculating the adjusted \\(R^2\\) value is the observation that adding more predictors into the model will always cause the \\(R^2\\) value to increase (or at least not decrease).\n[Additional technical detail9]\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted \\(R^2\\) value is that when you add more predictors to the model, the adjusted \\(R^2\\) value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted \\(R^2\\) value can’t be interpreted in the elegant way that \\(R^2\\) can. \\(R^2\\) has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted \\(R^2\\).\nAn obvious question then is whether you should report \\(R^2\\) or adjusted \\(R^2\\) . This is probably a matter of personal preference. If you care more about interpretability, then \\(R^2\\) is better. If you care more about correcting for bias, then adjusted \\(R^2\\) is probably better. Speaking just for myself, I prefer \\(R^2\\). My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in Hypothesis tests for regression models, if you’re worried that the improvement in \\(R^2\\) that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#hypothesis-tests-for-regression-models",
    "href": "12-Correlation-and-linear-regression.html#hypothesis-tests-for-regression-models",
    "title": "12  Correlation and linear regression",
    "section": "12.7 Hypothesis tests for regression models",
    "text": "12.7 Hypothesis tests for regression models\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n12.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail10]\nWe’ll see much more of the F statistic in Chapter 13, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n12.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the Multiple linear regression model we have already looked at (Table 12.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.11 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n12.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 12.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 12.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 12.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.12\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#regarding-regression-coefficients",
    "href": "12-Correlation-and-linear-regression.html#regarding-regression-coefficients",
    "title": "12  Correlation and linear regression",
    "section": "12.8 Regarding regression coefficients",
    "text": "12.8 Regarding regression coefficients\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n12.8.1 Confidence intervals for the coefficients\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 12.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n12.8.2 Calculating standardised regression coefficients\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 12.16.\n\n\n\n\n\nFigure 12.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#assumptions-of-regression",
    "href": "12-Correlation-and-linear-regression.html#assumptions-of-regression",
    "title": "12  Correlation and linear regression",
    "section": "12.9 Assumptions of regression",
    "text": "12.9 Assumptions of regression\nThe linear regression model that I’ve been discussing relies on several assumptions. In Model checking we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.\n\nLinearity. A pretty fundamental assumption of the linear regression model is that the relationship between \\(X\\) and \\(Y\\) actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.\nIndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.\nNormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors \\(X\\) and the outcome \\(Y\\) are non-normal, so long as the residuals \\(\\epsilon\\) are normal. See the Checking the normality of the residuals section.\nEquality (or ‘homogeneity’) of variance. Strictly speaking, the regression model assumes that each residual \\(\\epsilon_i\\) is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation \\(\\sigma\\) that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of \\(\\hat{Y}\\) , and (if we’re being especially paranoid) all values of every predictor \\(X\\) in the model.\n\nSo, we have four main assumptions for linear regression (that neatly form the acronym ‘LINE’). And there are also a couple of other things we should also check for:\n\nUncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See the Checking for collinearity section.\nNo “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on Three kinds of anomalous data."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-Model-checking",
    "href": "12-Correlation-and-linear-regression.html#sec-Model-checking",
    "title": "10  相闗與線性迴歸",
    "section": "10.10 診斷適用條件",
    "text": "10.10 診斷適用條件\nThe main focus of this section is regression diagnostics, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason. It’s not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgement when doing this. It’s easy to get lost in all the details of checking this thing or that thing, and it’s quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn all the tools, so instead they decide not to do any model checking. This is a bit of a worry!\nIn this section I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full space of things you could do, but it’s still much more detailed than what I see a lot of people doing in practice, and even I don’t usually cover all of this in my intro stats class either. However, I do think it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from Fox & Weisberg (2011), the book associated with the ‘car’ package that is used to conduct regression analysis in R. The ‘car’ package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don’t want to sound too gushy about it, but I do think that Fox & Weisberg (2011) is well worth reading, even if some of the advanced diagnostic techniques are only available in R and not jamovi.\n\n10.10.1 三種殘差\nThe majority of regression diagnostics revolve around looking at the residuals, and by now you’ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that, precisely because of the fact that we care a lot about the residuals, there are several different kinds of residual that we might consider. In particular, the following three kinds of residuals are referred to in this section: “ordinary residuals”, “standardised residuals”, and “Studentised residuals”. There is a fourth kind that you’ll see referred to in some of the Figures, and that’s the “Pearson residual”. However, for the models that we’re talking about in this chapter the Pearson residual is identical to the ordinary residual.\nThe first and simplest kind of residuals that we care about are ordinary residuals. These are the actual raw residuals that I’ve been talking about throughout this chapter so far. The ordinary residual is just the difference between the fitted value \\(\\hat{Y}_i\\) and the observed value \\(Y_i\\). I’ve been using the notation \\(\\epsilon_i\\) to refer to the i-th ordinary residual, and by gum I’m going to stick to it. With this in mind, we have the very simple equation\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\nThis is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here. I just wanted to repeat myself. One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0 but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the pattern of the residuals and not their actual values, it’s convenient to estimate the standardised residuals, which are normalised in such a way as to have standard deviation of 1.\n[Additional technical detail16]\nThe third kind of residuals are Studentised residuals (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual. 17\nBefore moving on, I should point out that you don’t often need to obtain these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various options that provide the diagnostics, or assumption checks, will take care of these calculations for you. Even so, it’s always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.\n\n\n10.10.2 三種反常資料\nOne danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. I discussed this idea previously in Section 5.2.3 in the context of discussing the outliers that get automatically identified by the boxplot option under ‘Exploration’ - ‘Descriptives’, but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous”. All three are interesting, but they have rather different implications for your analysis.\nThe first kind of unusual observation is an outlier. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in Figure 10.17. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, \\(\\epsilon_i^*\\). Outliers are interesting: a big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case and try to find out why it’s so different.\n\n\n\n\n\nFigure 10.17: An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line\n\n\n\n\nThe second way in which an observation can be unusual is if it has high 槓桿作用(leverage), which happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual. If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in Figure 10.18. The leverage of an observation is operationalised in terms of its hat value, usually written \\(h_i\\) . The formula for the hat value is rather complicated18 but its interpretation is not: \\(h_i\\) is a measure of the extent to which the i-th observation is “in control” of where the regression line ends up going.\n\n\n\n\n\nFigure 10.18: An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it\n\n\n\n\nIn general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to \\(K + 1\\)). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers.\nThis brings us to our third measure of unusualness, the 影響力(influence) of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in Figure 10.19. Notice the contrast to the previous two figures. Outliers don’t move the regression line much and neither do high leverage points. But something that is both an outlier and has high leverage, well that has a big effect on the regression line. That’s why we call these points high influence, and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as Cook’s distance. 19\n\n\n\n\n\nFigure 10.19: An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)\n\n\n\n\nIn order to have a large Cook’s distance an observation must be a fairly substantial outlier and have high leverage. As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule).\nIn jamovi, information about Cook’s distance can be calculated by clicking on the ‘Cook’s Distance’ checkbox in the ‘Assumption Checks’ - ‘Data Summary’ options. When you do this, for the multiple regression model we have been using as an example in this chapter, you get the results as shown in Figure 10.20.\n\n\n\n\n\nFigure 10.20: jamovi output showing the table for the Cook’s distance statistics\n\n\n\n\nYou can see that, in this example, the mean Cook’s distance value is \\(0.01\\), and the range is from \\(0.00\\) to \\(0.11\\), so this is some way off the rule of thumb figure mentioned above that a Cook’s distance greater than 1 is considered large.\nAn obvious question to ask next is, if you do have large values of Cook’s distance what should you do? As always, there’s no hard and fast rule. Probably the first thing to do is to try running the regression with the outlier with the greatest Cook’s distance20 excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study. Try to figure out why the point is so different. If you start to become convinced that this one data point is badly distorting your results then you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.\n\n\n10.10.3 檢測殘差常態性\nLike many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option. The output is shown in Figure 10.21, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.\n\n\n\n\n\nFigure 10.21: Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi\n\n\n\n\nAnother thing we should check is the relationship between the fitted values and the residuals themselves. We can get jamovi to do this using the ‘Residuals Plots’ option, which provides a scatterplot for each predictor variable, the outcome variable, and the fitted values against residuals, see Figure 10.22. In these plots we are looking for a fairly uniform distribution of ‘dots’, with no clear bunching or patterning of the ‘dots’. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.\n\n\n\n\n\nFigure 10.22: Residuals plots produced in jamovi\n\n\n\n\nIf we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in Section 6.3, but I do want to make special note of one additional possibility that I didn’t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it’s very widely used. 21\nYou can calculate it using the BOXCOX function in the ‘Compute’ variables screen in jamovi.\n\n\n10.10.4 檢測共線性\nThe last kind of regression diagnostic that I’m going to discuss in this chapter is the use of variance inflation factors (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor \\(X_k\\) in the model. 22\nThe square root of the VIF is pretty interpretable. It tells you how much wider the confidence interval for the corresponding coefficient bk is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options in jamovi. For both dani.sleep and baby.sleep the VIF is \\(1.65\\). And since the square root of \\(1.65\\) is \\(1.28\\), we see that the correlation between our two predictors isn’t causing much of a problem.\nTo give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables (Table 10.5).\n\n\n\n\nTable 10.5:  Correlation matrix for all four variables \n\ndani.sleepbaby.sleepdani.grumpday\n\ndani.sleep1.000000000.62794934$-0.90338404$$-0.09840768$\n\nbaby.sleep0.627949341.00000000$-0.56596373$$-0.01043394$\n\ndani.grump$-0.90338404$$-0.56596373$1.000000000.07647926\n\nday$-0.09840768$$-0.01043394$0.076479261.00000000\n\n\n\n\n\nWe have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in Figure 10.23 and you can see from the VIF values that, yep, that’s some mighty fine collinearity there.\n\n\n\n\n\nFigure 10.23: Collinearity statistics for multiple regression, produced in jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#model-selection",
    "href": "12-Correlation-and-linear-regression.html#model-selection",
    "title": "12  Correlation and linear regression",
    "section": "12.11 Model selection",
    "text": "12.11 Model selection\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 23\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n12.11.1 Backward elimination\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n12.11.2 Forward selection\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n12.11.3 A caveat\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n12.11.4 Comparing two regression models\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.24\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 25\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 12.24. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 12.24.\n\n\n\n\n\nFigure 12.24: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#summary",
    "href": "12-Correlation-and-linear-regression.html#summary",
    "title": "12  Correlation and linear regression",
    "section": "12.12 Summary",
    "text": "12.12 Summary\n\nWant to know how strong the relationship is between two variables? Calculate Correlations\nDrawing Scatterplots\nBasic ideas about What is a linear regression model? and Estimating a linear regression model\nMultiple linear regression\nQuantifying the fit of the regression model using \\(R^2\\).\nHypothesis tests for regression models\nIn Regarding regression coefficients we talked about calculating Confidence intervals for the coefficients and Calculating standardised regression coefficients\nThe Assumptions of regression and Model checking\nRegression Model selection\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html",
    "href": "13-Comparing-several-means-one-way-ANOVA.html",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "",
    "text": "This chapter introduces one of the most widely used tools in psychological statistics, known as “the analysis of variance”, but usually referred to as ANOVA. The basic technique was developed by Sir Ronald Fisher in the early 20th century and it is to him that we owe the rather unfortunate terminology. The term ANOVA is a little misleading, in two respects. Firstly, although the name of the technique refers to variances, ANOVA is concerned with investigating differences in means. Secondly, there are several different things out there that are all referred to as ANOVAs, some of which have only a very tenuous connection to one another. Later on in the book we’ll encounter a range of different ANOVA methods that apply in quite different situations, but for the purposes of this chapter we’ll only consider the simplest form of ANOVA, in which we have several different groups of observations, and we’re interested in finding out whether those groups differ in terms of some outcome variable of interest. This is the question that is addressed by a one-way ANOVA.\nThe structure of this chapter is as follows: first I’ll introduce a fictitious data set that we’ll use as a running example throughout the chapter. After introducing the data, I’ll describe the mechanics of how a one-way ANOVA actually works How ANOVA works and then focus on how you can run one in jamovi Running an ANOVA in jamovi. These two sections are the core of the chapter.\nThe remainder of the chapter discusses a range of important topics that inevitably arise when running an ANOVA, namely how to calculate effect sizes, post hoc tests and corrections for multiple comparisons and the assumptions that ANOVA relies upon. We’ll also talk about how to check those assumptions and some of the things you can do if the assumptions are violated. Then we’ll cover repeated measures ANOVA."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#an-illustrative-data-set",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#an-illustrative-data-set",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.1 An illustrative data set",
    "text": "13.1 An illustrative data set\nSuppose you’ve become involved in a clinical trial in which you are testing a new antidepressant drug called Joyzepam. In order to construct a fair test of the drug’s effectiveness, the study involves three separate drugs to be administered. One is a placebo, and the other is an existing antidepressant / anti-anxiety drug called Anxifree. A collection of 18 participants with moderate to severe depression are recruited for your initial testing. Because the drugs are sometimes administered in conjunction with psychological therapy, your study includes 9 people undergoing cognitive behavioural therapy (CBT) and 9 who are not. Participants are randomly assigned (doubly blinded, of course) a treatment, such that there are 3 CBT people and 3 no-therapy people assigned to each of the 3 drugs. A psychologist assesses the mood of each person after a 3 month run with each drug, and the overall improvement in each person’s mood is assessed on a scale ranging from \\(-5\\) to \\(+5\\). With that as the study design, let’s now load up the data file in clinicaltrial.csv. We can see that this data set contains the three variables drug, therapy and mood.gain.\nFor the purposes of this chapter, what we’re really interested in is the effect of drug on mood.gain. The first thing to do is calculate some descriptive statistics and draw some graphs. In the Chapter 4 chapter we showed you how to do this, and some of the descriptive statistics we can calculate in jamovi are shown in Figure 13.1\n\n\n\n\n\nFigure 13.1: Descriptives for mood gain, and box plots by drug administered\n\n\n\n\nAs the plot makes clear, there is a larger improvement in mood for participants in the Joyzepam group than for either the Anxifree group or the placebo group. The Anxifree group shows a larger mood gain than the control group, but the difference isn’t as large. The question that we want to answer is are these difference “real”, or are they just due to chance?"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.2 變異數分析的運作原理",
    "text": "13.2 變異數分析的運作原理\nIn order to answer the question posed by our clinical trial data we’re going to run a one-way ANOVA. I’m going to start by showing you how to do it the hard way, building the statistical tool from the ground up and showing you how you could do it if you didn’t have access to any of the cool built-in ANOVA functions in jamovi. And I hope you’ll read it carefully, try to do it the long way once or twice to make sure you really understand how ANOVA works, and then once you’ve grasped the concept never ever do it this way again.\nThe experimental design that I described in the previous section strongly suggests that we’re interested in comparing the average mood change for the three different drugs. In that sense, we’re talking about an analysis similar to the t-test (see Chapter 11) but involving more than two groups. If we let \\(\\mu_P\\) denote the population mean for the mood change induced by the placebo, and let \\(\\mu_A\\) and \\(\\mu_J\\) denote the corresponding means for our two drugs, Anxifree and Joyzepam, then the (somewhat pessimistic) null hypothesis that we want to test is that all three population means are identical. That is, neither of the two drugs is any more effective than a placebo. We can write out this null hypothesis as:\n\\[H_0: \\text{ it is true that } \\mu_P=\\mu_A=\\mu_J\\]\nAs a consequence, our alternative hypothesis is that at least one of the three different treatments is different from the others. It’s a bit tricky to write this mathematically, because (as we’ll discuss) there are quite a few different ways in which the null hypothesis can be false. So for now we’ll just write the alternative hypothesis like this:\n\\[H_1: \\text{ it } \\underline{ is \\text{ } not } \\text{ true that }\n\\mu_P=\\mu_A=\\mu_J\\]\nThis null hypothesis is a lot trickier to test than any of the ones we’ve seen previously. How shall we do it? A sensible guess would be to “do an ANOVA”, since that’s the title of the chapter, but it’s not particularly clear why an “analysis of variances” will help us learn anything useful about the means. In fact, this is one of the biggest conceptual difficulties that people have when first encountering ANOVA. To see how this works, I find it most helpful to start by talking about variances, specifically between group variability and within-group variability (Figure 13.2).\n\n\n\n\n\nFigure 13.2: Graphical illustration of ‘between groups’ variation (panel (a)) and ‘within groups’ variation (panel (b)). On the left the arrows show the differences in the group means. On the right the arrows highlight the variability within each group\n\n\n\n\n\n13.2.1 計算依變項變異數的兩套公式\nFirst, let’s start by introducing some notation. We’ll use G to refer to the total number of groups. For our data set there are three drugs, so there are \\(G = 3\\) groups. Next, we’ll use \\(N\\) to refer to the total sample size; there are a total of \\(N = 18\\) people in our data set. Similarly, let’s use \\(N_k\\) to denote the number of people in the k-th group. In our fake clinical trial, the sample size is \\(N_k = 6\\) for all three groups.1 Finally, we’ll use Y to denote the outcome variable. In our case, Y refers to mood change. Specifically, we’ll use Yik to refer to the mood change experienced by the i-th member of the k-th group. Similarly, we’ll use \\(\\bar{Y}\\) to be the average mood change, taken across all 18 people in the experiment, and \\(\\bar{Y}_k\\) to refer to the average mood change experienced by the 6 people in group \\(k\\).\nNow that we’ve got our notation sorted out we can start writing down formulas. To start with, let’s recall the formula for the variance that we used in Section 4.2, way back in those kinder days when we were just doing descriptive statistics. The sample variance of Y is defined as follows \\[Var(Y)=\\frac{1}{N}\\sum_{k=1}^{G}\\sum_{i=1}^{N_k}(Y_{ik}-\\bar{Y})^2\\] This formula looks pretty much identical to the formula for the variance in Section 4.2. The only difference is that this time around I’ve got two summations here: I’m summing over groups (i.e., values for \\(k\\)) and over the people within the groups (i.e., values for \\(i\\)). This is purely a cosmetic detail. If I’d instead used the notation \\(Y_p\\) to refer to the value of the outcome variable for person p in the sample, then I’d only have a single summation. The only reason that we have a double summation here is that I’ve classified people into groups, and then assigned numbers to people within groups.\nA concrete example might be useful here. Let’s consider Table 13.1, in which we have a total of \\(N = 5\\) people sorted into \\(G = 2\\) groups. Arbitrarily, let’s say that the “cool” people are group 1 and the “uncool” people are group 2. It turns out that we have three cool people (\\(N_1 = 3\\)) and two uncool people (\\(N_2 = 2\\))\n\n\n\n\nTable 13.1:  Grumpiness for people in cool and uncool groups \n\nnameperson Pgroupgroup num. kindex in groupgrumpiness \\( Y_{ik} \\) or \\( Y_p \\)\n\nAnn1cool1120\n\nBen2cool1255\n\nCat3cool1321\n\nTim4uncool2191\n\nEgg5uncool2222\n\n\n\n\n\nNotice that I’ve constructed two different labelling schemes here. We have a “person” variable p so it would be perfectly sensible to refer to Yp as the grumpiness of the p-th person in the sample. For instance, the table shows that Tim is the fourth so we’d say \\(p = 4\\). So, when talking about the grumpiness \\(Y\\) of this “Tim” person, whoever he might be, we could refer to his grumpiness by saying that \\(Y_p = 91\\), for person \\(p = 4\\) that is. However, that’s not the only way we could refer to Tim. As an alternative we could note that Tim belongs to the “uncool” group (\\(k = 2\\)), and is in fact the first person listed in the uncool group (\\(i = 1\\)). So it’s equally valid to refer to Tim’s grumpiness by saying that \\(Y_{ik} = 91\\), where \\(k = 2\\) and \\(i = 1\\).\nIn other words, each person p corresponds to a unique ik combination, and so the formula that I gave above is actually identical to our original formula for the variance, which would be \\[Var(Y)=\\frac{1}{N}\\sum_{p=1}^{N}(Y_p-\\bar{Y})^2\\] In both formulas, all we’re doing is summing over all of the observations in the sample. Most of the time we would just use the simpler Yp notation; the equation using \\(Y_p\\) is clearly the simpler of the two. However, when doing an ANOVA it’s important to keep track of which participants belong in which groups, and we need to use the Yik notation to do this.\n\n\n13.2.2 變異數與平方差總和\nOkay, now that we’ve got a good grasp on how the variance is calculated, let’s define something called the total sum of squares, which is denoted SStot. This is very simple. Instead of averaging the squared deviations, which is what we do when calculating the variance, we just add them up.2\nWhen we talk about analysing variances in the context of ANOVA, what we’re really doing is working with the total sums of squares rather than the actual variance. 3\nNext, we can define a third notion of variation which captures only the differences between groups. We do this by looking at the differences between the group means \\(\\bar{Y}_k\\) and grand mean \\(\\bar{Y}\\). 4\nIt’s not too difficult to show that the total variation among people in the experiment (\\(SS_{tot}\\) is actually the sum of the differences between the groups \\(SS_b\\) and the variation inside the groups \\(SS_w\\). That is,\n\\[SS_w+SS_b=SS_{tot}\\] Yay.\nOkay, so what have we found out? We’ve discovered that the total variability associated with the outcome variable (\\(SS_{tot}\\)) can be mathematically carved up into the sum of “the variation due to the differences in the sample means for the different groups” (\\(SS_b\\)) plus “all the rest of the variation” (\\(SS_w\\)) 5.\nHow does that help me find out whether the groups have different population means? Um. Wait. Hold on a second. Now that I think about it, this is exactly what we were looking for. If the null hypothesis is true then you’d expect all the sample means to be pretty similar to each other, right? And that would imply that you’d expect \\(SS_b\\) to be really small, or at least you’d expect it to be a lot smaller than “the variation associated with everything else”, \\(SS_w\\). Hmm. I detect a hypothesis test coming on.\n\n\n13.2.3 平方差總和與F檢定\nAs we saw in the last section, the qualitative idea behind ANOVA is to compare the two sums of squares values \\(SS_b\\) and \\(SS_w\\) to each other. If the between-group variation \\(SS_b\\) is large relative to the within-group variation \\(SS_w\\) then we have reason to suspect that the population means for the different groups aren’t identical to each other. In order to convert this into a workable hypothesis test, there’s a little bit of “fiddling around” needed. What I’ll do is first show you what we do to calculate our test statistic, the F ratio, and then try to give you a feel for why we do it this way.\nIn order to convert our SS values into an F-ratio the first thing we need to calculate is the degrees of freedom associated with the \\(SS_b\\) and \\(SS_w\\) values. As usual, the degrees of freedom corresponds to the number of unique “data points” that contribute to a particular calculation, minus the number of “constraints” that they need to satisfy. For the within-groups variability what we’re calculating is the variation of the individual observations (\\(N\\) data points) around the group means (\\(G\\) constraints). In contrast, for the between groups variability we’re interested in the variation of the group means (G data points) around the grand mean (1 constraint). Therefore, the degrees of freedom here are:\n\\[df_b=G-1\\] \\[df_w=N-G\\]\nOkay, that seems simple enough. What we do next is convert our summed squares value into a “mean squares” value, which we do by dividing by the degrees of freedom:\n\\[MS_b=\\frac{SS_b}{df_b}\\] \\[MS_w=\\frac{SS_w}{df_w}\\]\nFinally, we calculate the F-ratio by dividing the between-groups MS by the within-groups MS:\n\\[F=\\frac{MS_b}{MS_w}\\]\nAt a very general level, the intuition behind the F statistic is straightforward. Bigger values of F means that the between-groups variation is large relative to the within-groups variation. As a consequence, the larger the value of F the more evidence we have against the null hypothesis. But how large does \\(F\\) have to be in order to actually reject \\(H_0\\)? In order to understand this, you need a slightly deeper understanding of what ANOVA is and what the mean squares values actually are.\nThe next section discusses that in a bit of detail, but for readers that aren’t interested in the details of what the test is actually measuring I’ll cut to the chase. In order to complete our hypothesis test we need to know the sampling distribution for F if the null hypothesis is true. Not surprisingly, the sampling distribution for the F statistic under the null hypothesis is an \\(F\\) distribution. If you recall our discussion of the F distribution in Chapter 7, the \\(F\\) distribution has two parameters, corresponding to the two degrees of freedom involved. The first one \\(df_1\\) is the between groups degrees of freedom \\(df_b\\), and the second one \\(df_2\\) is the within groups degrees of freedom \\(df_w\\).\n\n\n\n\nTable 13.2:  All of the key quantities involved in an ANOVA organised into a ‘standard’ ANOVA table. The formulas for all quantities (except the p-value which has a very ugly formula and would be nightmarishly hard to calculate without a computer) are shown \n\nbetween\ngroupswithin\ngroups\n\ndf\\(  df_b=G-1  \\)\\(  df_w=N-G  \\)\n\nsum of squares\\(  SS_b=\\sum_{k=1}^{G} N_k  (\\bar{Y}_k-\\bar{Y})^2  \\)\\(  SS_w=\\sum_{k=1}^{G} \\sum_{i=1}^{N_k}   (Y_{ik}-\\bar{Y}_k)^2  \\)\n\nmean squares\\(  MS_b=\\frac{SS_b}{df_b}  \\)\\(  MS_w=\\frac{SS_w}{df_w}  \\)\n\nF-statistic\\(  F=\\frac{MS_b}{df_b}  \\)-\n\np-value[complicated]-\n\n\n\n\n\nA summary of all the key quantities involved in a one-way ANOVA, including the formulas showing how they are calculated, is shown in Table 13.2.\n[Additional technical detail 6]\n\n\n13.2.4 實例演練\nThe previous discussion was fairly abstract and a little on the technical side, so I think that at this point it might be useful to see a worked example. For that, let’s go back to the clinical trial data that I introduced at the start of the chapter. The descriptive statistics that we calculated at the beginning tell us our group means: an average mood gain of \\(0.45\\) for the placebo, \\(0.72\\) for Anxifree, and \\(1.48\\) for Joyzepam. With that in mind, let’s party like it’s 1899 7 and start doing some pencil and paper calculations. I’ll only do this for the first \\(5\\) observations because it’s not bloody \\(1899\\) and I’m very lazy. Let’s start by calculating \\(SS_w\\), the within-group sums of squares. First, let’s draw up a nice table to help us with our calculations (Table 13.3)\n\n\n\n\nTable 13.3:  A worked example…1 \n\ngroup koutcome \\( Y_{ik} \\)\n\nplacebo0.5\n\nplacebo0.3\n\nplacebo0.1\n\nanxifree0.6\n\nanxifree0.4\n\n\n\n\n\nAt this stage, the only thing I’ve included in the table is the raw data itself. That is, the grouping variable (i.e., drug) and outcome variable (i.e. mood.gain) for each person. Note that the outcome variable here corresponds to the \\(\\bar{Y}_{ik}\\) value in our equation previously. The next step in the calculation is to write down, for each person in the study, the corresponding group mean, \\(\\bar{Y}_k\\). This is slightly repetitive but not particularly difficult since we already calculated those group means when doing our descriptive statistics, see Table 13.4.\n\n\n\n\nTable 13.4:  A worked example…2 \n\ngroup koutcome \\( Y_{ik} \\)group mean \\( \\bar{Y}_k \\)\n\nplacebo0.50.45\n\nplacebo0.30.45\n\nplacebo0.10.45\n\nanxifree0.60.72\n\nanxifree0.40.72\n\n\n\n\n\nNow that we’ve written those down, we need to calculate, again for every person, the deviation from the corresponding group mean. That is, we want to subtract \\(Y_{ik} - \\bar{Y}_k\\). After we’ve done that, we need to square everything. When we do that, here’s what we get (Table 13.5)\n\n\n\n\nTable 13.5:  A worked example…3 \n\ngroup koutcome \\( Y_{ik} \\)group mean  \\( \\bar{Y}_k \\)dev. from group mean  \\( Y_{ik} - \\bar{Y}_k \\)squared deviation \\(  (Y_{ik}-\\bar{Y}_k)^2 \\)\n\nplacebo0.50.450.050.0025\n\nplacebo0.30.45-0.150.0225\n\nplacebo0.10.45-0.350.1225\n\nanxifree0.60.72-0.120.0136\n\nanxifree0.40.72-0.320.1003\n\n\n\n\n\nThe last step is equally straightforward. In order to calculate the within-group sum of squares we just add up the squared deviations across all observations:\n\\[\n\\begin{split}\nSS_w & = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 \\\\\n& = 0.2614\n\\end{split}\n\\]\nOf course, if we actually wanted to get the right answer we’d need to do this for all 18 observations in the data set, not just the first five. We could continue with the pencil and paper calculations if we wanted to, but it’s pretty tedious. Alternatively, it’s not too hard to do this in a dedicated spreadsheet programme such as OpenOffice or Excel. Try and do it yourself. The one that I did, in Excel, is in the file clinicaltrial_anova.xls. When you do it you should end up with a within-group sum of squares value of \\(1.39\\).\nOkay. Now that we’ve calculated the within groups variation, \\(SS_w\\), it’s time to turn our attention to the between-group sum of squares, \\(SS_b\\). The calculations for this case are very similar. The main difference is that instead of calculating the differences between an observation Yik and a group mean \\(\\bar{Y}_k\\) for all of the observations, we calculate the differences between the group means \\(\\bar{Y}_k\\) and the grand mean \\(\\bar{Y}\\) (in this case \\(0.88\\)) for all of the groups (Table 13.6).\n\n\n\n\nTable 13.6:  A worked example…4 \n\ngroup kgroup mean \\( \\bar{Y}_k \\)grand mean  \\( \\bar{Y} \\)deviation  \\( \\bar{Y}_k - \\bar{Y} \\)squared deviation \\(  ( \\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo0.450.88-0.430.19\n\nanxifree0.720.88-0.160.03\n\njoyzepam1.480.880.600.36\n\n\n\n\n\nHowever, for the between group calculations we need to multiply each of these squared deviations by \\(N_k\\), the number of observations in the group. We do this because every observation in the group (all \\(N_k\\) of them) is associated with a between group difference. So if there are six people in the placebo group and the placebo group mean differs from the grand mean by \\(0.19\\), then the total between group variation associated with these six people is \\(6 \\times 0.19 = 1.14\\). So we have to extend our little table of calculations (Table 13.7).\n\n\n\n\nTable 13.7:  A worked example…5 \n\ngroup k...squared deviations  \\( (\\bar{Y}_k-\\bar{Y})^2 \\)sample size  \\( N_k \\)weighted squared dev   \\(  N_k (\\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo...0.1961.14\n\nanxifree...0.0360.18\n\njoyzepam...0.3662.16\n\n\n\n\n\nAnd so now our between group sum of squares is obtained by summing these “weighted squared deviations” over all three groups in the study:\n\\[\\begin{aligned} SS_b & = 1.14 + 0.18 + 2.16 \\\\ &= 3.48 \\end{aligned}\\]\nAs you can see, the between group calculations are a lot shorter8. Now that we’ve calculated our sums of squares values, \\(SS_b\\) and \\(SS_w\\), the rest of the ANOVA is pretty painless. The next step is to calculate the degrees of freedom. Since we have \\(G = 3\\) groups and \\(N = 18\\) observations in total our degrees of freedom can be calculated by simple subtraction:\n\\[\n\\begin{split}\ndf_b & = G-1 = 2 \\\\\ndf_w & = N-G = 15\n\\end{split}\n\\]\nNext, since we’ve now calculated the values for the sums of squares and the degrees of freedom, for both the within-groups variability and the between-groups variability, we can obtain the mean square values by dividing one by the other:\n\\[\n\\begin{split}\nMS_b & = \\frac{SS_b}{df_b} = \\frac{3.48}{2} = 1.74 \\\\\nMS_w & = \\frac{SS_w}{df_w} = \\frac{1.39}{15} = 0.09\n\\end{split}\n\\]\nWe’re almost done. The mean square values can be used to calculate the F-value, which is the test statistic that we’re interested in. We do this by dividing the between-groups MS value by the within-groups MS value.\n\\[\n\\begin{split}\nF & = \\frac{MS_b}{MS_w}  = \\frac{1.74}{0.09} \\\\\n& = 19.3\n\\end{split}\n\\]\nWoohooo! This is terribly exciting, yes? Now that we have our test statistic, the last step is to find out whether the test itself gives us a significant result. As discussed in Chapter 9 back in the “old days” what we’d do is open up a statistics textbook or flick to the back section which would actually have a huge lookup table and we would find the threshold \\(F\\) value corresponding to a particular value of alpha (the null hypothesis rejection region), e.g. \\(0.05\\), \\(0.01\\) or \\(0.001\\), for 2 and 15 degrees of freedom. Doing it this way would give us a threshold F value for an alpha of \\(0.001\\) of \\(11.34\\). As this is less than our calculated \\(F\\) value we say that \\(p < 0.001\\). But those were the old days, and nowadays fancy stats software calculates the exact p-value for you. In fact, the exact p-value is \\(0.000071\\). So, unless we’re being extremely conservative about our Type I error rate, we’re pretty much guaranteed to reject the null hypothesis.\nAt this point, we’re basically done. Having completed our calculations, it’s traditional to organise all these numbers into an ANOVA table like the one in Table 13.1. For our clinical trial data, the ANOVA table would look like Table 13.8.\n\n\n\n\nTable 13.8:  The ANOVA results table \n\ndfsum of squaresmean squaresF-statisticp-value\n\nbetween groups23.481.7419.30.000071\n\nwithin groups151.390.09--\n\n\n\n\n\nThese days, you’ll probably never have much reason to want to construct one of these tables yourself, but you will find that almost all statistical software (jamovi included) tends to organise the output of an ANOVA into a table like this, so it’s a good idea to get used to reading them. However, although the software will output a full ANOVA table, there’s almost never a good reason to include the whole table in your write up. A pretty standard way of reporting the stats block for this result would be to write something like this:\n\nOne-way ANOVA showed a significant effect of drug on mood gain (F(2,15) = 19.3, p < .001).\n\nSigh. So much work for one short sentence."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.3 Running an ANOVA in jamovi",
    "text": "13.3 Running an ANOVA in jamovi\nI’m pretty sure I know what you’re thinking after reading the last section, especially if you followed my advice and did all of that by pencil and paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations yourself sucks. There’s quite a lot of calculations that we needed to do along the way, and it would be tedious to have to do this over and over again every time you wanted to do an ANOVA.\n\n13.3.1 Using jamovi to specify your ANOVA\nTo make life easier for you, jamovi can do ANOVA…hurrah! Go to the ‘ANOVA’ - ‘ANOVA’ analysis, and move the mood.gain variable across so it is in the ‘Dependent Variable’ box, and then move the drug variable across so it is in the ‘Fixed Factors’ box. This should give the results as shown in Figure 13.3. 9 Note I have also checked the \\(\\eta^2\\) checkbox, pronounced “eta” squared, under the ‘Effect Size’ option and this is also shown on the results table. We will come back to effect sizes a bit later.\n\n\n\n\n\nFigure 13.3: jamovi results table for ANOVA of mood gain by drug administered\n\n\n\n\nThe jamovi results table shows you the sums of squares values, the degrees of freedom, and a couple of other quantities that we’re not really interested in right now. Notice, however, that jamovi doesn’t use the names “between-group” and “within-group”. Instead, it tries to assign more meaningful names. In our particular example, the between groups variance corresponds to the effect that the drug has on the outcome variable, and the within groups variance corresponds to the “leftover” variability so it calls that the residuals. If we compare these numbers to the numbers that I calculated by hand in A worked example, you can see that they’re more or less the same, apart from rounding errors. The between groups sums of squares is \\(SS_b = 3.45\\), the within groups sums of squares is \\(SS_w = 1.39\\), and the degrees of freedom are \\(2\\) and \\(15\\) respectively. We also get the F-value and the p-value and, again, these are more or less the same, give or take rounding errors, to the numbers that we calculated ourselves when doing it the long and tedious way."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#effect-size",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#effect-size",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.4 Effect size",
    "text": "13.4 Effect size\nThere’s a few different ways you could measure the effect size in an ANOVA, but the most commonly used measures are \\(\\eta^2\\) (eta squared) and partial \\(\\eta^2\\). For a one way analysis of variance they’re identical to each other, so for the moment I’ll just explain \\(\\eta^2\\) . The definition of \\(\\eta^2\\) is actually really simple\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\nThat’s all it is. So when I look at the ANOVA table in Figure 13.3, I see that \\(SS_b = 3.45\\) and \\(SS_tot = 3.45 + 1.39 = 4.84\\). Thus we get an \\(\\eta^2\\) value of\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable (mood.gain) that can be explained in terms of the predictor (drug). A value of \\(\\eta^2=0\\) means that there is no relationship at all between the two, whereas a value of \\(\\eta^2=1\\) means that the relationship is perfect. Better yet, the \\(\\eta^2\\) value is very closely related to \\(R^2\\), as discussed previously in Section 12.6.1, and has an equivalent interpretation. Although many statistics text books suggest \\(\\eta^2\\) as the default effect size measure in ANOVA, there’s an interesting blog post by Daniel Lakens suggesting that eta-squared is perhaps not the best measure of effect size in real world data analysis, because it can be a biased estimator. Usefully, there is also an option in jamovi to specify omega-squared (\\(\\omega^2\\)), which is less biased, alongside eta-squared."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#multiple-comparisons-and-post-hoc-tests",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#multiple-comparisons-and-post-hoc-tests",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.5 Multiple comparisons and post hoc tests",
    "text": "13.5 Multiple comparisons and post hoc tests\nAny time you run an ANOVA with more than two groups and you end up with a significant effect, the first thing you’ll probably want to ask is which groups are actually different from one another. In our drugs example, our null hypothesis was that all three drugs (placebo, Anxifree and Joyzepam) have the exact same effect on mood. But if you think about it, the null hypothesis is actually claiming three different things all at once here. Specifically, it claims that:\n\nYour competitor’s drug (Anxifree) is no better than a placebo (i.e., \\(\\mu_A = \\mu_P\\) )\nYour drug (Joyzepam) is no better than a placebo (i.e., \\(\\mu_J = \\mu_P\\) )\nAnxifree and Joyzepam are equally effective (i.e., \\(\\mu_J = \\mu_A\\))\n\nIf any one of those three claims is false, then the null hypothesis is also false. So, now that we’ve rejected our null hypothesis, we’re thinking that at least one of those things isn’t true. But which ones? All three of these propositions are of interest. Since you certainly want to know if your new drug Joyzepam is better than a placebo, it would be nice to know how well it stacks up against an existing commercial alternative (i.e., Anxifree). It would even be useful to check the performance of Anxifree against the placebo. Even if Anxifree has already been extensively tested against placebos by other researchers, it can still be very useful to check that your study is producing similar results to earlier work.\nWhen we characterise the null hypothesis in terms of these three distinct propositions, it becomes clear that there are eight possible “states of the world” that we need to distinguish between (Table 13.9).\n\n\n\n\nTable 13.9:  The null hypothesis and eight possible ‘states of the world’ \n\npossibility:is \\( \\mu_P = \\mu_A \\)?is \\( \\mu_P = \\mu_J \\)?is \\( \\mu_A = \\mu_J \\)?which hypothesis?\n\n1\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)null\n\n2\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n3\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n4\\( \\checkmark \\)alternative\n\n5\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n6\\( \\checkmark \\)alternative\n\n7\\( \\checkmark \\)alternative\n\n8alternative\n\n\n\n\n\nBy rejecting the null hypothesis, we’ve decided that we don’t believe that #1 is the true state of the world. The next question to ask is, which of the other seven possibilities do we think is right? When faced with this situation, its usually helps to look at the data. For instance, if we look at the plots in Figure 13.1, it’s tempting to conclude that Joyzepam is better than the placebo and better than Anxifree, but there’s no real difference between Anxifree and the placebo. However, if we want to get a clearer answer about this, it might help to run some tests.\n\n13.5.1 Running “pairwise” t-tests\nHow might we go about solving our problem? Given that we’ve got three separate pairs of means (placebo versus Anxifree, placebo versus Joyzepam, and Anxifree versus Joyzepam) to compare, what we could do is run three separate t-tests and see what happens. This is easy to do in jamovi. Go to the ANOVA ‘Post Hoc Tests’ options, move the ‘drug’ variable across into the active box on the right, and then click on the ‘No correction’ checkbox. This will produce a neat table showing all the pairwise t-test comparisons amongst the three levels of the drug variable, as in Figure 13.4\n\n\n\n\n\nFigure 13.4: Uncorrected pairwise t-tests as post hoc comparisons in jamovi\n\n\n\n\n\n\n13.5.2 Corrections for multiple testing\nIn the previous section I hinted that there’s a problem with just running lots and lots of t-tests. The concern is that, when running these analyses, what we’re doing is going on a “fishing expedition”. We’re running lots and lots of tests without much theoretical guidance in the hope that some of them come up significant. This kind of theory-free search for group differences is referred to as post hoc analysis (“post hoc” being Latin for “after this”).10\nIt’s okay to run post hoc analyses, but a lot of care is required. For instance, the analysis that I ran in the previous section should be avoided, as each individual t-test is designed to have a 5% Type I error rate (i.e., \\(\\alpha = .05\\)) and I ran three of these tests. Imagine what would have happened if my ANOVA involved 10 different groups, and I had decided to run 45 “post hoc” t-tests to try to find out which ones were significantly different from each other, you’d expect 2 or 3 of them to come up significant by chance alone. As we saw in Chapter 9, the central organising principle behind null hypothesis testing is that we seek to control our Type I error rate, but now that I’m running lots of t-tests at once in order to determine the source of my ANOVA results, my actual Type I error rate across this whole family of tests has gotten completely out of control.\nThe usual solution to this problem is to introduce an adjustment to the p-value, which aims to control the total error rate across the family of tests (see Shaffer (1995)). An adjustment of this form, which is usually (but not always) applied because one is doing post hoc analysis, is often referred to as a correction for multiple comparisons, though it is sometimes referred to as “simultaneous inference”. In any case, there are quite a few different ways of doing this adjustment. I’ll discuss a few of them in this section and in Section 14.8 in the next chapter, but you should be aware that there are many other methods out there (see, e.g., Hsu (1996)).\n\n\n13.5.3 Bonferroni corrections\nThe simplest of these adjustments is called the Bonferroni correction (Dunn, 1961), and it’s very very simple indeed. Suppose that my post hoc analysis consists of m separate tests, and I want to ensure that the total probability of making any Type I errors at all is at most \\(\\alpha\\).11 If so, then the Bonferroni correction just says “multiply all your raw p-values by m”. If we let \\(p\\) denote the original p-value, and let \\(p_j^{'}\\) be the corrected value, then the Bonferroni correction tells that:\n\\[p_j^{'}=m \\times p\\]\nAnd therefore, if you’re using the Bonferroni correction, you would reject the null hypothesis if \\(p_j^{'} < \\alpha\\). The logic behind this correction is very straightforward. We’re doing m different tests, so if we arrange it so that each test has a Type I error rate of at most \\(\\frac{\\alpha}{m}\\), then the total Type I error rate across these tests cannot be larger than \\(\\alpha\\). That’s pretty simple, so much so that in the original paper, the author writes:\n\nThe method given here is so simple and so general that I am sure it must have been used before this. I do not find it, however, so can only conclude that perhaps its very simplicity has kept statisticians from realizing that it is a very good method in some situations (Dunn (1961), pp 52-53).\n\nTo use the Bonferroni correction in jamovi, just click on the ‘Bonferroni’ checkbox in the ‘Correction’ options, and you will see another column added to the ANOVA results table showing the adjusted p-values for the Bonferroni correction (Table 13.8). If we compare these three p-values to those for the uncorrected, pairwise t-tests, it is clear that the only thing that jamovi has done is multiply them by \\(3\\).\n\n\n13.5.4 Holm corrections\nAlthough the Bonferroni correction is the simplest adjustment out there, it’s not usually the best one to use. One method that is often used instead is the Holm correction (Holm, 1979). The idea behind the Holm correction is to pretend that you’re doing the tests sequentially, starting with the smallest (raw) p-value and moving onto the largest one. For the j-th largest of the p-values, the adjustment is either\n\\[p_j^{'}=j \\times p_j\\]\n(i.e., the biggest p-value remains unchanged, the second biggest p-value is doubled, the third biggest p-value is tripled, and so on), or\n\\[p_j^{'}=p_{j+1}^{'}\\]\nwhichever one is larger. This might sound a little confusing, so let’s go through it a little more slowly. Here’s what the Holm correction does. First, you sort all of your p-values in order, from smallest to largest. For the smallest p-value all you do is multiply it by \\(m\\), and you’re done. However, for all the other ones it’s a two-stage process. For instance, when you move to the second smallest p value, you first multiply it by \\(m - 1\\). If this produces a number that is bigger than the adjusted p-value that you got last time, then you keep it. But if it’s smaller than the last one, then you copy the last p-value. To illustrate how this works, consider Table 13.10 which shows the calculations of a Holm correction for a collection of five p-values.\n\n\n\n\nTable 13.10:  Holm corrected p values \n\nraw prank jp \\( \\times \\) jHolm p\n\n.0015.005.005\n\n.0054.020.020\n\n.0193.057.057\n\n.0222.044.057\n\n.1031.103.103\n\n\n\n\n\nHopefully that makes things clear.\nAlthough it’s a little harder to calculate, the Holm correction has some very nice properties. It’s more powerful than Bonferroni (i.e., it has a lower Type II error rate) but, counter-intuitive as it might seem, it has the same Type I error rate. As a consequence, in practice there’s never any reason to use the simpler Bonferroni correction since it is always outperformed by the slightly more elaborate Holm correction. Because of this, the Holm correction should be your go to multiple comparison correction. Figure 13.4 also shows the Holm corrected p-values and, as you can see, the biggest p-value (corresponding to the comparison between Anxifree and the placebo) is unaltered. At a value of .15, it is exactly the same as the value we got originally when we applied no correction at all. In contrast, the smallest p-value (Joyzepam versus placebo) has been multiplied by three.\n\n\n13.5.5 Writing up the post hoc test\nFinally, having run the post hoc analysis to determine which groups are significantly different to one another, you might write up the result like this:\n\nPost hoc tests (using the Holm correction to adjust p) indicated that Joyzepam produced a significantly larger mood change than both Anxifree (p = .001) and the placebo (\\((p = 9.0 \\times{10^{-5}}\\)). We found no evidence that Anxifree performed better than the placebo (\\(p = .15\\)).\n\nOr, if you don’t like the idea of reporting exact p-values, then you’d change those numbers to \\(p < .01\\), \\(p < .001\\) and \\(p > .05\\) respectively. Either way, the key thing is that you indicate that you used Holm’s correction to adjust the p-values. And of course, I’m assuming that elsewhere in the write up you’ve included the relevant descriptive statistics (i.e., the group means and standard deviations), since these p-values on their own aren’t terribly informative."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#the-assumptions-of-one-way-anova",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#the-assumptions-of-one-way-anova",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.6 The assumptions of one-way ANOVA",
    "text": "13.6 The assumptions of one-way ANOVA\nLike any statistical test, analysis of variance relies on some assumptions about the data, specifically the residuals. There are three key assumptions that you need to be aware of: normality, homogeneity of variance and independence.\n[Additional technical detail 12]\nSo, how do we check whether the assumption about the residuals is accurate? Well, as I indicated above, there are three distinct claims buried in this one statement, and we’ll consider them separately.\n\nHomogeneity of variance. Notice that we’ve only got the one value for the population standard deviation (i.e., \\(\\sigma\\)), rather than allowing each group to have it’s own value (i.e., \\(\\sigma_k\\)). This is referred to as the homogeneity of variance (sometimes called homoscedasticity) assumption. ANOVA assumes that the population standard deviation is the same for all groups. We’ll talk about this extensively in the Checking the homogeneity of variance assumption section.\nNormality. The residuals are assumed to be normally distributed. As we saw in Section 11.9, we can assess this by looking at QQ plots (or running a Shapiro-Wilk test. I’ll talk about this more in an ANOVA context in the Checking the normality assumption section.\nIndependence. The independence assumption is a little trickier. What it basically means is that, knowing one residual tells you nothing about any other residual. All of the \\(\\epsilon_{ik}\\) values are assumed to have been generated without any “regard for” or “relationship to” any of the other ones. There’s not an obvious or simple way to test for this, but there are some situations that are clear violations of this. For instance, if you have a repeated measures design, where each participant in your study appears in more than one condition, then independence doesn’t hold. There’s a special relationship between some observations, namely those that correspond to the same person! When that happens, you need to use something like a Repeated measures one-way ANOVA.\n\n\n13.6.1 Checking the homogeneity of variance assumption\n\nTo make the preliminary test on variances is rather like putting to sea in a rowing boat to find out whether conditions are sufficiently calm for an ocean liner to leave port!\n– George Box (Box, 1953)\n\nThere’s more than one way to skin a cat, as the saying goes, and more than one way to test the homogeneity of variance assumption, too (though for some reason no-one made a saying out of that). The most commonly used test for this that I’ve seen in the literature is the Levene test (Levene, 1960), and the closely related Brown-Forsythe test (Brown & Forsythe, 1974).\nRegardless of whether you’re doing the standard Levene test or the Brown-Forsythe test, the test statistic, which is sometimes denoted \\(F\\) but also sometimes written as \\(W\\), is calculated in exactly the same way that the F-statistic for the regular ANOVA is calculated, just using a \\(Z_{ik}\\) rather than \\(Y_{ik}\\). With that in mind, we can go on to look at how to run the test in jamovi.\n[Additional technical detail 13]\n\n\n13.6.2 Running the Levene test in jamovi\nOkay, so how do we run the Levene test? Simple really - under the ANOVA ‘Assumption Checks’ option, just click on the ‘Homogeneity tests’ checkbox. If we look at the output, shown in Figure 13.5, we see that the test is non-significant (\\(F_{2,15} = 1.45, p = .266\\)), so it looks like the homogeneity of variance assumption is fine. However, looks can be deceptive! If your sample size is pretty big, then the Levene test could show up a significant effect (i.e. p < .05) even when the homogeneity of variance assumption is not violated to an extent which troubles the robustness of ANOVA. This was the point George Box was making in the quote above. Similarly, if your sample size is quite small, then the homogeneity of variance assumption might not be satisfied and yet a Levene test could be non-significant (i.e. p > .05). What this means is that, alongside any statistical test of the assumption being met, you should always plot the standard deviation around the means for each group / category in the analysis…just to see if they look fairly similar (i.e. homogeneity of variance) or not.\n\n\n\n\n\nFigure 13.5: Levene test output for one-way ANOVA in jamovi\n\n\n\n\n\n\n13.6.3 Removing the homogeneity of variance assumption\nIn our example, the homogeneity of variance assumption turned out to be a pretty safe one: the Levene test came back non-significant (notwithstanding that we should also look at the plot of standard deviations), so we probably don’t need to worry. However, in real life we aren’t always that lucky. How do we save our ANOVA when the homogeneity of variance assumption is violated? If you recall from our discussion of t-tests, we’ve seen this problem before. The Student t-test assumes equal variances, so the solution was to use the Welch t-test, which does not. In fact, Welch (1951) also showed how we can solve this problem for ANOVA too (the Welch one-way test). It’s implemented in jamovi using the One-Way ANOVA analysis. This is a specific analysis approach just for one-way ANOVA, and to run the Welch one-way ANOVA for our example, we would re-run the analysis as previously, but this time use the jamovi ANOVA - One Way ANOVA analysis command, and check the option for Welch’s test (see Figure 13.6). To understand what’s happening here, let’s compare these numbers to what we got earlier when Running an ANOVA in jamovi originally. To save you the trouble of flicking back, this is what we got last time: \\(F(2, 15) = 18.611, p = .00009\\), also shown as the Fisher’s test in the One-Way ANOVA shown in Figure 13.6.\n\n\n\n\n\nFigure 13.6: Welch’s test as part of the One Way ANOVA analysis in jamovi\n\n\n\n\nOkay, so originally our ANOVA gave us the result \\(F(2, 15) = 18.6\\), whereas the Welch one way test gave us \\(F(2, 9.49) = 26.32\\). In other words, the Welch test has reduced the within-groups degrees of freedom from 15 to 9.49, and the F-value has increased from 18.6 to 26.32.\n\n\n13.6.4 Checking the normality assumption\nTesting the normality assumption is relatively straightforward. We covered most of what you need to know in Section 11.9. The only thing we really need to do is draw a QQ plot and, in addition if it is available, run the Shapiro-Wilk test. The QQ plot is shown in Figure 13.7 and it looks pretty normal to me. If the Shapiro-Wilk test is not significant (i.e. \\(p > .05\\)) then this indicates that the assumption of normality is not violated. However, as with Levene’s test, if the sample size is large then a significant Shapiro-Wilk test may in fact be a false positive, where the assumption of normality is not violated in any substantive problematic sense for the analysis. And, similarly, a very small sample can produce false negatives. That’s why a visual inspection of the QQ plot is important.\n\n\n\n\n\nFigure 13.7: QQ plot in the One Way ANOVA analysis in jamovi\n\n\n\n\nAlongside inspecting the QQ plot for any deviations from normality, the Shapiro-Wilk test for our data does show a non-significant effect, with p = 0.6053 (see Figure 13.6. This therefore supports the QQ plot assessment; both checks find no indication that normality is violated.\n\n\n13.6.5 Removing the normality assumption\nNow that we’ve seen how to check for normality, we are led naturally to ask what we can do to address violations of normality. In the context of a one-way ANOVA, the easiest solution is probably to switch to a non-parametric test (i.e., one that doesn’t rely on any particular assumption about the kind of distribution involved). We’ve seen non-parametric tests before, in Chapter 11. When you only have two groups, the Mann-Whitney or the Wilcoxon test provides the non-parametric alternative that you need. When you’ve got three or more groups, you can use the Kruskal-Wallis rank sum test (Kruskal & Wallis, 1952). So that’s the test we’ll talk about next.\n\n\n13.6.6 The logic behind the Kruskal-Wallis test\nThe Kruskal-Wallis test is surprisingly similar to ANOVA, in some ways. In ANOVA we started with \\(Y_{ik}\\), the value of the outcome variable for the ith person in the kth group. For the Kruskal Wallis test what we’ll do is rank order all of these \\(Y_{ik}\\) values and conduct our analysis on the ranked data. 14\n\n\n13.6.7 Additional details\nThe description in the previous section illustrates the logic behind the Kruskal-Wallis test. At a conceptual level, this is the right way to think about how the test works.15\nBut wait, there’s more! Dear lord, why is there always more? The story I’ve told so far is only actually true when there are no ties in the raw data. That is, if there are no two observations that have exactly the same value. If there are ties, then we have to introduce a correction factor to these calculations. At this point I’m assuming that even the most diligent reader has stopped caring (or at least formed the opinion that the tie-correction factor is something that doesn’t require their immediate attention). So I’ll very quickly tell you how it’s calculated, and omit the tedious details about why it’s done this way. Suppose we construct a frequency table for the raw data, and let fj be the number of observations that have the j-th unique value. This might sound a bit abstract, so here’s a concrete example from the frequency table of mood.gain from the clinicaltrials.csv data set (Table 13.11)\n\n\n\n\nTable 13.11:  Frequency table of mood gain from the clinicaltrials.csv data \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\nLooking at this table, notice that the third entry in the frequency table has a value of 2. Since this corresponds to a mood.gain of 0.3, this table is telling us that two people’s mood increased by 0.3. 16\nAnd so jamovi uses a tie-correction factor to calculate the tie-corrected Kruskall-Wallis statistic. And at long last, we’re actually finished with the theory of the Kruskal-Wallis test. I’m sure you’re all terribly relieved that I’ve cured you of the existential anxiety that naturally arises when you realise that you don’t know how to calculate the tie-correction factor for the Kruskal-Wallis test. Right?\n\n\n13.6.8 How to run the Kruskal-Wallis test in jamovi\nDespite the horror that we’ve gone through in trying to understand what the Kruskal Wallis test actually does, it turns out that running the test is pretty painless, since jamovi has an analysis as part of the ANOVA analysis set called ‘Non-Parametric’ - ‘One Way ANOVA (Kruskall-Wallis)’ Most of the time you’ll have data like the clinicaltrial.csv data set, in which you have your outcome variable mood.gain and a grouping variable drug. If so, you can just go ahead and run the analysis in jamovi. What this gives us is a Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\), as in Figure 13.8\n\n\n\n\n\nFigure 13.8: Kruskall-Wallis one-way non-parametric ANOVA in jamovi"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#repeated-measures-one-way-anova",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#repeated-measures-one-way-anova",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.7 Repeated measures one-way ANOVA",
    "text": "13.7 Repeated measures one-way ANOVA\nThe one-way repeated measures ANOVA test is a statistical method of testing for significant differences between three or more groups where the same participants are used in each group (or each participant is closely matched with participants in other experimental groups). For this reason, there should always be an equal number of scores (data points) in each experimental group. This type of design and analysis can also be called a ‘related ANOVA’ or a ‘within subjects ANOVA’.\nThe logic behind a repeated measures ANOVA is very similar to that of an independent ANOVA (sometimes called a ‘between-subjects’ ANOVA). You’ll remember that earlier we showed that in a between-subjects ANOVA total variability is partitioned into between-groups variability (\\(SS_b\\)) and within-groups variability (\\(SS_w\\)), and after each is divided by the respective degrees of freedom to give MSb and MSw (see Table 13.1) the F-ratio is calculated as:\n\\[F=\\frac{MS_b}{MS_w}\\]\nIn a repeated measures ANOVA, the F-ratio is calculated in a similar way, but whereas in an independent ANOVA the within-group variability (\\(SS_w\\)) is used as the basis for the \\(MS_w\\) denominator, in a repeated measures ANOVA the \\(SS_w\\) is partioned into two parts. As we are using the same subjects in each group, we can remove the variability due to the individual differences between subjects (referred to as SSsubjects) from the within-groups variability. We won’t go into too much technical detail about how this is done, but essentially each subject becomes a level of a factor called subjects. The variability in this within-subjects factor is then calculated in the same way as any between-subjects factor. And then we can subtract SSsubjects from \\(SS_w\\) to provide a smaller SSerror term:\n\\[\\text{Independent ANOVA: } SS_{error} = SS_w\\] \\[\\text{Repeated Measures ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] This change in \\(SS_{error}\\) term often leads to a more powerful statistical test, but this does depend on whether the reduction in the \\(SS_{error}\\) more than compensates for the reduction in degrees of freedom for the error term (as degrees of freedom go from \\((n - k)\\) 17 to \\((n - 1)(k - 1)\\) (remembering that there are more subjects in the independent ANOVA design).\n\n13.7.1 Repeated measures ANOVA in jamovi\nFirst, we need some data. Geschwind (1972) has suggested that the exact nature of a patient’s language deficit following a stroke can be used to diagnose the specific region of the brain that has been damaged. A researcher is concerned with identifying the specific communication difficulties experienced by six patients suffering from Broca’s Aphasia (a language deficit commonly experienced following a stroke) (Table 13.12).\n\n\n\n\nTable 13.12:  Word recognition task scores in stroke patients \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\nThe patients were required to complete three word recognition tasks. On the first (speech production) task, patients were required to repeat single words read out aloud by the researcher. On the second (conceptual) task, designed to test word comprehension, patients were required to match a series of pictures with their correct name. On the third (syntax) task, designed to test knowledge of correct word order, patients were asked to reorder syntactically incorrect sentences. Each patient completed all three tasks. The order in which patients attempted the tasks was counterbalanced between participants. Each task consisted of a series of 10 attempts. The number of attempts successfully completed by each patient are shown in Table 13.11. Enter these data into jamovi ready for analysis (or take a short-cut and load up the broca.csv file).\nTo perform a one-way related ANOVA in jamovi, open the one-way repeated measures ANOVA dialogue box, as in Figure 13.9, via ANOVA - Repeated Measures ANOVA.\n\n\n\n\n\nFigure 13.9: Repeated measures ANOVA dialogue box in jamovi\n\n\n\n\nThen:\n\nEnter a Repeated Measures Factor Name. This should be a label that you choose to describe the conditions repeated by all participants. For example, to describe the speech, conceptual and syntax tasks completed by all participants a suitable label would be ‘Task’. Note that this new factor name represents the independent variable in the analysis.\nAdd a third level in the Repeated Measures Factors text box, as there are three levels representing the three tasks: speech, conceptual and syntax. Change the labels of the levels accordingly.\nThen move each of the levels variables across to the Repeated Measures Cells text box.\nFinally, under the Assumption Checks option, tick the “Sphericity checks” text box.\n\njamovi output for a one-way repeated measures ANOVA is produced as shown in Figure 13.10 to Figure 13.13. The first output we should look at is Mauchly’s Test of Sphericity, which tests the hypothesis that the variances of the differences between the conditions are equal (meaning that the spread of difference scores between the study conditions is approximately the same). In Figure 13.10 Mauchly’s test significance level is \\(p = .720\\). If Mauchly’s test is non-significant (i.e. p < .05, as is the case in this analysis) then it is reasonable to conclude that the variances of the differences are not significantly different (i.e. they are roughly equal and sphericity can be assumed.).\n\n\n\n\n\nFigure 13.10: One-way repeated measures ANOVA output - Mauchly’s Test of Sphericity\n\n\n\n\nIf, on the other hand, Mauchly’s test had been significant (p < .05) then we would conclude that there are significant differences between the variance of the differences, and the requirement of sphericity has not been met. In this case, we should apply a correction to the F-value obtained in the one-way related ANOVA analysis:\n\nIf the Greenhouse-Geisser value in the “Tests of Sphericity” table is > .75 then you should use the Huynh-Feldt correction\nBut if the Greenhouse-Geisser value is < .75, then you should use the Greenhouse-Geisser correction.\n\nBoth these corrected F-values can be specified in the Sphericity Corrections check boxes under the Assumption Checks options, and the corrected F-values are then shown in the results table, as in Figure 13.11.\n\n\n\n\n\nFigure 13.11: One-way repeated measures ANOVA output - Tests of Within-Subjects Effects\n\n\n\n\nIn our analysis, we saw that the significance of Mauchly’s Test of Sphericity was p = .720 (i.e. p > 0.05). So, this means we can assume that the requirement of sphericity has been met so no correction to the F-value is needed. Therefore, we can use the ‘None’ Sphericity Correction output values for the repeated measure ‘Task’: \\(F = 6.93\\), \\(df = 2\\), \\(p = .013\\), and we can conclude that the number of tests successfully completed on each language task did vary significantly depending on whether the task was speech, comprehension or syntax based (\\(F(2, 10) = 6.93\\), \\(p = .013\\)).\nPost-hoc tests can also be specified in jamovi for repeated measures ANOVA in the same way as for independent ANOVA. The results are shown in Figure 13.12. These indicate that there is a significant difference between Speech and Syntax, but not between other levels.\n\n\n\n\n\nFigure 13.12: Post-hoc tests in repeated measures ANOVA in jamovi\n\n\n\n\nDescriptive statistics (marginal means) can be reviewed to help interpret the results, produced in the jamovi output as in Figure 13.13. Comparison of the mean number of trials successfully completed by participants shows that Broca’s Aphasics perform reasonably well on speech production (mean = 7.17) and language comprehension (mean = 6.17) tasks. However, their performance was considerably worse on the syntax task (mean = 4.33), with a significant difference in post-hoc tests between Speech and Syntax task performance.\n\n\n\n\n\nFigure 13.13: One-way repeated measures ANOVA output - Descriptive Statistics"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#the-friedman-non-parametric-repeated-measures-anova-test",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#the-friedman-non-parametric-repeated-measures-anova-test",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.8 The Friedman non-parametric repeated measures ANOVA test",
    "text": "13.8 The Friedman non-parametric repeated measures ANOVA test\nThe Friedman test is a non-parametric version of a repeated measures ANOVA and can be used instead of the Kruskall-Wallis test when testing for differences between three or more groups where the same participants are in each group, or each participant is closely matched with participants in other conditions. If the dependent variable is ordinal, or if the assumption of normality is not met, then the Friedman test can be used.\nAs with the Kruskall-Wallis test, the underlying mathematics is complicated, and won’t be presented here. For the purpose of this book, it is sufficient to note that jamovi calculates the tie-corrected version of the Friedman test, and in Figure 13.14 there is an example using the Broca’s Aphasia data we have already looked at.\n\n\n\n\n\nFigure 13.14: The ‘Repeated Measures ANOVA (Non-parametric)’ dialogue box and results in jamovi\n\n\n\n\nIt’s pretty straightforward to run a Friedman test in jamovi. Just select Analyses - ANOVA - Repeated Measures ANOVA (Non-parametric), as in Figure 13.14. Then highlight and transfer the names of the repeated measures variables you wish to compare (Speech, Conceptual, Syntax) into the ‘Measures:’ text box. To produce descriptive statistics (means and medians) for the three repeated measures variables, click on the Descriptives button\nThe jamovi results show descriptive statistics, chi-square value, degrees of freedom, and the p-value (Figure 13.14). Since the p-value is less than the level conventionally used to determine significance (p < .05), we can conclude that Broca’s Aphasics perform reasonably well on speech production (median = 7.5) and language comprehension (median = 6.5) tasks. However, their performance was considerably worse on the syntax task (median = 4.5), with a significant difference in post-hoc tests between Speech and Syntax task performance."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.9 變異數分析與t檢定的關聯",
    "text": "13.9 變異數分析與t檢定的關聯\nThere’s one last thing I want to point out before finishing. It’s something that a lot of people find kind of surprising, but it’s worth knowing about. An ANOVA with two groups is identical to the Student t-test. No, really. It’s not just that they are similar, but they are actually equivalent in every meaningful way. I won’t try to prove that this is always true, but I will show you a single concrete demonstration. Suppose that, instead of running an ANOVA on our mood.gain ~ drug model, let’s instead do it using therapy as the predictor. If we run this ANOVA we get an F-statistic of \\(F(1,16) = 1.71\\), and a p-value = \\(0.21\\). Since we only have two groups, I didn’t actually need to resort to an ANOVA, I could have just decided to run a Student t-test. So let’s see what happens when I do that: I get a t-statistic of \\(t(16) = -1.3068\\) and a \\(p-value = 0.21\\). Curiously, the p-values are identical. Once again we obtain a value of \\(p = .21\\). But what about the test statistic? Having run a t-test instead of an ANOVA, we get a somewhat different answer, namely \\(t(16) = -1.3068\\). However, there is a fairly straightforward relationship here. If we square the t-statistic then we get the F-statistic from before: \\(-1.3068^{2} = 1.7077\\)"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#summary",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#summary",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.10 Summary",
    "text": "13.10 Summary\nThere’s a fair bit covered in this chapter, but there’s still a lot missing 18. Most obviously, I haven’t discussed how to run an ANOVA when you are interested in more than one grouping variable, but that will be discussed in a lot of detail in Chapter 14. In terms of what we have discussed, the key topics were:\n\nThe basic logic behind How ANOVA works and Running an ANOVA in jamovi\nHow to compute an Effect size for an ANOVA.\nMultiple comparisons and post hoc tests for multiple testing.\nThe assumptions of one-way ANOVA\nChecking the homogeneity of variance assumption and what to do if it is violated: Removing the homogeneity of variance assumption\nChecking the normality assumption and what to do if it is violated: Removing the normality assumption\nRepeated measures one-way ANOVA and the non-parametric equivalent, The Friedman non-parametric repeated measures ANOVA test\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific American, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et al (Ed.), Contributions to probability and statistics: Essays in honor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance: Fixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An alternative approach. Biometrika, 38, 330–336."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#相關",
    "href": "12-Correlation-and-linear-regression.html#相關",
    "title": "10  相闗與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\nIn this section we’ll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data (Table 10.1).\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  Data for correlation analysis - descriptive statistics for the parenthood data \n\nvariableminmaxmeanmedianstd. devIQR\n\nDani's grumpiness419163.716210.0514\n\nDani's hours slept4.849.006.977.031.021.45\n\nDani's son's hours slept3.2512.078.057.952.073.21\n\n\n\n\n\nLet’s turn to a topic close to every parent’s heart: sleep. The data set we’ll use is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to \\(100\\) (grumpy as a very, very grumpy old man or woman). And lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for \\(100\\) days. And, being a nerd, I’ve saved the data as a file called parenthood.csv. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.1\nNext, I’ll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, Figure 10.1 plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.2 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s pretty standard.\n\n\n\n\n\nFigure 10.1: Histograms for the three interesting variables in the parenthood data set\n\n\n\n\n\n\n10.1.2 相關的強度與方向\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dani.sleep and dani.grump (Figure 10.1), left) with that between baby.sleep and dani.grump (Figure 10.2), right). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\n\n\n\n\n\nFigure 10.2: Scatterplots showing the relationship between dani.sleep and dani.grump (left) and the relationship between baby.sleep and dani.grump (right)\n\n\n\n\nIn contrast, let’s consider the two scatterplots shown in Figure 10.3. If we compare the scatterplot of “baby.sleep v dani.grump” (left) to the scatterplot of “’baby.sleep v dani.sleep” (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps more then I get less grumpy (negative relationship, left hand side).\n\n\n\n\n\nFigure 10.3: Scatterplots showing the relationship between baby.sleep and dani.grump (left), as compared to the relationship between baby.sleep and dani.sleep (right)\n\n\n\n\n\n\n10.1.3 相關係數\nWe can make these ideas a bit more explicit by introducing the idea of a 相關係數(correlation coefficient) (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\) ), which we’ll define more precisely in the next section, is a measure that varies from -1 to 1. When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure 10.4, you can see several plots showing what different correlations look like.\n[Additional technical detail 3]\n\n\n\n\n\nFigure 10.4: Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are \\(0, .33, .66\\) and \\(1\\). In the right hand column, the correlations are \\(0, -.33, -.66\\) and \\(-1\\)\n\n\n\n\nBy standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in the section on [Interpreting a correlation]. But before I do, let’s look at how to calculate correlations in jamovi.\n\n\n10.1.4 相關係數計算實務\nCalculating correlations in jamovi can be done by clicking on the ‘Regression’ - ‘Correlation Matrix’ button. Transfer all four continuous variables across into the box on the right to get the output in Figure 10.5.\n\n\n\n\n\nFigure 10.5: A jamovi screenshot showing correlations between variables in the parenthood.csv file\n\n\n\n\n\n\n10.1.5 解讀相關係數\nNaturally, in real life you don’t see many correlations of \\(1\\). So how should you interpret a correlation of, say, r = \\(.4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table 10.2 is pretty typical.\n\n\n\n\nTable 10.2:  A rough guide to interpreting correlations. Note that I say a rough guide. There aren’t hard and fast rules for what counts as strong or weak relationships. It depends on the context. \n\nCorrelationStrengthDirection\n\n-1.0 to -0.9Very strongNegative\n\n-0.9 to -0.7StrongNegative\n\n-0.7 to -0.4ModerateNegative\n\n-0.4 to -0.2WeakNegative\n\n-0.2 to 0NegligibleNegative\n\n0 to 0.2NegligiblePositive\n\n0.2 to 0.4WeakPositive\n\n0.4 to 0.7ModeratePositive\n\n0.7 to 0.9StrongPositive\n\n0.9 to 1.0Very strongPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” (Anscombe, 1973), a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is \\(9\\) and the mean for \\(Y\\) is \\(7.5\\). The standard deviations for all \\(X\\) variables are almost identical, as are those for the Y variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since I happen to have saved it in a file called anscombe.csv.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 10.6, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is “always graph your raw data” (see Chapter 5).\n\n\n\n\n\nFigure 10.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of r = .816, but they are qualitatively different from one another\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable Y , but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort (\\(X\\)) into learning a subject then you should expect a grade of \\(0\\%\\) (\\(Y\\)). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of \\(90\\%\\) than it takes to get a grade of \\(55\\%\\). What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure 10.7, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of \\(0.91\\). However, this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\n\n\n\n\n\nFigure 10.7: The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(\\rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved\n\n\n\n\nHow should we address this? Actually, it’s really easy. If we’re looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all \\(10\\) of our students in order of hours worked. That is, student \\(1\\) did the least work out of anyone (\\(2\\) hours) so they get the lowest rank (rank = \\(1\\)). Student \\(4\\) was the next laziest, putting in only \\(6\\) hours of work over the whole semester, so they get the next lowest rank (rank = \\(2\\)). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = \\(1\\)” to mean “top rank” rather than “bottom rank”. So be careful, you can rank “from smallest value to largest value” (i.e., small equals rank \\(1\\)) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, but as it’s really easy to forget which way you set things up you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward Table 10.3.\n\n\n\n\nTable 10.3:  Students ranked in terms of effort and reward \n\nrank (hours worked)rank (grade received)\n\nstudent 111\n\nstudent 21010\n\nstudent 366\n\nstudent 422\n\nstudent 533\n\nstudent 655\n\nstudent 744\n\nstudent 888\n\nstudent 977\n\nstudent 1099\n\n\n\n\n\nHmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.\nWhat we’ve just re-invented is 斯皮爾曼等級相關(Spearman’s rank order correlation), usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation r. We can calculate Spearman’s \\(\\rho\\) using jamovi simply by clicking the ‘Spearman’ check box in the ‘Correlation Matrix’ screen."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#散佈圖",
    "href": "12-Correlation-and-linear-regression.html#散佈圖",
    "title": "10  相闗與線性迴歸",
    "section": "10.2 散佈圖",
    "text": "10.2 散佈圖\nScatterplots are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on [Correlations]. It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in jamovi, using the same parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we’re after. The first way is to use the ‘Plot’ option under the ‘Regression’ - ‘Correlation Matrix’ button, giving us the output shown in Figure 10.8. Note that jamovi draws a line through the points, we’ll come onto this a bit later in the section on [What is a linear regression model?]. Plotting a scatterplot in this way also allow you to specify ‘Densities for variables’ and this option adds a density curve showing how the data in each variable is distributed.\n\n\n\n\n\nFigure 10.8: Scatterplot via the ‘Correlation Matrix’ command in jamovi\n\n\n\n\nThe second way do to it is to use one of the jamovi add-on modules. This module is called ‘scatr’ and you can install it by clicking on the large ‘\\(+\\)’ icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find ‘scatr’ and clicking ‘install’. When you have done this, you will find a new ‘Scatterplot’ command available under the ‘Exploration’ button. This plot is a bit different than the first way, see Figure 10.9, but the important information is the same.\n\n\n\n\n\nFigure 10.9: Scatterplot via the ‘scatr’ add-on module in - jamovi\n\n\n\n\n\n10.2.1 更多探討散佈圖的方案\nOften you will want to look at the relationships between several variables at once, using a scatterplot matrix (in jamovi via the ‘Correlation Matrix’ - ‘Plot’ command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in Figure 10.10.\n\n\n\n\n\nFigure 10.10: A matrix of scatterplots produced using jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "href": "12-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "title": "10  相闗與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see [Correlations]), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 10.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 10.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 10.11 (b).\n\n\n\n\n\nFigure 10.11: Panel (a) shows the sleep-grumpiness scatterplot from Figure 10.9 with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel (b) shows the same data, but with a very poor choice of regression line drawn over the top\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 10.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).4 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).5 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#線性迴歸模型參數估計",
    "href": "12-Correlation-and-linear-regression.html#線性迴歸模型參數估計",
    "title": "12  相闗與線性迴歸",
    "section": "12.4 線性迴歸模型參數估計",
    "text": "12.4 線性迴歸模型參數估計\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 12.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 12.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 12.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n12.4.1 實作線性迴歸模型\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 12.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 12.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 12.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n12.4.2 解讀線性迴歸模型參數估計\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#多元線性迴歸",
    "href": "12-Correlation-and-linear-regression.html#多元線性迴歸",
    "title": "10  相闗與線性迴歸",
    "section": "10.5 多元線性迴歸",
    "text": "10.5 多元線性迴歸\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n10.5.1 Doing it in jamovi\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 10.4.\n\n\n\n\nTable 10.4:  Adding multiple variables as predictors in a regression \n\n(Intercept)dani.sleepbaby.sleep\n\n125.97-8.950.01\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 10.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 10.14: A 3D visualisation of a multiple regression model. There are two predictors in the model, dani.sleep and baby.sleep and the outcome variable is dani.grump. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we’re trying to do is find a plane that is as close to all the blue dots as possible\n\n\n\n\n[Additional technical detail7]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "href": "12-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "title": "10  相闗與線性迴歸",
    "section": "10.6 量化迴歸模型的適配性",
    "text": "10.6 量化迴歸模型的適配性\nSo we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction \\(\\hat{Y}_i\\) about what my mood is like, but my actual mood is \\(Y_i\\) . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n\n10.6.1 The \\(R^2\\) value\nOnce again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\nwhich we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\nWhile we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the \\(\\hat{Y}\\) values, and for the simple model that uses only a single predictor we would do the following:\n\ncreate a new column called ‘Y.pred’ using the formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’\ncalculate the SS(resid) by creating a new column called ‘(Y-Y.pred)^2’ using the formula ’ = (dani.grump - Y.pred)^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ’ sum( ( Y-Y.pred)^2 ) .\nAt the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ‘mean’ in its function).\nThen create a new column, called ’ (Y - mean(Y))^2 )’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ‘sum( (Y - mean(Y))^2 )’.\nCalculate R.squared by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.\n\nThis gives a value for \\(R^2\\) of ‘0.8161018’. The \\(R^2\\) value, sometimes called the coefficient of determination8 has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained \\(R^2 = .816\\) means that the predictor (my.sleep) explains \\(81.6\\%\\) of the variance in the outcome (my.grump).\nNaturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the \\(R^2\\) value for your regression model. As we’ll see later on in the section on Running the hypothesis tests in jamovi, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of \\(R^2\\) that I want to point out.\n\n\n10.6.2 The relationship between regression and correlation\nAt this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol \\(r\\) to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient \\(r\\) and the \\(R^2\\) value from linear regression? Of course there is: the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.\n\n\n10.6.3 The adjusted \\(R^2\\) value\nOne final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted \\(R^2\\)”. The motivation behind calculating the adjusted \\(R^2\\) value is the observation that adding more predictors into the model will always cause the \\(R^2\\) value to increase (or at least not decrease).\n[Additional technical detail9]\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted \\(R^2\\) value is that when you add more predictors to the model, the adjusted \\(R^2\\) value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted \\(R^2\\) value can’t be interpreted in the elegant way that \\(R^2\\) can. \\(R^2\\) has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted \\(R^2\\).\nAn obvious question then is whether you should report \\(R^2\\) or adjusted \\(R^2\\) . This is probably a matter of personal preference. If you care more about interpretability, then \\(R^2\\) is better. If you care more about correcting for bias, then adjusted \\(R^2\\) is probably better. Speaking just for myself, I prefer \\(R^2\\). My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in [Hypothesis tests for regression models], if you’re worried that the improvement in \\(R^2\\) that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "href": "12-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "title": "10  相闗與線性迴歸",
    "section": "10.7 迴歸模型的假設檢定",
    "text": "10.7 迴歸模型的假設檢定\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n10.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail10]\nWe’ll see much more of the F statistic in Chapter 13, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n10.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the [Multiple linear regression] model we have already looked at (Table 10.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.11 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n10.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 10.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 10.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 10.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.12\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#更多迴歸模型透露的資訊",
    "href": "12-Correlation-and-linear-regression.html#更多迴歸模型透露的資訊",
    "title": "12  相闗與線性迴歸",
    "section": "12.8 更多迴歸模型透露的資訊",
    "text": "12.8 更多迴歸模型透露的資訊\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n12.8.1 迴歸係數的信賴區間\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 12.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n12.8.2 標準化迴歸係數的計算方法\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 12.16.\n\n\n\n\n\nFigure 12.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "href": "12-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "title": "10  相闗與線性迴歸",
    "section": "10.9 迴歸模型的適用條件",
    "text": "10.9 迴歸模型的適用條件\nThe linear regression model that I’ve been discussing relies on several assumptions. In [Model checking] we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.\n\nLinearity. A pretty fundamental assumption of the linear regression model is that the relationship between \\(X\\) and \\(Y\\) actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.\nIndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.\nNormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors \\(X\\) and the outcome \\(Y\\) are non-normal, so long as the residuals \\(\\epsilon\\) are normal. See the [Checking the normality of the residuals] section.\nEquality (or ‘homogeneity’) of variance. Strictly speaking, the regression model assumes that each residual \\(\\epsilon_i\\) is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation \\(\\sigma\\) that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of \\(\\hat{Y}\\) , and (if we’re being especially paranoid) all values of every predictor \\(X\\) in the model.\n\nSo, we have four main assumptions for linear regression (that neatly form the acronym ‘LINE’). And there are also a couple of other things we should also check for:\n\nUncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See the [Checking for collinearity] section.\nNo “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on [Three kinds of anomalous data]."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "href": "12-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "title": "10  相闗與線性迴歸",
    "section": "10.11 決定線性模型的變項組合",
    "text": "10.11 決定線性模型的變項組合\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 23\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n10.11.1 逐步排除法\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n10.11.2 逐步納入法\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n10.11.3 使用警告\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n10.11.4 比較迴歸模型\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.24\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 25\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 10.24. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 10.24.\n\n\n\n\n\nFigure 10.24: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#本章小結",
    "href": "12-Correlation-and-linear-regression.html#本章小結",
    "title": "10  相闗與線性迴歸",
    "section": "10.12 本章小結",
    "text": "10.12 本章小結\n\n想了解兩個變項之間的關聯性有多強？就計算相關係數\n散佈圖繪製方法\n前進下一章前必學的課題：什麼是線性迴歸模型 以及使用線性迴歸模型估計參數\n多元線性迴歸\n量化迴歸模型的適配性 要了解 \\(R^2\\) 。\n迴歸模型的假設檢定\n在迴歸係數的更多資訊 這一節，我們學習如何計算迴歸係數的信賴區間以及標準化迴歸係數的計算方法\n迴歸模型的適用條件 以及診斷適用條件\n決定線性模型的變項組合\n\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.1 本章示範資料",
    "text": "13.1 本章示範資料\nSuppose you’ve become involved in a clinical trial in which you are testing a new antidepressant drug called Joyzepam. In order to construct a fair test of the drug’s effectiveness, the study involves three separate drugs to be administered. One is a placebo, and the other is an existing antidepressant / anti-anxiety drug called Anxifree. A collection of 18 participants with moderate to severe depression are recruited for your initial testing. Because the drugs are sometimes administered in conjunction with psychological therapy, your study includes 9 people undergoing cognitive behavioural therapy (CBT) and 9 who are not. Participants are randomly assigned (doubly blinded, of course) a treatment, such that there are 3 CBT people and 3 no-therapy people assigned to each of the 3 drugs. A psychologist assesses the mood of each person after a 3 month run with each drug, and the overall improvement in each person’s mood is assessed on a scale ranging from \\(-5\\) to \\(+5\\). With that as the study design, let’s now load up the data file in clinicaltrial.csv. We can see that this data set contains the three variables drug, therapy and mood.gain.\nFor the purposes of this chapter, what we’re really interested in is the effect of drug on mood.gain. The first thing to do is calculate some descriptive statistics and draw some graphs. In the Chapter 4 chapter we showed you how to do this, and some of the descriptive statistics we can calculate in jamovi are shown in Figure 13.1\n\n\n\n\n\nFigure 13.1: Descriptives for mood gain, and box plots by drug administered\n\n\n\n\nAs the plot makes clear, there is a larger improvement in mood for participants in the Joyzepam group than for either the Anxifree group or the placebo group. The Anxifree group shows a larger mood gain than the control group, but the difference isn’t as large. The question that we want to answer is are these difference “real”, or are they just due to chance?"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.3 jamovi的變異數分析模組",
    "text": "13.3 jamovi的變異數分析模組\nI’m pretty sure I know what you’re thinking after reading the last section, especially if you followed my advice and did all of that by pencil and paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations yourself sucks. There’s quite a lot of calculations that we needed to do along the way, and it would be tedious to have to do this over and over again every time you wanted to do an ANOVA.\n\n13.3.1 使用jamovi完成變異數分析\nTo make life easier for you, jamovi can do ANOVA…hurrah! Go to the ‘ANOVA’ - ‘ANOVA’ analysis, and move the mood.gain variable across so it is in the ‘Dependent Variable’ box, and then move the drug variable across so it is in the ‘Fixed Factors’ box. This should give the results as shown in Figure 13.3. 9 Note I have also checked the \\(\\eta^2\\) checkbox, pronounced “eta” squared, under the ‘Effect Size’ option and this is also shown on the results table. We will come back to effect sizes a bit later.\n\n\n\n\n\nFigure 13.3: jamovi results table for ANOVA of mood gain by drug administered\n\n\n\n\nThe jamovi results table shows you the sums of squares values, the degrees of freedom, and a couple of other quantities that we’re not really interested in right now. Notice, however, that jamovi doesn’t use the names “between-group” and “within-group”. Instead, it tries to assign more meaningful names. In our particular example, the between groups variance corresponds to the effect that the drug has on the outcome variable, and the within groups variance corresponds to the “leftover” variability so it calls that the residuals. If we compare these numbers to the numbers that I calculated by hand in [A worked example], you can see that they’re more or less the same, apart from rounding errors. The between groups sums of squares is \\(SS_b = 3.45\\), the within groups sums of squares is \\(SS_w = 1.39\\), and the degrees of freedom are \\(2\\) and \\(15\\) respectively. We also get the F-value and the p-value and, again, these are more or less the same, give or take rounding errors, to the numbers that we calculated ourselves when doing it the long and tedious way."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#效果量",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#效果量",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.4 效果量",
    "text": "13.4 效果量\nThere’s a few different ways you could measure the effect size in an ANOVA, but the most commonly used measures are \\(\\eta^2\\) (eta squared) and partial \\(\\eta^2\\). For a one way analysis of variance they’re identical to each other, so for the moment I’ll just explain \\(\\eta^2\\) . The definition of \\(\\eta^2\\) is actually really simple\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\nThat’s all it is. So when I look at the ANOVA table in Figure 13.3, I see that \\(SS_b = 3.45\\) and \\(SS_tot = 3.45 + 1.39 = 4.84\\). Thus we get an \\(\\eta^2\\) value of\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable (mood.gain) that can be explained in terms of the predictor (drug). A value of \\(\\eta^2=0\\) means that there is no relationship at all between the two, whereas a value of \\(\\eta^2=1\\) means that the relationship is perfect. Better yet, the \\(\\eta^2\\) value is very closely related to \\(R^2\\), as discussed previously in Section 12.6.1, and has an equivalent interpretation. Although many statistics text books suggest \\(\\eta^2\\) as the default effect size measure in ANOVA, there’s an interesting blog post by Daniel Lakens suggesting that eta-squared is perhaps not the best measure of effect size in real world data analysis, because it can be a biased estimator. Usefully, there is also an option in jamovi to specify omega-squared (\\(\\omega^2\\)), which is less biased, alongside eta-squared."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.5 多重比較與事後檢定",
    "text": "13.5 多重比較與事後檢定\nAny time you run an ANOVA with more than two groups and you end up with a significant effect, the first thing you’ll probably want to ask is which groups are actually different from one another. In our drugs example, our null hypothesis was that all three drugs (placebo, Anxifree and Joyzepam) have the exact same effect on mood. But if you think about it, the null hypothesis is actually claiming three different things all at once here. Specifically, it claims that:\n\nYour competitor’s drug (Anxifree) is no better than a placebo (i.e., \\(\\mu_A = \\mu_P\\) )\nYour drug (Joyzepam) is no better than a placebo (i.e., \\(\\mu_J = \\mu_P\\) )\nAnxifree and Joyzepam are equally effective (i.e., \\(\\mu_J = \\mu_A\\))\n\nIf any one of those three claims is false, then the null hypothesis is also false. So, now that we’ve rejected our null hypothesis, we’re thinking that at least one of those things isn’t true. But which ones? All three of these propositions are of interest. Since you certainly want to know if your new drug Joyzepam is better than a placebo, it would be nice to know how well it stacks up against an existing commercial alternative (i.e., Anxifree). It would even be useful to check the performance of Anxifree against the placebo. Even if Anxifree has already been extensively tested against placebos by other researchers, it can still be very useful to check that your study is producing similar results to earlier work.\nWhen we characterise the null hypothesis in terms of these three distinct propositions, it becomes clear that there are eight possible “states of the world” that we need to distinguish between (Table 13.9).\n\n\n\n\nTable 13.9:  The null hypothesis and eight possible ‘states of the world’ \n\npossibility:is \\( \\mu_P = \\mu_A \\)?is \\( \\mu_P = \\mu_J \\)?is \\( \\mu_A = \\mu_J \\)?which hypothesis?\n\n1\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)null\n\n2\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n3\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n4\\( \\checkmark \\)alternative\n\n5\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n6\\( \\checkmark \\)alternative\n\n7\\( \\checkmark \\)alternative\n\n8alternative\n\n\n\n\n\nBy rejecting the null hypothesis, we’ve decided that we don’t believe that #1 is the true state of the world. The next question to ask is, which of the other seven possibilities do we think is right? When faced with this situation, its usually helps to look at the data. For instance, if we look at the plots in Figure 13.1, it’s tempting to conclude that Joyzepam is better than the placebo and better than Anxifree, but there’s no real difference between Anxifree and the placebo. However, if we want to get a clearer answer about this, it might help to run some tests.\n\n13.5.1 成對t檢定\nHow might we go about solving our problem? Given that we’ve got three separate pairs of means (placebo versus Anxifree, placebo versus Joyzepam, and Anxifree versus Joyzepam) to compare, what we could do is run three separate t-tests and see what happens. This is easy to do in jamovi. Go to the ANOVA ‘Post Hoc Tests’ options, move the ‘drug’ variable across into the active box on the right, and then click on the ‘No correction’ checkbox. This will produce a neat table showing all the pairwise t-test comparisons amongst the three levels of the drug variable, as in Figure 13.4\n\n\n\n\n\nFigure 13.4: Uncorrected pairwise t-tests as post hoc comparisons in jamovi\n\n\n\n\n\n\n13.5.2 多動檢定的校正鎮\nIn the previous section I hinted that there’s a problem with just running lots and lots of t-tests. The concern is that, when running these analyses, what we’re doing is going on a “fishing expedition”. We’re running lots and lots of tests without much theoretical guidance in the hope that some of them come up significant. This kind of theory-free search for group differences is referred to as post hoc analysis (“post hoc” being Latin for “after this”).10\nIt’s okay to run post hoc analyses, but a lot of care is required. For instance, the analysis that I ran in the previous section should be avoided, as each individual t-test is designed to have a 5% Type I error rate (i.e., \\(\\alpha = .05\\)) and I ran three of these tests. Imagine what would have happened if my ANOVA involved 10 different groups, and I had decided to run 45 “post hoc” t-tests to try to find out which ones were significantly different from each other, you’d expect 2 or 3 of them to come up significant by chance alone. As we saw in Chapter 9, the central organising principle behind null hypothesis testing is that we seek to control our Type I error rate, but now that I’m running lots of t-tests at once in order to determine the source of my ANOVA results, my actual Type I error rate across this whole family of tests has gotten completely out of control.\nThe usual solution to this problem is to introduce an adjustment to the p-value, which aims to control the total error rate across the family of tests (see Shaffer (1995)). An adjustment of this form, which is usually (but not always) applied because one is doing post hoc analysis, is often referred to as a correction for multiple comparisons, though it is sometimes referred to as “simultaneous inference”. In any case, there are quite a few different ways of doing this adjustment. I’ll discuss a few of them in this section and in Section 14.8 in the next chapter, but you should be aware that there are many other methods out there (see, e.g., Hsu (1996)).\n\n\n13.5.3 Bonferroni校正\nThe simplest of these adjustments is called the Bonferroni correction (Dunn, 1961), and it’s very very simple indeed. Suppose that my post hoc analysis consists of m separate tests, and I want to ensure that the total probability of making any Type I errors at all is at most \\(\\alpha\\).11 If so, then the Bonferroni correction just says “multiply all your raw p-values by m”. If we let \\(p\\) denote the original p-value, and let \\(p_j^{'}\\) be the corrected value, then the Bonferroni correction tells that:\n\\[p_j^{'}=m \\times p\\]\nAnd therefore, if you’re using the Bonferroni correction, you would reject the null hypothesis if \\(p_j^{'} < \\alpha\\). The logic behind this correction is very straightforward. We’re doing m different tests, so if we arrange it so that each test has a Type I error rate of at most \\(\\frac{\\alpha}{m}\\), then the total Type I error rate across these tests cannot be larger than \\(\\alpha\\). That’s pretty simple, so much so that in the original paper, the author writes:\n\nThe method given here is so simple and so general that I am sure it must have been used before this. I do not find it, however, so can only conclude that perhaps its very simplicity has kept statisticians from realizing that it is a very good method in some situations (Dunn (1961), pp 52-53).\n\nTo use the Bonferroni correction in jamovi, just click on the ‘Bonferroni’ checkbox in the ‘Correction’ options, and you will see another column added to the ANOVA results table showing the adjusted p-values for the Bonferroni correction (Table 13.8). If we compare these three p-values to those for the uncorrected, pairwise t-tests, it is clear that the only thing that jamovi has done is multiply them by \\(3\\).\n\n\n13.5.4 Holm校正\nAlthough the Bonferroni correction is the simplest adjustment out there, it’s not usually the best one to use. One method that is often used instead is the Holm correction (Holm, 1979). The idea behind the Holm correction is to pretend that you’re doing the tests sequentially, starting with the smallest (raw) p-value and moving onto the largest one. For the j-th largest of the p-values, the adjustment is either\n\\[p_j^{'}=j \\times p_j\\]\n(i.e., the biggest p-value remains unchanged, the second biggest p-value is doubled, the third biggest p-value is tripled, and so on), or\n\\[p_j^{'}=p_{j+1}^{'}\\]\nwhichever one is larger. This might sound a little confusing, so let’s go through it a little more slowly. Here’s what the Holm correction does. First, you sort all of your p-values in order, from smallest to largest. For the smallest p-value all you do is multiply it by \\(m\\), and you’re done. However, for all the other ones it’s a two-stage process. For instance, when you move to the second smallest p value, you first multiply it by \\(m - 1\\). If this produces a number that is bigger than the adjusted p-value that you got last time, then you keep it. But if it’s smaller than the last one, then you copy the last p-value. To illustrate how this works, consider Table 13.10 which shows the calculations of a Holm correction for a collection of five p-values.\n\n\n\n\nTable 13.10:  Holm corrected p values \n\nraw prank jp \\( \\times \\) jHolm p\n\n.0015.005.005\n\n.0054.020.020\n\n.0193.057.057\n\n.0222.044.057\n\n.1031.103.103\n\n\n\n\n\nHopefully that makes things clear.\nAlthough it’s a little harder to calculate, the Holm correction has some very nice properties. It’s more powerful than Bonferroni (i.e., it has a lower Type II error rate) but, counter-intuitive as it might seem, it has the same Type I error rate. As a consequence, in practice there’s never any reason to use the simpler Bonferroni correction since it is always outperformed by the slightly more elaborate Holm correction. Because of this, the Holm correction should be your go to multiple comparison correction. Figure 13.4 also shows the Holm corrected p-values and, as you can see, the biggest p-value (corresponding to the comparison between Anxifree and the placebo) is unaltered. At a value of .15, it is exactly the same as the value we got originally when we applied no correction at all. In contrast, the smallest p-value (Joyzepam versus placebo) has been multiplied by three.\n\n\n13.5.5 事後檢定的報告格式\nFinally, having run the post hoc analysis to determine which groups are significantly different to one another, you might write up the result like this:\n\nPost hoc tests (using the Holm correction to adjust p) indicated that Joyzepam produced a significantly larger mood change than both Anxifree (p = .001) and the placebo (\\((p = 9.0 \\times{10^{-5}}\\)). We found no evidence that Anxifree performed better than the placebo (\\(p = .15\\)).\n\nOr, if you don’t like the idea of reporting exact p-values, then you’d change those numbers to \\(p < .01\\), \\(p < .001\\) and \\(p > .05\\) respectively. Either way, the key thing is that you indicate that you used Holm’s correction to adjust the p-values. And of course, I’m assuming that elsewhere in the write up you’ve included the relevant descriptive statistics (i.e., the group means and standard deviations), since these p-values on their own aren’t terribly informative."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.6 單因子變異數分析的執行條件",
    "text": "13.6 單因子變異數分析的執行條件\nLike any statistical test, analysis of variance relies on some assumptions about the data, specifically the residuals. There are three key assumptions that you need to be aware of: normality, homogeneity of variance and independence.\n[Additional technical detail 12]\nSo, how do we check whether the assumption about the residuals is accurate? Well, as I indicated above, there are three distinct claims buried in this one statement, and we’ll consider them separately.\n\nHomogeneity of variance. Notice that we’ve only got the one value for the population standard deviation (i.e., \\(\\sigma\\)), rather than allowing each group to have it’s own value (i.e., \\(\\sigma_k\\)). This is referred to as the homogeneity of variance (sometimes called homoscedasticity) assumption. ANOVA assumes that the population standard deviation is the same for all groups. We’ll talk about this extensively in the [Checking the homogeneity of variance assumption] section.\nNormality. The residuals are assumed to be normally distributed. As we saw in Section 11.9, we can assess this by looking at QQ plots (or running a Shapiro-Wilk test. I’ll talk about this more in an ANOVA context in the [Checking the normality assumption] section.\nIndependence. The independence assumption is a little trickier. What it basically means is that, knowing one residual tells you nothing about any other residual. All of the \\(\\epsilon_{ik}\\) values are assumed to have been generated without any “regard for” or “relationship to” any of the other ones. There’s not an obvious or simple way to test for this, but there are some situations that are clear violations of this. For instance, if you have a repeated measures design, where each participant in your study appears in more than one condition, then independence doesn’t hold. There’s a special relationship between some observations, namely those that correspond to the same person! When that happens, you need to use something like a [Repeated measures one-way ANOVA].\n\n\n13.6.1 同質性檢核\n\nTo make the preliminary test on variances is rather like putting to sea in a rowing boat to find out whether conditions are sufficiently calm for an ocean liner to leave port!\n– George Box (Box, 1953)\n\nThere’s more than one way to skin a cat, as the saying goes, and more than one way to test the homogeneity of variance assumption, too (though for some reason no-one made a saying out of that). The most commonly used test for this that I’ve seen in the literature is the Levene test (Levene, 1960), and the closely related Brown-Forsythe test (Brown & Forsythe, 1974).\nRegardless of whether you’re doing the standard Levene test or the Brown-Forsythe test, the test statistic, which is sometimes denoted \\(F\\) but also sometimes written as \\(W\\), is calculated in exactly the same way that the F-statistic for the regular ANOVA is calculated, just using a \\(Z_{ik}\\) rather than \\(Y_{ik}\\). With that in mind, we can go on to look at how to run the test in jamovi.\n[Additional technical detail 13]\n\n\n13.6.2 jamovi的Levene檢定\nOkay, so how do we run the Levene test? Simple really - under the ANOVA ‘Assumption Checks’ option, just click on the ‘Homogeneity tests’ checkbox. If we look at the output, shown in Figure 13.5, we see that the test is non-significant (\\(F_{2,15} = 1.45, p = .266\\)), so it looks like the homogeneity of variance assumption is fine. However, looks can be deceptive! If your sample size is pretty big, then the Levene test could show up a significant effect (i.e. p < .05) even when the homogeneity of variance assumption is not violated to an extent which troubles the robustness of ANOVA. This was the point George Box was making in the quote above. Similarly, if your sample size is quite small, then the homogeneity of variance assumption might not be satisfied and yet a Levene test could be non-significant (i.e. p > .05). What this means is that, alongside any statistical test of the assumption being met, you should always plot the standard deviation around the means for each group / category in the analysis…just to see if they look fairly similar (i.e. homogeneity of variance) or not.\n\n\n\n\n\nFigure 13.5: Levene test output for one-way ANOVA in jamovi\n\n\n\n\n\n\n13.6.3 校正異質性的分析結果\nIn our example, the homogeneity of variance assumption turned out to be a pretty safe one: the Levene test came back non-significant (notwithstanding that we should also look at the plot of standard deviations), so we probably don’t need to worry. However, in real life we aren’t always that lucky. How do we save our ANOVA when the homogeneity of variance assumption is violated? If you recall from our discussion of t-tests, we’ve seen this problem before. The Student t-test assumes equal variances, so the solution was to use the Welch t-test, which does not. In fact, Welch (1951) also showed how we can solve this problem for ANOVA too (the Welch one-way test). It’s implemented in jamovi using the One-Way ANOVA analysis. This is a specific analysis approach just for one-way ANOVA, and to run the Welch one-way ANOVA for our example, we would re-run the analysis as previously, but this time use the jamovi ANOVA - One Way ANOVA analysis command, and check the option for Welch’s test (see Figure 13.6). To understand what’s happening here, let’s compare these numbers to what we got earlier when [Running an ANOVA in jamovi] originally. To save you the trouble of flicking back, this is what we got last time: \\(F(2, 15) = 18.611, p = .00009\\), also shown as the Fisher’s test in the One-Way ANOVA shown in Figure 13.6.\n\n\n\n\n\nFigure 13.6: Welch’s test as part of the One Way ANOVA analysis in jamovi\n\n\n\n\nOkay, so originally our ANOVA gave us the result \\(F(2, 15) = 18.6\\), whereas the Welch one way test gave us \\(F(2, 9.49) = 26.32\\). In other words, the Welch test has reduced the within-groups degrees of freedom from 15 to 9.49, and the F-value has increased from 18.6 to 26.32.\n\n\n13.6.4 常態性檢核\nTesting the normality assumption is relatively straightforward. We covered most of what you need to know in Section 11.9. The only thing we really need to do is draw a QQ plot and, in addition if it is available, run the Shapiro-Wilk test. The QQ plot is shown in Figure 13.7 and it looks pretty normal to me. If the Shapiro-Wilk test is not significant (i.e. \\(p > .05\\)) then this indicates that the assumption of normality is not violated. However, as with Levene’s test, if the sample size is large then a significant Shapiro-Wilk test may in fact be a false positive, where the assumption of normality is not violated in any substantive problematic sense for the analysis. And, similarly, a very small sample can produce false negatives. That’s why a visual inspection of the QQ plot is important.\n\n\n\n\n\nFigure 13.7: QQ plot in the One Way ANOVA analysis in jamovi\n\n\n\n\nAlongside inspecting the QQ plot for any deviations from normality, the Shapiro-Wilk test for our data does show a non-significant effect, with p = 0.6053 (see Figure 13.6. This therefore supports the QQ plot assessment; both checks find no indication that normality is violated.\n\n\n13.6.5 排除非常態性的分析結果\nNow that we’ve seen how to check for normality, we are led naturally to ask what we can do to address violations of normality. In the context of a one-way ANOVA, the easiest solution is probably to switch to a non-parametric test (i.e., one that doesn’t rely on any particular assumption about the kind of distribution involved). We’ve seen non-parametric tests before, in Chapter 11. When you only have two groups, the Mann-Whitney or the Wilcoxon test provides the non-parametric alternative that you need. When you’ve got three or more groups, you can use the Kruskal-Wallis rank sum test (Kruskal & Wallis, 1952). So that’s the test we’ll talk about next.\n\n\n13.6.6 Kruskal-Wallis檢定的邏輯\nThe Kruskal-Wallis test is surprisingly similar to ANOVA, in some ways. In ANOVA we started with \\(Y_{ik}\\), the value of the outcome variable for the ith person in the kth group. For the Kruskal Wallis test what we’ll do is rank order all of these \\(Y_{ik}\\) values and conduct our analysis on the ranked data. 14\n\n\n13.6.7 更多分析細節\nThe description in the previous section illustrates the logic behind the Kruskal-Wallis test. At a conceptual level, this is the right way to think about how the test works.15\nBut wait, there’s more! Dear lord, why is there always more? The story I’ve told so far is only actually true when there are no ties in the raw data. That is, if there are no two observations that have exactly the same value. If there are ties, then we have to introduce a correction factor to these calculations. At this point I’m assuming that even the most diligent reader has stopped caring (or at least formed the opinion that the tie-correction factor is something that doesn’t require their immediate attention). So I’ll very quickly tell you how it’s calculated, and omit the tedious details about why it’s done this way. Suppose we construct a frequency table for the raw data, and let fj be the number of observations that have the j-th unique value. This might sound a bit abstract, so here’s a concrete example from the frequency table of mood.gain from the clinicaltrials.csv data set (Table 13.11)\n\n\n\n\nTable 13.11:  Frequency table of mood gain from the clinicaltrials.csv data \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\nLooking at this table, notice that the third entry in the frequency table has a value of 2. Since this corresponds to a mood.gain of 0.3, this table is telling us that two people’s mood increased by 0.3. 16\nAnd so jamovi uses a tie-correction factor to calculate the tie-corrected Kruskall-Wallis statistic. And at long last, we’re actually finished with the theory of the Kruskal-Wallis test. I’m sure you’re all terribly relieved that I’ve cured you of the existential anxiety that naturally arises when you realise that you don’t know how to calculate the tie-correction factor for the Kruskal-Wallis test. Right?\n\n\n13.6.8 使用jamovi完成Kruskal-Wallis檢定\nDespite the horror that we’ve gone through in trying to understand what the Kruskal Wallis test actually does, it turns out that running the test is pretty painless, since jamovi has an analysis as part of the ANOVA analysis set called ‘Non-Parametric’ - ‘One Way ANOVA (Kruskall-Wallis)’ Most of the time you’ll have data like the clinicaltrial.csv data set, in which you have your outcome variable mood.gain and a grouping variable drug. If so, you can just go ahead and run the analysis in jamovi. What this gives us is a Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\), as in Figure 13.8\n\n\n\n\n\nFigure 13.8: Kruskall-Wallis one-way non-parametric ANOVA in jamovi"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.7 單因子重覆量數變異數分析",
    "text": "13.7 單因子重覆量數變異數分析\nThe one-way repeated measures ANOVA test is a statistical method of testing for significant differences between three or more groups where the same participants are used in each group (or each participant is closely matched with participants in other experimental groups). For this reason, there should always be an equal number of scores (data points) in each experimental group. This type of design and analysis can also be called a ‘related ANOVA’ or a ‘within subjects ANOVA’.\nThe logic behind a repeated measures ANOVA is very similar to that of an independent ANOVA (sometimes called a ‘between-subjects’ ANOVA). You’ll remember that earlier we showed that in a between-subjects ANOVA total variability is partitioned into between-groups variability (\\(SS_b\\)) and within-groups variability (\\(SS_w\\)), and after each is divided by the respective degrees of freedom to give MSb and MSw (see Table 13.1) the F-ratio is calculated as:\n\\[F=\\frac{MS_b}{MS_w}\\]\nIn a repeated measures ANOVA, the F-ratio is calculated in a similar way, but whereas in an independent ANOVA the within-group variability (\\(SS_w\\)) is used as the basis for the \\(MS_w\\) denominator, in a repeated measures ANOVA the \\(SS_w\\) is partioned into two parts. As we are using the same subjects in each group, we can remove the variability due to the individual differences between subjects (referred to as SSsubjects) from the within-groups variability. We won’t go into too much technical detail about how this is done, but essentially each subject becomes a level of a factor called subjects. The variability in this within-subjects factor is then calculated in the same way as any between-subjects factor. And then we can subtract SSsubjects from \\(SS_w\\) to provide a smaller SSerror term:\n\\[\\text{Independent ANOVA: } SS_{error} = SS_w\\] \\[\\text{Repeated Measures ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] This change in \\(SS_{error}\\) term often leads to a more powerful statistical test, but this does depend on whether the reduction in the \\(SS_{error}\\) more than compensates for the reduction in degrees of freedom for the error term (as degrees of freedom go from \\((n - k)\\) 17 to \\((n - 1)(k - 1)\\) (remembering that there are more subjects in the independent ANOVA design).\n\n13.7.1 jamovi的重覆量數變異數分析\nFirst, we need some data. Geschwind (1972) has suggested that the exact nature of a patient’s language deficit following a stroke can be used to diagnose the specific region of the brain that has been damaged. A researcher is concerned with identifying the specific communication difficulties experienced by six patients suffering from Broca’s Aphasia (a language deficit commonly experienced following a stroke) (Table 13.12).\n\n\n\n\nTable 13.12:  Word recognition task scores in stroke patients \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\nThe patients were required to complete three word recognition tasks. On the first (speech production) task, patients were required to repeat single words read out aloud by the researcher. On the second (conceptual) task, designed to test word comprehension, patients were required to match a series of pictures with their correct name. On the third (syntax) task, designed to test knowledge of correct word order, patients were asked to reorder syntactically incorrect sentences. Each patient completed all three tasks. The order in which patients attempted the tasks was counterbalanced between participants. Each task consisted of a series of 10 attempts. The number of attempts successfully completed by each patient are shown in Table 13.11. Enter these data into jamovi ready for analysis (or take a short-cut and load up the broca.csv file).\nTo perform a one-way related ANOVA in jamovi, open the one-way repeated measures ANOVA dialogue box, as in Figure 13.9, via ANOVA - Repeated Measures ANOVA.\n\n\n\n\n\nFigure 13.9: Repeated measures ANOVA dialogue box in jamovi\n\n\n\n\nThen:\n\nEnter a Repeated Measures Factor Name. This should be a label that you choose to describe the conditions repeated by all participants. For example, to describe the speech, conceptual and syntax tasks completed by all participants a suitable label would be ‘Task’. Note that this new factor name represents the independent variable in the analysis.\nAdd a third level in the Repeated Measures Factors text box, as there are three levels representing the three tasks: speech, conceptual and syntax. Change the labels of the levels accordingly.\nThen move each of the levels variables across to the Repeated Measures Cells text box.\nFinally, under the Assumption Checks option, tick the “Sphericity checks” text box.\n\njamovi output for a one-way repeated measures ANOVA is produced as shown in Figure 13.10 to Figure 13.13. The first output we should look at is Mauchly’s Test of Sphericity, which tests the hypothesis that the variances of the differences between the conditions are equal (meaning that the spread of difference scores between the study conditions is approximately the same). In Figure 13.10 Mauchly’s test significance level is \\(p = .720\\). If Mauchly’s test is non-significant (i.e. p < .05, as is the case in this analysis) then it is reasonable to conclude that the variances of the differences are not significantly different (i.e. they are roughly equal and sphericity can be assumed.).\n\n\n\n\n\nFigure 13.10: One-way repeated measures ANOVA output - Mauchly’s Test of Sphericity\n\n\n\n\nIf, on the other hand, Mauchly’s test had been significant (p < .05) then we would conclude that there are significant differences between the variance of the differences, and the requirement of sphericity has not been met. In this case, we should apply a correction to the F-value obtained in the one-way related ANOVA analysis:\n\nIf the Greenhouse-Geisser value in the “Tests of Sphericity” table is > .75 then you should use the Huynh-Feldt correction\nBut if the Greenhouse-Geisser value is < .75, then you should use the Greenhouse-Geisser correction.\n\nBoth these corrected F-values can be specified in the Sphericity Corrections check boxes under the Assumption Checks options, and the corrected F-values are then shown in the results table, as in Figure 13.11.\n\n\n\n\n\nFigure 13.11: One-way repeated measures ANOVA output - Tests of Within-Subjects Effects\n\n\n\n\nIn our analysis, we saw that the significance of Mauchly’s Test of Sphericity was p = .720 (i.e. p > 0.05). So, this means we can assume that the requirement of sphericity has been met so no correction to the F-value is needed. Therefore, we can use the ‘None’ Sphericity Correction output values for the repeated measure ‘Task’: \\(F = 6.93\\), \\(df = 2\\), \\(p = .013\\), and we can conclude that the number of tests successfully completed on each language task did vary significantly depending on whether the task was speech, comprehension or syntax based (\\(F(2, 10) = 6.93\\), \\(p = .013\\)).\nPost-hoc tests can also be specified in jamovi for repeated measures ANOVA in the same way as for independent ANOVA. The results are shown in Figure 13.12. These indicate that there is a significant difference between Speech and Syntax, but not between other levels.\n\n\n\n\n\nFigure 13.12: Post-hoc tests in repeated measures ANOVA in jamovi\n\n\n\n\nDescriptive statistics (marginal means) can be reviewed to help interpret the results, produced in the jamovi output as in Figure 13.13. Comparison of the mean number of trials successfully completed by participants shows that Broca’s Aphasics perform reasonably well on speech production (mean = 7.17) and language comprehension (mean = 6.17) tasks. However, their performance was considerably worse on the syntax task (mean = 4.33), with a significant difference in post-hoc tests between Speech and Syntax task performance.\n\n\n\n\n\nFigure 13.13: One-way repeated measures ANOVA output - Descriptive Statistics"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.8 Friedman無母數重覆量數變異數分析",
    "text": "13.8 Friedman無母數重覆量數變異數分析\nThe Friedman test is a non-parametric version of a repeated measures ANOVA and can be used instead of the Kruskall-Wallis test when testing for differences between three or more groups where the same participants are in each group, or each participant is closely matched with participants in other conditions. If the dependent variable is ordinal, or if the assumption of normality is not met, then the Friedman test can be used.\nAs with the Kruskall-Wallis test, the underlying mathematics is complicated, and won’t be presented here. For the purpose of this book, it is sufficient to note that jamovi calculates the tie-corrected version of the Friedman test, and in Figure 13.14 there is an example using the Broca’s Aphasia data we have already looked at.\n\n\n\n\n\nFigure 13.14: The ‘Repeated Measures ANOVA (Non-parametric)’ dialogue box and results in jamovi\n\n\n\n\nIt’s pretty straightforward to run a Friedman test in jamovi. Just select Analyses - ANOVA - Repeated Measures ANOVA (Non-parametric), as in Figure 13.14. Then highlight and transfer the names of the repeated measures variables you wish to compare (Speech, Conceptual, Syntax) into the ‘Measures:’ text box. To produce descriptive statistics (means and medians) for the three repeated measures variables, click on the Descriptives button\nThe jamovi results show descriptive statistics, chi-square value, degrees of freedom, and the p-value (Figure 13.14). Since the p-value is less than the level conventionally used to determine significance (p < .05), we can conclude that Broca’s Aphasics perform reasonably well on speech production (median = 7.5) and language comprehension (median = 6.5) tasks. However, their performance was considerably worse on the syntax task (median = 4.5), with a significant difference in post-hoc tests between Speech and Syntax task performance."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.10 本章小結",
    "text": "13.10 本章小結\n這一章份量不少，但是有一些細節我並未提到18。最明顯的是在此並未討論處理不只一個分組變項的資料，我們在下一章 Chapter 14 將學習其中一部分。本章的學習重點有：\n\n理解變異數分析的運作原理 以及使用jamovi完成變異數分析\n學習如何計算變異數分析的效果量\n多重比較與事後檢定\n單因子變異數分析的執行條件\n同質性檢核 以及 校正異質性的分析結果\n常態性檢核以及排除非常態性的分析結果\n單因子重覆量數變異數分析 以及其無母數版本單因子重覆量數變異數分析\n\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific American, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et al (Ed.), Contributions to probability and statistics: Essays in honor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance: Fixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An alternative approach. Biometrika, 38, 330–336."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-1-balanced-designs-no-interactions",
    "href": "14-Factorial-ANOVA.html#factorial-anova-1-balanced-designs-no-interactions",
    "title": "14  多因子變異數分析",
    "section": "14.1 Factorial ANOVA 1: balanced designs, no interactions",
    "text": "14.1 Factorial ANOVA 1: balanced designs, no interactions\nWhen we discussed analysis of variance in Chapter 13, we assumed a fairly simple experimental design. Each person is in one of several groups and we want to know whether these groups have different mean scores on some outcome variable. In this section, I’ll discuss a broader class of experimental designs known as factorial designs, in which we have more than one grouping variable. I gave one example of how this kind of design might arise above. Another example appears in Chapter 13 in which we were looking at the effect of different drugs on the mood.gain experienced by each person. In that chapter we did find a significant effect of drug, but at the end of the chapter we also ran an analysis to see if there was an effect of therapy. We didn’t find one, but there’s something a bit worrying about trying to run two separate analyses trying to predict the same outcome. Maybe there actually is an effect of therapy on mood gain, but we couldn’t find it because it was being “hidden” by the effect of drug? In other words, we’re going to want to run a single analysis that includes both drug and therapy as predictors. For this analysis each person is cross-classified by the drug they were given (a factor with 3 levels) and what therapy they received (a factor with 2 levels). We refer to this as a \\(3 \\times 2\\) factorial design.\nIf we cross-tabulate drug by therapy, using the ‘Frequencies’ - ‘Contingency Tables’ analysis in jamovi (see Section 6.1), we get the table shown in Figure 14.1.\n\n\n\n\n\nFigure 14.1: jamovi contingency table of drug by therapy\n\n\n\n\nAs you can see, not only do we have participants corresponding to all possible combinations of the two factors, indicating that our design is completely crossed, it turns out that there are an equal number of people in each group. In other words, we have a balanced design. In this section I’ll talk about how to analyse data from balanced designs, since this is the simplest case. The story for unbalanced designs is quite tedious, so we’ll put it to one side for the moment.\n\n14.1.1 What hypotheses are we testing?\nLike one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about population means. So a sensible place to start would be to be explicit about what our hypotheses actually are. However, before we can even get to that point, it’s really useful to have some clean and simple notation to describe the population means. Because of the fact that observations are cross-classified in terms of two different factors, there are quite a lot of different means that one might be interested in. To see this, let’s start by thinking about all the different sample means that we can calculate for this kind of design. Firstly, there’s the obvious idea that we might be interested in this list of group means (Table 14.1).\n\n\n\n\nTable 14.1:  Group means for drug and therapy groups in the clinicaltrial.csv data \n\ndrugtherapymood.gain\n\nplacebono.therapy0.300000\n\nanxifreeno.therapy0.400000\n\njoyzepamno.therapy1.466667\n\nplaceboCBT0.600000\n\nanxifreeCBT1.033333\n\njoyzepamCBT1.500000\n\n\n\n\n\nNow, the next Table (Table 14.2) shows a list of the group means for all possible combinations of the two factors (e.g., people who received the placebo and no therapy, people who received the placebo while getting CBT, etc.). It is helpful to organise all these numbers, plus the marginal and grand means, into a single table which looks like this:\n\n\n\n\nTable 14.2:  Group and total means for drug and therapy groups in the clintrial.csv data \n\nno therapyCBTtotal\n\nplacebo0.300.600.45\n\nanxifree0.401.030.72\n\njoyzepam1.471.501.48\n\ntotal0.721.040.88\n\n\n\n\n\nNow, each of these different means is of course a sample statistic. It’s a quantity that pertains to the specific observations that we’ve made during our study. What we want to make inferences about are the corresponding population parameters. That is, the true means as they exist within some broader population. Those population means can also be organised into a similar table, but we’ll need a little mathematical notation to do so (Table 14.3). As usual, I’ll use the symbol \\(\\mu\\) to denote a population mean. However, because there are lots of different means, I’ll need to use subscripts to distinguish between them.\nHere’s how the notation works. Our table is defined in terms of two factors. Each row corresponds to a different level of Factor A (in this case drug), and each column corresponds to a different level of Factor B (in this case therapy). If we let R denote the number of rows in the table, and \\(C\\) denote the number of columns, we can refer to this as an \\(R \\times C\\) factorial ANOVA. In this case \\(R = 3\\) and \\(C = 2\\). We’ll use lowercase letters to refer to specific rows and columns, so \\(\\mu_{rc}\\) refers to the population mean associated with the \\(r\\)-th level of Factor \\(A\\) (i.e. row number \\(r\\)) and the \\(c\\)-th level of Factor B (column number c).1 So the population means are now written like in Table 14.1:\n\n\n\n\nTable 14.3:  Notation for population means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\n\ntotal\n\n\n\n\n\nOkay, what about the remaining entries? For instance, how should we describe the average mood gain across the entire (hypothetical) population of people who might be given Joyzepam in an experiment like this, regardless of whether they were in CBT? We use the “dot” notation to express this. In the case of Joyzepam, notice that we’re talking about the mean associated with the third row in the table. That is, we’re averaging across two cell means (i.e., \\(\\mu_{31}\\) and \\(\\mu_{32}\\)). The result of this averaging is referred to as a marginal mean, and would be denoted \\(\\mu_3.\\) in this case. The marginal mean for CBT corresponds to the population mean associated with the second column in the table, so we use the notation because it is the mean obtained by averaging (marginalising2) over both. So our full table of population means can be written down like in Table 14.4.\n\n\n\n\nTable 14.4:  Notation for population and total means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\\( \\mu_{1.} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\\( \\mu_{2.} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\\( \\mu_{3.} \\)\n\ntotal\\( \\mu_{.1} \\)\\( \\mu_{.2} \\)\\( \\mu_{..} \\)\n\n\n\n\n\nNow that we have this notation, it is straightforward to formulate and express some hypotheses. Let’s suppose that the goal is to find out two things. First, does the choice of drug have any effect on mood? And second, does CBT have any effect on mood? These aren’t the only hypotheses that we could formulate of course, and we’ll see a really important example of a different kind of hypothesis in the section Factorial ANOVA 2: balanced designs, interactions allowed, but these are the two simplest hypotheses to test, and so we’ll start there. Consider the first test. If the drug has no effect then we would expect all of the row means to be identical, right? So that’s our null hypothesis. On the other hand, if the drug does matter then we should expect these row means to be different. Formally, we write down our null and alternative hypotheses in terms of the equality of marginal means:\n\\[\\text{Null hypothesis, } H_0 \\text{: row means are the same, i.e., } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{Alternative hypothesis, } H_1 \\text{: at least one row mean is different}\\]\nIt’s worth noting that these are exactly the same statistical hypotheses that we formed when we ran a one-way ANOVA on these data in Chapter 13. Back then I used the notation \\(\\mu \\times {P}\\) to refer to the mean mood gain for the placebo group, with \\(\\mu{A}\\) and \\(\\mu \\times {J}\\) corresponding to the group means for the two drugs, and the null hypothesis was \\(\\mu{P} = \\mu{A} = \\mu{J}\\) . So we’re actually talking about the same hypothesis, it’s just that the more complicated ANOVA requires more careful notation due to the presence of multiple grouping variables, so we’re now referring to this hypothesis as \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\) . However, as we’ll see shortly, although the hypothesis is identical the test of that hypothesis is subtly different due to the fact that we’re now acknowledging the existence of the second grouping variable.\nSpeaking of the other grouping variable, you won’t be surprised to discover that our second hypothesis test is formulated the same way. However, since we’re talking about the psychological therapy rather than drugs our null hypothesis now corresponds to the equality of the column means:\n\\[\\text{Null hypothesis, } H_0 \\text{: column means are the same, i.e., } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{Alternative hypothesis, } H_1 \\text{: column means are different, i.e., } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n14.1.2 Running the analysis in jamovi\nThe null and alternative hypotheses that I described in the last section should seem awfully familiar. They’re basically the same as the hypotheses that we were testing in our simpler oneway ANOVAs in Chapter 13. So you’re probably expecting that the hypothesis tests that are used in factorial ANOVA will be essentially the same as the F-test from Chapter 13. You’re expecting to see references to sums of squares (SS), mean squares (MS), degrees of freedom (df), and finally an F-statistic that we can convert into a p-value, right? Well, you’re absolutely and completely right. So much so that I’m going to depart from my usual approach. Throughout this book, I’ve generally taken the approach of describing the logic (and to an extent the mathematics) that underpins a particular analysis first and only then introducing the analysis in jamovi. This time I’m going to do it the other way around and show you how to do it in jamovi first. The reason for doing this is that I want to highlight the similarities between the simple one-way ANOVA tool that we discussed in Chapter 13, and the more complicated approach that we’re going to use in this chapter.\nIf the data you’re trying to analyse correspond to a balanced factorial design then running your analysis of variance is easy. To see how easy it is, let’s start by reproducing the original analysis from Chapter 13. In case you’ve forgotten, for that analysis we were using only a single factor (i.e., drug) to predict our outcome variable (i.e., mood.gain), and we got the results shown in Figure 14.2.\n\n\n\n\n\nFigure 14.2: jamovi one way anova of mood.gain by drug\n\n\n\n\nNow, suppose I’m also curious to find out if therapy has a relationship to mood.gain. In light of what we’ve seen from our discussion of multiple regression in Chapter 12, you probably won’t be surprised that all we have to do is add therapy as a second ‘Fixed Factor’ in the analysis, see Figure 14.3.\n\n\n\n\n\nFigure 14.3: jamovi two way anova of mood.gain by drug and therapy\n\n\n\n\nThis output is pretty simple to read too. The first row of the table reports a between-group sum of squares (SS) value associated with the drug factor, along with a corresponding between-group df value. It also calculates a mean square value (MS), an F-statistic and a p-value. is also a row corresponding to the therapy factor and a row corresponding to the residuals (i.e., the within groups variation).\nNot only are all of the individual quantities pretty familiar, the relationships between these different quantities has remained unchanged, just like we saw with the original one-way ANOVA. Note that the mean square value is calculated by dividing \\(SS\\) by the corresponding \\(df\\). That is, it’s still true that\n\\[MS=\\frac{SS}{df}\\]\nregardless of whether we’re talking about drug, therapy or the residuals. To see this, let’s not worry about how the sums of squares values are calculated. Instead, let’s take it on faith that jamovi has calculated the \\(SS\\) values correctly, and try to verify that all the rest of the numbers make sense. First, note that for the drug factor, we divide \\(3.45\\) by \\(2\\) and end up with a mean square value of \\(1.73\\). For the therapy factor, there’s only 1 degree of freedom, so our calculations are even simpler: dividing \\(0.47\\) (the \\(SS\\) value) by 1 gives us an answer of \\(0.47\\) (the \\(MS\\) value).\nTurning to the F statistics and the p values, notice that we have two of each; one corresponding to the drug factor and the other corresponding to the therapy factor. Regardless of which one we’re talking about, the F statistic is calculated by dividing the mean square value associated with the factor by the mean square value associated with the residuals. If we use “A” as shorthand notation to refer to the first factor (factor A; in this case drug) and “R” as shorthand notation to refer to the residuals, then the F statistic associated with factor A is denoted FA, and is calculated as follows:\n\\[F_A=\\frac{MS_A}{MS_R}\\]\nand an equivalent formula exists for factor B (i.e., therapy). Note that this use of “R” to refer to residuals is a bit awkward, since we also used the letter R to refer to the number of rows in the table, but I’m only going to use “R” to mean residuals in the context of SSR and MSR, so hopefully this shouldn’t be confusing. Anyway, to apply this formula to the drugs factor we take the mean square of 1.73 and divide it by the residual mean square value of \\(0.07\\), which gives us an F-statistic of 26.15. The corresponding calculation for the therapy variable would be to divide \\(0.47\\) by \\(0.07\\) which gives \\(7.08\\) as the F-statistic. Not surprisingly, of course, these are the same values that jamovi has reported in the ANOVA table above.\nAlso in the ANOVA table is the calculation of the p values. Once again, there is nothing new here. For each of our two factors what we’re trying to do is test the null hypothesis that there is no relationship between the factor and the outcome variable (I’ll be a bit more precise about this later on). To that end, we’ve (apparently) followed a similar strategy to what we did in the one way ANOVA and have calculated an F-statistic for each of these hypotheses. To convert these to p values, all we need to do is note that the sampling distribution for the F statistic under the null hypothesis (that the factor in question is irrelevant) is an F distribution. Also note that the two degrees of freedom values are those corresponding to the factor and those corresponding to the residuals. For the drug factor we’re talking about an F distribution with 2 and 14 degrees of freedom (I’ll discuss degrees of freedom in more detail later). In contrast, for the therapy factor the sampling distribution is F with 1 and 14 degrees of freedom.\nAt this point, I hope you can see that the ANOVA table for this more complicated factorial analysis should be read in much the same way as the ANOVA table for the simpler one way analysis. In short, it’s telling us that the factorial ANOVA for our \\(3 \\times 2\\) design found a significant effect of drug (\\(F_{2,14} = 26.15, p < .001\\)) as well as a significant effect of therapy (\\(F_{1,14} = 7.08, p = .02\\)). Or, to use the more technically correct terminology, we would say that there are two main effects of drug and therapy. At the moment, it probably seems a bit redundant to refer to these as “main” effects, but it actually does make sense. Later on, we’re going to want to talk about the possibility of “interactions” between the two factors, and so we generally make a distinction between main effects and interaction effects.\n\n\n14.1.3 How are the sum of squares calculated?\nIn the previous section I had two goals. Firstly, to show you that the jamovi method needed to do factorial ANOVA is pretty much the same as what we used for a one way ANOVA. The only difference is the addition of a second factor. Secondly, I wanted to show you what the ANOVA table looks like in this case, so that you can see from the outset that the basic logic and structure behind factorial ANOVA is the same as that which underpins one way ANOVA. Try to hold onto that feeling. It’s genuinely true, insofar as factorial ANOVA is built in more or less the same way as the simpler one-way ANOVA model. It’s just that this feeling of familiarity starts to evaporate once you start digging into the details. Traditionally, this comforting sensation is replaced by an urge to hurl abuse at the authors of statistics textbooks.\nOkay, let’s start by looking at some of those details. The explanation that I gave in the last section illustrates the fact that the hypothesis tests for the main effects (of drug and therapy in this case) are F-tests, but what it doesn’t do is show you how the sum of squares (SS) values are calculated. Nor does it tell you explicitly how to calculate degrees of freedom (df values) though that’s a simple thing by comparison. Let’s assume for now that we have only two predictor variables, Factor A and Factor B. If we use Y to refer to the outcome variable, then we would use Yrci to refer to the outcome associated with the i-th member of group rc (i.e., level/row r for Factor A and level/column c for Factor B). Thus, if we use \\(\\bar{Y}\\) to refer to a sample mean, we can use the same notation as before to refer to group means, marginal means and grand means. That is, \\(\\bar{Y}_{rc}\\) is the sample mean associated with the rth level of Factor A and the cth level of Factor: \\(\\bar{Y}_{r.}\\) would be the marginal mean for the rth level of Factor A, \\(\\bar{Y}_{.c}\\) would be the marginal mean for the cth level of Factor B, and \\(\\bar{Y}_{..}\\) is the grand mean. In other words, our sample means can be organised into the same table as the population means. For our clinical trial data, that table is shown in Table 14.5.\n\n\n\n\nTable 14.5:  Notation for sample means for the clinical trial data \n\nno therapyCBTtotal\n\nplacebo\\( \\bar{Y}_{11} \\)\\( \\bar{Y}_{12} \\)\\( \\bar{Y}_{1.} \\)\n\nanxifree\\( \\bar{Y}_{21} \\)\\( \\bar{Y}_{22} \\)\\( \\bar{Y}_{2.} \\)\n\njoyzepam\\( \\bar{Y}_{31} \\)\\( \\bar{Y}_{32} \\)\\( \\bar{Y}_{3.} \\)\n\ntotal\\( \\bar{Y}_{.1} \\)\\( \\bar{Y}_{.2} \\)\\( \\bar{Y}_{..} \\)\n\n\n\n\n\nAnd if we look at the sample means that I showed earlier, we have \\(\\bar{Y}_{11} = 0.30\\), \\(\\bar{Y}_{12} = 0.60\\) etc. In our clinical trial example, the drugs factor has 3 levels and the therapy factor has 2 levels, and so what we’re trying to run is a \\(3 \\times 2\\) factorial ANOVA. However, we’ll be a little more general and say that Factor A (the row factor) has R levels and Factor B (the column factor) has C levels, and so what we’re running here is an \\(R \\times C\\) factorial ANOVA.\n[Additional technical detail 3]\n\n\n14.1.4 What are our degrees of freedom?\nThe degrees of freedom are calculated in much the same way as for one-way ANOVA. For any given factor, the degrees of freedom is equal to the number of levels minus 1 (i.e., \\(R - 1\\) for the row variable Factor A, and \\(C - 1\\) for the column variable Factor B). So, for the drugs factor we obtain \\(df = 2\\), and for the therapy factor we obtain \\(df = 1\\). Later on, when we discuss the interpretation of ANOVA as a regression model (see Section 14.6), I’ll give a clearer statement of how we arrive at this number. But for the moment we can use the simple definition of degrees of freedom, namely that the degrees of freedom equals the number of quantities that are observed, minus the number of constraints. So, for the drugs factor, we observe 3 separate group means, but these are constrained by 1 grand mean, and therefore the degrees of freedom is 2. For the residuals, the logic is similar, but not quite the same. The total number of observations in our experiment is 18. The constraints correspond to 1 grand mean, the 2 additional group means that the drug factor introduces, and the 1 additional group mean that the the therapy factor introduces, and so our degrees of freedom is 14. As a formula, this is \\(N - 1 - (R - 1) - (C - 1)\\), which simplifies to \\(N - R - C + 1\\).\n\n\n14.1.5 Factorial ANOVA versus one-way ANOVAs\nNow that we’ve seen how a factorial ANOVA works, it’s worth taking a moment to compare it to the results of the one way analyses, because this will give us a really good sense of why it’s a good idea to run the factorial ANOVA. In Chapter 13 I ran a one-way ANOVA that looked to see if there are any differences between drugs, and a second one-way ANOVA to see if there were any differences between therapies. As we saw in the section Section 14.1.1, the null and alternative hypotheses tested by the one-way ANOVAs are in fact identical to the hypotheses tested by the factorial ANOVA. Looking even more carefully at the ANOVA tables, we can see that the sum of squares associated with the factors are identical in the two different analyses (3.45 for drug and 0.92 for therapy), as are the degrees of freedom (2 for drug, 1 for therapy). But they don’t give the same answers! Most notably, when we ran the one-way ANOVA for therapy in Section 13.9 we didn’t find a significant effect (the p-value was .21). However, when we look at the main effect of therapy within the context of the two-way ANOVA, we do get a significant effect (p = .019). The two analyses are clearly not the same.\nWhy does that happen? The answer lies in understanding how the residuals are calculated. Recall that the whole idea behind an F-test is to compare the variability that can be attributed to a particular factor with the variability that cannot be accounted for (the residuals). If you run a one-way ANOVA for therapy, and therefore ignore the effect of drug, the ANOVA will end up dumping all of the drug-induced variability into the residuals! This has the effect of making the data look more noisy than they really are, and the effect of therapy which is correctly found to be significant in the two-way ANOVA now becomes non-significant. If we ignore something that actually matters (e.g., drug) when trying to assess the contribution of something else (e.g., therapy) then our analysis will be distorted. Of course, it’s perfectly okay to ignore variables that are genuinely irrelevant to the phenomenon of interest. If we had recorded the colour of the walls, and that turned out to be a non-significant factor in a three-way ANOVA, it would be perfectly okay to disregard it and just report the simpler two-way ANOVA that doesn’t include this irrelevant factor. What you shouldn’t do is drop variables that actually make a difference!\n\n\n14.1.6 What kinds of outcomes does this analysis capture?\nThe ANOVA model that we’ve been talking about so far covers a range of different patterns that we might observe in our data. For instance, in a two-way ANOVA design there are four possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter, and (d) neither A nor B matters. An example of each of these four possibilities is plotted in Figure 14.4."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-2-balanced-designs-interactions-allowed",
    "href": "14-Factorial-ANOVA.html#factorial-anova-2-balanced-designs-interactions-allowed",
    "title": "14  多因子變異數分析",
    "section": "14.2 Factorial ANOVA 2: balanced designs, interactions allowed",
    "text": "14.2 Factorial ANOVA 2: balanced designs, interactions allowed\nThe four patterns of data shown in Figure 14.4 are all quite realistic. There are a great many data sets that produce exactly those patterns. However, they are not the whole story and the ANOVA model that we have been talking about up to this point is not sufficient to fully account for a table of group means. Why not? Well, so far we have the ability to talk about the idea that drugs can influence mood, and therapy can influence mood, but no way of talking about the possibility of an interaction between the two. An interaction between \\(A\\) and \\(B\\) is said to occur whenever the effect of Factor \\(A\\) is different, depending on which level of Factor \\(B\\) we’re talking about. Several examples of an interaction effect with the context of a \\(2 \\times 2\\) ANOVA are shown in Figure 14.5. To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed by quite different physiological mechanisms. One consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed in the previous section does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means. In jamovi this is done via the ANOVA ‘Estimated Marginal Means’ option - just move drug and therapy across into the ‘Marginal Means’ box under ‘Term 1’. This should look something like Figure 14.6. Our main concern relates to the fact that the two lines aren’t parallel. The effect of CBT (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However, when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this effect real, or is this just random variation due to chance? Our original ANOVA cannot answer this question, because we make no allowances for the idea that interactions even exist! In this section, we’ll fix this problem.\n\n\n\n\n\nFigure 14.4: The four different outcomes for a \\(2 \\times 2\\) ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor\n\n\n\n\n\n\n\n\n\nFigure 14.5: Qualitatively different interactions for a \\(2 \\times 2\\) ANOVA\n\n\n\n\n\n\n\n\n\nFigure 14.6: jamovi screen showing how to generate a descriptive interaction plot in ANOVA using the clinical trial data\n\n\n\n\n\n14.2.1 What exactly is an interaction effect?\nThe key idea that we’re going to introduce in this section is that of an interaction effect. In the ANOVA model we have looked at so far there are only two factors involved in our model (i.e., drug and therapy). But when we add an interaction we add a new component to the model: the combination of drug and therapy. Intuitively, the idea behind an interaction effect is fairly simple. It just means that the effect of Factor A is different, depending on which level of Factor B we’re talking about. But what does that actually mean in terms of our data? The plot in Figure 14.5 depicts several different patterns that, although quite different to each other, would all count as an interaction effect. So it’s not entirely straightforward to translate this qualitative idea into something mathematical that a statistician can work with.\n[Additional technical detail 4]\n\n\n14.2.2 Degrees of freedom for the interaction\nCalculating the degrees of freedom for the interaction is, once again, slightly trickier than the corresponding calculation for the main effects. To start with, let’s think about the ANOVA model as a whole. Once we include interaction effects in the model we’re allowing every single group to have a unique mean, \\(mu_{rc}\\). For an \\(R \\times C\\) factorial ANOVA, this means that there are \\(R \\times C\\) quantities of interest in the model and only the one constraint: all of the group means need to average out to the grand mean. So the model as a whole needs to have (\\(R \\times C\\)) - 1 degrees of freedom. But the main effect of Factor A has \\(R - 1\\) degrees of freedom, and the main effect of Factor B has \\(C - 1\\) degrees of freedom. This means that the degrees of freedom associated with the interaction is\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\nwhich is just the product of the degrees of freedom associated with the row factor and the column factor.\nWhat about the residual degrees of freedom? Because we’ve added interaction terms which absorb some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically, note that if the model with interaction has a total of \\((R \\times C) - 1\\), and there are \\(N\\) observations in your data set that are constrained to satisfy 1 grand mean, your residual degrees of freedom now become \\(N - (R \\times C) - 1 + 1\\), or just \\(N - (R \\times C)\\).\n\n\n14.2.3 Running the ANOVA in jamovi\nAdding interaction terms to the ANOVA model in jamovi is straightforward. In fact it is more than straightforward because it is the default option for ANOVA. This means that when you specify an ANOVA with two factors, e.g. drug and therapy then the interaction component - drug \\(\\times\\) therapy - is added automatically to the model5. When we run the ANOVA with the interaction term included, then we get the results shown in Figure 14.7.\n\n\n\n\n\nFigure 14.7: Results for the full factorial model, including the interaction component drug \\(\\times\\) therapy\n\n\n\n\nAs it turns out, while we do have a significant main effect of drug (\\(F_{2,12} = 31.7, p < .001\\)) and therapy type (\\(F_{1,12} = 8.6, p = .013\\)), there is no significant interaction between the two (\\(F_{2,12} = 2.5, p = 0.125\\)).\n\n\n14.2.4 Interpreting the results\nThere’s a couple of very important things to consider when interpreting the results of factorial ANOVA. First, there’s the same issue that we had with one-way ANOVA, which is that if you obtain a significant main effect of (say) drug, it doesn’t tell you anything about which drugs are different to one another. To find that out, you need to run additional analyses. We’ll talk about some analyses that you can run in later Sections: Different ways to specify contrasts and Post hoc tests. The same is true for interaction effects. Knowing that there’s a significant interaction doesn’t tell you anything about what kind of interaction exists. Again, you’ll need to run additional analyses.\nSecondly, there’s a very peculiar interpretation issue that arises when you obtain a significant interaction effect but no corresponding main effect. This happens sometimes. For instance, in the crossover interaction shown in Figure 14.5 a, this is exactly what you’d find. In this case, neither of the main effects would be significant, but the interaction effect would be. This is a difficult situation to interpret, and people often get a bit confused about it. The general advice that statisticians like to give in this situation is that you shouldn’t pay much attention to the main effects when an interaction is present. The reason they say this is that, although the tests of the main effects are perfectly valid from a mathematical point of view, when there is a significant interaction effect the main effects rarely test interesting hypotheses. Recall from Section 14.1.1 that the null hypothesis for a main effect is that the marginal means are equal to each other, and that a marginal mean is formed by averaging across several different groups. But if you have a significant interaction effect then you know that the groups that comprise the marginal mean aren’t homogeneous, so it’s not really obvious why you would even care about those marginal means.\nHere’s what I mean. Again, let’s stick with a clinical example. Suppose that we had a \\(2 \\times 2\\) design comparing two different treatments for phobias (e.g., systematic desensitisation vs flooding), and two different anxiety reducing drugs (e.g., Anxifree vs Joyzepam). Now, suppose what we found was that Anxifree had no effect when desensitisation was the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty effective for the other treatment. This is a classic crossover interaction, and what we’d find when running the ANOVA is that there is no main effect of drug, but a significant interaction. Now, what does it actually mean to say that there’s no main effect? Well, it means that if we average over the two different psychological treatments, then the average effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When treating someone for phobias it is never the case that a person can be treated using an “average” of flooding and desensitisation. That doesn’t make a lot of sense. You either get one or the other. For one treatment one drug is effective, and for the other treatment the other drug is effective. The interaction is the important thing and the main effect is kind of irrelevant.\nThis sort of thing happens a lot. The main effect are tests of marginal means, and when an interaction is present we often find ourselves not being terribly interested in marginal means because they imply averaging over things that the interaction tells us shouldn’t be averaged! Of course, it’s not always the case that a main effect is meaningless when an interaction is present. Often you can get a big main effect and a very small interaction, in which case you can still say things like “drug A is generally more effective than drug B” (because there was a big effect of drug), but you’d need to modify it a bit by adding that “the difference in effectiveness was different for different psychological treatments”. In any case, the main point here is that whenever you get a significant interaction you should stop and think about what the main effect actually means in this context. Don’t automatically assume that the main effect is interesting."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#effect-size",
    "href": "14-Factorial-ANOVA.html#effect-size",
    "title": "14  多因子變異數分析",
    "section": "14.3 Effect size",
    "text": "14.3 Effect size\nThe effect size calculation for a factorial ANOVA is pretty similar to those used in one way ANOVA (see Effect size section). Specifically, we can use \\(\\eta^2\\) (eta-squared) as a simple way to measure how big the overall effect is for any particular term. As before, \\(\\eta^2\\) is defined by dividing the sum of squares associated with that term by the total sum of squares. For instance, to determine the size of the main effect of Factor A, we would use the following formula:\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\nAs before, this can be interpreted in much the same way as \\(R^2\\) in regression.6 It tells you the proportion of variance in the outcome variable that can be accounted for by the main effect of Factor A. It is therefore a number that ranges from 0 (no effect at all) to 1 (accounts for all of the variability in the outcome). Moreover, the sum of all the \\(\\eta^2\\) values, taken across all the terms in the model, will sum to the the total \\(R^2\\) for the ANOVA model. If, for instance, the ANOVA model fits perfectly (i.e., there is no within-groups variability at all!), the \\(\\eta^2\\) values will sum to 1. Of course, that rarely if ever happens in real life.\nHowever, when doing a factorial ANOVA, there is a second measure of effect size that people like to report, known as partial \\(\\eta^2\\). The idea behind partial \\(\\eta^2\\) (which is sometimes denoted \\(p^{\\eta^2}\\) or \\(\\eta_p^2\\)) is that, when measuring the effect size for a particular term (say, the main effect of Factor A), you want to deliberately ignore the other effects in the model (e.g., the main effect of Factor B). That is, you would pretend that the effect of all these other terms is zero, and then calculate what the \\(\\eta^2\\) value would have been. This is actually pretty easy to calculate. All you have to do is remove the sum of squares associated with the other terms from the denominator. In other words, if you want the partial \\(\\eta^2\\) for the main effect of Factor A, the denominator is just the sum of the SS values for Factor A and the residuals\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\nThis will always give you a larger number than \\(\\eta^2\\), which the cynic in me suspects accounts for the popularity of partial \\(\\eta^2\\). And once again you get a number between 0 and 1, where 0 represents no effect. However, it’s slightly trickier to interpret what a large partial \\(\\eta^2\\) value means. In particular, you can’t actually compare the partial \\(\\eta^2\\) values across terms! Suppose, for instance, there is no within-groups variability at all: if so, \\(SS_R = 0\\). What that means is that every term has a partial \\(\\eta^2\\) value of 1. But that doesn’t mean that all terms in your model are equally important, or indeed that they are equally large. All it mean is that all terms in your model have effect sizes that are large relative to the residual variation. It is not comparable across terms.\nTo see what I mean by this, it’s useful to see a concrete example. First, let’s have a look at the effect sizes for the original ANOVA (Table 14.6) without the interaction term, from Figure 14.3.\n\n\n\n\nTable 14.6:  Effect sizes when the interaction term is not included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.79\n\ntherapy0.100.34\n\n\n\n\n\nLooking at the \\(\\eta^2\\) values first, we see that drug accounts for 71% of the variance (i.e. \\(\\eta^2 = 0.71\\)) in mood.gain, whereas therapy only accounts for 10%. This leaves a total of 19% of the variation unaccounted for (i.e., the residuals constitute 19% of the variation in the outcome). Overall, this implies that we have a very large effect7 of drug and a modest effect of therapy.\nNow let’s look at the partial \\(\\eta^2\\) values, shown in Figure 14.3. Because the effect of therapy isn’t all that large, controlling for it doesn’t make much of a difference, so the partial \\(\\eta^2\\) for drug doesn’t increase very much, and we obtain a value of \\(p^{\\eta^2} = 0.79\\). In contrast, because the effect of drug was very large, controlling for it makes a big difference, and so when we calculate the partial \\(\\eta^2\\) for therapy you can see that it rises to \\(p^{\\eta^2} = 0.34\\). The question that we have to ask ourselves is, what do these partial \\(\\eta^2\\) values actually mean? The way I generally interpret the partial \\(\\eta^2\\) for the main effect of Factor A is to interpret it as a statement about a hypothetical experiment in which only Factor A was being varied. So, even though in this experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was varied, and the partial \\(\\eta^2\\) statistic tells you how much of the variance in the outcome variable you would expect to see accounted for in that experiment. However, it should be noted that this interpretation, like many things associated with main effects, doesn’t make a lot of sense when there is a large and significant interaction effect.\nSpeaking of interaction effects, Table 14.7 shows what we get when we calculate the effect sizes for the model that includes the interaction term, as in Figure 14.7. As you can see, the \\(\\eta^2\\) values for the main effects don’t change, but the partial \\(\\eta^2\\) values do:\n\n\n\n\nTable 14.7:  Effect sizes when the interaction term is included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.84\n\ntherapy0.100.42\n\ndrug*therapy0.060.29\n\n\n\n\n\n\n14.3.1 Estimated group means\nIn many situations you will find yourself wanting to report estimates of all the group means based on the results of your ANOVA, as well as confidence intervals associated with them. You can use the ‘Estimated Marginal Means’ option in the jamovi ANOVA analysis to do this, as in Figure 14.8. If the ANOVA that you have run is a saturated model (i.e., contains all possible main effects and all possible interaction effects) then the estimates of the group means are actually identical to the sample means, though the confidence intervals will use a pooled estimate of the standard errors rather than use a separate one for each group.\n\n\n\n\n\nFigure 14.8: jamovi screenshot showing the marginal means for the saturated model, i.e. including the interaction component, with the clinical trial data set\n\n\n\n\nIn the output we see that the estimated mean mood gain for the placebo group with no therapy was \\(0.300\\), with a \\(95\\%\\) confidence interval from \\(0.006\\) to \\(0.594\\). Note that these are not the same confidence intervals that you would get if you calculated them separately for each group, because of the fact that the ANOVA model assumes homogeneity of variance and therefore uses a pooled estimate of the standard deviation.\nWhen the model doesn’t contain the interaction term, then the estimated group means will be different from the sample means. Instead of reporting the sample mean, jamovi will calculate the value of the group means that would be expected on the basis of the marginal means (i.e., assuming no interaction). Using the notation we developed earlier, the estimate reported for µrc, the mean for level r on the (row) Factor A and level c on the (column) Factor B would be \\(\\mu_{..} + \\alpha_r + \\beta_c\\). If there are genuinely no interactions between the two factors, this is actually a better estimate of the population mean than the raw sample mean would be. Removing the interaction term from the model, via the ‘Model’ options in the jamovi ANOVA analysis, provides the marginal means for the analysis shown in Figure 14.9.\n\n\n\n\n\nFigure 14.9: jamovi screenshot showing the marginal means for the unsaturated model, i.e. without the interaction component, with the clinical trial data set"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#assumption-checking",
    "href": "14-Factorial-ANOVA.html#assumption-checking",
    "title": "14  多因子變異數分析",
    "section": "14.4 Assumption checking",
    "text": "14.4 Assumption checking\nAs with one-way ANOVA, the key assumptions of factorial ANOVA are homogeneity of variance (all groups have the same standard deviation), normality of the residuals, and independence of the observations. The first two are things we can check for. The third is something that you need to assess yourself by asking if there are any special relationships between different observations, for example repeated measures where the independent variable is time so there is a relationship between the observations at time one and time two: observations at different time points are from the same people. Additionally, if you aren’t using a saturated model (e.g., if you’ve omitted the interaction terms) then you’re also assuming that the omitted terms aren’t important. Of course, you can check this last one by running an ANOVA with the omitted terms included and see if they’re significant, so that’s pretty easy. What about homogeneity of variance and normality of the residuals? As it turns out, these are pretty easy to check. It’s no different to the checks we did for a one-way ANOVA.\n\n14.4.1 Homogeneity of variance\nAs mentioned in Section 13.6.1 in the last chapter, it’s a good idea to visually inspect a plot of the standard deviations compared across different groups / categories, and also see if the Levene test is consistent with the visual inspection. The theory behind the Levene test was discussed in Section 13.6.1, so I won’t discuss it again. This test expects that you have a saturated model (i.e., including all of the relevant terms), because the test is primarily concerned with the within-group variance, and it doesn’t really make a lot of sense to calculate this any way other than with respect to the full model. The Levene test can be specified under the ANOVA ‘Assumption Checks’ - ‘Homogeneity Tests’ option in jamovi, with the result shown as in Figure 14.10. The fact that the Levene test is non-significant means that, providing it is consistent with a visual inspection of the plot of standard deviations, we can safely assume that the homogeneity of variance assumption is not violated.\n\n\n14.4.2 Normality of residuals\nAs with one-way ANOVA we can test for the normality of residuals in a straightforward fashion (see Section 13.6.4). Primarily though, it’s generally a good idea to examine the residuals graphically using a QQ plot. See Figure 14.10.\n\n\n\n\n\nFigure 14.10: Checking assumptions in an ANOVA model"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "href": "14-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "title": "14  多因子變異數分析",
    "section": "14.5 共變數分析 (ANCOVA)",
    "text": "14.5 共變數分析 (ANCOVA)\nA variation in ANOVA is when you have an additional continuous variable that you think might be related to the dependent variable. This additional variable can be added to the analysis as a covariate, in the aptly named analysis of covariance (ANCOVA).\nIn ANCOVA the values of the dependent variable are “adjusted” for the influence of the covariate, and then the “adjusted” score means are tested between groups in the usual way. This technique can increase the precision of an experiment, and therefore provide a more “powerful” test of the equality of group means in the dependent variable. How does ANCOVA do this? Well, although the covariate itself is typically not of any experimental interest, adjustment for the covariate can decrease the estimate of experimental error and thus, by reducing error variance, precision is increased. This means that an inappropriate failure to reject the null hypothesis (false negative or type II error) is less likely.\nDespite this advantage, ANCOVA runs the risk of undoing real differences between groups, and this should be avoided. Look at Figure 14.11, for example, which shows a plot of Statistics anxiety against age and shows two distinct groups – students who have either an Arts or Science background or preference. ANCOVA with age as a covariate might lead to the conclusion that statistics anxiety does not differ in the two groups. Would this conclusion be reasonable – probably not because the ages of the two groups do not overlap and analysis of variance has essentially “extrapolated into a region with no data” (Everitt (1996), p. 68).\n\n\n\n\n\nFigure 14.11: Plot of Statistics anxiety against age for two distinct groups\n\n\n\n\nClearly, careful thought needs to be given to an analysis of covariance with distinct groups. This applies to both one-way and factorial designs, as ANCOVA can be used with both.\n\n14.5.1 使用jamovi完成共變數分析\nA health psychologist was interested in the effect of routine cycling and stress on happiness levels, with age as a covariate. You can find the dataset in the file ancova.csv. Open this file in jamovi and then, to undertake an ANCOVA, select Analyses - ANOVA - ANCOVA to open the ANCOVA analysis window (Figure 14.12). Highlight the dependent variable ‘happiness’ and transfer it into the ‘Dependent Variable’ text box. Highlight the independent variables ‘stress’ and ‘commute’ and transfer them into the ‘Fixed Factors’ text box. Highlight the covariate ‘age’ and transfer it into the ‘Covariates’ text box. Then Click on Estimated Marginal Means to bring up the plots and tables options.\n\n\n\n\n\nFigure 14.12: The jamovi ANCOVA analysis window\n\n\n\n\nAn ANCOVA table showing Tests of Between-Subjects Effects is produced in the jamovi results window (Figure 14.13). The F value for the covariate ‘age’ is significant at \\(p = .023\\), suggesting that age is an important predictor of the dependent variable, happiness. When we look at the estimated marginal mean scores (Figure 14.14), adjustments have been made (compared to an analysis without the covariate) because of the inclusion of the covariate ‘age’ in this ANCOVA. A plot (Figure 14.15) is a good way of visualising and interpreting the significant effects.\n\n\n\n\n\nFigure 14.13: jamovi ANCOVA output for happiness as a function of stress and commuting method, with age as a covariate\n\n\n\n\n\n\n\n\n\nFigure 14.14: Table of mean happiness level as a function of stress and commuting method (adjusted for the covariate age) with 95% confidence intervals\n\n\n\n\nThe \\(F\\) value for the main effect ‘stress’ (52.61) has an associated probability of \\(p < .001\\). The \\(F\\) value for the main effect ‘commute’ (42.33) has an associated probability of \\(p < .001\\). Since both of these are less than the probability that is typically used to decide if a statistical result is significant (\\(p < .05\\)) we can conclude that there was a significant main effect of stress (\\(F(1, 15) = 52.61, p < .001\\)) and a significant main effect of commuting method (\\(F(1, 15) = 42.33, p < .001\\)). A significant interaction between stress and commuting method was also found (\\(F(1, 15) = 14.15, p = .002\\)).\nIn Figure 14.15 we can see the adjusted, marginal, mean happiness scores when age is a covariate in an ANCOVA. In this analysis there is a significant interaction effect, whereby people with low stress who cycle to work are happier than people with low stress who drive and people with high stress whether they cycle or drive to work. There is also a significant main effect of stress – people with low stress are happier than those with high stress. And there is also a significant main effect of commuting behaviour – people who cycle are happier, on average, than those who drive to work.\n\n\n\n\n\nFigure 14.15: Plot of mean happiness level as a function of stress and commuting method\n\n\n\n\nOne thing to be aware of is that, if you are thinking of including a covariate in your ANOVA, there is an additional assumption: the relationship between the covariate and the dependent variable should be similar for all levels of the independent variable. This can be checked by adding an interaction term between the covariate and each independent variable in the jamovi Model - Model terms option. If the interaction effect is not significant it can be removed. If it is significant then a different and more advanced statistical technique might be appropriate (which is beyond the scope of this book so you might want to consult a friendly statistician)."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "href": "14-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "title": "14  多因子變異數分析",
    "section": "14.6 變異數分析就是線性模型",
    "text": "14.6 變異數分析就是線性模型\nOne of the most important things to understand about ANOVA and regression is that they’re basically the same thing. On the surface of it, you maybe wouldn’t think this is true. After all, the way that I’ve described them so far suggests that ANOVA is primarily concerned with testing for group differences, and regression is primarily concerned with understanding the correlations between variables. And, as far as it goes that’s perfectly true. But when you look under the hood, so to speak, the underlying mechanics of ANOVA and regression are awfully similar. In fact, if you think about it, you’ve already seen evidence of this. ANOVA and regression both rely heavily on sums of squares (SS), both make use of F tests, and so on. Looking back, it’s hard to escape the feeling that Chapter 12 and Chapter 13 were a bit repetitive.\nThe reason for this is that ANOVA and regression are both kinds of linear models. In the case of regression, this is kind of obvious. The regression equation that we use to define the relationship between predictors and outcomes is the equation for a straight line, so it’s quite obviously a linear model, with the equation\n\\[Y_p=b_0+b_1 X_{1p} +b_2 X_{2p} + \\epsilon_p\\]\nwhere \\(Y_p\\) is the outcome value for the p-th observation (e.g., p-th person), \\(X_{1p}\\) is the value of the first predictor for the p-th observation, \\(X_{2p}\\) is the value of the second predictor for the p-th observation, the \\(b_0\\), \\(b_1\\), and \\(b_2\\) terms are our regression coefficients, and \\(\\epsilon_p\\) is the p-th residual. If we ignore the residuals \\(\\epsilon_p\\) and just focus on the regression line itself, we get the following formula:\n\\[\\hat{Y}_p=b_0+b_1 X_{1p} +b_2 X_{2p} \\]\nwhere \\(\\hat{Y}_p\\) is the value of Y that the regression line predicts for person p, as opposed to the actually-observed value \\(Y_p\\). The thing that isn’t immediately obvious is that we can write ANOVA as a linear model as well. However, it’s actually pretty straightforward to do this. Let’s start with a really simple example, rewriting a \\(2 \\times 2\\) factorial ANOVA as a linear model.\n\n14.6.1 示範資料\nTo make things concrete, let’s suppose that our outcome variable is the grade that a student receives in my class, a ratio-scale variable corresponding to a mark from \\(0%\\) to \\(100%\\). There are two predictor variables of interest: whether or not the student turned up to lectures (the attend variable) and whether or not the student actually read the textbook (the reading variable). We’ll say that attend = 1 if the student attended class, and attend = 0 if they did not. Similarly, we’ll say that reading = 1 if the student read the textbook, and reading = 0 if they did not.\nOkay, so far that’s simple enough. The next thing we need to do is to wrap some maths around this (sorry!). For the purposes of this example, let \\(Y_p\\) denote the grade of the p-th student in the class. This is not quite the same notation that we used earlier in this chapter. Previously, we’ve used the notation \\(Y_{rci}\\) to refer to the i-th person in the r-th group for predictor 1 (the row factor) and the c-th group for predictor 2 (the column factor). This extended notation was really handy for describing how the SS values are calculated, but it’s a pain in the current context, so I’ll switch notation here. Now, the \\(Y_p\\) notation is visually simpler than \\(Y_{rci}\\), but it has the shortcoming that it doesn’t actually keep track of the group memberships! That is, if I told you that \\(Y_{0,0,3} = 35\\), you’d immediately know that we’re talking about a student (the 3rd such student, in fact) who didn’t attend the lectures (i.e., attend = 0) and didn’t read the textbook (i.e. reading = 0), and who ended up failing the class (grade = 35). But if I tell you that \\(Y_p = 35\\), all you know is that the p-th student didn’t get a good grade. We’ve lost some key information here. Of course, it doesn’t take a lot of thought to figure out how to fix this. What we’ll do instead is introduce two new variables \\(X_{1p}\\) and \\(X_{2p}\\) that keep track of this information. In the case of our hypothetical student, we know that \\(X_{1p} = 0\\) (i.e., attend = 0) and \\(X_{2p} = 0\\) (i.e., reading = 0). So the data might look like Table 14.8.\n\n\n\n\nTable 14.8:  Data for grade, attendance and reading the textbook \n\nperson, \\(p\\)grade, \\(Y_p\\)attendance, \\(X_{1p}\\)reading, \\(X_{2p}\\)\n\n19011\n\n28711\n\n37501\n\n46010\n\n53500\n\n65000\n\n76510\n\n87001\n\n\n\n\n\nThis isn’t anything particularly special, of course. It’s exactly the format in which we expect to see our data! See the data file rtfm.csv. We can use the jamovi ‘Descriptives’ analysis to confirm that this data set corresponds to a balanced design, with 2 observations for each combination of attend and reading. In the same way we can also calculate the mean grade for each combination. This is shown in Figure 14.16. Looking at the mean scores, one gets the strong impression that reading the text and attending the class both matter a lot.\n\n\n\n\n\nFigure 14.16: jamovi descriptives for the rtfm data set\n\n\n\n\n\n\n14.6.2 以迴歸模型處理非連續因子\nOkay, let’s get back to talking about the mathematics. We now have our data expressed in terms of three numeric variables: the continuous variable \\(Y\\) and the two binary variables \\(X_1\\) and \\(X_2\\). What I want you to recognise is that our \\(2 \\times 2\\) factorial ANOVA is exactly equivalent to the regression model\n\\[Y_p=b_0+b_1 X_{1p} + b_2 X_{2p} + \\epsilon_p\\]\nThis is, of course, the exact same equation that I used earlier to describe a two-predictor regression model! The only difference is that \\(X_1\\) and \\(X_2\\) are now binary variables (i.e., values can only be 0 or 1), whereas in a regression analysis we expect that \\(X_1\\) and \\(X_2\\) will be continuous. There’s a couple of ways I could try to convince you of this. One possibility would be to do a lengthy mathematical exercise proving that the two are identical. However, I’m going to go out on a limb and guess that most of the readership of this book will find that annoying rather than helpful. Instead, I’ll explain the basic ideas and then rely on jamovi to show that ANOVA analyses and regression analyses aren’t just similar, they’re identical for all intents and purposes. Let’s start by running this as an ANOVA. To do this, we’ll use the rtfm data set, and Figure 14.17 shows what we get when we run the analysis in jamovi.\n\n\n\n\n\nFigure 14.17: ANOVA of the rtfm.csv data set in jamovi, without the interaction term\n\n\n\n\nSo, by reading the key numbers off the ANOVA table and the mean scores that we presented earlier, we can see that the students obtained a higher grade if they attended class (\\(F_{1,5} = 21.6, p = .0056\\)) and if they read the textbook (\\(F_{1,5} = 52.3, p = .0008\\)). Let’s make a note of those p-values and those \\(F\\) statistics.\nNow let’s think about the same analysis from a linear regression perspective. In the rtfm data set, we have encoded attend and reading as if they were numeric predictors. In this case, this is perfectly acceptable. There really is a sense in which a student who turns up to class (i.e. attend = 1) has in fact done “more attendance” than a student who does not (i.e. attend = 0). So it’s not at all unreasonable to include it as a predictor in a regression model. It’s a little unusual, because the predictor only takes on two possible values, but it doesn’t violate any of the assumptions of linear regression. And it’s easy to interpret. If the regression coefficient for attend is greater than 0 it means that students that attend lectures get higher grades. If it’s less than zero then students attending lectures get lower grades. The same is true for our reading variable.\nWait a second though. Why is this true? It’s something that is intuitively obvious to everyone who has taken a few stats classes and is comfortable with the maths, but it isn’t clear to everyone else at first pass. To see why this is true, it helps to look closely at a few specific students. Let’s start by considering the 6th and 7th students in our data set (i.e. \\(p = 6\\) and \\(p = 7\\)). Neither one has read the textbook, so in both cases we can set reading = 0. Or, to say the same thing in our mathematical notation, we observe \\(X_{2,6} = 0\\) and \\(X_{2,7} = 0\\). However, student number 7 did turn up to lectures (i.e., attend = 1, \\(X_{1,7} = 1\\)) whereas student number 6 did not (i.e., attend = 0, \\(X_{1,6} = 0\\)). Now let’s look at what happens when we insert these numbers into the general formula for our regression line. For student number 6, the regression predicts that\n\\[\n\\begin{split}\n\\hat{Y}_6 & = b_0 + b_1 X_{1,6} + b_2 X_{2,6} \\\\\n& = b_0 + (b_1 \\times 0) + (b_2 \\times 0) \\\\\n& = b_0\n\\end{split}\n\\]\nSo we’re expecting that this student will obtain a grade corresponding to the value of the intercept term \\(b_0\\). What about student 7? This time when we insert the numbers into the formula for the regression line, we obtain the following\n\\[\n\\begin{split}\n\\hat{Y}_7 & = b_0 + b_1 X_{1,7} + b_2 X_{2,7} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 0) \\\\\n& = b_0 + b_1\n\\end{split}\n\\]\nBecause this student attended class, the predicted grade is equal to the intercept term b0 plus the coefficient associated with the attend variable, \\(b_1\\). So, if \\(b_1\\) is greater than zero, we’re expecting that the students who turn up to lectures will get higher grades than those students who don’t. If this coefficient is negative we’re expecting the opposite: students who turn up at class end up performing much worse. In fact, we can push this a little bit further. What about student number 1, who turned up to class (\\(X_{1,1} = 1\\)) and read the textbook (\\(X_{2,1} = 1\\))? If we plug these numbers into the regression we get\n\\[\n\\begin{split}\n\\hat{Y}_1 & = b_0 + b_1 X_{1,1} + b_2 X_{2,1} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 1) \\\\\n& = b_0 + b_1 + b_2\n\\end{split}\n\\]\nSo if we assume that attending class helps you get a good grade (i.e., \\(b1 \\> 0\\)) and if we assume that reading the textbook also helps you get a good grade (i.e., \\(b2 \\> 0\\)), then our expectation is that student 1 will get a grade that that is higher than student 6 and student 7.\nAnd at this point you won’t be at all suprised to learn that the regression model predicts that student 3, who read the book but didn’t attend lectures, will obtain a grade of \\(b_{2} + b_{0}\\). I won’t bore you with yet another regression formula. Instead, what I’ll do is show you is Table 14.9 with the expected grades.\n\n\n\n\nTable 14.9:  Expected grades from the regression model \n\nread textbook\n\nnoyes\n\nattended?no\\( \\beta_0 \\)\\( \\beta_0 + \\beta_2 \\)\n\nyes\\( \\beta_0 + \\beta_1 \\)\\( \\beta_0 + \\beta_1 + \\beta_2 \\)\n\n\n\n\n\nAs you can see, the intercept term \\(b_0\\) acts like a kind of “baseline” grade that you would expect from those students who don’t take the time to attend class or read the textbook. Similarly, \\(b_1\\) represents the boost that you’re expected to get if you come to class, and \\(b_2\\) represents the boost that comes from reading the textbook. In fact, if this were an ANOVA you might very well want to characterise b1 as the main effect of attendance, and \\(b_2\\) as the main effect of reading! In fact, for a simple \\(2 \\times 2\\) ANOVA that’s exactly how it plays out.\nOkay, now that we’re really starting to see why ANOVA and regression are basically the same thing, let’s actually run our regression using the rtfm data and the jamovi regression analysis to convince ourselves that this is really true. Running the regression in the usual way gives the results shown in Figure 14.18.\n\n\n\n\n\nFigure 14.18: Regression analysis of the rtfm.csv data set in jamovi, without the interaction term\n\n\n\n\nThere’s a few interesting things to note here. First, notice that the intercept term is 43.5 which is close to the “group” mean of 42.5 observed for those two students who didn’t read the text or attend class. Second, notice that we have the regression coefficient of \\(b_1 = 18.0\\) for the attendance variable, suggesting that those students that attended class scored 18% higher than those who didn’t. So our expectation would be that those students who turned up to class but didn’t read the textbook would obtain a grade of \\(b_0 + b_1\\), which is equal to \\(43.5 + 18.0 = 61.5\\). You can verify for yourself that the same thing happens when we look at the students that read the textbook.\nActually, we can push a little further in establishing the equivalence of our ANOVA and our regression. Look at the p-values associated with the attend variable and the reading variable in the regression output. They’re identical to the ones we encountered earlier when running the ANOVA. This might seem a little surprising, since the test used when running our regression model calculates a t-statistic and the ANOVA calculates an F-statistic. However, if you can remember all the way back to Chapter 7, I mentioned that there’s a relationship between the t-distribution and the F-distribution. If you have some quantity that is distributed according to a t-distribution with k degrees of freedom and you square it, then this new squared quantity follows an F-distribution whose degrees of freedom are 1 and k. We can check this with respect to the t statistics in our regression model. For the attend variable we get a t value of 4.65. If we square this number we end up with 21.6, which matches the corresponding F statistic in our ANOVA.\nFinally, one last thing you should know. Because jamovi understands the fact that ANOVA and regression are both examples of linear models, it lets you extract the classic ANOVA table from your regression model using the ‘Linear Regression’ - ‘Model Coefficients’ - ‘Omnibus Test’ - ‘ANOVA Test’, and this will give you the table shown in Figure 14.19.\n\n\n\n\n\nFigure 14.19: Omnibus ANOVA Test results from the jamovi regression analysis\n\n\n\n\n\n\n14.6.3 比較因子間平均值的編碼\nHow to encode non binary factors as contrasts\nAt this point, I’ve shown you how we can view a \\(2 \\times 2\\) ANOVA into a linear model. And it’s pretty easy to see how this generalises to a \\(2 \\times 2 \\times 2\\) ANOVA or a \\(2 \\times 2 \\times 2 \\times 2\\) ANOVA. It’s the same thing, really. You just add a new binary variable for each of your factors. Where it begins to get trickier is when we consider factors that have more than two levels. Consider, for instance, the \\(3 \\times 2\\) ANOVA that we ran earlier in this chapter using the clinicaltrial.csv data. How can we convert the three-level drug factor into a numerical form that is appropriate for a regression?\nThe answer to this question is pretty simple, actually. All we have to do is realise that a three-level factor can be redescribed as two binary variables. Suppose, for instance, I were to create a new binary variable called druganxifree. Whenever the drug variable is equal to “anxifree” we set druganxifree = 1. Otherwise, we set druganxifree = 0. This variable sets up a contrast, in this case between anxifree and the other two drugs. By itself, of course, the druganxifree contrast isn’t enough to fully capture all of the information in our drug variable. We need a second contrast, one that allows us to distinguish between joyzepam and the placebo. To do this, we can create a second binary contrast, called drugjoyzepam, which equals 1 if the drug is joyzepam and 0 if it is not. Taken together, these two contrasts allows us to perfectly discriminate between all three possible drugs. Table 14.10 illustrates this.\n\n\n\n\nTable 14.10:  Binary contrasts to discriminate between all three possible drugs \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\nIf the drug administered to a patient is a placebo then both of the two contrast variables will equal 0. If the drug is Anxifree then the druganxifree variable will equal 1, and drugjoyzepam will be 0. The reverse is true for Joyzepam: drugjoyzepam is 1 and druganxifree is 0.\nCreating contrast variables is not too difficult to do using the jamovi compute new variable command. For example, to create the druganxifree variable, write this logical expression in the compute new variable formula box: IF(drug == ‘anxifree’, 1, 0)‘. Similarly, to create the new variable drugjoyzepam use this logical expression: IF(drug == ’joyzepam’, 1, 0). Likewise for CBTtherapy: IF(therapy == ‘CBT’, 1, 0). You can see these new variables, and the corresponding logical expressions, in the jamovi data file clinicaltrial2.omv.\nWe have now recoded our three-level factor in terms of two binary variables, and we’ve already seen that ANOVA and regression behave the same way for binary variables. However, there are some additional complexities that arise in this case, which we’ll discuss in the next section.\n\n\n14.6.4 變異數分析與非二元因子迴歸分析的等價性\nNow we have two different versions of the same data set. Our original data in which the drug variable from the clinicaltrial.csv file is expressed as a single three-level factor, and the expanded data clinicaltrial2.omv in which it is expanded into two binary contrasts. Once again, the thing that we want to demonstrate is that our original \\(3 \\times 2\\) factorial ANOVA is equivalent to a regression model applied to the contrast variables. Let’s start by re-running the ANOVA, with results shown in Figure 14.20.\n\n\n\n\n\nFigure 14.20: jamovi ANOVA results, without interaction component\n\n\n\n\nObviously, there are no surprises here. That’s the exact same ANOVA that we ran earlier. Next, let’s run a regression using druganxifree, drugjoyzepam and CBTtherapy as the predictors. The results are shown in Figure 14.21.\n\n\n\n\n\nFigure 14.21: jamovi regression results, with contrast variables druganxifree and drugjoyzepam\n\n\n\n\nHmm. This isn’t the same output that we got last time. Not surprisingly, the regression output prints out the results for each of the three predictors separately, just like it did every other time we conducted a regression analysis. On the one hand we can see that the p-value for the CBTtherapy variable is exactly the same as the one for the therapy factor in our original ANOVA, so we can be reassured that the regression model is doing the same thing as the ANOVA did. On the other hand, this regression model is testing the druganxifree contrast and the drugjoyzepam contrast separately, as if they were two completely unrelated variables. It’s not surprising of course, because the poor regression analysis has no way of knowing that drugjoyzepam and druganxifree are actually the two different contrasts that we used to encode our three-level drug factor. As far as it knows, drugjoyzepam and druganxifree are no more related to one another than drugjoyzepam and therapyCBT. However, you and I know better. At this stage we’re not at all interested in determining whether these two contrasts are individually significant. We just want to know if there’s an “overall” effect of drug. That is, what we want jamovi to do is to run some kind of “model comparison” test, one in which the two “drugrelated” contrasts are lumped together for the purpose of the test. Sound familiar? All we need to do is specify our null model, which in this case would include the CBTtherapy predictor, and omit both of the drug-related variables, as in Figure 14.22.\n\n\n\n\n\nFigure 14.22: Model comparison in jamovi regression, null model 1 vs. contrasts model 2\n\n\n\n\nAh, that’s better. Our F-statistic is 26.15, the degrees of freedom are 2 and 14, and the p-value is 0.00002. The numbers are identical to the ones we obtained for the main effect of drug in our original ANOVA. Once again we see that ANOVA and regression are essentially the same. They are both linear models, and the underlying statistical machinery for ANOVA is identical to the machinery used in regression. The importance of this fact should not be understated. Throughout the rest of this chapter we’re going to rely heavily on this idea.\nAlthough we went through all the faff of computing new variables in jamovi for the contrasts druganxifree and drugjoyzepam, just to show that ANOVA and regression are essentially the same, in the jamovi linear regression analysis there is actually a nifty shortcut to get these contrasts, see Figure 14.23. What jamovi is doing here is allowing you to enter the predictor variables that are factors as, wait for it…factors! Smart, eh. You can also specify which group to use as the reference level, via the ‘Reference Levels’ option. We’ve changed this to ‘placebo’ and ‘no.therapy’, respectively, because this makes most sense.\n\n\n\n\n\nFigure 14.23: Regression analysis with factors and contrasts in jamovi, including omnibus ANOVA test results\n\n\n\n\nIf you also click on the ‘ANOVA’ test checkbox under the ‘Model Coefficients’ - ‘Omnibus Test’ option, we see that the F-statistic is 26.15, the degrees of freedom are 2 and 14, and the p-value is 0.00002 (Figure 14.23). The numbers are identical to the ones we obtained for the main effect of drug in our original ANOVA. Once again, we see that ANOVA and regression are essentially the same. They are both linear models, and the underlying statistical machinery for ANOVA is identical to the machinery used in regression.\n\n\n14.6.5 自由度就是計算有多少參數\nAt long last, I can finally give a definition of degrees of freedom that I am happy with. Degrees of freedom are defined in terms of the number of parameters that have to be estimated in a model. For a regression model or an ANOVA, the number of parameters corresponds to the number of regression coefficients (i.e. b-values), including the intercept. Keeping in mind that any F-test is always a comparison between two models and the first df is the difference in the number of parameters. For example, in the model comparison above, the null model (mood.gain ~ therapyCBT) has two parameters: there’s one regression coefficient for the therapyCBT variable, and a second one for the intercept. The alternative model (mood.gain ~ druganxifree + drugjoyzepam + therapyCBT) has four parameters: one regression coefficient for each of the three contrasts, and one more for the intercept. So the degrees of freedom associated with the difference between these two models is \\(df_1 = 4 - 2 = 2\\).\nWhat about the case when there doesn’t seem to be a null model? For instance, you might be thinking of the F-test that shows up when you select ‘F Test’ under the ‘Linear Regression’ - ‘Model Fit’ options. I originally described that as a test of the regression model as a whole. However, that is still a comparison between two models. The null model is the trivial model that only includes 1 regression coefficient, for the intercept term. The alternative model contains \\(K + 1\\) regression coefficients, one for each of the K predictor variables and one more for the intercept. So the df value that you see in this F test is equal to \\(df_1 = K + 1 - 1 = K\\).\nWhat about the second df value that appears in the F-test? This always refers to the degrees of freedom associated with the residuals. It is possible to think of this in terms of parameters too, but in a slightly counter-intuitive way. Think of it like this. Suppose that the total number of observations across the study as a whole is N. If you wanted to perfectly describe each of these N values, you need to do so using, well… N numbers. When you build a regression model, what you’re really doing is specifying that some of the numbers need to perfectly describe the data. If your model has \\(K\\) predictors and an intercept, then you’ve specified \\(K + 1\\) numbers. So, without bothering to figure out exactly how this would be done, how many more numbers do you think are going to be needed to transform a K `1 parameter regression model into a perfect re-description of the raw data? If you found yourself thinking that \\((K + 1) + (N - K - 1) = N\\), and so the answer would have to be \\(N - K - 1\\), well done! That’s exactly right. In principle you can imagine an absurdly complicated regression model that includes a parameter for every single data point, and it would of course provide a perfect description of the data. This model would contain \\(N\\) parameters in total, but we’re interested in the difference between the number of parameters required to describe this full model (i.e. \\(N\\)) and the number of parameters used by the simpler regression model that you’re actually interested in (i.e., \\(K +1\\)), and so the second degrees of freedom in the F test is \\(df_2 = N - K - 1\\), where K is the number of predictors (in a regression model) or the number of contrasts (in an ANOVA). In the example I gave above, there are \\((N = 18\\) observations in the data set and \\(K + 1 = 4\\) regression coefficients associated with the ANOVA model, so the degrees of freedom for the residuals is \\(df_2 = 18 - 4 = 14\\)."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#different-ways-to-specify-contrasts",
    "href": "14-Factorial-ANOVA.html#different-ways-to-specify-contrasts",
    "title": "14  多因子變異數分析",
    "section": "14.7 Different ways to specify contrasts",
    "text": "14.7 Different ways to specify contrasts\nIn the previous section, I showed you a method for converting a factor into a collection of contrasts. In the method I showed you we specify a set of binary variables in which we defined a table like Table 14.11.\n\n\n\n\nTable 14.11:  Binary contrasts to discriminate between all three possible drugs \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\nEach row in the table corresponds to one of the factor levels, and each column corresponds to one of the contrasts. This table, which always has one more row than columns, has a special name. It is called a contrast matrix. However, there are lots of different ways to specify a contrast matrix. In this section I discuss a few of the standard contrast matrices that statisticians use and how you can use them in jamovi. If you’re planning to read the section on Factorial ANOVA 3: unbalanced designs later on, it’s worth reading this section carefully. If not, you can get away with skimming it, because the choice of contrasts doesn’t matter much for balanced designs.\n\n14.7.1 Treatment contrasts\nIn the particular kind of contrasts that I’ve described above, one level of the factor is special, and acts as a kind of “baseline” category (i.e., placebo in our example), against which the other two are defined. The name for these kinds of contrasts is treatment contrasts, also known as “dummy coding”. In this contrast each level of the factor is compared to a base reference level, and the base reference level is the value of the intercept.\nThe name reflects the fact that these contrasts are quite natural and sensible when one of the categories in your factor really is special because it actually does represent a baseline. That makes sense in our clinical trial example. The placebo condition corresponds to the situation where you don’t give people any real drugs, and so it’s special. The other two conditions are defined in relation to the placebo. In one case you replace the placebo with Anxifree, and in the other case your replace it with Joyzepam.\nThe table shown above is a matrix of treatment contrasts for a factor that has 3 levels. But suppose I want a matrix of treatment contrasts for a factor with 5 levels? You would set this out like Table 14.12.\n\n\n\n\nTable 14.12:  Matrix of treatment contrasts with 5 levels \n\nLevel2345\n\n10000\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nIn this example, the first contrast is level 2 compared with level 1, the second contrast is level 3 compared with level 1, and so on. Notice that, by default, the first level of the factor is always treated as the baseline category (i.e., it’s the one that has all zeros and doesn’t have an explicit contrast associated with it). In jamovi you can change which category is the first level of the factor by manipulating the order of the levels of the variable shown in the ‘Data Variable’ window (double click on the name of the variable in the spreadsheet column to bring up the ‘Data Variable’ view.\n\n\n14.7.2 Helmert contrasts\nTreatment contrasts are useful for a lot of situations. However, they make most sense in the situation when there really is a baseline category, and you want to assess all the other groups in relation to that one. In other situations, however, no such baseline category exists, and it may make more sense to compare each group to the mean of the other groups. This is where we meet Helmert contrasts, generated by the ‘helmert’ option in the jamovi ‘ANOVA’ - ‘Contrasts’ selection box. The idea behind Helmert contrasts is to compare each group to the mean of the “previous” ones. That is, the first contrast represents the difference between group 2 and group 1, the second contrast represents the difference between group 3 and the mean of groups 1 and 2, and so on. This translates to a contrast matrix that looks like Table 14.13 for a factor with five levels.\n\n\n\n\nTable 14.13:  Matrix of helmert contrasts with 5 levels \n\n1-1-1-1-1\n\n21-1-1-1\n\n302-1-1\n\n4003-1\n\n50004\n\n\n\n\n\nOne useful thing about Helmert contrasts is that every contrast sums to zero (i.e., all the columns sum to zero). This has the consequence that, when we interpret the ANOVA as a regression, the intercept term corresponds to the grand mean \\(\\mu_{..}\\) if we are using Helmert contrasts. Compare this to treatment contrasts, in which the intercept term corresponds to the group mean for the baseline category. This property can be very useful in some situations. It doesn’t matter very much if you have a balanced design, which we’ve been assuming so far, but it will turn out to be important later when we consider unbalanced designs. In fact, the main reason why I’ve even bothered to include this section is that contrasts become important if you want to understand unbalanced ANOVA.\n\n\n14.7.3 Sum to zero contrasts\nThe third option that I should briefly mention are “sum to zero” contrasts, called “Simple” contrasts in jamovi, which are used to construct pairwise comparisons between groups. Specifically, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the first group (Table 14.14).\n\n\n\n\nTable 14.14:  Matrix of ’sum-to’zero contrasts with 5 levels \n\n1-1-1-1-1\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nMuch like Helmert contrasts, we see that each column sums to zero, which means that the intercept term corresponds to the grand mean when ANOVA is treated as a regression model. When interpreting these contrasts, the thing to recognise is that each of these contrasts is a pairwise comparison between group 1 and one of the other four groups. Specifically, contrast 1 corresponds to a “group 2 minus group 1” comparison, contrast 2 corresponds to a “group 3 minus group 1” comparison, and so on.8\n\n\n14.7.4 Optional contrasts in jamovi\njamovi also comes with a variety of options that can generate different kinds of contrasts in ANOVA. These can be found in the ‘Contrasts’ option in the main ANOVA analysis window, where the contrast types in Table 14.15 are listed:\n\n\n\n\nTable 14.15:  Contrasts types available in the jamovi ANOVA analysis \n\nContrast type\n\nDeviationCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\nSimpleLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\nDifferenceCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\nHelmertCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\nRepeatedCompares the mean of each level (except the last) to the mean of the subsequent level\n\nPolynomialCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "href": "14-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "title": "14  多因子變異數分析",
    "section": "14.8 事後檢定",
    "text": "14.8 事後檢定\nTime to switch to a different topic. Rather than pre-planned comparisons that you have tested using contrasts, let’s suppose you’ve done your ANOVA and it turns out that you obtained some significant effects. Because of the fact that the F-tests are “omnibus” tests that only really test the null hypothesis that there are no differences among groups, obtaining a significant effect doesn’t tell you which groups are different to which other ones. We discussed this issue back in Chapter 13, and in that chapter our solution was to run t-tests for all possible pairs of groups, making corrections for multiple comparisons (e.g., Bonferroni, Holm) to control the Type I error rate across all comparisons. The methods that we used back in Chapter 13 have the advantage of being relatively simple and being the kind of tools that you can use in a lot of different situations where you’re testing multiple hypotheses, but they’re not necessarily the best choices if you’re interested in doing efficient post hoc testing in an ANOVA context. There are actually quite a lot of different methods for performing multiple comparisons in the statistics literature (Hsu, 1996), and it would be beyond the scope of an introductory text like this one to discuss all of them in any detail.\nThat being said, there’s one tool that I do want to draw your attention to, namely Tukey’s “Honestly Significant Difference”, or Tukey’s HSD for short. For once, I’ll spare you the formulas and just stick to the qualitative ideas. The basic idea in Tukey’s HSD is to examine all relevant pairwise comparisons between groups, and it’s only really appropriate to use Tukey’s HSD if it is pairwise differences that you’re interested in.9 For instance, earlier we conducted a factorial ANOVA using the clinicaltrial.csv data set, and where we specified a main effect for drug and a main effect of therapy we would be interested in the following four comparisons:\n\nThe difference in mood gain for people given Anxifree versus people given the placebo.\nThe difference in mood gain for people given Joyzepam versus people given the placebo.\nThe difference in mood gain for people given Anxifree versus people given Joyzepam.\nThe difference in mood gain for people treated with CBT and people given no therapy.\n\nFor any one of these comparisons, we’re interested in the true difference between (population) group means. Tukey’s HSD constructs simultaneous confidence intervals for all four of these comparisons. What we mean by 95% “simultaneous” confidence interval is that, if we were to repeat this study many times, then in 95% of the study results the confidence intervals would contain the relevant true value. Moreover, we can use these confidence intervals to calculate an adjusted p value for any specific comparison.\nThe TukeyHSD function in jamovi is pretty easy to use. You simply specify the ANOVA model term that you want to run the post hoc tests for. For example, if we were looking to run post hoc tests for the main effects but not the interaction, we would open up the ‘Post Hoc Tests’ option in the ANOVA analysis screen, move the drug and therapy variables across to the box on the right, and then select the ‘Tukey’ checkbox in the list of possible post hoc corrections that could be applied. This, along with the corresponding results table, is shown in Figure 14.24.\n\n\n\n\n\nFigure 14.24: Tukey HSD post hoc test in jamovi factorial ANOVA, without an interaction term\n\n\n\n\nThe output shown in the ‘Post Hoc Tests’ results table is (I hope) pretty straightforward. The first comparison, for example, is the Anxifree versus placebo difference, and the first part of the output indicates that the observed difference in group means is .27. The next number is the standard error for the difference, from which we could calculate the 95% confidence interval if we wanted, though jamovi does not currently provide this option. Then there is a column with the degrees of freedom, a column with the t-value, and finally a column with the p-value. For the first comparison the adjusted p-value is .21. In contrast, if you look at the next line, we see that the observed difference between joyzepam and the placebo is 1.03, and this result is significant (p < .001).\nSo far, so good. What about the situation where your model includes interaction terms? For instance, the default option in jamovi is to allow for the possibility that there is an interaction between drug and therapy. If that’s the case, the number of pairwise comparisons that we need to consider starts to increase. As before, we need to consider the three comparisons that are relevant to the main effect of drug and the one comparison that is relevant to the main effect of therapy. But, if we want to consider the possibility of a significant interaction (and try to find the group differences that underpin that significant interaction), we need to include comparisons such as the following:\n\nThe difference in mood gain for people given Anxifree and treated with CBT, versus people given the placebo and treated with CBT\nThe difference in mood gain for people given Anxifree and given no therapy, versus people given the placebo and given no therapy.\netc\n\nThere are quite a lot of these comparisons that you need to consider. So, when we run the Tukey post hoc analysis for this ANOVA model, we see that it has made a lot of pairwise comparisons (19 in total), as shown in Figure 14.25. You can see that it looks pretty similar to before, but with a lot more comparisons made.\n\n\n\n\n\nFigure 14.25: Tukey HSD post hoc test in jamovi factorial ANOVA with an interaction term"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "href": "14-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "title": "14  多因子變異數分析",
    "section": "14.9 事前檢定方法",
    "text": "14.9 事前檢定方法\nFollowing on from the previous sections on contrasts and post hoc tests in ANOVA, I think the method of planned comparisons is important enough to deserve a quick discussion. In our discussions of multiple comparisons, in the previous section and back in Chapter 13, I’ve been assuming that the tests you want to run are genuinely post hoc. For instance, in our drugs example above, maybe you thought that the drugs would all have different effects on mood (i.e., you hypothesised a main effect of drug), but you didn’t have any specific hypothesis about how they would be different, nor did you have any real idea about which pairwise comparisons would be worth looking at. If that is the case, then you really have to resort to something like Tukey’s HSD to do your pairwise comparisons.\nThe situation is rather different, however, if you genuinely did have real, specific hypotheses about which comparisons are of interest, and you never ever have any intention to look at any other comparisons besides the ones that you specified ahead of time. When this is true, and if you honestly and rigorously stick to your noble intentions to not run any other comparisons (even when the data look like they’re showing you deliciously significant effects for stuff you didn’t have a hypothesis test for), then it doesn’t really make a lot of sense to run something like Tukey’s HSD, because it makes corrections for a whole bunch of comparisons that you never cared about and never had any intention of looking at. Under those circumstances, you can safely run a (limited) number of hypothesis tests without making an adjustment for multiple testing. This situation is known as the method of planned comparisons, and it is sometimes used in clinical trials. However, further consideration is out of scope for this introductory book, but at least you know that this method exists!"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-3-unbalanced-designs",
    "href": "14-Factorial-ANOVA.html#factorial-anova-3-unbalanced-designs",
    "title": "14  多因子變異數分析",
    "section": "14.10 Factorial ANOVA 3: unbalanced designs",
    "text": "14.10 Factorial ANOVA 3: unbalanced designs\nFactorial ANOVA is a very handy thing to know about. It’s been one of the standard tools used to analyse experimental data for many decades, and you’ll find that you can’t read more than two or three papers in psychology without running into an ANOVA in there somewhere. However, there’s one huge difference between the ANOVAs that you’ll see in a lot of real scientific articles and the ANOVAs that I’ve described so far. In in real life we’re rarely lucky enough to have perfectly balanced designs. For one reason or another, it’s typical to end up with more observations in some cells than in others. Or, to put it another way, we have an unbalanced design.\nUnbalanced designs need to be treated with a lot more care than balanced designs, and the statistical theory that underpins them is a lot messier. It might be a consequence of this messiness, or it might be a shortage of time, but my experience has been that undergraduate research methods classes in psychology have a nasty tendency to ignore this issue completely. A lot of stats textbooks tend to gloss over it too. The net result of this, I think, is that a lot of active researchers in the field don’t actually know that there’s several different “types” of unbalanced ANOVAs, and they produce quite different answers. In fact, reading the psychological literature, I’m kind of amazed at the fact that most people who report the results of an unbalanced factorial ANOVA don’t actually give you enough details to reproduce the analysis. I secretly suspect that most people don’t even realise that their statistical software package is making a whole lot of substantive data analysis decisions on their behalf. It’s actually a little terrifying when you think about it. So, if you want to avoid handing control of your data analysis to stupid software, read on.\n\n14.10.1 The coffee data\nAs usual, it will help us to work with some data. The coffee.csv file contains a hypothetical data set that produces an unbalanced \\(3 \\times 2\\) ANOVA. Suppose we were interested in finding out whether or not the tendency of people to babble when they have too much coffee is purely an effect of the coffee itself, or whether there’s some effect of the milk and sugar that people add to the coffee. Suppose we took 18 people and gave them some coffee to drink. The amount of coffee / caffeine was held constant, and we varied whether or not milk was added, so milk is a binary factor with two levels, “yes” and “no”. We also varied the kind of sugar involved. The coffee might contain “real” sugar or it might contain “fake” sugar (i.e., artificial sweetener) or it might contain “none” at all, so the sugar variable is a three level factor. Our outcome variable is a continuous variable that presumably refers to some psychologically sensible measure of the extent to which someone is “babbling”. The details don’t really matter for our purpose. Take a look at the data in the jamovi spreadsheet view, as in Figure 14.26.\n\n\n\n\n\nFigure 14.26: The coffee.csv data set in jamovi, with descriptive information aggregated by factor levels\n\n\n\n\nLooking at the table of means in Figure 14.26 we get a strong impression that there are differences between the groups. This is especially true when we compare these means to the standard deviations for the babble variable. Across groups, this standard deviation varies from .14 to .71, which is fairly small relative to the differences in group means.10 Whilst this at first may seem like a straightforward factorial ANOVA, a problem arises when we look at how many observations we have in each group. See the different Ns for different groups shown in Figure 14.26. This violates one of our original assumptions, namely that the number of people in each group is the same. We haven’t really discussed how to handle this situation.\n\n\n14.10.2 “Standard ANOVA” does not exist for unbalanced designs\nUnbalanced designs lead us to the somewhat unsettling discovery that there isn’t really any one thing that we might refer to as a standard ANOVA. In fact, it turns out that there are three fundamentally different ways11 in which you might want to run an ANOVA in an unbalanced design. If you have a balanced design all three versions produce identical results, with the sums of squares, F-values, etc., all conforming to the formulas that I gave at the start of the chapter. However, when your design is unbalanced they don’t give the same answers. Furthermore, they are not all equally appropriate to every situation. Some methods will be more appropriate to your situation than others. Given all this, it’s important to understand what the different types of ANOVA are and how they differ from one another.\nThe first kind of ANOVA is conventionally referred to as Type I sum of squares. I’m sure you can guess what the other two are called. The “sum of squares” part of the name was introduced by the SAS statistical software package and has become standard nomenclature, but it’s a bit misleading in some ways. I think the logic for referring to them as different types of sum of squares is that, when you look at the ANOVA tables that they produce, the key difference in the numbers is the SS values. The degrees of freedom don’t change, the MS values are still defined as SS divided by df, etc. However, what the terminology gets wrong is that it hides the reason why the SS values are different from one another. To that end, it’s a lot more helpful to think of the three different kinds of ANOVA as three different hypothesis testing strategies. These different strategies lead to different SS values, to be sure, but it’s the strategy that is the important thing here, not the SS values themselves. Recall from the section ANOVA as a linear model that any particular F-test is best thought of as a comparison between two linear models. So, when you’re looking at an ANOVA table, it helps to remember that each of those F-tests corresponds to a pair of models that are being compared. Of course, this leads naturally to the question of which pair of models is being compared. This is the fundamental difference between ANOVA Types I, II and III: each one corresponds to a different way of choosing the model pairs for the tests.\n\n\n14.10.3 Type I sum of squares\nThe Type I method is sometimes referred to as the “sequential” sum of squares, because it involves a process of adding terms to the model one at a time. Consider the coffee data, for instance. Suppose we want to run the full \\(3 \\times 2\\) factorial ANOVA, including interaction terms. The full model contains the outcome variable babble, the predictor variables sugar and milk, and the interaction term sugar \\(\\times\\) milk. This can be written as \\(babble \\sim sugar + milk + sugar {\\times} milk\\). The Type I strategy builds this model up sequentially, starting from the simplest possible model and gradually adding terms.\nThe simplest possible model for the data would be one in which neither milk nor sugar is assumed to have any effect on babbling. The only term that would be included in such a model is the intercept, written as babble ~ 1. This is our initial null hypothesis. The next simplest model for the data would be one in which only one of the two main effects is included. In the coffee data, there are two different possible choices here, because we could choose to add milk first or to add sugar first. The order actually turns out to matter, as we’ll see later, but for now let’s just make a choice arbitrarily and pick sugar. So, the second model in our sequence of models is babble ~ sugar, and it forms the alternative hypothesis for our first test. We now have our first hypothesis test (Table 14.16).\n\n\n\n\nTable 14.16:  Null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim 1\\)\n\nAlternative model:\\(babble \\sim  sugar\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of sugar. The next step in our model building exercise is to add the other main effect term, so the next model in our sequence is babble ~ sugar + milk. The second hypothesis test is then formed by comparing the following pair of models (Table 14.17).\n\n\n\n\nTable 14.17:  Further null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar\\)\n\nAlternative model:\\(babble \\sim  sugar + milk\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of milk. In one sense, this approach is very elegant: the alternative hypothesis from the first test forms the null hypothesis for the second one. It is in this sense that the Type I method is strictly sequential. Every test builds directly on the results of the last one. However, in another sense it’s very inelegant, because there’s a strong asymmetry between the two tests. The test of the main effect of sugar (the first test) completely ignores milk, whereas the test of the main effect of milk (the second test) does take sugar into account. In any case, the fourth model in our sequence is now the full model, babble ~ sugar + milk + sugar \\(\\times\\) milk, and the corresponding hypothesis test is shown in Table 14.18.\n\n\n\n\nTable 14.18:  And more possible null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk + sugar * milk \\)\n\n\n\n\n\nType III sum of squares is the default hypothesis testing method used by jamovi ANOVA, so to run a Type I sum of squares analysis we have to select ‘Type 1’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options. This gives us the ANOVA table shown in Figure 14.27.\n\n\n\n\n\nFigure 14.27: ANOVA results table using Type I sum of squares in jamovi\n\n\n\n\nThe big problem with using Type I sum of squares is the fact that it really does depend on the order in which you enter the variables. Yet, in many situations the researcher has no reason to prefer one ordering over another. This is presumably the case for our milk and sugar problem. Should we add milk first or sugar first? It feels exactly as arbitrary as a data analysis question as it does as a coffee-making question. There may in fact be some people with firm opinions about ordering, but it’s hard to imagine a principled answer to the question. Yet, look what happens when we change the ordering, as in Figure 14.28.\n\n\n\n\n\nFigure 14.28: ANOVA results table using Type I sum of squares in jamovi, but with factors entered in a different order (milk first)\n\n\n\n\nThe p-values for both main effect terms have changed, and fairly dramatically. Among other things, the effect of milk has become significant (though one should avoid drawing any strong conclusions about this, as I’ve mentioned previously). Which of these two ANOVAs should one report? It’s not immediately obvious.\nWhen you look at the hypothesis tests that are used to define the “first” main effect and the “second” one, it’s clear that they’re qualitatively different from one another. In our initial example, we saw that the test for the main effect of sugar completely ignores milk, whereas the test of the main effect of milk does take sugar into account. As such, the Type I testing strategy really does treat the first main effect as if it had a kind of theoretical primacy over the second one. In my experience there is very rarely if ever any theoretically primacy of this kind that would justify treating any two main effects asymmetrically.\nThe consequence of all this is that Type I tests are very rarely of much interest, and so we should move on to discuss Type II tests and Type III tests.\n\n\n14.10.4 Type III sum of squares\nHaving just finished talking about Type I tests, you might think that the natural thing to do next would be to talk about Type II tests. However, I think it’s actually a bit more natural to discuss Type III tests (which are simple and the default in jamovi ANOVA) before talking about Type II tests (which are trickier). The basic idea behind Type III tests is extremely simple. Regardless of which term you’re trying to evaluate, run the F-test in which the alternative hypothesis corresponds to the full ANOVA model as specified by the user, and the null model just deletes that one term that you’re testing. For instance, in the coffee example, in which our full model was babble ~ sugar + milk + sugar \\(\\times\\) milk, the test for a main effect of sugar would correspond to a comparison between the following two models (Table 14.19).\n\n\n\n\nTable 14.19:  Null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  milk + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nSimilarly the main effect of milk is evaluated by testing the full model against a null model that removes the milk term, like in Table 14.20.\n\n\n\n\nTable 14.20:  Further null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nFinally, the interaction term sugar \\(\\times\\) milk is evaluated in exactly the same way. Once again, we test the full model against a null model that removes the sugar \\(\\times\\) milk interaction term, like in Table 14.21.\n\n\n\n\nTable 14.21:  Removing the interaction term from hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nThe basic idea generalises to higher order ANOVAs. For instance, suppose that we were trying to run an ANOVA with three factors, A, B and C, and we wanted to consider all possible main effects and all possible interactions, including the three way interaction A \\(\\times\\) B \\(\\times\\) C. (Table 14.22) shows you what the Type III tests look like for this situation).\n\n\n\n\nTable 14.22:  Type III tests with three factors and all main effect and interaction term \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB\\(A + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\nC\\(A + B + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B\\(A + B + C + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*C\\(A + B + C + A*B + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB*C\\(A + B + C + A*B + A*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nAs ugly as that table looks, it’s pretty simple. In all cases, the alternative hypothesis corresponds to the full model which contains three main effect terms (e.g. A), three two-way interactions (e.g. A*B) and one three-way interaction (i.e., A*B*C)). The null model always contains 6 of these 7 terms, and the missing one is the one whose significance we’re trying to test.\nAt first pass, Type III tests seem like a nice idea. Firstly, we’ve removed the asymmetry that caused us to have problems when running Type I tests. And because we’re now treating all terms the same way, the results of the hypothesis tests do not depend on the order in which we specify them. This is definitely a good thing. However, there is a big problem when interpreting the results of the tests, especially for main effect terms. Consider the coffee data. Suppose it turns out that the main effect of milk is not significant according to the Type III tests. What this is telling us is that babble ~ sugar + sugar*milk is a better model for the data than the full model. But what does that even mean? If the interaction term sugar*milk was also non significant, we’d be tempted to conclude that the data are telling us that the only thing that matters is sugar. But suppose we have a significant interaction term, but a non-significant main effect of milk. In this case, are we to assume that there really is an “effect of sugar”, an “interaction between milk and sugar”, but no “effect of milk”? That seems crazy. The right answer simply must be that it’s meaningless12 to talk about the main effect if the interaction is significant. In general, this seems to be what most statisticians advise us to do, and I think that’s the right advice. But if it really is meaningless to talk about non-significant main effects in the presence of a significant interaction, then it’s not at all obvious why Type III tests should allow the null hypothesis to rely on a model that includes the interaction but omits one of the main effects that make it up. When characterised in this fashion, the null hypotheses really don’t make much sense at all.\nLater on, we’ll see that Type III tests can be redeemed in some contexts, but first let’s take a look at the ANOVA results table using Type III sum of squares, see Figure 14.29.\n\n\n\n\n\nFigure 14.29: ANOVA results table using Type III sum of squares in jamovi\n\n\n\n\nBut be aware, one of the perverse features of the Type III testing strategy is that typically the results turn out to depend on the contrasts that you use to encode your factors (see the Different ways to specify contrasts section if you’ve forgotten what the different types of contrasts are).13\nOkay, so if the p-values that typically come out of Type III analyses (but not in jamovi) are so sensitive to the choice of contrasts, does that mean that Type III tests are essentially arbitrary and not to be trusted? To some extent that’s true, and when we turn to a discussion of Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\nIn Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\n\n\n14.10.5 Type II sum of squares\nOkay, so we’ve seen Type I and III tests now, and both are pretty straightforward. Type I tests are performed by gradually adding terms one at a time, whereas Type III tests are performed by taking the full model and looking to see what happens when you remove each term. However, both can have some limitations. Type I tests are dependent on the order in which you enter the terms, and Type III tests are dependent on how you code up your contrasts. Type II tests are a little harder to describe, but they avoid both of these problems, and as a result they are a little easier to interpret.\nType II tests are broadly similar to Type III tests. Start with a “full” model, and test a particular term by deleting it from that model. However, Type II tests are based on the marginality principle which states that you should not omit a lower order term from your model if there are any higher order ones that depend on it. So, for instance, if your model contains the two-way interaction A \\(\\times\\) B (a 2nd order term), then it really ought to contain the main effects A and B (1st order terms). Similarly, if it contains a three-way interaction term A \\(\\times\\) B \\(\\times\\) C, then the model must also include the main effects A, B and C as well as the simpler interactions A \\(\\times\\) B, A \\(\\times\\) C and B \\(\\times\\) C. Type III tests routinely violate the marginality principle. For instance, consider the test of the main effect of A in the context of a three-way ANOVA that includes all possible interaction terms. According to Type III tests, our null and alternative models are in Table 14.23.\n\n\n\n\nTable 14.23:  Type III tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\nNotice that the null hypothesis omits A, but includes A \\(\\times\\) B, A \\(\\times\\) C and A \\(\\times\\) B \\(\\times\\) C as part of the model. This, according to the Type II tests, is not a good choice of null hypothesis. What we should do instead, if we want to test the null hypothesis that A is not relevant to our outcome, is to specify the null hypothesis that is the most complicated model that does not rely on A in any form, even as an interaction. The alternative hypothesis corresponds to this null model plus a main effect term of A. This is a lot closer to what most people would intuitively think of as a “main effect of A”, and it yields the following as our Type II test of the main effect of A (Table 14.24). 14\n\n\n\n\nTable 14.24:  Type II tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\nAnyway, just to give you a sense of how the Type II tests play out, here’s the full table (Table 14.25) of tests that would be applied in a three-way factorial ANOVA:\n\n\n\n\nTable 14.25:  Type II tests for a three-way factorial model \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + B*C \\)\\(A + B + C + B*C \\)\n\nB\\(A + C + A*C \\)\\(A + B + C + A*C\\)\n\nC\\(A + B + A*B \\)\\(A + B + C + A*B\\)\n\nA*B\\(A + B + C + A*C + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*C\\(A + B + C + A*B + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nB*C\\(A + B + C + A*B + A*C \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nIn the context of the two way ANOVA that we’ve been using in the coffee data, the hypothesis tests are even simpler. The main effect of sugar corresponds to an F-test comparing these two models (Table 14.26).\n\n\n\n\nTable 14.26:  Type II tests for the main effect of sugar in the coffee data \n\nNull model:\\(babble \\sim milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nThe test for the main effect of milk is in Table 14.27.\n\n\n\n\nTable 14.27:  Type II tests for the main effect of milk in the coffee data \n\nNull model:\\(babble \\sim  sugar \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nFinally, the test for the interaction sugar \\(\\times\\) milk is in Table 14.28.\n\n\n\n\nTable 14.28:  Type II tests for the sugar x milk interaction term \n\nNull model:\\(babble \\sim  sugar + milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk  + sugar*milk \\)\n\n\n\n\n\nRunning the tests are again straightforward. Just select ‘Type 2’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options, This gives us the ANOVA table shown in Figure 14.30.\n\n\n\n\n\nFigure 14.30: ANOVA results table using Type II sum of squares in jamovi\n\n\n\n\nType II tests have some clear advantages over Type I and Type III tests. They don’t depend on the order in which you specify factors (unlike Type I), and they don’t depend on the contrasts that you use to specify your factors (unlike Type III). And although opinions may differ on this last point, and it will definitely depend on what you’re trying to do with your data, I do think that the hypothesis tests that they specify are more likely to correspond to something that you actually care about. As a consequence, I find that it’s usually easier to interpret the results of a Type II test than the results of a Type I or Type III test. For this reason my tentative advice is that, if you can’t think of any obvious model comparisons that directly map onto your research questions but you still want to run an ANOVA in an unbalanced design, Type II tests are probably a better choice than Type I or Type III.15\n\n\n14.10.6 Effect sizes (and non-additive sums of squares)\njamovi also provides the effect sizes \\(\\eta^2\\) and partial \\(\\eta^2\\) when you select these options, as in Figure 14.30. However, when you’ve got an unbalanced design there’s a bit of extra complexity involved.\nIf you remember back to our very early discussions of ANOVA, one of the key ideas behind the sums of squares calculations is that if we add up all the SS terms associated with the effects in the model, and add that to the residual SS, they’re supposed to add up to the total sum of squares. And, on top of that, the whole idea behind \\(\\eta^2\\) is that, because you’re dividing one of the SS terms by the total SS value, an \\(\\eta^2\\) value can be interpreted as the proportion of variance accounted for by a particular term. But this is not so straightforward in unbalanced designs because some of the variance goes “missing”.\nThis seems a bit odd at first, but here’s why. When you have unbalanced designs your factors become correlated with one another, and it becomes difficult to tell the difference between the effect of Factor A and the effect of Factor B. In the extreme case, suppose that we’d run a \\(2 \\times 2\\) design in which the number of participants in each group had been as in Table 14.29.\n\n\n\n\nTable 14.29:  N participants in a 2 x 2 very (very!) unbalanced factorial design \n\nsugarno sugar\n\nmilk1000\n\nno milk0100\n\n\n\n\n\nHere we have a spectacularly unbalanced design: 100 people have milk and sugar, 100 people have no milk and no sugar, and that’s all. There are 0 people with milk and no sugar, and 0 people with sugar but no milk. Now suppose that, when we collected the data, it turned out there is a large (and statistically significant) difference between the “milk and sugar” group and the “no-milk and no-sugar” group. Is this a main effect of sugar? A main effect of milk? Or an interaction? It’s impossible to tell, because the presence of sugar has a perfect association with the presence of milk. Now suppose the design had been a little more balanced (Table 14.30).\n\n\n\n\nTable 14.30:  N participants in a 2 x 2 still very unbalanced factorial design \n\nsugarno sugar\n\nmilk1005\n\nno milk5100\n\n\n\n\n\nThis time around, it’s technically possible to distinguish between the effect of milk and the effect of sugar, because we have a few people that have one but not the other. However, it will still be pretty difficult to do so, because the association between sugar and milk is still extremely strong, and there are so few observations in two of the groups. Again, we’re very likely to be in the situation where we know that the predictor variables (milk and sugar) are related to the outcome (babbling), but we don’t know if the nature of that relationship is a main effect of one or the other predictor, or the interaction."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#summary",
    "href": "14-Factorial-ANOVA.html#summary",
    "title": "14  多因子變異數分析",
    "section": "14.11 Summary",
    "text": "14.11 Summary\n\nFactorial ANOVA 1: balanced designs, no interactions and with interactions included\nEffect size, estimated means, and confidence intervals in a factorial ANOVA\nAssumption checking in ANOVA\nAnalysis of Covariance (ANCOVA)\nUnderstanding ANOVA as a linear model, including Different ways to specify contrasts\nPost hoc tests using Tukey’s HSD and a brief commentary on The method of planned comparisons\nFactorial ANOVA 3: unbalanced designs\n\n\n\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A second-level course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall."
  },
  {
    "objectID": "15-Factor-Analysis.html#exploratory-factor-analysis",
    "href": "15-Factor-Analysis.html#exploratory-factor-analysis",
    "title": "15  因素分析",
    "section": "15.1 Exploratory Factor Analysis",
    "text": "15.1 Exploratory Factor Analysis\nExploratory Factor Analysis (EFA) is a statistical technique for revealing any hidden latent factors that can be inferred from our observed data. This technique calculates to what extent a set of measured variables, for example \\(V1, V2, V3, V4\\), and \\(V5\\), can be represented as measures of an underlying latent factor. This latent factor cannot be measured through just one observed variable but instead is manifested in the relationships it causes in a set of observed variables.\nIn Figure 15.1 each observed variable \\(V\\) is ‘caused’ to some extent by the underlying latent factor (\\(F\\)), depicted by the coefficients \\(b_1\\) to \\(b_5\\) (also called factor loadings). Each observed variable also has an associated error term, e1 to e5. Each error term is the variance in the associated observed variable, \\(V_i\\) , that is unexplained by the underlying latent factor.\n\n\n\n\n\nFigure 15.1: Latent factor underlying the relationship between several observed variables\n\n\n\n\nIn Psychology, latent factors represent psychological phenomena or constructs that are difficult to directly observe or measure. For example, personality, or intelligence, or thinking style. In the example in Figure 15.1 we may have asked people five specific questions about their behaviour or attitudes, and from that we are able to get a picture about a personality construct called, for example, extraversion. A different set of specific questions may give us a picture about an individual’s introversion, or their conscientiousness.\nHere’s another example: we may not be able to directly measure statistics anxiety, but we can measure whether statistics anxiety is high or low with a set of questions in a questionnaire. For example, “\\(Q1\\): Doing the assignment for a statistics course”, “\\(Q2\\): Trying to understand the statistics described in a journal article”, and “\\(Q3\\): Asking the lecturer for help in understanding something from the course”, etc., each rated from low anxiety to high anxiety. People with high statistics anxiety will tend to give similarly high responses on these observed variables because of their high statistics anxiety. Likewise, people with low statistics anxiety will give similar low responses to these variables because of their low statistics anxiety.\nIn exploratory factor analysis (EFA), we are essentially exploring the correlations between observed variables to uncover any interesting, important underlying (latent) factors that are identified when observed variables co-vary. We can use statistical software to estimate any latent factors and to identify which of our variables have a high loading1 (e.g. loading > 0.5) on each factor, suggesting they are a useful measure, or indicator, of the latent factor. Part of this process includes a step called rotation, which to be honest is a pretty weird idea but luckily we don’t have to worry about understanding it; we just need to know that it is helpful because it makes the pattern of loadings on different factors much clearer. As such, rotation helps with seeing more clearly which variables are linked substantively to each factor. We also need to decide how many factors are reasonable given our data, and helpful in this regard is something called Eigen values. We’ll come back to this in a moment, after we have covered some of the main assumptions of EFA.\n\n15.1.1 Checking assumptions\nThere are a couple of assumptions that need to be checked as part of the analysis. The first assumption is sphericity, which essentially checks that the variables in your dataset are correlated with each other to the extent that they can potentially be summarised with a smaller set of factors. Bartlett’s test for sphericity checks whether the observed correlation matrix diverges significantly from a zero (or null) correlation matrix. So, if Bartlett’s test is significant (\\(p < .05\\)), this indicates that the observed correlation matrix is significantly divergent from the null, and is therefore suitable for EFA.\nThe second assumption is sampling adequacy and is checked using the Kaiser-MeyerOlkin (KMO) Measure of Sampling Adequacy (MSA). The KMO index is a measure of the proportion of variance among observed variables that might be common variance. Using partial correlations, it checks for factors that load just two items. We seldom, if ever, want EFA producing a lot of factors loading just two items each. KMO is about sampling adequacy because partial correlations are typically seen with inadequate samples. If the KMO index is high (\\(\\approx 1\\)), the EFA is efficient whereas if KMO is low (\\(\\approx 0\\)), the EFA is not relevant. KMO values smaller than \\(0.5\\) indicates that EFA is not suitable and a KMO value of \\(0.6\\) should be present before EFA is considered suitable. Values between \\(0.5\\) and \\(0.7\\) are considered adequate, values between \\(0.7\\) and \\(0.9\\) are good and values between \\(0.9\\) and \\(1.0\\) are excellent.\n\n\n15.1.2 What is EFA good for?\nIf the EFA has provided a good solution (i.e. factor model), then we need to decide what to do with our shiny new factors. Researchers often use EFA during psychometric scale development. They will develop a pool of questionnaire items that they think relate to one or more psychological constructs, use EFA to see which items “go together” as latent factors, and then they will assess whether some items should be removed because they don’t usefully or distinctly measure one of the latent factors.\nIn line with this approach, another consequence of EFA is to combine the variables that load onto distinct factors into a factor score, sometimes known as a scale score. There are two options for combining variables into a scale score:\n\nCreate a new variable with a score weighted by the factor loadings for each item that contributes to the factor.\nCreate a new variable based on each item that contributes to the factor, but weighting them equally.\n\nIn the first option each item’s contribution to the combined score depends on how strongly it relates to the factor. In the second option we typically just average across all the items that contribute substantively to a factor to create the combined scale score variable. Which to choose is a matter of preference, though a disadvantage with the first option is that loadings can vary quite a bit from sample to sample, and in behavioural and health sciences we are often interested in developing and using composite questionnaire scale scores across different studies and different samples. In which case it is reasonable to use a composite measure that is based on the substantive items contributing equally rather than weighting by sample specific loadings from a different sample. In any case, understanding a combined variable measure as an average of items is simpler and more intuitive than using a sample specific optimally-weighted combination.\nA more advanced statistical technique, one which is beyond the scope of this book, undertakes regression modelling where latent factors are used in prediction models of other latent factors. This is called “structural equation modelling” and there are specific software programmes and R packages dedicated to this approach. But let’s not get ahead of ourselves; what we should really focus on now is how to do an EFA in jamovi.\n\n\n15.1.3 EFA in jamovi\nFirst, we need some data. Twenty-five personality self-report items (see Figure 15.2) taken from the International Personality Item Pool were included as part of the Synthetic Aperture Personality Assessment (SAPA) web-based personality assessment (SAPA: http://sapa-project.org) project. The 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness.\nThe item data were collected using a 6-point response scale:\n\nVery Inaccurate\nModerately Inaccurate\nSlightly Inaccurate\nSlightly Accurate\nModerately Accurate\nVery Accurate.\n\nA sample of \\(N=250\\) responses is contained in the dataset bfi_sample.csv. As researchers, we are interested in exploring the data to see whether there are some underlying latent factors that are measured reasonably well by the \\(25\\) observed variables in the bfi_sample.csv data file. Open up the dataset and check that the \\(25\\) variables are coded as continuous variables (technically they are ordinal though for EFA in jamovi it mostly doesn’t matter, except if you decide to calculate weighted factor scores in which case continuous variables are needed). To perform EFA in jamovi:\n\n\n\n\n\nFigure 15.2: Twenty-five observed variable items organised by five putative personality factors in the dataset bfi_sample.csv\n\n\n\n\n\nSelect Factor - Exploratory Factor Analysis from the main jamovi button bar to open the EFA analysis window (Figure 15.3).\nSelect the 25 personality questions and transfer them into the ‘Variables’ box.\nCheck appropriate options, including ‘Assumption Checks’, but also Rotation ‘Method’, ‘Number of Factors’ to extract, and ‘Additional Output’ options. See Figure 15.3 for suggested options for this illustrative EFA, and please note that the Rotation ‘Method’ and ‘Number of Factors’ extracted is typically adjusted by the researcher during the analysis to find the best result, as described below.\n\n\n\n\n\n\nFigure 15.3: The jamovi EFA analysis window\n\n\n\n\n\n\n\n\n\nFigure 15.4: jamovi EFA assumption checks for the personality questionnaire data\n\n\n\n\nFirst, check the assumptions (Figure 15.4). You can see that (1) Bartlett’s test of sphericity is significant, so this assumption is satisfied; and (2) the KMO measure of sampling adequacy (MSA) is \\(0.81\\) overall, suggesting good sampling adequacy. No problems here then!\nThe next thing to check is how many factors to use (or “extract” from the data). Three different approaches are available:\n\nOne convention is to choose all components with Eigen values greater than 12 . This would give us four factors with our data (try it and see).\nExamination of the scree plot, as in Figure 15.5, lets you identify the “point of inflection”. This is the point at which the slope of the scree curve clearly levels off, below the “elbow”. This would give us five factors with our data. Interpreting scree plots is a bit of an art: in Figure 15.5 there is a noticeable step from \\(5\\) to \\(6\\) factors, but in other scree plots you look at it will not be so clear cut.\nUsing a parallel analysis technique, the obtained Eigen values are compared to those that would be obtained from random data. The number of factors extracted is the number with Eigen values greater than what would be found with random data.\n\n\n\n\n\n\nFigure 15.5: Scree plot of the personality data in jamovi EFA, showing a noticeable inflection and levelling off after point 5 (the ‘elbow’)\n\n\n\n\nThe third approach is a good one according to Fabrigar et al. (1999), although in practice researchers tend to look at all three and then make a judgement about the number of factors that are most easily or helpfully interpreted. This can be understood as the “meaningfulness criterion”, and researchers will typically examine, in addition to the solution from one of the approaches above, solutions with one or two more or fewer factors. They then adopt the solution which makes the most sense to them.\nAt the same time, we should also consider the best way to rotate the final solution. There are two main approaches to rotation: orthogonal (e.g. ‘varimax’) rotation forces the selected factors to be uncorrelated, whereas oblique (e.g. ‘oblimin’) rotation allows the selected factors to be correlated. Dimensions of interest to psychologists and behavioural scientists are not often dimensions we would expect to be orthogonal, so oblique solutions are arguably more sensible2\n\n\n\n\n\nFigure 15.6: Factor summary statistics and correlations for a five factor solution in jamovi EFA\n\n\n\n\nPractically, if in an oblique rotation the factors are found to be substantially correlated (positive or negative, and > 0.3), as in Figure 15.6 where a correlation between two of the extracted factors is 0.31, then this would confirm our intuition to prefer oblique rotation. If the factors are, in fact, correlated, then an oblique rotation will produce a better estimate of the true factors and a better simple structure than will an orthogonal rotation. And, if the oblique rotation indicates that the factors have close to zero correlations between one another, then the researcher can go ahead and conduct an orthogonal rotation (which should then give about the same solution as the oblique rotation).\nOn checking the correlation between the extracted factors at least one correlation was greater than 0.3 (Figure 15.6), so an oblique (‘oblimin’) rotation of the five extracted factors is preferred. We can also see in Figure 15.6 that the proportion of overall variance in the data that is accounted for by the five factors is 46%. Factor one accounts for around 10% of the variance, factors two to four around 9% each, and factor five just over 7%. This isn’t great; it would have been better if the overall solution accounted for a more substantive proportion of the variance in our data.\nBe aware that in every EFA you could potentially have the same number of factors as observed variables, but every additional factor you include will add a smaller amount of explained variance. If the first few factors explain a good amount of the variance in the original 25 variables, then those factors are clearly a useful, simpler substitute for the 25 variables. You can drop the rest without losing too much of the original variability. But if it takes 18 factors (for example) to explain most of the variance in those 25 variables, you might as well just use the original 25.\nFigure 15.7 shows the factor loadings. That is, how the 25 different personality items load onto each of the five selected factors. We have hidden loadings less than \\(0.3\\) (set in the options shown in Figure 15.3.\nFor Factors \\(1, 2, 3\\) and \\(4\\) the pattern of factor loadings closely matches the putative factors specified in Figure 15.2. Phew! And factor \\(5\\) is pretty close, with four of the five observed variables that putatively measure “openness” loading pretty well onto the factor. Variable \\(04\\) doesn’t quite seem to fit though, as the factor solution in Figure 15.7 suggests that it loads onto factor \\(4\\) (albeit with a relatively low loading) but not substantively onto factor \\(5\\).\nThe other thing to note is that those variables that were denoted as “R: reverse coding” in Figure 15.2 are those that have negative factor loadings. Take a look at the items A1 (“Am indifferent to the feelings of others”) and A2 (“Inquire about others’ well-being”). We can see that a high score on \\(A1\\) indicates low Agreeableness, whereas a high score on \\(A2\\) (and all the other “A” variables for that matter) indicates high Agreeableness. Therefore A1 will be negatively correlated with the other “A” variables, and this is why it has a negative factor loading, as shown in Figure 15.7.\n\n\n\n\n\nFigure 15.7: Factor loadings for a five factor solution in jamovi EFA\n\n\n\n\nWe can also see in Figure 15.7 the “uniqueness” of each variable. Uniqueness is the proportion of variance that is ‘unique’ to the variable and not explained by the factors3. For example, 72% of the variance in ‘A1’ is not explained by the factors in the five factor solution. In contrast, ‘N1’ has relatively low variance not accounted for by the factor solution (35%). Note that the greater the ‘uniqueness’, the lower the relevance or contribution of the variable in the factor model.\nTo be honest, it’s unusual to get such a neat solution in EFA. It’s typically quite a bit more messy than this, and often interpreting the meaning of the factors is more challenging. It’s not often that you have such a clearly delineated item pool. More often you will have a whole heap of observed variables that you think may be indicators of a few underlying latent factors, but you don’t have such a strong sense of which variables are going to go where!\nSo, we seem to have a pretty good five factor solution, albeit accounting for a relatively low overall proportion of the observed variance. Let’s assume we are happy with this solution and want to use our factors in further analysis. The straightforward option is to calculate an overall (average) score for each factor by adding together the score for each variable that loads substantively onto the factor and then dividing by the number of variables (in other words create a ‘mean score’ for each person across the items for each scale. For each person in our dataset that entails, for example for the Agreeableness factor, adding together \\(A1 + A2 + A3 + A4 + A5\\), and then dividing by 5. 4 In essence, the factor score we have calculated is based on equally weighted scores from each of the included variables/itmes. We can do this in jamovi in two steps:\n\nRecode A1 into “A1R” by reverse scoring the values in the variable (i.e. \\(6 = 1\\); \\(5 = 2\\); \\(4 = 3\\); \\(3 = 4\\); \\(2 = 5\\); \\(1 = 6\\)) using the jamovi transform variable command (see Figure 15.8).\nCompute a new variable, called “Agreeableness’, by calculating the mean of A1R, A2, A3, A4 and A5. Do this using the jamovi compute new variable command (see Figure 15.9).\n\n\n\n\n\n\nFigure 15.8: Recode variable using the jamovi Transform command\n\n\n\n\n\n\n\n\n\nFigure 15.9: Compute new scale score variable using the jamovi Computed variable command\n\n\n\n\nAnother option is to create an optimally-weighted factor score index. To do this, save the factor scores to the data set, using the ‘Save’ - ‘Factor scores’ checkbox. Once you have done this you will see that five new variables (columns) have been added to the data, one for each factor extracted. See Figure 15.10 and Figure 15.11.\n\n\n\n\n\nFigure 15.10: jamovi option for factor scores for the five factor solution, using the ‘Bartlett’ optimal weighting method\n\n\n\n\n\n\n\n\n\nFigure 15.11: Data sheet view showing the five newly created factor score variables\n\n\n\n\nNow you can go ahead and undertake further analyses, using either the mean score based factor scales (e.g. as in Figure 15.9) or using the optimally-weighted factor scores calculated by jamovi. Your choice! For example, one thing you might like to do is see whether there are any gender differences in each of our personality scales. We did this for the Agreeableness score that we calculated using the mean score approach, and although the t test plot (Figure 15.12) showed that males were less agreeable than females, this was not a significant difference (Mann-Whitney \\(U = 5768\\), \\(p = .075\\)).\n\n\n\n\n\nFigure 15.12: Comparing differences in Agreeableness factor-based scores between males and females\n\n\n\n\n\n\n15.1.4 Writing up an EFA\nHopefully, so far we have given you some sense of EFA and how to undertake EFA in jamovi. So, once you have completed your EFA, how do you write it up? There is not a formal standard way to write up an EFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nWhat are the theoretical underpinnings for the area you are studying, and specifically for the constructs that you are interested in uncovering through EFA.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nDescribe how you went about testing the assumptions for EFA. Details regarding sphericity checks and measures of sampling adequacy should be reported.\nExplain what FA extraction method (e.g. ‘Minimum residuals’ or ‘Maximum likelihood’) was used.\nExplain the criteria and process used for deciding how many factors were extracted in the final solution, and which items were selected. Clearly explain the rationale for key decisions during the EFA process.\nExplain what rotation methods were attempted, the reasons why, and the results.\nFinal factor loadings should be reported in the results, in a table. This table should also report the uniqueness (or communality) for each variable (in the final column). Factor loadings should be reported with descriptive labels in addition to item numbers. Correlations between the factors should also be included, either at the bottom of this table, in a separate table.\nMeaningful names for the extracted factors should be provided. You may like to use previously selected factor names, but on examining the actual items and factors you may think a different name is more appropriate"
  },
  {
    "objectID": "15-Factor-Analysis.html#principal-component-analysis",
    "href": "15-Factor-Analysis.html#principal-component-analysis",
    "title": "15  因素分析",
    "section": "15.2 Principal Component Analysis",
    "text": "15.2 Principal Component Analysis\nIn the previous section we saw that EFA works to identify underlying latent factors. And, as we saw, in one scenario the smaller number of latent factors can be used in further statistical analysis using some sort of combined factor scores.\nIn this way EFA is being used as a “data reduction” technique. Another type of data reduction technique, sometimes seen as part of the EFA family, is principal component analysis (PCA) . However, PCA does not identify underlying latent factors. Instead it creates a linear composite score from a larger set of measured variables.\nPCA simply produces a mathematical transformation to the original data with no assumptions about how the variables co-vary. The aim of PCA is to calculate a few linear combinations (components) of the original variables that can be used to summarize the observed data set without losing much information. However, if identification of underlying structure is a goal of the analysis, then EFA is to be preferred. And, as we saw, EFA produces factor scores that can be used for data reduction purposes just like principal component scores (Fabrigar et al., 1999).\nPCA has been popular in Psychology for a number of reasons, and therefore it’s worth mentioning, although nowadays EFA is just as easy to do given the power of desktop computers and can be less susceptible to bias than PCA, especially with a small number of factors and variables. Much of the procedure is similar to EFA, so although there are some conceptual differences, practically the steps are the same, and with large samples and a sufficient number of factors and variables, the results from PCA and EFA should be fairly similar.\nTo undertake PCA in jamovi, all you need to do is select ‘Factor’ - ‘Principal Component Analysis’ from the main jamovi button bar to open the PCA analysis window. Then you can follow the same steps from EFA in jamovi above."
  },
  {
    "objectID": "15-Factor-Analysis.html#confirmatory-factor-analysis",
    "href": "15-Factor-Analysis.html#confirmatory-factor-analysis",
    "title": "15  因素分析",
    "section": "15.3 Confirmatory Factor Analysis",
    "text": "15.3 Confirmatory Factor Analysis\nSo, our attempt to identify underlying latent factors using EFA with carefully selected questions from the personality item pool seemed to be pretty successful. The next step in our quest to develop a useful measure of personality is to check the latent factors we identified in the original EFA with a different sample. We want to see if the factors hold up, if we can confirm their existence with different data. This is a more rigorous check, as we will see. And it’s called Confirmatory Factor Analysis (CFA) as we will, unsurprisingly, be seeking to confirm a pre-specified latent factor structure.5\nIn CFA, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.13, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the personality items would therefore specify five latent factors as shown in Figure 15.13, each measured by five observed variables. Each variable is a measure of an underlying latent factor. For example, A1 is predicted by the underlying latent factor Agreeableness. And because A1 is not a perfect measure of the Agreeableness factor, there is an error term, \\(e\\), associated with it. In other words, \\(e\\) represents the variance in A1 that is not accounted for by the Agreeableness factor. This is sometimes called measurement error.\n\n\n\n\n\nFigure 15.13: Initial pre-specification of latent factor structure for the five factor personality scales, for use in CFA\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that some of our personality factors may be correlated with each other. So, in our model, we should allow these latent factors to co-vary, as shown by the double-headed arrows in Figure 15.13.\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. One reason for this might be that there is a shared methodological feature for particular sub-sets of the observed variables such that the observed variables might be correlated for methodological rather than substantive latent factor reasons. We’ll return to this possibility in a later section but, for now, there are no clear reasons that we can see that would justify correlating some of the error terms with each other\nWithout any correlated error terms, the model we are testing to see how well it fits with our observed data is just as specified in Figure 15.13. Only parameters that are included in the model are expected to be found in the data, so in CFA all other possible parameters (coefficients) are set to zero. So, if these other parameters are not zero (for example there may be a substantial loading from A1 onto the latent factor Extraversion in the observed data, but not in our model) then we may find a poor fit between our model and the observed data.\nRight, let’s take a look at how we set this CFA analysis up in jamovi.\n\n15.3.1 CFA in jamovi\nOpen up the bfi_sample2.csv file, check that the 25 variables are coded as ordinal (or continuous; it won’t make any difference for this analysis). To perform CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.14).\nSelect the 5 A variables and transfer them into the ‘Factors’ box and give then the label “Agreeableness”.\nCreate a new Factor in the ‘Factors’ box and label it “Conscientiousness”. Select the 5 C variables and transfer them into the ‘Factors’ box under the “Conscientiousness” label.\nCreate another new Factor in the ‘Factors’ box and label it “Extraversion”. Select the 5 E variables and transfer them into the ‘Factors’ box under the “Extraversion” label.\nCreate another new Factor in the ‘Factors’ box and label it “Neuroticism”. Select the 5 N variables and transfer them into the ‘Factors’ box under the “Neuroticism” label.\nCreate another new Factor in the ‘Factors’ box and label it “Openness”. Select the 5 O variables and transfer them into the ‘Factors’ box under the “Openness” label.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.13.\n\n\n\n\n\n\nFigure 15.14: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is model fit (Figure 15.15) as this tells us how good a fit our model is to the observed data. NB in our model only the pre-specified covariances are estimated, including the factor correlations by default. Everything else is set to zero.\n\n\n\n\n\nFigure 15.15: The jamovi CFA Model Fit results for our CFA model\n\n\n\n\nThere are several ways of assessing model fit. The first is a chi-square statistic that, if small, indicates that the model is a good fit to the data. However, the chi-squared statistic used for assessing model fit is pretty sensitive to sample size, meaning that with a large sample a good enough fit between the model and the data almost always produces a large and significant (p < .05) chi-square value.\nSo, we need some other ways of assessing model fit. In jamovi several are provided by default. These are the Comparative Fit Index (CFI), the Tucker Lewis Index (TLI) and the Root Mean Square Error of Approximation (RMSEA) together with the 90% confidence interval for the RMSEA. Some useful rules of thumb are that a satisfactory fit is indicated by CFI > 0.9, TLI > 0.9, and RMSEA of about 0.05 to 0.08. A good fit is CFI > 0.95, TLI > 0.95, and RMSEA and upper CI for RMSEA < 0.05.\nSo, looking at Figure 15.15 we can see that the chi-square value is large and highly significant. Our sample size is not too large, so this possibly indicates a poor fit. The CFI is \\(0.762\\) and the TLI is 0.731, indicating poor fit between the model and the data. The RMSEA is \\(0.085\\) with a \\(90\\%\\) confidence interval from \\(0.077\\) to \\(0.092\\), again this does not indicate a good fit.\nPretty disappointing, huh? But perhaps not too surprising given that in the earlier EFA, when we ran with a similar data set (see Exploratory Factor Analysis section), only around half of the variance in the data was accounted for by the five factor model.\nLet’s go on to look at the factor loadings and the factor covariance estimates, shown in Figure 15.16 and Figure 15.17. The Z-statistic and p-value for each of these parameters indicates they make a reasonable contribution to the model (i.e. they are not zero) so there doesn’t appear to be any reason to remove any of the specified variable-factor paths, or factor-factor correlations from the model. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.16: The jamovi CFA Factor Loadings table for our CFA model\n\n\n\n\n\n\n\n\n\nFigure 15.17: The jamovi CFA Factor Covariances table for our CFA model\n\n\n\n\nHow could we improve the model? One option is to go back a few stages and think again about the items / measures we are using and how they might be improved or changed. Another option is to make some post hoc tweaks to the model to improve the fit. One way of doing this is to use “modification indices” (Figure 15.18), specified as an ‘Additional output’ option in jamovi.\n\n\n\n\n\nFigure 15.18: The jamovi CFA Factor Loadings Modification Indices\n\n\n\n\nWhat we are looking for is the highest modification index (MI) value. We would then judge whether it makes sense to add that additional term into the model, using a post hoc rationalisation. For example, we can see in Figure 15.18 that the largest MI for the factor loadings that are not already in the model is a value of 28.786 for the loading of N4 (“Often feel blue”) onto the latent factor Extraversion. This indicates that if we add this path into the model then the chi-square value will reduce by around the same amount.\nBut in our model adding this path arguably doesn’t really make any theoretical or methodological sense, so it’s not a good idea (unless you can come up with a persuasive argument that “Often feel blue” measures both Neuroticism and Extraversion). I can’t think of a good reason. But, for the sake of argument, let’s pretend it does make some sense and add this path into the model. Go back to the CFA analysis window (see Figure 15.14) and add N4 into the Extraversion factor. The results of the CFA will now change (not shown); the chi-square has come down to around 709 (a drop of around 30, roughly similar to the size of the MI) and the other fit indices have also improved, though only a bit. But it’s not enough: it’s still not a good fitting model.\nIf you do find yourself adding new parameters to a model using the MI values then always re-check the MI tables after each new addition, as the MIs are refreshed each time.\nThere is also a Table of Residual Covariance Modification Indices produced by jamovi (Figure 15.19). In other words, a table showing which correlated errors, if added to the model, would improve the model fit the most. It’s a good idea to look across both MI tables at the same time, spot the largest MI, think about whether the addition of the suggested parameter can be reasonably justified and, if it can, add it to the model. And then you can start again looking for the biggest MI in the re-calculated results.\n\n\n\n\n\nFigure 15.19: Residual Covariance Modification Indices produced by jamovi\n\n\n\n\nYou can keep going this way for as long as you like, adding parameters to the model based on the largest MI, and eventually you will achieve a satisfactory fit. But there will also be a strong possibility that in doing this you will have created a monster! A model that is ugly and deformed and doesn’t have any theoretical sense or purity. In other words, be very careful!\nSo far, we have checked out the factor structure obtained in the EFA using a second sample and CFA. Unfortunately, we didn’t find that the factor structure from the EFA was confirmed in the CFA, so it’s back to the drawing board as far as the development of this personality scale goes.\nAlthough we could have tweaked the CFA using modification indexes, there really were not any good reasons (that I could think of) for these suggested additional factor loadings or residual covariances to be included. However, sometimes there is a good reason for residuals to be allowed to co-vary (or correlate), and a good example of this is shown in the next section on Multi-Trait Multi-Method CFA. Before we do that, let’s cover how to report the results of a CFA.\n\n\n15.3.2 Reporting a CFA\nThere is not a formal standard way to write up a CFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nA theoretical and empirical justification for the hypothesized model.\nA complete description of how the model was specified (e.g. the indicator variables for each latent factor, covariances between latent variables, and any correlations between error terms). A path diagram, like the one in Figure 15.13 would be good to include.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nTests of assumptions and estimation method used.\nA description of missing data and how the missing data were handled.\nThe software and version used to fit the model.\nMeasures, and the criteria used, to judge model fit.\nAny alterations made to the original model based on model fit or modification indices.\nAll parameter estimates (i.e., loadings, error variances, latent (co)variances) and their standard errors, probably in a table."
  },
  {
    "objectID": "15-Factor-Analysis.html#multi-trait-multi-method-cfa",
    "href": "15-Factor-Analysis.html#multi-trait-multi-method-cfa",
    "title": "15  因素分析",
    "section": "15.4 Multi-Trait Multi-Method CFA",
    "text": "15.4 Multi-Trait Multi-Method CFA\nIn this section we’re going to consider how different measurement techniques or questions can be an important source of data variability, known as method variance. To do this, we’ll use another psychological data set, one that contains data on “attributional style”.\nThe Attributional Style Questionnaire (ASQ) was used (Hewitt et al., 2004) to collect psychological wellbeing data from young people in the United Kingdom and New Zealand. They measured attributional style for negative events, which is how people habitually explain the cause of bad things that happen to them (Peterson & Seligman, 1984). The attributional style questionnaire (ASQ) measures three aspects of attributional style:\n\nInternality is the extent to which a person believes that the cause of a bad event is due to his/her own actions.\nStability refers to the extent to which a person habitually believes the cause of a bad event is stable across time.\nGlobality refers to the extent to which a person habitually believes that the cause of a bad event in one area will affect other areas of their lives.\n\nThere are six hypothetical scenarios and for each scenario respondents answer a question aimed at (a) internality, (b) stability and (c) globality. So there are \\(6 \\times 3 = 18\\) items overall. See Figure 15.20 for more details.\n\n\n\n\n\nFigure 15.20: The Attributional Style Questionnaire (ASQ) for negative events\n\n\n\n\nResearchers are interested in checking their data to see whether there are some underlying latent factors that are measured reasonably well by the 18 observed variables in the ASQ.\nFirst, they try EFA with these 18 variables (not shown), but no matter how they extract or rotate, they can’t find a good factor solution. Their attempt to identify underlying latent factors in the Attributional Style Questionnaire (ASQ) proved fruitless. If you get results like this then either your theory is wrong (there is no underlying latent factor structure for attributional style, which is possible), the sample is not relevant (which is unlikely given the size and characteristics of this sample of young adults from the United Kingdom and New Zealand), or the analysis was not the right tool for the job. We’re going to look at this third possibility.\nRemember that there were three dimensions measured in the ASQ: Internality, Stability and Globality, each measured by six questions as shown in Figure 15.21.\nWhat if, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.21, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the ASQ would therefore specify three latent factors as shown in the columns of Figure 15.27, each measured by six observed variables.\n\n\n\n\n\nFigure 15.21: Six questions on the ASQ for each of the Internality, Stability and Globality dimensions\n\n\n\n\nWe could depict this as in the diagram in Figure 15.22, which shows that each variable is a measure of an underlying latent factor. For example INT1 is predicted by the underlying latent factor Internality. And because INT1 is not a perfect measure of the Internality factor, there is an error term, e1, associated with it. In other words, e1 represents the variance in INT1 that is not accounted for by the Internality factor. This is sometimes called “measurement error”.\n\n\n\n\n\nFigure 15.22: Initial pre-specification of latent factor structure for the ASQ\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that Internality, Stability, and Globality might be correlated with each other, so in our model we should allow these latent factors to co-vary, as shown in Figure 15.23.\n\n\n\n\n\nFigure 15.23: Final pre-specification of latent factor structure for the ASQ, including latent factor correlations, and shared method error term correlations for the observed variable INT1, STAB1 and GLOB1, in a CFA MTMM model. For clarity, other pre-specified error term correlations are not shown\n\n\n\n\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. Thinking back to the ASQ questions, there were three different sub-questions (a, b and c) for each main question (1-6). Q1 was about unsuccessful job hunting and it is plausible that this question has some distinctive artefactual or methodological aspects over and above the other questions (2-5), something to do with job hunting perhaps. Similarly, Q2 was about not helping a friend with a problem, and there may be some distinctive artefactual or methodological aspects to do with not helping a friend that is not present in the other questions (1, and 3-5).\nSo, as well as multiple factors, we also have multiple methodological features in the ASQ, where each of Questions 1-6 has a slightly different “method”, but each “method” is shared across the sub-questions a, b and c. In order to incorporate these different methodological features into the model we can specify that certain error terms are correlated with each other. For example, the errors associated with INT1, STAB1 and GLOB1 should be correlated with each other to reflect the distinct and shared methodological variance of Q1a, Q1b and Q1c. Looking at Figure 15.21, this means that as well as the latent factors represented by the columns, we will have correlated measurement errors for the variables in each row of the Table.\nWhilst a basic CFA model like the one shown in Figure 15.22 could be tested against our observed data, we have in fact come up with a more sophisticated model, as shown in the diagram in Figure 15.23. This more sophisticated CFA model is known as a Multi-Trait Multi-Method (MTMM) model, and it is the one we will test in jamovi.\n\n15.4.1 MTMM CFA in jamovi\nOpen up the ASQ.csv file and check that the 18 variables (six “Internality”, six “Stability” and six “Globality” variables) are specified as continuous variables.\nTo perform MTMM CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.24).\nSelect the 6 INT variables and transfer them into the ‘Factors’ box and give them the label “Internality”.\nCreate a new Factor in the ‘Factors’ box and label it “Stability”. Select the 6 STAB variables and transfer them into the ‘Factors’ box under the “Stability” label.\nCreate another new Factor in the ‘Factors’ box and label it “Globality”. Select the 6 GLOB variables and transfer them into the ‘Factors’ box under the “Globality” label.\nOpen up the Residual Covariances options, and for each of our pre-specified correlations move the associated variables across into the ‘Residual Covariances’ box on the right. For example, highlight both INT1 and STAB1 and then click the arrow to move these across. Now do the same for INT1 and GLOB1, for STAB1 and GLOB1, for INT2 and STAB2, for INT2 and GLOB2, for STAB2 and GLOB2, for INT3 and STAB3, and so on.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.23, and including all the error term correlations that we have added above.\n\n\n\n\n\n\nFigure 15.24: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is “Model fit” as this tells us how good a fit our model is to the observed data (Figure 15.25). NB in our model only the pre-specified covariances are estimated, everything else is set to zero, so model fit is testing both whether the pre-specified “free” parameters are not zero, and conversely whether the other relationships in the data – the ones we have not specified in the model – can be held at zero.\n\n\n\n\n\nFigure 15.25: The jamovi CFA Model Fit results for our CFA MTMM model\n\n\n\n\nLooking at Figure 15.25 we can see that the chi-square value is highly significant, which is not a surprise given the large sample size (N = 2748). The CFI is 0.98 and the TLI is also 0.98, indicating a very good fit. The RMSEA is 0.02 with a 90% confidence interval from 0.02 to 0.02 – pretty tight!\nOverall, I think we can be satisfied that our pre-specified model is a very good fit to the observed data, lending support to our MTMM model for the ASQ.\nWe can now go on to look at the factor loadings and the factor covariance estimates, as in Figure 15.26. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.26: The jamovi CFA Factor Loadings and Covariances tables for our CFA MTMM model\n\n\n\n\nYou can see from Figure 15.26 that all of our pre-specified factor loadings and factor covariances are significantly different from zero. In other words, they all seem to be making a useful contribution to the model.\nWe’ve been pretty lucky with this analysis, getting a very good fit on our first attempt!"
  },
  {
    "objectID": "15-Factor-Analysis.html#sec-Internal-consistency-reliability-analysis",
    "href": "15-Factor-Analysis.html#sec-Internal-consistency-reliability-analysis",
    "title": "15  因素分析",
    "section": "15.5 內部一致性信度分析",
    "text": "15.5 內部一致性信度分析\nAfter you have been through the process of initial scale development using EFA and CFA, you should have reached a stage where the scale holds up pretty well using CFA with different samples. One thing that you might also be interested in at this stage is to see how well the factors are measured using a scale that combines the observed variables.\nIn psychometrics we use reliability analysis to provide information about how consistently a scale measures a psychological construct (See earlier section on Section 2.3). Internal consistency is what we are concerned with here, and that refers to the consistency across all the individual items that make up a measurement scale. So, if we have \\(V1, V2, V3, V4\\) and \\(V5\\) as observed item variables, then we can calculate a statistic that tells us how internally consistent these items are in measuring the underlying construct.\nA popular statistic used to check the internal consistency of a scale is Cronbach’s alpha (Chronbach, 1951). Cronbach’s alpha is a measure of equivalence (whether different sets of scale items would give the same measurement outcomes). Equivalence is tested by dividing the scale items into two groups (a “split-half”) and seeing whether analysis of the two parts gives comparable results. Of course, there are many ways a set of items could be split, but if all possible splits are made then it is possible to produce a statistic that reflects the overall pattern of split-half coefficients. Cronbach’s alpha (\\(\\alpha\\)) is such a statistic: a function of all the split-half coefficients for a scale. If a set of items that measure a construct (e.g. an Extraversion scale) has an \\(\\alpha\\) of \\(0.80\\), then the proportion of error variance in the scale is \\(0.20\\). In other words, a scale with an \\(\\alpha\\) of \\(0.80\\) includes approximately 20% error.\nBUT, (and that’s a BIG “BUT”), Cronbach’s alpha is not a measure of unidimensionality (i.e. an indicator that a scale is measuring a single factor or construct rather than multiple related constructs). Scales that are multidimensional will cause alpha to be under-estimated if not assessed separately for each dimension, but high values for alpha are not necessarily indicators of unidimensionality. So, an \\(\\alpha\\) of 0.80 does not mean that 80% of a single underlying construct is accounted for. It could be that the 80% comes from more than one underlying construct. That’s why EFA and CFA are useful to do first.\nFurther, another feature of \\(\\alpha\\) is that it tends to be sample specific: it is not a characteristic of the scale, but rather a characteristic of the sample in which the scale has been used. A biased, unrepresentative, or small sample could produce a very different \\(\\alpha\\) coefficient than a large, representative sample. \\(\\alpha\\) can even vary from large sample to large sample. Nevertheless, despite these limitations, Cronbach’s \\(\\alpha\\) has been popular in Psychology for estimating internal consistency reliability. It’s pretty easy to calculate, understand and interpret, and therefore it can be a useful initial check on scale performance when you administer a scale with a different sample, from a different setting or population, for example.\nAn alternative is McDonald’s omega (\\(\\omega\\)), and jamovi also provides this statistic. Whereas \\(\\alpha\\) makes the following assumptions: (a) no residual correlations, (b) items have identical loadings, and (c) the scale is unidimensional, \\(\\omega\\) does not and is therefore a more robust reliability statistic. If these assumptions are not violated then \\(\\alpha\\) and \\(\\omega\\) will be similar, but if they are then \\(\\omega\\) is to be preferred.\nSometimes a threshold for \\(\\alpha\\) or \\(\\omega\\) is provided, suggesting a “good enough” value. This might be something like \\(\\alpha\\)s of \\(0.70\\) or \\(0.80\\) representing “acceptable” and “good” reliability, respectively. However, this does depend on what exactly the scale is supposed to be measuring, so thresholds like this should be used cautiously. It could be better to simply state that an \\(\\alpha\\) or \\(\\omega\\) of \\(0.70\\) is associated with 30% error variance in a scale, and an \\(\\alpha\\) or \\(\\omega\\) of \\(0.80\\) is associated with 20%.\nCan \\(\\alpha\\) be too high? Probably: if you are getting an \\(\\alpha\\) coefficient above \\(0.95\\) then this indicates high inter-correlations between the items and that there might be too much overly redundant specificity in the measurement, with a risk that the construct being measured is perhaps overly narrow.\n\n15.5.1 使用jamovi完成內部一致性信度分析\nWe have a third sample of personality data to use to undertake reliability analysis: in the bfi_sample3.csv file. Once again, check that the 25 personality item variables are coded as continuous. To perform reliability analysis in jamovi:\n\nSelect Factor - Reliability Analysis from the main jamovi button bar to open the reliability analysis window (Figure 15.27).\nSelect the 5 A variables and transfer them into the ‘Items’ box.\nUnder the “Reverse Scaled Items” option, select variable A1 in the “Normal Scaled Items” box and move it across to the “Reverse Scaled Items” box.\nCheck other appropriate options, as in Figure 15.27.\n\n\n\n\n\n\nFigure 15.27: The jamovi Reliability Analysis window\n\n\n\n\nOnce done, look across at the jamovi results window. You should see something like Figure 15.28. This tells us that the Cronbach’s \\(\\alpha\\) coefficient for the Agreeableness scale is 0.72. This means that just under 30% of the Agreeableness scale score is error variance. McDonald’s \\(\\omega\\) is also given, and this is 0.74, not much different from \\(\\alpha\\).\n\n\n\n\n\nFigure 15.28: The jamovi Reliability Analysis results for the Agreeableness factor\n\n\n\n\nWe can also check how \\(\\alpha\\) or \\(\\omega\\) can be improved if a specific item is dropped from the scale. For example, \\(\\alpha\\) would increase to 0.74 and \\(\\omega\\) to 0.75 if we dropped item A1. This isn’t a big increase, so probably not worth doing.\nThe process of calculating and checking scale statistics (\\(\\alpha\\) and \\(\\omega\\)) is the same for all the other scales, and they all had similar reliability estimates apart from Openness. For Openness, the amount of error variance in the Scale score is around 40%, which is high and indicates that Openness is substantially less consistent as a reliable measure of a personality attribute than the other personality scales."
  },
  {
    "objectID": "15-Factor-Analysis.html#summary",
    "href": "15-Factor-Analysis.html#summary",
    "title": "15  因素分析",
    "section": "15.6 Summary",
    "text": "15.6 Summary\nIn this chapter on factor analysis and related techniques we have introduced and demonstrated statistical analyses that assess the pattern of relationships in a data set. Specifically, we have covered:\n\nExploratory Factor Analysis (EFA). EFA is a statistical technique for identifying underlying latent factors in a data set. Each observed variable is conceptualised as representing the latent factor to some extent, indicated by a factor loading. Researchers also use EFA as a way of data reduction, i.e. identifying observed variables than can be combined into new factor variables for subsequent analysis.\nPrincipal Component Analysis (PCA) is a data reduction technique which, strictly speaking, does not identify underlying latent factors. Instead, PCA simply produces a linear combination of observed variables.\nConfirmatory Factor Analysis (CFA). Unlike EFA, with CFA you start with an idea - a model - of how the variables in your data are related to each other. You then test your model against the observed data and assess how good a fit the model is to the data.\nIn Multi-Trait Multi-Method CFA (MTMM CFA), both latent factor and method variance are included in the model in an approach that is useful when there are different methodological approaches used and therefore method variance is an important consideration.\nInternal consistency reliability analysis. This form of reliability analysis tests how consistently a scale measures a measurement (psychological) construct.\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "Prelude-Part-IV.html#邏輯推理的侷限",
    "href": "Prelude-Part-IV.html#邏輯推理的侷限",
    "title": "中場故事",
    "section": "邏輯推理的侷限",
    "text": "邏輯推理的侷限\n\nThe whole art of war consists in getting at what is on the other side of the hill, or, in other words, in learning what we do not know from what we do.\n- Arthur Wellesley, 1st Duke of Wellington\n\nI am told that quote above came about as a consequence of a carriage ride across the countryside.1 He and his companion, J. W. Croker, were playing a guessing game, each trying to predict what would be on the other side of each hill. In every case it turned out that Wellesley was right and Croker was wrong. Many years later when Wellesley was asked about the game he explained that “the whole art of war consists in getting at what is on the other side of the hill”. Indeed, war is not special in this respect. All of life is a guessing game of one form or another, and getting by on a day to day basis requires us to make good guesses. So let’s play a guessing game of our own.2\nSuppose you and I are observing the Wellesley-Croker competition and after every three hills you and I have to predict who will win the next one, Wellesley or Croker. Let’s say that W refers to a Wellesley victory and C refers to a Croker victory. After three hills, our data set looks like this:\n\\(WWW\\)\nOur conversation goes like this:\n\nyou: Three in a row doesn’t mean much. I suppose Wellesley might be better at this than Croker, but it might just be luck. Still, I’m a bit of a gambler. I’ll bet on Wellesley.\n\n\nme: I agree that three in a row isn’t informative and I see no reason to prefer Wellesley’s guesses over Croker’s. I can’t justify betting at this stage. Sorry. No bet for me.\n\nYour gamble paid off: three more hills go by and Wellesley wins all three. Going into the next round of our game the score is 1-0 in favour of you and our data set looks like this: \\(WWW\\) \\(WWW\\) I’ve organised the data into blocks of three so that you can see which batch corresponds to the observations that we had available at each step in our little side game. After seeing this new batch, our conversation continues:\n\nyou: Six wins in a row for Duke Wellesley. This is starting to feel a bit suspicious. I’m still not certain, but I reckon that he’s going to win the next one too.\n\n\nme: I guess I don’t see that. Sure, I agree that Wellesley has won six in a row, but I don’t see any logical reason why that means he’ll win the seventh one. No bet. you: Do you really think so? Fair enough, but my bet worked out last time and I’m okay with my choice.\n\nFor a second time you were right, and for a second time I was wrong. Wellesley wins the next three hills, extending his winning record against Croker to 9-0. The data set available to us is now this: \\(WWW\\) \\(WWW\\) \\(WWW\\) And our conversation goes like this:\n\nyou: Okay, this is pretty obvious. Wellesley is way better at this game. We both agree he’s going to win the next hill, right?\n\n\nme: Is there really any logical evidence for that? Before we started this game, there were lots of possibilities for the first 10 outcomes, and I had no idea which one to expect. \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\) was one possibility, but so was \\(WCC\\) \\(CWC\\) \\(WWC\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or even \\(CCC\\) \\(CCC\\) \\(CCC\\) \\(C\\). Because I had no idea what would happen so I’d have said they were all equally likely. I assume you would have too, right? I mean, that’s what it means to say you have “no idea”, isn’t it?\n\n\nyou: I suppose so.\n\n\nme: Well then, the observations we’ve made logically rule out all possibilities except two: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). Both of these are perfectly consistent with the evidence we’ve encountered so far, aren’t they?\n\n\nyou: Yes, of course they are. Where are you going with this? me: So what’s changed then? At the start of our game, you’d have agreed with me that these are equally plausible and none of the evidence that we’ve encountered has discriminated between these two possibilities. Therefore, both of these possibilities remain equally plausible and I see no logical reason to prefer one over the other. So yes, while I agree with you that Wellesley’s run of 9 wins in a row is remarkable, I can’t think of a good reason to think he’ll win the 10th hill. No bet.\n\n\nyou: I see your point, but I’m still willing to chance it. I’m betting on Wellesley.\n\nWellesley’s winning streak continues for the next three hills. The score in the Wellesley-Croker game is now 12-0, and the score in our game is now 3-0. As we approach the fourth round of our game, our data set is this: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) and the conversation continues:\n\nyou: Oh yeah! Three more wins for Wellesley and another victory for me. Admit it, I was right about him! I guess we’re both betting on Wellesley this time around, right?\n\n\nme: I don’t know what to think. I feel like we’re in the same situation we were in last round, and nothing much has changed. There are only two legitimate possibilities for a sequence of 13 hills that haven’t already been ruled out, \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). It’s just like I said last time. If all possible outcomes were equally sensible before the game started, shouldn’t these two be equally sensible now given that our observations don’t rule out either one? I agree that it feels like Wellesley is on an amazing winning streak, but where’s the logical evidence that the streak will continue?\n\n\nyou: I think you’re being unreasonable. Why not take a look at our scorecard, if you need evidence? You’re the expert on statistics and you’ve been using this fancy logical analysis, but the fact is you’re losing. I’m just relying on common sense and I’m winning. Maybe you should switch strategies.\n\n\nme: Hmm, that is a good point and I don’t want to lose the game, but I’m afraid I don’t see any logical evidence that your strategy is better than mine. It seems to me that if there were someone else watching our game, what they’d have observed is a run of three wins to you. Their data would look like this: \\(YYY\\). Logically, I don’t see that this is any different to our first round of watching Wellesley and Croker. Three wins to you doesn’t seem like a lot of evidence, and I see no reason to think that your strategy is working out any better than mine. If I didn’t think that \\(WWW\\) was good evidence then for Wellesley being better than Croker at their game, surely I have no reason now to think that YYY is good evidence that you’re better at ours?\n\n\nyou: Okay, now I think you’re being a jerk.\n\n\nme: I don’t see the logical evidence for that."
  },
  {
    "objectID": "Prelude-Part-IV.html#學習統計不需要預設條件嗎",
    "href": "Prelude-Part-IV.html#學習統計不需要預設條件嗎",
    "title": "中場故事",
    "section": "學習統計不需要預設條件嗎",
    "text": "學習統計不需要預設條件嗎\nThere are lots of different ways in which we could dissect this dialogue, but since this is a statistics book pitched at psychologists and not an introduction to the philosophy and psychology of reasoning, I’ll keep it brief. What I’ve described above is sometimes referred to as 歸納之謎the riddle of induction. It seems entirely reasonable to think that a 12-0 winning record by Wellesley is pretty strong evidence that he will win the 13th game, but it is not easy to provide a proper logical justification for this belief. On the contrary, despite the obviousness of the answer, it’s not actually possible to justify betting on Wellesley without relying on some assumption that you don’t have any logical justification for.\nThe riddle of induction is most associated with the philosophical work of David Hume and more recently 納爾遜·古德曼(Nelson Goodman), but you can find examples of the problem popping up in fields as diverse as literature 愛麗絲夢遊仙境路易斯·卡羅(Lewis Carroll) and machine learning 沒有免費的午餐定理(the “no free lunch” theorem). There really is something weird about trying to “learn what we do not know from what we do know”. The critical point is that assumptions3 and biases are unavoidable if you want to learn anything about the world. There is no escape from this, and it is just as true for statistical inference as it is for human reasoning. In the dialogue I was taking aim at your perfectly sensible inferences as a human being, but the common sense reasoning that you relied on is no different to what a statistician would have done. Your “common sense” half of the dialog relied on an implicit assumption that there exists some difference in skill between Wellesley and Croker, and what you were doing was trying to work out what that difference in skill level would be. My “logical analysis” rejects that assumption entirely. All I was willing to accept is that there are sequences of wins and losses and that I did not know which sequences would be observed. Throughout the dialogue I kept insisting that all logically possible data sets were equally plausible at the start of the Wellesely-Croker game, and the only way in which I ever revised my beliefs was to eliminate those possibilities that were factually inconsistent with the observations.\nThat sounds perfectly sensible on its own terms. In fact, it even sounds like the hallmark of good deductive reasoning. Like Sherlock Holmes, my approach was to rule out that which is impossible in the hope that what would be left is the truth. Yet as we saw, ruling out the impossible never led me to make a prediction. On its own terms everything I said in my half of the dialogue was entirely correct. An inability to make any predictions is the logical consequence of making “no assumptions”. In the end I lost our game because you did make some assumptions and those assumptions turned out to be right. Skill is a real thing, and because you believed in the existence of skill you were able to learn that Wellesley had more of it than Croker. Had you relied on a less sensible assumption to drive your learning you might not have won the game.\nUltimately there are two things you should take away from this. First, as I’ve said, you cannot avoid making assumptions if you want to learn anything from your data. But second, once you realise that assumptions are necessary it becomes important to make sure you make the right ones! A data analysis that relies on few assumptions is not necessarily better than one that makes many assumptions, it all depends on whether those assumptions are good ones for your data. As we go through the rest of this book I’ll often point out the assumptions that underpin a particular statistical technique, and how you can check whether those assumptions are sensible."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "href": "14-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.1 平衡且無交互作用的因子設計分析",
    "text": "14.1 平衡且無交互作用的因子設計分析\nWhen we discussed analysis of variance in Chapter 13, we assumed a fairly simple experimental design. Each person is in one of several groups and we want to know whether these groups have different mean scores on some outcome variable. In this section, I’ll discuss a broader class of experimental designs known as factorial designs, in which we have more than one grouping variable. I gave one example of how this kind of design might arise above. Another example appears in Chapter 13 in which we were looking at the effect of different drugs on the mood.gain experienced by each person. In that chapter we did find a significant effect of drug, but at the end of the chapter we also ran an analysis to see if there was an effect of therapy. We didn’t find one, but there’s something a bit worrying about trying to run two separate analyses trying to predict the same outcome. Maybe there actually is an effect of therapy on mood gain, but we couldn’t find it because it was being “hidden” by the effect of drug? In other words, we’re going to want to run a single analysis that includes both drug and therapy as predictors. For this analysis each person is cross-classified by the drug they were given (a factor with 3 levels) and what therapy they received (a factor with 2 levels). We refer to this as a \\(3 \\times 2\\) factorial design.\nIf we cross-tabulate drug by therapy, using the ‘Frequencies’ - ‘Contingency Tables’ analysis in jamovi (see Section 6.1), we get the table shown in Figure 14.1.\n\n\n\n\n\nFigure 14.1: jamovi contingency table of drug by therapy\n\n\n\n\nAs you can see, not only do we have participants corresponding to all possible combinations of the two factors, indicating that our design is completely crossed, it turns out that there are an equal number of people in each group. In other words, we have a balanced design. In this section I’ll talk about how to analyse data from balanced designs, since this is the simplest case. The story for unbalanced designs is quite tedious, so we’ll put it to one side for the moment.\n\n14.1.1 多因子設計是因應什麼樣的假設？\nLike one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about population means. So a sensible place to start would be to be explicit about what our hypotheses actually are. However, before we can even get to that point, it’s really useful to have some clean and simple notation to describe the population means. Because of the fact that observations are cross-classified in terms of two different factors, there are quite a lot of different means that one might be interested in. To see this, let’s start by thinking about all the different sample means that we can calculate for this kind of design. Firstly, there’s the obvious idea that we might be interested in this list of group means (Table 14.1).\n\n\n\n\nTable 14.1:  Group means for drug and therapy groups in the clinicaltrial.csv data \n\ndrugtherapymood.gain\n\nplacebono.therapy0.300000\n\nanxifreeno.therapy0.400000\n\njoyzepamno.therapy1.466667\n\nplaceboCBT0.600000\n\nanxifreeCBT1.033333\n\njoyzepamCBT1.500000\n\n\n\n\n\nNow, the next Table (Table 14.2) shows a list of the group means for all possible combinations of the two factors (e.g., people who received the placebo and no therapy, people who received the placebo while getting CBT, etc.). It is helpful to organise all these numbers, plus the marginal and grand means, into a single table which looks like this:\n\n\n\n\nTable 14.2:  Group and total means for drug and therapy groups in the clintrial.csv data \n\nno therapyCBTtotal\n\nplacebo0.300.600.45\n\nanxifree0.401.030.72\n\njoyzepam1.471.501.48\n\ntotal0.721.040.88\n\n\n\n\n\nNow, each of these different means is of course a sample statistic. It’s a quantity that pertains to the specific observations that we’ve made during our study. What we want to make inferences about are the corresponding population parameters. That is, the true means as they exist within some broader population. Those population means can also be organised into a similar table, but we’ll need a little mathematical notation to do so (Table 14.3). As usual, I’ll use the symbol \\(\\mu\\) to denote a population mean. However, because there are lots of different means, I’ll need to use subscripts to distinguish between them.\nHere’s how the notation works. Our table is defined in terms of two factors. Each row corresponds to a different level of Factor A (in this case drug), and each column corresponds to a different level of Factor B (in this case therapy). If we let R denote the number of rows in the table, and \\(C\\) denote the number of columns, we can refer to this as an \\(R \\times C\\) factorial ANOVA. In this case \\(R = 3\\) and \\(C = 2\\). We’ll use lowercase letters to refer to specific rows and columns, so \\(\\mu_{rc}\\) refers to the population mean associated with the \\(r\\)-th level of Factor \\(A\\) (i.e. row number \\(r\\)) and the \\(c\\)-th level of Factor B (column number c).1 So the population means are now written like in Table 14.1:\n\n\n\n\nTable 14.3:  Notation for population means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\n\ntotal\n\n\n\n\n\nOkay, what about the remaining entries? For instance, how should we describe the average mood gain across the entire (hypothetical) population of people who might be given Joyzepam in an experiment like this, regardless of whether they were in CBT? We use the “dot” notation to express this. In the case of Joyzepam, notice that we’re talking about the mean associated with the third row in the table. That is, we’re averaging across two cell means (i.e., \\(\\mu_{31}\\) and \\(\\mu_{32}\\)). The result of this averaging is referred to as a marginal mean, and would be denoted \\(\\mu_3.\\) in this case. The marginal mean for CBT corresponds to the population mean associated with the second column in the table, so we use the notation because it is the mean obtained by averaging (marginalising2) over both. So our full table of population means can be written down like in Table 14.4.\n\n\n\n\nTable 14.4:  Notation for population and total means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\\( \\mu_{1.} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\\( \\mu_{2.} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\\( \\mu_{3.} \\)\n\ntotal\\( \\mu_{.1} \\)\\( \\mu_{.2} \\)\\( \\mu_{..} \\)\n\n\n\n\n\nNow that we have this notation, it is straightforward to formulate and express some hypotheses. Let’s suppose that the goal is to find out two things. First, does the choice of drug have any effect on mood? And second, does CBT have any effect on mood? These aren’t the only hypotheses that we could formulate of course, and we’ll see a really important example of a different kind of hypothesis in the section [Factorial ANOVA 2: balanced designs, interactions allowed], but these are the two simplest hypotheses to test, and so we’ll start there. Consider the first test. If the drug has no effect then we would expect all of the row means to be identical, right? So that’s our null hypothesis. On the other hand, if the drug does matter then we should expect these row means to be different. Formally, we write down our null and alternative hypotheses in terms of the equality of marginal means:\n\\[\\text{Null hypothesis, } H_0 \\text{: row means are the same, i.e., } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{Alternative hypothesis, } H_1 \\text{: at least one row mean is different}\\]\nIt’s worth noting that these are exactly the same statistical hypotheses that we formed when we ran a one-way ANOVA on these data in Chapter 13. Back then I used the notation \\(\\mu \\times {P}\\) to refer to the mean mood gain for the placebo group, with \\(\\mu{A}\\) and \\(\\mu \\times {J}\\) corresponding to the group means for the two drugs, and the null hypothesis was \\(\\mu{P} = \\mu{A} = \\mu{J}\\) . So we’re actually talking about the same hypothesis, it’s just that the more complicated ANOVA requires more careful notation due to the presence of multiple grouping variables, so we’re now referring to this hypothesis as \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\) . However, as we’ll see shortly, although the hypothesis is identical the test of that hypothesis is subtly different due to the fact that we’re now acknowledging the existence of the second grouping variable.\nSpeaking of the other grouping variable, you won’t be surprised to discover that our second hypothesis test is formulated the same way. However, since we’re talking about the psychological therapy rather than drugs our null hypothesis now corresponds to the equality of the column means:\n\\[\\text{Null hypothesis, } H_0 \\text{: column means are the same, i.e., } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{Alternative hypothesis, } H_1 \\text{: column means are different, i.e., } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n14.1.2 使用jamovi完成多因子變異數分析\nThe null and alternative hypotheses that I described in the last section should seem awfully familiar. They’re basically the same as the hypotheses that we were testing in our simpler oneway ANOVAs in Chapter 13. So you’re probably expecting that the hypothesis tests that are used in factorial ANOVA will be essentially the same as the F-test from Chapter 13. You’re expecting to see references to sums of squares (SS), mean squares (MS), degrees of freedom (df), and finally an F-statistic that we can convert into a p-value, right? Well, you’re absolutely and completely right. So much so that I’m going to depart from my usual approach. Throughout this book, I’ve generally taken the approach of describing the logic (and to an extent the mathematics) that underpins a particular analysis first and only then introducing the analysis in jamovi. This time I’m going to do it the other way around and show you how to do it in jamovi first. The reason for doing this is that I want to highlight the similarities between the simple one-way ANOVA tool that we discussed in Chapter 13, and the more complicated approach that we’re going to use in this chapter.\nIf the data you’re trying to analyse correspond to a balanced factorial design then running your analysis of variance is easy. To see how easy it is, let’s start by reproducing the original analysis from Chapter 13. In case you’ve forgotten, for that analysis we were using only a single factor (i.e., drug) to predict our outcome variable (i.e., mood.gain), and we got the results shown in Figure 14.2.\n\n\n\n\n\nFigure 14.2: jamovi one way anova of mood.gain by drug\n\n\n\n\nNow, suppose I’m also curious to find out if therapy has a relationship to mood.gain. In light of what we’ve seen from our discussion of multiple regression in Chapter 12, you probably won’t be surprised that all we have to do is add therapy as a second ‘Fixed Factor’ in the analysis, see Figure 14.3.\n\n\n\n\n\nFigure 14.3: jamovi two way anova of mood.gain by drug and therapy\n\n\n\n\nThis output is pretty simple to read too. The first row of the table reports a between-group sum of squares (SS) value associated with the drug factor, along with a corresponding between-group df value. It also calculates a mean square value (MS), an F-statistic and a p-value. is also a row corresponding to the therapy factor and a row corresponding to the residuals (i.e., the within groups variation).\nNot only are all of the individual quantities pretty familiar, the relationships between these different quantities has remained unchanged, just like we saw with the original one-way ANOVA. Note that the mean square value is calculated by dividing \\(SS\\) by the corresponding \\(df\\). That is, it’s still true that\n\\[MS=\\frac{SS}{df}\\]\nregardless of whether we’re talking about drug, therapy or the residuals. To see this, let’s not worry about how the sums of squares values are calculated. Instead, let’s take it on faith that jamovi has calculated the \\(SS\\) values correctly, and try to verify that all the rest of the numbers make sense. First, note that for the drug factor, we divide \\(3.45\\) by \\(2\\) and end up with a mean square value of \\(1.73\\). For the therapy factor, there’s only 1 degree of freedom, so our calculations are even simpler: dividing \\(0.47\\) (the \\(SS\\) value) by 1 gives us an answer of \\(0.47\\) (the \\(MS\\) value).\nTurning to the F statistics and the p values, notice that we have two of each; one corresponding to the drug factor and the other corresponding to the therapy factor. Regardless of which one we’re talking about, the F statistic is calculated by dividing the mean square value associated with the factor by the mean square value associated with the residuals. If we use “A” as shorthand notation to refer to the first factor (factor A; in this case drug) and “R” as shorthand notation to refer to the residuals, then the F statistic associated with factor A is denoted FA, and is calculated as follows:\n\\[F_A=\\frac{MS_A}{MS_R}\\]\nand an equivalent formula exists for factor B (i.e., therapy). Note that this use of “R” to refer to residuals is a bit awkward, since we also used the letter R to refer to the number of rows in the table, but I’m only going to use “R” to mean residuals in the context of SSR and MSR, so hopefully this shouldn’t be confusing. Anyway, to apply this formula to the drugs factor we take the mean square of 1.73 and divide it by the residual mean square value of \\(0.07\\), which gives us an F-statistic of 26.15. The corresponding calculation for the therapy variable would be to divide \\(0.47\\) by \\(0.07\\) which gives \\(7.08\\) as the F-statistic. Not surprisingly, of course, these are the same values that jamovi has reported in the ANOVA table above.\nAlso in the ANOVA table is the calculation of the p values. Once again, there is nothing new here. For each of our two factors what we’re trying to do is test the null hypothesis that there is no relationship between the factor and the outcome variable (I’ll be a bit more precise about this later on). To that end, we’ve (apparently) followed a similar strategy to what we did in the one way ANOVA and have calculated an F-statistic for each of these hypotheses. To convert these to p values, all we need to do is note that the sampling distribution for the F statistic under the null hypothesis (that the factor in question is irrelevant) is an F distribution. Also note that the two degrees of freedom values are those corresponding to the factor and those corresponding to the residuals. For the drug factor we’re talking about an F distribution with 2 and 14 degrees of freedom (I’ll discuss degrees of freedom in more detail later). In contrast, for the therapy factor the sampling distribution is F with 1 and 14 degrees of freedom.\nAt this point, I hope you can see that the ANOVA table for this more complicated factorial analysis should be read in much the same way as the ANOVA table for the simpler one way analysis. In short, it’s telling us that the factorial ANOVA for our \\(3 \\times 2\\) design found a significant effect of drug (\\(F_{2,14} = 26.15, p < .001\\)) as well as a significant effect of therapy (\\(F_{1,14} = 7.08, p = .02\\)). Or, to use the more technically correct terminology, we would say that there are two main effects of drug and therapy. At the moment, it probably seems a bit redundant to refer to these as “main” effects, but it actually does make sense. Later on, we’re going to want to talk about the possibility of “interactions” between the two factors, and so we generally make a distinction between main effects and interaction effects.\n\n\n14.1.3 計算多因子變異數分析的平方差總和\nIn the previous section I had two goals. Firstly, to show you that the jamovi method needed to do factorial ANOVA is pretty much the same as what we used for a one way ANOVA. The only difference is the addition of a second factor. Secondly, I wanted to show you what the ANOVA table looks like in this case, so that you can see from the outset that the basic logic and structure behind factorial ANOVA is the same as that which underpins one way ANOVA. Try to hold onto that feeling. It’s genuinely true, insofar as factorial ANOVA is built in more or less the same way as the simpler one-way ANOVA model. It’s just that this feeling of familiarity starts to evaporate once you start digging into the details. Traditionally, this comforting sensation is replaced by an urge to hurl abuse at the authors of statistics textbooks.\nOkay, let’s start by looking at some of those details. The explanation that I gave in the last section illustrates the fact that the hypothesis tests for the main effects (of drug and therapy in this case) are F-tests, but what it doesn’t do is show you how the sum of squares (SS) values are calculated. Nor does it tell you explicitly how to calculate degrees of freedom (df values) though that’s a simple thing by comparison. Let’s assume for now that we have only two predictor variables, Factor A and Factor B. If we use Y to refer to the outcome variable, then we would use Yrci to refer to the outcome associated with the i-th member of group rc (i.e., level/row r for Factor A and level/column c for Factor B). Thus, if we use \\(\\bar{Y}\\) to refer to a sample mean, we can use the same notation as before to refer to group means, marginal means and grand means. That is, \\(\\bar{Y}_{rc}\\) is the sample mean associated with the rth level of Factor A and the cth level of Factor: \\(\\bar{Y}_{r.}\\) would be the marginal mean for the rth level of Factor A, \\(\\bar{Y}_{.c}\\) would be the marginal mean for the cth level of Factor B, and \\(\\bar{Y}_{..}\\) is the grand mean. In other words, our sample means can be organised into the same table as the population means. For our clinical trial data, that table is shown in Table 14.5.\n\n\n\n\nTable 14.5:  Notation for sample means for the clinical trial data \n\nno therapyCBTtotal\n\nplacebo\\( \\bar{Y}_{11} \\)\\( \\bar{Y}_{12} \\)\\( \\bar{Y}_{1.} \\)\n\nanxifree\\( \\bar{Y}_{21} \\)\\( \\bar{Y}_{22} \\)\\( \\bar{Y}_{2.} \\)\n\njoyzepam\\( \\bar{Y}_{31} \\)\\( \\bar{Y}_{32} \\)\\( \\bar{Y}_{3.} \\)\n\ntotal\\( \\bar{Y}_{.1} \\)\\( \\bar{Y}_{.2} \\)\\( \\bar{Y}_{..} \\)\n\n\n\n\n\nAnd if we look at the sample means that I showed earlier, we have \\(\\bar{Y}_{11} = 0.30\\), \\(\\bar{Y}_{12} = 0.60\\) etc. In our clinical trial example, the drugs factor has 3 levels and the therapy factor has 2 levels, and so what we’re trying to run is a \\(3 \\times 2\\) factorial ANOVA. However, we’ll be a little more general and say that Factor A (the row factor) has R levels and Factor B (the column factor) has C levels, and so what we’re running here is an \\(R \\times C\\) factorial ANOVA.\n[Additional technical detail 3]\n\n\n14.1.4 計算自由度的規則\nThe degrees of freedom are calculated in much the same way as for one-way ANOVA. For any given factor, the degrees of freedom is equal to the number of levels minus 1 (i.e., \\(R - 1\\) for the row variable Factor A, and \\(C - 1\\) for the column variable Factor B). So, for the drugs factor we obtain \\(df = 2\\), and for the therapy factor we obtain \\(df = 1\\). Later on, when we discuss the interpretation of ANOVA as a regression model (see Section 14.6), I’ll give a clearer statement of how we arrive at this number. But for the moment we can use the simple definition of degrees of freedom, namely that the degrees of freedom equals the number of quantities that are observed, minus the number of constraints. So, for the drugs factor, we observe 3 separate group means, but these are constrained by 1 grand mean, and therefore the degrees of freedom is 2. For the residuals, the logic is similar, but not quite the same. The total number of observations in our experiment is 18. The constraints correspond to 1 grand mean, the 2 additional group means that the drug factor introduces, and the 1 additional group mean that the the therapy factor introduces, and so our degrees of freedom is 14. As a formula, this is \\(N - 1 - (R - 1) - (C - 1)\\), which simplifies to \\(N - R - C + 1\\).\n\n\n14.1.5 多因子與單因子變異數分析\nNow that we’ve seen how a factorial ANOVA works, it’s worth taking a moment to compare it to the results of the one way analyses, because this will give us a really good sense of why it’s a good idea to run the factorial ANOVA. In Chapter 13 I ran a one-way ANOVA that looked to see if there are any differences between drugs, and a second one-way ANOVA to see if there were any differences between therapies. As we saw in the section Section 14.1.1, the null and alternative hypotheses tested by the one-way ANOVAs are in fact identical to the hypotheses tested by the factorial ANOVA. Looking even more carefully at the ANOVA tables, we can see that the sum of squares associated with the factors are identical in the two different analyses (3.45 for drug and 0.92 for therapy), as are the degrees of freedom (2 for drug, 1 for therapy). But they don’t give the same answers! Most notably, when we ran the one-way ANOVA for therapy in Section 13.9 we didn’t find a significant effect (the p-value was .21). However, when we look at the main effect of therapy within the context of the two-way ANOVA, we do get a significant effect (p = .019). The two analyses are clearly not the same.\nWhy does that happen? The answer lies in understanding how the residuals are calculated. Recall that the whole idea behind an F-test is to compare the variability that can be attributed to a particular factor with the variability that cannot be accounted for (the residuals). If you run a one-way ANOVA for therapy, and therefore ignore the effect of drug, the ANOVA will end up dumping all of the drug-induced variability into the residuals! This has the effect of making the data look more noisy than they really are, and the effect of therapy which is correctly found to be significant in the two-way ANOVA now becomes non-significant. If we ignore something that actually matters (e.g., drug) when trying to assess the contribution of something else (e.g., therapy) then our analysis will be distorted. Of course, it’s perfectly okay to ignore variables that are genuinely irrelevant to the phenomenon of interest. If we had recorded the colour of the walls, and that turned out to be a non-significant factor in a three-way ANOVA, it would be perfectly okay to disregard it and just report the simpler two-way ANOVA that doesn’t include this irrelevant factor. What you shouldn’t do is drop variables that actually make a difference!\n\n\n14.1.6 解讀多因子變異數分析的結果\nThe ANOVA model that we’ve been talking about so far covers a range of different patterns that we might observe in our data. For instance, in a two-way ANOVA design there are four possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter, and (d) neither A nor B matters. An example of each of these four possibilities is plotted in Figure 14.4."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "href": "14-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.2 平衡且有交互作用的因子設計分析",
    "text": "14.2 平衡且有交互作用的因子設計分析\nThe four patterns of data shown in Figure 14.4 are all quite realistic. There are a great many data sets that produce exactly those patterns. However, they are not the whole story and the ANOVA model that we have been talking about up to this point is not sufficient to fully account for a table of group means. Why not? Well, so far we have the ability to talk about the idea that drugs can influence mood, and therapy can influence mood, but no way of talking about the possibility of an interaction between the two. An interaction between \\(A\\) and \\(B\\) is said to occur whenever the effect of Factor \\(A\\) is different, depending on which level of Factor \\(B\\) we’re talking about. Several examples of an interaction effect with the context of a \\(2 \\times 2\\) ANOVA are shown in Figure 14.5. To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed by quite different physiological mechanisms. One consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed in the previous section does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means. In jamovi this is done via the ANOVA ‘Estimated Marginal Means’ option - just move drug and therapy across into the ‘Marginal Means’ box under ‘Term 1’. This should look something like Figure 14.6. Our main concern relates to the fact that the two lines aren’t parallel. The effect of CBT (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However, when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this effect real, or is this just random variation due to chance? Our original ANOVA cannot answer this question, because we make no allowances for the idea that interactions even exist! In this section, we’ll fix this problem.\n\n\n\n\n\nFigure 14.4: The four different outcomes for a \\(2 \\times 2\\) ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor\n\n\n\n\n\n\n\n\n\nFigure 14.5: Qualitatively different interactions for a \\(2 \\times 2\\) ANOVA\n\n\n\n\n\n\n\n\n\nFigure 14.6: jamovi screen showing how to generate a descriptive interaction plot in ANOVA using the clinical trial data\n\n\n\n\n\n14.2.1 交互作用代表什麼？\nThe key idea that we’re going to introduce in this section is that of an interaction effect. In the ANOVA model we have looked at so far there are only two factors involved in our model (i.e., drug and therapy). But when we add an interaction we add a new component to the model: the combination of drug and therapy. Intuitively, the idea behind an interaction effect is fairly simple. It just means that the effect of Factor A is different, depending on which level of Factor B we’re talking about. But what does that actually mean in terms of our data? The plot in Figure 14.5 depicts several different patterns that, although quite different to each other, would all count as an interaction effect. So it’s not entirely straightforward to translate this qualitative idea into something mathematical that a statistician can work with.\n[Additional technical detail 4]\n\n\n14.2.2 交互作用的自由度\nCalculating the degrees of freedom for the interaction is, once again, slightly trickier than the corresponding calculation for the main effects. To start with, let’s think about the ANOVA model as a whole. Once we include interaction effects in the model we’re allowing every single group to have a unique mean, \\(mu_{rc}\\). For an \\(R \\times C\\) factorial ANOVA, this means that there are \\(R \\times C\\) quantities of interest in the model and only the one constraint: all of the group means need to average out to the grand mean. So the model as a whole needs to have (\\(R \\times C\\)) - 1 degrees of freedom. But the main effect of Factor A has \\(R - 1\\) degrees of freedom, and the main effect of Factor B has \\(C - 1\\) degrees of freedom. This means that the degrees of freedom associated with the interaction is\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\nwhich is just the product of the degrees of freedom associated with the row factor and the column factor.\nWhat about the residual degrees of freedom? Because we’ve added interaction terms which absorb some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically, note that if the model with interaction has a total of \\((R \\times C) - 1\\), and there are \\(N\\) observations in your data set that are constrained to satisfy 1 grand mean, your residual degrees of freedom now become \\(N - (R \\times C) - 1 + 1\\), or just \\(N - (R \\times C)\\).\n\n\n14.2.3 使用jamovi完成多因子變異數分析\nAdding interaction terms to the ANOVA model in jamovi is straightforward. In fact it is more than straightforward because it is the default option for ANOVA. This means that when you specify an ANOVA with two factors, e.g. drug and therapy then the interaction component - drug \\(\\times\\) therapy - is added automatically to the model5. When we run the ANOVA with the interaction term included, then we get the results shown in Figure 14.7.\n\n\n\n\n\nFigure 14.7: Results for the full factorial model, including the interaction component drug \\(\\times\\) therapy\n\n\n\n\nAs it turns out, while we do have a significant main effect of drug (\\(F_{2,12} = 31.7, p < .001\\)) and therapy type (\\(F_{1,12} = 8.6, p = .013\\)), there is no significant interaction between the two (\\(F_{2,12} = 2.5, p = 0.125\\)).\n\n\n14.2.4 解讀分析結果\nThere’s a couple of very important things to consider when interpreting the results of factorial ANOVA. First, there’s the same issue that we had with one-way ANOVA, which is that if you obtain a significant main effect of (say) drug, it doesn’t tell you anything about which drugs are different to one another. To find that out, you need to run additional analyses. We’ll talk about some analyses that you can run in later Sections: [Different ways to specify contrasts] and [Post hoc tests]. The same is true for interaction effects. Knowing that there’s a significant interaction doesn’t tell you anything about what kind of interaction exists. Again, you’ll need to run additional analyses.\nSecondly, there’s a very peculiar interpretation issue that arises when you obtain a significant interaction effect but no corresponding main effect. This happens sometimes. For instance, in the crossover interaction shown in Figure 14.5 a, this is exactly what you’d find. In this case, neither of the main effects would be significant, but the interaction effect would be. This is a difficult situation to interpret, and people often get a bit confused about it. The general advice that statisticians like to give in this situation is that you shouldn’t pay much attention to the main effects when an interaction is present. The reason they say this is that, although the tests of the main effects are perfectly valid from a mathematical point of view, when there is a significant interaction effect the main effects rarely test interesting hypotheses. Recall from Section 14.1.1 that the null hypothesis for a main effect is that the marginal means are equal to each other, and that a marginal mean is formed by averaging across several different groups. But if you have a significant interaction effect then you know that the groups that comprise the marginal mean aren’t homogeneous, so it’s not really obvious why you would even care about those marginal means.\nHere’s what I mean. Again, let’s stick with a clinical example. Suppose that we had a \\(2 \\times 2\\) design comparing two different treatments for phobias (e.g., systematic desensitisation vs flooding), and two different anxiety reducing drugs (e.g., Anxifree vs Joyzepam). Now, suppose what we found was that Anxifree had no effect when desensitisation was the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty effective for the other treatment. This is a classic crossover interaction, and what we’d find when running the ANOVA is that there is no main effect of drug, but a significant interaction. Now, what does it actually mean to say that there’s no main effect? Well, it means that if we average over the two different psychological treatments, then the average effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When treating someone for phobias it is never the case that a person can be treated using an “average” of flooding and desensitisation. That doesn’t make a lot of sense. You either get one or the other. For one treatment one drug is effective, and for the other treatment the other drug is effective. The interaction is the important thing and the main effect is kind of irrelevant.\nThis sort of thing happens a lot. The main effect are tests of marginal means, and when an interaction is present we often find ourselves not being terribly interested in marginal means because they imply averaging over things that the interaction tells us shouldn’t be averaged! Of course, it’s not always the case that a main effect is meaningless when an interaction is present. Often you can get a big main effect and a very small interaction, in which case you can still say things like “drug A is generally more effective than drug B” (because there was a big effect of drug), but you’d need to modify it a bit by adding that “the difference in effectiveness was different for different psychological treatments”. In any case, the main point here is that whenever you get a significant interaction you should stop and think about what the main effect actually means in this context. Don’t automatically assume that the main effect is interesting."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#變異數分析的效果量",
    "href": "14-Factorial-ANOVA.html#變異數分析的效果量",
    "title": "14  多因子變異數分析",
    "section": "14.3 變異數分析的效果量",
    "text": "14.3 變異數分析的效果量\nThe effect size calculation for a factorial ANOVA is pretty similar to those used in one way ANOVA (see [Effect size] section). Specifically, we can use \\(\\eta^2\\) (eta-squared) as a simple way to measure how big the overall effect is for any particular term. As before, \\(\\eta^2\\) is defined by dividing the sum of squares associated with that term by the total sum of squares. For instance, to determine the size of the main effect of Factor A, we would use the following formula:\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\nAs before, this can be interpreted in much the same way as \\(R^2\\) in regression.6 It tells you the proportion of variance in the outcome variable that can be accounted for by the main effect of Factor A. It is therefore a number that ranges from 0 (no effect at all) to 1 (accounts for all of the variability in the outcome). Moreover, the sum of all the \\(\\eta^2\\) values, taken across all the terms in the model, will sum to the the total \\(R^2\\) for the ANOVA model. If, for instance, the ANOVA model fits perfectly (i.e., there is no within-groups variability at all!), the \\(\\eta^2\\) values will sum to 1. Of course, that rarely if ever happens in real life.\nHowever, when doing a factorial ANOVA, there is a second measure of effect size that people like to report, known as partial \\(\\eta^2\\). The idea behind partial \\(\\eta^2\\) (which is sometimes denoted \\(p^{\\eta^2}\\) or \\(\\eta_p^2\\)) is that, when measuring the effect size for a particular term (say, the main effect of Factor A), you want to deliberately ignore the other effects in the model (e.g., the main effect of Factor B). That is, you would pretend that the effect of all these other terms is zero, and then calculate what the \\(\\eta^2\\) value would have been. This is actually pretty easy to calculate. All you have to do is remove the sum of squares associated with the other terms from the denominator. In other words, if you want the partial \\(\\eta^2\\) for the main effect of Factor A, the denominator is just the sum of the SS values for Factor A and the residuals\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\nThis will always give you a larger number than \\(\\eta^2\\), which the cynic in me suspects accounts for the popularity of partial \\(\\eta^2\\). And once again you get a number between 0 and 1, where 0 represents no effect. However, it’s slightly trickier to interpret what a large partial \\(\\eta^2\\) value means. In particular, you can’t actually compare the partial \\(\\eta^2\\) values across terms! Suppose, for instance, there is no within-groups variability at all: if so, \\(SS_R = 0\\). What that means is that every term has a partial \\(\\eta^2\\) value of 1. But that doesn’t mean that all terms in your model are equally important, or indeed that they are equally large. All it mean is that all terms in your model have effect sizes that are large relative to the residual variation. It is not comparable across terms.\nTo see what I mean by this, it’s useful to see a concrete example. First, let’s have a look at the effect sizes for the original ANOVA (Table 14.6) without the interaction term, from Figure 14.3.\n\n\n\n\nTable 14.6:  Effect sizes when the interaction term is not included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.79\n\ntherapy0.100.34\n\n\n\n\n\nLooking at the \\(\\eta^2\\) values first, we see that drug accounts for 71% of the variance (i.e. \\(\\eta^2 = 0.71\\)) in mood.gain, whereas therapy only accounts for 10%. This leaves a total of 19% of the variation unaccounted for (i.e., the residuals constitute 19% of the variation in the outcome). Overall, this implies that we have a very large effect7 of drug and a modest effect of therapy.\nNow let’s look at the partial \\(\\eta^2\\) values, shown in Figure 14.3. Because the effect of therapy isn’t all that large, controlling for it doesn’t make much of a difference, so the partial \\(\\eta^2\\) for drug doesn’t increase very much, and we obtain a value of \\(p^{\\eta^2} = 0.79\\). In contrast, because the effect of drug was very large, controlling for it makes a big difference, and so when we calculate the partial \\(\\eta^2\\) for therapy you can see that it rises to \\(p^{\\eta^2} = 0.34\\). The question that we have to ask ourselves is, what do these partial \\(\\eta^2\\) values actually mean? The way I generally interpret the partial \\(\\eta^2\\) for the main effect of Factor A is to interpret it as a statement about a hypothetical experiment in which only Factor A was being varied. So, even though in this experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was varied, and the partial \\(\\eta^2\\) statistic tells you how much of the variance in the outcome variable you would expect to see accounted for in that experiment. However, it should be noted that this interpretation, like many things associated with main effects, doesn’t make a lot of sense when there is a large and significant interaction effect.\nSpeaking of interaction effects, Table 14.7 shows what we get when we calculate the effect sizes for the model that includes the interaction term, as in Figure 14.7. As you can see, the \\(\\eta^2\\) values for the main effects don’t change, but the partial \\(\\eta^2\\) values do:\n\n\n\n\nTable 14.7:  Effect sizes when the interaction term is included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.84\n\ntherapy0.100.42\n\ndrug*therapy0.060.29\n\n\n\n\n\n\n14.3.1 估計組間平均\nIn many situations you will find yourself wanting to report estimates of all the group means based on the results of your ANOVA, as well as confidence intervals associated with them. You can use the ‘Estimated Marginal Means’ option in the jamovi ANOVA analysis to do this, as in Figure 14.8. If the ANOVA that you have run is a saturated model (i.e., contains all possible main effects and all possible interaction effects) then the estimates of the group means are actually identical to the sample means, though the confidence intervals will use a pooled estimate of the standard errors rather than use a separate one for each group.\n\n\n\n\n\nFigure 14.8: jamovi screenshot showing the marginal means for the saturated model, i.e. including the interaction component, with the clinical trial data set\n\n\n\n\nIn the output we see that the estimated mean mood gain for the placebo group with no therapy was \\(0.300\\), with a \\(95\\%\\) confidence interval from \\(0.006\\) to \\(0.594\\). Note that these are not the same confidence intervals that you would get if you calculated them separately for each group, because of the fact that the ANOVA model assumes homogeneity of variance and therefore uses a pooled estimate of the standard deviation.\nWhen the model doesn’t contain the interaction term, then the estimated group means will be different from the sample means. Instead of reporting the sample mean, jamovi will calculate the value of the group means that would be expected on the basis of the marginal means (i.e., assuming no interaction). Using the notation we developed earlier, the estimate reported for µrc, the mean for level r on the (row) Factor A and level c on the (column) Factor B would be \\(\\mu_{..} + \\alpha_r + \\beta_c\\). If there are genuinely no interactions between the two factors, this is actually a better estimate of the population mean than the raw sample mean would be. Removing the interaction term from the model, via the ‘Model’ options in the jamovi ANOVA analysis, provides the marginal means for the analysis shown in Figure 14.9.\n\n\n\n\n\nFigure 14.9: jamovi screenshot showing the marginal means for the unsaturated model, i.e. without the interaction component, with the clinical trial data set"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "href": "14-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "title": "14  多因子變異數分析",
    "section": "14.4 檢核變異數分析的執行條件",
    "text": "14.4 檢核變異數分析的執行條件\nAs with one-way ANOVA, the key assumptions of factorial ANOVA are homogeneity of variance (all groups have the same standard deviation), normality of the residuals, and independence of the observations. The first two are things we can check for. The third is something that you need to assess yourself by asking if there are any special relationships between different observations, for example repeated measures where the independent variable is time so there is a relationship between the observations at time one and time two: observations at different time points are from the same people. Additionally, if you aren’t using a saturated model (e.g., if you’ve omitted the interaction terms) then you’re also assuming that the omitted terms aren’t important. Of course, you can check this last one by running an ANOVA with the omitted terms included and see if they’re significant, so that’s pretty easy. What about homogeneity of variance and normality of the residuals? As it turns out, these are pretty easy to check. It’s no different to the checks we did for a one-way ANOVA.\n\n14.4.1 變異數的同質性\nAs mentioned in Section 13.6.1 in the last chapter, it’s a good idea to visually inspect a plot of the standard deviations compared across different groups / categories, and also see if the Levene test is consistent with the visual inspection. The theory behind the Levene test was discussed in Section 13.6.1, so I won’t discuss it again. This test expects that you have a saturated model (i.e., including all of the relevant terms), because the test is primarily concerned with the within-group variance, and it doesn’t really make a lot of sense to calculate this any way other than with respect to the full model. The Levene test can be specified under the ANOVA ‘Assumption Checks’ - ‘Homogeneity Tests’ option in jamovi, with the result shown as in Figure 14.10. The fact that the Levene test is non-significant means that, providing it is consistent with a visual inspection of the plot of standard deviations, we can safely assume that the homogeneity of variance assumption is not violated.\n\n\n14.4.2 殘差的常態性\nAs with one-way ANOVA we can test for the normality of residuals in a straightforward fashion (see Section 13.6.4). Primarily though, it’s generally a good idea to examine the residuals graphically using a QQ plot. See Figure 14.10.\n\n\n\n\n\nFigure 14.10: Checking assumptions in an ANOVA model"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#各種多重比較方案",
    "href": "14-Factorial-ANOVA.html#各種多重比較方案",
    "title": "14  多因子變異數分析",
    "section": "14.7 各種多重比較方案",
    "text": "14.7 各種多重比較方案\nIn the previous section, I showed you a method for converting a factor into a collection of contrasts. In the method I showed you we specify a set of binary variables in which we defined a table like Table 14.11.\n\n\n\n\nTable 14.11:  Binary contrasts to discriminate between all three possible drugs \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\nEach row in the table corresponds to one of the factor levels, and each column corresponds to one of the contrasts. This table, which always has one more row than columns, has a special name. It is called a contrast matrix. However, there are lots of different ways to specify a contrast matrix. In this section I discuss a few of the standard contrast matrices that statisticians use and how you can use them in jamovi. If you’re planning to read the section on [Factorial ANOVA 3: unbalanced designs] later on, it’s worth reading this section carefully. If not, you can get away with skimming it, because the choice of contrasts doesn’t matter much for balanced designs.\n\n14.7.1 比較操作效果\nIn the particular kind of contrasts that I’ve described above, one level of the factor is special, and acts as a kind of “baseline” category (i.e., placebo in our example), against which the other two are defined. The name for these kinds of contrasts is treatment contrasts, also known as “dummy coding”. In this contrast each level of the factor is compared to a base reference level, and the base reference level is the value of the intercept.\nThe name reflects the fact that these contrasts are quite natural and sensible when one of the categories in your factor really is special because it actually does represent a baseline. That makes sense in our clinical trial example. The placebo condition corresponds to the situation where you don’t give people any real drugs, and so it’s special. The other two conditions are defined in relation to the placebo. In one case you replace the placebo with Anxifree, and in the other case your replace it with Joyzepam.\nThe table shown above is a matrix of treatment contrasts for a factor that has 3 levels. But suppose I want a matrix of treatment contrasts for a factor with 5 levels? You would set this out like Table 14.12.\n\n\n\n\nTable 14.12:  Matrix of treatment contrasts with 5 levels \n\nLevel2345\n\n10000\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nIn this example, the first contrast is level 2 compared with level 1, the second contrast is level 3 compared with level 1, and so on. Notice that, by default, the first level of the factor is always treated as the baseline category (i.e., it’s the one that has all zeros and doesn’t have an explicit contrast associated with it). In jamovi you can change which category is the first level of the factor by manipulating the order of the levels of the variable shown in the ‘Data Variable’ window (double click on the name of the variable in the spreadsheet column to bring up the ‘Data Variable’ view.\n\n\n14.7.2 Helmert 比較法\nTreatment contrasts are useful for a lot of situations. However, they make most sense in the situation when there really is a baseline category, and you want to assess all the other groups in relation to that one. In other situations, however, no such baseline category exists, and it may make more sense to compare each group to the mean of the other groups. This is where we meet Helmert contrasts, generated by the ‘helmert’ option in the jamovi ‘ANOVA’ - ‘Contrasts’ selection box. The idea behind Helmert contrasts is to compare each group to the mean of the “previous” ones. That is, the first contrast represents the difference between group 2 and group 1, the second contrast represents the difference between group 3 and the mean of groups 1 and 2, and so on. This translates to a contrast matrix that looks like Table 14.13 for a factor with five levels.\n\n\n\n\nTable 14.13:  Matrix of helmert contrasts with 5 levels \n\n1-1-1-1-1\n\n21-1-1-1\n\n302-1-1\n\n4003-1\n\n50004\n\n\n\n\n\nOne useful thing about Helmert contrasts is that every contrast sums to zero (i.e., all the columns sum to zero). This has the consequence that, when we interpret the ANOVA as a regression, the intercept term corresponds to the grand mean \\(\\mu_{..}\\) if we are using Helmert contrasts. Compare this to treatment contrasts, in which the intercept term corresponds to the group mean for the baseline category. This property can be very useful in some situations. It doesn’t matter very much if you have a balanced design, which we’ve been assuming so far, but it will turn out to be important later when we consider unbalanced designs. In fact, the main reason why I’ve even bothered to include this section is that contrasts become important if you want to understand unbalanced ANOVA.\n\n\n14.7.3 簡單比較\nThe third option that I should briefly mention are “sum to zero”總和至零 contrasts, called “Simple” contrasts in jamovi, which are used to construct pairwise comparisons between groups. Specifically, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the first group (Table 14.14).\n\n\n\n\nTable 14.14:  Matrix of ’sum-to’zero contrasts with 5 levels \n\n1-1-1-1-1\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nMuch like Helmert contrasts, we see that each column sums to zero, which means that the intercept term corresponds to the grand mean when ANOVA is treated as a regression model. When interpreting these contrasts, the thing to recognise is that each of these contrasts is a pairwise comparison between group 1 and one of the other four groups. Specifically, contrast 1 corresponds to a “group 2 minus group 1” comparison, contrast 2 corresponds to a “group 3 minus group 1” comparison, and so on.8\n\n\n14.7.4 jamovi的各種比較選項\njamovi also comes with a variety of options that can generate different kinds of contrasts in ANOVA. These can be found in the ‘Contrasts’ option in the main ANOVA analysis window, where the contrast types in Table 14.15 are listed:\n\n\n\n\nTable 14.15:  Contrasts types available in the jamovi ANOVA analysis \n\nContrast type\n\nDeviationCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\nSimpleLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\nDifferenceCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\nHelmertCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\nRepeatedCompares the mean of each level (except the last) to the mean of the subsequent level\n\nPolynomialCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#不平衡的因子設計分析",
    "href": "14-Factorial-ANOVA.html#不平衡的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.10 不平衡的因子設計分析",
    "text": "14.10 不平衡的因子設計分析\nFactorial ANOVA is a very handy thing to know about. It’s been one of the standard tools used to analyse experimental data for many decades, and you’ll find that you can’t read more than two or three papers in psychology without running into an ANOVA in there somewhere. However, there’s one huge difference between the ANOVAs that you’ll see in a lot of real scientific articles and the ANOVAs that I’ve described so far. In in real life we’re rarely lucky enough to have perfectly balanced designs. For one reason or another, it’s typical to end up with more observations in some cells than in others. Or, to put it another way, we have an unbalanced design.\nUnbalanced designs need to be treated with a lot more care than balanced designs, and the statistical theory that underpins them is a lot messier. It might be a consequence of this messiness, or it might be a shortage of time, but my experience has been that undergraduate research methods classes in psychology have a nasty tendency to ignore this issue completely. A lot of stats textbooks tend to gloss over it too. The net result of this, I think, is that a lot of active researchers in the field don’t actually know that there’s several different “types” of unbalanced ANOVAs, and they produce quite different answers. In fact, reading the psychological literature, I’m kind of amazed at the fact that most people who report the results of an unbalanced factorial ANOVA don’t actually give you enough details to reproduce the analysis. I secretly suspect that most people don’t even realise that their statistical software package is making a whole lot of substantive data analysis decisions on their behalf. It’s actually a little terrifying when you think about it. So, if you want to avoid handing control of your data analysis to stupid software, read on.\n\n14.10.1 咖啡飲用資料\nAs usual, it will help us to work with some data. The coffee.csv file contains a hypothetical data set that produces an unbalanced \\(3 \\times 2\\) ANOVA. Suppose we were interested in finding out whether or not the tendency of people to babble when they have too much coffee is purely an effect of the coffee itself, or whether there’s some effect of the milk and sugar that people add to the coffee. Suppose we took 18 people and gave them some coffee to drink. The amount of coffee / caffeine was held constant, and we varied whether or not milk was added, so milk is a binary factor with two levels, “yes” and “no”. We also varied the kind of sugar involved. The coffee might contain “real” sugar or it might contain “fake” sugar (i.e., artificial sweetener) or it might contain “none” at all, so the sugar variable is a three level factor. Our outcome variable is a continuous variable that presumably refers to some psychologically sensible measure of the extent to which someone is “babbling”. The details don’t really matter for our purpose. Take a look at the data in the jamovi spreadsheet view, as in Figure 14.26.\n\n\n\n\n\nFigure 14.26: The coffee.csv data set in jamovi, with descriptive information aggregated by factor levels\n\n\n\n\nLooking at the table of means in Figure 14.26 we get a strong impression that there are differences between the groups. This is especially true when we compare these means to the standard deviations for the babble variable. Across groups, this standard deviation varies from .14 to .71, which is fairly small relative to the differences in group means.10 Whilst this at first may seem like a straightforward factorial ANOVA, a problem arises when we look at how many observations we have in each group. See the different Ns for different groups shown in Figure 14.26. This violates one of our original assumptions, namely that the number of people in each group is the same. We haven’t really discussed how to handle this situation.\n\n\n14.10.2 不平衡設計不適用「標準變異數分析」\nUnbalanced designs lead us to the somewhat unsettling discovery that there isn’t really any one thing that we might refer to as a standard ANOVA. In fact, it turns out that there are three fundamentally different ways11 in which you might want to run an ANOVA in an unbalanced design. If you have a balanced design all three versions produce identical results, with the sums of squares, F-values, etc., all conforming to the formulas that I gave at the start of the chapter. However, when your design is unbalanced they don’t give the same answers. Furthermore, they are not all equally appropriate to every situation. Some methods will be more appropriate to your situation than others. Given all this, it’s important to understand what the different types of ANOVA are and how they differ from one another.\nThe first kind of ANOVA is conventionally referred to as Type I sum of squares. I’m sure you can guess what the other two are called. The “sum of squares” part of the name was introduced by the SAS statistical software package and has become standard nomenclature, but it’s a bit misleading in some ways. I think the logic for referring to them as different types of sum of squares is that, when you look at the ANOVA tables that they produce, the key difference in the numbers is the SS values. The degrees of freedom don’t change, the MS values are still defined as SS divided by df, etc. However, what the terminology gets wrong is that it hides the reason why the SS values are different from one another. To that end, it’s a lot more helpful to think of the three different kinds of ANOVA as three different hypothesis testing strategies. These different strategies lead to different SS values, to be sure, but it’s the strategy that is the important thing here, not the SS values themselves. Recall from the section [ANOVA as a linear model] that any particular F-test is best thought of as a comparison between two linear models. So, when you’re looking at an ANOVA table, it helps to remember that each of those F-tests corresponds to a pair of models that are being compared. Of course, this leads naturally to the question of which pair of models is being compared. This is the fundamental difference between ANOVA Types I, II and III: each one corresponds to a different way of choosing the model pairs for the tests.\n\n\n14.10.3 第一型平方差總和\nThe Type I method is sometimes referred to as the “sequential” sum of squares, because it involves a process of adding terms to the model one at a time. Consider the coffee data, for instance. Suppose we want to run the full \\(3 \\times 2\\) factorial ANOVA, including interaction terms. The full model contains the outcome variable babble, the predictor variables sugar and milk, and the interaction term sugar \\(\\times\\) milk. This can be written as \\(babble \\sim sugar + milk + sugar {\\times} milk\\). The Type I strategy builds this model up sequentially, starting from the simplest possible model and gradually adding terms.\nThe simplest possible model for the data would be one in which neither milk nor sugar is assumed to have any effect on babbling. The only term that would be included in such a model is the intercept, written as babble ~ 1. This is our initial null hypothesis. The next simplest model for the data would be one in which only one of the two main effects is included. In the coffee data, there are two different possible choices here, because we could choose to add milk first or to add sugar first. The order actually turns out to matter, as we’ll see later, but for now let’s just make a choice arbitrarily and pick sugar. So, the second model in our sequence of models is babble ~ sugar, and it forms the alternative hypothesis for our first test. We now have our first hypothesis test (Table 14.16).\n\n\n\n\nTable 14.16:  Null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim 1\\)\n\nAlternative model:\\(babble \\sim  sugar\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of sugar. The next step in our model building exercise is to add the other main effect term, so the next model in our sequence is babble ~ sugar + milk. The second hypothesis test is then formed by comparing the following pair of models (Table 14.17).\n\n\n\n\nTable 14.17:  Further null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar\\)\n\nAlternative model:\\(babble \\sim  sugar + milk\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of milk. In one sense, this approach is very elegant: the alternative hypothesis from the first test forms the null hypothesis for the second one. It is in this sense that the Type I method is strictly sequential. Every test builds directly on the results of the last one. However, in another sense it’s very inelegant, because there’s a strong asymmetry between the two tests. The test of the main effect of sugar (the first test) completely ignores milk, whereas the test of the main effect of milk (the second test) does take sugar into account. In any case, the fourth model in our sequence is now the full model, babble ~ sugar + milk + sugar \\(\\times\\) milk, and the corresponding hypothesis test is shown in Table 14.18.\n\n\n\n\nTable 14.18:  And more possible null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk + sugar * milk \\)\n\n\n\n\n\nType III sum of squares is the default hypothesis testing method used by jamovi ANOVA, so to run a Type I sum of squares analysis we have to select ‘Type 1’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options. This gives us the ANOVA table shown in Figure 14.27.\n\n\n\n\n\nFigure 14.27: ANOVA results table using Type I sum of squares in jamovi\n\n\n\n\nThe big problem with using Type I sum of squares is the fact that it really does depend on the order in which you enter the variables. Yet, in many situations the researcher has no reason to prefer one ordering over another. This is presumably the case for our milk and sugar problem. Should we add milk first or sugar first? It feels exactly as arbitrary as a data analysis question as it does as a coffee-making question. There may in fact be some people with firm opinions about ordering, but it’s hard to imagine a principled answer to the question. Yet, look what happens when we change the ordering, as in Figure 14.28.\n\n\n\n\n\nFigure 14.28: ANOVA results table using Type I sum of squares in jamovi, but with factors entered in a different order (milk first)\n\n\n\n\nThe p-values for both main effect terms have changed, and fairly dramatically. Among other things, the effect of milk has become significant (though one should avoid drawing any strong conclusions about this, as I’ve mentioned previously). Which of these two ANOVAs should one report? It’s not immediately obvious.\nWhen you look at the hypothesis tests that are used to define the “first” main effect and the “second” one, it’s clear that they’re qualitatively different from one another. In our initial example, we saw that the test for the main effect of sugar completely ignores milk, whereas the test of the main effect of milk does take sugar into account. As such, the Type I testing strategy really does treat the first main effect as if it had a kind of theoretical primacy over the second one. In my experience there is very rarely if ever any theoretically primacy of this kind that would justify treating any two main effects asymmetrically.\nThe consequence of all this is that Type I tests are very rarely of much interest, and so we should move on to discuss Type II tests and Type III tests.\n\n\n14.10.4 第三型平方差總和\nHaving just finished talking about Type I tests, you might think that the natural thing to do next would be to talk about Type II tests. However, I think it’s actually a bit more natural to discuss Type III tests (which are simple and the default in jamovi ANOVA) before talking about Type II tests (which are trickier). The basic idea behind Type III tests is extremely simple. Regardless of which term you’re trying to evaluate, run the F-test in which the alternative hypothesis corresponds to the full ANOVA model as specified by the user, and the null model just deletes that one term that you’re testing. For instance, in the coffee example, in which our full model was babble ~ sugar + milk + sugar \\(\\times\\) milk, the test for a main effect of sugar would correspond to a comparison between the following two models (Table 14.19).\n\n\n\n\nTable 14.19:  Null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  milk + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nSimilarly the main effect of milk is evaluated by testing the full model against a null model that removes the milk term, like in Table 14.20.\n\n\n\n\nTable 14.20:  Further null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nFinally, the interaction term sugar \\(\\times\\) milk is evaluated in exactly the same way. Once again, we test the full model against a null model that removes the sugar \\(\\times\\) milk interaction term, like in Table 14.21.\n\n\n\n\nTable 14.21:  Removing the interaction term from hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nThe basic idea generalises to higher order ANOVAs. For instance, suppose that we were trying to run an ANOVA with three factors, A, B and C, and we wanted to consider all possible main effects and all possible interactions, including the three way interaction A \\(\\times\\) B \\(\\times\\) C. (Table 14.22) shows you what the Type III tests look like for this situation).\n\n\n\n\nTable 14.22:  Type III tests with three factors and all main effect and interaction term \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB\\(A + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\nC\\(A + B + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B\\(A + B + C + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*C\\(A + B + C + A*B + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB*C\\(A + B + C + A*B + A*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nAs ugly as that table looks, it’s pretty simple. In all cases, the alternative hypothesis corresponds to the full model which contains three main effect terms (e.g. A), three two-way interactions (e.g. A*B) and one three-way interaction (i.e., A*B*C)). The null model always contains 6 of these 7 terms, and the missing one is the one whose significance we’re trying to test.\nAt first pass, Type III tests seem like a nice idea. Firstly, we’ve removed the asymmetry that caused us to have problems when running Type I tests. And because we’re now treating all terms the same way, the results of the hypothesis tests do not depend on the order in which we specify them. This is definitely a good thing. However, there is a big problem when interpreting the results of the tests, especially for main effect terms. Consider the coffee data. Suppose it turns out that the main effect of milk is not significant according to the Type III tests. What this is telling us is that babble ~ sugar + sugar*milk is a better model for the data than the full model. But what does that even mean? If the interaction term sugar*milk was also non significant, we’d be tempted to conclude that the data are telling us that the only thing that matters is sugar. But suppose we have a significant interaction term, but a non-significant main effect of milk. In this case, are we to assume that there really is an “effect of sugar”, an “interaction between milk and sugar”, but no “effect of milk”? That seems crazy. The right answer simply must be that it’s meaningless12 to talk about the main effect if the interaction is significant. In general, this seems to be what most statisticians advise us to do, and I think that’s the right advice. But if it really is meaningless to talk about non-significant main effects in the presence of a significant interaction, then it’s not at all obvious why Type III tests should allow the null hypothesis to rely on a model that includes the interaction but omits one of the main effects that make it up. When characterised in this fashion, the null hypotheses really don’t make much sense at all.\nLater on, we’ll see that Type III tests can be redeemed in some contexts, but first let’s take a look at the ANOVA results table using Type III sum of squares, see Figure 14.29.\n\n\n\n\n\nFigure 14.29: ANOVA results table using Type III sum of squares in jamovi\n\n\n\n\nBut be aware, one of the perverse features of the Type III testing strategy is that typically the results turn out to depend on the contrasts that you use to encode your factors (see the [Different ways to specify contrasts] section if you’ve forgotten what the different types of contrasts are).13\nOkay, so if the p-values that typically come out of Type III analyses (but not in jamovi) are so sensitive to the choice of contrasts, does that mean that Type III tests are essentially arbitrary and not to be trusted? To some extent that’s true, and when we turn to a discussion of Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\nIn Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\n\n\n14.10.5 第二型平方差總和\nOkay, so we’ve seen Type I and III tests now, and both are pretty straightforward. Type I tests are performed by gradually adding terms one at a time, whereas Type III tests are performed by taking the full model and looking to see what happens when you remove each term. However, both can have some limitations. Type I tests are dependent on the order in which you enter the terms, and Type III tests are dependent on how you code up your contrasts. Type II tests are a little harder to describe, but they avoid both of these problems, and as a result they are a little easier to interpret.\nType II tests are broadly similar to Type III tests. Start with a “full” model, and test a particular term by deleting it from that model. However, Type II tests are based on the marginality principle which states that you should not omit a lower order term from your model if there are any higher order ones that depend on it. So, for instance, if your model contains the two-way interaction A \\(\\times\\) B (a 2nd order term), then it really ought to contain the main effects A and B (1st order terms). Similarly, if it contains a three-way interaction term A \\(\\times\\) B \\(\\times\\) C, then the model must also include the main effects A, B and C as well as the simpler interactions A \\(\\times\\) B, A \\(\\times\\) C and B \\(\\times\\) C. Type III tests routinely violate the marginality principle. For instance, consider the test of the main effect of A in the context of a three-way ANOVA that includes all possible interaction terms. According to Type III tests, our null and alternative models are in Table 14.23.\n\n\n\n\nTable 14.23:  Type III tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\nNotice that the null hypothesis omits A, but includes A \\(\\times\\) B, A \\(\\times\\) C and A \\(\\times\\) B \\(\\times\\) C as part of the model. This, according to the Type II tests, is not a good choice of null hypothesis. What we should do instead, if we want to test the null hypothesis that A is not relevant to our outcome, is to specify the null hypothesis that is the most complicated model that does not rely on A in any form, even as an interaction. The alternative hypothesis corresponds to this null model plus a main effect term of A. This is a lot closer to what most people would intuitively think of as a “main effect of A”, and it yields the following as our Type II test of the main effect of A (Table 14.24). 14\n\n\n\n\nTable 14.24:  Type II tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\nAnyway, just to give you a sense of how the Type II tests play out, here’s the full table (Table 14.25) of tests that would be applied in a three-way factorial ANOVA:\n\n\n\n\nTable 14.25:  Type II tests for a three-way factorial model \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + B*C \\)\\(A + B + C + B*C \\)\n\nB\\(A + C + A*C \\)\\(A + B + C + A*C\\)\n\nC\\(A + B + A*B \\)\\(A + B + C + A*B\\)\n\nA*B\\(A + B + C + A*C + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*C\\(A + B + C + A*B + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nB*C\\(A + B + C + A*B + A*C \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nIn the context of the two way ANOVA that we’ve been using in the coffee data, the hypothesis tests are even simpler. The main effect of sugar corresponds to an F-test comparing these two models (Table 14.26).\n\n\n\n\nTable 14.26:  Type II tests for the main effect of sugar in the coffee data \n\nNull model:\\(babble \\sim milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nThe test for the main effect of milk is in Table 14.27.\n\n\n\n\nTable 14.27:  Type II tests for the main effect of milk in the coffee data \n\nNull model:\\(babble \\sim  sugar \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nFinally, the test for the interaction sugar \\(\\times\\) milk is in Table 14.28.\n\n\n\n\nTable 14.28:  Type II tests for the sugar x milk interaction term \n\nNull model:\\(babble \\sim  sugar + milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk  + sugar*milk \\)\n\n\n\n\n\nRunning the tests are again straightforward. Just select ‘Type 2’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options, This gives us the ANOVA table shown in Figure 14.30.\n\n\n\n\n\nFigure 14.30: ANOVA results table using Type II sum of squares in jamovi\n\n\n\n\nType II tests have some clear advantages over Type I and Type III tests. They don’t depend on the order in which you specify factors (unlike Type I), and they don’t depend on the contrasts that you use to specify your factors (unlike Type III). And although opinions may differ on this last point, and it will definitely depend on what you’re trying to do with your data, I do think that the hypothesis tests that they specify are more likely to correspond to something that you actually care about. As a consequence, I find that it’s usually easier to interpret the results of a Type II test than the results of a Type I or Type III test. For this reason my tentative advice is that, if you can’t think of any obvious model comparisons that directly map onto your research questions but you still want to run an ANOVA in an unbalanced design, Type II tests are probably a better choice than Type I or Type III.15\n\n\n14.10.6 效果量(還有非加成性平方差總和)\njamovi also provides the effect sizes \\(\\eta^2\\) and partial \\(\\eta^2\\) when you select these options, as in Figure 14.30. However, when you’ve got an unbalanced design there’s a bit of extra complexity involved.\nIf you remember back to our very early discussions of ANOVA, one of the key ideas behind the sums of squares calculations is that if we add up all the SS terms associated with the effects in the model, and add that to the residual SS, they’re supposed to add up to the total sum of squares. And, on top of that, the whole idea behind \\(\\eta^2\\) is that, because you’re dividing one of the SS terms by the total SS value, an \\(\\eta^2\\) value can be interpreted as the proportion of variance accounted for by a particular term. But this is not so straightforward in unbalanced designs because some of the variance goes “missing”.\nThis seems a bit odd at first, but here’s why. When you have unbalanced designs your factors become correlated with one another, and it becomes difficult to tell the difference between the effect of Factor A and the effect of Factor B. In the extreme case, suppose that we’d run a \\(2 \\times 2\\) design in which the number of participants in each group had been as in Table 14.29.\n\n\n\n\nTable 14.29:  N participants in a 2 x 2 very (very!) unbalanced factorial design \n\nsugarno sugar\n\nmilk1000\n\nno milk0100\n\n\n\n\n\nHere we have a spectacularly unbalanced design: 100 people have milk and sugar, 100 people have no milk and no sugar, and that’s all. There are 0 people with milk and no sugar, and 0 people with sugar but no milk. Now suppose that, when we collected the data, it turned out there is a large (and statistically significant) difference between the “milk and sugar” group and the “no-milk and no-sugar” group. Is this a main effect of sugar? A main effect of milk? Or an interaction? It’s impossible to tell, because the presence of sugar has a perfect association with the presence of milk. Now suppose the design had been a little more balanced (Table 14.30).\n\n\n\n\nTable 14.30:  N participants in a 2 x 2 still very unbalanced factorial design \n\nsugarno sugar\n\nmilk1005\n\nno milk5100\n\n\n\n\n\nThis time around, it’s technically possible to distinguish between the effect of milk and the effect of sugar, because we have a few people that have one but not the other. However, it will still be pretty difficult to do so, because the association between sugar and milk is still extremely strong, and there are so few observations in two of the groups. Again, we’re very likely to be in the situation where we know that the predictor variables (milk and sugar) are related to the outcome (babbling), but we don’t know if the nature of that relationship is a main effect of one or the other predictor, or the interaction."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#本章小結",
    "href": "14-Factorial-ANOVA.html#本章小結",
    "title": "14  多因子變異數分析",
    "section": "14.11 本章小結",
    "text": "14.11 本章小結\n\n平衡且無交互作用的因子設計分析以及有交互作用因子設計分析\n因子設計變異數分析的效果量估計平均值以及信賴區間。\n檢核變異數分析的執行條件\n共變數分析 (ANCOVA)\n變異數分析就是線性模型還有各種多重比較方案\n事後檢定談到杜凱氏HSD，還有提到規劃使用事前檢定方法要思考的條件。\n不平衡的因子設計分析\n\n\n\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A second-level course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall."
  },
  {
    "objectID": "15-Factor-Analysis.html#探索性因素分析",
    "href": "15-Factor-Analysis.html#探索性因素分析",
    "title": "15  因素分析",
    "section": "15.1 探索性因素分析",
    "text": "15.1 探索性因素分析\nExploratory Factor Analysis (EFA) is a statistical technique for revealing any hidden latent factors that can be inferred from our observed data. This technique calculates to what extent a set of measured variables, for example \\(V1, V2, V3, V4\\), and \\(V5\\), can be represented as measures of an underlying latent factor. This latent factor cannot be measured through just one observed variable but instead is manifested in the relationships it causes in a set of observed variables.\nIn Figure 15.1 each observed variable \\(V\\) is ‘caused’ to some extent by the underlying latent factor (\\(F\\)), depicted by the coefficients \\(b_1\\) to \\(b_5\\) (also called factor loadings). Each observed variable also has an associated error term, e1 to e5. Each error term is the variance in the associated observed variable, \\(V_i\\) , that is unexplained by the underlying latent factor.\n\n\n\n\n\nFigure 15.1: Latent factor underlying the relationship between several observed variables\n\n\n\n\nIn Psychology, latent factors represent psychological phenomena or constructs that are difficult to directly observe or measure. For example, personality, or intelligence, or thinking style. In the example in Figure 15.1 we may have asked people five specific questions about their behaviour or attitudes, and from that we are able to get a picture about a personality construct called, for example, extraversion. A different set of specific questions may give us a picture about an individual’s introversion, or their conscientiousness.\nHere’s another example: we may not be able to directly measure statistics anxiety, but we can measure whether statistics anxiety is high or low with a set of questions in a questionnaire. For example, “\\(Q1\\): Doing the assignment for a statistics course”, “\\(Q2\\): Trying to understand the statistics described in a journal article”, and “\\(Q3\\): Asking the lecturer for help in understanding something from the course”, etc., each rated from low anxiety to high anxiety. People with high statistics anxiety will tend to give similarly high responses on these observed variables because of their high statistics anxiety. Likewise, people with low statistics anxiety will give similar low responses to these variables because of their low statistics anxiety.\nIn exploratory factor analysis (EFA), we are essentially exploring the correlations between observed variables to uncover any interesting, important underlying (latent) factors that are identified when observed variables co-vary. We can use statistical software to estimate any latent factors and to identify which of our variables have a high loading1 (e.g. loading > 0.5) on each factor, suggesting they are a useful measure, or indicator, of the latent factor. Part of this process includes a step called rotation, which to be honest is a pretty weird idea but luckily we don’t have to worry about understanding it; we just need to know that it is helpful because it makes the pattern of loadings on different factors much clearer. As such, rotation helps with seeing more clearly which variables are linked substantively to each factor. We also need to decide how many factors are reasonable given our data, and helpful in this regard is something called Eigen values. We’ll come back to this in a moment, after we have covered some of the main assumptions of EFA.\n\n15.1.1 探索性因素分析的執行條件\nThere are a couple of assumptions that need to be checked as part of the analysis. The first assumption is sphericity, which essentially checks that the variables in your dataset are correlated with each other to the extent that they can potentially be summarised with a smaller set of factors. Bartlett’s test for sphericity checks whether the observed correlation matrix diverges significantly from a zero (or null) correlation matrix. So, if Bartlett’s test is significant (\\(p < .05\\)), this indicates that the observed correlation matrix is significantly divergent from the null, and is therefore suitable for EFA.\nThe second assumption is sampling adequacy and is checked using the Kaiser-MeyerOlkin (KMO) Measure of Sampling Adequacy (MSA). The KMO index is a measure of the proportion of variance among observed variables that might be common variance. Using partial correlations, it checks for factors that load just two items. We seldom, if ever, want EFA producing a lot of factors loading just two items each. KMO is about sampling adequacy because partial correlations are typically seen with inadequate samples. If the KMO index is high (\\(\\approx 1\\)), the EFA is efficient whereas if KMO is low (\\(\\approx 0\\)), the EFA is not relevant. KMO values smaller than \\(0.5\\) indicates that EFA is not suitable and a KMO value of \\(0.6\\) should be present before EFA is considered suitable. Values between \\(0.5\\) and \\(0.7\\) are considered adequate, values between \\(0.7\\) and \\(0.9\\) are good and values between \\(0.9\\) and \\(1.0\\) are excellent.\n\n\n15.1.2 探索性因素分析的用途\nIf the EFA has provided a good solution (i.e. factor model), then we need to decide what to do with our shiny new factors. Researchers often use EFA during psychometric scale development. They will develop a pool of questionnaire items that they think relate to one or more psychological constructs, use EFA to see which items “go together” as latent factors, and then they will assess whether some items should be removed because they don’t usefully or distinctly measure one of the latent factors.\nIn line with this approach, another consequence of EFA is to combine the variables that load onto distinct factors into a factor score, sometimes known as a scale score. There are two options for combining variables into a scale score:\n\nCreate a new variable with a score weighted by the factor loadings for each item that contributes to the factor.\nCreate a new variable based on each item that contributes to the factor, but weighting them equally.\n\nIn the first option each item’s contribution to the combined score depends on how strongly it relates to the factor. In the second option we typically just average across all the items that contribute substantively to a factor to create the combined scale score variable. Which to choose is a matter of preference, though a disadvantage with the first option is that loadings can vary quite a bit from sample to sample, and in behavioural and health sciences we are often interested in developing and using composite questionnaire scale scores across different studies and different samples. In which case it is reasonable to use a composite measure that is based on the substantive items contributing equally rather than weighting by sample specific loadings from a different sample. In any case, understanding a combined variable measure as an average of items is simpler and more intuitive than using a sample specific optimally-weighted combination.\nA more advanced statistical technique, one which is beyond the scope of this book, undertakes regression modelling where latent factors are used in prediction models of other latent factors. This is called “structural equation modelling” and there are specific software programmes and R packages dedicated to this approach. But let’s not get ahead of ourselves; what we should really focus on now is how to do an EFA in jamovi.\n\n\n15.1.3 使用jamovi完成探索性因素分析\nFirst, we need some data. Twenty-five personality self-report items (see Figure 15.2) taken from the International Personality Item Pool were included as part of the Synthetic Aperture Personality Assessment (SAPA) web-based personality assessment (SAPA: http://sapa-project.org) project. The 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness.\nThe item data were collected using a 6-point response scale:\n\nVery Inaccurate\nModerately Inaccurate\nSlightly Inaccurate\nSlightly Accurate\nModerately Accurate\nVery Accurate.\n\nA sample of \\(N=250\\) responses is contained in the dataset bfi_sample.csv . As researchers, we are interested in exploring the data to see whether there are some underlying latent factors that are measured reasonably well by the \\(25\\) observed variables in the bfi_sample.csv data file. Open up the dataset and check that the \\(25\\) variables are coded as continuous variables (technically they are ordinal though for EFA in jamovi it mostly doesn’t matter, except if you decide to calculate weighted factor scores in which case continuous variables are needed). To perform EFA in jamovi:\n\n\n\n\n\nFigure 15.2: Twenty-five observed variable items organised by five putative personality factors in the dataset bfi_sample.csv\n\n\n\n\n\nSelect Factor - Exploratory Factor Analysis from the main jamovi button bar to open the EFA analysis window (Figure 15.3).\nSelect the 25 personality questions and transfer them into the ‘Variables’ box.\nCheck appropriate options, including ‘Assumption Checks’, but also Rotation ‘Method’, ‘Number of Factors’ to extract, and ‘Additional Output’ options. See Figure 15.3 for suggested options for this illustrative EFA, and please note that the Rotation ‘Method’ and ‘Number of Factors’ extracted is typically adjusted by the researcher during the analysis to find the best result, as described below.\n\n\n\n\n\n\nFigure 15.3: The jamovi EFA analysis window\n\n\n\n\n\n\n\n\n\nFigure 15.4: jamovi EFA assumption checks for the personality questionnaire data\n\n\n\n\nFirst, check the assumptions (Figure 15.4). You can see that (1) Bartlett’s test of sphericity is significant, so this assumption is satisfied; and (2) the KMO measure of sampling adequacy (MSA) is \\(0.81\\) overall, suggesting good sampling adequacy. No problems here then!\nThe next thing to check is how many factors to use (or “extract” from the data). Three different approaches are available:\n\nOne convention is to choose all components with Eigen values greater than 12 . This would give us four factors with our data (try it and see).\nExamination of the scree plot, as in Figure 15.5, lets you identify the “point of inflection”. This is the point at which the slope of the scree curve clearly levels off, below the “elbow”. This would give us five factors with our data. Interpreting scree plots is a bit of an art: in Figure 15.5 there is a noticeable step from \\(5\\) to \\(6\\) factors, but in other scree plots you look at it will not be so clear cut.\nUsing a parallel analysis technique, the obtained Eigen values are compared to those that would be obtained from random data. The number of factors extracted is the number with Eigen values greater than what would be found with random data.\n\n\n\n\n\n\nFigure 15.5: Scree plot of the personality data in jamovi EFA, showing a noticeable inflection and levelling off after point 5 (the ‘elbow’)\n\n\n\n\nThe third approach is a good one according to Fabrigar et al. (1999), although in practice researchers tend to look at all three and then make a judgement about the number of factors that are most easily or helpfully interpreted. This can be understood as the “meaningfulness criterion”, and researchers will typically examine, in addition to the solution from one of the approaches above, solutions with one or two more or fewer factors. They then adopt the solution which makes the most sense to them.\nAt the same time, we should also consider the best way to rotate the final solution. There are two main approaches to rotation: orthogonal (e.g. ‘varimax’) rotation forces the selected factors to be uncorrelated, whereas oblique (e.g. ‘oblimin’) rotation allows the selected factors to be correlated. Dimensions of interest to psychologists and behavioural scientists are not often dimensions we would expect to be orthogonal, so oblique solutions are arguably more sensible2\n\n\n\n\n\nFigure 15.6: Factor summary statistics and correlations for a five factor solution in jamovi EFA\n\n\n\n\nPractically, if in an oblique rotation the factors are found to be substantially correlated (positive or negative, and > 0.3), as in Figure 15.6 where a correlation between two of the extracted factors is 0.31, then this would confirm our intuition to prefer oblique rotation. If the factors are, in fact, correlated, then an oblique rotation will produce a better estimate of the true factors and a better simple structure than will an orthogonal rotation. And, if the oblique rotation indicates that the factors have close to zero correlations between one another, then the researcher can go ahead and conduct an orthogonal rotation (which should then give about the same solution as the oblique rotation).\nOn checking the correlation between the extracted factors at least one correlation was greater than 0.3 (Figure 15.6), so an oblique (‘oblimin’) rotation of the five extracted factors is preferred. We can also see in Figure 15.6 that the proportion of overall variance in the data that is accounted for by the five factors is 46%. Factor one accounts for around 10% of the variance, factors two to four around 9% each, and factor five just over 7%. This isn’t great; it would have been better if the overall solution accounted for a more substantive proportion of the variance in our data.\nBe aware that in every EFA you could potentially have the same number of factors as observed variables, but every additional factor you include will add a smaller amount of explained variance. If the first few factors explain a good amount of the variance in the original 25 variables, then those factors are clearly a useful, simpler substitute for the 25 variables. You can drop the rest without losing too much of the original variability. But if it takes 18 factors (for example) to explain most of the variance in those 25 variables, you might as well just use the original 25.\nFigure 15.7 shows the factor loadings. That is, how the 25 different personality items load onto each of the five selected factors. We have hidden loadings less than \\(0.3\\) (set in the options shown in Figure 15.3.\nFor Factors \\(1, 2, 3\\) and \\(4\\) the pattern of factor loadings closely matches the putative factors specified in Figure 15.2. Phew! And factor \\(5\\) is pretty close, with four of the five observed variables that putatively measure “openness” loading pretty well onto the factor. Variable \\(04\\) doesn’t quite seem to fit though, as the factor solution in Figure 15.7 suggests that it loads onto factor \\(4\\) (albeit with a relatively low loading) but not substantively onto factor \\(5\\).\nThe other thing to note is that those variables that were denoted as “R: reverse coding” in Figure 15.2 are those that have negative factor loadings. Take a look at the items A1 (“Am indifferent to the feelings of others”) and A2 (“Inquire about others’ well-being”). We can see that a high score on \\(A1\\) indicates low Agreeableness, whereas a high score on \\(A2\\) (and all the other “A” variables for that matter) indicates high Agreeableness. Therefore A1 will be negatively correlated with the other “A” variables, and this is why it has a negative factor loading, as shown in Figure 15.7.\n\n\n\n\n\nFigure 15.7: Factor loadings for a five factor solution in jamovi EFA\n\n\n\n\nWe can also see in Figure 15.7 the “uniqueness” of each variable. Uniqueness is the proportion of variance that is ‘unique’ to the variable and not explained by the factors3. For example, 72% of the variance in ‘A1’ is not explained by the factors in the five factor solution. In contrast, ‘N1’ has relatively low variance not accounted for by the factor solution (35%). Note that the greater the ‘uniqueness’, the lower the relevance or contribution of the variable in the factor model.\nTo be honest, it’s unusual to get such a neat solution in EFA. It’s typically quite a bit more messy than this, and often interpreting the meaning of the factors is more challenging. It’s not often that you have such a clearly delineated item pool. More often you will have a whole heap of observed variables that you think may be indicators of a few underlying latent factors, but you don’t have such a strong sense of which variables are going to go where!\nSo, we seem to have a pretty good five factor solution, albeit accounting for a relatively low overall proportion of the observed variance. Let’s assume we are happy with this solution and want to use our factors in further analysis. The straightforward option is to calculate an overall (average) score for each factor by adding together the score for each variable that loads substantively onto the factor and then dividing by the number of variables (in other words create a ‘mean score’ for each person across the items for each scale. For each person in our dataset that entails, for example for the Agreeableness factor, adding together \\(A1 + A2 + A3 + A4 + A5\\), and then dividing by 5. 4 In essence, the factor score we have calculated is based on equally weighted scores from each of the included variables/itmes. We can do this in jamovi in two steps:\n\nRecode A1 into “A1R” by reverse scoring the values in the variable (i.e. \\(6 = 1\\); \\(5 = 2\\); \\(4 = 3\\); \\(3 = 4\\); \\(2 = 5\\); \\(1 = 6\\)) using the jamovi transform variable command (see Figure 15.8).\nCompute a new variable, called “Agreeableness’, by calculating the mean of A1R, A2, A3, A4 and A5. Do this using the jamovi compute new variable command (see Figure 15.9).\n\n\n\n\n\n\nFigure 15.8: Recode variable using the jamovi Transform command\n\n\n\n\n\n\n\n\n\nFigure 15.9: Compute new scale score variable using the jamovi Computed variable command\n\n\n\n\nAnother option is to create an optimally-weighted factor score index. To do this, save the factor scores to the data set, using the ‘Save’ - ‘Factor scores’ checkbox. Once you have done this you will see that five new variables (columns) have been added to the data, one for each factor extracted. See Figure 15.10 and Figure 15.11.\n\n\n\n\n\nFigure 15.10: jamovi option for factor scores for the five factor solution, using the ‘Bartlett’ optimal weighting method\n\n\n\n\n\n\n\n\n\nFigure 15.11: Data sheet view showing the five newly created factor score variables\n\n\n\n\nNow you can go ahead and undertake further analyses, using either the mean score based factor scales (e.g. as in Figure 15.9) or using the optimally-weighted factor scores calculated by jamovi. Your choice! For example, one thing you might like to do is see whether there are any gender differences in each of our personality scales. We did this for the Agreeableness score that we calculated using the mean score approach, and although the t test plot (Figure 15.12) showed that males were less agreeable than females, this was not a significant difference (Mann-Whitney \\(U = 5768\\), \\(p = .075\\)).\n\n\n\n\n\nFigure 15.12: Comparing differences in Agreeableness factor-based scores between males and females\n\n\n\n\n\n\n15.1.4 探索性因素分析的報告須知\nHopefully, so far we have given you some sense of EFA and how to undertake EFA in jamovi. So, once you have completed your EFA, how do you write it up? There is not a formal standard way to write up an EFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nWhat are the theoretical underpinnings for the area you are studying, and specifically for the constructs that you are interested in uncovering through EFA.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nDescribe how you went about testing the assumptions for EFA. Details regarding sphericity checks and measures of sampling adequacy should be reported.\nExplain what FA extraction method (e.g. ‘Minimum residuals’ or ‘Maximum likelihood’) was used.\nExplain the criteria and process used for deciding how many factors were extracted in the final solution, and which items were selected. Clearly explain the rationale for key decisions during the EFA process.\nExplain what rotation methods were attempted, the reasons why, and the results.\nFinal factor loadings should be reported in the results, in a table. This table should also report the uniqueness (or communality) for each variable (in the final column). Factor loadings should be reported with descriptive labels in addition to item numbers. Correlations between the factors should also be included, either at the bottom of this table, in a separate table.\nMeaningful names for the extracted factors should be provided. You may like to use previously selected factor names, but on examining the actual items and factors you may think a different name is more appropriate"
  },
  {
    "objectID": "15-Factor-Analysis.html#主成分分析",
    "href": "15-Factor-Analysis.html#主成分分析",
    "title": "15  因素分析",
    "section": "15.2 主成分分析",
    "text": "15.2 主成分分析\nIn the previous section we saw that EFA works to identify underlying latent factors. And, as we saw, in one scenario the smaller number of latent factors can be used in further statistical analysis using some sort of combined factor scores.\nIn this way EFA is being used as a “data reduction” technique. Another type of data reduction technique, sometimes seen as part of the EFA family, is principal component analysis (PCA) . However, PCA does not identify underlying latent factors. Instead it creates a linear composite score from a larger set of measured variables.\nPCA simply produces a mathematical transformation to the original data with no assumptions about how the variables co-vary. The aim of PCA is to calculate a few linear combinations (components) of the original variables that can be used to summarize the observed data set without losing much information. However, if identification of underlying structure is a goal of the analysis, then EFA is to be preferred. And, as we saw, EFA produces factor scores that can be used for data reduction purposes just like principal component scores (Fabrigar et al., 1999).\nPCA has been popular in Psychology for a number of reasons, and therefore it’s worth mentioning, although nowadays EFA is just as easy to do given the power of desktop computers and can be less susceptible to bias than PCA, especially with a small number of factors and variables. Much of the procedure is similar to EFA, so although there are some conceptual differences, practically the steps are the same, and with large samples and a sufficient number of factors and variables, the results from PCA and EFA should be fairly similar.\nTo undertake PCA in jamovi, all you need to do is select ‘Factor’ - ‘Principal Component Analysis’ from the main jamovi button bar to open the PCA analysis window. Then you can follow the same steps from [EFA in jamovi] above."
  },
  {
    "objectID": "15-Factor-Analysis.html#驗證性因素分析",
    "href": "15-Factor-Analysis.html#驗證性因素分析",
    "title": "15  因素分析",
    "section": "15.3 驗證性因素分析",
    "text": "15.3 驗證性因素分析\nSo, our attempt to identify underlying latent factors using EFA with carefully selected questions from the personality item pool seemed to be pretty successful. The next step in our quest to develop a useful measure of personality is to check the latent factors we identified in the original EFA with a different sample. We want to see if the factors hold up, if we can confirm their existence with different data. This is a more rigorous check, as we will see. And it’s called Confirmatory Factor Analysis (CFA) as we will, unsurprisingly, be seeking to confirm a pre-specified latent factor structure.5\nIn CFA, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.13, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the personality items would therefore specify five latent factors as shown in Figure 15.13, each measured by five observed variables. Each variable is a measure of an underlying latent factor. For example, A1 is predicted by the underlying latent factor Agreeableness. And because A1 is not a perfect measure of the Agreeableness factor, there is an error term, \\(e\\), associated with it. In other words, \\(e\\) represents the variance in A1 that is not accounted for by the Agreeableness factor. This is sometimes called measurement error.\n\n\n\n\n\nFigure 15.13: Initial pre-specification of latent factor structure for the five factor personality scales, for use in CFA\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that some of our personality factors may be correlated with each other. So, in our model, we should allow these latent factors to co-vary, as shown by the double-headed arrows in Figure 15.13.\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. One reason for this might be that there is a shared methodological feature for particular sub-sets of the observed variables such that the observed variables might be correlated for methodological rather than substantive latent factor reasons. We’ll return to this possibility in a later section but, for now, there are no clear reasons that we can see that would justify correlating some of the error terms with each other\nWithout any correlated error terms, the model we are testing to see how well it fits with our observed data is just as specified in Figure 15.13. Only parameters that are included in the model are expected to be found in the data, so in CFA all other possible parameters (coefficients) are set to zero. So, if these other parameters are not zero (for example there may be a substantial loading from A1 onto the latent factor Extraversion in the observed data, but not in our model) then we may find a poor fit between our model and the observed data.\nRight, let’s take a look at how we set this CFA analysis up in jamovi.\n\n15.3.1 使用jamovi完成驗證性因素分析\nOpen up the bfi_sample2.csv file, check that the 25 variables are coded as ordinal (or continuous; it won’t make any difference for this analysis). To perform CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.14).\nSelect the 5 A variables and transfer them into the ‘Factors’ box and give then the label “Agreeableness”.\nCreate a new Factor in the ‘Factors’ box and label it “Conscientiousness”. Select the 5 C variables and transfer them into the ‘Factors’ box under the “Conscientiousness” label.\nCreate another new Factor in the ‘Factors’ box and label it “Extraversion”. Select the 5 E variables and transfer them into the ‘Factors’ box under the “Extraversion” label.\nCreate another new Factor in the ‘Factors’ box and label it “Neuroticism”. Select the 5 N variables and transfer them into the ‘Factors’ box under the “Neuroticism” label.\nCreate another new Factor in the ‘Factors’ box and label it “Openness”. Select the 5 O variables and transfer them into the ‘Factors’ box under the “Openness” label.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.13.\n\n\n\n\n\n\nFigure 15.14: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is model fit (Figure 15.15) as this tells us how good a fit our model is to the observed data. NB in our model only the pre-specified covariances are estimated, including the factor correlations by default. Everything else is set to zero.\n\n\n\n\n\nFigure 15.15: The jamovi CFA Model Fit results for our CFA model\n\n\n\n\nThere are several ways of assessing model fit. The first is a chi-square statistic that, if small, indicates that the model is a good fit to the data. However, the chi-squared statistic used for assessing model fit is pretty sensitive to sample size, meaning that with a large sample a good enough fit between the model and the data almost always produces a large and significant (p < .05) chi-square value.\nSo, we need some other ways of assessing model fit. In jamovi several are provided by default. These are the Comparative Fit Index (CFI), the Tucker Lewis Index (TLI) and the Root Mean Square Error of Approximation (RMSEA) together with the 90% confidence interval for the RMSEA. Some useful rules of thumb are that a satisfactory fit is indicated by CFI > 0.9, TLI > 0.9, and RMSEA of about 0.05 to 0.08. A good fit is CFI > 0.95, TLI > 0.95, and RMSEA and upper CI for RMSEA < 0.05.\nSo, looking at Figure 15.15 we can see that the chi-square value is large and highly significant. Our sample size is not too large, so this possibly indicates a poor fit. The CFI is \\(0.762\\) and the TLI is 0.731, indicating poor fit between the model and the data. The RMSEA is \\(0.085\\) with a \\(90\\%\\) confidence interval from \\(0.077\\) to \\(0.092\\), again this does not indicate a good fit.\nPretty disappointing, huh? But perhaps not too surprising given that in the earlier EFA, when we ran with a similar data set (see [Exploratory Factor Analysis] section), only around half of the variance in the data was accounted for by the five factor model.\nLet’s go on to look at the factor loadings and the factor covariance estimates, shown in Figure 15.16 and Figure 15.17. The Z-statistic and p-value for each of these parameters indicates they make a reasonable contribution to the model (i.e. they are not zero) so there doesn’t appear to be any reason to remove any of the specified variable-factor paths, or factor-factor correlations from the model. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.16: The jamovi CFA Factor Loadings table for our CFA model\n\n\n\n\n\n\n\n\n\nFigure 15.17: The jamovi CFA Factor Covariances table for our CFA model\n\n\n\n\nHow could we improve the model? One option is to go back a few stages and think again about the items / measures we are using and how they might be improved or changed. Another option is to make some post hoc tweaks to the model to improve the fit. One way of doing this is to use “modification indices” (Figure 15.18), specified as an ‘Additional output’ option in jamovi.\n\n\n\n\n\nFigure 15.18: The jamovi CFA Factor Loadings Modification Indices\n\n\n\n\nWhat we are looking for is the highest modification index (MI) value. We would then judge whether it makes sense to add that additional term into the model, using a post hoc rationalisation. For example, we can see in Figure 15.18 that the largest MI for the factor loadings that are not already in the model is a value of 28.786 for the loading of N4 (“Often feel blue”) onto the latent factor Extraversion. This indicates that if we add this path into the model then the chi-square value will reduce by around the same amount.\nBut in our model adding this path arguably doesn’t really make any theoretical or methodological sense, so it’s not a good idea (unless you can come up with a persuasive argument that “Often feel blue” measures both Neuroticism and Extraversion). I can’t think of a good reason. But, for the sake of argument, let’s pretend it does make some sense and add this path into the model. Go back to the CFA analysis window (see Figure 15.14) and add N4 into the Extraversion factor. The results of the CFA will now change (not shown); the chi-square has come down to around 709 (a drop of around 30, roughly similar to the size of the MI) and the other fit indices have also improved, though only a bit. But it’s not enough: it’s still not a good fitting model.\nIf you do find yourself adding new parameters to a model using the MI values then always re-check the MI tables after each new addition, as the MIs are refreshed each time.\nThere is also a Table of Residual Covariance Modification Indices produced by jamovi (Figure 15.19). In other words, a table showing which correlated errors, if added to the model, would improve the model fit the most. It’s a good idea to look across both MI tables at the same time, spot the largest MI, think about whether the addition of the suggested parameter can be reasonably justified and, if it can, add it to the model. And then you can start again looking for the biggest MI in the re-calculated results.\n\n\n\n\n\nFigure 15.19: Residual Covariance Modification Indices produced by jamovi\n\n\n\n\nYou can keep going this way for as long as you like, adding parameters to the model based on the largest MI, and eventually you will achieve a satisfactory fit. But there will also be a strong possibility that in doing this you will have created a monster! A model that is ugly and deformed and doesn’t have any theoretical sense or purity. In other words, be very careful!\nSo far, we have checked out the factor structure obtained in the EFA using a second sample and CFA. Unfortunately, we didn’t find that the factor structure from the EFA was confirmed in the CFA, so it’s back to the drawing board as far as the development of this personality scale goes.\nAlthough we could have tweaked the CFA using modification indexes, there really were not any good reasons (that I could think of) for these suggested additional factor loadings or residual covariances to be included. However, sometimes there is a good reason for residuals to be allowed to co-vary (or correlate), and a good example of this is shown in the next section on [Multi-Trait Multi-Method CFA]. Before we do that, let’s cover how to report the results of a CFA.\n\n\n15.3.2 驗證性因素分析的報告須知\nThere is not a formal standard way to write up a CFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nA theoretical and empirical justification for the hypothesized model.\nA complete description of how the model was specified (e.g. the indicator variables for each latent factor, covariances between latent variables, and any correlations between error terms). A path diagram, like the one in Figure 15.13 would be good to include.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nTests of assumptions and estimation method used.\nA description of missing data and how the missing data were handled.\nThe software and version used to fit the model.\nMeasures, and the criteria used, to judge model fit.\nAny alterations made to the original model based on model fit or modification indices.\nAll parameter estimates (i.e., loadings, error variances, latent (co)variances) and their standard errors, probably in a table."
  },
  {
    "objectID": "15-Factor-Analysis.html#多種特質多項相關驗證性因素分析",
    "href": "15-Factor-Analysis.html#多種特質多項相關驗證性因素分析",
    "title": "15  因素分析",
    "section": "15.4 多種特質多項相關驗證性因素分析",
    "text": "15.4 多種特質多項相關驗證性因素分析\nIn this section we’re going to consider how different measurement techniques or questions can be an important source of data variability, known as method variance. To do this, we’ll use another psychological data set, one that contains data on “attributional style”.\nThe Attributional Style Questionnaire (ASQ) was used (Hewitt et al., 2004) to collect psychological wellbeing data from young people in the United Kingdom and New Zealand. They measured attributional style for negative events, which is how people habitually explain the cause of bad things that happen to them (Peterson & Seligman, 1984). The attributional style questionnaire (ASQ) measures three aspects of attributional style:\n\nInternality is the extent to which a person believes that the cause of a bad event is due to his/her own actions.\nStability refers to the extent to which a person habitually believes the cause of a bad event is stable across time.\nGlobality refers to the extent to which a person habitually believes that the cause of a bad event in one area will affect other areas of their lives.\n\nThere are six hypothetical scenarios and for each scenario respondents answer a question aimed at (a) internality, (b) stability and (c) globality. So there are \\(6 \\times 3 = 18\\) items overall. See Figure 15.20 for more details.\n\n\n\n\n\nFigure 15.20: The Attributional Style Questionnaire (ASQ) for negative events\n\n\n\n\nResearchers are interested in checking their data to see whether there are some underlying latent factors that are measured reasonably well by the 18 observed variables in the ASQ.\nFirst, they try EFA with these 18 variables (not shown), but no matter how they extract or rotate, they can’t find a good factor solution. Their attempt to identify underlying latent factors in the Attributional Style Questionnaire (ASQ) proved fruitless. If you get results like this then either your theory is wrong (there is no underlying latent factor structure for attributional style, which is possible), the sample is not relevant (which is unlikely given the size and characteristics of this sample of young adults from the United Kingdom and New Zealand), or the analysis was not the right tool for the job. We’re going to look at this third possibility.\nRemember that there were three dimensions measured in the ASQ: Internality, Stability and Globality, each measured by six questions as shown in Figure 15.21.\nWhat if, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.21, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the ASQ would therefore specify three latent factors as shown in the columns of Figure 15.27, each measured by six observed variables.\n\n\n\n\n\nFigure 15.21: Six questions on the ASQ for each of the Internality, Stability and Globality dimensions\n\n\n\n\nWe could depict this as in the diagram in Figure 15.22, which shows that each variable is a measure of an underlying latent factor. For example INT1 is predicted by the underlying latent factor Internality. And because INT1 is not a perfect measure of the Internality factor, there is an error term, e1, associated with it. In other words, e1 represents the variance in INT1 that is not accounted for by the Internality factor. This is sometimes called “measurement error”.\n\n\n\n\n\nFigure 15.22: Initial pre-specification of latent factor structure for the ASQ\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that Internality, Stability, and Globality might be correlated with each other, so in our model we should allow these latent factors to co-vary, as shown in Figure 15.23.\n\n\n\n\n\nFigure 15.23: Final pre-specification of latent factor structure for the ASQ, including latent factor correlations, and shared method error term correlations for the observed variable INT1, STAB1 and GLOB1, in a CFA MTMM model. For clarity, other pre-specified error term correlations are not shown\n\n\n\n\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. Thinking back to the ASQ questions, there were three different sub-questions (a, b and c) for each main question (1-6). Q1 was about unsuccessful job hunting and it is plausible that this question has some distinctive artefactual or methodological aspects over and above the other questions (2-5), something to do with job hunting perhaps. Similarly, Q2 was about not helping a friend with a problem, and there may be some distinctive artefactual or methodological aspects to do with not helping a friend that is not present in the other questions (1, and 3-5).\nSo, as well as multiple factors, we also have multiple methodological features in the ASQ, where each of Questions 1-6 has a slightly different “method”, but each “method” is shared across the sub-questions a, b and c. In order to incorporate these different methodological features into the model we can specify that certain error terms are correlated with each other. For example, the errors associated with INT1, STAB1 and GLOB1 should be correlated with each other to reflect the distinct and shared methodological variance of Q1a, Q1b and Q1c. Looking at Figure 15.21, this means that as well as the latent factors represented by the columns, we will have correlated measurement errors for the variables in each row of the Table.\nWhilst a basic CFA model like the one shown in Figure 15.22 could be tested against our observed data, we have in fact come up with a more sophisticated model, as shown in the diagram in Figure 15.23. This more sophisticated CFA model is known as a Multi-Trait Multi-Method (MTMM) model, and it is the one we will test in jamovi.\n\n15.4.1 使用jamovi完成多種特質多項相關驗證性因素分析\nOpen up the ASQ.csv file and check that the 18 variables (six “Internality”, six “Stability” and six “Globality” variables) are specified as continuous variables.\nTo perform MTMM CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.24).\nSelect the 6 INT variables and transfer them into the ‘Factors’ box and give them the label “Internality”.\nCreate a new Factor in the ‘Factors’ box and label it “Stability”. Select the 6 STAB variables and transfer them into the ‘Factors’ box under the “Stability” label.\nCreate another new Factor in the ‘Factors’ box and label it “Globality”. Select the 6 GLOB variables and transfer them into the ‘Factors’ box under the “Globality” label.\nOpen up the Residual Covariances options, and for each of our pre-specified correlations move the associated variables across into the ‘Residual Covariances’ box on the right. For example, highlight both INT1 and STAB1 and then click the arrow to move these across. Now do the same for INT1 and GLOB1, for STAB1 and GLOB1, for INT2 and STAB2, for INT2 and GLOB2, for STAB2 and GLOB2, for INT3 and STAB3, and so on.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.23, and including all the error term correlations that we have added above.\n\n\n\n\n\n\nFigure 15.24: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is “Model fit” as this tells us how good a fit our model is to the observed data (Figure 15.25). NB in our model only the pre-specified covariances are estimated, everything else is set to zero, so model fit is testing both whether the pre-specified “free” parameters are not zero, and conversely whether the other relationships in the data – the ones we have not specified in the model – can be held at zero.\n\n\n\n\n\nFigure 15.25: The jamovi CFA Model Fit results for our CFA MTMM model\n\n\n\n\nLooking at Figure 15.25 we can see that the chi-square value is highly significant, which is not a surprise given the large sample size (N = 2748). The CFI is 0.98 and the TLI is also 0.98, indicating a very good fit. The RMSEA is 0.02 with a 90% confidence interval from 0.02 to 0.02 – pretty tight!\nOverall, I think we can be satisfied that our pre-specified model is a very good fit to the observed data, lending support to our MTMM model for the ASQ.\nWe can now go on to look at the factor loadings and the factor covariance estimates, as in Figure 15.26. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.26: The jamovi CFA Factor Loadings and Covariances tables for our CFA MTMM model\n\n\n\n\nYou can see from Figure 15.26 that all of our pre-specified factor loadings and factor covariances are significantly different from zero. In other words, they all seem to be making a useful contribution to the model.\nWe’ve been pretty lucky with this analysis, getting a very good fit on our first attempt!"
  },
  {
    "objectID": "15-Factor-Analysis.html#section",
    "href": "15-Factor-Analysis.html#section",
    "title": "15  因素分析",
    "section": "15.6 ",
    "text": "15.6 \nIn this chapter on factor analysis and related techniques we have introduced and demonstrated statistical analyses that assess the pattern of relationships in a data set. Specifically, we have covered:\n\n[Exploratory Factor Analysis] (EFA). EFA is a statistical technique for identifying underlying latent factors in a data set. Each observed variable is conceptualised as representing the latent factor to some extent, indicated by a factor loading. Researchers also use EFA as a way of data reduction, i.e. identifying observed variables than can be combined into new factor variables for subsequent analysis.\n[Principal Component Analysis] (PCA) is a data reduction technique which, strictly speaking, does not identify underlying latent factors. Instead, PCA simply produces a linear combination of observed variables.\n[Confirmatory Factor Analysis] (CFA). Unlike EFA, with CFA you start with an idea - a model - of how the variables in your data are related to each other. You then test your model against the observed data and assess how good a fit the model is to the data.\nIn [Multi-Trait Multi-Method CFA] (MTMM CFA), both latent factor and method variance are included in the model in an approach that is useful when there are different methodological approaches used and therefore method variance is an important consideration.\n[Internal consistency reliability analysis]. This form of reliability analysis tests how consistently a scale measures a measurement (psychological) construct.\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "15-Factor-Analysis.html#本章小結",
    "href": "15-Factor-Analysis.html#本章小結",
    "title": "15  因素分析",
    "section": "15.6 本章小結",
    "text": "15.6 本章小結\n這一章我們學習因素分析的相關技術，特別是評估資料內各種相關性的方法。本章的學習重點包括：\n\n探索性因素分析 (EFA)用於辨識資料內的潛在因素。根據因素負荷量，每個觀察變項都有可能代表某個潛在因素。研究者也會使用EFA簡化資料項目，像是使用序列分析整合數個觀察變項為一個因素。\n主成分分析 (PCA)是一種簡化資料項目的技術，但是並非用於辨識潛在變項。PCA只是生成觀察變項的線性組合。\n驗證性因素分析 (CFA)不同於EFA，執行前已經有一個理想的模型～也就是觀察變項之間的關聯模型。CFA的用途是檢測理想模性與資料模型的擬合度。\n多種特質多項相關驗證性因素分析 (MTMM CFA) 用於分析潛在因素模型的方法不只一種，需要評估各種分析方法所所估計的變異合理程度。\n內部一致性信度分析用於評估量表所測對象，與假設的心理建構之間的一致性程度。\n\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "16-Bayesian-statistics.html#理性人類的機率推論",
    "href": "16-Bayesian-statistics.html#理性人類的機率推論",
    "title": "16  貝氏統計",
    "section": "16.1 理性人類的機率推論",
    "text": "16.1 理性人類的機率推論\nFrom a Bayesian perspective statistical inference is all about belief revision. I start out with a set of candidate hypotheses h about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.\n\nI’m carrying an umbrella. Do you think it will rain?\n\nIn this problem I have presented you with a single piece of data (d = I’m carrying the umbrella), and I’m asking you to tell me your belief or hypothesis about whether it’s raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem?\n\n16.1.1 事前機率：你一開始的信念\nThe first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia2 and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see Table 16.1). If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you I’m betting that this really is what you believe) then what I have written here is your prior distribution, written \\(P(h)\\).\n\n\n\n\nTable 16.1:  How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall \n\nHypothesisDegree of Belief\n\nRainy day0.15\n\nDry day0.85\n\n\n\n\n\n\n\n16.1.2 似然值: 對手上資料的理論\nTo solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,3 and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write this out as in Table 16.2.\n\n\n\n\nTable 16.2:  How likely am I to be carrying an umbrella on rainy and dry days \n\nDataData\n\nHypothesisUmbrellaNo umbrella\n\nRainy day0.300.70\n\nDry day0.050.95\n\n\n\n\n\nIt’s important to remember that each cell in this table describes your beliefs about what data d will be observed, given the truth of a particular hypothesis \\(h\\). This “conditional probability” is written \\(P(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. In Bayesian statistics, this is referred to as the likelihood of the data \\(d\\) given the hypothesis \\(h\\).4\n\n\n16.1.3 資料與理論的聯合機率\nAt this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it so I’m going to try to build it up from more basic ideas.\nLet’s start out with one of the rules of probability theory. I listed it way back in Table 7.1, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I’m carrying an umbrella (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(P(d,h)\\), and you can calculate it by multiplying the prior \\(P(h)\\) by the likelihood \\(P(d|h)\\). Mathematically, we say that\n\\[P(d,h)=P(d|h)P(h)\\]\nSo, what is the probability that today is a rainy day and I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is \\(30\\%\\). So the probability that both of these things are true is calculated by multiplying the two\n\\[\n\\begin{split}\nP(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\n& = 0.30 \\times 0.15 \\\\\n& = 0.045\n\\end{split}\n\\]\nIn other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with Table 16.3.\n\n\n\n\nTable 16.3:  Four possibilities combining rain (or not) and umbrella carrying (or not) \n\nUmbrellaNo-umbrella\n\nRainy0.0450.105\n\nDry0.04250.807\n\n\n\n\n\nThis table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us Table 16.4.\n\n\n\n\nTable 16.4:  Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals \n\nUmbrellaNo-umbrellaTotal\n\nRainy0.0450.1050.15\n\nDry0.04250.8070.85\n\nTotal0.08750.9121\n\n\n\n\n\nThis is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course, as that’s our prior.5 The important thing isn’t the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.\nNow, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (Table 16.5):\n\n\n\n\nTable 16.5:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities \n\nUmbrellaNo-umbrella\n\nRainyP(Umbrella, Rainy)P(No-umbrella, Rainy)P(Rainy)\n\nDryP(Umbrella, Dry)P(No-umbrella, Dry)P(Dry)\n\nP(Umbrella)P(No-umbrella)\n\n\n\n\n\nFinally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let \\(d_1\\) refer to the possibility that you observe me carrying an umbrella, and \\(d_2\\) refers to you observing me not carrying one. Similarly, \\(h_1\\) is your hypothesis that today is rainy, and \\(h_2\\) is the hypothesis that it is not. Using this notation, the table looks like Table 16.6.\n\n\n\n\nTable 16.6:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities \n\n\\( d_1 \\)\\( d_2 \\)\n\n\\( h_1 \\)\\(P(h_1, d_1)\\)\\(P(h_1, d_2)\\)\\( P(h_1) \\)\n\n\\( h_2 \\)\\(P(h_2, d_1)\\)\\(P(h_2, d_2)\\)\\( P(h_2) \\)\n\n\\( P(d_1) \\)\\( P(d_2) \\)\n\n\n\n\n\n\n\n16.1.4 透過貝氏法則更新信念\nThe table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now know that I have an umbrella.6 To reflect this new knowledge, our revised table must have the following numbers. (see Table 16.7).\n\n\n\n\nTable 16.7:  Revising beliefs given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0\n\nDry0\n\nTotal10\n\n\n\n\n\nIn other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that \\(P(umbrella) = 1\\).\nWhat two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is slightly more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like Table 16.8, perhaps?\n\n\n\n\nTable 16.8:  Revising probabilities given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0.5140\n\nDry0.4860\n\nTotal10\n\n\n\n\n\nWhat this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The posterior probability of rain \\(P(h\\|d)\\) given that I am carrying an umbrella is 51.4%\nHow did I calculate these numbers? You can probably guess. To work out that there was a \\(0.514\\) probability of “rain”, all I did was take the \\(0.045\\) probability of “rain and umbrella” and divide it by the \\(0.0875\\) chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data \\(P(d, h)\\) by the marginal probability of the data \\(P(d)\\), and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: 7\n\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]\nHowever, remember what I said at the start of the last section, namely that the joint probability \\(P(d, h)\\) is calculated by multiplying the prior Pphq by the likelihood \\(P(d|h)\\). In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:\n\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule."
  },
  {
    "objectID": "16-Bayesian-statistics.html#貝氏假設檢定",
    "href": "16-Bayesian-statistics.html#貝氏假設檢定",
    "title": "16  貝氏統計",
    "section": "16.2 貝氏假設檢定",
    "text": "16.2 貝氏假設檢定\nIn Chapter 9 I described the orthodox approach to hypothesis testing. It took an entire chapter to describe, because null hypothesis testing is a very elaborate contraption that people find very hard to make sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let’s pick a setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to compare, a null hypothesis \\(h_0\\) and an alternative hypothesis \\(h_1\\). Prior to running the experiment we have some beliefs \\(P(h)\\) about which hypotheses are true. We run an experiment and obtain data d. Unlike frequentist statistics, Bayesian statistics does allow us to talk about the probability that the null hypothesis is true. Better yet, it allows us to calculate the posterior probability of the null hypothesis, using Bayes’ rule:\n\\[P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}\\]\nThis formula tells us exactly how much belief we should have in the null hypothesis after having observed the data d. Similarly, we can work out how much belief to place in the alternative hypothesis using essentially the same equation. All we do is change the subscript\n\\[P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}\\]\nIt’s all so simple that I feel like an idiot even bothering to write these equations down, since all I’m doing is copying Bayes rule from the previous section.8\n\n16.2.1 貝氏因子\nIn practice, most Bayesian data analysts tend not to talk in terms of the raw posterior probabilities \\(P(h_0|d)\\) and \\(P(h_1|d)\\). Instead, we tend to talk in terms of the posterior odds ratio. Think of it like betting. Suppose, for instance, the posterior probability of the null hypothesis is 25%, and the posterior probability of the alternative is 75%. The alternative hypothesis is three times as probable as the null, so we say that the odds are 3:1 in favour of the alternative. Mathematically, all we have to do to calculate the posterior odds is divide one posterior probability by the other\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3\\]\nOr, to write the same thing in terms of the equations above\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}\\]\nActually, this equation is worth expanding on. There are three different terms here that you should know. On the left hand side, we have the posterior odds, which tells you what you believe about the relative plausibilty of the null hypothesis and the alternative hypothesis after seeing the data. On the right hand side, we have the prior odds, which indicates what you thought before seeing the data. In the middle, we have the Bayes factor, which describes the amount of evidence provided by the data. (Table 16.9).\n\n\n\n\nTable 16.9:  Posterior odds given the Bsyes factor and prior odds \n\n\\(\\frac{P(h_1|d)}{h_0|d}\\)\\(=\\)\\(\\frac{P(d|h_1)}{d|h_0}\\)\\(\\times \\)\\(\\frac{P(h_1)}{h_0}\\)\n\n\\(\\Uparrow\\)\\(\\Uparrow\\)\\(\\Uparrow\\)\n\nPosterior oddsBayes factorPrior odds\n\n\n\n\n\nThe Bayes factor (sometimes abbreviated as BF) has a special place in Bayesian hypothesis testing, because it serves a similar role to the p-value in orthodox hypothesis testing. The Bayes factor quantifies the strength of evidence provided by the data, and as such it is the Bayes factor that people tend to report when running a Bayesian hypothesis test. The reason for reporting Bayes factors rather than posterior odds is that different researchers will have different priors. Some people might have a strong bias to believe the null hypothesis is true, others might have a strong bias to believe it is false. Because of this, the polite thing for an applied researcher to do is report the Bayes factor. That way, anyone reading the paper can multiply the Bayes factor by their own personal prior odds, and they can work out for themselves what the posterior odds would be. In any case, by convention we like to pretend that we give equal consideration to both the null hypothesis and the alternative, in which case the prior odds equals 1, and the posterior odds becomes the same as the Bayes factor\n\n\n16.2.2 解讀貝氏因子\nOne of the really nice things about the Bayes factor is the numbers are inherently meaningful. If you run an experiment and you compute a Bayes factor of 4, it means that the evidence provided by your data corresponds to betting odds of 4:1 in favour of the alternative. However, there have been some attempts to quantify the standards of evidence that would be considered meaningful in a scientific context. The two most widely used are from Jeffreys (1961) and Kass & Raftery (1995). Of the two, I tend to prefer the Kass & Raftery (1995) table because it’s a bit more conservative. So here it is (Table 16.10).\n\n\n\n\nTable 16.10:  Bayes factors and strength of evidence \n\nBayes factorInterpretation\n\n1 - 3Negligible evidence\n\n3 - 20Positive evidence\n\n20 - 150Strong evidence\n\n> 150Very strong evidence\n\n\n\n\n\nAnd to be perfectly honest, I think that even the Kass & Raftery (1995) standards are being a bit charitable. If it were up to me, I’d have called the “positive evidence” category “weak evidence”. To me, anything in the range 3:1 to 20:1 is “weak” or “modest” evidence at best. But there are no hard and fast rules here. What counts as strong or weak evidence depends entirely on how conservative you are and upon the standards that your community insists upon before it is willing to label a finding as “true”.\nIn any case, note that all the numbers listed above make sense if the Bayes factor is greater than 1 (i.e., the evidence favours the alternative hypothesis). However, one big practical advantage of the Bayesian approach relative to the orthodox approach is that it also allows you to quantify evidence for the null. When that happens, the Bayes factor will be less than 1. You can choose to report a Bayes factor less than 1, but to be honest I find it confusing. For example, suppose that the likelihood of the data under the null hypothesis \\(P(d|h_0)\\) is equal to 0.2, and the corresponding likelihood \\(P(d|h_1)\\) under the alternative hypothesis is 0.1. Using the equations given above, Bayes factor here would be\n\\[BF=\\frac{P(d|h_1)}{P(d|h_0)}=\\frac{0.1}{0.2}=0.5\\]\nRead literally, this result tells is that the evidence in favour of the alternative is 0.5 to 1. I find this hard to understand. To me, it makes a lot more sense to turn the equation “upside down”, and report the amount op evidence in favour of the null. In other words, what we calculate is this\n\\[BF^{'}=\\frac{P(d|h_0)}{P(d|h_1)}=\\frac{0.2}{0.1}=2\\]\nAnd what we would report is a Bayes factor of 2:1 in favour of the null. Much easier to understand, and you can interpret this using the table above."
  },
  {
    "objectID": "16-Bayesian-statistics.html#為何需要貝氏統計",
    "href": "16-Bayesian-statistics.html#為何需要貝氏統計",
    "title": "16  貝氏統計",
    "section": "16.3 為何需要貝氏統計",
    "text": "16.3 為何需要貝氏統計\nUp to this point I’ve focused exclusively on the logic underpinning Bayesian statistics. We’ve talked about the idea of “probability as a degree of belief”, and what it implies about how a rational agent should reason about the world. The question that you have to answer for yourself is this: how do you want to do your statistics? Do you want to be an orthodox statistician, relying on sampling distributions and p-values to guide your decisions? Or do you want to be a Bayesian, relying on things like prior beliefs, Bayes factors and the rules for rational belief revision? And to be perfectly honest, I can’t answer this question for you. Ultimately it depends on what you think is right. It’s your call and your call alone. That being said, I can talk a little about why I prefer the Bayesian approach.\n\n16.3.1 以統計學呈現你所相信的世界面貌\n\nYou keep using that word. I do not think it means what you think it means\n– Inigo Montoya, The Princess Bride 9\n\nTo me, one of the biggest advantages to the Bayesian approach is that it answers the right questions. Within the Bayesian framework, it is perfectly sensible and allowable to refer to “the probability that a hypothesis is true”. You can even try to calculate this probability. Ultimately, isn’t that what you want your statistical tests to tell you? To an actual human being, this would seem to be the whole point of doing statistics, i.e., to determine what is true and what isn’t. Any time that you aren’t exactly sure about what the truth is, you should use the language of probability theory to say things like “there is an 80% chance that Theory A is true, but a 20% chance that Theory B is true instead”.\nThis seems so obvious to a human, yet it is explicitly forbidden within the orthodox framework. To a frequentist, such statements are a nonsense because “the theory is true” is not a repeatable event. A theory is true or it is not, and no probabilistic statements are allowed, no matter how much you might want to make them. There’s a reason why, back in Section 9.5, I repeatedly warned you not to interpret the p-value as the probability that the null hypothesis is true. There’s a reason why almost every textbook on statstics is forced to repeat that warning. It’s because people desperately want that to be the correct interpretation. Frequentist dogma notwithstanding, a lifetime of experience of teaching undergraduates and of doing data analysis on a daily basis suggests to me that most actual humans think that “the probability that the hypothesis is true” is not only meaningful, it’s the thing we care most about. It’s such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a p-value this way. For example, here is a quote from an official Newspoll report in 2013, explaining how to interpret their (frequentist) data analysis:10\n\nThroughout the report, where relevant, statistically significant changes have been noted. All significance tests have been based on the 95 percent level of confidence. This means that if a change is noted as being statistically significant, there is a 95 percent probability that a real change has occurred, and is not simply due to chance variation. (emphasis added)\n\nNope! That’s not what p < .05 means. That’s not what 95% confidence means to a frequentist statistician. The bolded section is just plain wrong. Orthodox methods cannot tell you that “there is a 95% chance that a real change has occurred”, because this is not the kind of event to which frequentist probabilities may be assigned. To an ideological frequentist, this sentence should be meaningless. Even if you’re a more pragmatic frequentist, it’s still the wrong definition of a p-value. It is simply not an allowed or correct thing to say if you want to rely on orthodox statistical tools.\nOn the other hand, let’s suppose you are a Bayesian. Although the bolded passage is the wrong definition of a p-value, it’s pretty much exactly what a Bayesian means when they say that the posterior probability of the alternative hypothesis is greater than 95%. And here’s the thing. If the Bayesian posterior is actually the thing you want to report, why are you even trying to use orthodox methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.\nSpeaking for myself, I found this to be the most liberating thing about switching to the Bayesian view. Once you’ve made the jump, you no longer have to wrap your head around counter-intuitive definitions of p-values. You don’t have to bother remembering why you can’t say that you’re 95% confident that the true mean lies within some interval. All you have to do is be honest about what you believed before you ran the study and then report what you learned from doing it. Sounds nice, doesn’t it? To me, this is the big promise of the Bayesian approach. You do the analysis you really want to do, and express what you really believe the data are telling you.\n\n\n16.3.2 你能相信的證據標準\n\nIf \\(p\\) is below .02 it is strongly indicated that the \\(null\\) hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that smaller values of \\(p\\) indicate a real discrepancy.\n– Sir Ronald Fisher (Fisher, 1925)\n\nConsider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of p-values, it’s Fisher. In this passage, taken from his classic guide Statistical Methods for Research Workers, he’s pretty clear about what it means to reject a null hypothesis at p < .05. In his opinion, if we take p < .05 to mean there is “a real effect”, then “we shall not often be astray”. This view is hardly unusual. In my experience, most practitioners express views very similar to Fisher’s. In essence, the p < .05 convention is assumed to represent a fairly stringent evidential standard.\nWell, how true is that? One way to approach this question is to try to convert p-values to Bayes factors, and see how the two compare. It’s not an easy thing to do because a p-value is a fundamentally different kind of calculation to a Bayes factor, and they don’t measure the same thing. However, there have been some attempts to work out the relationship between the two, and it’s somewhat surprising. For example, Johnson (2013) presents a pretty compelling case that (for t-tests at least) the p < .05 threshold corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If that’s right, then Fisher’s claim is a bit of a stretch. Let’s suppose that the null hypothesis is true about half the time (i.e., the prior probability of \\(H_0\\) is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at p < .05. Using the data from Johnson (2013), we see that if you reject the null at p ă .05, you’ll be correct about 80% of the time. I don’t know about you but, in my opinion, an evidential standard that ensures you’ll be wrong on 20% of your decisions isn’t good enough. The fact remains that, quite contrary to Fisher’s claim, if you reject at p < .05 you shall quite often go astray. It’s not a very stringent evidential threshold at all.\n\n\n16.3.3 p值是幻象\n\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\n– Portal11\n\nOkay, at this point you might be thinking that the real problem is not with orthodox statistics, just the p < .05 standard. In one sense, that’s true. The recommendation that Johnson (2013) gives is not that “everyone must be a Bayesian now”. Instead, the suggestion is that it would be wiser to shift the conventional standard to something like a p < .01 level. That’s not an unreasonable view to take, but in my view the problem is a little more severe than that. In my opinion, there’s a fairly big problem built into the way most (but not all) orthodox hypothesis tests are constructed. They are grossly naive about how humans actually do research, and because of this most p-values are wrong.\nSounds like an absurd claim, right? Well, consider the following scenario. You’ve come up with a really exciting research hypothesis and you design a study to test it. You’re very diligent, so you run a power analysis to work out what your sample size should be, and you run the study. You run your hypothesis test and out pops a p-value of 0.072. Really bloody annoying, right?\nWhat should you do? Here are some possibilities:\n\nYou conclude that there is no effect and try to publish it as a null result\nYou guess that there might be an effect and try to publish it as a “borderline significant” result\nYou give up and try a new study\nYou collect some more data to see if the p value goes up or (preferably!) drops below the “magic” criterion of p < .05\n\nWhich would you choose? Before reading any further, I urge you to take some time to think about it. Be honest with yourself. But don’t stress about it too much, because you’re screwed no matter what you choose. Based on my own experiences as an author, reviewer and editor, as well as stories I’ve heard from others, here’s what will happen in each case:\n\nLet’s start with option 1. If you try to publish it as a null result, the paper will struggle to be published. Some reviewers will think that p = .072 is not really a null result. They’ll argue it’s borderline significant. Other reviewers will agree it’s a null result but will claim that even though some null results are publishable, yours isn’t. One or two reviewers might even be on your side, but you’ll be fighting an uphill battle to get it through.\nOkay, let’s think about option number 2. Suppose you try to publish it as a borderline significant result. Some reviewers will claim that it’s a null result and should not be published. Others will claim that the evidence is ambiguous, and that you should collect more data until you get a clear significant result. Again, the publication process does not favour you.\nGiven the difficulties in publishing an “ambiguous” result like p = .072, option number 3 might seem tempting: give up and do something else. But that’s a recipe for career suicide. If you give up and try a new project every time you find yourself faced with ambiguity, your work will never be published. And if you’re in academia without a publication record you can lose your job. So that option is out.\nIt looks like you’re stuck with option 4. You don’t have conclusive results, so you decide to collect some more data and re-run the analysis. Seems sensible, but unfortunately for you, if you do this all of your p-values are now incorrect. All of them. Not just the p-values that you calculated for this study. All of them. All the p-values you calculated in the past and all the p-values you will calculate in the future. Fortunately, no-one will notice. You’ll get published, and you’ll have lied.\n\nWait, what? How can that last part be true? I mean, it sounds like a perfectly reasonable strategy doesn’t it? You collected some data, the results weren’t conclusive, so now what you want to do is collect more data until the the results are conclusive. What’s wrong with that?\nHonestly, there’s nothing wrong with it. It’s a reasonable, sensible and rational thing to do. In real life, this is exactly what every researcher does. Unfortunately, the theory of null hypothesis testing as I described it in Chapter 9 forbids you from doing this.12 The reason is that the theory assumes that the experiment is finished and all the data are in. And because it assumes the experiment is over, it only considers two possible decisions. If you’re using the conventional p < .05 threshold, those decisions are shown oin Table 16.11.\n\n\n\n\nTable 16.11:  Conventional Null hypothesis signicance testing (NHST) with p < .05) \n\nOutcomeAction\n\np less than .05Reject the null\n\np greater than .05Retain the null\n\n\n\n\n\nWhat you’re doing is adding a third possible action to the decision making problem. Specifically, what you’re doing is using the p-value itself as a reason to justify continuing the experiment. And as a consequence you’ve transformed the decision-making procedure into one that looks more like Table 16.12.\n\n\n\n\nTable 16.12:  Carrying on data collecting based on p-values obtained in preliminary testing \n\nOutcomeAction\n\np less than .05Stop the experiment and reject the null\n\np between .05 and .1Continue the experiment\n\np greater than .1Stop the experiment and retain the null\n\n\n\n\n\nThe “basic” theory of null hypothesis testing isn’t built to handle this sort of thing, not in the form I described in Chapter 9. If you’re the kind of person who would choose to “collect more data” in real life, it implies that you are not making decisions in accordance with the rules of null hypothesis testing. Even if you happen to arrive at the same decision as the hypothesis test, you aren’t following the decision process it implies, and it’s this failure to follow the process that is causing the problem.13 Your p-values are a lie.\nWorse yet, they’re a lie in a dangerous way, because they’re all too small. To give you a sense of just how bad it can be, consider the following (worst case) scenario. Imagine you’re a really super-enthusiastic researcher on a tight budget who didn’t pay any attention to my warnings above. You design a study comparing two groups. You desperately want to see a significant result at the \\(p < .05\\) level, but you really don’t want to collect any more data than you have to (because it’s expensive). In order to cut costs you start collecting data but every time a new observation arrives you run a t-test on your data. If the t-tests says \\(p < .05\\) then you stop the experiment and report a significant result. If not, you keep collecting data. You keep doing this until you reach your pre-defined spending limit for this experiment. Let’s say that limit kicks in at \\(N = 1000\\) observations. As it turns out, the truth of the matter is that there is no real effect to be found: the null hypothesis is true. So, what’s the chance that you’ll make it to the end of the experiment and (correctly) conclude that there is no effect? In an ideal world, the answer here should be 95%. After all, the whole point of the \\(p < .05\\) criterion is to control the Type I error rate at 5%, so what we’d hope is that there’s only a 5% chance of falsely rejecting the null hypothesis in this situation. However, there’s no guarantee that will be true. You’re breaking the rules. Because you’re running tests repeatedly, “peeking” at your data to see if you’ve gotten a significant result, all bets are off.\n\nSo how bad is it? The answer is shown as the solid black line in Figure 16.1, and it’s astoundingly bad. If you peek at your data after every single observation, there is a 49% chance that you will make a Type I error. That’s, um, quite a bit bigger than the 5% that it’s supposed to be. By way of comparison, imagine that you had used the following strategy. Start collecting data. Every single time an observation arrives, run [Bayesian t-tests] and look at the Bayes factor. I’ll assume that Johnson (2013) is right, and I’ll treat a Bayes factor of 3:1 as roughly equivalent to a p-value of .05.14 This time around, our trigger happy researcher uses the following procedure. If the Bayes factor is 3:1 or more in favour of the null, stop the experiment and retain the null. If it is 3:1 or more in favour of the alternative, stop the experiment and reject the null. Otherwise continue testing. Now, just like last time, let’s assume that the null hypothesis is true. What happens? As it happens, I ran the simulations for this scenario too, and the results are shown as the dashed line in Figure 16.1. It turns out that the Type I error rate is much much lower than the 49% rate that we were getting by using the orthodox t-test.\n\n\n\n\n\nFigure 16.1: How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is very wrong\n\n\n\n\nIn some ways, this is remarkable. The entire point of orthodox null hypothesis testing is to control the Type I error rate. Bayesian methods aren’t actually designed to do this at all. Yet, as it turns out, when faced with a “trigger happy” researcher who keeps running hypothesis tests as the data come in, the Bayesian approach is much more effective. Even the 3:1 standard, which most Bayesians would consider unacceptably lax, is much safer than the p < .05 rule.\n\n\n16.3.4 真的那麼糟糕嗎？\nThe example I gave in the previous section is a pretty extreme situation. In real life, people don’t run hypothesis tests every time a new observation arrives. So it’s not fair to say that the p < .05 threshold “really” corresponds to a 49% Type I error rate (i.e., \\(p = .49\\)). But the fact remains that if you want your p-values to be honest then you either have to switch to a completely different way of doing hypothesis tests or enforce a strict rule of no peeking. You are not allowed to use the data to decide when to terminate the experiment. You are not allowed to look at a “borderline” p-value and decide to collect more data. You aren’t even allowed to change your data analyis strategy after looking at data. You are strictly required to follow these rules, otherwise the p-values you calculate will be nonsense.\nAnd yes, these rules are surprisingly strict. As a class exercise a couple of years back, I asked students to think about this scenario. Suppose you started running your study with the intention of collecting \\(N = 80\\) people. When the study starts out you follow the rules, refusing to look at the data or run any tests. But when you reach \\(N = 50\\) your willpower gives in… and you take a peek. Guess what? You’ve got a significant result! Now, sure, you know you said that you’d keep running the study out to a sample size of \\(N = 80\\), but it seems sort of pointless now, right? The result is significant with a sample size of \\(N = 50\\), so wouldn’t it be wasteful and inefficient to keep collecting data? Aren’t you tempted to stop? Just a little? Well, keep in mind that if you do, your Type I error rate at \\(p < .05\\) just ballooned out to 8%. When you report \\(p < .05\\) in your paper, what you’re really saying is \\(p < .08\\). That’s how bad the consequences of “just one peek” can be.\nNow consider this. The scientific literature is filled with t-tests, ANOVAs, regressions and chi-square tests. When I wrote this book I didn’t pick these tests arbitrarily. The reason why these four tools appear in most introductory statistics texts is that these are the bread and butter tools of science. None of these tools include a correction to deal with “data peeking”: they all assume that you’re not doing it. But how realistic is that assumption? In real life, how many people do you think have “peeked” at their data before the experiment was finished and adapted their subsequent behaviour after seeing what the data looked like? Except when the sampling procedure is fixed by an external constraint, I’m guessing the answer is “most people have done it”. If that has happened, you can infer that the reported p-values are wrong. Worse yet, because we don’t know what decision process they actually followed, we have no way to know what the p-values should have been. You can’t compute a p-value when you don’t know the decision making procedure that the researcher used. And so the reported p-value remains a lie.\nGiven all of the above, what is the take home message? It’s not that Bayesian methods are foolproof. If a researcher is determined to cheat, they can always do so. Bayes’ rule cannot stop people from lying, nor can it stop them from rigging an experiment. That’s not my point here. My point is the same one I made at the very beginning of the book in Section 1.1: the reason why we run statistical tests is to protect us from ourselves. And the reason why “data peeking” is such a concern is that it’s so tempting, even for honest researchers. A theory for statistical inference has to acknowledge this. Yes, you might try to defend p-values by saying that it’s the fault of the researcher for not using them properly, but to my mind that misses the point. A theory of statistical inference that is so completely naive about humans that it doesn’t even consider the possibility that the researcher might look at their own data isn’t a theory worth having. In essence, my point is this:\n\nGood laws have their origins in bad morals.\n– Ambrosius Macrobius 15\n\nGood rules for statistical testing have to acknowledge human frailty. None of us are without sin. None of us are beyond temptation. A good system for statistical inference should still work even when it is used by actual humans. Orthodox null hypothesis testing does not.16"
  },
  {
    "objectID": "16-Bayesian-statistics.html#貝氏t檢定",
    "href": "16-Bayesian-statistics.html#貝氏t檢定",
    "title": "16  貝氏統計",
    "section": "16.4 貝氏t檢定",
    "text": "16.4 貝氏t檢定\nAn important type of statistical inference problem discussed in this book is comparing two means, discussed in some detail in Chapter 11 on t-tests. If you can remember back that far, you’ll recall that there are several versions of the t-test. I’ll talk a little about Bayesian versions of the independent samples t-tests and the paired samples t-test in this section.\n\n16.4.1 獨立樣本t檢定\nThe most common type of t-test is the independent samples t-test, and it arises when you have data as in the harpo.csv data set that we used in Chapter 11 on t-tests. In this data set, we have two groups of students, those who received lessons from Anastasia and those who took their classes with Bernadette. The question we want to answer is whether there’s any difference in the grades received by these two groups of students. Back in Chapter 11 I suggested you could analyse this kind of data using the Independent Samples t-test in jamovi, which gave us the results in Figure 16.2. As we obtain a p-value less than 0.05, we reject the null hypothesis.\n\n\n\n\n\nFigure 16.2: Independent Samples t-test result in jamovi\n\n\n\n\nWhat does the Bayesian version of the t-test look like? We can get the Bayes factor analysis by selecting the ‘Bayes factor’ checkbox under the ‘Tests’ option, and accepting the suggested default value for the ‘Prior’. This gives the results shown in the table in Figure 16.3. What we get in this table is a Bayes factor statistic of 1.75, meaning that the evidence provided by these data are about 1.8:1 in favour of the alternative hypothesis.\nBefore moving on, it’s worth highlighting the difference between the orthodox test results and the Bayesian one. According to the orthodox test, we obtained a significant result, though only barely. Nevertheless, many people would happily accept p = .043 as reasonably strong evidence for an effect. In contrast, notice that the Bayesian test doesn’t even reach 2:1 odds in favour of an effect, and would be considered very weak evidence at best. In my experience that’s a pretty typical outcome. Bayesian methods usually require more evidence before rejecting the null.\n\n\n\n\n\nFigure 16.3: Bayes factors analysis alongside Independent Samples t-Test\n\n\n\n\n\n\n16.4.2 相依樣本t檢定\nBack in Section 11.5 I discussed the chico.csv data set in which student grades were measured on two tests, and we were interested in finding out whether grades went up from test 1 to test 2. Because every student did both tests, the tool we used to analyse the data was a paired samples t-test. Figure 16.4 shows the jamovi results table for the conventional paired t-test alongside the Bayes factor analysis. At this point, I hope you can read this output without any difficulty. The data provide evidence of about 6000:1 in favour of the alternative. We could probably reject the null with some confidence!\n\n\n\n\n\nFigure 16.4: Paired samples T-Test and Bayes Factor result in jamovi"
  },
  {
    "objectID": "16-Bayesian-statistics.html#本章小結",
    "href": "16-Bayesian-statistics.html#本章小結",
    "title": "16  貝氏統計",
    "section": "16.5 本章小結",
    "text": "16.5 本章小結\n本章前半部主要討論貝氏統計學的理論基礎。理性者的機率推論這一節介紹貝氏推論的數學推導，接著概述貝氏假設檢定。然後我在為何需要貝氏統計說明希望同學學習貝氏統計的理由。\n後半部是實例說明及演練貝氏t檢定。如果同學想知道更多貝氏統計，市面上已經有許多好書問世，等著你去研讀學習。 Kruschke (2011) 撰寫的教科書實作貝氏資料分析(Doing Bayesian Data Analysis)結合理與實作範例，是不錯的第一選擇17。這本書並非以本章介紹的”貝氏因子”介紹具氏統計，所以你不必充分了解本章內容也能閱讀學習。如果是對認知心理學有興趣的同學，應該要找 Lee & Wagenmakers (2014) 的教科書做為進階學習。原作者推薦以上兩本是因為作者的專長領域相近，還有許多不同領域學者所撰寫的貝氏統計教科書或教程，有心學習的同學請好好留意。\n\n\n\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nJeffreys, H. (1961). The theory of probability (3rd ed.). Oxford.\n\n\nJohnson, V. E. (2013). Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 48, 19313–19317.\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773–795.\n\n\nKruschke, J. K. (2011). Doing Bayesian data analysis: A tutorial with R and BUGS. Academic Press.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press."
  },
  {
    "objectID": "index.html#作者序",
    "href": "index.html#作者序",
    "title": "用jamovi上手統計學",
    "section": "作者序",
    "text": "作者序\n本書內容涵蓋各大學心理學、公共衛生或社會科學大學部基礎統計的學習項目。本書同時提供以jamovi做為處理資料的工具操作指引。如同一般的統計教科書，本書從描述統計及統計圖開始，接著討論機率理論，取樣及估計，還有虛無假設檢定。理解理論概念之後，接著學習統計方法：包括列聯表分析、相關、t檢定、迴歸、變異數分析以及因素分析。最後一章將介紹貝氏統計。"
  },
  {
    "objectID": "index.html#譯者序",
    "href": "index.html#譯者序",
    "title": "用jamovi上手統計學",
    "section": "譯者序",
    "text": "譯者序\n\n\n原書作者Danielle Navarro與David Foxcroft是任教於澳洲阿得雷德大學心理學系，教授統計學的講師，本書內容是根據Danielle Navarro自行開發的講義Learning Statistics with R改編，書中有許多例子是採用澳洲政府或民間機構收集的資料。除了解釋統計原理，原作者的大部分用詞接近日常對話，因此除了討論重要觀念的部分，中文翻譯儘可能使用日常用語。如果有部分內容需要進一步解釋或增加說明，可參考譯者自行增加的腳註。 本書使用的專用名詞譯名，主要取自國家教育研究院營運的樂詞網以及中文維基百科。未提供翻譯的名詞皆保留英文名稱；不同領域有不一樣的翻譯名詞，儘可能優先考慮心理學或統計學的譯名，未來學術界對於專有名詞有統一翻譯，本書將配合更新。本書部分內容使用OpenAI開發的自然語言聊天機器人ChatGPT進行初步翻譯，再由譯者編修。有關中文版內容的任何指教，歡迎到本書github儲存庫的討論區提出。\n\n使用這本電子書修習任何實體或線上課程的同學，請先觀看導覽影片，了解如何使用這本書及jamovi開發團隊的資源學習。\n\n\n\n\n引用建議(英): Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#談一談辛普森悖論",
    "href": "01-Why-do-we-learn-statistics.html#談一談辛普森悖論",
    "title": "1  為什麼要學習統計",
    "section": "1.2 談一談辛普森悖論",
    "text": "1.2 談一談辛普森悖論\n接著來談一則真實故事(我想應該是真的)。1973年，美國加州大學柏克萊分校高層擔憂該年研究所的入學申請人數及錄取的狀況。更明白的說，他們覺得錄取的學生呈現性別不平等的狀況(見表1.4)。\n\n\n  表1.4 柏克萊男女新生報考人數及錄取比例\n    \n         \n        申請者人數\n        通過比例\n    \n    \n        男性\n        8442\n        44\\(\\%\\)\n    \n    \n        女性\n        4321\n        35\\(\\%\\)\n    \n\n\n當年的柏克萊校方擔心被申請入學的學生告上法院！由於將近有13,000名申請者，男女之間的錄取率相差9％，這樣的差距實在太大了，不可能是巧合。而且人數如此龐大，可說是鐵一般的事實。但是如果我對你說，這些數據實際上反映了對女性申請者些微的偏袒，你可能會認為我搞錯了或者有性別歧視。\n\n\n然而，實際情況卻有點出人意料。仔細檢視錄取資料後，有人發現了另一個版本的故事(Bickel et al., 1975)。具體地說，當校方按照學系逐一計算錄取率時，會看到多數學系的女性錄取率實際上略高於男性。表1.5 顯示了六個最多申請者的學系錄取情形（為了保障隱私，以下省略學系名稱）：\n\n\n  表1.5 1973年伯克萊大學六個學系錄取學生的性別分佈\n    \n         \n         男性\n         \n         女性\n         \n    \n    \n        學系\n        申請者人數\n        錄取比例\n        申請者人數\n        錄取比例\n    \n    \n        A\n        825\n        62\\(\\%\\)\n        108\n        82\\(\\%\\)\n    \n    \n        B\n        560\n        63\\(\\%\\)\n        25\n        68\\(\\%\\)\n    \n    \n        C\n        325\n        37\\(\\%\\)\n        593\n        34\\(\\%\\)\n    \n    \n        D\n        417\n        33\\(\\%\\)\n        375\n        35\\(\\%\\)\n    \n    \n        E\n        191\n        28\\(\\%\\)\n        393\n        24\\(\\%\\)\n    \n    \n        F\n        272\n        6\\(\\%\\)\n        341\n        7\\(\\%\\)\n    \n\n\n令人費解的是，大多數系所的女性錄取率都比男性高！但是整個大學的女性錄取率卻低於男性。這怎麼可能？這兩種說法怎麼可能同時成立？\n\n其中究竟發生了什麼事。首先，請留意各系的錄取率並不相同：某些學系，像是A系和B系，傾向錄取最多合格申請者，而其他學系則寧缺勿濫，像是F系，即使申請者資質不差，也傾向不錄取多數申請者。表1.5顯示的六個學系，A系是最好上榜的，其他五系錄取率依遞減。其次請注意，男生和女生申請的學系並不相同。以男性申請者人數排序，會看到六個系的錄取率排序是A>B>D>C>F>E(粗體字是“最好上“的學系)。整體而言，男生偏好申請錄取率高的學系。接著比較一下各系女性申請者的分佈情況。以女生申請者人數排序，就會發現六個系錄取率的排序是C>E>D>F>A>B。也就是說，申請人數似乎顯示多數女生申請”很硬”的學系。事實上，如果我們看一下圖1.1，會發現這樣的趨勢是系統性的，並且非常顯著。這種效應被稱為辛普森悖論。這種效應並不常見，但確實在現實世界曾經發生，大多數人第一次遇到這種現象都會非常驚訝，甚至許多人拒絕相信這是真實的現象。但是這種現象再真實不過。雖然其中有很多很微妙的統計教訓，但我想用這個例子指出一個更重要的教訓：每個研究都是困難的，往往隱藏很多很微妙，違反人類直覺的陷阱等著不謹慎的人掉進去。這也是科學家喜歡統計的第二個原因，也是為什麼這門課要教研究方法的原因。因為科學很難，而且真相有時會巧妙地隱藏在複雜數據的縫隙之間。\n\n在結束這個主題之前，我想指出一些研究方法課程常常忽略的事情。那就是統計只解決了問題的一部分。記得我們一開始關注是伯克利大學的招生程序可能對女性申請者不公平。當我們檢視”聚合”的資料時，整體似乎指向柏克萊明顯歧視女性，但是當我們”分解”各個學系的資料並深入檢視男女生個人行為，其實資料顯示各學系招生狀況的差異。如果真的有偏見，其實是各系輕微偏好錄取女生。總錄取率的性別偏差其實是因為女生傾向選擇較難上榜的系所。從法律角度來看，大學高層並沒有任何責仼。要錄取誰當研究生，權責是在個別系所，並且每個系所都有充分的理由決定要怎麼做。在系所的層次，錄取決策幾乎是公平的（各系所偏好錄取女性偏好很微弱，並且不一致）。由於大學高層無法決定學生想申請哪些科系，並且決策權限在各系所，所以校方幾乎不能干預招生程序，也無需對任何偏見負起責任。\n\n\n\n\n\n\n\nFigure 1.1: 1973年柏克萊大學招生數據，取自 Bickel et al. (1975) 的圖1。圖中85個點分別代表至少接受一位女性申請入學的學系，依不分性別錄取率與女性申請入學比例繪圖。圓圈代表申請者超過40人的學系；圓圈面積代表該系申請人數佔全校申請人數的比例。十字代表申請者未達40人的學系。\n\n\n\n\n這是一開始我用輕鬆閒話介紹這個案例的真正原由，但故事沒這麼簡單，對吧？畢竟，如果從社會學和心理學的角度來看這個問題，我們更想知道為什麼各系申請者有這麼大的性別差異。為什麼工程科學系的男性申請者比女性多，而英語系則是相反呢？為什麼那些女性申請者較多的科系，錄取率比男性申請者較多的系別低呢？為什麽女性申請人數較多的科系錄取率偏低，而男性申請人數較多科系錄取率卻偏高？即使每個科系取才程序都是公平的，但這是否也是一種性別偏見？也許是吧。我們可以探討為何多數男生想唸“硬科學”領域的系所，而女生大都偏好“人文”領域。我們還能探討為何人文領域系所錄取率低？是因為政府部門給的補助不夠多嗎？例如博士級職缺與政府能給的專案補助經費額度有關。這些都是造成性別偏誤的條件嗎？還是人文領域的價值並未被重視？如果政府官員都覺得人文領域只是“沒什麼用處的小玩意”，動輒刪減相關經費。這樣是不是一種公然的性別歧視？至此討論的各種問題，都超出統計能解決的範圍，不過都能形成有意義的研究專案。若是你想了解造成性別歧視的整體結構因果關係，可能要”聚合”與“分解”的資料都要檢視。若是只想探討柏克萊校內負責招生的各級部門如何決策，只要檢討“分解”的資料就行。\n\n簡而言之，有很多重要問題是無法只靠統計數據回答的，但是分析和解釋數據對於回答這些問題有相當巨大的作用。這就是為什麼你應該把統計學當成解析數據的工具，因為正好符合心理學等領域的需要。即使統計是一個非常好用的工具，人類在獲得謹慎思考的道路上並沒有捷徑。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#匯入非示範格式的資料檔",
    "href": "03-Getting-started-with-jamovi.html#匯入非示範格式的資料檔",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.5 匯入非示範格式的資料檔",
    "text": "3.5 匯入非示範格式的資料檔\n本書的示範資料都是以jamovi專案檔(.omv)或csv文字檔提供讀者操作7。不過在你的實際統計實務場景，非常有可能遇到各式各樣的資料檔。這個小節我分享一些很多人常遇到的其他格式的資料，以及不合乎格式的資料檔處理技巧。\n\n\n3.5.1 格式純出錯的文字資料檔\n首先來談談遇到純文字資料檔案不符合csv格式，要如何修正才能讓jamovi開啟。出錯的地方大致來自以下幾個細節，有時需要邊修邊試，才能讓純文字資料檔案符合csv格式：\n\n變項名稱。通常第一列應該是變項名稱而非資料，如果不是的話jamovi可以開啟，但是無法做正確的資料分析。這種狀況可以用試算表軟體重新開啟，手動編輯應該有的變項名稱。\n欄位間隔。合格的csv檔以半形逗號(,)區隔變項欄位，不過有些歐洲國家習慣用半形逗號代表小數點，因此csv檔的欄位間隔是以半形分號(;)代表。台灣各單位資料的csv檔會混合使用半形逗號與製表鍵(Tab鍵)代表欄位間隔。如果遇到請先置換並存檔後再用jamovi開啟。\n引號。如同 booksales.csv 的示範，csv資料檔案內的文字資料都是包 在兩個半形雙引號(“)之間。有的csv資料檔案會用半形單引號(’)。\n跳列資訊。有的csv檔案內會在開頭幾行插入非資料的說明資訊，因為不符合檔案格式，無法被正確讀取。\n遺漏值。統計實務最常要處理的狀況就是遺漏值。遺漏值的來源千奇百怪，用心的資料收集者會用特別的記號代表遺漏值。不論是數字還是文字資料，jamovi預設以NA代表遺漏值8。請注意用jamovi開啟資料檔之前，確保資料內的遺漏值都已轉換為預設記號。開啟資料庫之後，試算表介面可看到的遺漏值會以空白或灰階顯示。如果某個變項裡有不符合預設記號的遺漏值，可以開啟Data面板的Setup對話視窗，自行加入遺漏值記號。\n\n\n\n\n\n3.5.2 套裝軟體專用格式(如SPSS)\n如果你習慣了使用jamovi進行統計分析，在非本書示範的真實場景，你有可能要處理的資料檔是套裝軟體專用格式。最常見的是SPSS資料檔(.sav)，最新版本的jamovi都可以匯入，操作如同開啟csv資料檔。但是要注意資料中的遺漏值除非標記是”system missing”或jamovi的預設記號，其他遺漏值記號都會被當成是有效數值。建議第一次匯入SPSS資料庫，馬上匯出為csv資料檔，在試算表軟體裡處理好遺漏值再用jamovi重新開啟9。\n如果遺漏值問題能解決，jamovi能做和SPSS一樣的分析。其他套裝統計軟體的資料檔，像是SAS與STATA，也可以用同樣的操作處理。\n\n\n\n3.5.3 微軟Excel資料格式\n本書原作者很不喜歡資料以Excel格式存檔，因為用jamovi開啟Excel檔案問題更多。如果你處理的作業遇到的話，最好先用試算表軟體開啟，另存為csv文字檔，再用jamovi開啟。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#本章小結",
    "href": "03-Getting-started-with-jamovi.html#本章小結",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 本章小結",
    "text": "3.9 本章小結\n每本統計軟體教科書都會有這些大同小異的起始課程。本書也不例外，幫助讀者通過最少障礙上手jamovi。這一章的各節主題重點如下：\n\n安裝jamovi 下載、安裝、啟動jamovi\n分析模組 簡介主要介面各部分功能，像是如何開始分析，結果輸出位置。\n資料試算表 了解資料試算表的特色，各種變項設定，建立計算變項。\n載入資料檔案 示範開啟要分析的資料。\n匯入非示範格式的資料檔 開啟各種格式資料庫的注意事項。\n更動資料變項性質 如何改變變項尺度。\n安裝jamovi擴充模組 找到及安裝社區使用者開發的模組。\n關閉jamovi 關閉之前如何存檔的建議。\n\n到這裡還沒有真的開始處理資料，下一章才是真正的開始。"
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯運算",
    "href": "06-Pragmatic-matters.html#邏輯運算",
    "title": "6  實務課題",
    "section": "6.2 邏輯運算",
    "text": "6.2 邏輯運算\n在jamovi進行資料轉換，多數狀況都要使用邏輯值設定轉換條件。邏輯值是指根據某種條件評估，評估結果為真或為假。用jamovi執行邏輯評估相當直覺，執行結果只有TRUE或FALSE兩種數值。儘管簡單，邏輯值在許多統計實務場景相當有用。以下介紹常見的運作範例。\n\n\n6.2.1 判斷算式真假值\n喬治．歐威爾的著名反烏托邦小說「1984」虛構的極權政府經常向不順從的人民宣傳「2 + 2 = 5」。這個設定表達了當政治極權壓過了人類的自由思想，有可能會顛覆人們對現實世界的基本認知。小說中驚悚的高潮，是當男主角史密斯．溫斯頓終於無法忍受刑求，接受了極權宣傳，同意「2 + 2 = 5」。小說有個主張「人的可塑性是無限的」，原作者相當不同意這樣的主張，同意的話就無法使用jamovi了。至少在處理基本邏輯運算的問題，jamovi不可能給出「刻意塑造」的答案。同學們可以試著在jamovi計算變項設定選單，輸入 \\(2 + 2\\) 看看會不會得到極權政府要人民相信的52！\n到目前所學的，都是使用jamovi進行計算。還沒有做過判斷 \\(2 + 2 = 4\\) 為真的例子。同學只要在計算變項設定選單的公式視窗裡輸入 \\(2 + 2 == 4\\) ，就會在試算表介面看到真值(true)。\n這個例子用到了一種邏輯運算子：相等運算子 \\(==\\) 3。我們現在可以用這個運算子，看看jamovi會不會服從極權統治，請在公式視窗輸入小說裡的判斷式：\n\\[2 + 2 == 5\\]\n這個例子是讓同學們牛刀小試，了解如何在jamovi裡設定邏輯判斷式。如果同學有機會使用其他程式語言測試，像是R，會得到錯誤訊息。最新版的jamovi也會偵測使用者輸入的相等運算子是否正確，如果輸入錯誤，就會得到相當醒目的錯誤訊息4。\n\n\n\n6.2.2 邏輯運算子\n透過上一節的簡單示範，我們了解邏輯運算子如何運作，結合其他運算子與函式會有更多用途。像是這兩個例子： \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) 還有 \\(SQRT(25) == 5\\)\n除此之外，還有其他的基本邏輯運算子，讓我們能在適合條件下運用，如同 Table 6.2 的示範。希望同學能從示範中自行認識各種運算子的功能。像是運算子 \\(<\\) 用來計算左邊的數值是不是小於右邊的數值。如果運算結果為真，jamovi會傳回TRUE。如果數值相等，或者右邊的數值實際小於左邊的數值，jamovi則會傳回FALSE。\n另一方面，小於或等於運算子 \\(<=\\) 的功能正如字面的意思，只要左邊的數值小於或等於右邊的數值就會傳回TRUE。相對地，大於運算子 \\(>\\) 與大於或等於運算子 \\(>=\\) 則是會傳回相反的真假值。\n現在要理解不等於運算子 \\(!=\\) 的功能就比較簡單了。只有兩邊的數值都不一樣，運算結果才會是TRUE。如以下的例子，我們只要心算就知道 \\(2 + 2\\) 不會等於 \\(5\\)。同學可以開啟你的jamovi試試看。\n\n\\[2 + 2 \\text{ != } 5\\]\n\n\n\n\nTable 6.2:  比較數值的邏輯運算子 \n \n  \n    運算功能 \n    運算子 \n    輸入範例 \n    輸出 \n  \n \n\n  \n    小於 \n    < \n    2 < 3 \n    TRUE \n  \n  \n    小於或等於 \n    <= \n    2 <= 2 \n    TRUE \n  \n  \n    大於 \n    > \n    2 > 3 \n    FALSE \n  \n  \n    大於或等於 \n    >= \n    2 >= 2 \n    TRUE \n  \n  \n    等於 \n    == \n    2 == 3 \n    FALSE \n  \n  \n    不等於 \n    != \n    2 != 3 \n    TRUE \n  \n\n\n\n\n\n\n在 Table 6.3 還有三個邏輯運算子是我們必須學會的。他們是差集運算子\\(NOT\\)，交集運算子\\(and\\)，以及聯集運算子\\(or\\)。每個運算子的功能也如同名字的字面意思。像是我現在問同學”\\(2 + 2 = 4\\) 或 \\(2 + 2 = 5\\)“其中一個算式是真的嗎？同學一定要答”是“。因為用”或”連結的兩個命題，我們只要確定其中一個算式是真的，整個命題就為真。5\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\n\n換個問法，如果現在是問你”\\(2 + 2 = 4\\) 且 \\(2 + 2 = 5\\)“都是真的嗎？你應該要答“否”。因為這個命題是交集，兩個算式都要為真，整個命題才為真。在jamovi計算變項設定，我們可以這樣編輯：\n\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\n最後來看拗口的差集運算子。假如我問你“\\(2 + 2 = 5\\)這個算式非真嗎?”，你應該會答“是”。因為正確的命題應該是“\\(2 + 2 = 5\\)為假”。在jamovi計算變項設定，我們可以這樣編輯：\n\n\\[NOT(2+2 == 5)\\]\n也就是 \\(2+2 == 5\\) 的輸出是FALSE，\\(NOT(2+2 == 5)\\) 的輸出就是TRUE。這個例子讓我們曉得“非假”就是“真”。雖然現實世界許多事情真假難辨，至少在jamovi的世界一切非黑即白。資料放到jamovi不是真就是假，沒有灰色地帶。\n其實我們不需要用差集運算子，得到 \\(2+2 == 5\\) 為假的答案，使用“等於”運算子\\(==\\)就是FALSE的輸出，如果要得到TRUE的輸出，使用”不等於”運算子\\(!=\\)就行了：\n\n\\[2+2 \\text{ != } 5\\]\n\n\n\n\nTable 6.3:  差集，交集以及聯集運算子 \n \n  \n    運算功能 \n    運算子 \n    輸入範例 \n    輸出 \n  \n \n\n  \n    差集 \n    NOT \n    NOT(1==1) \n    FALSE \n  \n  \n    聯集 \n    or \n    (1==1) or (2==3) \n    TRUE \n  \n  \n    交集 \n    and \n    (1==1) and (2==3) \n    FALSE \n  \n\n\n\n\n\n\n\n\n6.2.3 在報告中表達邏輯運算子\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#數學函式及運算子",
    "href": "06-Pragmatic-matters.html#數學函式及運算子",
    "title": "6  實務課題",
    "section": "6.4 數學函式及運算子",
    "text": "6.4 數學函式及運算子\nIn the section on [Transforming and recoding a variable] I discussed the ideas behind variable transformations and showed that a lot of the transformations that you might want to apply to your data are based on fairly simple mathematical functions and operations. In this section I want to return to that discussion and mention several other mathematical functions and arithmetic operations that are actually quite useful for a lot of real world data analysis. Table 6.5 gives a brief overview of the various mathematical functions I want to talk about here, or later.9 Obviously this doesn’t even come close to cataloguing the range of possibilities available, but it does cover a range of functions that are used regularly in data analysis and that are available in jamovi.\n\n6.4.1 對數與指數\nAs I’ve mentioned earlier, jamovi has an useful range of mathematical functions built into it and there really wouldn’t be much point in trying to describe or even list all of them. For the most part, I’ve focused only on those functions that are strictly necessary for this book. However I do want to make an exception for logarithms and exponentials. Although they aren’t needed anywhere else in this book, they are everywhere in statistics more broadly. And not only that, there are a lot of situations in which it is convenient to analyse the logarithm of a variable (i.e., to take a “log-transform” of the variable). I suspect that many (maybe most) readers of this book will have encountered logarithms and exponentials before, but from past experience I know that there’s a substantial proportion of students who take a social science statistics class who haven’t touched logarithms since high school, and would appreciate a bit of a refresher.\nIn order to understand logarithms and exponentials, the easiest thing to do is to actually calculate them and see how they relate to other simple calculations. There are three jamovi functions in particular that I want to talk about, namely LN(), LOG10() and EXP(). To start with, let’s consider LOG10(), which is known as the “logarithm in base 10”. The trick to understanding a logarithm is to understand that it’s basically the “opposite” of taking a power. Specifically, the logarithm in base 10 is closely related to the powers of 10. So let’s start by noting that 10-cubed is 1000. Mathematically, we would write this:\n\\[10^3=1000\\]\nThe trick to understanding a logarithm is to recognise that the statement that “10 to the power of 3 is equal to 1000” is equivalent to the statement that “the logarithm (in base 10) of 1000 is equal to 3”. Mathematically, we write this as follows,\n\\[log_{10}(1000)=3\\]\nOkay, since the LOG10() function is related to the powers of 10, you might expect that there are other logarithms (in bases other than 10) that are related to other powers too. And of course that’s true: there’s not really anything mathematically special about the number 10. You and I happen to find it useful because decimal numbers are built around the number 10, but the big bad world of mathematics scoffs at our decimal numbers. Sadly, the universe doesn’t actually care how we write down numbers. Anyway, the consequence of this cosmic indifference is that there’s nothing particularly special about calculating logarithms in base 10. You could, for instance, calculate your logarithms in base 2. Alternatively, a third type of logarithm, and one we see a lot more of in statistics than either base 10 or base 2, is called the natural logarithm, and corresponds to the logarithm in base e. Since you might one day run into it, I’d better explain what e is. The number e, known as Euler’s number, is one of those annoying “irrational” numbers whose decimal expansion is infinitely long, and is considered one of the most important numbers in mathematics. The first few digits of e are:\n\\[e = 2.718282 \\]\nThere are quite a few situation in statistics that require us to calculate powers of \\(e\\), though none of them appear in this book. Raising e to the power \\(x\\) is called the exponential of \\(x\\), and so it’s very common to see \\(e^x\\) written as exppxq. And so it’s no surprise that jamovi has a function that calculates exponentials, called EXP(). Because the number e crops up so often in statistics, the natural logarithm (i.e., logarithm in base e) also tends to turn up. Mathematicians often write it as \\(log_e(x)\\) or \\(ln(x)\\). In fact, jamovi works the same way: the LN() function corresponds to the natural logarithm.\nAnd with that, I think we’ve had quite enough exponentials and logarithms for this book!"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群抽樣",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群抽樣",
    "title": "8  運用樣本估計未知量數",
    "section": "8.1 樣本、母群、抽樣",
    "text": "8.1 樣本、母群、抽樣\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 何謂母群\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 簡單隨機樣本\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 母群參數與樣本統計\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about [Estimating population parameters] using your sample statistics and also [Estimating a confidence interval] but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "09-Hypothesis-testing.html#值得繼續學習的主題",
    "href": "09-Hypothesis-testing.html#值得繼續學習的主題",
    "title": "9  假設檢定",
    "section": "9.9 值得繼續學習的主題",
    "text": "9.9 值得繼續學習的主題\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n9.9.1 尼曼與費雪\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually14 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n9.9.2 貝氏統計與次數主義統計\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 7) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in Chapter 16, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n9.9.3 決策陷阱\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,15 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#兩種決策失誤",
    "href": "09-Hypothesis-testing.html#兩種決策失誤",
    "title": "9  假設檢定",
    "section": "9.2 兩種決策失誤",
    "text": "9.2 兩種決策失誤\n在深入了解如何設定統計檢定程序之前，理解其哲學基礎是很有幫助的。我(原作者)曾經提過，虛無假設檢定和法庭審判之間的相似之處，不過現在要說得更清楚。最理想情況，我們希望每個檢定的結果都不會出錯。不幸的是，由於現實世界太複雜了，這是不可能達成的理想。有時候只是運氣不好，例如像是你測試擲一枚硬幣是否符合隨機，但是連續擲了10次，每次都是正面朝上。這似乎是表明硬幣不夠隨機的有力證據。然而，即使硬幣是完全公平的，出現這種結果的機率是1/1024。換句話說，在現實生活中，我們必須接受假設檢定出錯的可能性。因此，統計假設檢定的目標不是消除錯誤，而是儘可能減低錯誤的機率。\n說到這裡，我們需要更精確地定義什麼是「錯誤」。首先要知道虛無假設最後會被判定為真或為假，會發生什麼狀況。也就是根據檢定，我們會保留或拒絕虛無假設5。如同 Table 9.2 所示，在我們進行檢定並做出選擇後，可能會發生的四種狀況：\n\n\n\n\n\nTable 9.2:  圖解虛無假設檢定(NHST)的四種結果 \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\n我們看到實際上有兩種不同的錯誤類型。如果我們拒絕了實際為真的虛無假設，那麼我們就是犯了型一錯誤。另一方面，當虛無假設實際上是錯的，我們仍然保留它，那麼我們就犯了型二錯誤。\n還記得我之前說過統計檢定有點像法庭審判嗎？我這樣說是認真的。要認定被告有罪，審判長會要求你要提出「超越合理的懷疑」。所有關於證據的法律規定（至少在理論上），都是確保幾乎沒有人能錯誤地判定一個無辜的被告有罪。審判標準如此嚴格的目的是保護被告的權利，英國法官威廉·布萊克斯通曾說過「與其冤枉一個無辜的人，不如放過十個有罪的人。」換句話說，法庭裡審判長不會等價看待兩種類型的錯誤。因為冤枉無辜者的代價遠高於讓罪犯逍遙法外的代價。統計檢定基本上也是這樣。可靠的假設檢定最重要設計原則是控制「型一錯誤」的發生機率，講求控制在某個固定機率以下。這個機率通常以 \\(\\alpha\\) 表示，稱為檢定的顯著水準。我再強調一次，因為這是整個設計的核心，如果「型一錯誤率」不大於 \\(\\alpha\\)，那麼假設檢定的顯著水準就是 \\(\\alpha\\)。\n那麼要怎麼處理型二錯誤率呢？我們也希望能控制在一定範圍內，通常用 \\(\\beta\\) 來表示型二錯誤的發生機率。不過更常見做法的是估計檢定力，也就是虛無假設為假時，拒絕虛無假設的機率，這個機率是 \\(1 - \\beta\\)。為了更好地理解，我們將 Table 9.2 改寫一下，並加入相關數字（見 Table 9.3）：\n\n\n\n\n\n\nTable 9.3:  進一步解析虛無假設檢定(NHST)的四種結果 \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\n一個「有力」的假設檢定是指保持最小 \\(\\beta\\) 值，同時保持 \\(\\alpha\\) 在預期的水準。依照領域慣例，科學家通常會使用三種 \\(\\alpha\\) 水準：\\(0.05\\)、\\(0.01\\) 和 \\(0.001\\)。值得注意的是，兩者的控制強度有一種不對稱性：檢定的目的是確保 \\(\\alpha\\) 值如同預期的小，但是對於 \\(\\beta\\) 則不預其能控制到相應的水準。當然，我們也希望型二錯誤率能夠保持最小，並設計一些測試來實現，但這方面的控制需求通常不如型一錯誤率的迫切。倘若把布萊克斯通 的話換成統計學家的說法，那就是說“保留十個結論錯誤的虛無假設，總比拒絕一個真實的假設要好”。老實說，我不完全同意這種哲學觀點。在某些情況下，我認為這種觀點是有道理的，其他情況則不然。但是這不是重點，重點是這就是假設檢定的設計原則。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "href": "12-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "title": "10  相闗與線性迴歸",
    "section": "10.4 線性迴歸模型的參數估計",
    "text": "10.4 線性迴歸模型的參數估計\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 10.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 10.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 10.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n10.4.1 實作線性迴歸模型\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 10.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 10.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 10.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n10.4.2 解讀線性迴歸模型參數估計\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "href": "12-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "title": "10  相闗與線性迴歸",
    "section": "10.8 迴歸係數的更多資訊",
    "text": "10.8 迴歸係數的更多資訊\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n10.8.1 迴歸係數的信賴區間\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 10.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n10.8.2 標準化迴歸係數的計算方法\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 10.16.\n\n\n\n\n\nFigure 10.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "16-Bayesian-statistics.html#理性者的機率推論",
    "href": "16-Bayesian-statistics.html#理性者的機率推論",
    "title": "16  貝氏統計",
    "section": "16.1 理性者的機率推論",
    "text": "16.1 理性者的機率推論\nFrom a Bayesian perspective statistical inference is all about belief revision. I start out with a set of candidate hypotheses h about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.\n\nI’m carrying an umbrella. Do you think it will rain?\n\nIn this problem I have presented you with a single piece of data (d = I’m carrying the umbrella), and I’m asking you to tell me your belief or hypothesis about whether it’s raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem?\n\n16.1.1 事前機率：你一開始的信念\nThe first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia2 and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see Table 16.1). If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you I’m betting that this really is what you believe) then what I have written here is your prior distribution, written \\(P(h)\\).\n\n\n\n\nTable 16.1:  How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall \n\nHypothesisDegree of Belief\n\nRainy day0.15\n\nDry day0.85\n\n\n\n\n\n\n\n16.1.2 似然值: 對手上資料的理論\nTo solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,3 and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write this out as in Table 16.2.\n\n\n\n\nTable 16.2:  How likely am I to be carrying an umbrella on rainy and dry days \n\nDataData\n\nHypothesisUmbrellaNo umbrella\n\nRainy day0.300.70\n\nDry day0.050.95\n\n\n\n\n\nIt’s important to remember that each cell in this table describes your beliefs about what data d will be observed, given the truth of a particular hypothesis \\(h\\). This “conditional probability” is written \\(P(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. In Bayesian statistics, this is referred to as the likelihood of the data \\(d\\) given the hypothesis \\(h\\).4\n\n\n16.1.3 資料與理論的聯合機率\nAt this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it so I’m going to try to build it up from more basic ideas.\nLet’s start out with one of the rules of probability theory. I listed it way back in Table 7.1, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I’m carrying an umbrella (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(P(d,h)\\), and you can calculate it by multiplying the prior \\(P(h)\\) by the likelihood \\(P(d|h)\\). Mathematically, we say that\n\\[P(d,h)=P(d|h)P(h)\\]\nSo, what is the probability that today is a rainy day and I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is \\(30\\%\\). So the probability that both of these things are true is calculated by multiplying the two\n\\[\n\\begin{split}\nP(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\n& = 0.30 \\times 0.15 \\\\\n& = 0.045\n\\end{split}\n\\]\nIn other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with Table 16.3.\n\n\n\n\nTable 16.3:  Four possibilities combining rain (or not) and umbrella carrying (or not) \n\nUmbrellaNo-umbrella\n\nRainy0.0450.105\n\nDry0.04250.807\n\n\n\n\n\nThis table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us Table 16.4.\n\n\n\n\nTable 16.4:  Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals \n\nUmbrellaNo-umbrellaTotal\n\nRainy0.0450.1050.15\n\nDry0.04250.8070.85\n\nTotal0.08750.9121\n\n\n\n\n\nThis is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course, as that’s our prior.5 The important thing isn’t the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.\nNow, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (Table 16.5):\n\n\n\n\nTable 16.5:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities \n\nUmbrellaNo-umbrella\n\nRainyP(Umbrella, Rainy)P(No-umbrella, Rainy)P(Rainy)\n\nDryP(Umbrella, Dry)P(No-umbrella, Dry)P(Dry)\n\nP(Umbrella)P(No-umbrella)\n\n\n\n\n\nFinally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let \\(d_1\\) refer to the possibility that you observe me carrying an umbrella, and \\(d_2\\) refers to you observing me not carrying one. Similarly, \\(h_1\\) is your hypothesis that today is rainy, and \\(h_2\\) is the hypothesis that it is not. Using this notation, the table looks like Table 16.6.\n\n\n\n\nTable 16.6:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities \n\n\\( d_1 \\)\\( d_2 \\)\n\n\\( h_1 \\)\\(P(h_1, d_1)\\)\\(P(h_1, d_2)\\)\\( P(h_1) \\)\n\n\\( h_2 \\)\\(P(h_2, d_1)\\)\\(P(h_2, d_2)\\)\\( P(h_2) \\)\n\n\\( P(d_1) \\)\\( P(d_2) \\)\n\n\n\n\n\n\n\n16.1.4 透過貝氏法則更新信念\nThe table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now know that I have an umbrella.6 To reflect this new knowledge, our revised table must have the following numbers. (see Table 16.7).\n\n\n\n\nTable 16.7:  Revising beliefs given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0\n\nDry0\n\nTotal10\n\n\n\n\n\nIn other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that \\(P(umbrella) = 1\\).\nWhat two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is slightly more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like Table 16.8, perhaps?\n\n\n\n\nTable 16.8:  Revising probabilities given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0.5140\n\nDry0.4860\n\nTotal10\n\n\n\n\n\nWhat this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The posterior probability of rain \\(P(h\\|d)\\) given that I am carrying an umbrella is 51.4%\nHow did I calculate these numbers? You can probably guess. To work out that there was a \\(0.514\\) probability of “rain”, all I did was take the \\(0.045\\) probability of “rain and umbrella” and divide it by the \\(0.0875\\) chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data \\(P(d, h)\\) by the marginal probability of the data \\(P(d)\\), and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: 7\n\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]\nHowever, remember what I said at the start of the last section, namely that the joint probability \\(P(d, h)\\) is calculated by multiplying the prior Pphq by the likelihood \\(P(d|h)\\). In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:\n\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-sample-population-sampling",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-sample-population-sampling",
    "title": "8  運用樣本估計未知量數",
    "section": "8.1 樣本、母群、取樣",
    "text": "8.1 樣本、母群、取樣\n中場報告提到一則歸納之謎，讓同學從中學習如何建構能判斷故事結局的假設。如果現在你能接受這樣的思考方式，應該能同意任何統計實務都是從接受原始資料的通用假設開始，這就是我們要學習取樣理論的原因。如果說統計理論都是要建立在機率理論的地基之上，取樣理論則是組建統計理論的材料。有效統計推論所需要的條件都是來自取樣理論，也就是要達到統計學者們認可的“推論過程”，我們必須清楚說明推論來自什麼樣本，推論適用於什麼樣的母群。\n身為研究者，在任何要使用統計的狀況，最感興趣的都是資料代表的樣本。像是參加實驗的受試者反應，民調公司打電話詢問民眾投票意向等等。透過這類方式收集的資料，通常是有限且不完整的。因為我們不可能請全世界的人來參加實驗，民調公司也沒有時間和經費詢問所有選民。在 Chapter 4 學習的描述統計，目標只有處理手上使用的資料。那一章只有學到如何描述、總結和視覺化樣本的特點。現在我們要再往前一步了。\n\n\n8.1.1 何謂母群\n樣本是實在的，打看一份資料檔案，儲存的資料就是你能處理的樣本。然而，母群就抽象多了。這個詞是指所有可以用來推導結論的人類行為紀錄，可以觀測到的數值，而且比你能處理的樣本大上許多。最理想的狀態是研究者清楚曉得想研究的母群是什麼模樣，因為設計實驗和分析資料檢測假設的程序，都是根據研究者對於母群的基本認識。\n只有很少的狀況，我們能清楚了解想研究的母群是什麼模樣。例如前面提到的民調公司，要研究的母群是有資格投票的選民，能調查的樣本就是這群選民之中的1000人。多數研究要面對的母群都是模糊的。典型的心理學實驗設定的母群模樣相當複雜。假如我做了一個探討人類如何思考的認知實驗，找了一百位大學生參加實驗。那麼，這個實驗的母群是以下那一種呢？\n\n澳洲阿得雷德大學心理系大學部所有學生?\n世界上所有大學心理系的大學生?\n所有住在台灣的台灣人?\n和原作者/譯者年紀相仿的國民?\n任何世界上活生生的人類?\n過去、現在、和未來的人類?\n任何能在行星環境生存，有足夠智能的生物個體?\n任何有智能的實體?\n\n上述每一條都是有智慧的個體所組成的群體，不過到底是那一個群體才是原作者設計的認知實驗目標母群呢？另一個很難定義清楚母群條件的狀況，就是中場故事提到的瓶子裡裝什麼球的例子1。 例子提到抽出的12個球都是白球，並沒有彩球。我們要如何設定這個例子的母群呢？似乎以下每一條設定都有道理。\n\n瓶子裡的球都被抽完？\n負責抽球的人抽到不想抽為止？\n用機器人抽球，抽到機器人無法運作為止？\n找奇異博士去無限多的平行宇宙查看，看看每個宇宙抽出12個球的結果？\n\n\n\n\n8.1.2 簡單隨機樣本\n不論要採用那種母群的定義，重點是我們要找出代表母群一部分的樣本，還有運用從樣本學到的知識推測母群的性質。樣本和母群的關係建立在選取樣本的的程序。這樣的程序就是取樣方法(sampling method)，理解取樣方法是正確有效運用推論統計的關鍵。\n我們用一個裝有十顆球的的袋子舉例說明取樣方法。每顆球各印有一個字母區別，並且被塗上黑色或白色。這十顆球構成的母群能繪製成像 Figure 8.1 的示意圖一覽無遺。十顆球裡有四顆黑球和六顆白球，但是請假裝還沒打開袋子之前，我們完全不知道每顆球長什麼樣子。接著我們做一次「想像實驗」：先把袋子拿起來搖幾下，矇起眼睛從袋子裡依序一次一顆，抽出其中四顆球，將他們排在眼前。第一輪我們拿出的是字母a黑球，字母c白球，字母j白球，還有字母b黑球。紀錄完畢再將四顆球放回袋子裡，我們可以重覆同樣的抽球程序數輪，每次紀錄累積就如 Figure 8.1 右邊的樣子。如此不斷重覆、結果卻每次不同的抽球程序是一種隨機程序。2 我們能認可這是隨機程序的主要理由是每次抽球之前，都要先搖一搖袋子，讓每顆球被抽出的機會是相等的。能讓母群裡的每一份子以相等機會抽出，形成的樣本就是簡單隨機樣本。每取出一顆球都不會放回袋子，能確保每一次取樣得到的樣本不會有兩個同樣的球，這種取樣限制稱為不放回取樣。\n\n\n\n\n\n\nFigure 8.1: 自有限母群不放回取樣的可能樣本。\n\n\n\n\n為了讓同學更了解取樣方法如何影響樣本的組合，我們來想像另一種取樣狀況。想像有個五歲小朋友跑進來，擅自打開袋子拿出全部黑球，沒放回去又跑開了。旁邊做紀錄的同學沒注意到還當成是一次實驗結果，如果發生好幾次，就會看到像 Figure 8.2 展示的偏誤樣本。請想想如果做實驗的同學每次依照隨機取樣規則抽球，看到四顆球的樣本可能性有多高？取樣規則確實會影響樣本組成。如果我們了解全是黑球是有偏誤的取樣方法造成的，那麼這樣的偏誤樣本無法有效推測母群的性質！這是為什麼統計學者特別重視資料檔案裡的紀錄是不是來自簡單隨機樣本，隨機樣本的資料進行分析不大需要太多處理。\n\n\n\n\n\n\nFigure 8.2: 自有限母群偏誤取樣的樣本。\n\n\n\n\n我們再來想像第三種取樣狀況。這次每輪抽出一顆球之前，先搖一搖袋子，取出一顆球做好紀錄，再放回袋子重新搖一搖袋子，再取出一顆球紀錄，如此重覆直到完成四次紀錄。如此程序取得的樣本也是簡單隨機樣本，因為每次取球都要放回袋子，因此這樣的方法稱為放回取樣。放回取樣與不放回取樣的主要差別在於，放回取樣得到的樣本有可能看到同一顆球在樣本裡出現兩次，如果 Figure 8.3 的展示。\n\n\n\n\n\n\nFigure 8.3: 自有限母群放回取樣的可能樣本。\n\n\n\n\n大多數心理學實驗取得的樣本是不放回取樣的結果，因為同一個人不大能參加同一項實驗兩次。不過，大多數統計理論是建立在放回取樣形成的簡單隨機樣本。現實與理論的差異在多數研究實務並不會有太多影響。如果研究對象的母群組成份子多到一個程度，放回取樣和不放回取樣的隨機樣本幾乎沒有差別。另一方面，簡單隨機樣本與偏誤樣本之間的差別，在很多實務狀況裡很難看得出來。\n\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\n就像前面說明的範例所展示的，我們幾乎不可能從想研究的母群取得真正的簡單隨機樣本。許多在大學實驗室執行的心理學實驗，都是直接找該校大學部同學來參加，設計實驗的教授和研究生都要假裝他們的參與者都是隨機樣本的一部分。取樣方法還有很多種，而且其實是超出這門課程的學習範圍，在此還是做點介紹，給好奇心強的同學一些進階學習的指引。\n\n分層取樣\n滾雪球取樣\n方便取樣\n\n\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 3 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\n好吧，現實世界收集的資料經常不是簡單隨機樣本。這有關係嗎？萬一資料不是簡單隨機樣本，稍微思考一下就會明白，這可能會造成糟糕的分析。想想 Figure 8.1 和 Figure 8.2 兩種取樣程序的差異就能明白。然而，事實沒有聽起來那麼糟。某些類型的偏誤樣本是不會影響分析結果的。例如，使用分層取樣時，我們要知道什麼條件 會造成偏誤，因為我們是有意識地製造樣本。這通常是為了增加研究的有效性，而且有統計技術可以用來調整引入資料的偏差（本課程不談這部分！）。因此，在某些情況下，偏誤樣本並不是一個問題。\n在一般情況的重要關鍵是，隨機取樣只是達成目標的手段，而不是目的本身。假設你採用的是方便取樣，因此可以假定樣本具有偏誤性。只有取樣方法未控制偏誤，才會導致錯誤的結論，這才是問題所在。從這個角度來看，我認為我們並不需要在每個條件都使用隨機化樣本，我們只需要針對感興趣的心理現象進行隨機取樣即可。假定我正在進行一項探討工作記憶容量研究。在第一個研究中，我能從全世界活生生的人群裡隨機取樣，只有一個例外：我只能取樣星期一出生的人。在第二個研究中，我能夠從所有澳洲人中隨機取樣，然後將結果類推到所有人類。哪一個研究結果比較好呢？答案顯然是第一個研究。為什麼？因為我們沒有理由認為“出生在星期一”與工作記憶容量有任何有意思的關聯。相比之下，我可以想到幾個原因，認為“澳洲人”可能是重要的偏誤因素。澳洲是一個富裕、工業化的國家，擁有非常發達的教育系統。在這個國家成長的人們會有很多與設計測試方法的人相似的生活經驗。這種共同經驗可能很容易轉化為相似的測試方法、心理實驗假設等等。這些考慮可能真的很重要。例如，澳洲參與者可能已經習慣專注“測試方法”彙整的相對抽象的測試材料，比起不是在類似環境中成長的人有更多“應付”經驗，而這可能導致我對工作記憶容量的錯誤推論。\n這個小節的討論隱含兩個觀點。首先是身為研究的設計者立場，重要的是要考慮你所關心的母群條件，儘可能以最適合的方式進行取樣。實務上，我們通常只能使用“方便樣本”（例如，心理學教師找修心理學的學生收集資料，因為這是收集資料最便宜的方法，而我們的經費通常很有限）。如果必須使用的話，設計者應該要好好思考這種取樣方法可能存在的偏誤。另一方面是評論他人研究的立場，如果他們不得不使用方便樣本，而不是從整個人類母群進行隨機取樣，那麼我們要有禮貌地提供一個具體的理論，解釋他們的取樣方法可能如何扭曲結果。\n\n\n\n8.1.5 母群參數與樣本統計\n好的。撇開獲取隨機樣本的棘手方法論問題，讓我們考慮一個略微不同的問題。到目前為止，我們一直是以科學家的觀點討論母群。對於心理學家來說，母群可能是一群人；對於生態學家來說，母群可能是一群熊。在大多數情況，各類科學家關心的母群是現實世界中實際存在的具體事物。然而，統計學家有點與眾不同。他們像其他科學家一樣對現實世界的資料和科學感興趣，也像數學家一樣探討抽象符號的操作。因此，統計學理論定義母群的方式通常有些抽象。就像心理學研究人員想用具體的測量方法將抽象理論轉換成操作型定義(參考 Section 2.1)，統計學家將“母群”的抽象概念轉換為可操作的數學符號。同學們已經在前一章(Chapter 7)學習了這些知識。它們被稱為機率分布。\n在此簡單示範一下。假定我們的研究對象是一群人的智力分數。對於心理學家而言，這裡的母群是一群有智力測驗成績的真實人類。而統計學家會透過 Figure 8.4 (a) 展示的機率分佈定義母群，來「簡化」這個問題。智力測驗成續的平均智商是100，標準差是15，而且分佈是常態分佈。這些數值被稱為母群參數，因為它們代表整個母群的特徵。也就是說，我們會定義這個母群的平均值\\(\\mu\\)是100，母群標準差 \\(\\sigma\\) 是15。\n\n\n\n\n\n\nFigure 8.4: 智力測驗分數的母群分佈（圖a）以及從中取得的兩個隨機樣本。圖（b）是一個由100個觀察值組成的樣本，圖（c）是一個由10,000個觀察值組成的樣本。\n\n\n\n\n假設現在我做了一個實驗。我隨機選出100個人，請他們進行智力測驗，這樣我就得到了母群的一個簡單隨機樣本。我的樣本裡有以下一系列數字：\n\n106 101 98 80 74 … 107 72 100\n每個分數都是從一個平均值為100、標準差為15的常態分佈隨機取樣得到的。如果我繪製樣本的直方圖，就會得到像 Figure 8.4 (b)的結果。你可以看到，直方圖的形狀與常態分佈大致相似，但只是粗略近似真實母群（如 Figure 8.4 (a)）的分佈。以這群樣本計算樣本平均值，我得到了一個與母群平均值100相當接近但不完全相同的數字。在這個例子中，我得到的樣本平均智商是98.5，樣本標準差是15.9。這些來自樣本的統計量數是呈現資料的特徵，雖然它們接近真實母群的值，卻並不相同。通常，樣本統計量數是從資料集計算出來的，而母群參數是你想要了解的。在本章稍後，我將談到如何使用樣本統計量來估計母群參數，以及估計置信區間，但在那之前，你需要了解更多有關取樣理論的概念。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters",
    "title": "8  運用樣本估計未知量數",
    "section": "8.4 母群參數的點估計",
    "text": "8.4 母群參數的點估計\n在前面的章節有關智力測驗分數的所有例子中，我們已經知道了母群的參數。就像在每一位大學生的心理測驗學第一堂課中所學的，智力測驗分數的平均值被定義為100，標準差為15。但這其實是有點取巧的。我們怎麼知道智力測驗分數的真實母群平均值是100？這是因為測驗的設計者對大樣本進行了測試，然後在測驗規則做點“手腳”，使得樣本平均值為100。當然，這不是什麼壞事，這是心理測學重要的一部分。但是，需要牢記的是，這個理論上的平均值100僅適用於測試設計者用來設計測驗的母群。優秀的測驗設計師實際上會花費一些心思來提供可以應用於許多不同母群（例如不同的年齡組、不同國籍等）的“測試常模”。\n這樣設計很方便，但是幾乎每個有趣的研究都要能適用於研究不同於建立測試常模的其他人群。例如，假設您想要測量南澳大利亞州一個工業鎮Port Pirie中低水平的鉛中毒對認知功能的影響。也許您會想要將Port Pirie中的智力測驗分數與南澳另一個工業城市Whyalla的樣本進行比較。6不論您考慮的是哪個城鎮，僅僅假設真實的母群平均智商是100並不合理。在我所知道的範圍內，沒有人提出合理的測量數據，能適用於南澳大利亞的所有工業城鎮。因此，我們不得不通過樣本資料估計母群參數。\n\n\n8.4.1 母群平均值\n假設我們前往 Port Pirie，邀請100位當地居民參加智力測驗。這些人的平均智力測驗分數為 \\(\\bar{X}=98.5\\)。那麼整個 Port Pirie 的母群的智力測驗平均分數是多少呢？我們顯然無法知道。可能是 97.2，也可能是 103.5。由於我們的樣本並不是全部的母群，因此無法給出確定的答案。不過，如果非要我給一個“最好的猜測”，我會說是 98.5。這就是統計估計的精髓：給出最佳的猜測。\n在這個例子中，未知的母群參數估計值是簡單明瞭的。直接用樣本平均值當成母群平均值的估計值。在下一節中，我將解釋這個類似直覺的答案的統計學理由。但是，現在同學們要確實了解樣本統計量和母群參數估計值是不同的概念。樣本統計量是您的數據描述，而估計值是對母群的猜測。考慮兩者的差異，統計學家通常使用不同的符號來指代它們。例如，真實母群平均值的符號為 \\(\\mu\\)，母群平均值的估計值符號則是 \\(\\hat{\\mu}\\) 。另一方面，樣本平均值符號為 \\(\\bar{X}\\)，也可以寫成 m。不過，簡單隨機樣本的母群平均值之估計值與樣本平均值相同。如果我觀察到樣本平均值為 \\(\\bar{X}=98.5\\)，則我對母群平均值的估計也是 \\(\\hat{\\mu}=98.5\\)。為了保持符號清晰，我整理成以下表格 (Table 8.1) ：\n\n\n\n\n\n\nTable 8.1:  各種平均值符號釋義 \n \n  \n    符號 \n    統計學名詞 \n    從那裡來的. \n  \n \n\n  \n     \n\n    樣本平均值 \n    從樣本資料計算出來的 \n  \n  \n     \n\n    母群平均值 \n    完全無法知道的 \n  \n  \n     \n\n    母群平均值之估計值 \n    使用簡單隨機樣本就會等於樣本平均值 \n  \n\n\n\n\n\n\n\n\n8.4.2 母群標準差\n到這裡，估計母群參數看起來相當簡單，但同學可能疑 為什麼要先理解取樣理論。當母群參數是平均值，估計值（\\(\\hat{\\mu}\\)）與相應的樣本統計量（\\(\\bar{X}\\)）恰好相同。但這種關係不是通用的。為了說明這一點，讓我們思考如何估計母群標準差，數學符號是 \\(\\hat{\\sigma}\\)。我們該使用什麼作為母群標準差的估計值呢？第一個可能的想法是，我們可以像估計平均值那樣，使用樣本統計量作為估計值。這樣做幾乎是正確的，但是不完全正確。\n以下是我的說明。假如我有一個只有一個觀察值的樣本，這個例子能讓 對母群參數真實數值一無所知的情況，比較容易理解。在此我們使用一些完全虛構的資料：假設這個觀察值是我的鞋子的“光滑度”，經過測量，我的鞋子“光滑度”是\\(20\\)。以下是這個樣本的描述：\n這是一個完全合格的樣本，即使它只有\\(N=1\\)個觀察值。它的樣本平均值為\\(20\\)，並且每個觀察值都等於樣本平均值（這是多說的！），所以它的樣本標準差為0。作為對樣本的描述，這似乎是正確的，因為樣本只有了一個觀察值，因此在樣本內觀察不到任何變異。此例報告樣本標準差\\(s=0\\)是正確的。但是，作為母群標準差的估計值，這樣做完全不合理，對吧？即使同學們和我都對“光滑度”一無所知，但我們對於資料處理都有一些了解。我們看不到樣本內任何變異性的唯一原因是樣本太小而無法顯示任何變異性！因此，如果樣本大小為\\(N=1\\)，那麼關於母群標準差我們只能回答“根本不知道”。\n留意一下，稍早談樣本平均值和母群平均值時，你並沒有察覺不對勁。如果一定要猜測母群平均值，猜母群平均值是 \\(20\\) 並不感覺完全不合理。當然，同學可能對這樣的猜測自信程度比不上我，因為你看到樣本只有一個觀察值，但這依然是你能做出的最好猜測。\n讓我們擴大一下這個樣本，假如現在有了第二個觀察值。我的資料集現在有 \\(N=2\\) 個鞋子的”光滑度”觀察值，完整的樣本看起來像這樣：\n\n\\[20, 22\\]\n這一次的樣本大小剛好足夠大到能觀察一些變異性：兩個觀察值是能觀察到任何資料變異性的最低條件！以新資料集計算的樣本平均值是 \\(\\bar{X} = 21\\)，樣本標準差是 \\(s=1\\)。那麼我們能如何猜測母群參數？同樣地，母群平均值的最佳猜測就是樣本平均值，也就是說“光滑度”的母群平均值為 \\(21\\)。那麼標準差呢？這就有點複雜了。樣本標準差只是基於兩個觀察值，如果同學學得夠認真，可能會覺得僅僅只有兩個觀察值，是不足以“充分展現” 真正的母群變異性。這不僅僅是估計值是否正確的問題，畢竟，只有兩個觀察值的估計值，我們能合理懷疑在某些程度是錯誤的。更要擔憂的問題是誤差是系統性的。具體而言，我們懷疑樣本標準差可能比真正的母群標準差小。\n如果同學有這樣的警覺非常不錯，能夠證明這一點的話就更好。其實一些數學家已經證明了系統性誤差的直覺是對的，只是除非你有一定程度的數學知識，否則了解這些證明對我們學習統計沒有太大幫助。另一方面，我們可以做一些模擬實驗來展示系統性誤差。讓我們回到智力測驗分數研究：假設真正的母群平均智力測驗分數為100，標準差為15。第一份模擬實驗先測量2個智力測驗分數，然後計算樣本標準差。如果我做好幾遍模擬，然後繪製這些樣本標準差的直方圖，得到的就是標準差的取樣分配。 Figure 8.10 展示樣本標準差取樣分配的直方圖。儘管真實的母群標準差為15，樣本標準差的平均值只有8.5。請注意，這與我們在 Figure 8.8 (b)中繪製的平均值取樣分配的完全不同，我們設定母群平均值為100，所有樣本平均值的平均值也是100。\n\n\n\n\n\n\nFigure 8.10: 此為「兩個智力測驗分數」模擬實驗的樣本標準差取樣分佈。虛線標示設定的母群標準差為15，但是從直方圖可以看出，大多數實驗所得的樣本標準差遠小於母群標準差。平均而言，這個實驗所產生的樣本標準差只有8.5，明顯低於母群標準差！換句話說，樣本標準差是母群標準差的偏誤估計值。\n\n\n\n\n現在讓我們擴展模擬實驗規模。不再只做於N = 2的情況，使用樣本大小從1到10，將這個模擬實驗重複進行。如果分別將各種樣本大小(橫軸)情況取得的樣本平均值和樣本標準差之平均值(縱軸)，繪製折線圖，就可以得到 Figure 8.11 的成品。其中圖（a）是樣本平均值的總體平均，圖（b）是樣本標準差的變化曲線。兩幅圖的意義非常不同：通常不論樣本大小，樣本平均值等於母群平均值，代表這是一種不偏的估計值，這就是為什麼母群平均值的最佳估計值是樣本平均值的原因 。7 圖（b）則顯示另一回事：多數情況的樣本標準差\\(s\\)總是小於母群標準差\\(\\sigma\\) ，代表這是一種有偏誤的估計值。也就是說，如果要為母群標準差\\(\\hat{\\sigma}\\)做出“最佳猜测” \\(\\hat{\\sigma}\\)，最好用比樣本標準差\\(s\\)大一點的數值。\n\n\n\n\n\n\nFigure 8.11: 模擬實驗結果證實樣本平均值是母群平均值的不偏估計值（圖a），但是樣本標準差是母群標準差的有偏誤估計值（圖b）。兩幅圖的每個點都是來自10,000筆模擬資料，首先每筆模擬資料都有1個觀察值，再生成10,000組2個觀察值、以此類推直到10個觀察值為止。每個資料都來自虛構的智力測驗分數母群參數設定，即符合常態分佈且母群平均值為100、 標準差為15。平均來看，不論樣本大小如何（圖a），取樣的樣本平均值幾乎 等於100。然而，取樣的的樣本標準差通常小於母群標準差，小樣本情況特别明顯（圖b）。\n\n\n\n\n解決這個系統性偏誤的方法其實非常簡單，以下說明如何解決。細談標準差之前，先回顧一下變異數。回顧一下4.2.4 變異數所提到的，變異數是所有資料偏離平均值的平方和之平均值。也就是以下公式：\n\\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\]\n樣本變異數 \\(s^2\\) 是母群變異數 \\(\\sigma^2\\) 的有偏誤估計量。不過在統計實務，我們只需要做個微小的調整，就可以將其轉換為不偏估計值。方法就是改成除以 \\(N-1\\) 而不是除以 \\(N\\)。\n\n修改後的公式就是母群變異數 \\(\\sigma\\) 的不偏估計量。這也回答了我們在4.2.4 變異數遇到的問題。為什麼 jamovi 給我們的變異數稍有不同？那是因為 jamovi 計算的是 \\(\\hat{\\sigma}^2\\) 而不是 \\(s^2\\)，標準差也是一樣。如果我們除以 \\(N-1\\) 而不是 \\(N\\)，就能得到母群標準差的不偏估計值。請記住使用 jamovi 內建的標準差函數時，計算的是 \\(\\hat{\\sigma}\\) 而不是 \\(s\\)。8\n最後一點，很多人進行統計實務都習慣稱呼 \\(\\hat{\\sigma}\\)（分母是 \\(N-1\\) 的標準差公式）為樣本標準差。嚴格來說這並不正確。樣本標準差應該是\\(s\\)（分母是\\(N\\) 的標準差公式）。兩者在概念和數值的意義都不相同。前者是一種母群標準差的估計值，後者是一種樣本性質。不過，幾乎所有現實世界的統計問題都是要找出母群參數的合理估計值，因此多數報告都是呈現\\(\\hat{\\sigma}\\)，而非\\(s\\)。雖然這是正確的報告方式，只是多數人在撰寫報告時，對於術語往往不夠精確，因為「樣本標準差」寫起來比「估計母體標準差」字比較少。這並不是嚴重的問題，實際上我也和其他人一樣這樣使用詞彙。儘管如此，我認為了解兩者在概念上的差異非常重要。混淆「已知樣本的已知屬性」和「透過樣本對母群的猜測」永遠都不是一件好事。當你開始認為 \\(s\\) 和 \\(\\hat{\\sigma}\\) 是相同的東西時，你就要好好拿出這本書複習了。\n最後，我將這一節的重要符號與概念整理一下(Table 8.2 與 Table 8.3)。\n\n\n\n\n\n\nTable 8.2:  各種標準差符號釋義 \n \n  \n    符號 \n    統計學名詞 \n    從那裡來的. \n  \n \n\n  \n     \n\n    樣本標準差 \n    從樣本資料計算出來的 \n  \n  \n     \n\n    母群標準差 \n    完全無法知道的 \n  \n  \n     \n\n    母群標準差之估計值 \n    可使用簡單隨機樣本計算，但是不會等於樣本標準差 \n  \n\n\n\n\n\n\n\n\n\n\nTable 8.3:  各種變異數符號釋義 \n \n  \n    符號 \n    統計學名詞 \n    從那裡來的. \n  \n \n\n  \n     \n\n    樣本變異數 \n    從樣本資料計算出來的 \n  \n  \n     \n\n    母群變異數 \n    完全無法知道的 \n  \n  \n     \n\n    母群變異數之估計值 \n    可使用簡單隨機樣本計算，但是不會等於樣本變異數"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-What-does-probability-mean",
    "href": "07-Introduction-to-probability.html#sec-What-does-probability-mean",
    "title": "7  機率入門",
    "section": "7.2 如何解讀機率？",
    "text": "7.2 如何解讀機率？\n\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.3\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n7.2.1 次數主義觀點\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the 次數主義觀點(frequentist view) and it defines probability as a 長期累積的相對次數(long-run frequency). Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 7.1:  Coin flips and proportion of heads \n\nnumber of flips12345678910\n\nnumber of heads0123444567\n\nproportion00.50.670.750.80.670.570.630.670.7\n\nnumber of flips11121314151617181920\n\nnumber of heads88910101010101011\n\nproportion0.730.670.690.710.670.630.590.560.530.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 7.1.4 As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.5 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 8.5).\n\n\n\n\n\nFigure 7.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n7.2.2 貝氏觀點\n貝氏觀點(The Bayesian view) of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the 事件發生的信念程度(degree of belief) that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.6\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n7.2.3 觀點之間的差異是什麼？何者正確？\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in Chapter 16."
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-Basic-probability-theory",
    "href": "07-Introduction-to-probability.html#sec-Basic-probability-theory",
    "title": "7  機率入門",
    "section": "7.3 基本機率理論",
    "text": "7.3 基本機率理論\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n7.3.1 機率分佈入門7\n\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 7.2 shows an example of a probability distribution.\n\n\n\n\nTable 7.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see Section 5.3) to visualise this distribution, as shown in Figure 7.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 7.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 7.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\n\nTable 7.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "09-Hypothesis-testing.html#運用取樣分佈檢測統計值",
    "href": "09-Hypothesis-testing.html#運用取樣分佈檢測統計值",
    "title": "9  假設檢定",
    "section": "9.3 運用取樣分佈檢測統計值",
    "text": "9.3 運用取樣分佈檢測統計值\n現在我們可以開始討論如何建立一個假設檢定的具體步驟，讓我們回到一開始提的靈異感知（ESP）研究題目。暫時不管我們實際獲得的資料，先專注於實驗設計。無論資料裡的數值是什麼，都是表示 \\(N\\) 個人當中有 \\(X\\) 個人正確地辨認出隱藏卡牌的顏色。此外，若是虛無假設（null hypothesis）確實是真的，也就是說靈異感知不存在，每個人正確辨認出卡牌顏色的真實機率 \\(\\theta\\) 等於 \\(0.5\\)。在這種狀況，我們預期的資料是什麼樣子呢？很明顯，我們期望作出正確反應的人數比例接近 \\(50%\\)。換言之，我們可以用更數學式的語言表達 \\(\\frac{X}{N}\\) 大約等於 \\(0.5\\)。當然，我們不會預期真實的比例完全等於 \\(0.5\\)。例如，如果我們測試了 \\(N=100\\) 個人，其中有 \\(X=53\\) 人回答正確，我們也許不得不承認這筆資料與虛無假設是相當一致的。另一方面，如果有 \\(X=99\\) 個參與者回答正確，我們會非常有信心地認為虛無假設是錯的。同樣地，如果只有 \\(X=3\\) 人回答正確，我們也會非常有信心地認為虛無假設是錯的。現在讓我們用更專業的方式描述這些推論：我們有一個數值 \\(X\\) ，可以通過觀察資料計算出來。評估 \\(X\\) 之後，我們就必須決定是相信虛無假設，還是拒絕虛無假設並接受另一種假設。用來幫助我們作出決策的數值就稱為統計檢定值。\n選定了一個統計檢定值後，下一步是正式宣告那些統計值將導致我們拒絕虛無假設，那些統計值將導致我們接受虛無假設。為了做出決策，我們需要確定當實際結果符合虛無假設時，統計值的取樣分佈是什麼（不大記得的話，可以回到 Section 8.3.1 復習）。為什麼我們需要設定取樣分佈？因為這能告訴我們，如果虛無假設是正確的，我們可以預期會得到那些 \\(X\\) 的數值。因此，我們可以使用這個分佈作為評估虛無假設與我們的資料是否一致的工具。\n如何確定統計值的取樣分佈呢？對於大多數假設檢定程序來說，這個步驟通常相當複雜，甚至有些假設檢定原作者和譯者自己都不是很懂，稍後在本書中同學們會看到某些檢定程序的介紹會有些含糊其辭。不過在某些檢定程序，設定取樣分佈是非常簡單的。ESP研究案例所使用的檢定程序，剛好是最簡單的一種。這個案例的母群參數 \\(\\theta\\) 就是參與者們回答問題時的總機率，而統計值 \\(X\\) 就等於所有參與者人數 \\(N\\) 裡正確回答的人數。我們之前在 Section 7.4 這一節裡已經見過「二項分佈」，ESP案例的取樣分佈性質正好符合二項分佈！因此，為了使用二項分佈的符號和術語，我們會說虛無假設預測 \\(X\\) 的分佈是二項分佈，數學式就寫作\n\n\\[X \\sim Binomial(\\theta,N)\\]\n既然虛無假設主張 \\(\\theta = 0.5\\)，而我們的實驗有 \\(N=100\\) 位參與者 ，所以我們已經擁有所需要的取樣分佈。這個取樣分佈的視覺化如同 Figure 9.1 。沒有什麼特別的，由視覺化繪圖可知，既然虛無假設說 \\(X=50\\) 是最有可能的結果，那麼我們有很大的機會看到 \\(40\\) 到 \\(60\\) 個正確的回答。\n\n\n\n\n\n\nFigure 9.1: 這是當虛無假設為真時，我們測試統計值 \\(X\\) 的取樣分佈。對於 ESP 的案例，取樣分佈會符合二項式分佈。因為虛無假設主張正確回答的機率是 \\(\\theta = 0.5\\)，所以取樣分佈顯示，100次測試裡有50次 正確回答，是最有可能發生的結果。大多數的機率質量(probability mass)分散在40次到60次之間。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-Hypothesis-testing-decision",
    "href": "09-Hypothesis-testing.html#sec-Hypothesis-testing-decision",
    "title": "9  假設檢定",
    "section": "9.4 統計推論的決策要素",
    "text": "9.4 統計推論的決策要素\n我們已經非常接近最後一步了。前一步設定了一個統計檢定值 \\((X)\\)，並且選擇了我們相當有信心的檢定值數值。如果 \\(X\\) 接近 \\(\\frac{N}{2}\\)，我們應該保留虛無假設，否則我們就應該拒絕虛無假設。剩下的問題是什麼呢？確切地說，我們應該設定那些統計檢定值是對應虛無假設，那些統計檢定值對應對立假設？以ESP研究案為例，假如我觀察到一個值 \\(X=62\\)。我應該做出什麼決策呢？我應該相信虛無假設還是對立假設？\n\n\n9.4.1 棄卻域與臨界值\n要回答這個問題，我需要向各位介紹統計檢定值 \\(X\\) 的棄卻域（critical region）。檢定的棄卻域對應那些會讓我們拒絕虛無假設的 \\(X\\) 數值集合（這就是為什麼棄卻域有時也被稱為拒絕域）。我們如何找到這個棄卻域呢？嗯，讓我們想一想已知的條件：\n\n為了拒絕虛無假設，\\(X\\) 應該非常大或非常小\n如果虛無假設為真，\\(X\\) 的取樣分佈會是 \\(Binomial(0.5, N)\\)\n如果 \\(\\alpha = .05\\)，則棄卻域必須包含這個取樣分佈的 5%。\n\n最後一點非常重要。棄卻域的範圍是指那些會導致我們拒絕虛無假設的 \\(X\\) 數值範圍，而這個範圍是經由取樣分佈代換後的機率質量所決定的。如果我們選擇了一個其涵蓋 \\(20%\\) 機率質量的棄卻域，且虛無假設是符合事實的，那麼拒絕虛無假設的錯誤機率就是 \\(20%\\)。換言之，我們完成了一個顯著水準為 \\(0.2\\) 的檢驗。如果我們要求顯著水準是 \\(\\alpha = .05\\)，那麼棄卻域只能涵蓋統計檢定量取樣分佈的 \\(5%\\) 機率質量。\n\n我們在此總結解決完成假設檢定程序的三個要點。我們的棄卻域包括了機率分佈的最極端數值，也就是機率分佈的尾部。 Figure 9.2 展示了這個概念的視覺化。如果我們希望 \\(\\alpha = .05\\)，那麼對應的棄卻域是 \\(X \\leq 40\\) 和 \\(X \\geq 60\\)。6也就是說，如果回答正確的人數在 41 到 59 之間，那麼我們應該保留虛無假設。如果回答正確的人數在 0 到 40 或 60 到 100 之間，那麼我們就應該拒絕虛無假設。數字 40 和 60 通常被稱為臨界值(critical values)，因為這些數值定義了棄卻域的邊界。\n\n\n\n\n\n\nFigure 9.2: 這張圖與 Figure 9.1 的虛無假設 \\(X\\) 取樣分佈一樣，進一步展示ESP研究的假設檢定的棄卻域，假設檢定的顯著水準為\\(\\alpha = .05\\) 。灰色的柱子表示我們會保留虛無假設的 \\(X\\) 數值集合。深藍色的柱子表示棄卻域，也就是我們會拒絕虛無假設的 \\(X\\) 值。由於對立假設的主張是雙側的（即允許 \\(\\theta < .5\\) 和 \\(\\theta > .5\\)），因此棄卻域涵蓋分佈的兩個尾部。為確保 \\(\\alpha\\) 水準為 \\(.05\\)，我們需要確保左右區域各涵蓋了取樣分佈的 \\(2.5%\\)。\n\n\n\n\n最後，總結一下完成假設檢驗的主要步驟：\n\n選擇一個顯著水準 (例如，\\(\\alpha = .05\\))；\n選擇一個適當的統計檢定值 (例如，\\(X\\))，並且設定有比較意義的\\(H_0\\)和\\(H_1\\);\n假設虛無假設是符合事實的，找出該統計檢定值的取樣分佈（在ESP案例為二項分佈）；\n計算會產生符合 \\(\\alpha\\) 的棄卻域（0-40和60-100）。\n\n現在我們所要做的就是用實際資料計算統計檢定值（例如 \\(X=62\\)），然後比較檢定值與臨界值做出決策。由於 \\(62\\) 大於臨界值 \\(60\\)，我們可以拒絕虛無假設。也可以說，我們根據檢定結果得到一個在統計顯著的結論。\n\n\n\n9.4.2 小心使用統計“顯著”\n\n統計學和其他占卜術一樣，擁有一套專門術語，故意設計成讓非專業人員無法從字面理解術語的意思。 – G. O. Ashley 7\n\n在此需要講個關於 “significant”(常見中文說法“顯著”) 這個詞怎麼來的題外話。“significant” 在統計學中的概念其實很簡單，但這樣的命名並不夠好。如果實際資料能讓我們拒絕虛無假設，我們會說 “the result is statistically significant”(常見中文說法”結果有統計顯著性”)，通常簡單寫成 “the result is significant”(常見中文說法”有顯著結果”)。這個英文詞彙其實由來已久，來源可以追溯到 “significant” 的意思只是表達 “indicated”(已確認)的時代，並沒有現代英語的 “重要” 之類的含義。因此，今天許多讀者在開始學習統計學時會感到非常困惑，因為他們認為 “significant result” 必定是一個重要的結果。實際上，這並不是最早統計學家開始使用這個詞的意思。所有用”statistically significant”表達的主張， 只是表示資料允許我們拒絕一個虛無假設。至於結果是不是真的重要，則是另一個完全不同的問題，並且有其他各種因素的影響。\n\n\n\n9.4.3 單側與雙側檢定的不同\n還有一件事情要提醒各位同學，就是虛無假設與對立假設的設定方式。假如前面我所使用的統計假設是：\\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] 我們會發現對立假設涵蓋了 \\(\\theta < .5\\) 和 \\(\\theta > .5\\) 這兩種可能的數值集合。這代表我認為超感官知覺可能造成優於純粹猜測的表現，也可能產生比純粹猜測還差的表現（有些人就是會這麼認為），那麼這樣的設定是有意義的。在統計學的語彙庫，這稱為雙側檢定(two-sided test)。這是因為對立假設涵蓋了 無假設兩側的數值集合，因此檢定的棄卻域覆蓋取樣分佈的兩側尾部（如果 \\(\\alpha = .05\\)，則每側尾部佔取樣分佈的2.5％），如同 Figure 9.2 的展示。不過，這不是唯一的可能結果。如果我只在乎超感官知覺能夠產生優於純粹猜測的表現時，才願意相信這是事實，那麼對立假設就只會涵蓋 \\(\\theta > .5\\) 的數值集合。因此，虛無假設和對立假設就會變成\\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] 這種檢定條件 就是所謂的單側檢定(one-sided test)，此時檢定的棄卻域只有覆蓋取樣分佈的右側尾部，如同 Figure 9.3 的展示。\n\n\n\n\n\n\nFigure 9.3: 單側檢定的臨界區間。此狀況的對立假設是\\(\\theta \\geq .5\\)，當\\(X\\)的值很大，我們才能拒絕虛無假設。因此，臨界區間僅覆蓋取樣分佈的較大數值，具體來說是分佈的右側的 \\(5%\\) 。比較一下 Figure 9.2 的雙側檢定狀況。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設的層次",
    "href": "09-Hypothesis-testing.html#假設的層次",
    "title": "9  假設檢定",
    "section": "9.1 假設的層次",
    "text": "9.1 假設的層次\n我們從一個原作者虛構的狂想開始談吧：我認為每個人活得夠老，都會向瘋狂的想法屈服。我這輩子真正想做的研究，將在我升任正教授的那一天開始，因為我在象牙塔中會受到終身職的保障，那天我總算能夠拋棄理智，完全投入那個最沒有成效的心理研究領域：證實人類有超感官知覺（ESP）。3\n假如這一天終於來臨了，我要做的第一項研究是一項簡單的透視力測試實驗。每個參與者坐在桌子前，由實驗者向他展示一張卡片。這張卡片的一面是黑色的，另一面是白色的。實驗者把卡片放在桌子上，接著帶領參與者到隔壁房間。在實驗者與參與者離開後，第二位實驗者會隨機地把卡片翻到黑面或白面朝上，然後第二位實驗者到隔壁房間，詢問參與者現在卡片的那一面朝上。這個實驗的參與者只會進行一次測試，每個人只會看到一張卡片，只回答一個問題，而參與者在回答問題前都不會與知道正確答案的第二位實驗者接觸。因此，我的資料紀錄非常簡單：我問了N個人的問題，其中有 \\(X\\) 個人給了正確的答案。為了具體說明，若是這次實驗我測試了100個人，其中有62個人回答正確。這會是一個令人驚訝的大數字，但是這個數字是否足夠讓我聲稱發現了ESP的證據呢？這就是檢驗假設有無效用的情況。然而，在我們談論如何檢驗假設之前，我們需要明白要如何設定這項實驗的假設。\n\n\n\n9.1.1 研究假設還是統計假設\n首先需要清楚區別的是研究假設和統計假設之間的差別。在我設想的ESP研究中，我的整體科學目標是證實人類有透視能力。在這樣的場景，我有一個清晰的研究目標：我希望發現ESP的證據。在其他場景，我的想法可能會比較中立，也就是我可能會說我的研究目標是確定人類是否有透視能力。無論如何描述我的目標，我想傳達給各位同學的基本觀點是，制定研究假設是提出一個實際的、可測試的科學主張。如果你是一名心理學家，那麼你的研究假設基本上是關於心理學構念的。以下任何一種案例都可說是研究假設：\n\n聆聽音樂會降低你對其他事物的注意力能力。這是一種關於兩個有心理學意義的構念之間有因果關係的主張（聆聽音樂和注意力），因此這是一個非常合理的研究假設。\n智力與個性有關。和前一個一樣，這個假設主張兩個有心理學意義的構念之間存在關係性（智力和個性），但這個主張立埸比較弱：探討相關性而不是因果關係。\n智力是訊息處理速度。這個假設與前面兩個很不一樣。實際上這並不是一個因果關係或關聯性的假設，而是關於智力基本特性的本體論主張（我相當確定是這樣的）。通常來說，設計實驗測試像是“\\(X\\)是否影響\\(Y\\)？”，比回答“\\(X\\)是什麼？”這樣的問題要容易得多。在實際情況通常是你會找到方法，測試基本特性所形成的關聯性假設。例如，如果我相信智力的本質是大腦中訊息處理速度，我就會設計實驗探討智力和訊息處理之間的關係。因此，大多數日常生活中想到的研究問題雖然都與本質有關，但是通常是基於好奇關於自然界本體論問題的更深層動機。\n\n請注意在真實的實驗室，我會設定幾個互相重疊的研究假設。儘管我設計ESP實驗的終極目標是測試“人類有ESP”這樣的本體論主張，但是實際操作會限制自己只測試目標更狹窄的假設，像是“某些人可以用透視‘看見’物體”。話雖如此，有一些看似目標明確的主張，在任何意義上都不算是合適的研究假設：\n\n愛情就像戰場。這個假設過於模糊，無法進行測試。雖然研究假設可以有一定程度的模糊性，但是必須能夠將理論觀念具體化。也許我不夠有創造力，想不到能將這個假設轉化為具體研究設計的方式。如果真的有辦法，那麼應該不是科學的研究假設，而是一首流行歌曲。這並不是說這樣的假設不有趣，而是要指出許多人能想到的深刻問題都是屬於這種類別。也許有一天科學家能夠構建關於愛情的可測試理論，或者測試上帝是否存在等等。但是現在的我們還做不到，我不會指望看到一個令人滿意的科學方法來解決這些問題。\n套套邏輯俱樂部的第一條規則就是套套邏輯俱樂部的第一條規則。這是不具備任何實質意義的主張，儘管形式上是符合邏輯的。因為在任何自然狀態都不能提出與此主張相反的看法，我們會說這是一個不可證偽的假設，因此這樣的主張不屬於科學研究的領域。在科學研究，無論你想研究的問題是什麼，你提出的主張都必須有可能是錯誤的。\n在我的實驗裡，較多的參與者會說‘是’，而不是‘否’。這並不是一個有意義的研究假設，因為重點的是資料本身而非心理學問題（當然，除非真正要研究的問題是關於多數人是不是有回答“是”的偏好！）。實際上，這個假設看起來更像是一個統計假設而非研究假設。\n\n正如同學所見，有的研究假設的主張可能會有些混亂，不過都是各樣科學主張的一種。統計假設則不是一種主張。統計假設必須具有數學精確性，並且必須對資料的生成機制（也就是“母群”）的特徵提出具體的條件。即便如此，統計假設的內在意圖必須有與真正的研究假設有一個明確的關係！例如，在我的ESP研究中，我的研究假設是有些人能夠透視牆壁看到隔壁的物體。我要做的是將這樣的研究假設對應到產生資料的方法陳述。因此，現在我們考慮一下要如何表達這樣的陳述。我感興趣的實驗數值是 \\(P(correct)\\)，也就是實驗參與者正確回答問題的理論上為真但未知之機率。讓我們使用希臘字母\\(\\theta\\)（theta）來表示這項機率。以下是四種不同的統計假設：\n\n如果ESP不存在，而我的實驗設計沒有偏誤，那麼參與者的回答只是猜測。因此，我應該期望有一半參與者的回答是正確的，所以我的統計假設是，回答正確的理論機率是 \\(\\theta=0.5\\)。\n或者，假設ESP存在並且參與者真的能夠看到卡片。如果實驗結果真的是這樣，參與者回答的正確率會高於只是猜測，所以統計假設是 \\(\\theta > 0.5\\)。\n第三種可能是ESP確實存在，但是參與者並沒有意識到透視看到的物體顏色是相反的（好吧，這有些荒謬，但我們永遠無法知道）。如果是這樣的實驗結果，我會期望參與者回答的正確率會低於只是猜測。所以統計假設是 \\(\\theta < 0.5\\)。\n最後，如果人類確實有ESP，但是我不知道參與者是否看到了正確的顏色。在這種情況下，我只能期望參與者回答的正確率不等於0.5。所以統計假設是 \\(\\theta \\neq 0.5\\)。\n\n以上例子都是合乎科學研究目標的統計假設，因為每條陳述都有定義母群參數，並且緊密扣連我的實驗目的。\n我希望這些例子可以讓同學清楚了解，當研究者要構建一個統計假設檢定程序時，實際上要考慮兩種不同層次的假設。首先，研究者要有一個研究假設（關於心理學的主張），能對應到一個統計假設（關於數據生成母群的主張）。以我的ESP實驗來說，會像 Table 9.1 的表達。\n\n\n\n\n\n\nTable 9.1:  原作者狂想的研究假設與統計假設 \n \n  \n    研究假設 \n    人類有超感官知覺 \n  \n \n\n  \n    統計假設 \n     \n\n  \n\n\n\n\n\n\n小結一下兩種假設的差別。統計假設檢定的測試對象是統計假設，而非研究假設。假如你的研究設計不良，會造成研究假設和統計假設之間的斷裂。舉個有點荒謬的狀況，要是我的ESP研究是在參與者可以從窗戶反光看到卡片的環境裡進行，那麼我肯定能得到非常強的證據證明 \\(\\theta \\neq 0.5\\)，但是這並不能告訴我們”人類真的有ESP”。\n\n\n\n9.1.2 虛無假設與對立假設\n到目前為止還算順利。我有一個研究假設，對應我想相信的世界，還有映射到一個對應於資料生成方式的統計假設。接下來我要創造一個新的統計假設（“虛無假設”，\\(H_0\\)），這對很多人來說有些違反直覺。因為”虛無假設”對應與我想相信的事情完全相反，然後專注於驗證這條統計假設，並且忽略實際關心的事情（現在被稱為”對立假設”，\\(H_1\\)）。在我的ESP研究裡，虛無假設是 \\(\\theta = 0.5\\)，因為如果人類沒有ESP，我會期望看到這個結果。當然，我期望ESP是真的，所以這個虛無假設對立的假設就是 \\(\\theta \\neq 0.5\\)。實際上，我們是在將 \\(\\theta\\) 涵括的可能數值分成兩類：我真心期望不是真的那些數值（虛無假設），以及如果ESP被證實是存在的，我會很高興的那些數值（對立假設）。完成這些設定之後，需要意識到的關鍵是，假設檢定的真正目標不是證實對立假設（可能）是真的，而是證實虛無假設（可能）是假的。很多初學的同學會覺得這樣的邏輯很奇怪。\n就我的學習經驗，最好的比喻是把假設檢定當作刑事法庭審判4。虛無假設就是被告，研究者就像檢察官，統計檢定程序是法官。像真正的刑事審判程序一樣，一開始我們要以無罪推定原則看待虛無假設，也就是說它的主張應被認為是真實的，直到位研究者能夠明確證實它的主張是錯的。研究者可以憑自由意志設計實驗（當然也要合理），目的就是要用可能性最大的資料證實虛無假設是錯的。然而，統計檢定(法官)設定了審判的規則，而這些規則是為了保護虛無假設而設計的，這些規則是要確保如果虛無假設的主張確實是真的，讓法官誤判的機會保持在很低的水準。這非常重要，畢竟虛無假設沒有律師幫忙辨護，而研究者卻在拼命地嘗試證實它的主張是錯的，所以必須有一方提供虛無假設一些保護。"
  },
  {
    "objectID": "Prelude-Part-V.html",
    "href": "Prelude-Part-V.html",
    "title": "線性模型學習進程",
    "section": "",
    "text": "教程原文1\n簡体中文翻譯2\n各章節的線性模型版示範，集合於此。\n\n\n\n\n\n\nhttps://lindeloev.github.io/tests-as-linear/↩︎\nhttps://cosx.org/2019/09/common-tests-as-linear-models/↩︎"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#相關",
    "href": "10-Correlation-and-linear-regression.html#相關",
    "title": "10  相闗與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\n這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的相關。首先，讓我們看一下列在 Table 14.1 的本章示範資料描述統計。\n\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。 \n \n  \n    變項 \n    最小值 \n    最大值 \n    平均值 \n    中位數 \n    標準差 \n    四分位數間距 \n  \n \n\n  \n    老爸的沮喪程度 \n    41.00 \n    91.00 \n    63.71 \n    62.00 \n    10.05 \n    14.00 \n  \n  \n    老爸睡眠小時數 \n    4.84 \n    9.00 \n    6.97 \n    7.03 \n    1.02 \n    1.45 \n  \n  \n    小嬰兒睡眠小時數 \n    3.25 \n    12.07 \n    8.05 \n    7.95 \n    2.07 \n    3.21 \n  \n\n\n\n\n\n\n讓我們從一個與每個新生兒父母都息息相關的主題談起：睡眠。這裡使用的資料集是虛構的，但是來自本人(原作者)的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有測量我的沮喪分數、我的睡眠習慣和我兒子的睡眠習慣持續是100天。身為一位數位時代的書呆子，我把資料保存在一個名為parenthood.csv的檔案。匯入jamovi，我們可以看到四個變項：dani.sleep，baby.sleep，dani.grump和day。請注意，當您首次打開這份檔案，jamovi可能無法正確猜測每個變項的資料類型，同學可以自行修正：dani.sleep，baby.sleep，dani.grump和day都可以被指定為連續變項，而ID是一個名義且為整數的變項。2\n接著我會看一下一些基本的描述性統計數據，並且三個我有興趣的變項視覺化，也就是 Figure 14.1 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計數據，你就要報告所有數據。如果我要以此結果撰寫報告，我會挑出那些我自己以及我的讀者最感興趣的統計數據，然後將它們放入像 Table 14.1 這樣的簡潔的表格裡。3 需要注意的是，當我將數據放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。\n\n\n\n\n\n\n\nFigure 10.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。\n\n\n\n\n\n\n10.1.2 相關的強度與方向\n我們可以繪製散佈圖，讓我們能俯瞰兩個變項之間的相關性。雖 然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較dani.sleep和dani.grump之間的關係（ fig-fig10-2 ，左）與baby.sleep和dani.grump之間的關係（ fig-fig10-2 ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我或者我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，dani.sleep和dani.grump之間的關係比baby.sleep和dani.grump之間的關係更強：左圖比右圖更加整齊。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。\n\n\n\n\n\n\nFigure 10.2: 左圖是dani.sleep(老爸睡眠小時數)與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖。\n\n\n\n\n相反地， Figure 14.3 的另外兩個散佈圖告訴我們另一個角度的故事。比較“baby.sleep 與 dani.grump”的散佈圖（左）和“baby.sleep 與 dani.sleep”的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。\n\n\n\n\n\n\nFigure 10.3: 左圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.sleep(老爸睡眠小時數)的散佈圖。\n\n\n\n\n\n\n10.1.3 相關係數\n現在我們要進一步延伸上述的概念，也就是正式認識 相關係數(correlation coefficient)。更具體地說，本節主要介紹皮爾森相關係數（Pearson’s correlation），慣例書寫符號是 \\(r\\)。在下一節，我們會用更精確符號 \\(r_{XY}\\) ，表示兩個變項 \\(X\\) 和 \\(Y\\) 之間的相關係數，值域涵蓋-1到1。當\\(r = -1\\)時，表示變項之間是完全的負相關；當\\(r = 1\\)時，表示變項之間是完全的正相關；當\\(r = 0\\)時，表示變項之間是完全沒有關係。 Figure 14.4 展示幾種不同相關性的散佈圖。\n[其他技術細節 4]\n\n\n\n\n\n\nFigure 10.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為\\(0, .33, .66, 1\\)。右欄的相關係數由上而下為\\(0, -.33, -.66, -1\\)。\n\n\n\n\n標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: \\(r = 1\\)代表著完美的正相關，\\(r = -1\\)代表著完美的負相關。稍後解讀相關係數這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。\n\n\n\n10.1.4 相關係數計算實務\n只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 Figure 14.5 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。\n\n\n\n\n\n\nFigure 10.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。\n\n\n\n\n\n\n10.1.5 解讀相關係數\n在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀\\(r = 0.4\\)的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於\\(0.95\\)的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到\\(0.9\\)，任何使用這個測驗預測的理論就會失效5。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過\\(0.3\\)，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 Table 14.2 的概略式解讀原則。\n\n\n\n\n\nTable 10.2:  解讀相關係數的粗略指南。強調粗略是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。 \n \n  \n    相關係數 \n    強度 \n    方向 \n  \n \n\n  \n    -1.0 ~ -0.9 \n    非常強 \n    負相關 \n  \n  \n    -0.9 ~ -0.7 \n    強 \n    負相關 \n  \n  \n    -0.7 to -0.4 \n    中等 \n    負相關 \n  \n  \n    -0.4 ~ -0.2 \n    弱 \n    負相關 \n  \n  \n    -0.2 ~ 0 \n    微弱 \n    負相關 \n  \n  \n    0 ~ 0.2 \n    微弱 \n    正相關 \n  \n  \n    0.2 ~ 0.4 \n    弱 \n    正相關 \n  \n  \n    0.4 ~ 0.7 \n    中等 \n    正相關 \n  \n  \n    0.7 ~ 0.9 \n    強 \n    正相關 \n  \n  \n    0.9 ~ 1.0 \n    非常強 \n    正相關 \n  \n\n\n\n\n\n\n然而，有一件點任何一位統計學教師都會不厭其煩地提醒同學，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)(Anscombe, 1973)，其中有四個資料集。每個資料集都有兩個變項， \\(X\\) 與 \\(Y\\)。四個資料集的 \\(X\\) 平均值都是 \\(9\\)， \\(Y\\) 的平均值都是 \\(7.5\\)。所有 \\(X\\) 變項的標準差幾乎相同，\\(Y\\) 變項的標準差也是一致。每種資料集的\\(X\\) 和 \\(Y\\) 相關係數均為 \\(r = 0.816\\)。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。\n也許你認為這四個資料集看起來很相似，其實上並非如此。從 Figure 14.6 的散佈圖可以發現，所有四個資料集的\\(X\\) 和 \\(Y\\) 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始數據」（見 Chapter 5 ）。\n\n\n\n\n\n\nFigure 10.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\n皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。\n線性關係是當一個變項\\(X\\)的數值增加，也能反映另一個變項\\(Y\\)的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（\\(X\\)）準備一個科目，那麼他排名的成績應該只有0％（\\(Y\\)）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。\n我們用 Figure 14.7 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的次序關係(ordinal relationship)。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數\\(r=0.91\\)能表達的。\n\n\n\n\n\n\nFigure 10.7: 這個圖解展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數\\(r = .91\\)。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數\\(\\rho = 1\\)。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。\n\n\n\n\n那麼我們要如何解決這個問題呢？其實很容易。如果我們要評估變項之間的次序關係，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”來衡量學生的努力，而是按照他們的讀書時間長短，將這\\(10\\)名學生排序。也就是說，學生\\(2\\)花在讀書的時間最少（\\(2\\)個小時），所以他獲得了最低的排名（排名=\\(1\\)）。接下來最懶惰的是學生\\(4\\)，整個學期只讀了\\(6\\)個小時的書，所以他獲得了次低的排名（排名=\\(2\\)）。請注意，在此用“排名=\\(1\\)”來表示“低排名”。在日常言談裡，多數人使用“排名=\\(1\\)”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！\n好的，讓我們從最努力且最成功的學生開始排名。 Table 14.3 顯示從最努力且最成功的學生排名的次序值。6\n\n\n\n\n\nTable 10.3:  十位學生的工作時間與得分數值次序 \n \n  \n    學生編號 \n    讀書時間序列 \n    成績序列 \n  \n \n\n  \n    學生 1 \n    10 \n    10 \n  \n  \n    學生 2 \n    1 \n    1 \n  \n  \n    學生 3 \n    5 \n    5 \n  \n  \n    學生 4 \n    8 \n    8 \n  \n  \n    學生 5 \n    9 \n    9 \n  \n  \n    學生 6 \n    6 \n    6 \n  \n  \n    學生 7 \n    7 \n    7 \n  \n  \n    學生 8 \n    3 \n    3 \n  \n  \n    學生 9 \n    4 \n    4 \n  \n  \n    學生 10 \n    2 \n    2 \n  \n\n\n\n\n\n\n有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。\n至此我們等於重新發現 斯皮爾曼等級相關(Spearman’s rank order correlation)，通常用符號 \\(\\rho\\) 表示，以區分皮爾森相關係數\\(r\\)。我們可以在“相關矩陣”選單選擇“Spearman”，使用jamovi計算斯皮爾曼等級相關係數。7"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#散佈圖",
    "href": "10-Correlation-and-linear-regression.html#散佈圖",
    "title": "10  相闗與線性迴歸",
    "section": "10.2 散佈圖",
    "text": "10.2 散佈圖\n散佈圖是一種簡單但有效的視覺化工具，用於具現兩個變項之間的關係，就像相關這一節所展示的圖表。通常提到“散佈圖”這個術語時，指的是具體的視覺化結果。在散佈圖中，每個觀察值都是對應一個資料點。一個點的水平位置表示一個變項的觀察值，垂直位置表示觀察值在另一個變項的數值。在許多使用情境，我們對於變項間的因果關係並沒有清晰的看法（例如，A是否引起B，還是B引起A，還是其他變項C控制A和B）。若是這樣，x軸和y軸上代表那個變項並不重要。然而在許多情境，研究者對於那個變量最有可能是原因或結果，會有一個相當明確的想法，或者對於何者為因至少有一些懷疑。若是這樣，用x軸代表原因的自變項，用y軸代表效應的應變項是一種傳統的繪圖規範。了解這樣的規範，讓我們來看一下如何合理運用jamovi繪製散佈圖，同樣使用在相關這一節做為示範的資料集（parenthood.csv）。\n假定我的目標是繪製一個顯示我睡眠時間（dani.sleep）與隔天沮喪程度（dani.grump）兩個變項關係的散佈圖，我們有兩種不同的方法使用jamovi得到我們想要的圖。第一種方法是設定’Regression’ - ‘Correlation Matrix’選單下方的’Plot’選項，這樣可以得到如圖 Figure 14.8 的結果。請注意，jamovi會繪製一條通過資料點的直線，稍後在認識線性迴歸模型這一節進一步說明。以這種方法繪製散佈圖也能繪製’變項密度’，這個選項會添加一條密度曲線，顯示每個變項的資料分佈狀況。\n\n\n\n\n\n\nFigure 10.8: 使用jamovi相關分析模組的’Correlation Matrix’所繪製的散佈圖。\n\n\n\n\n第二種方法是使用jamovi的附加模組之一scatr，只要點擊jamovi介面右上角的那個大「\\(+\\)」，在jamovi模組庫裡找到scatr，然後點擊「install」進行安裝。安裝成功後，在「Exploration」的選單下方會多出新的「Scatterplot」選項。這種方法繪製的散佈圖和第一種方法不大一樣，如同 Figure 14.9 所顯示，但是透露的訊息是一樣的。\n\n\n\n\n\n\nFigure 10.9: 使用jamovi擴充模組’scatr’繪製的散佈圖\n\n\n\n\n\n10.2.1 更多解讀散佈圖的方法\n通常我們會需要查看多個變項之間的關係，可以在 jamovi 的 ‘Correlation Matrix’選單下方的’Plot’ 選項，勾選繪制散佈圖矩陣。只要加入另一個變項到要變項列表，例如 baby.sleep，jamovi 就會生成一個散佈圖矩陣，如同 Figure 10.10 的示範。\n\n\n\n\n\n\nFigure 10.10: jamovi繪製的散佈圖矩陣"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "href": "10-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "title": "10  相闗與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see [Correlations]), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 14.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 10.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 10.11 (b).\n\n\n\n\n\nFigure 10.11: 圖a展示同 Figure 14.9 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 10.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).8 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).9 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "href": "10-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "title": "10  相闗與線性迴歸",
    "section": "10.4 線性迴歸模型的參數估計",
    "text": "10.4 線性迴歸模型的參數估計\n(本節待編修)\n好的，現在讓我們重新畫圖，這次我會添加一些線條以顯示所有觀察值的殘差大小。當回歸線很好時，我們的殘差（實心黑線的長度）看起來都很小，如圖@fig-fig10-12 (a)所示，但是當回歸線是一條壞線時，殘差就會大得多，這可以從觀察圖@fig-fig10-12 (b)中看出。嗯，也許在追求一條迴歸模型時，我們希望得到小的殘差。是的，這確實有道理。事實上，我想我可以說，「最適合」的迴歸線是具有最小殘差的線。或者，更好的是，因為統計學家似乎喜歡將所有東西都平方，為什麼不說：\n\n估計的迴歸係數\\(\\hat{b}_0\\)和\\(\\hat{b}_1\\)是最小化殘差平方和的係數，我們可以將其寫為\\(\\sum_i (Y_i - \\hat{Y}_i)^2\\)或\\(\\sum_i \\epsilon_i^2\\)。\n\n\n\n\n\n\n\nFigure 10.12: 圖a展示各資料點與穿越資料中心地帶的最佳迴歸線之殘差，圖b展示各資料點與最差迴歸線的殘差。前者的殘差總和明顯小於後者。\n\n\n\n\n是的，沒錯，這聽起來更好。而且我把它縮排了，這可能意味著這是正確的答案。既然這是正確的答案，那麼可能值得注意的是，我們的回歸係數是估計值（我們試圖猜測描述一個母體的參數！），這就是為什麼我加了小帽子，以便我們得到\\(\\hat{b}_0\\)和\\(\\hat{b}_1\\)而不是 \\(b_0\\) 和 \\(b_1\\)。最後，我還應該指出的是，由於實際上有多種方法來估計迴歸模型，更技術性的名稱是普通最小平方法（OLS）。\n以下是翻譯：\n此時，我們已經有了「最佳」迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 的具體定義。下一個自然而然的問題是：如果我們的最佳迴歸係數是那些最小化殘差平方和的係數，我們該如何找到這些奇妙的數字呢？實際上，這個問題的答案比較複雜，並且並沒有幫助你理解迴歸的邏輯。10這一次，我放過你，直接介紹 jamovi 提供的快速捷徑，讓它來處理所有的瑣碎細節。\n\n\n10.4.1 實作線性迴歸模型\n以下是執行線性回歸的步驟，請打開 jamovi 的 ‘Regression’ - ‘Linear Regression’ 分析，使用 parenthood.csv 資料檔案。接著，將 dani.grump 指定為 ‘Dependent Variable’，dani.sleep 輸入到 ‘Covariates’ 方塊中。這將給出如圖 Figure 10.13 所示的結果，顯示截距 \\(\\hat{b}_0 = 125.96\\) 和斜率 \\(\\hat{b}_1 = -8.94\\)。換言之，在圖 Figure 10.11 中我繪製的最佳擬合迴歸線的公式為：\n\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 10.13: jamovi的線性迴歸分析示範畫面。\n\n\n\n\n\n\n10.4.2 解讀線性迴歸模型參數估計\n最重要的是理解如何解釋這些係數。讓我們從 \\(\\hat{b}_1\\) 開始，也就是斜率。如果我們回想斜率的定義，\\(\\hat{b}_1=-8.94\\) 意味著如果我將 \\(X_i\\) 增加 1，那麼 \\(Y_i\\) 就會減少 8.94。換言之，每多睡一個小時，我的心情就會改善，我的脾氣就會降低 8.94 個脾氣值。那截距呢？由於 \\(\\hat{b}_0\\) 對應著「當 \\(X_i\\) 為 0 時 \\(Y_i\\) 的期望值」，它很容易理解。這意味著如果我一夜都沒睡 (\\(X_i = 0\\))，我的脾氣就會瘋狂到達不可想像的值 (\\(Y_i = 125.96\\))。我想最好還是避免這種情況。\n\n\n\n還有關於等級資料(Rank data)的迴歸分析，請參考線性模型的學習取向的相關與線性迴歸這一節。\n至此是一般教科書的基礎統計範圍，接下來本章的單元屬於高等統計。使用這本電子書學習的學生與教學的老師們，可根據自身的學習目標調整。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#多元線性迴歸",
    "href": "10-Correlation-and-linear-regression.html#多元線性迴歸",
    "title": "10  相闗與線性迴歸",
    "section": "10.5 多元線性迴歸",
    "text": "10.5 多元線性迴歸\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n10.5.1 jamovi實務示範\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 14.4.\n\n\n\n\nTable 10.4:  增加預測變項迴歸係數的示 \n \n  \n    截距 \n    老爸睡眠小時數 \n    小嬰兒睡眠小時數 \n  \n \n\n  \n    125.97 \n    -8.95 \n    0.01 \n  \n\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 10.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 10.14: 這張圖展示多元迴歸模型的三維立體視覺化。模型中有兩個預測變項，分別是 “dani.sleep” 和 “baby.sleep”，而目標變項是 “dani.grump”，這三個變項構成圖中的三維空間。每筆資料都是這個空間中的一個點。就像簡單線性迴歸模型在二維空間形成一條線一樣，此多元迴歸模型在三維空間形成一個平面。當我們估計迴歸係數時，我們能做的就是找到一個盡可能靠近所有資料點的平面。\n\n\n\n\n[Additional technical detail11]"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "href": "10-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "title": "10  相闗與線性迴歸",
    "section": "10.6 量化迴歸模型的適配性",
    "text": "10.6 量化迴歸模型的適配性\nSo we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction \\(\\hat{Y}_i\\) about what my mood is like, but my actual mood is \\(Y_i\\) . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n\n10.6.1 \\(R^2\\)\nOnce again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\nwhich we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\nWhile we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the \\(\\hat{Y}\\) values, and for the simple model that uses only a single predictor we would do the following:\n\ncreate a new column called ‘Y.pred’ using the formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’\ncalculate the SS(resid) by creating a new column called ‘(Y-Y.pred)^2’ using the formula ’ = (dani.grump - Y.pred)^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ’ sum( ( Y-Y.pred)^2 ) .\nAt the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ‘mean’ in its function).\nThen create a new column, called ’ (Y - mean(Y))^2 )’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ‘sum( (Y - mean(Y))^2 )’.\nCalculate R.squared by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.\n\nThis gives a value for \\(R^2\\) of ‘0.8161018’. The \\(R^2\\) value, sometimes called the coefficient of determination12 has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained \\(R^2 = .816\\) means that the predictor (my.sleep) explains \\(81.6\\%\\) of the variance in the outcome (my.grump).\nNaturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the \\(R^2\\) value for your regression model. As we’ll see later on in the section on Running the hypothesis tests in jamovi, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of \\(R^2\\) that I want to point out.\n\n\n10.6.2 迴歸與相關的關聯\nAt this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol \\(r\\) to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient \\(r\\) and the \\(R^2\\) value from linear regression? Of course there is: the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.\n\n\n10.6.3 校正後 \\(R^2\\)\nOne final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted \\(R^2\\)”. The motivation behind calculating the adjusted \\(R^2\\) value is the observation that adding more predictors into the model will always cause the \\(R^2\\) value to increase (or at least not decrease).\n[Additional technical detail13]\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted \\(R^2\\) value is that when you add more predictors to the model, the adjusted \\(R^2\\) value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted \\(R^2\\) value can’t be interpreted in the elegant way that \\(R^2\\) can. \\(R^2\\) has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted \\(R^2\\).\nAn obvious question then is whether you should report \\(R^2\\) or adjusted \\(R^2\\) . This is probably a matter of personal preference. If you care more about interpretability, then \\(R^2\\) is better. If you care more about correcting for bias, then adjusted \\(R^2\\) is probably better. Speaking just for myself, I prefer \\(R^2\\). My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in [Hypothesis tests for regression models], if you’re worried that the improvement in \\(R^2\\) that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that."
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "href": "10-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "title": "10  相闗與線性迴歸",
    "section": "10.7 迴歸模型的假設檢定",
    "text": "10.7 迴歸模型的假設檢定\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n10.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail14]\nWe’ll see much more of the F statistic in Chapter 12, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n10.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the [Multiple linear regression] model we have already looked at (Table 14.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.15 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n10.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 10.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 10.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 10.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.16\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "href": "10-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "title": "10  相闗與線性迴歸",
    "section": "10.8 迴歸係數的更多資訊",
    "text": "10.8 迴歸係數的更多資訊\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n10.8.1 迴歸係數的信賴區間\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail17]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 10.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n10.8.2 標準化迴歸係數的計算方法\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.18 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail19]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 10.16.\n\n\n\n\n\nFigure 10.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "href": "10-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "title": "10  相闗與線性迴歸",
    "section": "10.9 迴歸模型的適用條件",
    "text": "10.9 迴歸模型的適用條件\nThe linear regression model that I’ve been discussing relies on several assumptions. In [Model checking] we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.\n\nLinearity. A pretty fundamental assumption of the linear regression model is that the relationship between \\(X\\) and \\(Y\\) actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.\nIndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.\nNormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors \\(X\\) and the outcome \\(Y\\) are non-normal, so long as the residuals \\(\\epsilon\\) are normal. See the [Checking the normality of the residuals] section.\nEquality (or ‘homogeneity’) of variance. Strictly speaking, the regression model assumes that each residual \\(\\epsilon_i\\) is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation \\(\\sigma\\) that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of \\(\\hat{Y}\\) , and (if we’re being especially paranoid) all values of every predictor \\(X\\) in the model.\n\nSo, we have four main assumptions for linear regression (that neatly form the acronym ‘LINE’). And there are also a couple of other things we should also check for:\n\nUncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See the [Checking for collinearity] section.\nNo “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on [Three kinds of anomalous data]."
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-Model-checking",
    "href": "10-Correlation-and-linear-regression.html#sec-Model-checking",
    "title": "10  相闗與線性迴歸",
    "section": "10.10 診斷適用條件",
    "text": "10.10 診斷適用條件\nThe main focus of this section is regression diagnostics, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason. It’s not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgement when doing this. It’s easy to get lost in all the details of checking this thing or that thing, and it’s quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn all the tools, so instead they decide not to do any model checking. This is a bit of a worry!\nIn this section I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full space of things you could do, but it’s still much more detailed than what I see a lot of people doing in practice, and even I don’t usually cover all of this in my intro stats class either. However, I do think it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from Fox & Weisberg (2011), the book associated with the ‘car’ package that is used to conduct regression analysis in R. The ‘car’ package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don’t want to sound too gushy about it, but I do think that Fox & Weisberg (2011) is well worth reading, even if some of the advanced diagnostic techniques are only available in R and not jamovi.\n\n10.10.1 三種殘差\nThe majority of regression diagnostics revolve around looking at the residuals, and by now you’ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that, precisely because of the fact that we care a lot about the residuals, there are several different kinds of residual that we might consider. In particular, the following three kinds of residuals are referred to in this section: “ordinary residuals”, “standardised residuals”, and “Studentised residuals”. There is a fourth kind that you’ll see referred to in some of the Figures, and that’s the “Pearson residual”. However, for the models that we’re talking about in this chapter the Pearson residual is identical to the ordinary residual.\nThe first and simplest kind of residuals that we care about are ordinary residuals. These are the actual raw residuals that I’ve been talking about throughout this chapter so far. The ordinary residual is just the difference between the fitted value \\(\\hat{Y}_i\\) and the observed value \\(Y_i\\). I’ve been using the notation \\(\\epsilon_i\\) to refer to the i-th ordinary residual, and by gum I’m going to stick to it. With this in mind, we have the very simple equation\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\nThis is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here. I just wanted to repeat myself. One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0 but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the pattern of the residuals and not their actual values, it’s convenient to estimate the standardised residuals, which are normalised in such a way as to have standard deviation of 1.\n[Additional technical detail20]\nThe third kind of residuals are Studentised residuals (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual. 21\nBefore moving on, I should point out that you don’t often need to obtain these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various options that provide the diagnostics, or assumption checks, will take care of these calculations for you. Even so, it’s always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.\n\n\n10.10.2 三種反常資料\nOne danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. I discussed this idea previously in Section 5.2.3 in the context of discussing the outliers that get automatically identified by the boxplot option under ‘Exploration’ - ‘Descriptives’, but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous”. All three are interesting, but they have rather different implications for your analysis.\nThe first kind of unusual observation is an outlier. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in Figure 10.17. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, \\(\\epsilon_i^*\\). Outliers are interesting: a big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case and try to find out why it’s so different.\n\n\n\n\n\nFigure 10.17: An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line\n\n\n\n\nThe second way in which an observation can be unusual is if it has high 槓桿作用(leverage), which happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual. If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in Figure 10.18. The leverage of an observation is operationalised in terms of its hat value, usually written \\(h_i\\) . The formula for the hat value is rather complicated22 but its interpretation is not: \\(h_i\\) is a measure of the extent to which the i-th observation is “in control” of where the regression line ends up going.\n\n\n\n\n\nFigure 10.18: An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it\n\n\n\n\nIn general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to \\(K + 1\\)). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers.\nThis brings us to our third measure of unusualness, the 影響力(influence) of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in Figure 10.19. Notice the contrast to the previous two figures. Outliers don’t move the regression line much and neither do high leverage points. But something that is both an outlier and has high leverage, well that has a big effect on the regression line. That’s why we call these points high influence, and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as Cook’s distance. 23\n\n\n\n\n\nFigure 10.19: An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)\n\n\n\n\nIn order to have a large Cook’s distance an observation must be a fairly substantial outlier and have high leverage. As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule).\nIn jamovi, information about Cook’s distance can be calculated by clicking on the ‘Cook’s Distance’ checkbox in the ‘Assumption Checks’ - ‘Data Summary’ options. When you do this, for the multiple regression model we have been using as an example in this chapter, you get the results as shown in Figure 10.20.\n\n\n\n\n\nFigure 10.20: jamovi output showing the table for the Cook’s distance statistics\n\n\n\n\nYou can see that, in this example, the mean Cook’s distance value is \\(0.01\\), and the range is from \\(0.00\\) to \\(0.11\\), so this is some way off the rule of thumb figure mentioned above that a Cook’s distance greater than 1 is considered large.\nAn obvious question to ask next is, if you do have large values of Cook’s distance what should you do? As always, there’s no hard and fast rule. Probably the first thing to do is to try running the regression with the outlier with the greatest Cook’s distance24 excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study. Try to figure out why the point is so different. If you start to become convinced that this one data point is badly distorting your results then you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.\n\n\n10.10.3 檢測殘差常態性\nLike many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option. The output is shown in Figure 10.21, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.\n\n\n\n\n\nFigure 10.21: Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi\n\n\n\n\nAnother thing we should check is the relationship between the fitted values and the residuals themselves. We can get jamovi to do this using the ‘Residuals Plots’ option, which provides a scatterplot for each predictor variable, the outcome variable, and the fitted values against residuals, see Figure 10.22. In these plots we are looking for a fairly uniform distribution of ‘dots’, with no clear bunching or patterning of the ‘dots’. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.\n\n\n\n\n\nFigure 10.22: Residuals plots produced in jamovi\n\n\n\n\nIf we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in Section 6.3, but I do want to make special note of one additional possibility that I didn’t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it’s very widely used. 25\nYou can calculate it using the BOXCOX function in the ‘Compute’ variables screen in jamovi.\n\n\n10.10.4 檢測共線性\nThe last kind of regression diagnostic that I’m going to discuss in this chapter is the use of variance inflation factors (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor \\(X_k\\) in the model. 26\nThe square root of the VIF is pretty interpretable. It tells you how much wider the confidence interval for the corresponding coefficient bk is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options in jamovi. For both dani.sleep and baby.sleep the VIF is \\(1.65\\). And since the square root of \\(1.65\\) is \\(1.28\\), we see that the correlation between our two predictors isn’t causing much of a problem.\nTo give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables (Figure 10.23).\n\n\n\n\n\nFigure 10.23: 四個資料變項之間的相關係數矩陣\n\n\n\n\nWe have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in Figure 10.24 and you can see from the VIF values that, yep, that’s some mighty fine collinearity there.\n\n\n\n\n\nFigure 10.24: Collinearity statistics for multiple regression, produced in jamovi"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "href": "10-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "title": "10  相闗與線性迴歸",
    "section": "10.11 決定線性模型的變項組合",
    "text": "10.11 決定線性模型的變項組合\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 27\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n10.11.1 逐步排除法\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n10.11.2 逐步納入法\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n10.11.3 使用警告\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n10.11.4 比較迴歸模型\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.28\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 29\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 10.25. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 10.25.\n\n\n\n\n\nFigure 10.25: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#本章小結",
    "href": "10-Correlation-and-linear-regression.html#本章小結",
    "title": "10  相闗與線性迴歸",
    "section": "10.12 本章小結",
    "text": "10.12 本章小結\n\n想了解兩個變項之間的關聯性有多強？就計算相關係數\n散佈圖繪製方法\n前進下一章前必學的課題：什麼是線性迴歸模型 以及使用線性迴歸模型估計參數\n多元線性迴歸\n量化迴歸模型的適配性 要了解 \\(R^2\\) 。\n迴歸模型的假設檢定\n在迴歸係數的更多資訊 這一節，我們學習如何計算迴歸係數的信賴區間以及標準化迴歸係數的計算方法\n迴歸模型的適用條件 以及診斷適用條件\n決定線性模型的變項組合\n\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.1 本章示範資料",
    "text": "12.1 本章示範資料\n假設您已參與一個臨床試驗，該試驗正在測試一種名為Joyzepam的新抗抑鬱藥。為了公平地測試藥物的有效性，該研究涉及三種獨立的藥物。其中一種是安慰劑，另一種是現有的抗抑鬱/抗焦慮藥物，名為Anxifree。您的初步測試招募了18名患有中度至重度抑鬱症的參與者。由於藥物有時與心理治療結合使用，因此您的研究包括9個正在進行認知行為治療（CBT）的人和9個未進行治療的人。參與者被隨機分配（當然是雙盲的）治療，使得3個接受CBT的人和3個無治療的人被分配到3種藥物中的每一種。心理學家在每個人使用每種藥物3個月後評估每個人的情緒，並在從\\(-5\\)到\\(+5\\)的範圍內評估每個人情緒的整體改善。在這種研究設計下，讓我們現在加載數據文件 clinicaltrial.csv。我們可以看到這個數據集包含了三個變量，分別是藥物、治療和情緒提升。\n在本章中，我們真正感興趣的是藥物對情緒提升的影響。首先要做的是計算一些描述性統計數據並繪製一些圖表。在@sec-Descriptive-statistics章節中，我們向您展示了如何做到這一點，jamovi中可以計算的一些描述性統計數據顯示在@fig-fig12-1中。\n\n\n\n\n\n\nFigure 12.1: 情緒提升的描述性統計數據，以及按給予的藥物繪製的盒形圖。\n\n\n\n\n如圖所示，對於Joyzepam組的參與者，情緒的改善比Anxifree組或安慰劑組要大。Anxifree組的情緒提升比對照組要大，但差距不是那麼大。我們想要回答的問題是，這些差異是否“真實”，或者僅僅是因為偶然？"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.2 變異數分析的運作原理",
    "text": "12.2 變異數分析的運作原理\n為了回答我們的臨床試驗數據所提出的問題，我們將進行單因素變異數分析（one-way ANOVA）。首先，我將通過自下而上地構建統計工具並向您展示，如果您無法使用jamovi中的任何酷炫內置ANOVA功能，該如何做。我希望您能仔細閱讀，嘗試一兩次用較長的方法來確保您真正了解ANOVA是如何運作的，然後一旦您掌握了概念，就永遠不要再用這種方法了。\n在上一節中我描述的實驗設計強烈表明，我們對比較三種不同藥物的平均心情變化感興趣。在這個意義上，我們討論的分析類似於t檢驗（參見@sec-Comparing-two-means），但涉及多於兩個組別。如果我們讓\\(\\mu_P\\)表示安慰劑引起的情緒變化的母體平均值，並讓\\(\\mu_A\\)和\\(\\mu_J\\)表示我們的兩種藥物Anxifree和Joyzepam的對應平均值，那麼我們要檢驗的（有些悲觀的）虛無假設是：所有三個母體平均值都相同。也就是說，這兩種藥物都沒有比安慰劑更有效。我們可以將此虛無假設寫為：\n\\[H_0: \\text{ 事實上 } \\mu_P=\\mu_A=\\mu_J\\]\n因此，我們的替代假設是：三種不同治療中至少有一種與其他治療不同。將其用數學表示有點困難，因為（正如我們將要討論的那樣）虛無假設可能以很多不同的方式是錯誤的。所以目前我們只將替代假設寫成這樣：\n\\[H_1: \\text{ 事實 }\\underline{ 不是 }\\text{  } \\mu_P=\\mu_A=\\mu_J\\]\n這個虛無假設比我們之前見過的任何一個都要棘手得多。我們應該如何檢驗它？一個明智的猜測是「進行方差分析」，因為這是本章的標題，但是目前還不太清楚為什麼「方差分析」會幫助我們了解有關均值的有用信息。事實上，這是人們首次接觸方差分析時遇到的最大概念困難之一。要了解其原理，我認為從方差開始談起是最有幫助的，具體來說就是組間變異性和組內變異性（Figure 12.2）。\n\n\n\n\n\n\nFigure 12.2: 圖形說明 ‘組間’ 變異 (面板 (a)) 和 ‘組內’ 變異 (面板 (b))。在左側，箭頭顯示組平均值之間的差異。在右側，箭頭強調每個組內的變異性。\n\n\n\n\n\n12.2.1 計算依變項變異數的兩套公式\n首先，讓我們引入一些符號。我們將使用 G 來表示組的總數。對於我們的數據集，有三種藥物，所以有 \\(G = 3\\) 個組。接下來，我們將使用 \\(N\\) 表示總樣本大小；在我們的數據集中，一共有 \\(N = 18\\) 人。同樣地，讓我們用 \\(N_k\\) 表示第 k 個組中的人數。在我們的虛擬臨床試驗中，所有三組的樣本大小都是 \\(N_k = 6\\)。1 最後，我們將使用 Y 表示結果變量。在我們的案例中，Y 指的是心情變化。具體來說，我們將使用 Yik 指代第 k 個組中第 i 個成員所經歷的心情變化。同樣，我們將使用 \\(\\bar{Y}\\) 作為實驗中所有 18 人的平均心情變化，並使用 \\(\\bar{Y}_k\\) 指代第 k 組中 6 人所經歷的平均心情變化。\n現在我們已經整理好符號，我們可以開始寫下公式。首先，讓我們回想一下在 Section 4.2 中使用的方差公式，在那個做描述性統計的較早時期。Y 的樣本方差被定義為以下公式 \\[Var(Y)=\\frac{1}{N}\\sum_{k=1}^{G}\\sum_{i=1}^{N_k}(Y_{ik}-\\bar{Y})^2\\] 這個公式看起來與 Section 4.2 中的方差公式幾乎相同。唯一的區別是這次我有兩個求和：我對組進行求和（即 \\(k\\) 的值）以及對組內的人進行求和（即 \\(i\\) 的值）。這只是一個純粹的表面細節。如果我使用符號 \\(Y_p\\) 來表示樣本中第 p 個人的結果變量值，那麼我只有一個求和。我們在這裡有兩個求和的唯一原因是我將人分類到組，然後為組內的人分配數字。\n在這裡，具體的例子可能很有用。讓我們考慮 Table 12.1，在這個表格中，我們有總共 \\(N = 5\\) 個人分成 \\(G = 2\\) 個組。任意地說，讓我們說「酷」的人是第 1 組，「不酷」的人是第 2 組。結果發現我們有三個酷人（\\(N_1 = 3\\)）和兩個不酷的人（\\(N_2 = 2\\)）。\n\n\n\n\n\nTable 12.1:  在酷和不酷的團體中的脾氣。 \n\nnameperson Pgroupgroup num. kindex in groupgrumpiness \\( Y_{ik} \\) or \\( Y_p \\)\n\nAnn1cool1120\n\nBen2cool1255\n\nCat3cool1321\n\nTim4uncool2191\n\nEgg5uncool2222\n\n\n\n\n\n注意到這裡我構建了兩個不同的標記方案。我們有一個「人」變量 p，所以說到 Yp 作為樣本中的第 p 人的脾氣是完全合理的。例如，表格顯示 Tim 是第四個，所以我們會說 \\(p = 4\\)。所以，在談到這個「Tim」這個人（無論他是誰）的脾氣 \\(Y\\) 時，我們可以通過說 \\(Y_p = 91\\) 來指稱他的脾氣，即對於人 \\(p = 4\\)。然而，這不是我們唯一可以指稱 Tim 的方法。作為一個替代方法，我們可以注意到 Tim 屬於「不酷」的組（\\(k = 2\\)），實際上是不酷組中列出的第一個人（\\(i = 1\\)）。所以，通過說 \\(Y_{ik} = 91\\)，在 \\(k = 2\\) 和 \\(i = 1\\) 的情況下，同樣有效地指稱 Tim 的脾氣。\n換句話說，每個人 p 都對應一個唯一的 ik 組合，所以我之前給出的公式實際上與我們原始的方差公式是相同的，即 \\[Var(Y)=\\frac{1}{N}\\sum_{p=1}^{N}(Y_p-\\bar{Y})^2\\] 在兩個公式中，我們所做的就是對樣本中的所有觀察值求和。大多數時候，我們只使用更簡單的 Yp 記號；使用 \\(Y_p\\) 的等式顯然是兩者中更簡單的一個。然而，在進行方差分析（ANOVA）時，我們需要跟踪哪些參與者屬於哪個組別，並且我們需要使用 Yik 記號來完成這項工作。\n\n\n\n\n12.2.2 變異數與平方差總和\n好的，既然我們對方差的計算有了很好的了解，讓我們定義一個叫做總平方和（total sum of squares）的東西，記作 SStot。這很簡單。計算方差時，我們是對平方偏差求平均，而計算總平方和時，我們只需將它們加起來。2\n當我們在 ANOVA 的上下文中談論分析變異數時，我們實際上是在處理總平方和，而不是實際的方差。3\n接下來，我們可以定義一個僅捕捉組間差異的變異概念。我們通過查看組平均值 \\(\\bar{Y}_k\\) 和整體平均值 \\(\\bar{Y}\\) 之間的差異來實現這一點。4\n這並不太難以證明，實驗中人們之間的總變異（\\(SS_{tot}\\)）實際上是組間差異（\\(SS_b\\)）和組內變異（\\(SS_w\\)）之和。即，\n\\[SS_w+SS_b=SS_{tot}\\] 好耶。\n好的，那麼我們發現了什麼？我們已經發現了與結果變量相關的總變異（\\(SS_{tot}\\)）可以在數學上被劃分為“由於不同組的樣本均值之間的差異所產生的變異”（\\(SS_b\\)）加上“其他所有變異”（\\(SS_w\\)）之和5。\n那怎麼幫助我找出這些組是否有不同的母體均值呢？嗯。等等。稍等一下。現在想想，這正是我們在尋找的。如果原假設成立，那麼您會期望所有樣本均值彼此非常相似，對吧？這將意味著您會期望 \\(SS_b\\) 非常小，或者至少您會期望它比“與其他所有事物相關的變異”（\\(SS_w\\)）小得多。嗯。我感覺到了一個假設檢驗的來臨。\n\n\n\n12.2.3 平方差總和與F檢定\n正如我們在上一節中看到的，ANOVA 的定性思想是將兩個平方和值 \\(SS_b\\) 和 \\(SS_w\\) 相互比較。如果組間變異 \\(SS_b\\) 相對於組內變異 \\(SS_w\\) 較大，那麼我們有理由懷疑不同組的母體均值彼此並不相同。為了將這一點轉化為可操作的假設檢驗，我們需要進行一些“小小的調整”。首先，我將向您展示我們如何計算檢驗統計量——F 值(F ratio)，然後嘗試讓您了解為什麼我們要這樣做。\n為了將我們的 SS 值轉換為 F 比，我們首先需要計算與 \\(SS_b\\) 和 \\(SS_w\\) 值相關的自由度。通常情況下，自由度對應於對特定計算做出貢獻的唯一“數據點”的數量，減去它們需要滿足的“約束”條件的數量。對於組內變異性，我們計算的是個體觀測值（\\(N\\) 個數據點）與組平均值（\\(G\\) 個約束）之間的變異。相反，對於組間變異性，我們關心的是組平均值（\\(G\\) 個數據點）在整體平均值（1 個約束）周圍的變化。因此，在這裡的自由度為：\n\\[df_b=G-1\\] \\[df_w=N-G\\]\n好吧，這似乎很簡單。接下來，我們將平方和值轉換為“平均平方”值，方法是除以自由度：\n\\[MS_b=\\frac{SS_b}{df_b}\\] \\[MS_w=\\frac{SS_w}{df_w}\\]\n最後，我們通過將組間 MS 除以組內 MS 來計算 F 比：\n\\[F=\\frac{MS_b}{MS_w}\\]\n從非常一般的層面上，F 統計量背後的直覺很簡單。F 值越大，表示組間變異相對於組內變異越大。因此，F 值越大，我們反駁虛無假設的證據就越多。但是 \\(F\\) 必須多大才能實際拒絕 \\(H_0\\)？要理解這一點，您需要更深入地了解 ANOVA 是什麼以及平均平方值實際上是什麼。\n下一節將詳細討論這個問題，但對於不感興趣實際衡量試驗內容的讀者，我將直接進入主題。為了完成我們的假設檢定，我們需要知道在虛無假設為真時 F 的抽樣分佈。不足為奇的是，在虛無假設下 F 統計量的抽樣分佈是一個 \\(F\\) 分佈。如果您回顧我們在 Chapter 7 中關於 F 分佈的討論，\\(F\\) 分佈有兩個參數，對應於涉及的兩個自由度。第一個 \\(df_1\\) 是組間自由度 \\(df_b\\)，第二個 \\(df_2\\) 是組內自由度 \\(df_w\\)。\n\n\n\n\n\nTable 12.2:  ANOVA 中涉及的所有關鍵數量都組織成一個“標準” ANOVA 表。所有數量的公式（除了 p 值，它有一個非常難看的公式，如果沒有計算機，計算起來會非常困難）都有顯示。 \n\nbetween\ngroupswithin\ngroups\n\ndf\\(  df_b=G-1  \\)\\(  df_w=N-G  \\)\n\nsum of squares\\(  SS_b=\\sum_{k=1}^{G} N_k  (\\bar{Y}_k-\\bar{Y})^2  \\)\\(  SS_w=\\sum_{k=1}^{G} \\sum_{i=1}^{N_k}   (Y_{ik}-\\bar{Y}_k)^2  \\)\n\nmean squares\\(  MS_b=\\frac{SS_b}{df_b}  \\)\\(  MS_w=\\frac{SS_w}{df_w}  \\)\n\nF-statistic\\(  F=\\frac{MS_b}{df_b}  \\)-\n\np-value[complicated]-\n\n\n\n\n\n在 Table 12.2 中顯示了涉及單因素 ANOVA 的所有關鍵數量的概要，包括顯示如何計算它們的公式。\n[額外的技術細節 6]\n\n\n\n12.2.4 實例演練\n先前的討論相當抽象且有點技術性，所以我認為此刻可能需要看一個實際示例。為此，讓我們回到本章開頭介紹的臨床試驗數據。我們在開始時計算的描述性統計數據告訴我們各組的平均值：安慰劑的平均情緒增益為 \\(0.45\\)，Anxifree 為 \\(0.72\\)，Joyzepam 為 \\(1.48\\)。有了這個想法，讓我們像 1899 年一樣開趴[^13-comparing-several-means-one-way-anova-7]，開始用鉛筆和紙做一些計算。我只會對前 \\(5\\) 個觀察值進行此操作，因為現在不是該死的 \\(1899\\) 年，而且我非常懶。讓我們從計算 \\(SS_w\\) 開始，即組內平方和。首先，讓我們繪製一個漂亮的表格來協助我們的計算（Table 12.3）\n[^13-comparing-several-means-one-way-anova-7]：或者，確切地說，像 “1899年那樣狂歡，當時我們沒有朋友，也沒有比做一些計算更好的事情可做，因為直到 1920 年左右，ANOVA 都不存在。”\n\n\n\n\n\n\nTable 12.3:  示範演算第一步 \n\ngroup koutcome \\( Y_{ik} \\)\n\nplacebo0.5\n\nplacebo0.3\n\nplacebo0.1\n\nanxifree0.6\n\nanxifree0.4\n\n\n\n\n\n在這個階段，我在表格中包含的只是原始數據本身。也就是說，每個人的分組變量（即藥物）和結果變量（即心情增益）。請注意，這裡的結果變量對應於我們先前方程式中的 \\(\\bar{Y}_{ik}\\) 值。接下來的計算步驟是為研究中的每個人寫下相應的組平均值，\\(\\bar{Y}_k\\)。這有點重複，但並不是特別困難，因為我們在進行描述性統計時已經計算了這些組平均值，見 Table 12.4。\n\n\n\n\n\n\nTable 12.4:  示範演算第二步 \n\ngroup koutcome \\( Y_{ik} \\)group mean \\( \\bar{Y}_k \\)\n\nplacebo0.50.45\n\nplacebo0.30.45\n\nplacebo0.10.45\n\nanxifree0.60.72\n\nanxifree0.40.72\n\n\n\n\n\n既然我們已經寫下了這些，我們需要再次為每個人計算與相應組平均值的偏差。也就是說，我們想要減去 \\(Y_{ik} - \\bar{Y}_k\\)。在我們做完這個之後，我們需要將所有東西平方。當我們這樣做時，這就是我們得到的結果（Table 12.5）\n\n\n\n\n\nTable 12.5:  示範演算第三步 \n\ngroup koutcome \\( Y_{ik} \\)group mean  \\( \\bar{Y}_k \\)dev. from group mean  \\( Y_{ik} - \\bar{Y}_k \\)squared deviation \\(  (Y_{ik}-\\bar{Y}_k)^2 \\)\n\nplacebo0.50.450.050.0025\n\nplacebo0.30.45-0.150.0225\n\nplacebo0.10.45-0.350.1225\n\nanxifree0.60.72-0.120.0136\n\nanxifree0.40.72-0.320.1003\n\n\n\n\n\n最後一步同樣簡單。為了計算組內平方和，我們只需將所有觀察值的平方偏差相加：\n\\[\n\\begin{split}\nSS_w & = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 \\\\\n& = 0.2614\n\\end{split}\n\\]\n當然，如果我們真的想得到正確的答案，我們需要對數據集中的所有18個觀察值進行此操作，而不僅僅是前五個。如果我們想要的話，我們可以繼續使用鉛筆和紙進行計算，但這相當繁瑣。或者，使用專用的電子表格程序（如 OpenOffice 或 Excel）也不是很困難。嘗試自己做。我在 Excel 中做的那個文件名為 clinicaltrial_anova.xls。當你做完後，你應該得到一個組內平方和值為 \\(1.39\\)。\n好的。現在我們已經計算了組內變異 \\(SS_w\\)，是時候將我們的注意力轉向組間平方和 \\(SS_b\\) 了。對於這種情況，計算非常相似。主要區別在於，對於所有觀察值，我們不再計算觀察值 Yik 和組平均值 \\(\\bar{Y}_k\\) 之間的差異，而是計算所有組的組平均值 \\(\\bar{Y}_k\\) 和總平均值 \\(\\bar{Y}\\)（在這種情況下為 \\(0.88\\)）之間的差異（Table 12.6）。\n\n\n\n\n\nTable 12.6:  示範演算第4步 \n\ngroup kgroup mean \\( \\bar{Y}_k \\)grand mean  \\( \\bar{Y} \\)deviation  \\( \\bar{Y}_k - \\bar{Y} \\)squared deviation \\(  ( \\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo0.450.88-0.430.19\n\nanxifree0.720.88-0.160.03\n\njoyzepam1.480.880.600.36\n\n\n\n\n\n然而，對於組間計算，我們需要將每個平方偏差乘以 \\(N_k\\)，即組中的觀察值數量。我們這樣做是因為該組中的每個觀察值（所有 \\(N_k\\) 個觀察值）都與組間差異有關。因此，如果安慰劑組有六個人，並且安慰劑組的平均值與總平均值相差 \\(0.19\\)，那麼這六個人與組間變異之間的關聯總和為 \\(6 \\times 0.19 = 1.14\\)。因此，我們必須擴展我們的計算表格（Table 12.7）。\n\n\n\n\n\n\nTable 12.7:  示範演算第5步 \n\ngroup k...squared deviations  \\( (\\bar{Y}_k-\\bar{Y})^2 \\)sample size  \\( N_k \\)weighted squared dev   \\(  N_k (\\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo...0.1961.14\n\nanxifree...0.0360.18\n\njoyzepam...0.3662.16\n\n\n\n\n\n現在，我們的組間平方和是通過將這些“加權平方偏差”在研究中的所有三組中求和而得到的：\n\\[\\begin{aligned} SS_b & = 1.14 + 0.18 + 2.16 \\\\ &= 3.48 \\end{aligned}\\]\n如您所見，組間計算要短得多 7。現在我們已經計算出了平方和值 \\(SS_b\\) 和 \\(SS_w\\)，剩下的 ANOVA 分析就相當簡單了。下一步是計算自由度。由於我們有 \\(G = 3\\) 個組和 \\(N = 18\\) 個觀察值，我們的自由度可以通過簡單的減法來計算：\n\\[\n\\begin{split}\ndf_b & = G-1 = 2 \\\\\ndf_w & = N-G = 15\n\\end{split}\n\\]\n接下來，由於我們已經計算了平方和值和自由度的值，對於組內變異性和組間變異性，我們可以通過將一個除以另一個來獲得平均平方值：\n\\[\n\\begin{split}\nMS_b & = \\frac{SS_b}{df_b} = \\frac{3.48}{2} = 1.74 \\\\\nMS_w & = \\frac{SS_w}{df_w} = \\frac{1.39}{15} = 0.09\n\\end{split}\n\\]\n我們快完成了。平均平方值可用於計算我們感興趣的 F 值，這是我們感興趣的檢驗統計量。我們通過將組間 MS 值除以組內 MS 值來完成此操作。\n\\[\n\\begin{split}\nF & = \\frac{MS_b}{MS_w}  = \\frac{1.74}{0.09} \\\\\n& = 19.3\n\\end{split}\n\\]\n哇！這真的非常令人興奮，對嗎？現在我們有了檢驗統計量，最後一步是找出檢驗本身是否給我們一個顯著結果。如 Chapter 9 在“過去的日子”中所討論的，我們要做的是打開一本統計教科書或翻到後面的部分，這裡會有一個巨大的查找表，我們會找到對應特定 alpha 值（空假設拒絕區域）的閾值 F 值，例如 \\(0.05\\)，\\(0.01\\) 或 \\(0.001\\)，對於 2 和 15 度的自由度。用這種方法，對於 alpha 為 \\(0.001\\) 的情況，我們會得到一個閾值 F 值為 \\(11.34\\)。由於這小於我們計算出的 F 值，我們說 \\(p < 0.001\\)。但那是過去的日子，現在花哨的統計軟件會為您計算出確切的 p 值。實際上，確切的 p 值為 \\(0.000071\\)。所以，除非我們對 Type I 錯誤率非常保守，否則我們幾乎可以保證拒絕虛無假設。\n此刻，我們基本上完成了。完成計算後，將所有這些數字整理成類似於表 12.1 的 ANOVA 表是傳統做法。對於我們的臨床試驗數據，ANOVA 表將如 Table 12.8。\n\n\n\n\n\n\nTable 12.8:  完整的變異數分析結果表 \n\ndfsum of squaresmean squaresF-statisticp-value\n\nbetween groups23.481.7419.30.000071\n\nwithin groups151.390.09--\n\n\n\n\n\n如今，您可能永遠沒有太多理由想要自己構建這樣的表格，但您會發現幾乎所有的統計軟件（包括 jamovi）都傾向於將 ANOVA 的輸出組織成這樣的表格，所以最好習慣閱讀它們。然而，儘管軟件將輸出完整的 ANOVA 表，但幾乎從來沒有充分理由在您的撰寫中包含整個表格。報告此結果的統計塊的一種非常標準的方法是寫下類似以下的內容：\n\n單因素 ANOVA 顯示藥物對情緒增益有顯著影響（F(2,15) = 19.3，p < .001）。\n\n嘆氣。這麼多工作，只為了一個簡短的句子。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.3 jamovi的變異數分析模組",
    "text": "12.3 jamovi的變異數分析模組\nI’m pretty sure I know what you’re thinking after reading the last section, especially if you followed my advice and did all of that by pencil and paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations yourself sucks. There’s quite a lot of calculations that we needed to do along the way, and it would be tedious to have to do this over and over again every time you wanted to do an ANOVA.\n\n12.3.1 使用jamovi完成變異數分析\nTo make life easier for you, jamovi can do ANOVA…hurrah! Go to the ‘ANOVA’ - ‘ANOVA’ analysis, and move the mood.gain variable across so it is in the ‘Dependent Variable’ box, and then move the drug variable across so it is in the ‘Fixed Factors’ box. This should give the results as shown in Figure 12.3. 9 Note I have also checked the \\(\\eta^2\\) checkbox, pronounced “eta” squared, under the ‘Effect Size’ option and this is also shown on the results table. We will come back to effect sizes a bit later.\n\n\n\n\n\nFigure 12.3: jamovi results table for ANOVA of mood gain by drug administered\n\n\n\n\nThe jamovi results table shows you the sums of squares values, the degrees of freedom, and a couple of other quantities that we’re not really interested in right now. Notice, however, that jamovi doesn’t use the names “between-group” and “within-group”. Instead, it tries to assign more meaningful names. In our particular example, the between groups variance corresponds to the effect that the drug has on the outcome variable, and the within groups variance corresponds to the “leftover” variability so it calls that the residuals. If we compare these numbers to the numbers that I calculated by hand in [A worked example], you can see that they’re more or less the same, apart from rounding errors. The between groups sums of squares is \\(SS_b = 3.45\\), the within groups sums of squares is \\(SS_w = 1.39\\), and the degrees of freedom are \\(2\\) and \\(15\\) respectively. We also get the F-value and the p-value and, again, these are more or less the same, give or take rounding errors, to the numbers that we calculated ourselves when doing it the long and tedious way."
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#效果量",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#效果量",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.4 效果量",
    "text": "12.4 效果量\n有幾種不同的方法可以衡量 ANOVA 中的效應大小，但最常用的衡量指標是 \\(\\eta^2\\)（eta 平方）和偏 \\(\\eta^2\\)。對於單因素變異數分析，它們彼此相同，所以目前我只解釋 \\(\\eta^2\\)。\\(\\eta^2\\) 的定義實際上非常簡單：\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\n就是這樣。所以當我查看 Figure 12.3 中的 ANOVA 表時，我看到 \\(SS_b = 3.45\\) 和 \\(SS_tot = 3.45 + 1.39 = 4.84\\)。因此，我們得到一個 \\(\\eta^2\\) 值：\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\n\\(\\eta^2\\) 的解釋同樣直接。它表示可以根據預測變量（藥物）解釋的結果變量（mood.gain）可變性的比例。\\(\\eta^2=0\\) 表示兩者之間完全沒有關係，而 \\(\\eta^2=1\\) 表示關係是完美的。更好的是，\\(\\eta^2\\) 值與 Section 10.6.1 中討論的 \\(R^2\\) 關係非常密切，並具有等效的解釋。儘管許多統計教科書建議在 ANOVA 中使用 \\(\\eta^2\\) 作為默認的效應大小衡量指標，但 Daniel Lakens 的一篇有趣的博客文章表明，eta 平方在實際數據分析中可能不是最好的效應大小衡量指標，因為它可能是一個有偏估計量。有用的是，jamovi 中還有一個選項可以指定 ω 平方（\\(\\omega^2\\)），它與 η 平方相比偏差較小。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.5 多重比較與事後檢定",
    "text": "12.5 多重比較與事後檢定\n每當您對多於兩個組進行 ANOVA，並得到顯著效應時，您可能首先想問的是哪些組之間實際上存在差異。在我們的藥物示例中，我們的零假設是所有三種藥物（安慰劑、Anxifree 和 Joyzepam）對情緒的影響完全相同。但是如果你仔細想一想，實際上零假設一次聲稱了三個不同的事情。具體來說，它聲稱：\n\n您的競爭對手的藥物（Anxifree）並不比安慰劑更好（即，\\(\\mu_A = \\mu_P\\) ）\n您的藥物（Joyzepam）並不比安慰劑更好（即，\\(\\mu_J = \\mu_P\\) ）\nAnxifree 和 Joyzepam 同樣有效（即，\\(\\mu_J = \\mu_A\\)）\n\n如果上述三個聲稱中的任何一個是偽的，那麼零假設也是偽的。因此，現在我們已經拒絕了我們的零假設，我們認為至少有一件事是不正確的。但哪些呢？所有三個命題都很有趣。既然您肯定想知道您的新藥 Joyzepam 是否比安慰劑更好，那麼了解它與現有商業替代品（即 Anxifree）的比較如何就變得很重要了。甚至有用的是檢查 Anxifree 與安慰劑的表現。即使 Anxifree 已經被其他研究人員廣泛地與安慰劑進行了對照測試，但檢查您的研究是否產生了與早期工作相似的結果仍然非常有用。\n當我們根據這三個不同的命題來描述零假設時，我們需要區分的八種可能的“世界狀態”變得清晰了（Table 12.9）。\n\n\n\n\n\nTable 12.9:  虛無假設與八種可能的”現實世界” \n\npossibility:is \\( \\mu_P = \\mu_A \\)?is \\( \\mu_P = \\mu_J \\)?is \\( \\mu_A = \\mu_J \\)?which hypothesis?\n\n1\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)null\n\n2\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n3\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n4\\( \\checkmark \\)alternative\n\n5\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n6\\( \\checkmark \\)alternative\n\n7\\( \\checkmark \\)alternative\n\n8alternative\n\n\n\n\n\n通過拒絕零假設，我們已經決定我們不相信 #1 是真實的世界狀態。下一個問題是，我們認為其他七個可能性中的哪一個*是對的？面對這種情況，通常最好先看看數據。例如，如果我們查看 Figure 12.1 中的繪圖，我們很容易得出 Joyzepam 優於安慰劑和 Anxifree，但 Anxifree 和安慰劑之間沒有實際差別的結論。然而，如果我們想對此得到更清晰的答案，則可能需要進行一些測試。\n\n\n12.5.1 成對t檢定\n我們如何解決問題？考慮到我們需要比較三對不同的平均值（安慰劑對 Anxifree，安慰劑對 Joyzepam，和 Anxifree 對 Joyzepam），我們可以執行三個單獨的 t 檢驗，看看會發生什麼。在 jamovi 中這很容易做到。轉到 ANOVA 的 ‘Post Hoc Tests’（事後檢驗）選項，將 ‘drug’（藥物）變量移到右側的活動框中，然後單擊 ‘No correction’（無校正）複選框。這將產生一個整齊的表格，顯示藥物變量的三個水平之間的所有成對 t 檢驗比較，如 Figure 12.4 中所示。\n\n\n\n\n\n\n\nFigure 12.4: 未經校正的成對 t 檢驗作為 jamovi 中的事後比較。\n\n\n\n\n\n\n12.5.2 多重檢定的校正\n在上一節中，我暗示了執行大量 t 檢驗存在問題。我們擔心的是，在執行這些分析時，我們正在進行一個「捕魚之旅」。我們在沒有太多理論指導的情況下執行了大量測試，希望其中一些測試顯示出顯著性。這種對團體差異的無理論基礎的搜索被稱為事後分析（“post hoc” 是拉丁語，意為 “after this”）。[^13-comparing-several-means-one-way-anova-10]\n[^13-comparing-several-means-one-way-anova-10]：如果您確實有一些理論基礎，希望研究某些比較而不是其他比較，那就是另一回事了。在這種情況下，您實際上並不是在執行「事後分析」，而是在進行「預先計劃的比較」。我確實在本書後面談到了這種情況- Section 13.9，但現在我想保持簡單。\n進行事後分析是可以的，但需要非常小心。例如，在上一節中進行的分析應該避免，因為每個單獨的 t 檢驗都設計為 5% 的第一型錯誤率（即 \\(\\alpha = .05\\)），而我執行了其中的三個檢驗。想象一下，如果我的 ANOVA 涉及 10 個不同的組，我決定執行 45 個「事後」t 檢驗，試圖找出哪些組之間存在顯著差異，那麼僅憑機會就會出現 2 到 3 個顯著結果。正如我們在 Chapter 9 中看到的那樣，虛無假設檢驗背後的核心組織原則是控制我們的第一型錯誤率，但是現在，由於我同時執行了大量 t 檢驗以確定 ANOVA 結果的來源，整個試驗家族的實際第一型錯誤率已經完全失控。\n解決這個問題的常用方法是對 p 值進行調整，目的是控制整個試驗家族的總誤差率（參見 Shaffer (1995)）。這種調整通常（但不總是）應用於事後分析，通常被稱為多重比較校正，儘管有時也被稱為「同時推斷」。無論如何，進行這種調整的方法有很多。我將在本節和下一章節 Section 13.8 中討論其中的一些方法，但您應該意識到還有很多其他方法（例如，參見 Hsu (1996)）。\n\n\n\n12.5.3 Bonferroni校正\n這些調整中最簡單的一種被稱為邦弗隆尼校正(Dunn, 1961)，它確實非常簡單。假設我的事後分析包括 m 個單獨的檢驗，我希望確保出現任何第一型錯誤的總概率最多為 \\(\\alpha\\)。[^13-comparing-several-means-one-way-anova-11] 如果是這樣，那麼邦弗隆尼校正只是說「將所有原始 p 值乘以 m」。如果讓 \\(p\\) 表示原始 p 值，讓 \\(p_j^{'}\\) 表示經過校正的值，那麼邦弗隆尼校正告訴我們：\n[^13-comparing-several-means-one-way-anova-11]：順便值得一提的是，並非所有調整方法都試圖這樣做。我在這裡描述的是一種用於控制「家族式第一型錯誤率」的方法。然而，還有其他事後檢驗試圖控制「偽發現率」，這是一個有點不同的概念。\n\\[p_j^{'}=m \\times p\\]\n因此，如果您使用邦弗隆尼校正，則在 \\(p_j^{'} < \\alpha\\) 的情況下拒絕零假設。這種校正背後的邏輯非常簡單。我們正在進行 m 個不同的檢驗，因此，如果我們安排使每個檢驗的第一型錯誤率至多為 \\(\\frac{\\alpha}{m}\\)，那麼這些檢驗的總第一型錯誤率不能大於 \\(\\alpha\\)。這很簡單，簡單到在原始論文中，作者寫道：\n\n在這裡給出的方法如此簡單，而且如此通用，我確信它肯定已經被使用過了。然而，我沒有找到它，所以只能得出一個結論：也許正是它的極簡單讓統計學家意識不到它在某些情況下是一個非常好的方法（Dunn (1961)，第52-53頁）。\n\n要在 jamovi 中使用邦弗隆尼校正，只需單擊「校正」選項中的「邦弗隆尼」復選框，您將在 ANOVA 結果表中看到另一列，顯示邦弗隆尼校正的調整後 p 值（Table 12.8）。如果我們將這三個 p 值與未校正的成對 t 檢驗的 p 值進行比較，很明顯 jamovi 所做的唯一事情就是將它們乘以 \\(3\\)。\n\n\n\n\n12.5.4 Holm校正\n雖然邦弗隆尼校正是最簡單的調整方法，但它通常不是最好的選擇。經常使用的另一種方法是霍爾姆校正（Holm correction）(Holm, 1979)。霍爾姆校正背後的思路是假設您正在按順序進行測試，從最小（原始）的 p 值開始，然後移動到最大的 p 值。對於第 j 大的 p 值，調整是以下兩者之一\n\\[p_j^{'}=j \\times p_j\\]\n（即最大的 p 值保持不變，第二大的 p 值翻倍，第三大的 p 值翻三倍，依此類推），或者\n\\[p_j^{'}=p_{j+1}^{'}\\]\n其中較大者。這可能聽起來有點困惑，所以讓我們慢慢解釋。霍爾姆校正的工作原理如下。首先，您按順序對所有 p 值進行排序，從最小到最大。對於最小的 p 值，您只需將其乘以 \\(m\\)，然後就完成了。然而，對於其他所有的 p 值，這是一個兩階段的過程。例如，當您移動到第二小的 p 值時，首先將其乘以 \\(m - 1\\)。如果這產生的數字大於您上次得到的調整後的 p 值，那麼保留它。但如果它比上一個小，那麼您將複製上一個 p 值。為了說明這是如何工作的，請考慮 Table 12.10，該表顯示了五個 p 值的霍爾姆校正計算。\n\n\n\n\n\nTable 12.10:  經過霍爾姆校正計算的p值 \n\nraw prank jp \\( \\times \\) jHolm p\n\n.0015.005.005\n\n.0054.020.020\n\n.0193.057.057\n\n.0222.044.057\n\n.1031.103.103\n\n\n\n\n\n希望這能讓事情變得清晰。\n雖然計算起來稍微困難一些，但霍爾姆校正具有一些非常好的特性。它比邦弗隆尼更具威力（即具有更低的 Type II 錯誤率），但是，儘管可能令人反直覺，它具有相同的 Type I 錯誤率。因此，在實踐中，沒有理由使用更簡單的邦弗隆尼校正，因為它總是被稍微複雜一點的霍爾姆校正所超越。正因為如此，霍爾姆校正應該是您的首選多重比較校正。Figure 12.4 還顯示了霍爾姆校正後的 p 值，如您所見，最大的 p 值（對應於 Anxifree 和安慰劑之間的比較）沒有改變。它的值為 .15，與我們最初在完全不做校正時得到的值完全相同。相比之下，最小的 p 值（Joyzepam 與安慰劑）已乘以三。\n\n\n\n12.5.5 事後檢定的報告格式\n最後，在執行事後分析以確定哪些組別之間的差異顯著之後，您可以這樣寫出結果：\n\n事後檢驗（使用霍爾姆校正來調整 p 值）表明，與 Anxifree（p = .001）和安慰劑（\\(（p = 9.0 \\times{10^{-5}}\\)）相比，Joyzepam 產生了顯著更大的心情變化。我們沒有發現 Anxifree 表現優於安慰劑的證據（\\(p = .15\\)）。\n\n或者，如果您不喜歡報告精確的 p 值，那麼分別將這些數字更改為 \\(p < .01\\)、\\(p < .001\\) 和 \\(p > .05\\)。無論哪種方式，關鍵是要表明您使用了霍爾姆的校正來調整 p 值。當然，我假設在撰寫的其他部分，您已經包括了相關的描述性統計資料（即組平均值和標準差），因為這些 p 值本身並不是很有信息量。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.6 單因子變異數分析的執行條件",
    "text": "12.6 單因子變異數分析的執行條件\n像任何統計檢驗一樣，變異數分析依賴於關於數據（特別是殘差）的一些假設。您需要了解三個關鍵假設：正態性、方差同質性和獨立性。\n[額外的技術細節 [^13-comparing-several-means-one-way-anova-12]]\n[^13-comparing-several-means-one-way-anova-12]：如果您記得回到[一個實例]，我希望您至少瀏覽了一遍，即使您沒有讀完整篇文章，我以這種方式描述了支撐ANOVA的統計模型：\\[H_0:Y_{ik}=\\mu + \\epsilon_{ik}\\] \\[H_1:Y_{ik}=\\mu_k + \\epsilon_{ik}\\]在這些等式中，\\(\\mu\\)指的是對所有組別都相同的單個總群體均值，µk是第k個組的群體均值。到目前為止，我們主要關心的是我們的數據是最好用單個總均值（零假設）來描述，還是用不同的特定組均值（替代假設）來描述。當然，這是有道理的，因為這實際上是重要的研究問題！然而，我們所有的檢驗過程都是在一個關於殘差 \\(\\epsilon_{ik}\\) 的具體假設下進行的，即：\\[\\epsilon_{ik} \\sim Normal(0,\\sigma^2)\\]如果沒有這部分，所有的數學都不能正常工作。或者，確切地說，您仍然可以進行所有計算，最終得到一個F統計量，但是您無法保證這個F統計量實際上衡量了您認為它衡量的內容，因此您可能基於F檢驗得出的任何結論都可能是錯誤的。\n那麼，我們如何檢查對殘差的假設是否準確呢？嗯，正如我上面所指出的，這個陳述中隱含了三個不同的主張，我們將分別考慮它們。\n\n方差同質性。注意到我們只有一個群體標準差的值（即，\\(\\sigma\\)），而不是讓每個組都有它自己的值（即，\\(\\sigma_k\\)）。這被稱為方差同質性（有時稱為等方差性）假設。ANOVA假定所有組的群體標準差相同。我們將在[檢查方差同質性假設]部分詳細論述這一點。\n正態性。假定殘差呈正態分布。正如我們在@sec-Checking-the-normality-of-a-sample中看到的，我們可以通過查看QQ圖（或運行Shapiro-Wilk檢驗）來評估這一點。我將在[檢查正態性假設]部分中更多地討論這個問題。\n獨立性。獨立性假設有點棘手。它基本上的意思是，了解一個殘差對於了解任何其他殘差都沒有幫助。所有的 \\(\\epsilon_{ik}\\) 值都被假定是在不考慮或與其他任何值無關的情況下生成的。對於這一點，沒有顯而易見或簡單的檢驗方法，但有些情況是明顯違反這一假設的。例如，如果您有一個重複測量設計，每個參與者在研究中出現在多個條件下，那麼獨立性就不成立了。在這種情況下，某些觀察之間存在特殊關係，即對應於同一個人的觀察！當這種情況發生時，您需要使用類似[重複測量單因子ANOVA]的方法。\n\n\n\n12.6.1 同質性檢核\n\n要進行方差的初步檢驗，就像乘坐划艇出海，看看海面條件是否足夠平靜，讓一艘大型遊輪離港！\n– 喬治·博克斯 (Box, 1953)\n\n俗話說，殺貓有很多方法，檢驗方差同質性假設也有很多方法（不過出於某種原因，沒有人把它變成一句俗話）。在文獻中，我見過的最常用的檢驗方法是Levene檢驗(Levene, 1960)，以及與之密切相關的Brown-Forsythe檢驗(Brown & Forsythe, 1974)。\n無論您是進行標準Levene檢驗還是Brown-Forsythe檢驗，檢驗統計量（有時表示為\\(F\\)，但也有時表示為\\(W\\)），都是按照計算常規ANOVA中的F-統計量的方式，只是使用\\(Z_{ik}\\)而不是\\(Y_{ik}\\)。有了這個思路，我們可以繼續看看如何在jamovi中運行檢驗。\n[額外的技術細節[^13-comparing-several-means-one-way-anova-13]]\n[^13-comparing-several-means-one-way-anova-13]：Levene檢驗非常簡單。假設我們有結果變量\\(Y_{ik}\\)。我們所要做的就是定義一個新變量，我將其稱為\\(Z_{ik}\\)，表示與組均值的絕對偏差：\\[Z_{ik}=Y_{ik}-\\bar{Y}_{k}\\]好吧，這對我們有什麼好處呢？那麼，讓我們花一點時間來思考一下\\(Z_{ik}\\)到底是什麼以及我們要檢驗什麼。\\(Z_{ik}\\)的值是度量第\\(i\\)次觀測在第\\(k\\)個組中與其組平均值的偏差程度。我們的零假設是所有組的方差都相同，即所有組平均值的總偏差相同！因此，Levene檢驗中的零假設是所有組的\\(Z\\)的母體平均值相同。嗯。那麼我們現在需要的是一個統計檢驗來檢驗所有組均值相同的零假設。我們在哪裡見過這個檢驗？哦對了，這就是ANOVA，所以Levene檢驗所做的就是對新變量\\(Z_{ik}\\)進行ANOVA。Brown-Forsythe檢驗呢？它有做什麼特別不同的事情嗎？不，與Levene檢驗唯一的不同是它以稍微不同的方式構建轉換變量Z，使用組中位數的偏差而不是組平均值的偏差。也就是說，對於Brown-Forsythe檢驗：\\[Z_{ik}=Y_{ik}-median_k(Y)\\]其中，\\(median_k(Y)\\)是第k組的中位數。\n\n\n\n12.6.2 jamovi的Levene檢定\n好的，那麼我們該如何進行Levene檢驗呢？其實很簡單 - 在ANOVA的”假設檢查”選項下，只需點擊”變異數同質性檢驗”複選框。如果我們查看@fig-fig12-5中的輸出，我們可以看到檢驗結果並無顯著差異（\\(F_{2,15} = 1.45, p = .266\\)），所以變異數同質性假設看起來沒有問題。然而，外表可能會讓人受騙！如果您的樣本量相當大，那麼即使變異數同質性假設沒有被違反到影響ANOVA的穩健性，Levene檢驗也可能顯示出顯著效應（即p < .05）。這正是George Box在上面引述中所指出的觀點。同樣地，如果您的樣本量相當小，那麼變異數同質性假設可能不被滿足，而Levene檢驗可能不顯著（即p > .05）。這意味著，在對假設是否被滿足進行任何統計檢驗的同時，您應該總是繪製每個分組/類別的均值周圍的標準差……只是為了看看它們是否看起來相當相似（即變異數同質性）或不相似。\n\n\n\n\n\n\nFigure 12.5: jamovi中單因素ANOVA的Levene檢驗輸出\n\n\n\n\n\n\n12.6.3 校正異質性的分析結果\n在我們的示例中，變異數同質性假設被證明是相當可靠的：Levene檢驗結果並無顯著差異（儘管我們還應該查看標準差的圖形），因此我們可能不需要擔心。然而，在現實生活中，我們並非總是如此幸運。當變異數同質性假設被違反時，我們該如何拯救我們的ANOVA呢？如果您回想一下我們對t檢驗的討論，我們之前遇到過這個問題。Student t檢驗假設等方差，所以解決方法是使用不需要等方差假設的Welch t檢驗。實際上，(Welch1951還展示了我們如何解決ANOVA的這個問題?)（Welch單因素檢驗）。它在jamovi中使用One-Way ANOVA分析實現。這是一種專為單因素ANOVA設計的分析方法，要在我們的示例中執行Welch單因素ANOVA，我們將按照之前的方式重新運行分析，但這次使用jamovi的ANOVA - One Way ANOVA分析命令，並選擇Welch檢驗的選項（參見@fig-fig12-6）。為了理解這裡發生了什麼，讓我們將這些數字與我們在[最初在jamovi中運行ANOVA]時得到的數字進行比較。為了省去您回顧的麻煩，上次我們得到的是：\\(F(2, 15) = 18.611, p = .00009\\)，這也顯示為@fig-fig12-6中One-Way ANOVA的Fisher檢驗。\n\n\n\n\n\n\nFigure 12.6: Welch檢驗作為jamovi中One Way ANOVA分析的一部分\n\n\n\n\n好的，最初我們的ANOVA結果是\\(F(2, 15) = 18.6\\)，而Welch單因素檢驗給出的是\\(F(2, 9.49) = 26.32\\)。換句話說，Welch檢驗將組內自由度從15降低到了9.49，而F值從18.6上升到了26.32。\n\n\n\n12.6.4 常態性檢核\n檢驗正態性假設相對簡單。我們在@sec-Checking-the-normality-of-a-sample中介紹了大部分你需要了解的內容。我們真正需要做的只是繪製一個QQ圖，並在可行的情況下，運行Shapiro-Wilk檢驗。QQ圖顯示在@fig-fig12-7，對我來說看起來相當正常。如果Shapiro-Wilk檢驗不顯著（即\\(p > .05\\)），那麼這表明正態性假設沒有被違反。然而，與Levene檢驗一樣，如果樣本量很大，那麼顯著的Shapiro-Wilk檢驗實際上可能是偽陽性，也就是說，正態性假設在實質上沒有對分析造成任何問題。同樣地，非常小的樣本量可能會產生偽陰性。這就是為什麼視覺檢查QQ圖很重要。\n\n\n\n\n\n\nFigure 12.7: jamovi中One Way ANOVA分析的QQ圖\n\n\n\n\n除了檢查QQ圖中是否有偏離正態性的情況外，我們的數據的Shapiro-Wilk檢驗確實顯示出非顯著效應，p = 0.6053（見@fig-fig12-6）。因此，這支持了QQ圖的評估；兩個檢查都沒有發現正態性被違反的跡象。\n\n\n\n12.6.5 排除非常態性的分析結果\n現在我們已經了解了如何檢查正態性，我們自然會問可以採取哪些措施來解決正態性的違反。在單因素ANOVA的背景下，最簡單的解決方案可能是轉向非參數檢驗（即不依賴於任何特定的分佈假設的檢驗）。在@sec-Comparing-two-means中，我們之前已經介紹過非參數檢驗。當你只有兩個組別時，Mann-Whitney或Wilcoxon檢驗可以提供你所需的非參數替代方法。當你有三個或更多組別時，你可以使用Kruskal-Wallis秩和檢驗(Kruskal & Wallis, 1952)。接下來我們將講解這個檢驗。\n\n\n\n12.6.6 Kruskal-Wallis檢定的邏輯\nKruskal-Wallis檢驗在某些方面與ANOVA驚人地相似。在ANOVA中，我們從\\(Y_{ik}\\)開始，對於第k個組中的第i個人，這是結果變量的值。對於Kruskal-Wallis檢驗，我們要做的是對所有的\\(Y_{ik}\\)值進行排序，並對排名數據進行分析。9\n\n\n\n12.6.7 更多分析細節\n上一節的描述說明了Kruskal-Wallis檢驗背後的邏輯。從概念上講，這是考慮測試如何工作的正確方法。[^13-comparing-several-means-one-way-anova-15]\n[^13-comparing-several-means-one-way-anova-15]：然而，從純粹的數學角度來看，這是不必要的複雜。我不會向您展示推導，但您可以使用一些代數技巧\\(^b\\)來顯示K的方程式可以是\\[K=\\frac{12}{N(N-1)}\\sum_k N_k \\bar{R}_k^2 -3(N+1)\\] 最後一個方程式有時給出了K的值。這比我在上一節中描述的版本要容易得多，但問題是對實際人類完全沒有意義。將K視為基於排名的ANOVA類比可能是最好的方式。但請記住，計算出來的檢驗統計量與我們最初用於ANOVA的統計量有很大不同。— \\(b\\)就是一些數學運算術語。\n但等等，還有更多！天啊，為什麼總是有更多呢？到目前為止，我講的故事實際上只在原始數據中沒有相同數值的情況下才成立。也就是說，如果沒有兩個觀測值具有完全相同的值。如果有相同的值，那麼我們必須引入一個校正因子來進行這些計算。在這一點上，我假設即使是最勤奮的讀者也已經不再關心（或者至少形成了繫結校正因子不需要他們立即關注的看法）。因此，我將非常快速地告訴您如何計算它，並省略為什麼以這種方式進行的繁瑣細節。假設我們為原始數據構建一個頻率表，讓fj表示具有第j個唯一值的觀測值的數量。這聽起來可能有點抽象，因此我們將從clinicaltrials.csv數據集中的mood.gain頻率表（Table 12.11）給出一個具體的例子。\n\n\n\n\n\n\nTable 12.11:  數據中clinicaltrials.csv心情增益的次數表 \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\n觀察此表，請注意頻率表中的第三個條目值為2。由於這對應於心情增益為0.3，因此此表告訴我們有兩個人的心情增加了0.3。[^13-comparing-several-means-one-way-anova-16]\n[^13-comparing-several-means-one-way-anova-16]：更重要的是，在我上面介紹的數學表示法中，這告訴我們\\(f_3 = 2\\)。耶。那麼，現在我們知道了這一點，繫結校正因子（TCF）是：\\[TCF=1-\\frac{\\sum_j f_j^3 - f_j}{N^3 - N}\\]通過將K值除以這個數量，可以得到Kruskal-Wallis統計量的繫結校正值。這是jamovi計算的繫結校正版本。\n因此，jamovi使用繫結校正因子來計算繫結校正的Kruskall-Wallis統計量。最後，我們實際上已經完成了Kruskal-Wallis檢驗的理論。我確信你們都對我治愈了你們在意識到你們不知道如何計算Kruskal-Wallis檢驗的繫結校正因子時自然產生的存在焦慮感到非常寬慰。對吧？\n\n\n\n\n12.6.8 使用jamovi完成Kruskal-Wallis檢定\n儘管我們在努力理解Kruskal Wallis檢驗實際上做了什麼方面經歷了恐懼，但事實證明，進行該檢驗相當無痛，因為jamovi在ANOVA分析集中有一個名為「非參數」-「單因子ANOVA（Kruskall-Wallis）」的分析。大多數時候，你將擁有像clinicaltrial.csv這樣的數據集，其中包含你的結果變量mood.gain和一個分組變量drug。如果是這樣，你可以直接在jamovi中運行分析。這給我們提供了一個Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\)，如@fig-fig12-8所示。\n\n\n\n\n\n\nFigure 12.8: jamovi中的Kruskall-Wallis單因子非參數ANOVA"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.7 單因子重覆量數變異數分析",
    "text": "12.7 單因子重覆量數變異數分析\n單因子重複測量ANOVA檢驗是一種統計方法，用於檢驗三個或更多組之間的顯著差異，其中每個組都使用相同的參與者（或者每個參與者與其他實驗組的參與者密切匹配）。因此，每個實驗組中應該始終具有相等數量的分數（數據點）。這種類型的設計和分析也可以稱為「相關ANOVA」或「內部主題ANOVA」。\n重複測量ANOVA的邏輯與獨立ANOVA（有時稱為「間題」ANOVA）非常相似。您可能還記得，我們之前顯示在一個間題ANOVA總變異性可以分為組間變異性（\\(SS_b\\)）和組內變異性（\\(SS_w\\)），在將每個變異性除以相應的自由度後得到MSb和MSw（見表13.1），F比值計算為：\n\\[F=\\frac{MS_b}{MS_w}\\]\n在重複測量ANOVA中，F比值的計算方式類似，但是在獨立ANOVA中，組內變異性（\\(SS_w\\)）被用作\\(MS_w\\)的分母，而在重複測量ANOVA中，\\(SS_w\\)被劃分為兩部分。由於我們在每個組中都使用相同的受試者，因此可以從組內變異性中移除受試者間個別差異（稱為SSsubjects）的變異性。我們不會深入討論這是如何實現的，但本質上，每個受試者都成為名為受試者的因子的一個水平。然後以與任何間題因子相同的方式計算此內部受試者因子中的變異性。然後我們可以將SSsubjects從\\(SS_w\\)中減去，以提供一個較小的SSerror項：\n\\[\\text{獨立ANOVA: } SS_{error} = SS_w\\] \\[\\text{重複測量ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] 這個\\(SS_{error}\\)項的變化通常會導致統計檢驗更加強大，但這確實取決於\\(SS_{error}\\)的減少是否超過了誤差項自由度的減少（因為自由度從\\((n - k)\\)10變為\\((n - 1)(k - 1)\\)（請記住，獨立ANOVA設計中的受試者更多）。\n\n\n\n12.7.1 jamovi的重覆量數變異數分析\n首先，我們需要一些數據。(Geschwind1972表示?)，患者在中風後語言缺陷的確切性質可以用來診斷已受損的大腦特定區域。一位研究人員關心的是確定六位患有Broca失語症（中風後常見的語言缺陷）的患者所經歷的具體交流困難（Table 12.12）。\n\n\n\n\n\nTable 12.12:  中風患者單詞識別作業分數 \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\n患者需要完成三個單詞識別任務。在第一個（言語生成）任務中，患者需要重複研究者大聲朗讀的單詞。在第二個（概念性）任務中，旨在測試單詞理解能力，患者需要將一系列圖片與其正確名稱匹配。在第三個（語法）任務中，旨在測試正確單詞順序的知識，要求患者對語法不正確的句子進行重新排序。每位患者都完成了所有三個任務。患者嘗試任務的順序在參與者之間進行了平衡。每個任務包括一系列10次嘗試。每位患者成功完成的嘗試次數如 Table 12.11 所示。將這些數據輸入jamovi以進行分析（或者使用捷徑加載broca.csv文件）。\n要在jamovi中執行一個單因素相關ANOVA，打開一個單因素重複測量ANOVA對話框，如 Figure 12.9 中所示，通過ANOVA - Repeated Measures ANOVA進行。\n\n\n\n\n\n\nFigure 12.9: jamovi中的重複測量ANOVA對話框\n\n\n\n\n然後：\n\n輸入一個重複測量因子名稱。這應該是您選擇的標籤，用於描述所有參與者重複的條件。例如，要描述所有參與者完成的語音、概念和語法任務，一個合適的標籤是“任務”。請注意，這個新的因子名稱代表了分析中的自變量。\n在重複測量因子文本框中添加第三個級別，因為有三個級別代表三個任務：語音、概念和語法。相應地更改級別的標籤。\n然後將每個級別變量移動到重複測量單元文本框中。\n最後，在“假設檢查”選項下，選中“球形性檢查”文本框。\n\njamovi輸出一個單因素重複測量ANOVA，如@fig-fig12-10至?fig-fig12-13所示。我們應該首先查看的是Mauchly球形性檢驗，該檢驗測試各條件之間的差異方差是否相等（意味著研究條件之間的差異得分的分佈大致相同）。在@fig-fig12-10中，Mauchly檢驗的顯著性水平為\\(p = .720\\)。如果Mauchly檢驗的結果不顯著（即p > .05，正如此分析中的情況），那麼我們有理由得出差異的方差並無顯著差異（即它們大致相等，可以假定球形性。）。\n\n\n\n\n\n\n\nFigure 12.10: 單因子重複測量ANOVA輸出 - Mauchly球形性檢驗\n\n\n\n\n如果另一方面，Mauchly檢驗顯著（p < .05），那麼我們將得出差異方差之間存在顯著差異，並且未滿足球形性要求。在這種情況下，我們應該對單因素相關ANOVA分析中獲得的F值進行修正：\n\n如果”球形性檢驗”表中的Greenhouse-Geisser值> .75，那麼您應該使用Huynh-Feldt修正\n但如果Greenhouse-Geisser值< .75，那麼您應該使用Greenhouse-Geisser修正。\n\n這兩個修正過的F值都可以在“假設檢查”選項下的球形性修正復選框中指定，修正過的F值將顯示在結果表中，如@fig-fig12-11所示。\n\n\n\n\n\n\n\nFigure 12.11: 單因素重複測量ANOVA輸出 - 內部受試者效應檢驗\n\n\n\n\n在我們的分析中，我們發現Mauchly的球形性檢驗的顯著性為p = .720（即p > 0.05）。因此，這意味著我們可以假設已滿足球形性要求，因此無需對F值進行修正。因此，我們可以使用’無’球形性修正輸出值用於重複測量”任務”：\\(F = 6.93\\)，\\(df = 2\\)，\\(p = .013\\)，我們可以得出結論，語言任務中成功完成的測試次數確實會根據任務是語音、理解還是語法為基礎而顯著不同（\\(F(2, 10) = 6.93\\)，\\(p = .013\\)）。\n在jamovi中，與獨立ANOVA相同，也可以為重複測量ANOVA指定事後檢驗。結果顯示在@fig-fig12-12。這些表明語音和語法之間存在顯著差異，但其他級別之間沒有差異。\n\n\n\n\n\n\nFigure 12.12: 重複測量ANOVA中jamovi的事後檢驗\n\n\n\n\n描述性統計（邊際均值）可以用於幫助解釋結果，在jamovi輸出中生成，如@fig-fig12-13。通過比較參與者成功完成試驗的平均次數，可以看出布洛卡失語症患者在語音產生（平均= 7.17）和語言理解（平均= 6.17）任務上表現相對較好。然而，他們在語法任務上的表現明顯較差（平均= 4.33），事後檢驗中語音和語法任務表現之間存在顯著差異。\n\n\n\n\n\n\nFigure 12.13: 單因子重複測量ANOVA輸出-描述性統計"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.8 Friedman無母數重覆量數變異數分析",
    "text": "12.8 Friedman無母數重覆量數變異數分析\nFriedman檢驗是一元重複測量ANOVA的非參數版本，可以在測試三個或更多組之間的差異時使用，其中每個組中的參與者相同，或者每個參與者與其他條件中的參與者密切匹配。如果因變量是序數，或者未滿足正態性假設，則可以使用Friedman檢驗。\n與Kruskall-Wallis檢驗一樣，基本數學知識很複雜，這裡不會介紹。對於本書的目的，僅需注意jamovi計算了Friedman檢驗的綁定修正版本，在@fig-fig12-14中有一個我們已經查看過的布洛卡失語症數據的示例。\n\n\n\n\n\n\nFigure 12.14: jamovi中的“重複測量ANOVA（非參數）”對話框和結果\n\n\n\n\n在jamovi中運行Friedman檢驗非常簡單。只需選擇分析 - ANOVA - 重複測量ANOVA（非參數），如@fig-fig12-14所示。然後將要比較的重複測量變量的名稱（語言、概念、語法）突顯並轉移到“測量：”文本框中。要為三個重複測量變量生成描述性統計（平均值和中位數），請單擊描述性按鈕。\njamovi結果顯示描述性統計、卡方值、自由度和p值（Figure 12.14）。由於p值小於通常用於確定顯著性的水平（p < .05），我們可以得出結論，布洛卡失語症患者在語言生產（中位數= 7.5）和語言理解（中位數= 6.5）任務上表現相當好。然而，他們在語法任務上的表現明顯較差（中位數= 4.5），在事後檢驗中語言和語法任務表現之間存在顯著差異。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.9 變異數分析與t檢定的關聯",
    "text": "12.9 變異數分析與t檢定的關聯\n在結束之前，我想指出的最後一點是，許多人對此感到驚訝，但了解它是很有價值的。具有兩個組別的ANOVA與學生t檢驗相同。不，真的。它們不僅相似，而且在每個有意義的方面實際上都是等效的。我不會試圖證明這總是成立，但我將給你展示一個具體的演示。假設，我們不對mood.gain ~ drug模型進行ANOVA，而是使用療法作為預測指標。如果我們運行此ANOVA，我們將得到一個F統計量 \\(F(1,16) = 1.71\\)，和一個 p值 = \\(0.21\\)。由於我們只有兩組，實際上我不需要求助於ANOVA，我可以選擇運行一個學生t檢驗。那麼，讓我們看看這樣做會發生什麼：我得到一個t統計量 \\(t(16) = -1.3068\\) 和一個 \\(p值 = 0.21\\)。好奇的是，p值是相同的。再一次，我們得到一個值 \\(p = .21\\)。但是，檢驗統計量呢？運行t檢驗而不是ANOVA，我們得到了一個略有不同的答案，即 \\(t(16) = -1.3068\\)。然而，這裡有一個相當直接的關係。如果將t統計量平方，我們就會得到之前的F統計量：\\(-1.3068^{2} = 1.7077\\)"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.10 本章小結",
    "text": "12.10 本章小結\n這一章份量不少，但是有一些細節我並未提到11。最明顯的是在此並未討論處理不只一個分組變項的資料，我們在下一章 Chapter 13 將學習其中一部分。本章的學習重點有：\n\n理解變異數分析的運作原理 以及使用jamovi完成變異數分析\n學習如何計算變異數分析的效果量\n多重比較與事後檢定\n單因子變異數分析的執行條件\n同質性檢核 以及 校正異質性的分析結果\n常態性檢核以及排除非常態性的分析結果\n單因子重覆量數變異數分析 以及其無母數版本單因子重覆量數變異數分析\n\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et al (Ed.), Contributions to probability and statistics: Essays in honor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance: Fixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584."
  },
  {
    "objectID": "13-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "href": "13-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.1 平衡且無交互作用的因子設計分析",
    "text": "13.1 平衡且無交互作用的因子設計分析\n當我們在 Chapter 12 中討論變異數分析時，我們假設了一個相對簡單的實驗設計。每個人都在幾個小組中，我們想知道這些小組在某些結果變量上的平均分數是否有所不同。在本節中，我將討論一個更廣泛的實驗設計類別，稱為因子設計，其中我們有多個分組變量。在上面給出了這種設計可能產生的一個例子。另一個例子出現在 Chapter 12 中，我們在其中研究了不同藥物對每個人的情緒增益的影響。在那一章中，我們確實發現了藥物的顯著影響，但在章節的最後，我們還進行了一個分析，以查看治療是否有影響。我們沒有找到，但是在試圖預測相同結果的兩個單獨分析中有點令人擔憂。也許治療對情緒增益確實有影響，但我們找不到它，因為它被藥物的影響“隱藏”了？換句話說，我們將要進行一個包括藥物和治療作為預測因子的單一分析。對於這種分析，每個人都按照他們給定的藥物（具有3個水平的因子）和接受的治療（具有2個水平的因子）進行交叉分類。我們將此稱為\\(3 \\times 2\\)因子設計。\n如果我們用jamovi（見 Section 6.1 ）中的“頻率” - “應急表”分析交叉制表藥物和治療，我們將獲得在 Figure 13.1 中顯示的表格。\n\n\n\n\n\n\n\nFigure 13.1: jamovi藥物與治療的應急表\n\n\n\n\n如您所見，我們不僅有與兩個因子的所有可能組合相對應的參與者，表明我們的設計是完全交叉，而且事實上每個組中都有相等數量的人。換句話說，我們擁有一個平衡設計。在本節中，我將談論如何分析來自平衡設計的數據，因為這是最簡單的情況。對於不平衡設計的情況相當繁瑣，所以我們暫時將其擱置。\n\n\n13.1.1 多因子設計是因應什麼樣的假設？\n就像單因子變異數分析一樣，因子變異數分析是一個用於測試關於母體均值的某些類型假設的工具。因此，一個明智的開始方式是明確我們的假設實際上是什麼。然而，在我們甚至到達這一點之前，有一個簡單清晰的表示法來描述母體均值是非常有用的。由於觀察是根據兩個不同因子進行交叉分類的事實，可能有很多不同的均值會引起我們的興趣。為了理解這一點，讓我們首先考慮在這種設計中可以計算出所有不同樣本均值。首先，很明顯，我們可能對此類組均值感興趣（ Table 13.1 ）。\n\n\n\n\n\nTable 13.1:  藥物和治療組的組均值在* clinicaltrial.csv *數據中 \n\ndrugtherapymood.gain\n\nplacebono.therapy0.30\n\nanxifreeno.therapy0.40\n\njoyzepamno.therapy1.47\n\nplaceboCBT0.60\n\nanxifreeCBT1.03\n\njoyzepamCBT1.50\n\n\n\n\n\n接下來，下表（ Table 13.2 ）顯示了兩個因子所有可能組合的組均值列表（例如，接受安慰劑且未接受治療的人、接受安慰劑並接受CBT的人等）。將所有這些數字，以及邊際和總體均值，整合到一個單一的表格中是非常有幫助的，這個表格看起來是這樣的：\n\n\n\n\n\n\nTable 13.2:  藥物和治療組的組和總體均值在clintrial.csv數據中 \n\nno therapyCBTtotal\n\nplacebo0.300.600.45\n\nanxifree0.401.030.72\n\njoyzepam1.471.501.48\n\ntotal0.721.040.88\n\n\n\n\n\n現在，這些不同的均值當然是樣本統計量。它是一個與我們在研究過程中所做的具體觀察相關的數量。我們想要對應的母體參數進行推斷。也就是說，真實的均值是在某個更廣泛的母體內存在的。這些母體均值也可以整理成一個類似的表格，但是我們需要一些數學符號來表示（Table 13.3）。像往常一樣，我將使用符號\\(\\mu\\)來表示母體均值。然而，由於有很多不同的均值，我需要使用下標來區分它們。\n這裡是符號如何運作的。我們的表格是根據兩個因子定義的。每行對應於因子A（在本例中為藥物）的不同水平，每列對應於因子B（在本例中為治療）的不同水平。如果我們讓R表示表格中的行數，並讓\\(C\\)表示列數，我們可以將其稱為\\(R \\times C\\)因子變異數分析。在這種情況下\\(R = 3\\)和\\(C = 2\\)。我們將使用小寫字母來表示特定的行和列，因此\\(\\mu_{rc}\\)表示與因子\\(A\\)的第\\(r\\)級（即第\\(r\\)行）和因子B的第\\(c\\)級（第c列）相關的母體均值。1 所以現在母體的均值是寫成@tbl-tab13-1的形式：\n\n\n\n\n\n\nTable 13.3:  因子表中母體均值的符號表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\n\ntotal\n\n\n\n\n\n好的，那剩下的項目呢？例如，我們應該如何描述在這樣一個實驗中可能被給予Joyzepam的整個（假設的）人群的平均情緒提升，而不管他們是否接受了CBT治療？我們使用“點”符號來表示這一點。在Joyzepam的例子中，注意到我們正在討論表中第三行相關的均值。也就是說，我們將兩個單元格的均值（即\\(\\mu_{31}\\)和\\(\\mu_{32}\\)）求平均。這個求平均的結果被稱為邊際均值，並在這種情況下表示為\\(\\mu_3.\\)。CBT的邊際均值對應於表中第二列相關的母體均值，因此我們使用表示法，因為它是通過平均（邊際化2）兩者而得到的均值。因此，我們的整個母體均值表格可以寫成@tbl-tab13-4。\n\n\n\n\n\n\nTable 13.4:  因子表中母體和總體均值的符號表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\\( \\mu_{1.} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\\( \\mu_{2.} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\\( \\mu_{3.} \\)\n\ntotal\\( \\mu_{.1} \\)\\( \\mu_{.2} \\)\\( \\mu_{..} \\)\n\n\n\n\n\n現在我們有了這個表示法，很容易就可以形成和表達一些假設。假設目標是找出兩件事。首先，藥物的選擇是否對情緒有影響？其次，CBT 是否對情緒有影響？當然，這些不是我們可以制定的唯一假設，並且我們將在[因子 ANOVA 2：平衡設計，允許交互作用]一節中看到一個不同類型假設的非常重要示例，但這兩個假設是最簡單的檢驗，所以我們從這裡開始。考慮第一個檢驗。如果藥物沒有影響，那麼我們應該期望所有行均值相同，對吧？所以那就是我們的虛無假設。另一方面，如果藥物確實有關，那麼我們應該期望這些行均值不同。形式上，我們將虛無假設和替代假設表示為邊際均值的相等性：\n\\[\\text{虛無假設, } H_0 \\text{: 行均值相同，即 } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{替代假設, } H_1 \\text{: 至少有一個行均值不同}\\]\n值得注意的是，這些與我們在 Chapter 12 中對這些數據進行單因素 ANOVA 時形成的統計假設完全相同。當時我使用符號 \\(\\mu_{P}\\) 來表示安慰劑組的平均情緒增益，\\(\\mu_{A}\\) 和 \\(\\mu_{J}\\) 分別對應兩種藥物的組均值，並且虛無假設是 \\(\\mu_{P} = \\mu_{A} = \\mu_{J}\\)。所以我們實際上在談論相同的假設，只不過由於存在多個分組變量，更複雜的 ANOVA 需要更仔細的表示法，因此我們現在將此假設表示為 \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\)。然而，正如我們將很快看到的那樣，儘管假設相同，但由於我們現在承認了第二個分組變量的存在，對該假設的檢驗存在微妙的不同。\n談到其他分組變量，你可能不會感到驚訝地發現，我們的第二個假設檢驗也以相同的方式制定。然而，由於我們談論的是心理治療而不是藥物，我們的虛無假設現在對應於列均值的相等：\n\\[\\text{虛無假設, } H_0 \\text{: 列均值相同，即， } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{替代假設, } H_1 \\text{: 列均值不同，即， } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n\n13.1.2 使用jamovi完成多因子變異數分析\n我在上一節中描述的虛無假設和替代假設應該讓你感到非常熟悉。它們基本上與我們在 Chapter 12 中執行的更簡單的單因素ANOVA檢驗相同。所以你可能期望因子ANOVA中使用的假設檢驗本質上與 Chapter 12 中的F檢驗相同。您期望看到對平方和（SS）、平均平方（MS）、自由度（df）以及最終可以將之轉換為p值的F統計量的引用，對吧？好吧，你絕對是對的。 這麼多，以至於我要改變我的常規方法。在本書中，我通常採用先描述特定分析的基礎邏輯（在某程度上還有數學），然後再介紹jamovi中的分析的方法。這次我要相反地做，先告訴你如何在jamovi中執行它。這樣做的原因是我想強調 Chapter 12 中討論的簡單的單因素ANOVA工具，以及我們將在本章中使用的更複雜的方法之間的相似之處。\n如果您試圖分析的數據對應於平衡的因子設計，那麼執行方差分析就很容易。要了解它有多容易，讓我們先重現 Chapter 12 中的原始分析。以防你忘了，對於那個分析，我們只使用一個因素（即藥物）來預測我們的結果變量（即mood.gain），並且得到了 Figure 13.2 中顯示的結果。\n\n\n\n\n\n\nFigure 13.2: 藥物對 mood.gain 的單因子變異分析\n\n\n\n\n現在，假設我也好奇心理治療是否與mood.gain有關。根據我們在 Chapter 10 中對多元回歸的討論，你可能不會感到意外，我們所要做的就是在分析中將治療作為第二個”固定因素”，如 Figure 13.3 所示。 \n\n\n\n\n\nFigure 13.3: 藥物和心理治療對 mood.gain 的雙因子變異數分析\n\n\n\n\n這個輸出也很容易閱讀。表格的第一行報告了與藥物因素相關的組間平方和（SS）值，以及相應的組間 df 值。它還計算了平均平方（MS）、F統計量和p值。還有一行對應於心理治療因素和一行對應於殘差（即組內變異）。\n所有的單個量都非常熟悉，這些不同量之間的關係也保持不變，就像我們在原始單因素ANOVA中看到的一樣。注意，平均平方值是通過將\\(SS\\)除以相應的\\(df\\)來計算的。也就是說，無論我們談論的是藥物、治療還是殘差，都還是\n\\[MS=\\frac{SS}{df}\\]\n為了看到這一點，讓我們不要擔心平方和值是如何計算的。相反，讓我們相信 jamovi 已經正確計算了 \\(SS\\) 值，並嘗試驗證所有其他數字是否合理。首先，注意對於藥物因素，我們將 \\(3.45\\) 除以 \\(2\\)，得到平均平方值為 \\(1.73\\)。對於心理治療因素，只有1個自由度，所以我們的計算更簡單：將 \\(0.47\\)（\\(SS\\) 值）除以1，得到答案為 \\(0.47\\)（\\(MS\\) 值）。\n轉向 F 統計量和 p 值，注意我們有兩個；一個對應藥物因素，另一個對應心理治療因素。無論我們談論的是哪一個，F 統計量都是將與因素相關的平均平方值除以與殘差相關的平均平方值。如果我們用 “A” 作為簡寫符號來指代第一個因素（因素 A；在本例中是藥物），用 “R” 作為簡寫符號來指代殘差，那麼與因素 A 相關的 F 統計量表示為 \\(F_A\\)，並按如下方式計算：\n\\[F_A=\\frac{MS_A}{MS_R}\\]\n因素 B（即心理治療）也有等效公式。注意，這裡使用 “R” 來指代殘差有點尷尬，因為我們也用字母 R 來指代表格中的行數，但我只會在 SSR 和 MSR 的上下文中用 “R” 表示殘差，所以希望這不會令人困惑。無論如何，將這個公式應用於藥物因素，我們將平均平方值 1.73 除以殘差平均平方值 \\(0.07\\)，得到 F 統計量為 26.15。對於心理治療變量的相應計算將是將 \\(0.47\\) 除以 \\(0.07\\)，得到 \\(7.08\\) 作為 F 統計量。當然，這些值與 jamovi 在上面的 ANOVA 表中報告的值相同。\n同樣在 ANOVA 表中的是 p 值的計算。再次，這裡沒有什麼新鮮事物。對於我們的兩個因素，我們試圖做的是測試關於因素與結果變量之間沒有關係的虛無假設（稍後我將更加明確）。為此，我們（顯然）遵循了類似於單因素 ANOVA 的策略，為每個假設計算了一個 F 統計量。要將這些轉換為 p 值，我們只需要注意，在虛無假設下（即所謂因素無關）的 F 統計量的抽樣分佈是一個 F 分佈。還要注意，兩個自由度值分別對應於因素和殘差。對於藥物因素，我們談論的是具有 2 和 14 自由度的 F 分佈（稍後我將更詳細地討論自由度）。相反，對於心理治療因素，抽樣分佈是具有 1 和 14 自由度的 F 分佈。\n在這一點上，我希望您能看到，這個更複雜的因子分析的 ANOVA 表應該以與較簡單的單因素分析的 ANOVA 表相同的方式進行閱讀。簡而言之，它告訴我們，我們的 \\(3 \\times 2\\) 設計的因子 ANOVA 發現藥物的顯著效應（\\(F_{2,14} = 26.15, p < .001\\)）以及心理治療的顯著效應（\\(F_{1,14} = 7.08, p = .02\\)）。或者，使用更技術正確的術語，我們會說藥物和心理治療有兩個主要效果。此刻，將這些稱為“主要”效果似乎有點多餘，但實際上是有道理的。稍後，我們將討論兩個因素之間可能存在的“交互作用”，因此我們通常區分主要效果和交互作用。\n\n\n\n\n13.1.3 計算多因子變異數分析的平方差總和\n在上一節中，我有兩個目標。首先，向您顯示執行因子 ANOVA 所需的 jamovi 方法與我們用於單因素 ANOVA 的方法幾乎相同。唯一的區別是添加了第二個因素。其次，我想向您展示在這種情況下 ANOVA 表的樣子，以便您從一開始就可以看到因子 ANOVA 背後的基本邏輯和結構與支撐單因素 ANOVA 的那些相同。試著抱著這種感覺。這是真實的，因為因子 ANOVA 的建立方式與更簡單的單因素 ANOVA 模型大致相同。只是一旦您開始挖掘細節，這種熟悉的感覺就會消失。傳統上，這種令人欣慰的感覺會被向統計教科書作者傾訴怒氣的衝動所替代。\n好的，讓我們先看看其中一些細節。上一節中的解釋表明了主效應（本例中為藥物和心理治療）的假設檢驗是 F 檢驗，但它沒有顯示如何計算平方和（SS）值。也沒有明確告訴您如何計算自由度（df 值），儘管相比之下這是一個簡單的事情。現在讓我們假設我們只有兩個預測變量，因子 A 和因子 B。如果我們用 Y 來表示結果變量，那麼我們可以用 Yrci 來表示與 rc 組的第 i 位成員相關的結果（即因子 A 的第 r 級/行和因子 B 的第 c 級/列）。因此，如果我們用 \\(\\bar{Y}\\) 表示樣本均值，我們可以使用與之前相同的表示法來表示組均值、邊際均值和總均值。也就是說，\\(\\bar{Y}_{rc}\\) 是與因子 A 的第 r 級和因子 B 的第 c 級相關的樣本均值：\\(\\bar{Y}_{r.}\\) 將是因子 A 的第 r 級的邊際均值，\\(\\bar{Y}_{.c}\\) 將是因子 B 的第 c 級的邊際均值，\\(\\bar{Y}_{..}\\) 是總體均值。換句話說，我們的樣本均值可以按照與母體均值相同的表格進行組織。對於我們的臨床試驗數據，該表格如@tbl-tab13-5所示。\n\n\n\n\n\n\nTable 13.5:  臨床試驗數據的樣本均值表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\bar{Y}_{11} \\)\\( \\bar{Y}_{12} \\)\\( \\bar{Y}_{1.} \\)\n\nanxifree\\( \\bar{Y}_{21} \\)\\( \\bar{Y}_{22} \\)\\( \\bar{Y}_{2.} \\)\n\njoyzepam\\( \\bar{Y}_{31} \\)\\( \\bar{Y}_{32} \\)\\( \\bar{Y}_{3.} \\)\n\ntotal\\( \\bar{Y}_{.1} \\)\\( \\bar{Y}_{.2} \\)\\( \\bar{Y}_{..} \\)\n\n\n\n\n\n如果我們查看之前顯示的樣本均值，我們有 \\(\\bar{Y}_{11} = 0.30\\)，\\(\\bar{Y}_{12} = 0.60\\) 等。在我們的臨床試驗示例中，藥物因子有 3 個水平，治療因子有 2 個水平，因此我們試圖執行的是一個 \\(3 \\times 2\\) 因子方差分析。然而，我們將更一般地說，因子 A（行因子）有 R 個水平，因子 B（列因子）有 C 個水平，因此我們在這裡執行的是一個 \\(R \\times C\\) 因子方差分析。\n[額外的技術細節3]\n\n\n\n\n13.1.4 計算自由度的規則\n自由度的計算方式與單因素變異數分析非常相似。對於任何給定因子，自由度等於級別數減 1（即，行變量因子 A 的 \\(R - 1\\)，列變量因子 B 的 \\(C - 1\\)）。因此，對於藥物因子，我們得到 \\(df = 2\\)，對於療法因子，我們得到 \\(df = 1\\)。稍後，在我們討論將 ANOVA 解釋為回歸模型時（參見 Section 13.6），我將更清楚地說明我們如何得出此數字。但就目前而言，我們可以使用自由度的簡單定義，即自由度等於觀察到的數量數目減去約束數目。因此，對於藥物因子，我們觀察到 3 個單獨的組平均值，但這些受到 1 個總平均值的約束，因此自由度為 2。對於殘差，邏輯相似但不完全相同。我們實驗中的總觀察次數是 18。約束對應於 1 個總平均值，藥物因子引入的 2 個額外組平均值，以及療法因子引入的 1 個額外組平均值，因此我們的自由度為 14。作為公式，這是 \\(N - 1 - (R - 1) - (C - 1)\\)，簡化後是 \\(N - R - C + 1\\)。\n\n\n\n13.1.5 多因子與單因子變異數分析\n既然我們已經了解了因子變異數分析（factorial ANOVA）的運作方式，那麼花一點時間將其與單因素分析的結果進行比較是值得的，因為這將讓我們真正理解為什麼進行因子變異數分析是個好主意。在 Chapter 12 中，我進行了一個單因素變異數分析，以查看藥物之間是否存在差異，並進行了第二個單因素變異數分析，以查看療法之間是否存在差異。正如我們在 Section 13.1.1 節中看到的，單因素變異數分析所檢驗的零假設和對立假設實際上與因子變異數分析所檢驗的假設相同。更仔細地查看變異數分析表，我們可以看到，在兩種不同的分析中，與因子相關的平方和是相同的（藥物為 3.45，療法為 0.92），自由度也是相同的（藥物為 2，療法為 1）。但它們的答案並不相同！最值得注意的是，當我們在 Section 12.9 中對療法進行單因素變異數分析時，我們沒有發現顯著效應（p 值為 .21）。然而，當我們在兩因素變異數分析的背景下查看療法的主效應時，我們確實得到了顯著效應（p = .019）。這兩種分析顯然不同。\n為什麼會發生這種情況？答案在於理解殘差是如何計算的。回想一下 F 檢驗背後的整個概念是將可以歸因於特定因子的變異性與無法解釋的變異性（殘差）進行比較。如果您對療法進行單因素變異數分析，因此忽略了藥物的影響，那麼變異數分析將把所有藥物引起的變異性放入殘差中！這會使數據看起來比實際情況更加嘈雜，而在兩因素變異數分析中被正確認為顯著的療法效果現在變得不顯著。如果我們在試圖評估其他事物（例如，療法）的貢獻時忽略了實際上有意義的事物（例如，藥物），那麼我們的分析將受到扭曲。當然，如果我們記錄了墻壁的顏色，並且在三因素變異數分析中發現這是一個無關緊要的因素，那麼完全可以忽略它，僅報告不包括這個無關因素的更簡單的兩因素變異數分析。您不應該放棄那些確實有所作為的變數！\n\n總之，比較因子變異數分析與單因素變異數分析的結果可以使我們更好地理解為什麼進行因子變異數分析是一個好主意。當我們忽略了真正重要的變數時，分析將受到扭曲。因此，為了確保我們的分析更加準確，我們應該在分析中包括所有具有重要影響的變數。\n\n\n\n\n\n13.1.6 解讀多因子變異數分析的結果\n迄今為止我們討論的變異數分析模型涵蓋了我們可能在數據中觀察到的各種不同模式。例如，在兩因素變異數分析設計中有四種可能性：（a）只有因素A有意義，（b）只有因素B有意義，（c）A和B都有意義，（d）A和B都無意義。 ?fig-fig13-4中繪製了這四種可能性的示例。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "href": "13-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.2 平衡且有交互作用的因子設計分析",
    "text": "13.2 平衡且有交互作用的因子設計分析\n?fig-fig13-4中顯示的四種數據模式都非常現實。有很多數據集正好產生這些模式。然而，它們並非全部故事，到目前為止我們一直在談論的變異數分析模型並不足以充分解釋一個組均值表格。為什麼呢？嗯，到目前為止，我們可以討論藥物如何影響心情，以及治療如何影響心情，但無法談論兩者之間可能存在的交互作用。只有當因素\\(A\\)的效果因為我們討論的因素\\(B\\)的水平而有差異時，我們才說\\(A\\)和\\(B\\)之間存在交互作用。?fig-fig13-5中顯示了在$2 $ ANOVA環境下的幾個交互作用效果實例。舉一個更具體的例子，假設Anxifree和Joyzepam的運作受到完全不同的生理機制控制。這導致了一個結果，即儘管在接受治療的情況下，Joyzepam對心情的影響基本相同，但與CBT一起使用時，Anxifree實際上更有效。我們在上一節中開發的ANOVA無法捕捉到這個想法。要判斷此處是否確實存在交互作用，最好繪製各個組的均值。在jamovi中，這是通過ANOVA的“估計邊際均值”選項完成的——只需將藥物和治療移至“條款1”下的“邊際均值”框中。這應該看起來像@fig-fig13-6。我們的主要關注點與這兩條線不平行的事實有關。當藥物為Joyzepam（右側）時，CBT的效果（實線與虛線之間的差異）似乎接近零，甚至比使用安慰劑時（左側）的CBT效果還要小。然而，當給予Anxifree時，CBT的效果大於安慰劑（中間）。這種效果是真實的，還是僅僅由於機會引起的隨機變化？我們最初的ANOVA無法回答這個問題，因為我們根本不允許交互作用的存在！在本節中，我們將解決這個問題。\n\n\n\n\n\n\n\nFigure 13.4: 在沒有交互作用的情況下，\\(2 \\times 2\\) ANOVA的四種不同結果。在面板（a）中，我們看到因子A的主要效應以及因子B的無效應。面板（b）顯示因子B的主要效應，但因子A沒有影響。面板（c）顯示因子A和因子B的主要影響。最後，面板（d）顯示兩個因子都沒有影響。\n\n\n\n\n\n\n\n\n\nFigure 13.5: \\(2 \\times 2\\) ANOVA中質量上不同的交互作用\n\n\n\n\n\n\n\n\n\nFigure 13.6: 使用臨床試驗數據的ANOVA中，jamovi屏幕顯示如何生成描述性交互作用圖\n\n\n\n\n\n13.2.1 交互作用代表什麼？\n本節我們要介紹的關鍵概念是交互作用效應。在我們迄今為止討論的ANOVA模型中，我們的模型中只有兩個因素（即藥物和治療）。但是，當我們添加交互作用時，我們在模型中添加了一個新的組件：藥物和治療的組合。直觀地說，交互作用效應的概念相當簡單。這只是意味著因子A的效應會因為我們談論的因子B的不同水平而有所不同。但是，在我們的數據方面，這實際上意味著什麼呢？ ?fig-fig13-5中的圖表描述了幾種不同的模式，儘管它們彼此相當不同，但它們都被視為交互作用效應。因此，將這個質的概念轉化為統計學家可以使用的數學概念並不完全簡單。\n[附加技術細節4]\n\n\n\n\n13.2.2 交互作用的自由度\n將交互作用納入計算後，計算自由度變得稍微複雜一些。首先，讓我們考慮整個ANOVA模型。一旦我們在模型中包括交互效應，我們允許每個單獨組具有唯一的平均值，\\(mu_{rc}\\)。對於一個\\(R \\times C\\)的因子ANOVA，這意味著模型中有\\(R \\times C\\)個感興趣的數量，並且只有一個約束：所有組均值需要平均為總體均值。因此，整個模型需要有(\\(R \\times C\\)) - 1個自由度。但是因子A的主效應具有\\(R-1\\)個自由度，因子B的主效應具有\\(C-1\\)個自由度。這意味著與交互作用相關的自由度為\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\n這只是與行因子和列因子相關的自由度之積。\n那剩餘自由度呢？由於我們添加了吸收一些自由度的交互項，剩餘的自由度變得更少。具體來說，請注意，如果具有交互作用的模型總共有\\((R \\times C) - 1\\)，並且在數據集中有\\(N\\)個受1個總體均值約束的觀察值，那麼您的剩餘自由度現在變為\\(N-(R \\times C)-1+1\\)，或者只是\\(N-(R \\times C)\\)。\n\n\n\n13.2.3 使用jamovi完成多因子變異數分析\n在jamovi中將交互項添加到ANOVA模型非常簡單。實際上，這不僅簡單，而且是ANOVA的默認選項。這意味著當您為ANOVA指定兩個因子時，例如藥物和療法，則交互組件 - 藥物\\(\\times\\)療法 - 會自動添加到模型中5。當我們將交互項納入ANOVA運行後，我們將得到@fig-fig13-7中顯示的結果。\n\n\n\n\n\n\nFigure 13.7: 包括交互組件藥物\\(\\times\\)療法的完整因子模型結果\n\n\n\n\n結果顯示，儘管我們確實對藥物有顯著的主效應（\\(F_{2,12} = 31.7, p < .001\\)）和療法類型（\\(F_{1,12} = 8.6, p = .013\\)），但兩者之間沒有顯著的交互作用（\\(F_{2,12} = 2.5, p = 0.125\\)）。\n\n\n\n13.2.4 解讀分析結果\n在解釋因子ANOVA結果時，有幾個非常重要的事情需要考慮。首先，我們在單因子ANOVA中遇到的相同問題，即如果您獲得（例如）藥物的顯著主效應，它並不能告訴您哪些藥物彼此有差異。要找出這個答案，您需要進行額外的分析。稍後我們將討論可以在[指定對比方式的不同方法]和[事後檢驗]中運行的一些分析。對於交互作用效果也是如此。知道有顯著的交互作用並不能告訴您存在哪種類型的交互作用。同樣，您需要進行額外的分析。\n其次，在獲得顯著的交互作用效果但沒有相應主效應的情況下，會出現非常奇特的解釋問題。有時候會發生這種情況。例如，在@fig-fig13-5 a中顯示的交叉互動中，這正是您會發現的情況。在這種情況下，主效應都不顯著，但交互作用效果顯著。這是一個難以解釋的情況，人們通常對此感到困惑。統計學家在這種情況下喜歡給出的一般建議是，當存在交互作用時，您不應該過多地關注主效應。他們這樣說的原因是，雖然從數學的角度看，主效應的檢驗完全有效，但是當存在顯著的交互作用效果時，主效應很少檢驗有趣的假設。回想一下@sec-What-hypotheses-are-we-testing，主效應的虛無假設是邊際均值彼此相等，邊際均值是由幾個不同組的平均值形成的。但是，如果您有一個顯著的交互作用效果，那麼您就知道組成邊際均值的組並不是同質的，所以真的不明顯為什麼您會關心那些邊際均值。\n以下是我的意思。再次以臨床實例為例。假設我們有一個\\(2 \\times 2\\)設計，比較了兩種不同的恐懼症治療方法（例如，系統性緩解法和淹沒療法），以及兩種不同的減輕焦慮藥物（例如，Anxifree和Joyzepam）。現在，假設我們發現當緩解法是治療方法時，Anxifree無效；當淹沒療法是治療方法時，Joyzepam無效。但對於另一種治療方法，兩者都相當有效。這是一個典型的交叉互動，當我們運行ANOVA時，我們會發現沒有藥物的主要效果，但有顯著的互動。那麼，說沒有主效應究竟意味著什麼呢？那意味著，如果我們平均兩種不同的心理治療方法，那麼Anxifree和Joyzepam的平均效果是相同的。但是，有誰會在意這個呢？在治療恐懼症時，從來沒有一個人可以使用“平均”的淹沒療法和緩解法進行治療。這並不合理。您要么得到一個，要么得到另一個。對於一種治療方法，一種藥物是有效的，對於另一種治療方法，另一種藥物是有效的。交互作用是重要的，而主效應在某種程度上是無關緊要的。\n這樣的事情經常發生。主效應是邊際均值的檢驗，當交互作用存在時，我們經常會發現自己對邊際均值不感興趣，因為它們意味著在交互作用告訴我們不應該取平均值的事物上取平均值！當然，並非總是在存在交互作用的情況下，主效應就毫無意義。經常出現的情況是，主效應很大，交互作用很小，這種情況下，您仍然可以說類似於“藥物A通常比藥物B更有效”（因為藥物效果很大），但您需要對其進行一些修改，添加“對於不同的心理治療，有效性差異有所不同。”無論如何，這裡的主要觀點是，每當您獲得顯著的交互作用時，您應該停下來思考主效應在這個語境中的真正意義。不要自動假設主效應是有趣的。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#變異數分析的效果量",
    "href": "13-Factorial-ANOVA.html#變異數分析的效果量",
    "title": "13  多因子變異數分析",
    "section": "13.3 變異數分析的效果量",
    "text": "13.3 變異數分析的效果量\n對於因子分析變異數分析（factorial ANOVA），效應量計算與單因素變異數分析中使用的非常相似（參見[效應量]部分）。具體來說，我們可以使用 \\(\\eta^2\\)（eta 平方）作為簡單衡量任何特定條款的整體效應大小的方法。與以前一樣，\\(\\eta^2\\)是通過將與該條款相關的平方和除以總平方和來定義的。例如，要確定因子A主效應的大小，我們將使用以下公式：\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\n與以前一樣，這可以以與回歸中的 \\(R^2\\) 類似的方式進行解釋。6 它告訴您由因子A的主效應解釋的結果變量變異的比例。因此，這是一個從0（完全沒有效果）到1（解釋結果變異的全部）的範圍內的數字。此外，所有\\(\\eta^2\\)值的總和，跨越模型中的所有條款，將總和為ANOVA模型的總\\(R^2\\)。例如，如果ANOVA模型完美適合（即，根本沒有組內變異！），則\\(\\eta^2\\)值將總和為1。當然，這在現實生活中很少（如果有的話）發生。\n然而，在進行因子分析變異數分析時，人們喜歡報告的效應量有第二種衡量方法，稱為部分 \\(\\eta^2\\)。部分 \\(\\eta^2\\)（有時表示為\\(p^{\\eta^2}\\) 或\\(\\eta_p^2\\)）背後的想法是，當衡量特定條款的效應量時（例如，因子A的主效應），您希望刻意忽略模型中的其他效應（例如，因子B的主效應）。也就是說，您想假設所有這些其他條款的效應都為零，然後計算\\(\\eta^2\\)值本來是什麼。這實際上很容易計算。您所要做的就是從分母中移除與其他條款相關的平方和。換句話說，如果您想要因子A主效應的部分\\(\\eta^2\\)，分母就是因子A和殘差的SS值之和。\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\n這將始終給您一個比\\(\\eta^2\\)更大的數字，這我猜想是部分\\(\\eta^2\\)受歡迎的原因。再次，您得到一個介於0和1之間的數字，其中0表示沒有影響。然而，解釋較大的部分\\(\\eta^2\\)值意味著什麼，這有點棘手。尤其是，您實際上無法比較不同條款的部分\\(\\eta^2\\)值！例如，假設根本沒有組內變異：如果是這樣，\\(SS_R = 0\\)。這意味著每個條款的部分\\(\\eta^2\\)值都是1。但這並不意味著模型中的所有條款都同樣重要，或者它們的大小相同。這只是意味著模型中所有條款的效應量相對於殘差變化都很大。它無法跨條款進行比較。\n要了解我的意思，查看具體示例非常有用。首先，讓我們看一下原始ANOVA（Table 13.6）中的效應量，無交互作用條款，來自@fig-fig13-3。\n\n\n\n\n\n\nTable 13.6:  ANOVA模型未包括交互作用項目的效應量 \n\neta.sqpartial.eta.sq\n\ndrug0.710.79\n\ntherapy0.100.34\n\n\n\n\n\n首先觀察\\(\\eta^2\\)值，我們可以看到藥物解釋了心情改善的71%變異（即\\(\\eta^2 = 0.71\\)），而治療僅解釋了10%。這使得總共有19%的變異未被解釋（即，殘差佔結果變異的19%）。整體來說，這意味著我們有非常大的藥物效應[^factorial-anova-7]和適中的治療效應。\n[^factorial-anova-7]：我認為這個數值大得令人難以置信。這個數據集的人工特徵現在真的開始顯露出來了！\n現在讓我們看看部分\\(\\eta^2\\)值，如@fig-fig13-3所示。由於治療的效果並不是很大，因此對其進行控制並不會產生很大的差異，所以藥物的部分\\(\\eta^2\\)不會增加很多，我們得到一個值\\(p^{\\eta^2} = 0.79\\)。相反，因為藥物的效果非常大，對其進行控制會產生很大的差異，因此當我們計算治療的部分\\(\\eta^2\\)時，可以看到它上升到\\(p^{\\eta^2} = 0.34\\)。我們必須問自己的問題是，這些部分\\(\\eta^2\\)值實際上意味著什麼？我通常將因子A主效應的部分\\(\\eta^2\\)解釋為關於僅因子A變化的假設實驗的說明。所以，即使在這個實驗中我們改變了A和B，我們可以很容易地想像一個僅因子A變化的實驗，部分\\(\\eta^2\\)統計數據告訴您在該實驗中預期結果變量的多少變異會被解釋。然而，應該注意的是，這種解釋，就像許多與主效應相關的事物一樣，在存在大的和顯著的交互作用效應時沒有太多意義。\n說到交互作用效應，?tbl-tab13-7顯示了我們在包含交互作用條款的模型中計算效應大小時會得到什麼結果，如@fig-fig13-7所示。如您所見，主效應的\\(\\eta^2\\)值沒有改變，但部分\\(\\eta^2\\)值發生了變化：\n\n\n\n\n\nTable 13.7:  ANOVA模型包含交互作用項目的效應大小 \n\neta.sqpartial.eta.sq\n\ndrug0.710.84\n\ntherapy0.100.42\n\ndrug*therapy0.060.29\n\n\n\n\n\n\n13.3.1 估計組間平均\n在許多情況下，您可能會想要根據ANOVA結果報告所有組均值的估計以及與之相關的置信區間。您可以使用jamovi ANOVA分析中的“估計邊際均值”選項來執行此操作，如@fig-fig13-8所示。如果您運行的ANOVA是一個飽和模型（即，包含所有可能的主效應和所有可能的交互作用效應），那麼組均值的估計實際上與樣本均值相同，儘管置信區間將使用標準誤差的合併估計而不是為每個組使用單獨的估計。\n\n\n\n\n\n\nFigure 13.8: jamovi屏幕截圖顯示了飽和模型的邊際均值，即包括臨床試驗數據集中的交互作用分量\n\n\n\n\n在輸出中，我們看到安慰劑組無治療時的估計平均情緒增益為\\(0.300\\)，\\(95\\%\\)置信區間從\\(0.006\\)到\\(0.594\\)。請注意，由於ANOVA模型假定方差同質性並因此使用方差的合併估計，這些置信區間與您單獨為每個組計算的置信區間不同。\n當模型不包含交互作用項時，估計的組均值將與樣本均值不同。jamovi將根據邊際均值（即假定無交互作用）計算預期的組均值，而不是報告樣本均值。使用我們之前開發的符號，報告了μrc，對於（行）因子A上的第r級和（列）因子B上的第c級的均值，將是\\(\\mu_{..} + \\alpha_r + \\beta_c\\)。如果兩個因素之間確實沒有交互作用，那麼這實際上比原始樣本均值更好的估計了母體均值。通過jamovi ANOVA分析中的“Model”選項從模型中刪除交互作用項，為@fig-fig13-9所示的分析提供邊際均值。\n\n\n\n\n\n\nFigure 13.9: jamovi屏幕截圖顯示了未飽和模型的邊際均值，即未包括臨床試驗數據集中的交互作用分量"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "href": "13-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "title": "13  多因子變異數分析",
    "section": "13.4 檢核變異數分析的執行條件",
    "text": "13.4 檢核變異數分析的執行條件\n與單因素ANOVA一樣，因子ANOVA的關鍵假設是方差同質性（所有組具有相同的標準差）、殘差正態性和觀察值獨立性。前兩者是我們可以檢查的內容。對於第三點，您需要自己評估不同觀察值之間是否存在特殊關係，例如獨立變量是時間的重複測量，因此觀察值在時間1和時間2之間存在關係：不同時間點的觀察值來自同一個人。此外，如果您沒有使用飽和模型（例如，如果您省略了交互作用項），那麼您還假定省略的項不重要。當然，您可以通過運行包含省略項的ANOVA來檢查這最後一點，看看它們是否顯著，所以這非常簡單。那麼方差同質性和殘差正態性呢？事實證明，這些非常容易檢查。這與我們為單因素ANOVA所做的檢查沒有區別。\n\n\n13.4.1 變異數的同質性\n正如上一章 Section 12.6.1 中提到的，最好是通過視覺檢查標準差在不同組/類別之間的比較圖，並查看Levene檢驗是否與視覺檢查一致。Levene檢驗的理論在 ?sec-Checking-the-homogeneity-of-variance-assumption中 討論過，所以我不再討論。該檢驗要求您使用一個飽和模型（即，包含所有相關條款），因為該檢驗主要關注的是組內方差，而不是使用與完整模型相關的其他方法計算。在jamovi中，可以在ANOVA ‘Assumption Checks’ - ’Homogeneity Tests’選項下指定Levene檢驗，結果如 Figure 13.10 所示。Levene檢驗的非顯著性意味著，只要與標準差圖的視覺檢查一致，我們就可以放心地假定方差同質性假設沒有被違反。\n\n\n\n13.4.2 殘差的常態性\n與單因素ANOVA一樣，我們可以用直接的方式測試殘差的正態性（參見 Section 12.6.4 )。然而，通常最好是使用QQ圖以圖形方式檢查殘差。請參見 Figure 13.10 。\n\n\n\n\n\n\nFigure 13.10: 檢查ANOVA模型的假設"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "href": "13-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "title": "13  多因子變異數分析",
    "section": "13.5 共變數分析 (ANCOVA)",
    "text": "13.5 共變數分析 (ANCOVA)\nANOVA的一種變體是當您擁有一個可能與因變量相關的額外連續變量時。這個額外的變量可以作為協變量添加到分析中，正如協方差分析（ANCOVA）這個貼切的名稱所示。\n在ANCOVA中，因變量的值會根據協變量的影響進行「調整」，然後以通常的方式在各組之間測試「調整後」的均值。這種技術可以提高實驗的精確性，因此提供了對因變量中組均值相等性的更「有效」的檢驗。ANCOVA是如何做到這一點的呢？儘管協變量本身通常不是實驗性的，但對協變量的調整可以降低實驗誤差的估計，從而通過減少誤差變異，提高精確性。這意味著不適當地無法拒絕虛無假設（偽陰性或第二類錯誤）的可能性較小。\n儘管存在這個優勢，ANCOVA仍然存在解除組間實際差異的風險，這應該避免。例如，看看@fig-fig13-11，它顯示了統計焦慮與年齡的關係，並顯示了兩個不同的組別–具有藝術或科學背景或偏好的學生。以年齡為協變量的ANCOVA可能會得出統計焦慮在兩個組別之間沒有差異的結論。這個結論是否合理呢？很可能不合理，因為兩個組別的年齡不重疊，方差分析實質上是「將結果外推到沒有數據的區域」（Everitt (1996)，第68頁）。\n\n\n\n\n\n\nFigure 13.11: 統計焦慮與年齡的圖示，對於兩個不同的組別\n\n\n\n\n顯然，需要仔細考慮對區別鮮明的組別進行協方差分析。這適用於單因素和因子設計，因為ANCOVA可以用於兩者。\n\n\n13.5.1 使用jamovi完成共變數分析\n一位健康心理學家對例行騎自行車和壓力對幸福程度的影響感興趣，並將年齡作為協變量。您可以在ancova.csv文件中找到數據集。在jamovi中打開此文件，然後選擇分析 - ANOVA - ANCOVA 以打開ANCOVA分析窗口（Figure 13.12）。突顯因變量「幸福」，將其轉移到「因變量」文本框中。突顯自變量「壓力」和「通勤」，將它們轉移到「固定因素」文本框中。突顯協變量「年齡」，將其轉移到「協變量」文本框中。然後單擊估計邊際均值以顯示圖表和表格選項。\n\n\n\n\n\n\n\nFigure 13.12: jamovi ANCOVA分析窗口\n\n\n\n\njamovi結果窗口中產生了一個顯示主題效應測試的ANCOVA表格（Figure 13.13）。協變量「年齡」的F值在 \\(p = .023\\) 上顯著，這表明年齡是影響因變量幸福的重要預測因子。當我們查看估計的邊際平均分數（Figure 13.14）時，由於在此ANCOVA中包含協變量「年齡」，所以已進行了調整（與未包含協變量的分析相比）。圖表（Figure 13.15）是一個很好的視覺化和解釋顯著效應的方法。\n\n\n\n\n\n\n\nFigure 13.13: jamovi ANCOVA輸出，將幸福度作為壓力和通勤方法的函數，年齡作為協變量\n\n\n\n\n\n\n\n\n\nFigure 13.14: 作為壓力和通勤方式函數的平均幸福水平表（根據協變量年齡進行調整），帶有95％置信區間\n\n\n\n\n\\(F\\) 值主要效果「壓力」（52.61）的相應概率為 \\(p < .001\\)。主要效果「通勤」（42.33）的 \\(F\\) 值的相應概率為 \\(p < .001\\)。由於這兩者都小於通常用於判定統計結果是否顯著的概率（\\(p < .05\\)），我們可以得出壓力的顯著主要效應（\\(F(1, 15) = 52.61, p < .001\\)）和通勤方式的顯著主要效應（\\(F(1, 15) = 42.33, p < .001\\)）。還發現了壓力和通勤方式之間的顯著交互作用（\\(F(1, 15) = 14.15, p = .002\\)）。\n在 Figure 13.15 中，我們可以看到年齡作為協變量時的調整後、邊際的平均幸福分數。在這個分析中，存在一個顯著的交互效應，即壓力較低的騎自行車上班的人比壓力較低的開車上班的人和壓力較高的人（無論他們是騎自行車還是開車上班）更幸福。還有壓力的顯著主要效應——壓力較低的人比壓力較高的人更幸福。而且通勤行為的顯著主要效應也是如此——平均而言，騎自行車上班的人比開車上班的人更幸福。\n\n\n\n\n\n\nFigure 13.15: 作為壓力和通勤方式函數的平均幸福水平圖\n\n\n\n\n需要注意的一點是，如果您想在 ANOVA 中包含協變量，那麼還有一個額外的假設：協變量與因變量之間的關係應對所有自變量的水平都是相似的。這可以通過在 jamovi Model - Model terms 選項中為協變量和每個自變量添加交互項來檢查。如果交互效應不顯著，則可以將其移除。如果它顯著，則可能需要使用更高級的統計技術（這超出了本書的範疇，所以您可能需要諮詢一位友好的統計學家）。總之，在進行 ANCOVA 分析時，要仔細考慮協變量與自變量的關係，以確保結果的準確性和有效性。\n\n總的來說，ANCOVA 分析可以幫助我們更好地瞭解不同變量之間的關係，並通過引入協變量來提高實驗的精確性。然而，在實際應用中，我們需要仔細思考和評估協變量的選擇，以確保結果的可靠性。在進行分析時，要注意檢查假設，並在必要時尋求統計專家的幫助。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "href": "13-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "title": "13  多因子變異數分析",
    "section": "13.6 變異數分析就是線性模型",
    "text": "13.6 變異數分析就是線性模型\n一個非常重要的事情是要了解 ANOVA 和回歸實際上是一回事。從表面上看，您可能不會認為這是真的。畢竟，到目前為止我對它們的描述表明 ANOVA 主要關注測試組間差異，而回歸主要關注瞭解變量之間的相關性。在這一點上，這完全沒錯。但是當你深入了解時，所謂的 ANOVA 和回歸的基本機制非常相似。事實上，如果你仔細想想，你已經看到了這一點的證據。 ANOVA 和回歸都依賴於平方和 (SS)，都使用 F 檢驗等。回顧過去，很難逃避這樣的感覺： Chapter 10 和 Chapter 12 有點重複。\n這樣做的原因是 ANOVA 和回歸都是線性模型。在回歸的情況下，這是顯而易見的。我們用來定義預測因素和結果之間關係的回歸方程是一條直線的方程，所以這顯然是一個線性模型，方程式為\n\\[Y_p=b_0+b_1 X_{1p} +b_2 X_{2p} + \\epsilon_p\\]\n其中 \\(Y_p\\) 是第 p 次觀察（例如，第 p 個人）的結果值，\\(X_{1p}\\) 是第 p 次觀察的第一個預測因子的值，\\(X_{2p}\\) 是第 p 次觀察的第二個預測因子的值，\\(b_0\\)，\\(b_1\\) 和 \\(b_2\\) 是我們的回歸係數，\\(\\epsilon_p\\) 是第 p 個殘差。如果我們忽略殘差 \\(\\epsilon_p\\)，僅關注回歸線本身，我們得到以下公式：\n\\[\\hat{Y}_p=b_0+b_1 X_{1p} +b_2 X_{2p} \\]\n其中 \\(\\hat{Y}_p\\) 是回歸線為第 p 個人預測的 Y 值，而不是實際觀察到的值 \\(Y_p\\)。不是立即顯而易見的事情是，我們可以將 ANOVA 也寫成線性模型。然而，實際上這非常簡單。讓我們從一個非常簡單的例子開始，將 \\(2 \\times 2\\) 因子 ANOVA 重寫為線性模型。\n\n\n\n13.6.1 示範資料\n為了具體說明，假設我們的結果變量是學生在我的課堂上獲得的成績，這是一個比例尺度變量，對應於 \\(0％\\) 到 \\(100％\\) 的分數。有兩個感興趣的預測變量：學生是否參加了課程（出席變量）以及學生是否真正閱讀了教科書（閱讀變量）。我們將假設如果學生參加課程，那麼 attend = 1，如果他們沒有參加，那麼 attend = 0。同樣，如果學生閱讀了教科書，我們將說 reading = 1，如果他們沒有，那麼 reading = 0。\n好吧，到目前為止，這還算簡單。接下來我們需要做的是將一些數學概念應用在這裡（抱歉！）。為了舉例，讓 \\(Y_p\\) 表示課堂中第 p 個學生的成績。這與我們在本章前面使用的符號不完全相同。以前，我們用符號 \\(Y_{rci}\\) 來表示第 1 個預測因子的第 r 個組中的第 i 個人（行因子）和第 2 個預測因子的第 c 個組（列因子）。這種擴展符號對於描述如何計算 SS 值非常方便，但在目前的情況下很繁瑣，所以我在這裡更換符號。現在，\\(Y_p\\) 符號比 \\(Y_{rci}\\) 更簡單，但它的缺點是它實際上沒有跟踪組成員資訊！也就是說，如果我告訴你 \\(Y_{0,0,3} = 35\\)，你會立刻知道我們在談論一個沒有上課（即 attend = 0）並且沒有閱讀教科書（即 reading = 0）的學生（事實上是第 3 個這樣的學生），並且最終未通過課程（成績 = 35）。但如果我告訴你 \\(Y_p = 35\\)，你只知道第 p 個學生沒有取得好成績。我們在這裡丟失了一些關鍵資訊。當然，想想如何解決這個問題並不費力。相反，我們將引入兩個新變量 \\(X_{1p}\\) 和 \\(X_{2p}\\) 來追踪這些資訊。在我們假設的學生案例中，我們知道 \\(X_{1p} = 0\\)（即 attend = 0）和 \\(X_{2p} = 0\\)（即 reading = 0）。因此，數據可能如 Table 13.8 所示。\n\n\n\n\n\nTable 13.8:  成績，出勤和閱讀教科書的數據 \n\nperson, \\(p\\)grade, \\(Y_p\\)attendance, \\(X_{1p}\\)reading, \\(X_{2p}\\)\n\n19011\n\n28711\n\n37501\n\n46010\n\n53500\n\n65000\n\n76510\n\n87001\n\n\n\n\n\n當然，這並沒有什麼特別之處。這正是我們期望看到的數據格式！請參閱數據文件 rtfm.csv。我們可以使用 jamovi 的 ‘Descriptives’ 分析來確認這個數據集對應於一個平衡設計，對於 attend 和 reading 的每個組合，都有 2 個觀測值。同樣，我們還可以為每個組合計算平均成績。這在 Figure 13.16 中顯示。看著平均分數，人們會強烈感覺到閱讀課本和上課都非常重要。\n\n\n\n\n\n\nFigure 13.16: rtfm 數據集的 jamovi 描述性統計\n\n\n\n\n\n\n13.6.2 以迴歸模型處理非連續因子\n好吧，讓我們回到數學上的討論。現在，我們的數據已用三個數值變量表示：連續變量 \\(Y\\) 和兩個二元變量 \\(X_1\\) 和 \\(X_2\\)。我希望您能認識到，我們的 \\(2 \\times 2\\) 因子分析變異數完全等同於迴歸模型\n\\[Y_p=b_0+b_1 X_{1p} + b_2 X_{2p} + \\epsilon_p\\]\n當然，這正是我之前用來描述具有兩個預測變量的迴歸模型的完全相同的方程式！唯一的區別是 \\(X_1\\) 和 \\(X_2\\) 現在是二元變量（即，值只能為 0 或 1），而在迴歸分析中，我們期望 \\(X_1\\) 和 \\(X_2\\) 是連續的。有幾種方法可以說服您相信這一點。一個可能性是進行冗長的數學練習，證明這兩者是相同的。然而，我要冒昧地猜測，這本書的大多數讀者會覺得這很煩人而不是有幫助。相反，我將解釋基本思想，然後依賴 jamovi 來說明 ANOVA 分析和迴歸分析不僅相似，而且在所有意圖和目的上是相同的。讓我們首先將其作為 ANOVA 運行。為此，我們將使用 rtfm 數據集，Figure 13.17 顯示了在 jamovi 中運行分析時我們得到的結果。\n\n\n\n\n\n\nFigure 13.17: 在 jamovi 中的 rtfm.csv 數據集 ANOVA，不包含交互作用項\n\n\n\n\n好的，通過從 ANOVA 表和我們之前呈現的平均分數中讀取關鍵數字，我們可以看到，如果學生參加課程（\\(F_{1,5} = 21.6, p = .0056\\)），他們的成績會更高，如果他們閱讀教材（\\(F_{1,5} = 52.3, p = .0008\\)）。讓我們記下這些 p 值和這些 \\(F\\) 統計數字。\n現在讓我們從線性迴歸的角度考慮相同的分析。在 rtfm 數據集中，我們將 attend 和 reading 編碼為數值預測變量。在這種情況下，這是完全可以接受的。在某種意義上，參加課程的學生（即 attend = 1）事實上的確比沒有參加的學生（即 attend = 0）做了“更多的出席”。因此，將其作為迴歸模型中的預測變量完全不是不合理的。這有點不尋常，因為預測變量只有兩個可能的值，但這並不違反線性迴歸的任何假設。而且易於解釋。如果 attend 的迴歸係數大於 0，則意味著參加課程的學生會獲得更高的成績。如果小於零，那麼參加課程的學生會獲得較低的成績。對於我們的閱讀變量也是如此。\n等一下。為什麼會這樣？這對於接受過幾堂統計課程並熟悉數學的人來說是直觀明顯的，但對其他人來說一開始並不清楚。要理解為什麼會這樣，有助於仔細觀察幾個特定的學生。讓我們首先考慮我們數據集中的第 6 位和第 7 位學生（即 \\(p = 6\\) 和 \\(p = 7\\)）。兩者都沒有閱讀教科書，因此在這兩種情況下，我們都可以將 reading 設為 0。換句話說，用我們的數學符號表示，我們觀察到 \\(X_{2,6} = 0\\) 和 \\(X_{2,7} = 0\\)。然而，第 7 位學生參加了課程（即 attend = 1，\\(X_{1,7} = 1\\)），而第 6 位學生沒有參加（即 attend = 0，\\(X_{1,6} = 0\\)）。現在讓我們看看當我們將這些數字插入迴歸線的一般公式時會發生什麼。對於第 6 位學生，迴歸預測：\n\\[\n\\begin{split}\n\\hat{Y}_6 & = b_0 + b_1 X_{1,6} + b_2 X_{2,6} \\\\\n& = b_0 + (b_1 \\times 0) + (b_2 \\times 0) \\\\\n& = b_0\n\\end{split}\n\\]\n因此，我們預計這位學生將獲得與截距項 \\(b_0\\) 相對應的成績。那麼第 7 位學生呢？這次當我們將數字插入迴歸線公式時，我們得到以下結果：\n\\[\n\\begin{split}\n\\hat{Y}_7 & = b_0 + b_1 X_{1,7} + b_2 X_{2,7} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 0) \\\\\n& = b_0 + b_1\n\\end{split}\n\\]\n因為這位學生參加了課程，預計成績等於截距項 b0 加上與 attend 變量相關的係數 \\(b_1\\)。所以，如果 \\(b_1\\) 大於零，我們預期參加課程的學生將比那些沒有參加的學生獲得更高的成績。如果這個係數為負，我們則預期相反：上課的學生表現會更差。實際上，我們可以更進一步。第一位學生呢？他參加了課程（\\(X_{1,1} = 1\\)），並且閱讀了教科書（\\(X_{2,1} = 1\\)）？如果我們將這些數字插入迴歸，我們得到：\n\\[\n\\begin{split}\n\\hat{Y}_1 & = b_0 + b_1 X_{1,1} + b_2 X_{2,1} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 1) \\\\\n& = b_0 + b_1 + b_2\n\\end{split}\n\\]\n因此，如果我們假設參加課程有助於獲得好成績（即 \\(b1 \\> 0\\)），並假設閱讀教科書也有助於獲得好成績（即 \\(b2 \\> 0\\)），那麼我們的預期是，第 1 位學生將比第 6 位學生和第 7 位學生獲得更高的成績。\n此時，你可能一點也不會感到驚訝地了解到迴歸模型預測，讀了書但沒有參加課程的第 3 位學生將獲得 \\(b_{2} + b_{0}\\) 的成績。我不會再用另一個迴歸公式來煩悶你。相反，我將向你展示的是帶有預期成績的 Table 13.9。\n\n\n\n\n\n\nTable 13.9:  迴歸模型的預期成績 \n\nread textbook\n\nnoyes\n\nattended?no\\( \\beta_0 \\)\\( \\beta_0 + \\beta_2 \\)\n\nyes\\( \\beta_0 + \\beta_1 \\)\\( \\beta_0 + \\beta_1 + \\beta_2 \\)\n\n\n\n\n\n正如你所看到的，截距項 \\(b_0\\) 作為一種基線成績，用來表示那些沒有花時間參加課程或閱讀教科書的學生所期望的成績。同樣，\\(b_1\\) 表示你預期能從上課中得到的提升，而 \\(b_2\\) 表示閱讀教科書帶來的提升。事實上，如果這是一個 ANOVA，你可能很想將 b1 稱為出席的主要效應，將 \\(b_2\\) 稱為閱讀的主要效應！事實上，對於一個簡單的 \\(2 \\times 2\\) ANOVA，情況確實是這樣。\n好的，既然我們已經開始看到為什麼 ANOVA 和迴歸基本上是同一回事，讓我們實際運用 rtfm 數據和 jamovi 迴歸分析來確信這確實是真的。以通常的方式運行迴歸會得到 Figure 13.18 中顯示的結果。\n\n\n\n\n\n\nFigure 13.18: 數據集rtfm.csv 在 jamovi 中的迴歸分析，無交互作用項\n\n\n\n\n這裡有幾個有趣的地方需要注意。首先，注意截距項是 43.5，接近觀察到的那兩個既沒有閱讀文本也沒有參加課程的學生的 “組” 平均值 42.5。其次，注意我們得到了參加變量的迴歸係數 \\(b_1 = 18.0\\)，這表明參加課程的學生比沒有參加課程的學生高出 18%。因此，我們的期望是，那些上課但沒有閱讀教科書的學生將獲得 \\(b_0 + b_1\\) 的成績，即 \\(43.5 + 18.0 = 61.5\\)。當我們觀察閱讀教科書的學生時，你可以自己驗證同樣的事情。\n實際上，我們可以在建立 ANOVA 和迴歸等價性方面進一步推進。看看迴歸輸出中與 attend 變量和 reading 變量相關的 p 值。它們與我們之前在運行 ANOVA 時遇到的完全相同。這可能看起來有點奇怪，因為運行我們的迴歸模型時使用的檢驗計算了一個 t 統計量，而 ANOVA 計算了一個 F 統計量。然而，如果您還記得我們在 Chapter 7 中提到的內容，t 分布和 F 分布之間存在著某種關係。如果你有一個根據 k 自由度的 t 分布的數量，然後將其平方，那麼這個新的平方數量就遵循一個自由度為 1 和 k 的 F 分布。對於我們迴歸模型中的 t 統計量，我們可以檢查這一點。對於 attend 變量，我們得到一個 t 值為 4.65。如果我們將這個數字平方，我們最終得到的是 21.6，這與我們 ANOVA 中相應的 F 統計量相匹配。\n最後，你還應該知道一件事。因為 jamovi 瞭解到 ANOVA 和迴歸都是線性模型的例子，所以它允許您使用 ‘Linear Regression’ - ‘Model Coefficients’ - ‘Omnibus Test’ - ‘ANOVA Test’ 從迴歸模型中提取經典的 ANOVA 表，這將給你在 Figure 13.19 中顯示的表格。\n\n\n\n\n\n\n\nFigure 13.19: jamovi迴歸分析的Omnibus ANOVA Test結果\n\n\n\n\n\n\n13.6.3 比較因子間平均值的編碼\n\n在這一點上，我已經向您展示了如何將 \\(2 \\times 2\\) ANOVA 轉換為線性模型。從而很容易看出這如何擴展到 \\(2 \\times 2 \\times 2\\) ANOVA 或 \\(2 \\times 2 \\times 2 \\times 2\\) ANOVA。事實上，這是同一件事。對於每個因子，你只需添加一個新的二元變量。當我們考慮具有多於兩個級別的因子時，問題變得更加複雜。例如，考慮我們在本章前面使用clinicaltrial.csv 數據運行的 \\(3 \\times 2\\) ANOVA。我們如何將具有三個級別的藥物因子轉換為適合迴歸的數值形式？\n事實上，這個問題的答案相當簡單。我們所要做的就是意識到三級因子可以被重新描述為兩個二元變量。假設，例如，我要創建一個名為 druganxifree 的新二元變量。每當藥物變量等於 “anxifree” 時，我們將 druganxifree 設為 1。否則，將 druganxifree 設為 0。這個變量設立了一個對比，在這種情況下是在 anxifree 和其他兩種藥物之間。當然，僅憑 druganxifree 對比還不足以完全捕捉我們藥物變量中的所有信息。我們需要第二個對比，一個能讓我們區分 joyzepam 和安慰劑的對比。為此，我們可以創建第二個二元對比，名為 drugjoyzepam，如果藥物是 joyzepam，則等於 1，否則等於 0。這兩個對比結合在一起，使我們能夠完美區分所有三種可能的藥物。Table 13.10 說明了這一點。\n\n\n\n\n\nTable 13.10:  二元對比以區分所有三種可能的藥物 \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\n如果給病人用的藥物是安慰劑，那麼這兩個對比變量都將等於 0。如果藥物是 Anxifree，那麼 druganxifree 變量將等於 1，而 drugjoyzepam 將為 0。對於 Joyzepam，情況剛好相反：drugjoyzepam 為 1，而 druganxifree 為 0。\n使用 jamovi 計算新變量命令創建對比變量並不困難。例如，要創建 druganxifree 變量，請在計算新變量公式框中編寫此邏輯表達式：IF(drug == ‘anxifree’, 1, 0)‘。同樣，要創建新變量 drugjoyzepam，請使用此邏輯表達式：IF(drug == ’joyzepam’, 1, 0)。對於 CBTtherapy，請使用：IF(therapy == ‘CBT’, 1, 0)。您可以在 jamovi 數據文件 clinicaltrial2.omv 中查看這些新變量和相應的邏輯表達式。\n我們現在已經將三級因子根據兩個二元變量進行了重新編碼，我們已經看到，對於二元變量，ANOVA 和迴歸的行為方式是相同的。然而，在這種情況下，還有一些額外的複雜性，我們將在下一節中討論。\n\n\n\n13.6.4 變異數分析與非二元因子迴歸分析的等價性\n現在，我們有兩個不同版本的相同數據集。我們的原始數據中，clinicaltrial.csv 文件中的藥物變量表示為單個三級因子，而在擴展數據 clinicaltrial2.omv 中，它擴展為兩個二元對比。再次，我們想要證明的是，我們原來的 \\(3 \\times 2\\) 因子 ANOVA 等同於應用於對比變量的迴歸模型。讓我們首先重新執行 ANOVA，結果顯示在 Figure 13.20。\n\n\n\n\n\n\nFigure 13.20: jamovi ANOVA 結果，無交互組件\n\n\n\n\n顯然，這裡沒有什麼驚喜。這正是我們之前執行的相同 ANOVA。接下來，讓我們使用 druganxifree、drugjoyzepam 和 CBTtherapy 作為預測因子進行迴歸。結果顯示在 Figure 13.21。\n\n\n\n\n\n\nFigure 13.21: jamovi 迴歸結果，包含對比變量 druganxifree 和 drugjoyzepam\n\n\n\n\n嗯。這不是我們上次得到的相同輸出。毫不奇怪，迴歸輸出將每個預測因子的結果分別打印出來，就像我們之前進行迴歸分析的每一次一樣。一方面，我們可以看到 CBTtherapy 變量的 p 值與我們原始 ANOVA 中治療因子的 p 值完全相同，因此我們可以放心，迴歸模型與 ANOVA 做的事情相同。另一方面，這個迴歸模型分別測試 druganxifree 對比和 drugjoyzepam 對比，好像它們是兩個完全無關的變量。當然，這並不奇怪，因為可憐的迴歸分析根本無法知道 drugjoyzepam 和 druganxifree 實際上是我們用來編碼三級藥物因子的兩個不同對比。就它所知，drugjoyzepam 和 druganxifree 與 drugjoyzepam 和 therapyCBT 之間的關係沒有任何區別。然而，您和我都知道得更好。在這個階段，我們根本不感興趣確定這兩個對比是否各自具有顯著性。我們只想知道是否存在藥物的“整體”效果。也就是說，我們希望 jamovi 執行某種“模型比較”檢驗，在此檢驗中，為了檢驗的目的，將兩個“與藥物相關”的對比合併在一起。聽起來熟悉嗎？我們所需要做的就是指定我們的零假設模型，該模型將包括 CBTtherapy 預測因子，並省略所有與藥物相關的變量，如 Figure 13.22 所示。\n\n\n\n\n\n\nFigure 13.22: jamovi 迴歸中的模型比較，零模型 1 與對比模型 2\n\n\n\n\n啊，這樣好多了。我們的 F 統計量是 26.15，自由度是 2 和 14，p 值是 0.00002。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。我們再次看到，變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。這一事實的重要性不應被低估。在本章的其餘部分，我們將重點依賴這個想法。\n雖然我們在 jamovi 中計算了新變量 druganxifree 和 drugjoyzepam 進行對比，僅僅為了顯示變異數分析和迴歸本質上是相同的，在 jamovi 線性迴歸分析中實際上有一個巧妙的捷徑來獲得這些對比，見 Figure 13.23。jamovi 在這裡做的是允許您將因子作為預測變量輸入，等待它…因子！聰明，對吧。您還可以通過 ‘Reference Levels’ 選項指定要用作參考級別的組。我們將其分別更改為 ‘placebo’ 和 ‘no.therapy’，因為這是最有意義的。\n\n\n\n\n\n\nFigure 13.23: jamovi 中帶有因子和對比的迴歸分析，包括整體變異數分析檢驗結果\n\n\n\n\n如果您還在 ‘Model Coefficients’ - ‘Omnibus Test’ 選項下勾選 ‘ANOVA’ 檢驗復選框，我們會看到 F 統計量為 26.15，自由度為 2 和 14，p 值為 0.00002（Figure 13.23）。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。再次，我們看到變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。\n\n\n\n13.6.5 自由度就是計算有多少參數\n經過漫長的等待，我終於可以給出一個我滿意的自由度定義。自由度是根據模型中需要估計的參數數量來定義的。對於迴歸模型或變異數分析，參數數量對應於迴歸係數的數量（即 b 值），包括截距。請記住，任何 F 檢驗都始終是兩個模型之間的比較，第一個 df 是參數數量的差。例如，在上面的模型比較中，零模型（mood.gain ~ therapyCBT）有兩個參數：therapyCBT 變量的一個迴歸係數和截距的第二個參數。替代模型（mood.gain ~ druganxifree + drugjoyzepam + therapyCBT）有四個參數：三個對比中的一個迴歸係數和截距的一個參數。因此，這兩個模型之間的差的自由度是 \\(df_1 = 4 - 2 = 2\\)。\n那麼，在似乎沒有零模型的情況下呢？例如，您可能正在考慮在「線性迴歸」-「模型擬合」選項下選擇「F 檢驗」時出現的 F 檢驗。我最初將其描述為對整個迴歸模型的檢驗。然而，這仍然是兩個模型之間的比較。零模型是僅包含 1 個迴歸係數的簡單模型，用於截距項。替代模型包含 \\(K + 1\\) 個迴歸係數，每個 K 個預測變量一個，再加上截距。因此，您在此 F 檢驗中看到的 df 值等於 \\(df_1 = K + 1 - 1 = K\\)。\n那麼，在 F 檢驗中出現的第二個 df 值呢？這總是指與殘差相關的自由度。也可以用參數的方式來思考這個問題，但這有點反直覺。想象一下，假設整個研究的觀察值總數為 N。如果您想完美地描述這些 N 個值，您需要使用… N 個數字。當您建立迴歸模型時，您實際上在指定一些數字需要完美地描述數據。如果您的模型有 \\(K\\) 個預測變量和一個截距，那麼您已經指定了 \\(K + 1\\) 個數字。那麼，無需確定這將如何完成，您認為還需要多少數字才能將 K 个 1 參數迴歸模型轉換為原始數據的完美描述呢？如果您發現自己在想 \\((K + 1) + (N - K - 1) = N\\)，因此答案必須是 \\(N - K - 1\\)，那就做得很好！這正是對的。原則上，您可以想像一個包含每個數據點的參數的極其複雜的迴歸模型，它當然可以完美地描述數據。這個模型總共包含了 \\(N\\) 個參數，但是我們感興趣的是描述這個完整模型（即 \\(N\\)）所需的參數數量與您實際感興趣的更簡單的迴歸模型所使用的參數數量（即 \\(K + 1\\)）之間的差別，因此 F 檢驗中的第二個自由度是 \\(df_2 = N - K - 1\\)，其中 K 是預測變量的數量（在迴歸模型中）或對比的數量（在變異數分析中）。在我上面給出的示例中，數據集中有 \\((N = 18\\) 觀察值，並且與變異數分析模型相關的 \\(K + 1 = 4\\) 個迴歸係數，因此殘差的自由度是 \\(df_2 = 18 - 4 = 14\\)。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#各種多重比較方案",
    "href": "13-Factorial-ANOVA.html#各種多重比較方案",
    "title": "13  多因子變異數分析",
    "section": "13.7 各種多重比較方案",
    "text": "13.7 各種多重比較方案\n在上一節中，我向您展示了一種將因子轉換為對比組合的方法。在我向您展示的方法中，我們指定了一組二進制變量，其中我們定義了一個類似於 Table 13.11 的表格。\n\n\n\n\n\n\nTable 13.11:  二進制對比以區分所有三種可能的藥物 \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\n表格中的每一行對應於因子水平之一，每一列對應於對比之一。這個表格，始終比列多一行，有一個特殊的名稱。它被稱為對比矩陣。然而，有很多不同的方法可以指定對比矩陣。在本節中，我討論了統計學家使用的一些標準對比矩陣以及如何在 jamovi 中使用它們。如果您打算稍後閱讀[因子變異數分析 3：不平衡設計]一節，那麼仔細閱讀本節是值得的。如果沒有，您可以瀏覽一下它，因為對於平衡設計來說，對比的選擇並不重要。\n\n\n\n13.7.1 比較操作效果\n在我上面描述的這種對比中，因子的一個水平是特殊的，並作為一種 “基線” 類別（例如，在我們的示例中是安慰劑），其他兩個則是根據這個基線來定義的。這種對比的名稱是治療對比，也稱為 “哑变量编码”。在此對比中，因子的每個水平都與基本參考水平進行比較，基本參考水平是截距的值。\n這個名字反映了這樣一個事實，當你的因子中的一個類別確實很特殊，因為它確實代表了基線時，這些對比是非常自然和合理的。這在我們的臨床試驗示例中是有道理的。安慰劑條件對應於不給人們使用任何真正的藥物的情況，因此它是特殊的。其他兩個條件是相對於安慰劑定義的。在一種情況下，您用 Anxifree 替換安慰劑，而在另一種情況下，您用 Joyzepam 替換安慰劑。\n上面顯示的表格是具有3個水平的因子的治療對比矩陣。但是，如果我想要一個具有5個水平的因子的治療對比矩陣呢？您可以像 Table 13.12 那樣排列它。\n\n\n\n\n\n\nTable 13.12:  具有5個水平的治療對比矩陣 \n\nLevel2345\n\n10000\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\n在這個例子中，第一個對比是第2級與第1級比較，第二個對比是第3級與第1級比較，依此類推。請注意，默認情況下，因子的第一水平始終被視為基線類別（即，它是所有零的那個，並且沒有與之相關的顯式對比）。在 jamovi 中，您可以通過操作 “數據變量” 窗口中顯示的變量的水平來更改哪個類別是因子的第一個水平（雙擊電子表格列中的變量名稱以彈出 “數據變量” 視圖。\n\n\n\n13.7.2 Helmert 比較法\n治療對比在很多情況下都很有用。然而，它們在真正存在基線類別的情況下最有意義，並且您希望根據該類別評估所有其他組。然而，在其他情況下，可能不存在這樣的基線類別，與其將每個組與其他組的平均值進行比較可能更有意義。這就是我們遇到 Helmert 對比的地方，由 jamovi ‘ANOVA’ - ‘Contrasts’ 選擇框中的 ‘helmert’ 選項生成。Helmert 對比背後的想法是將每個組與 “前一個” 組的平均值進行比較。也就是說，第一個對比表示第2組和第1組之間的差異，第二個對比表示第3組與第1組和第2組的平均值之間的差異，依此類推。對於具有五個水平的因子，這轉換為看起來像 Table 13.13 的對比矩陣。\n\n\n\n\n\nTable 13.13:  具有5個水平的 helmert 對比矩陣 \n\n1-1-1-1-1\n\n21-1-1-1\n\n302-1-1\n\n4003-1\n\n50004\n\n\n\n\n\nHelmert 對比的一個有用之處是每個對比都加起來為零（即，所有列加起來為零）。這導致了當我們將 ANOVA 解釋為回歸時，如果我們使用 Helmert 對比，截距項對應於大平均數 \\(\\mu_{..}\\)。將其與治療對比進行比較，在治療對比中，截距項對應於基線類別的組平均值。這個性質在某些情況下可能非常有用。如果您有一個平衡設計，這並不太重要，到目前為止，我們一直在這樣假設，但是當我們考慮不平衡設計時，它將變得重要。事實上，我甚至麻煩包括這一部分的主要原因是，如果您想了解不平衡 ANOVA，對比變得很重要。\n\n\n\n13.7.3 簡單比較\n第三個選項是我應該簡要提及的 “總和至零” 對比，在 jamovi 中稱為 “簡單” 對比，它們用於構建組間的兩兩比較。具體來說，每個對比都編碼了某個組與基線類別之間的差異，這種情況下基線類別對應於第一組 (Table 13.14)。\n\n\n\n\n\nTable 13.14:  具有5個水平的 ‘總和至’ 零對比矩陣 \n\n1-1-1-1-1\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\n與 Helmert 對比非常相似，我們看到每一列的和都是零，這意味著當 ANOVA 被視為回歸模型時，截距項對應於整體平均值。在解釋這些對比時，需要認識到的是，每個對比都是第1組與其他四個組之間的兩兩比較。具體來說，對比1對應於 “第2組減去第1組” 的比較，對比2對應於 “第3組減去第1組” 的比較，依此類推。7\n\n\n\n13.7.4 jamovi的各種比較選項\njamovi 還提供了多種可以在 ANOVA 中生成不同類型對比的選項。這些可以在主要的 ANOVA 分析窗口的 ‘對比’ 選項中找到，其中 Table 13.15 列出了對比類型：\n\n\n\n\n\nTable 13.15:  在 jamovi ANOVA 分析中可用的對比類型 \n\nContrast type\n\nDeviationCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\nSimpleLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\nDifferenceCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\nHelmertCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\nRepeatedCompares the mean of each level (except the last) to the mean of the subsequent level\n\nPolynomialCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "href": "13-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "title": "13  多因子變異數分析",
    "section": "13.8 事後檢定",
    "text": "13.8 事後檢定\n現在轉換到另一個主題。而不是使用對比測試您已經計劃好的比較，假設您已經完成了 ANOVA，結果發現您獲得了一些顯著的效果。因為 F 檢驗是“全面”檢驗，它們實際上只測試各組之間沒有差異的虛無假設，所以獲得顯著效果並不能告訴你哪些組與其他組有所不同。我們在 Chapter 12 中討論了這個問題，並且在那章節中，我們的解決方案是對所有可能的組對執行 t 檢驗，對多重比較（例如，Bonferroni、Holm）進行校正，以控制所有比較中的 I 類型誤差率。我們在 Chapter 12 中使用的方法具有相對簡單的優點，並且可以在您在測試多個假設的多種不同情況下使用，但它們並非在 ANOVA 背景下進行有效的事後檢驗的最佳選擇。統計文獻中有很多用於執行多重比較的方法(Hsu, 1996)，本書將超出範疇，無法詳細討論所有這些方法。\n話雖如此，有一個工具我想引起您的注意，那就是 Tukey 的 “誠實顯著差異”，簡稱Tukey’s HSD。這次，我將不提供公式，只講解質性思路。Tukey’s HSD 的基本思想是檢查所有相關的組之間的成對比較，而且只有在您對成對差異感興趣時，使用 Tukey’s HSD 才合適。8 例如，前面我們使用 clinicaltrial.csv 數據集進行了因子 ANOVA，並指定了藥物的主要作用和治療的主要作用，我們對以下四種比較感興趣：\n\n給予 Anxifree 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Joyzepam 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Anxifree 的人與給予 Joyzepam 的人的情緒提升之間的差異。\n接受 CBT 治療的人與未接受治療的人的情緒提升之間的差異。\n\n對於這些比較中的任何一個，我們都對（群體）組平均值之間的真實差異感興趣。Tukey 的 HSD 會為這四種比較構建同時置信區間。我們所說的 95% “同時”置信區間是指，如果我們重複這個研究很多次，那麼在 95% 的研究結果中，置信區間將包含相關的真實值。此外，我們可以使用這些置信區間計算任何特定比較的校正 p 值。\n在 jamovi 中使用 TukeyHSD 函數非常容易。您只需指定要為其執行事後檢驗的 ANOVA 模型項。例如，如果我們要為主效應進行事後檢驗，但不考慮交互作用，我們將在 ANOVA 分析屏幕中打開“事後檢驗”選項，將藥物和治療變量移到右側的框中，然後在可能的事後校正列表中選中“Tukey”複選框。這與相應的結果表在 Figure 13.24 中顯示。\n\n\n\n\n\n\n\nFigure 13.24: 不帶交互作用的 jamovi 因子 ANOVA 中的 Tukey HSD 事後檢驗\n\n\n\n\n在「事後檢驗」結果表中顯示的輸出非常直觀。例如，第一個比較是Anxifree與安慰劑之間的差異，輸出的第一部分顯示組均值之間的觀察差異為0.27。接下來的數字是差異的標準誤，如果我們想要，我們可以根據此計算出95%置信區間，儘管jamovi目前尚不提供此選項。然後有一列是自由度，一列是t值，最後一列是p值。對於第一個比較，調整後的p值為0.21。相比之下，如果您看下一行，我們會發現安慰劑和Joyzepam之間的觀察差異為1.03，並且這個結果顯著（p < .001）。\n到目前為止，一切都很好。那麼，如果您的模型包括交互作用項怎麼辦？例如，jamovi中的默認選項是允許藥物和療法之間存在交互作用。如果是這樣，我們需要考慮的兩兩比較數量開始增加。像以前一樣，我們需要考慮與藥物主效應相關的三個比較以及與療法主效應相關的一個比較。但是，如果我們要考慮顯著交互作用的可能性（並嘗試找到支持這一顯著交互作用的組差異），我們需要包括以下比較：\n\n使用Anxifree並接受CBT治療的人的情緒增益與使用安慰劑並接受CBT治療的人的情緒增益之間的差異\n使用Anxifree並不接受治療的人與使用安慰劑並不接受治療的人的情緒增益之間的差異。\n等等\n\n您需要考慮相當多的這些比較。因此，當我們對此ANOVA模型運行Tukey事後分析時，我們會發現它進行了很多兩兩比較（共19個），如@fig-fig13-25所示。您會發現它看起來與之前非常相似，但進行了更多的比較。\n\n\n\n\n\n\n\nFigure 13.25: jamovi因子ANOVA中的Tukey HSD事後檢驗，包含交互作用項"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "href": "13-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "title": "13  多因子變異數分析",
    "section": "13.9 事前檢定方法",
    "text": "13.9 事前檢定方法\n延續前面關於ANOVA中對比和事後檢驗的部分，我認為預先設定的比較方法（planned comparisons）非常重要，值得簡要討論。在前面的章節以及@sec-Comparing-several-means-one-way-ANOVA中對多重比較的討論中，我們假設你想運行的檢驗確實是事後檢驗。例如，在上面的藥物示例中，可能你認為這些藥物對情緒的影響各有不同（即你假設藥物有主效應），但你沒有關於它們如何不同的具體假設，也沒有任何真正的想法關於哪些兩兩比較值得觀察。如果是這樣，那麼你確實需要使用Tukey的HSD來進行兩兩比較。\n然而，情況會有所不同，如果你真的有關於哪些比較感興趣的確切、具體的假設，而且你絕對無意觀察除了提前指定的那些比較以外的任何其他比較。當這是真的，並且如果你真誠並嚴格地堅持不進行任何其他比較的高尚意圖（即使數據看起來似乎對你沒有假設的東西顯示出非常顯著的效應），那麼使用像Tukey的HSD這樣的方法並不合理，因為它對一整套你從未關心過，也從未打算觀察的比較進行了矯正。在這種情況下，你可以安全地運行有限數量的假設檢驗，而無需對多重檢驗進行調整。這種情況被稱為預先設定的比較方法，有時用於臨床試驗。然而，進一步的考慮超出了這本入門書的範疇，但至少你知道這種方法是存在的！"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#不平衡的因子設計分析",
    "href": "13-Factorial-ANOVA.html#不平衡的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.10 不平衡的因子設計分析",
    "text": "13.10 不平衡的因子設計分析\n因子ANOVA是一個非常方便的工具。它已經成為分析實驗數據的標準工具之一，已有數十年的歷史，你會發現在心理學上，你無法閱讀超過兩三篇論文而不在其中某個地方遇到ANOVA。然而，在很多真實的科學文章中，你將會看到的ANOVA和我迄今為止描述的ANOVA之間有一個巨大的差異。在現實生活中，我們很少有幸擁有完美平衡的設計。出於某種原因，通常會在某些單元中得到比其他單元更多的觀察結果。換句話說，我們有一個不平衡的設計。\n不平衡設計需要比平衡設計更加謹慎地處理，支撐它們的統計理論也更為混亂。這種混亂可能是結果，也可能是時間短缺，但我的經驗是心理學本科研究方法課程有一個令人討厭的傾向，那就是完全忽略這個問題。很多統計教科書也容易忽略它。我認為，這導致了很多領域內的在職研究人員實際上並不知道存在幾種不同類型的不平衡ANOVA，而它們產生的答案相差很大。事實上，閱讀心理學文獻時，我對大多數報告不平衡因子ANOVA結果的人實際上無法提供足夠的詳細信息來重現分析感到驚訝。我暗自懷疑，大多數人甚至沒有意識到他們的統計軟件包正在代替他們做出大量實質性的數據分析決策。當你想到這一點時，它實際上是有點恐怖的。因此，如果你想避免將數據分析的控制權交給愚蠢的軟件，請繼續閱讀。\n\n\n13.10.1 咖啡飲用資料\n跟往常一樣，使用一些數據將對我們有所幫助。coffee.csv 文件包含了一個產生不平衡 \\(3 \\times 2\\) ANOVA 的假設數據集。假設我們有興趣了解人們在喝太多咖啡時胡言亂語的趨勢是否純粹是咖啡本身的影響，還是人們在咖啡中加入牛奶和糖所產生的影響。假設我們找了18個人，給他們喝了一些咖啡。咖啡/咖啡因的含量保持恆定，我們改變是否加入牛奶，所以牛奶是一個有兩個水平的二元因子，分別是“是”和“否”。我們還改變了涉及的糖的種類。咖啡中可能含有“真正”的糖，或者可能含有“假”的糖（即人工甜味劑），或者可能根本不含糖，所以糖變量是一個有三個水平的因子。我們的結果變量是一個連續變量，這可能意味著某個心理上有意義的衡量某人“胡言亂語”程度的指標。對於我們的目的，細節並不真正重要。查看 jamovi 試算表視圖中的數據，如 Figure 13.26。\n\n\n\n\n\n\nFigure 13.26: coffee.csv數據集在 jamovi 中，描述性信息按因子水平匯總\n\n\n\n\n查看 Figure 13.26 中的平均值表，我們可以強烈感受到各組之間的差異。與 babble 變量的標準偏差相比，這一點尤其明顯。在各組中，這個標準偏差從 .14 到 .71 不等，這相對於組間平均值的差異來說相當小。9 雖然這一開始看起來像是一個簡單的因子ANOVA，但當我們查看每個組中有多少個觀察值時，問題就出現了。參見 Figure 13.26 中顯示的不同組別的不同 N 值。這違反了我們最初的假設，即每個組中的人數是相同的。我們還沒有真正討論如何處理這種情況。\n\n\n\n\n13.10.2 不平衡設計不適用「標準變異數分析」\n不平衡設計讓我們發現，實際上並不存在我們可能稱之為標準 ANOVA 的任何一種事物。事實上，你可能希望在不平衡設計中以三種根本不同的方式 10來運行 ANOVA。如果您有一個平衡設計，這三個版本都會產生相同的結果，與我在本章開始時給出的公式一致的平方和、F 值等。然而，當您的設計不平衡時，它們的答案並不相同。此外，它們對於每種情況的適用程度也不盡相同。有些方法對您的情況可能更適用。鑒於此，了解不同類型的 ANOVA 及其相互之間的差異非常重要。\n第一種 ANOVA 通常被稱為第一類平方和。我敢肯定你能猜到其他兩個叫什麼。名稱中的“平方和”部分是由 SAS 統計軟件包引入的，並已成為標準術語，但在某些方面有點誤導。我認為把它們稱為不同類型的平方和的邏輯是，當你看到它們產生的 ANOVA 表時，數字之間的關鍵區別是 SS 值。自由度沒有變化，MS 值仍然被定義為 SS 除以 df 等。然而，這種術語的錯誤之處在於它掩蓋了 SS 值之間為何會有差異的原因。為此，了解三種不同類型的 ANOVA 作為三種不同的假設檢驗策略要有幫助得多。這些不同的策略確實導致了不同的 SS 值，但這裡重要的是策略，而不是 SS 值本身。回想一下[ANOVA 作為線性模型]一節，任何特定的 F 檢驗最好被認為是兩個線性模型之間的比較。因此，當您查看 ANOVA 表時，請記住每個 F 檢驗對應於要比較的模型對。當然，這自然引出了要比較哪對模型的問題。這就是 ANOVA 類型 I、II 和 III 之間的根本區別：每一種都對應於為檢驗選擇模型對的不同方式。\n\n\n\n\n13.10.3 第一型平方差總和\n類型 I 方法有時被稱為”序列”平方和，因為它涉及一次添加一個術語到模型的過程。以 coffee 數據為例。假設我們要運行完整的 \\(3 \\times 2\\) 因子 ANOVA，包括交互作用術語。完整的模型包含了結果變量 babble，預測變量 sugar 和 milk，以及交互術語 sugar \\(\\times\\) milk。這可以寫成 \\(babble \\sim sugar + milk + sugar {\\times} milk\\)。類型 I 策略會按順序構建這個模型，從最簡單的模型開始，逐步添加術語。\n數據的最簡單模型將是一個假設牛奶和糖都不影響胡言亂語的模型。這樣的模型只包括截距，寫成 babble ~ 1。這是我們最初的零假設。數據的下一個最簡單模型將是其中只包含兩個主效應之一的模型。在 coffee 數據中，這裡有兩個不同的可能選擇，因為我們可以選擇先添加 milk 或者先添加 sugar。實際上，順序是很重要的，我們稍後會看到，但現在讓我們只是隨意選擇一個，選擇 sugar。所以，我們模型序列中的第二個模型是 babble ~ sugar，它形成了我們第一次檢驗的替代假設。我們現在有了我們的第一個假設檢驗（Table 13.16）。\n\n\n\n\n\nTable 13.16:  使用結果變量 ‘babble’ 的零假設和替代假設。 \n\nNull model:\\(babble \\sim 1\\)\n\nAlternative model:\\(babble \\sim  sugar\\)\n\n\n\n\n\n這個比較形成了我們對糖的主效應的假設檢驗。我們模型建立練習的下一步是添加另一個主效應術語，因此我們序列中的下一個模型是 babble ~ sugar + milk。然後，通過比較以下模型對（Table 13.17）形成第二個假設檢驗。\n\n\n\n\n\nTable 13.17:  使用結果變量 ‘babble’ 的進一步零假設和替代假設 \n\nNull model:\\(babble \\sim  sugar\\)\n\nAlternative model:\\(babble \\sim  sugar + milk\\)\n\n\n\n\n\n這個比較形成了我們對牛奶主效應的假設檢驗。在某種意義上，這種方法非常優雅：第一次測試的替代假設形成了第二次測試的零假設。正是在這個意義上，類型 I 方法是嚴格有序的。每個測試都直接基於上一個測試的結果。然而，在另一個意義上，它非常不優雅，因為這兩個測試之間有很強的不對稱性。糖的主效應測試（第一次測試）完全忽略了牛奶，而牛奶的主效應測試（第二次測試）確實考慮了糖。無論如何，我們序列中的第四個模型現在是完整的模型，babble ~ sugar + milk + sugar \\(\\times\\) milk，相應的假設檢驗顯示在 Table 13.18 中。\n\n\n\n\n\nTable 13.18:  使用結果變量 ‘babble’ 的更多可能的零假設和替代假設 \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk + sugar * milk \\)\n\n\n\n\n\n類型 III 平方和是 jamovi ANOVA 使用的默認假設檢驗方法，因此要運行類型 I 平方和分析，我們必須在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘平方和’ 選擇框中選擇 ‘Type 1’。這給我們提供了 Figure 13.27 中顯示的 ANOVA 表。\n\n\n\n\n\n\n\nFigure 13.27: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表\n\n\n\n\n使用類型 I 平方和的最大問題是它確實取決於您輸入變量的順序。然而，在許多情況下，研究人員無需優先考慮一種排序而不考慮另一種排序。這可能是我們牛奶和糖問題的情況。我們應該先添加牛奶還是糖？在數據分析問題中，這與製作咖啡問題一樣隨意。事實上，可能有一些人對排序有堅定的看法，但很難想像這個問題有一個原則性的答案。然而，當我們改變順序時，看看會發生什麼，如 Figure 13.28 中所示。\n\n\n\n\n\n\nFigure 13.28: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表，但因子以不同的順序（首先是牛奶）輸入\n\n\n\n\n兩個主效應術語的 p 值都發生了變化，而且變化相當大。在其他方面，牛奶的效果已經顯著（儘管我之前提到過，人們應該避免對此得出任何強烈的結論）。應該報告這兩個 ANOVA 中的哪一個？這並不是立即明顯的。\n當您查看用於定義“第一”主效應和“第二”主效應的假設檢驗時，很明顯它們之間有質的不同。在我們最初的示例中，我們看到了糖的主效應檢驗完全忽略了牛奶，而牛奶的主效應檢驗確實考慮了糖。因此，類型 I 檢驗策略確實將第一個主效應視為在第二個主效應之上具有某種理論優越性。根據我的經驗，很少（甚至從未）有這種理論優越性，可以證明將任何兩個主效應非對稱地對待。\n所有這些的結果是類型 I 測試很少有趣，所以我們應該繼續討論類型 II 測試和類型 III 測試。\n\n\n\n13.10.4 第三型平方差總和\n剛剛談完類型 I 測試後，您可能會認為接下來自然要談論類型 II 測試。然而，我認為在談論類型 II 測試（比較棘手）之前，先討論類型 III 測試（簡單且是 jamovi ANOVA 的默認設置）實際上更自然。類型 III 測試背後的基本思想非常簡單。無論您要評估哪個術語，都要運行 F 檢驗，其中替代假設對應用戶指定的完整 ANOVA 模型，而零模型僅刪除您正在測試的那一個術語。例如，在咖啡示例中，我們的完整模型是 babble ~ sugar + milk + sugar × milk，糖的主效應檢驗將對應於以下兩個模型之間的比較（Table 13.19）。\n\n\n\n\n\nTable 13.19:  以’babble’作為結果變量的零假設和替代假設，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  milk + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n同樣，通過將完整模型與刪除牛奶術語的零模型進行檢驗，可以評估牛奶的主效應，如 Table 13.20。\n\n\n\n\n\nTable 13.20:  以’babble’作為結果變量的更多零假設和替代假設，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  sugar + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n最後，用完全相同的方式評估糖 × 牛奶的交互項。再次，我們將完整模型與刪除糖 × 牛奶交互項的零模型進行比較，如 Table 13.21。\n\n\n\n\n\nTable 13.21:  從以“babble”為結果變量的假設中刪除交互項，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n基本思想可以推廣到更高階 ANOVA。例如，假設我們嘗試運行一個具有三個因素 A、B 和 C 的 ANOVA，並且我們希望考慮所有可能的主效應和所有可能的交互作用，包括三者之間的交互作用 A × B × C。(Table 13.22)為您展示了這種情況下類型 III 測試的外觀。\n\n\n\n\n\nTable 13.22:  具有三個因素和所有主效應和交互項的類型 III 測試 \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB\\(A + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\nC\\(A + B + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B\\(A + B + C + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*C\\(A + B + C + A*B + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB*C\\(A + B + C + A*B + A*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n儘管該表看起來很難看，但它相當簡單。在所有情況下，替代假設對應於包含三個主效應術語（例如 A）、三個雙向交互作用（例如 A * B）和一個三向交互作用（即 A * B * C）的完整模型。零模型總是包含其中 7 項中的 6 項，而缺少的一項就是我們正在嘗試測試其顯著性的那一項。\n初步看來，類型 III 測試似乎是一個好主意。首先，我們消除了在運行類型 I 測試時遇到問題的不對稱性。而且，由於我們現在以相同的方式對待所有術語，假設測試的結果不依賴於我們指定它們的順序。這絕對是一件好事。然而，在解釋測試結果，特別是主效應術語時存在一個大問題。考慮咖啡數據。假設根據類型 III 測試，牛奶的主要效果並不顯著。這告訴我們的是，相比於完整模型，babble ~ sugar + sugar * milk 是數據的更好模型。但這意味著什麼呢？如果糖 * 牛奶的交互項也不顯著，我們會想得出結論說數據告訴我們唯一重要的事情是糖。但是，假設我們有一個顯著的交互項，但是牛奶的主效應不顯著。在這種情況下，我們是否應該認為真的有一個“糖的效果”，一個“牛奶和糖之間的交互作用”，但沒有“牛奶的效果”？那看起來很瘋狂。正確的答案簡直一定是，在交互作用顯著的情況下，談論主效應是毫無意義的11。一般來說，這似乎是大多數統計學家給我們的建議，而我認為這是正確的建議。但是，如果談論存在顯著交互作用的非顯著主效應確實是毫無意義的，那麼類型 III 測試應該允許零假設依賴於一個包括交互作用但省略了其中一個主效應的模型就不是很明顯了。以這種方式表述的零假設實際上根本沒有什麼意義。\n稍後，我們將看到類型 III 測試在某些情況下可以挽救，但首先讓我們看一下使用類型 III 和平方的 ANOVA 結果表，見 Figure 13.29。\n\n\n\n\n\n\n\nFigure 13.29: 在 jamovi 中使用類型 III 和平方的 ANOVA 結果表\n\n\n\n\n但要注意，類型 III 測試策略的一個怪異特徵是，通常結果依賴於用於編碼因子的對比（如果您忘記了不同類型對比是什麼，請參見 [指定對比的不同方式]一節）。12\n那麼，如果類型 III 分析（但不是在 jamovi 中）通常產生的 p 值對對比的選擇非常敏感，那是否意味著類型 III 測試本質上是任意的，不值得信任？在某種程度上，這是真的，當我們轉向討論類型 II 測試時，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n在類型 II 測試中，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n\n\n\n13.10.5 第二型平方差總和\n好的，到目前為止，我們已經看過了類型 I 和 III 測試，兩者都非常簡單。類型 I 測試是通過逐一添加條款進行的，而類型 III 測試是通過使用完整模型並檢查在刪除每個條款時會發生什麼來執行的。然而，這兩者都可能有一些局限性。類型 I 測試取決於您輸入條款的順序，而類型 III 測試則取決於您如何編碼對比。類型 II 測試描述起來稍微困難一些，但它們避免了這兩個問題，因此解釋起來稍微容易一些。\n類型 II 測試與類型 III 測試大致相似。從一個“完整”模型開始，通過從該模型中刪除特定條款來進行測試。然而，類型 II 測試是基於邊際性原則的，該原則規定如果您的模型中有任何依賴於較低階條款的較高階條款，則不應從模型中省略較低階條款。所以，例如，如果您的模型包含兩個因素的交互作用 A × B（二階條款），那麼它確實應該包含主效應 A 和 B（一階條款）。同樣，如果它包含三個因素的交互作用項 A × B × C，那麼模型還必須包括主效應 A、B 和 C 以及簡單的交互作用 A × B、A × C 和 B × C。類型 III 測試經常違反邊際性原則。例如，考慮在包含所有可能交互作用條款的三因子 ANOVA 中測試 A 的主效應。根據類型 III 測試，我們的零假設和對立假設在 Table 13.23 中。\n\n\n\n\n\nTable 13.23:  類型 III 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試 \n\nNull model:\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\n注意，零假設省略了 A，但將 A × B、A × C 和 A × B × C 作為模型的一部分。根據類型 II 測試，這並不是一個好的零假設選擇。相反，如果我們希望檢驗 A 對結果無關的零假設，我們應該指定一個不依賴於 A 的任何形式（即使是交互作用）的最復雜的模型。對立假設對應於該零模型加上 A 的主效應項。這個概念更接近大多數人對 “A 的主效應” 的直觀理解，並且得出了 A 的主效應的類型 II 測試（Table 13.24）。13\n\n\n\n\n\nTable 13.24:  類型 II 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試 \n\nNull model:\\(outcome \\sim B + C + B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\n無論如何，僅為了讓您了解類型 II 測試是如何進行的，這是在三因子因素分析中應用的完整測試表格（Table 13.25）：\n\n\n\n\n\n\nTable 13.25:  三因子因素模型的類型 II 測試 \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + B*C \\)\\(A + B + C + B*C \\)\n\nB\\(A + C + A*C \\)\\(A + B + C + A*C\\)\n\nC\\(A + B + A*B \\)\\(A + B + C + A*B\\)\n\nA*B\\(A + B + C + A*C + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*C\\(A + B + C + A*B + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nB*C\\(A + B + C + A*B + A*C \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n在我們一直在咖啡數據中使用的雙因子 ANOVA 的背景下，假設檢驗更為簡單。糖的主效應對應於比較這兩個模型的 F 檢驗（Table 13.26）。\n\n\n\n\n\nTable 13.26:  咖啡數據中糖主效應的類型 II 測試 \n\nNull model:\\(babble \\sim milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n對牛奶主效應的檢驗位於 Table 13.27。\n\n\n\n\n\nTable 13.27:  咖啡數據中牛奶主效應的類型 II 測試 \n\nNull model:\\(babble \\sim  sugar \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n最後，糖與牛奶交互作用的檢驗位於 Table 13.28。\n\n\n\n\n\nTable 13.28:  以類型 II方法分析糖與牛奶交互作用 \n\nNull model:\\(babble \\sim  sugar + milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk  + sugar*milk \\)\n\n\n\n\n\n運行測試再次很簡單。只需在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘Sum of squares’ 選擇框中選擇 ‘Type 2’，這將給我們提供 ANOVA 表格，如 Figure 13.30 所示。\n\n\n\n\n\n\n\nFigure 13.30: 在 jamovi 中使用類型 II 平方和的 ANOVA 結果表\n\n\n\n\n類型 II 測試比類型 I 和類型 III 測試具有一些明顯的優勢。它們不依賴於指定因子的順序（與類型 I 不同），也不依賴於用於指定因子的對比（與類型 III 不同）。雖然對於最後一點意見可能會有所不同，而且這肯定取決於你想用你的數據做什麼，但我認為它們指定的假設檢驗更有可能對應於你真正關心的事物。因此，我發現解釋類型 II 測試的結果通常比解釋類型 I 或類型 III 測試的結果更容易。基於這個原因，我的初步建議是，如果您無法想出任何直接映射到研究問題的明顯模型比較，但仍想在不平衡設計中運行 ANOVA，類型 II 測試可能是比類型 I 或類型 III 更好的選擇。14\n\n\n\n13.10.6 效果量(還有非加成性平方差總和)\njamovi 在您選擇這些選項時還會提供效應大小 \\(\\eta^2\\) 和部分 \\(\\eta^2\\)，如 Figure 13.30。然而，在不平衡設計中，涉及的額外複雜性有所增加。\n如果您回顧我們對 ANOVA 的早期討論，其中一個關鍵想法是在平方和計算的背後，如果我們把所有與模型中的效應相關的 SS 項加起來，再加上殘差 SS，它們應該加起來等於總平方和。此外，\\(\\eta^2\\) 背後的整個想法是，因為您將某個 SS 項除以總 SS 值，\\(\\eta^2\\) 值可以解釋為由特定項解釋的變異比例。但在不平衡設計中，這並不那麼簡單，因為有些變異會”丟失”。\n起初這似乎有點奇怪，但原因是這樣的。當您擁有不平衡設計時，您的因子會相互關聯，因此很難分辨因子 A 的效應和因子 B 的效應。在極端情況下，假設我們進行了一個 \\(2 \\times 2\\) 設計，每個組中的參與者人數如 Table 13.29。\n\n\n\n\n\nTable 13.29:  2 x 2 非常（非常！）不平衡的因子設計中的 N 參與者 \n\nsugarno sugar\n\nmilk1000\n\nno milk0100\n\n\n\n\n\n在這裡，我們有一個非常不平衡的設計：100人有牛奶和糖，100人沒有牛奶和糖，就是這樣。有0人有牛奶沒有糖，也有0人有糖沒有牛奶。現在假設，當我們收集數據時，發現”牛奶和糖”組與”無牛奶無糖”組之間存在很大（並且具有統計顯著性）的差異。這是糖的主要效應嗎？牛奶的主要效應？還是交互作用？這是不可能知道的，因為糖的存在與牛奶的存在具有完美的關聯。現在假設設計稍微平衡一些（Table 13.30）。\n\n\n\n\n\nTable 13.30:  2 x 2 仍然非常不平衡的因子設計中的 N 參與者 \n\nsugarno sugar\n\nmilk1005\n\nno milk5100\n\n\n\n\n\n這次，從技術上講，可以區分牛奶和糖的效應，因為我們有一些人只擁有其中之一。然而，由於糖和牛奶之間的關聯仍然非常強烈，兩個小組中的觀察值非常少，因此進行區分仍然相當困難。再次，我們很可能處於這樣一種情況：我們知道預測變量（牛奶和糖）與結果（發聲）相關，但我們不知道這種關係的本質是一個或另一個預測因子的主要效應，還是交互作用。\n在不平衡設計的背景下，當糖和牛奶的效應相互關聯時，應該選擇 Type II ANOVA 進行分析，因為它更易於解釋並可以避免因變量順序或對比方式帶來的問題。無論您是使用什麼軟件，都應該在報告結果時說明您使用的是哪種類型的 ANOVA 測試，以確保分析結果的準確性。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#本章小結",
    "href": "13-Factorial-ANOVA.html#本章小結",
    "title": "13  多因子變異數分析",
    "section": "13.11 本章小結",
    "text": "13.11 本章小結\n\n平衡且無交互作用的因子設計分析以及有交互作用因子設計分析\n因子設計變異數分析的效果量估計平均值以及信賴區間。\n檢核變異數分析的執行條件\n共變數分析 (ANCOVA)\n變異數分析就是線性模型還有各種多重比較方案\n事後檢定談到杜凱氏HSD，還有提到規劃使用事前檢定方法要思考的條件。\n不平衡的因子設計分析\n\n\n\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A second-level course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall."
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方適合度檢定",
    "href": "14-Categorical-data-analysis.html#卡方適合度檢定",
    "title": "14  類別資料分析",
    "section": "14.1 卡方適合度檢定",
    "text": "14.1 卡方適合度檢定\n卡方適合度檢定是最古老的假設檢定之一。它由 Karl Pearson 在上世紀初發明 (Pearson, 1900)，稍後由 Sir Ronald Fisher (Fisher, 1922) 進行了一些修正。它檢驗名義變量的觀察頻率分佈是否符合預期的頻率分佈。例如，假設一組病人正在接受實驗性治療，並對他們的健康狀況進行評估，以了解他們的病情是否有改善、保持不變或惡化。適合度檢定可以用來確定每個類別中的數字 - 改善、無變化、惡化 - 是否與標準治療選項預期的數字相匹配。讓我們用一些心理學研究案例來更深入認識。\n\n\n14.1.1 撲克牌花色T恤銷售數據\n多年來，許多研究表明人類很難模擬隨機性。儘管我們努力以“隨機”方式行事，但我們以模式和結構進行思考，因此，當被要求“隨機做某事”時，人們實際上做的事情完全不是隨機的。因此，研究人類隨機性（或者可能是非隨機性）揭示了關於我們如何看待世界的許多深刻心理問題。考慮到這一點，讓我們思考一個非常簡單的研究。假設我要求人們想像一副洗牌的牌，然後在這副想象中的牌中“隨機”選擇一張牌。在他們選擇一張牌之後，我要求他們心理上選擇第二張牌。對於兩個選擇，我們要看的是人們選擇的花色（紅心、梅花、黑桃或方塊）。在問了，例如，\\(N = 200\\)個人之後，我想查看數據並確定人們假裝選擇的牌是否真的隨機。數據包含在 randomness.csv 文件中，當你用 jamovi 打開它並查看電子表格視圖時，你會看到三個變量。這些是：為每個參與者分配唯一標識符的 id 變量，以及表示人們選擇的紙牌花色的兩個變量 choice_1 和 choice_2。\n目前，讓我們只專注於人們做出的第一個選擇。我們將使用“探索” - “描述性”下的頻率表選項來計算我們觀察到的人們選擇每個花色的次數。我們得到了這個（Table 14.1）：\n\n\n\n\n\n\nTable 14.1:  選擇每個花色的次數 \n\nclubsdiamondsheartsspades\n\n35516450\n\n\n\n\n\n這個小頻率表非常有幫助。看著它，人們可能比選擇梅花更容易選擇紅心的暗示有點明顯，但僅僅看它並不完全明顯，這是真的還是僅僅是巧合。所以我們可能需要進行某種統計分析來找出答案，這就是我將在下一節談論的內容。\n很好。從這一點開始，我們將把這個表視為我們要分析的數據。然而，由於我將不得不用數學術語（抱歉！）談論這些數據，所以明確表示符號可能是個好主意。在數學符號中，我們將易讀的單詞“觀察”縮短為字母 \\(O\\)，並使用下標表示觀察的位置。所以我們表格中的第二個觀察用數學表示為 \\(O_2\\)。英語描述與數學符號之間的關係在 Table 14.2 中說明。\n\n\n\n\n\n\nTable 14.2:  英語描述與數學符號之間的關係 \n\nlabelindex, imath. symbolthe value\n\nclubs, \\( \\clubsuit \\)1\\( O_1 \\)35\n\ndiamonds, \\( \\diamondsuit \\)2\\( O_2 \\)51\n\nhearts, \\( \\heartsuit \\)3\\( O_3 \\)64\n\nspades, \\( \\spadesuit \\)4\\( O_4 \\)50\n\n\n\n\n\n希望這很清楚。同時值得注意的是，數學家更喜歡談論一般事物而不是具體事物，因此您還會看到符號 \\(O_i\\)，它指的是在第 i 類中的觀察數（其中 i 可能是 1，2，3 或 4）。最後，如果我們要引用所有觀察到的頻率，統計學家將所有觀察值組合成一個向量 2，我將其稱為 \\(O\\)。\n\\[O = (O_1, O_2, O_3, O_4)\\]\n同樣，這沒有什麼新鮮有趣的地方。這只是符號。如果我說 \\(O = (35, 51, 64, 50)\\)，我所做的就是描述觀察到的頻率表（即觀察到的），但我使用數學符號來表示它。\n\n\n\n\n14.1.2 虛無假設與對立假設\n正如上一節所示，我們的研究假設是“人們不會隨機選擇牌”。現在我們要做的是將其轉換為一些統計假設，然後構建對這些假設的統計檢驗。我將向您介紹的檢驗是 Pearson 的 \\(\\chi^2\\) (chi-square) 適合度檢驗，正如往往是這樣的情況，我們必須通過仔細構建零假設來開始。在這種情況下，這很容易。首先，讓我們用文字陳述零假設：\n\\[H_0: \\text{ 四種花色被選擇的概率相等}\\]\n現在，由於這是統計學，我們必須用數學方式表達相同的事情。為此，讓我們使用表示符號 \\(P_j\\) 來表示選擇第 j 種花色的真實概率。如果零假設成立，那麼四種花色中的每一種都有 25% 的概率被選中。換句話說，我們的零假設聲稱 \\(P_1 = .25\\)，\\(P_2 = .25\\)，\\(P_3 = .25\\)，最後 \\(P_4 = .25\\)。然而，正如我們可以將觀察到的頻率分組成一個向量 O 來概括整個數據集一樣，我們可以使用 P 來表示與我們零假設相對應的概率。所以，如果我讓向量 \\(P = (P_1, P_2, P_3, P_4)\\) 表示描述我們零假設的概率集合，那麼我們有：\n\\[H_0: P =(.25, .25, .25, .25)\\]\n在這個特定實例中，我們的零假設對應於一個概率向量 P，其中所有的概率彼此相等。但這不一定是這樣。例如，如果實驗任務是讓人們想像他們正在抽取一副牌，這副牌的梅花數量是其他花色的兩倍，那麼零假設將對應於像 \\(P = (.4, .2, .2, .2)\\) 這樣的東西。只要求概率都是正數，並且它們的總和為 1，那麼它就是零假設的合法選擇。然而，適合度檢驗最常用於檢驗所有類別等可能的零假設，所以我們將在我們的示例中堅持使用這個。\n那麼我們的對立假設 \\(H_1\\) 呢？我們真正感興趣的是證明所涉及的概率並不都相同（也就是說，人們的選擇不是完全隨機的）。因此，我們假設的“適合人類理解”的版本看起來像這樣：\n\\(H_0: \\text{ 四種花色被選擇的概率相等}\\) \\(H_1: \\text{ 至少有一個選擇花色的概率不是 0.25}\\)\n…以及“數學家友好”版本是：\n\n\\(H_0: P= (.25, .25, .25, .25)\\) \\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n14.1.3 適合度檢定統計程序\n在此階段，我們有我們觀察到的頻率 O 和一組與我們要測試的零假設相對應的概率 P。我們現在要做的是構造一個零假設的檢驗。與往常一樣，如果我們想要檢驗 \\(H_0\\) 和 \\(H_1\\)，我們將需要一個檢驗統計量。適合度檢驗所使用的基本技巧是構造一個檢驗統計量，用來衡量數據與零假設之間的“接近度”。如果數據與零假設為真時所期望的情況不相符，那麼它可能不是真的。好吧，如果零假設是真的，我們會看到什麼呢？或者，使用正確的術語，這些是什麼期望頻率。總共有 \\(N = 200\\) 次觀察，並且（如果零假設為真），任何一個選擇紅心的概率是 \\(P_3 = .25\\)，所以我猜我們期望 \\(200 \\times .25 = 50\\) 顆紅心，對吧？或者，更具體地說，如果我們讓 Ei 指的是“在零假設為真時，我們期望觀察到的第 i 類反應的數量”，那麼\n\\[E_i=N \\times P_i\\]\n這非常容易計算。如果有 200 個觀察可以分為四個類別，我們認為所有四個類別的可能性相等，那麼平均每個類別應該有 50 個觀察，對吧？\n那麼，我們如何將這個轉換為檢驗統計量呢？顯然，我們要做的是比較每個類別的期望觀察數量（\\(E_i\\)）與該類別的實際觀察數量（\\(O_i\\)）。基於這種比較，我們應該能夠得出一個好的檢驗統計量。首先，讓我們計算零假設期望我們找到的數量和我們實際找到的數量之間的差異，也就是，我們計算“觀察到的值減去期望值”的差分數，\\(O_i - E_i\\)。這在 Table 14.3 中說明。\n\n\n\n\n\nTable 14.3:  預期和觀察到的頻率 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)50505050\n\nobserved frequency \\( O_i\\)35516450\n\ndifference score \\( O_i-E_i\\)-151140\n\n\n\n\n\n因此，根據我們的計算，顯然人們選擇紅心的次數比零假設預測的多，而選擇梅花的次數則少。然而，稍作思考便會發現，這些原始差異並非我們所尋求的。直覺上，當零假設預測觀察次數過少（就像紅心那樣）時，它與預測觀察次數過多（就像梅花那樣）一樣糟糕。因此，對於梅花有負數，對於紅心有正數有點奇怪。解決這個問題的一個簡單方法是將所有數字平方，這樣我們現在就可以計算平方差，\\((E_i - O_i)^2\\)。就像以前一樣，我們可以手工計算（Table 14.4）。\n\n\n\n\n\nTable 14.4:  將差異得分平方 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n22511960\n\n\n\n\n\n現在我們取得了進展。現在我們有了一組數字，在零假設做出糟糕預測時（梅花和紅心），數字很大，但在做出好的預測時（方塊和黑桃），數字很小。接下來，由於我將在稍後解釋的一些技術原因，讓我們也將這些數字除以期望頻率 Ei，因此我們實際上計算的是 \\(\\frac{(E_i-O_i)^2}{E_i}\\)。由於我們例子中所有類別的 \\(E_i = 50\\)，所以這不是一個非常有趣的計算，但無論如何，讓我們這麼做（Table 14.5）。\n\n\n\n\n\nTable 14.5:  將平方差異得分除以期望頻率以提供一個’誤差’得分 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n4.500.023.920.00\n\n\n\n\n\n實際上，我們在這裡得到的是四個不同的“誤差”得分，每個都告訴我們在嘗試用零假設預測我們的觀察頻率時，空假設所犯的“錯誤”有多大。因此，為了將其轉換為有用的檢驗統計量，我們可以做的一件事就是將這些數字加起來。結果被稱為適合度統計量，通常表示為 \\(\\chi^2\\)（卡方）或 GOF。我們可以像 Table 14.6 中那樣計算它。\n\\[\\sum( (observed - expected)^2 / expected )\\]\n這給了我們一個 8.44 的值。\n[額外的技術細節 3]\n如我們從計算中看到的，在我們的卡片數據集中，我們得到了一個 \\(\\chi^2\\) = 8.44 的值。那麼現在的問題是，這個值是否足夠大以拒絕零假設？\n\n\n\n14.1.4 適合度檢定的樣本分佈\n要確定某個 \\(\\chi^2\\) 值是否足夠大，以便拒絕零假設，我們需要確定如果零假設為真，\\(\\chi^2\\) 的抽樣分佈會是什麼。因此，在本節中我將做的就是這件事。我將詳細向您展示這個抽樣分佈的構建方式，然後在下一節中使用它構建一個假設檢驗。如果你想跳過本節的其他部分，並且願意相信抽樣分佈是具有 \\(k - 1\\) 自由度的 \\(\\chi^2\\)（卡方）分佈，你可以跳過本節的其他部分。然而，如果您想了解為什麼適合度檢驗的工作方式是這樣的，請繼續閱讀。\n好吧，讓我們假設零假設實際上是真的。如果是這樣，那麼觀察值落入第 i 類的真實概率是 \\(P_i\\)。畢竟，這幾乎就是我們的零假設的定義。讓我們思考一下這實際上意味著什麼。這有點像說“自然”通過翻轉一個加權硬幣（即，得到正面的概率是 \\(P_j\\) ）來決定觀察值是否最終落入第 i 類。因此，我們可以將觀察到的頻率 \\(O_i\\) 想像成自然界翻轉了 N 個這樣的硬幣（數據集中每個觀察值各有一個），並且其中正好 \\(O_i\\) 個硬幣是正面朝上。顯然，這是一種非常奇怪的思考實驗的方式。但是，這樣做（希望）是提醒你我們之前其實已經見過這種情景。這與 Chapter 7 中 Section 7.4 部分所提到的設置完全相同。換句話說，如果零假設是真的，那麼我們的觀察頻率是由抽樣自二項分佈生成的：\n\\[O_i \\sim Binomial(P_i,N) \\]\n現在，如果您還記得我們對 Section 8.3.3 限定定理的討論，特別是當 \\(N\\) 較大且 \\(P_i\\) 與 0 或 1 不太接近時，二項分佈看起來幾乎與正態分佈相同。換句話說，只要 \\(N^P_i\\) 足夠大。或者，換句話說，當期望頻率 Ei 足夠大時，\\(O_i\\) 的理論分佈近似為正態分佈。更好的是，如果 \\(O_i\\) 是正態分佈的，那麼 \\((O_i-E_i)/\\sqrt{(E_i)}\\) 也是正態分佈的。因為 \\(E_i\\) 是一個固定值，所以減去 Ei 並除以？ Ei 改變了正態分佈的均值和標準差，但這就是它所做的全部。好吧，現在讓我們看看我們的適合度統計量實際上是什麼。我們正在做的是取一堆近似正態分佈的東西，將它們平方，然後加起來。等等。我們之前也見過這個！正如我們在談論@sec-Other-useful-distributions時所討論的那樣，當您取一堆具有標準正態分佈（即，均值為 0 且標準差為 1）的東西，將它們平方然後加起來時，所得到的數量具有卡方分佈。所以現在我們知道了零假設預測適合度統計量的抽樣分佈是卡方分佈。很酷。\n還有一個最後的細節要談論，即自由度。如果您回想一下@sec-Other-useful-distributions，我說過如果你加起來的東西數量是 k，那麼生成的卡方分佈的自由度就是 k。然而，在本節開始時我所說的是，卡方適合度檢驗的實際自由度是 \\(k - 1\\)。這是怎麼回事呢？答案在於，我們應該考慮的是被加在一起的真正獨立的東西數量。正如我將在下一節中談論的那樣，儘管我們加起來有 k 個東西，但只有 \\(k - 1\\) 個東西是真正獨立的，所以自由度實際上只有 \\(k - 1\\)。這就是下一節的主題4。\n\n\n\n14.1.5 自由度\n\n\n\n\n\nFigure 14.1: 卡方分佈具有不同“自由度”值\n\n\n\n\n當我在@sec-Other-useful-distributions中介紹卡方分佈時，對於“自由度”的含義有點籠統。顯然，它很重要。觀察@fig-fig10-1，可以看到如果我們改變自由度，那麼卡方分佈的形狀會發生很大變化。但它究竟是什麼呢？同樣，當我介紹分佈並解釋它與正態分佈的關係時，我確實提供了一個答案：它是我要平方並相加的“正態分布變量”的數量。但是，對於大多數人來說，這有點抽象，並不十分有幫助。我們真正需要做的是嘗試根據我們的數據來理解自由度。下面就讓我們開始吧。\n自由度背後的基本概念非常簡單。您計算它的方法是將描述數據的不同“數量”加起來，然後減去這些數據必須滿足的所有“約束”。5這有點籠統，所以讓我們用我們的撲克牌數據作為一個具體例子。我們使用四個數字來描述我們的數據，分別是 \\(O1, O2, O3\\) 和 \\(O4\\)，對應於四個不同類別（紅心，梅花，方塊，黑桃）的觀察頻率。這四個數字是我們實驗的隨機結果。但是我的實驗實際上內置了一個固定的約束：樣本大小 \\(N\\)。6也就是說，如果我們知道\n有多少人選擇紅心，有多少人選擇方塊，以及有多少人選擇梅花，那麼我們就能確切地知道有多少人選擇黑桃。換句話說，雖然我們的數據是用四個數字描述的，但它們實際上只對應於 \\(4 - 1 = 3\\) 个自由度。稍微不同的思考方式是注意到我們感興趣的四個概率（同樣，對應於四個不同類別），但是這些概率必須加起來等於一，這將施加一個約束。因此，自由度是 \\(4 - 1 = 3\\)。無論您想用觀察頻率還是概率的方式來思考它，答案都是一樣的。通常，當進行涉及 \\(k\\) 個組的卡方適合度檢定時，自由度將為 \\(k - 1\\)。\n\n\n\n14.1.6 檢定虛無假設\n構建假設檢定的過程的最後一步是找出拒絕域是什麼。也就是說，哪些 \\(\\chi^2\\) 值會讓我們拒絕零假設。如我們之前所見，\\(\\chi^2\\) 的大值意味著零假設在預測我們實驗中的數據方面做得很差，而 \\(\\chi^2\\) 的小值則意味著它實際上做得相當好。因此，一個相當明智的策略是說，有一個臨界值，如果 \\(\\chi^2\\) 大於臨界值，我們拒絕零假設；但如果 \\(\\chi^2\\) 小於這個值，我們保留零假設。換句話說，用我們在 Chapter 9 中引入的語言，卡方適合度檢定總是一個單邊檢定。好的，所以我們要做的就是找出這個臨界值。這很簡單。如果我們希望檢定具有顯著性水平 \\(\\alpha = .05\\)（即，我們願意容忍 Type I 錯誤率為 \\(5%\\)），那麼我們必須選擇我們的臨界值，使得在零假設成立的情況下，\\(\\chi^2\\) 達到那麼大的概率只有 5%。這在 Figure 14.2 中得到了說明。\n\n\n\n\n\n\nFigure 14.2: \\(\\chi^2\\)（卡方）適合度檢定的假設檢定如何運作的示意圖\n\n\n\n\n啊，但我聽到你問，如何找到具有 \\(k-1\\) 自由度的卡方分布的臨界值？很多很多年前，當我第一次上心理統計課時，我們曾經在一本臨界值表的書中查找這些臨界值，就像 Figure 14.3 中那樣。看這個圖，我們可以看到具有 3 個自由度的 \\(\\chi^2\\) 分布的臨界值，且 p=0.05 是 7.815。\n\n\n\n\n\n\n\nFigure 14.3: 卡方分布的臨界值表\n\n\n\n\n因此，如果我們計算出的 \\(\\chi^2\\) 統計量大於 7.815 的臨界值，那麼我們可以拒絕零假設（記住，零假設 \\(H_0\\) 是所有四個花色都以相等的概率被選擇）。既然我們之前已經計算過了（即，\\(\\chi^2\\) = 8.44），我們可以拒絕零假設。基本上就是這樣了。現在你知道了“皮爾森卡方適合度檢定”。真幸運。\n\n\n\n14.1.7 jamovi實作\n毫不意外地，jamovi 提供了一個分析工具，可以幫你完成這些計算。讓我們使用 Randomness.omv 文件。在主要的“分析”工具欄中，選擇“頻率” - “單樣本比例檢驗” - “\\(N\\) 個結果”。然後在出現的分析視窗中將要分析的變量（從選擇 1 開始）移到“變量”框中。此外，單擊“預期計數”復選框，以便將這些數據顯示在結果表中。完成所有這些操作後，你應該會在 jamovi 中看到分析結果，如 Figure 14.4。然後不出所料，jamovi 提供了與我們上面手動計算相同的預期計數和統計數據，\\(\\chi^2\\) 值為 \\((8.44\\)，自由度為 \\(3\\)，\\(p=0.038\\)。注意，我們不再需要查找臨界 p 值閾值，因為 jamovi 給出了在 \\(3\\) 自由度下計算得出的 \\(\\chi^2\\) 的實際 p 值。\n\n\n\n\n\n\n\nFigure 14.4: jamovi 中的 \\(\\chi^2\\) 單樣本比例檢驗，表格顯示觀察到的頻率和比例以及期望的頻率和比例\n\n\n\n\n\n\n14.1.8 另一種虛無假設\n此時，你可能會想知道如果你想進行適合度檢驗，但你的零假設不是所有類別的概率都相等該怎麼辦。例如，假設有人提出了這樣的理論預測，即人們應該以 \\(60\\%\\) 的概率選擇紅色牌，以 \\(40\\%\\) 的概率選擇黑色牌（我不知道為什麼你會這樣預測），但沒有其他偏好。如果是這樣，零假設將期望選擇愛心的比例為 \\(30\\%\\)，選擇方塊的比例為 \\(30\\%\\)，選擇黑桃的比例為 \\(20\\%\\)，選擇梅花的比例為 \\(20\\%\\)。換句話說，我們期望愛心和方塊的出現次數是黑桃和梅花的 1.5 倍（\\(30\\%\\) : \\(20\\%\\) 的比例與 1.5 : 1 相同）。對我來說，這似乎是一個愚蠢的理論，但是用我們的 jamovi 分析可以很容易地測試這個明確指定的零假設。在分析視窗中（標記為“比例檢驗（N個結果）”的 Figure 14.4 中，你可以展開“預期比例”的選項。當你這樣做時，將會出現一些選項，讓你為選定的變量輸入不同的比例值，在我們的案例中，這個變量是 choice 1。將比例更改為反映新的零假設，如 Figure 14.5 所示，並觀察結果如何變化。\n\n\n\n\n\n\nFigure 14.5: 在 jamovi 中更改 \\(\\\\chi^2\\) 單樣本比例檢驗的預期比例\n\n\n\n\n預期計數現在顯示在 Table 14.6 中。\n\n\n\n\n\nTable 14.6:  不同零假設的預期計數 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)40606040\n\n\n\n\n\n\\(\\chi^2\\) 統計量為 4.74，自由度為 3，\\(p = 0.182\\)。現在，我們更新的假設和預期頻率與上次的結果有所不同。因此，我們的 \\(\\chi^2\\) 檢驗統計量和 p 值也有所不同。令人惱火的是，p 值為 \\(.182\\)，因此我們不能拒絕零假設（回顧 Section 9.5 提醒自己為什麼）。可悲的是，儘管零假設對應著一個非常愚蠢的理論，這些數據並沒有提供足夠的證據來反駁它。\n\n\n\n14.1.9 適合度檢定的報告寫作\n現在你知道了這個測試的運作方式，也知道如何使用神奇的jamovi計算盒來進行測試。接下來你需要知道的是如何撰寫結果。畢竟，設計和執行實驗，然後分析數據，如果不告訴別人結果是沒有意義的！所以讓我們來談談在報告分析時需要做的事情。讓我們繼續以撲克牌花色為例。如果我想將這個結果寫成一篇論文之類的東西，那麼慣常的報告方式是這樣寫的：\n\n在實驗的200名參與者中，有64人首選紅心，51人選擇方塊，50人選擇黑桃，35人選擇梅花。進行了卡方適合度檢驗以測試四種花色的選擇概率是否相同。結果顯著（\\(\\chi^2(3) = 8.44, p< .05\\)），這表明人們在選擇花色時並非完全隨機。\n\n這相當直接，希望它看起來很不起眼。儘管如此，你應該注意到這個描述中的幾點內容：\n\n描述統計數據在統計檢驗之前。也就是說，在進行檢驗之前，我告訴讀者有關數據的一些信息。通常，這是一個很好的做法。永遠記住，你的讀者對你的數據了解得遠不如你。因此，除非你妥善地向他們描述，否則統計檢驗對他們來說毫無意義，他們會感到沮喪和哭泣。\n描述告訴你正在測試的虛無假設是什麼。老實說，作者並不總是這樣做，但在存在一定歧義的情況下，或者在你不能依賴你的讀者非常熟悉你正在使用的統計工具時，這通常是一個好主意。很多時候讀者可能不知道（或記不起）你正在使用的檢驗的所有細節，所以提醒他們是一種禮貌！對於適合度檢驗來說，你通常可以依賴科學觀眾了解它的運作方式（因為它涵蓋在大多數入門統計課程中）。然而，明確陳述虛無假設（簡要地！）仍然是一個好主意，因為虛無假設可能因你使用檢驗的目的而有所不同。例如，在撲克牌的例子中，我的虛無假設是四個花色的概率相同（即，\\(P1 = P2 = P3 = P4 = 0.25\\)），但這個假設並沒有什麼特別的。我可以同樣使用適合度檢驗測試虛無假設，即\\(P_1 = 0.7\\)和\\(P2 = P3 = P4 = 0.1\\)。所以，向讀者解釋你的虛無假設是有幫助的。另外，注意到我用文字而不是數學描述虛無假設。這是完全可以接受的。你可以用數學描述它，但是因為大多數讀者發現文字比符號更容易閱讀，所以大多數作者傾向於用文字描述虛無假設（如果可以的話）。\n包括”統計塊”。在報告檢驗結果本身時，我不僅僅說結果顯著，還包括了一個“統計塊”（即括號內密集的數學部分），其中報告了所有“關鍵”統計信息。對於卡方適應度檢驗，報告的信息包括檢驗統計量（即適應度統計量為8.44）、用於檢驗的分布信息（具有3個自由度的\\(\\chi^2\\)，通常縮寫為\\(\\chi^2\\)(3)），然後是結果是否顯著（在本例中為\\(p< .05\\)）。每個檢驗所需的統計塊中的特定信息各不相同，因此每次我介紹一個新檢驗時，我都會向您展示統計塊應該是什麼樣子。7 但是，一般原則是您應該始終提供足夠的信息，以便讀者在需要時可以自己檢查測試結果。\n對結果進行解釋。除了指出結果顯著之外，我還提供了結果的解釋（即，人們沒有隨機選擇）。這對讀者也是一種友善，因為它告訴他們關於數據中發生了什麼事的一些信息。如果不包括這樣的東西，讀者很難理解發生了什麼事。8\n\n正如其他所有事物一樣，你應該首要關注的是向讀者解釋事物。永遠記住，報告結果的目的是與另一個人溝通。我無法告訴您我看過多少次報告、論文甚至科學文章的結果部分就是胡言亂語，因為作者只關注確保包含所有數字，卻忘記了與人類讀者真正交流。\n\n撒旦在統計和引用經文中同樣感到高興9 – H.G. 威爾斯"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方獨立性檢定",
    "href": "14-Categorical-data-analysis.html#卡方獨立性檢定",
    "title": "14  類別資料分析",
    "section": "14.2 卡方獨立性檢定",
    "text": "14.2 卡方獨立性檢定\n\n警衛機器人 1：停！\n警衛機器人 2：你是機器人還是人類？\n莉娜：機器人…我們是。\n弗萊：呃，對！只是兩個機器人在機器人世界裡像機器人一樣生活！呃？\n警衛機器人 1：進行測試。\n警衛機器人 2：以下哪一個是你最喜歡的？A：一只小狗，B：來自你心上人的漂亮花朵，或C：一個大的、格式正確的數據文件？\n警衛機器人 1：選擇！\n《飛出個未來》第一季第5集”Fear of a Bot Planet”台詞(1999~2003的美國情境喜劇；台灣無代理商引進播映)\n\n有一天，我在觀看一部動畫紀錄片，研究Chapek 9星球上當地人的古怪風俗。顯然，要進入他們的首府，訪客必須證明他們是機器人而不是人類。為了確定訪客是否是人類，當地人會詢問訪客是喜歡小狗、花還是大型、格式正確的數據文件。我心想：“相當聰明，但如果人類和機器人有相同的喜好呢？那可能不是一個很好的測試了，對吧？”事實上，我得到了Chapek 9市民當局用來檢查這個問題的測試數據。他們所做的非常簡單。他們找了一堆機器人和一堆人，問他們喜歡什麼。我把他們的數據保存在一個名為chapek9.omv的文件中，現在我們可以將其加載到jamovi中。除了識別個人的ID變量外，還有兩個名義文本變量，species和choice。總共有180個條目在數據集中，每個人（將機器人和人類都算作“人”）被要求做出選擇。具體而言，有93個人類和87個機器人，絕大多數人選擇了數據文件。你可以通過在’探索’-‘描述統計’按鈕下詢問jamovi的頻率表來自己檢查這一點。然而，這個總結並沒有解決我們感興趣的問題。要做到這一點，我們需要對數據進行更詳細的描述。我們想要做的是查看按物種劃分的選擇。也就是說，我們需要對數據進行交叉分類（見@sec-Tabulating-and-cross-tabulating-data）。在jamovi中，我們使用’頻率’-‘列聯表’-’獨立樣本’分析來完成這個操作，我們應該得到一個類似@tbl-tab10-7的表格。\n\n\n\n\n\n\nTable 14.7:  交叉分類數據 \n\nRobotHumanTotal\n\nPuppy131528\n\nFlower301343\n\nData4465109\n\nTotal8793180\n\n\n\n\n\n從這個表格中，我們可以很清楚地看到，絕大多數人類選擇了數據文件，而機器人在他們的選擇中則相對更均衡。撇開為什麼人類可能更喜歡選擇數據文件的問題（這確實看起來有點奇怪，承認吧），我們首要任務是確定數據集中人類選擇和機器人選擇之間的差異是否具有統計顯著性。\n\n\n14.2.1 建立獨立性的假設檢定\n我們如何分析這些數據？具體來說，由於我的研究假設是”人類和機器人回答問題的方式不同”，我該如何構建對虛無假設的檢驗，即”人類和機器人以相同的方式回答問題”？與以前一樣，我們首先建立一些符號來描述數據（Table 14.8）。\n\n\n\n\n\nTable 14.8:  描述數據的符號 \n\nRobotHumanTotal\n\nPuppy\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nFlower\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nData\\(O_{31}\\)\\(O_{32}\\)\\(R_{3}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)N\n\n\n\n\n\n在這個符號中，我們用 \\(O_{ij}\\) 表示被調查者在物種 j（機器人或人類）中選擇 i（小狗，花或數據）的計數（觀察頻率）。總觀察數通常表示為 \\(N\\)。最後，我用 \\(R_i\\) 表示行總數（例如，\\(R_1\\) 是選擇花的人的總數），用 \\(C_j\\) 表示列總數（例如，\\(C_1\\) 是機器人的總數）。10\n那麼，現在讓我們考慮一下虛無假設是什麼。如果機器人和人類對這個問題的回答方式相同，那麼“機器人說小狗”的概率與“人類說小狗”的概率相同，對其他兩種可能性也是如此。所以，如果我們用 \\(P_{ij}\\) 表示“物種 j 的成員給出回應 i 的概率”，那麼我們的虛無假設是：\n\\[\n\\begin{aligned}\nH_0 &: \\text{以下全部成立：} \\\\\n&P_{11} = P_{12}\\text{ （選擇“小狗”的概率相同），} \\\\\n&P_{21} = P_{22}\\text{ （選擇“花”的概率相同），以及} \\\\\n&P_{31} = P_{32}\\text{ （選擇“數據”的概率相同）}\n\\end{aligned}\n\\]\n事實上，由於虛無假設聲稱真實的選擇概率不取決於做出選擇的人的物種，我們可以讓 Pi 代表這個概率，例如，P1 是選擇小狗的真實概率。\n接下來，就像我們在適配度檢驗中所做的那樣，我們需要計算期望頻率。也就是說，對於每個觀察到的計數 \\(O_{ij}\\)，我們需要弄清楚虛無假設告訴我們期望什麼。我們用 \\(E_{ij}\\) 表示這個期望頻率。這次，情況有點棘手。如果物種 \\(j\\) 中有 \\(C_j\\) 人，而無論物種如何選擇選項 \\(i\\) 的真實概率為 \\(P_i\\)，那麼期望頻率只是：\n\\[E_{ij}=C_j \\times P_i\\]\n現在，這固然很好，但我們遇到了一個問題。與適配度檢驗的情況不同，虛無假設實際上並未指定 Pi 的特定值。\n這是我們必須估計（見 Chapter 8）的數據！幸運的是，這非常容易做到。如果有 28 位擇一的人選擇了花朵，那麼選擇花朵的概率的自然估計就是 \\(\\frac{28}{180}\\)，大約是 \\(0.16\\)。如果我們用數學語言來表示，我們所說的是，選擇選項 i 的概率估計只是行總數除以總樣本量：\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\n因此，我們的期望頻率可以寫成行總數和列總數的乘積（即相乘），除以總觀察次數：11\n\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]\n\n[額外的技術細節12]\n與以前一樣，\\(X^2\\) 的大值表示虛無假設對數據的描述效果不佳，而 \\(X^2\\) 的小值則表示虛無假設對數據的解釋效果很好。因此，就像上次一樣，如果 \\(X^2\\) 太大，我們希望拒絕虛無假設。\n不出所料，這個統計量遵循 \\(\\chi^2\\) 分布。我們需要做的就是弄清楚有多少自由度，實際上這並不困難。如我之前提到的，你可以（通常）將自由度視為等於你正在分析的數據點數量，減去約束的數量。具有 r 行和 c 列的列聯表包含總共 \\(r^{c}\\) 個觀察到的頻率，所以這是觀察到的總數量。約束呢？在這裡，情況稍微複雜一些。答案始終是相同的\n\\[df=(r-1)(c-1)\\]\n但是根據實驗設計，解釋為什麼自由度具有此值的原因是不同的。為了便於說明，假設我們確實打算調查 87 台機器人和 93 人（實驗者固定列總數），但讓行總數自由變化（行總數是隨機變量）。讓我們考慮在這裡適用的約束。好吧，因為我們故意通過實驗者的行為固定了列總數，所以在這裡就有 c 個約束。但實際上，還有更多的約束。記住我們的虛無假設中有一些自由參數（即，我們不得不估計 Pi 值）？這些也很重要。在這本書中，我不會解釋為什麼，但虛無假設中的每個自由參數都像是一個額外的約束。那麼，這些參數有多少呢？好吧，由於這些概率必須加起來等於 1，所以只有 \\(r - 1\\) 個。因此，我們的自由度總數是：\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\n另一種假設，假設實驗者確定的唯一事物是總樣本量 N。也就是說，我們對見到的前 180 人進行了問卷調查，結果發現 87 人是機器人，93 人是人類。這一次，我們的推理會有所不同，但仍然會得到相同的答案。我們的虛無假設仍然有 \\(r - 1\\) 個自由參數，對應於選擇概率，但現在還有 \\(c - 1\\) 個自由參數，對應於種類概率，因為我們還必須估計隨機抽樣的人類確實是機器人的概率。13 最後，由於我們確實確定了觀察數量的總數 N，所以這是另一個約束。因此，現在我們有 rc 次觀察，並且有 \\((c-1)+(r-1)+1\\) 約束。這會給出什麼呢？\n\\[\\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r- 1)(c - 1) \\end{split}\n\\]\n真是令人驚奇。\n\n\n\n\n14.2.2 獨立性檢定實作\n好吧，既然我們知道了檢驗是如何進行的，讓我們看看如何在 jamovi 中完成它。雖然讓您長時間地經歷繁瑣的計算以便被迫學習可能很有吸引力，但我認為這是沒有意義的。在上一節中，我已經向您展示了如何針對適合度檢驗進行長時間的操作，而且由於獨立性檢驗在概念上沒有任何不同，所以您不會通過長時間的操作學到任何新的東西。因此，我將直接向您展示簡單的方法。在 jamovi 中運行檢驗（“頻率” - “列聯表” - “獨立樣本”）之後，您只需查看 jamovi 結果窗口中列聯表下方，那裡就是 \\(\\chi^2\\) 統計量。這顯示了一個 \\(\\chi^2\\) 統計值為 10.72，2 d.f.，p-value = 0.005。\n那很簡單，不是嗎？您還可以要求 jamovi 顯示預期計數 - 只需單擊“Cells”選項中的“Counts” - “Expected”複選框，預期計數將出現在列聯表中。同時，在此操作中，效果大小度量會有所幫助。我們將選擇 Cramér’s \\(V\\)，您可以在“Statistics”選項中的複選框中指定它，它會給出 Cramér’s \\(V\\) 的值為 \\(0.24\\)。參見 Figure 14.6。我們稍後會再談論這個問題。\n\n\n\n\n\n\n\nFigure 14.6: 在 jamovi 中使用 Chapek 9 數據進行獨立樣本 \\(\\chi^2\\) 檢驗\n\n\n\n\n這個輸出為我們提供了足夠的信息來寫出結果：\n\nPearson 的 \\(\\chi^2\\) 顯示了物種和選擇之間存在顯著關聯（\\(\\chi^2(2) = 10.7, p< .01\\)）。機器人似乎更傾向於說他們喜歡花，而人類更傾向於說他們喜歡數據。\n\n注意，再次，我提供了一些解釋，以幫助人類讀者理解數據發生的情況。稍後在我的討論部分，我會提供更多的上下文。舉例來說，這是我可能會在之後說的：\n\n人類似乎比機器人更喜歡原始數據文件，這有點反直覺。但在某種程度上，它是有道理的，因為 Chapek 9 上的民事當局往往在發現人類時會將其殺死並解剖。因此，最有可能的是，人類參與者並未如實回答問題，以避免可能產生不良後果。這應該被認為是一個嚴重的方法論缺陷。\n\n我想，這可以被歸類為反應效應的一個極端例子。顯然，在這種情況下，問題嚴重到研究幾乎毫無價值，作為理解人類和機器人之間的差異偏好的工具。然而，我希望這能夠說明在獲得統計顯著結果（我們拒絕零假設，轉而接受替代假設）和找到具有科學價值的東西（由於嚴重的方法論缺陷，數據對我們研究假設的興趣一無所知）之間的區別。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的校正",
    "href": "14-Categorical-data-analysis.html#卡方檢定的校正",
    "title": "14  類別資料分析",
    "section": "14.3 卡方檢定的校正",
    "text": "14.3 卡方檢定的校正\n好吧，是時候稍作偏離了。到目前為止，我對您有點不誠實。當您只有 1 個自由度時，需要對計算稍作改變。這被稱為 “連續性修正”，或者有時稱為葉氏修正。請記住我之前指出的：\\(\\chi^2\\) 檢定是基於一個近似值，具體來說，是假設當 \\(N\\) 較大時，二項分佈開始類似於正態分佈。這樣做的一個問題是，它通常並不完全奏效，尤其是當你只有 1 個自由度時（例如，當你對一個 \\(2 \\times 2\\) 的列聯表進行獨立性檢定時）。造成這一現象的主要原因是，\\(X^{2}\\) 統計量的真實抽樣分佈實際上是離散的（因為您在處理分類數據！），但 \\(\\chi^2\\) 分佈是連續的。這可能引入系統性問題。具體來說，當 N 很小且 \\(df = 1\\) 時，適合度統計量往往 “太大”，這意味著您實際上的α值比您想象的要大（或者等效地說，p值稍微太小）。\n根據我從閱讀葉氏論文14中獲得的了解，修正基本上是一個突破。它不是源於任何原則性的理論。相反，它是基於對檢定行為的觀察，並發現經過修正的版本似乎效果更好。您可以在 jamovi 的 ‘Statistics’ 選項中的復選框中指定這個修正，其中它被稱為 ‘\\(\\chi^2\\) 連續性修正’。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的效果量",
    "href": "14-Categorical-data-analysis.html#卡方檢定的效果量",
    "title": "14  類別資料分析",
    "section": "14.4 卡方檢定的效果量",
    "text": "14.4 卡方檢定的效果量\n正如我們在 Section 9.8 中早先討論的，要求研究人員報告某種效應量測的情況越來越普遍。因此，假設您已經進行了卡方檢定，結果顯示具有顯著性。所以您現在知道您的變量之間存在某種關聯（獨立性檢定）或與指定概率的某種偏差（適合度檢定）。現在，您想報告一個效應量測。也就是說，假設存在關聯或偏差，其強度如何？\n您可以選擇報告幾種不同的測量值，並使用幾種不同的工具來計算它們。我不會討論所有這些測量值，而是將重點放在最常報告的效應大小測量值上。\n默認情況下，人們最常報告的兩個度量是 \\(\\phi\\) 統計量，以及稍稍優越的版本，稱為克拉默的 \\(V\\) 。\n[額外的技術細節15]\n完成後，這似乎是一個相當受歡迎的度量標準，可能是因為它易於計算，並且給出的答案並不是完全愚蠢的。有了克拉默的 \\(V\\)，您知道這個值確實在 0（完全無關聯）和 1（完全關聯）之間變化。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的適用條件",
    "href": "14-Categorical-data-analysis.html#卡方檢定的適用條件",
    "title": "14  類別資料分析",
    "section": "14.5 卡方檢定的適用條件",
    "text": "14.5 卡方檢定的適用條件\n所有統計檢驗都有一定的假設，通常檢查這些假設是否符合是一個好主意。對於本章迄今為止討論的卡方檢驗，其假設包括：\n\n期望頻率足夠大。還記得我們在上一節看到的 \\(\\chi^2\\) 抽樣分佈是如何產生的嗎？因為二項分佈非常類似於正態分佈，正如我們在 Chapter 7 中討論的那樣，這只有在觀察次數足夠多的情況下才成立。實際上，這意味著所有的期望頻率都需要相對較大。什麼是合理的大小？意見不一，但默認的假設似乎是你通常希望看到所有的期望頻率都大於大約5，儘管對於更大的表格，如果至少80%的期望頻率在5以上，並且沒有一個低於1，那可能還可以。然而，從我所能發現的資料（例如，Cochran (1954)）來看，這些似乎是作為粗略指導原則提出的，而不是硬性規定，而且它們似乎有些保守 (Larntz, 1978)。\n數據彼此獨立。卡方檢驗的一個稍微隱藏的假設是，你必須真正相信觀察結果是獨立的。我舉個例子。假設我對在某個特定醫院出生的男嬰的比例感興趣。我在產科病房走來走去，觀察到20個女孩和只有10個男孩。看起來是相當明顯的差異，對吧？但稍後，原來我實際上走進了同一個病房10次，事實上我只看到了2個女孩和1個男孩。現在不那麼令人信服了，是吧？我的原始30個觀察結果大量地不獨立，實際上只相當於3個獨立的觀察結果。顯然這是一個極端（而且非常愚蠢）的例子，但它說明了基本問題。非獨立性會“搞砸事情”。有時它會導致你錯誤地拒絕零假設，就像愚蠢的醫院例子那樣，但它也可能朝相反的方向發展。為了給出一個稍微不那麼愚蠢的例子，讓我們考慮一下如果我對卡片實驗做了一些不同的嘗試會怎麼樣。假設我不是要求200個人嘗試想像隨機抽取一張卡片，而是要求50個人選擇4張卡片。其中一種可能是每個人都選擇一張紅心、一張梅花、一張方塊和一張黑桃（符合“代表性啟發法則”(Tversky & Kahneman, 1974)）。這是人們的高度非隨機行為，但在這種情況下，我對四種花色都會得到50的觀察頻率。對於這個例子，觀察結果非獨立性（因為你選擇的四張卡片之間將彼此關聯）實際上導致了相反的效果，即錯誤地保留了零假設。\n\n如果你碰巧處於獨立性受到違反的情況，你可以使用 McNemar 檢驗（我們將討論）或 Cochran 檢驗（我們不會討論）。同樣，如果你的預期單元計數太小，可以查看 Fisher 確切檢驗。我們現在將轉向這些主題。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#sec-The-Fisher-test",
    "href": "14-Categorical-data-analysis.html#sec-The-Fisher-test",
    "title": "14  類別資料分析",
    "section": "14.6 費雪精確檢定",
    "text": "14.6 費雪精確檢定\n如果你的單元計數太小，但你仍然想檢驗兩個變量是否獨立的零假設，該怎麼辦？一個答案是“收集更多數據”，但這太輕浮了。有很多情況下，進行此操作要么不可行，要么不道德。如果是這樣，統計學家有一種道德責任，為科學家提供更好的檢驗方法。在這個例子中，Fisher (1922) 恰好提供了問題的正確答案。為了說明這個基本概念，假設我們正在分析一個田野實驗的數據，研究被指控為巫術的人的情感狀況，其中一些人正在被燒死。16 不幸的是，對於科學家來說（但對普通大眾來說相當幸運），實際上很難找到正在被點火的人，所以在某些情況下，單元計數非常小。salem.csv 數據的列聯表說明了這一點（Table 14.9）。\n\n\n\n\n\n\nTable 14.9:  salem.csv 數據的列聯表 \n\nhappyFALSETRUE\n\non.fireFALSE310\n\nTRUE30\n\n\n\n\n\n查看這些數據，你會很難不懷疑沒有被點火的人比正在被點火的人更可能感到快樂。然而，由於樣本量很小，卡方檢驗使得這一點很難檢驗。所以，作為一個不想被點火的人，我非常希望能得到比這更好的答案。這就是費雪精確檢定（Fisher’s exact test）(Fisher, 1922) 派上用場的地方。\n費雪精確檢定的運作方式與卡方檢定（事實上，在本書中我談論的任何其他假設檢定）有所不同，因為它沒有檢定統計量，而是“直接”計算 p 值。我將解釋該檢定在 \\(2 \\times 2\\) 列聯表中的運作基本原理。與以前一樣，讓我們使用一些符號（Table 14.10）。\n\n\n\n\n\n\nTable 14.10:  費雪精確檢定的符號 \n\nHappySadTotal\n\nSet on fire\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nNot set on fire\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)\\(N\\)\n\n\n\n\n\n為了構建檢定，費雪將行和列總數（\\(R_1, R_2, C_1\\) 和 \\(C_2\\)）都視為已知的固定量，然後計算在給定這些總數的情況下，我們會得到我們所觀察到的頻率（\\(O_{11}, O_{12}, O_{21}\\) 和 \\(O_{22}\\)）的概率。在我們在 Chapter 7 中開發的表示法中，這是寫作：\n\\[P(O_{11}, O_{12}, O_{21}, O_{22} \\text{ | } R_1, R_2, C_1, C_2)\\] 並且如您所料，弄清楚這個概率是什麼有點困難。但事實證明，這個概率是由一個稱為超幾何分佈的分佈描述的。要計算我們的 p 值，我們必須計算觀察到這個特定表格或者一個更“極端”的表格的概率。17 在 20 世紀 20 年代，即使在最簡單的情況下，計算這個和也是令人生畏的，但如今只要表格不是太大，樣本量不是太大，這就相當容易了。概念上棘手的問題是弄清楚一個列聯表比另一個列聯表更“極端”的含義。最簡單的解決方案是說，概率最低的表格是最極端的。這將給我們 p 值。\n您可以在 jamovi 中的“列聯表”分析中的“統計”選項中的復選框中指定此檢定。當您使用 salem.csv 文件中的數據執行此操作時，費雪精確檢定統計量將顯示在結果中。我們在這里感興趣的主要是 p 值，這個例子中 p 值足夠小（p = .036），足以證明拒絕那些正在燃燒的人和沒有燃燒的人一樣快樂的零假設。參見 Figure 14.7。\n\n\n\n\n\n\n\n\nFigure 14.7: jamovi 中的費雪精確檢定分析"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#sec-The-McNemar-test",
    "href": "14-Categorical-data-analysis.html#sec-The-McNemar-test",
    "title": "14  類別資料分析",
    "section": "14.7 麥內瑪檢定",
    "text": "14.7 麥內瑪檢定\n假設您被聘請為澳大利亞通用政治黨(AGPP)工作18，您的工作的一部分是了解AGPP政治廣告的有效性。因此，您決定組織一個包含\\(N = 100\\)人的樣本，讓他們觀看AGPP的廣告。在他們看到任何內容之前，您問他們是否打算投票支持AGPP，然後在播放廣告之後再問他們一遍，看看有沒有人改變主意。顯然，如果你擅長你的工作，你還會做很多其他事情，但讓我們考慮一下這個簡單的實驗。描述數據的一種方法是通過@tbl-tab10-11中顯示的列聯表。\n\n\n\n\n\n\nTable 14.11:  帶有AGPP政治廣告數據的列聯表 \n\nBeforeAfterTotal\n\nYes301040\n\nNo7090160\n\nTotal100100200\n\n\n\n\n\n首先，您可能會認為這種情況適合用皮爾森\\(\\chi^2\\)檢驗獨立性（如[獨立性（或關聯性）的\\(\\chi^2\\)檢驗]）。然而，稍加思考就會發現我們遇到了一個問題。我們有100名參與者，但卻有200個觀察值。這是因為每個人在”之前”的列和”之後”的列中都給出了答案。這意味著這200個觀察值彼此之間並不獨立。如果選民A第一次說“是”，選民B說“否”，那麼您就可以預期選民A第二次比選民B更可能說“是”！由於獨立性假設的違反，這意味著通常的\\(\\chi^2\\)檢驗不會給出可靠的答案。現在，如果這是一個非常不常見的情況，我不會浪費你的時間來討論它。但它並不罕見。這是一個標準的重複測量設計，而到目前為止，我們考慮的所有測試都無法處理它。哎呀。\n問題的解決方案是麥內瑪（McNemar，1947）發表的。訣竅是先用稍微不同的方式整理數據（Table 14.12）。\n\n\n\n\n\nTable 14.12:  當你有重複測量數據時，以不同的方式整理數據 \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes5510\n\nAfter: No256590\n\nTotal3070100\n\n\n\n\n\n接下來，讓我們思考一下我們的零假設是什麼：它是“之前”測試和“之後”測試中有相同比例的人說：“是的，我會投票支持AGPP。” 由於我們已經重新整理了數據，這意味著我們現在正在測試行總數和列總數來自相同分佈的假設。因此，麥內瑪檢驗中的零假設是邊際同質性。也就是說，行總數和列總數具有相同的分佈：\\(P_a + P_b = P_a + P_c\\)，同樣地，\\(P_c + P_d = P_b + P_d\\)。注意，這意味著零假設實際上簡化為Pb = Pc。換句話說，就麥內瑪檢驗而言，只有這個表格中的對角線條目（即b和c）才是重要的！注意到這一點後，麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity)與通常的\\(\\chi^2\\)檢驗沒有什麼不同。在應用Yates修正後，我們的檢驗統計量變為：\n\\[\\chi^2=\\frac{(|b-c|-0.5)^2}{b+c}\\] 或者，恢復到我們在本章前面使用的表示法：\n\\[\\chi^2=\\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}\\] 這個統計量具有\\(\\chi^2\\)分佈（近似），自由度df = 1。然而，請記住，就像其他\\(\\chi^2\\)檢驗一樣，它只是一個近似值，因此您需要預期較大的單元格計數才能使其正常工作。\n\n\n\n14.7.1 實作麥內瑪檢定\n現在您已經了解麥內瑪檢驗的所有內容，讓我們實際運行一個。agpp.csv 文件包含了我之前討論過的原始數據。agpp 數據集包含三個變量，一個id變量標記數據集中的每個參與者（我們將在片刻之後看到這有什麼用），一個response_before 變量記錄了當他們第一次被問到這個問題時的答案，以及一個response_after變量顯示他們在第二次被問到同樣問題時給出的答案。注意每個參與者在這個數據集中只出現一次。在jamovi中，轉到 ‘Analyses’ - ‘Frequencies’ - ‘Contingency Tables’ - ‘Paired Samples’ 分析，並將response_before 放入 ‘Rows’ 框，將response_after 放入 ‘Columns’ 框。然後，您將在結果窗口中獲得一個列聯表，麥內瑪檢驗的統計數據就在它下面，參見@fig-fig10-8。\n\n\n\n\n\n\n\nFigure 14.8: jamovi中的麥內瑪檢驗輸出\n\n\n\n\n我們完成了。我們剛剛運行了一個麥內瑪檢驗，以確定人們在廣告後是否和廣告前一樣有可能投票支持AGPP。檢驗是顯著的（\\(\\chi^2(1)= 12.03, p< .001\\)），表明他們並非如此。事實上，看起來廣告產生了負面影響：人們在看過廣告後，投票支持AGPP的可能性更低。考慮到典型政治廣告的質量，這是很合理的。\n\n\n\n14.7.2 與獨立性檢定有可分別?\n讓我們回到本章的開頭，再次查看卡片數據集。如果您回憶一下，我描述的實際實驗設計涉及人們進行兩次選擇。因為我們有關於每個人第一次選擇和第二次選擇的信息，我們可以構建以下列聯表，用於將第一次選擇與第二次選擇交叉列聯（Table 14.13）。\n\n\n\n\n\n\nTable 14.13:  用Randomness.omv（卡片）數據交叉列聯第一次與第二次選擇 \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes\\(a \\)\\(b \\)\\(a + b \\)\n\nAfter: No\\(c  \\)\\(d  \\)\\(c + d  \\)\n\nTotal\\(a+c  \\)\\(b+d  \\)\\(n  \\)\n\n\n\n\n\n假設我想知道第二次選擇是否取決於第一次選擇。這是獨立性檢驗有用的地方，我們試圖做的是看看這個表格的行和列之間是否存在某種關係。\n另外，假設我想知道平均而言，第二次選擇的花色頻率是否與第一次選擇不同。在這種情況下，我真正想做的是看看行總數是否不同於列總數。這就是您使用麥內瑪檢驗的時候。\n這些不同分析產生的不同統計數據顯示在 Figure 14.9 中。注意結果是不同的！這些檢驗並不相同。\n\n\n\n\n\n\n\nFigure 14.9: Randomness.omv（卡片）數據中的獨立與成對（麥內瑪）"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#本章小結",
    "href": "14-Categorical-data-analysis.html#本章小結",
    "title": "14  類別資料分析",
    "section": "14.8 本章小結",
    "text": "14.8 本章小結\n本章的學習重點有：\n\n卡方適合度檢定用於你的表列資料是來自不同類別的觀察次數，虛無假設是可相互比較的已知機率。\n卡方獨立性或關聯性檢定用於你的資料是能化為列聯表的觀察次數。虛無假設是兩種變項之間沒有關聯性。\n列聯表的效果量有多種測量方法。在此介紹最常見的Cramér’s V。\n上述的卡方檢定法有兩種適用條件：期望值次數夠大，觀察值彼此獨立。如果期望值次數不夠大，可以使用費雪精確檢定；如果觀察值並非彼此獨立，可以使用麥內瑪檢定。\n\n如果想學習更多類別資料分析方法，推薦閱讀 Agresti (1996) 的專書”類別資料分析導論”。如果基礎教科書無法滿足你的需要，或者未提供解決手上問題的方法，可以參考 Agresti (2002) 的進階書藉。因為是進階書，建議先充分理解導論再來學習進階教科書。\n\n\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nCramer, H. (1946). Mathematical methods of statistics. Princeton University Press.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-intro",
    "href": "Prelude-Part-V.html#prelude-intro",
    "title": "10  線性模型的學習取向",
    "section": "10.1 教程前言",
    "text": "10.1 教程前言\n\n本節改編自簡体中文版3\n\n大部分常見的統計檢定方法（t 檢定、相關性檢定、變異數分析（ANOVA）、卡方檢定等），本質都是線性模型的一種特例或者是非常逼近的模型。這種優雅的簡潔性意味著我們並不需要掌握太多的技巧就能學習。具體來說，所有模型的來源都是多數學生在高中時期就學過的一元一次線性模型：\\(y = a \\cdot x + b\\) 。然而，很多基礎統計課程是把各種檢定方法分開教，給學生和老師們增加了很多不必要的麻煩。在學習每一個檢定方法的基本假設時，如果不是從線性模型切入，而是每個檢定方法都死記硬背，這讓學習的複雜度倍增。因此，我認為先教線性模型，然後對線性模型的一些特殊形式更改名稱是一種優秀的教學策略，這有助於更深刻地理解假設檢定。線性模型在次數主義學派、貝氏學派和基於置換的U檢定的統計推論方法之間是互通的，對初學者而言，從線性模型開始比從認識什麼是p值、型I錯誤、貝氏因子或其它術語更為友好。\n在入門課程教授到無母數檢定的時候，可以避開自我欺騙的手段，直接告訴學生無母數檢定其實就是參數本質是次序(rank)的檢定方法。對學生來說，接受向量的概念比相信你可以神奇地放棄各種假設好得多。實際上，在統計軟體如 JASP 裡，無母數檢定的貝氏等價模型就是使用潛在次序（Latent Rank）處理資料的。次數主義學派的無母數檢定在樣本量 N>15 的時非常準確。"
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-suggestions",
    "href": "Prelude-Part-V.html#prelude-suggestions",
    "title": "線性模型的學習取向",
    "section": "教與學的建議",
    "text": "教與學的建議\n\n本節改編自簡体中文版4\n\n\n迴歸的基礎知識\n\n回憶高中學過的線性模型：\\(y = a \\cdot x + b\\)，培養學生對斜率和截距的直覺。理解到這條公式能改寫成各種變項名稱。例如 money = profit * time + starting_money，或 \\(y = \\beta_1x + \\beta_2*1\\)，去除係數可寫成 y ~ x + 1。如果學生能接受的話，可以探索如何用微分方程推道這些模型是，並了解 \\(y\\) 是如何隨著 \\(x\\) 的數值改變。\n擴展到多元迴歸模型。這部分有非常多可用的生活實例做為練習作業，讓這些概念很容易用直覺理解。加深同學如何使用這些簡潔的模型描述巨量資料集的印象。\n介紹如何將非數值型資料轉換為次序尺度，並進行各種練習。\n以三種前提假設規劃教學：資料的獨立性，殘差分佈的常態性，以及集中量數平方差的同質性（homoscedasticity）。\n參數的信賴（confidence）與可信（credible）區間。說明為何很難計算極大似然估計值（Maximum-Likelihood estimate），因此區間估計更為重要。\n對以上簡單的迴歸模型，簡要地認識 \\(R^2\\)。順便提及一下，這就是 Pearson 和 Spearman 相關係數。\n\n\n\n特殊情況 1：一個或兩個平均值（t 檢定、Wilcoxon 檢定、Mann-Whitney 檢定）\n\n單一平均值：當只有一個變項 x 的時候，迴歸模型簡化為 \\(y = b\\)。如果 \\(y\\) 不是連續變項，可以轉換為等比尺度。應用模型假設（只有一個 \\(x\\)，因此這些檢定方法不適用平方差的同質性）。順便提及一下，這些僅有截距的模型又名為單一樣本 t 檢定和 Wilcoxon 符號次序檢定。\n分組平均值：如果我們把兩個變項放在 x 軸，兩者平均值的差異就是斜率。這樣就能用如同瑞士刀的線性模型來解決！應用模型的假設條件，檢查兩組平均值的平方差是否相等，相等即符合平方差的同質性。如此就能稱這個模型為獨立 t 檢定。試著創造一些虛擬資料，做一些練習，也許還能加上 Welch 檢定，再加上次序轉換，就能變成 Mann-Whitney U 檢定。\n相依樣本：這種模型違反了獨立性假設。通過計算配對組的資料差異值，模型就會與 \\(y = b\\) 等價，儘管這些方法另有專有名詞：相依 t 檢定和 Wilcoxon 配對組檢定。\n\n\n\n特殊情況 2：三個或多個均值（方差分析（ANOVA））\n\n對類別轉化後的示性變量：類別的每一個取值範圍對應的迴歸係數，是如何通過乘以一個二元（binary）示性變量，來對每個取值範圍對應的截距來進行建模的。（How one regression coefficient for each level of a factor models an intercept for each level when multiplied by a binary indicator.）這只是我們為了使資料能用線性模型建模，而擴展了在 2.1 所做的事情而已。\n一個變量的均值：單因素方差分析（one-way ANOVA）.\n兩個變量的均值：雙因素方差分析（two-way ANOVA）.\n\n\n\n特殊情況 3：三個或多個比率（卡方檢驗）\n\n對數變換：通過對數變換，把“多元乘法”模型轉化成線性模型，從而可以對比率進行建模。關於對數線性模型和對比率的卡方檢驗的等價性，可以查閱這個非常優秀的介紹。此外，還需要介紹 (log-) odds ratio（一般翻譯為“比值比”或“優勢比”）。當“多元乘法”模型使用對數變換轉化為“加法”模型之後，我們僅加上來自 3.1 的示性變量技巧，就會在接下來發現模型等價於 3.2 和 3.3 的方差分析—-除了係數的解釋發生了變化。\n單變量的比率：擬合優度檢驗.\n雙變量的比率：列聯表.\n\n\n\n假設檢定\n\n視為模型比較的假設檢定：假設檢驗用於全模型和某個參數固定了（通常為 0，也即從模型中去除）的模型進行比較，而不是對模型進行估計。比如說，在 t 檢驗 把兩個均值之一固定為零之後，我們探究單獨一個均值（單樣本 t 檢驗）對兩個組的數據的解釋程度。如果解釋程度比較好，那麼我們更傾向於這個單均值模型，而不是雙均值模型，因為前者更為簡單。假設檢驗其實是比較多個線性模型，來獲得更多的定量描述。單參數的檢驗，假設檢驗包含的信息更少。但是，同時對多個參數（如方差分析的類別變量）進行檢驗的話，模型比較就會變得沒有價值了。\n似然比：似然比是一把瑞士軍刀，它適用於單樣本 t 檢驗到 GLMM 等情況。BIC 對模型複雜度進行懲罰。還有，加上先驗（prior）的話，你就能得到貝葉斯因子（Bayes Factor）。一個工具，就能解決所有問題。我在上文方差分析中使用了似然比檢驗。"
  },
  {
    "objectID": "Prelude-Part-V.html#Prelude-corr-linear-reg",
    "href": "Prelude-Part-V.html#Prelude-corr-linear-reg",
    "title": "線性模型的學習取向",
    "section": "相闗與線性迴歸",
    "text": "相闗與線性迴歸"
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-fatorial-design",
    "href": "Prelude-Part-V.html#prelude-fatorial-design",
    "title": "線性模型的學習取向",
    "section": "多因子變異數分析",
    "text": "多因子變異數分析"
  },
  {
    "objectID": "Prelude-Part-V.html#線型模型版本示範集合",
    "href": "Prelude-Part-V.html#線型模型版本示範集合",
    "title": "線性模型的學習取向",
    "section": "線型模型版本示範集合",
    "text": "線型模型版本示範集合\n\n相闗與線性迴歸\n\n\n多因子變異數分析"
  },
  {
    "objectID": "Prelude-Part-V.html#教程前言",
    "href": "Prelude-Part-V.html#教程前言",
    "title": "線性模型的學習取向",
    "section": "教程前言",
    "text": "教程前言\n\n本節改編自簡体中文版3\n\n大部分常見的統計檢定方法（t 檢定、相關性檢定、變異數分析（ANOVA）、卡方檢定等），本質都是線性模型的一種特例或者是非常逼近的模型。這種優雅的簡潔性意味著我們並不需要掌握太多的技巧就能學習。具體來說，所有模型的來源都是多數學生在高中時期就學過的一元一次線性模型：\\(y = a \\cdot x + b\\) 。然而，很多基礎統計課程是把各種檢定方法分開教，給學生和老師們增加了很多不必要的麻煩。在學習每一個檢定方法的基本假設時，如果不是從線性模型切入，而是每個檢定方法都死記硬背，這讓學習的複雜度倍增。因此，我認為先教線性模型，然後對線性模型的一些特殊形式更改名稱是一種優秀的教學策略，這有助於更深刻地理解假設檢定。線性模型在次數主義學派、貝氏學派和基於置換的U檢定的統計推論方法之間是互通的，對初學者而言，從線性模型開始比從認識什麼是p值、型I錯誤、貝氏因子或其它術語更為友好。\n在入門課程教授到無母數統計方法的時候，可以避開自我欺騙的手段，直接告訴學生無母數檢定其實就是參數本質是等級(rank)的檢定方法。對學生來說，接受向量的概念，比相信你可以神奇地放棄各種母數統計方法所依賴的假設要好得多。實際上，在統計軟體如 JASP 裡，無母數檢定的貝氏等價模型就是使用潛在次序（Latent Rank）處理資料，而次數主義學派的無母數檢定方法用在樣本量 N > 15 的資料非常準確。\nLindeløv教程提供簡明整理表，有需要的讀者可參考簡体中文版。\n創建計算變項或轉換變項，將原始資料轉換為等級資料的函式語法：\nIF($source==0,0,IF($source>1,1,-1))*RANK($source)"
  },
  {
    "objectID": "Prelude-Part-V.html#基礎統計方法線型模型示範說明",
    "href": "Prelude-Part-V.html#基礎統計方法線型模型示範說明",
    "title": "線性模型的學習取向",
    "section": "基礎統計方法線型模型示範說明",
    "text": "基礎統計方法線型模型示範說明\n\n相關與線性迴歸\n我們運用 Figure 14.7 的虛構資料製作資料檔案，示範等級資料的相關與迴歸分析。根據教程摘要，使用等級資料計算斯皮爾曼等級相關與迴歸分析，總樣本量N必須至少大於10，才能得到精確逼近的估計值。因此這份虛構資料的分析結果可能是錯誤的估計。\n此外，jamovi迴歸模組無法處理等級資料，因此資料檔案以R程式碼示範。如果同學有機會處理真正的等級資料，可選擇使用R或其他統計軟體如JASP進行迴歸分析。\n\n\n比較兩組平均值\n\n\n比較多組平均值(單因子變異數分析)\n\n\n多因子變異數分析\n\n\n類別資料分析"
  },
  {
    "objectID": "13-Factorial-ANOVA.html",
    "href": "13-Factorial-ANOVA.html",
    "title": "13  多因子變異數分析",
    "section": "",
    "text": "14 ### 第二型平方差總和"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "href": "10-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "title": "10  相闗與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\n這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的相關。首先，讓我們看一下列在 Table 14.1 的本章示範資料描述統計。\n\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。 \n \n  \n    變項 \n    最小值 \n    最大值 \n    平均值 \n    中位數 \n    標準差 \n    四分位數間距 \n  \n \n\n  \n    老爸的沮喪程度 \n    41.00 \n    91.00 \n    63.71 \n    62.00 \n    10.05 \n    14.00 \n  \n  \n    老爸睡眠小時數 \n    4.84 \n    9.00 \n    6.97 \n    7.03 \n    1.02 \n    1.45 \n  \n  \n    小嬰兒睡眠小時數 \n    3.25 \n    12.07 \n    8.05 \n    7.95 \n    2.07 \n    3.21 \n  \n\n\n\n\n\n\n讓我們從一個與每個新生兒父母都息息相關的主題談起：睡眠。這裡使用的資料集是虛構的，但是來自本人(原作者)的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有測量我的沮喪分數、我的睡眠習慣和我兒子的睡眠習慣持續是100天。身為一位數位時代的書呆子，我把資料保存在一個名為parenthood.csv的檔案。匯入jamovi，我們可以看到四個變項：dani.sleep，baby.sleep，dani.grump和day。請注意，當您首次打開這份檔案，jamovi可能無法正確猜測每個變項的資料類型，同學可以自行修正：dani.sleep，baby.sleep，dani.grump和day都可以被指定為連續變項，而ID是一個名義且為整數的變項。2\n接著我會看一下一些基本的描述性統計數據，並且三個我有興趣的變項視覺化，也就是 Figure 14.1 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計數據，你就要報告所有數據。如果我要以此結果撰寫報告，我會挑出那些我自己以及我的讀者最感興趣的統計數據，然後將它們放入像 Table 14.1 這樣的簡潔的表格裡。3 需要注意的是，當我將數據放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。\n\n\n\n\n\n\n\nFigure 10.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。\n\n\n\n\n\n\n10.1.2 相關的強度與方向\n我們可以繪製散佈圖，讓我們能俯瞰兩個變項之間的相關性。雖 然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較dani.sleep和dani.grump之間的關係（ fig-fig10-2 ，左）與baby.sleep和dani.grump之間的關係（ fig-fig10-2 ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我或者我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，dani.sleep和dani.grump之間的關係比baby.sleep和dani.grump之間的關係更強：左圖比右圖更加整齊。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。\n\n\n\n\n\n\nFigure 10.2: 左圖是dani.sleep(老爸睡眠小時數)與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖。\n\n\n\n\n相反地， Figure 14.3 的另外兩個散佈圖告訴我們另一個角度的故事。比較“baby.sleep 與 dani.grump”的散佈圖（左）和“baby.sleep 與 dani.sleep”的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。\n\n\n\n\n\n\nFigure 10.3: 左圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.sleep(老爸睡眠小時數)的散佈圖。\n\n\n\n\n\n\n10.1.3 相關係數\n現在我們要進一步延伸上述的概念，也就是正式認識 相關係數(correlation coefficient)。更具體地說，本節主要介紹皮爾森相關係數（Pearson’s correlation），慣例書寫符號是 \\(r\\)。在下一節，我們會用更精確符號 \\(r_{XY}\\) ，表示兩個變項 \\(X\\) 和 \\(Y\\) 之間的相關係數，值域涵蓋-1到1。當\\(r = -1\\)時，表示變項之間是完全的負相關；當\\(r = 1\\)時，表示變項之間是完全的正相關；當\\(r = 0\\)時，表示變項之間是完全沒有關係。 Figure 14.4 展示幾種不同相關性的散佈圖。\n[其他技術細節 4]\n\n\n\n\n\n\nFigure 10.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為\\(0, .33, .66, 1\\)。右欄的相關係數由上而下為\\(0, -.33, -.66, -1\\)。\n\n\n\n\n標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: \\(r = 1\\)代表著完美的正相關，\\(r = -1\\)代表著完美的負相關。稍後解讀相關係數這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。\n\n\n\n10.1.4 相關係數計算實務\n只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 Figure 14.5 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。\n\n\n\n\n\n\nFigure 10.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。\n\n\n\n\n\n\n10.1.5 解讀相關係數\n在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀\\(r = 0.4\\)的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於\\(0.95\\)的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到\\(0.9\\)，任何使用這個測驗預測的理論就會失效5。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過\\(0.3\\)，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 Table 14.2 的概略式解讀原則。\n\n\n\n\n\nTable 10.2:  解讀相關係數的粗略指南。強調粗略是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。 \n \n  \n    相關係數 \n    強度 \n    方向 \n  \n \n\n  \n    -1.0 ~ -0.9 \n    非常強 \n    負相關 \n  \n  \n    -0.9 ~ -0.7 \n    強 \n    負相關 \n  \n  \n    -0.7 to -0.4 \n    中等 \n    負相關 \n  \n  \n    -0.4 ~ -0.2 \n    弱 \n    負相關 \n  \n  \n    -0.2 ~ 0 \n    微弱 \n    負相關 \n  \n  \n    0 ~ 0.2 \n    微弱 \n    正相關 \n  \n  \n    0.2 ~ 0.4 \n    弱 \n    正相關 \n  \n  \n    0.4 ~ 0.7 \n    中等 \n    正相關 \n  \n  \n    0.7 ~ 0.9 \n    強 \n    正相關 \n  \n  \n    0.9 ~ 1.0 \n    非常強 \n    正相關 \n  \n\n\n\n\n\n\n然而，有一件點任何一位統計學教師都會不厭其煩地提醒同學，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)(Anscombe, 1973)，其中有四個資料集。每個資料集都有兩個變項， \\(X\\) 與 \\(Y\\)。四個資料集的 \\(X\\) 平均值都是 \\(9\\)， \\(Y\\) 的平均值都是 \\(7.5\\)。所有 \\(X\\) 變項的標準差幾乎相同，\\(Y\\) 變項的標準差也是一致。每種資料集的\\(X\\) 和 \\(Y\\) 相關係數均為 \\(r = 0.816\\)。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。\n也許你認為這四個資料集看起來很相似，其實上並非如此。從 Figure 14.6 的散佈圖可以發現，所有四個資料集的\\(X\\) 和 \\(Y\\) 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始數據」（見 Chapter 5 ）。\n\n\n\n\n\n\nFigure 10.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\n皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。\n線性關係是當一個變項\\(X\\)的數值增加，也能反映另一個變項\\(Y\\)的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（\\(X\\)）準備一個科目，那麼他排名的成績應該只有0％（\\(Y\\)）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。\n我們用 Figure 14.7 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的次序關係(ordinal relationship)。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數\\(r=0.91\\)能表達的。\n\n\n\n\n\n\nFigure 10.7: 這個圖解展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數\\(r = .91\\)。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數\\(\\rho = 1\\)。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。\n\n\n\n\n那麼我們要如何解決這個問題呢？其實很容易。如果我們要評估變項之間的次序關係，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”來衡量學生的努力，而是按照他們的讀書時間長短，將這\\(10\\)名學生排序。也就是說，學生\\(2\\)花在讀書的時間最少（\\(2\\)個小時），所以他獲得了最低的排名（排名=\\(1\\)）。接下來最懶惰的是學生\\(4\\)，整個學期只讀了\\(6\\)個小時的書，所以他獲得了次低的排名（排名=\\(2\\)）。請注意，在此用“排名=\\(1\\)”來表示“低排名”。在日常言談裡，多數人使用“排名=\\(1\\)”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！\n好的，讓我們從最努力且最成功的學生開始排名。 Table 14.3 顯示從最努力且最成功的學生排名的次序值。6\n\n\n\n\n\nTable 10.3:  十位學生的工作時間與得分數值次序 \n \n  \n    學生編號 \n    讀書時間序列 \n    成績序列 \n  \n \n\n  \n    學生 1 \n    10 \n    10 \n  \n  \n    學生 2 \n    1 \n    1 \n  \n  \n    學生 3 \n    5 \n    5 \n  \n  \n    學生 4 \n    8 \n    8 \n  \n  \n    學生 5 \n    9 \n    9 \n  \n  \n    學生 6 \n    6 \n    6 \n  \n  \n    學生 7 \n    7 \n    7 \n  \n  \n    學生 8 \n    3 \n    3 \n  \n  \n    學生 9 \n    4 \n    4 \n  \n  \n    學生 10 \n    2 \n    2 \n  \n\n\n\n\n\n\n有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。\n至此我們等於重新發現 斯皮爾曼等級相關(Spearman’s rank order correlation)，通常用符號 \\(\\rho\\) 表示，以區分皮爾森相關係數\\(r\\)。我們可以在jamovi的“相關矩陣”選單選擇“Spearman”，計算斯皮爾曼等級相關係數。7"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "href": "10-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "title": "10  相闗與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\n\n譯者註 20230415初步以ChatGPT-4完成翻譯，以下內容待編修。\n\n我們可以將線性迴歸模型理解為稍微複雜一點的皮爾森相關係數（請見相關這一節），稍後我們會看到的，迴歸模型是用途更多的統計方法。\n由於迴歸模型的基本觀念與相關係數緊密相關，我們將使用用來說明相關係數的parenthood.csv資料集來介紹。回想一下，我們想從這個資料集找出為什麼我(原作者Dani)總是非常沮喪的原因，而我的研究假設是我沒有得到足夠的睡眠。我們畫了一些散佈圖，檢查我的實際睡眠時間與隔天沮喪程度之間的關係，就像 Figure 14.9 展示的散佈圖，兩者之間的相關係數達到\\(r=-0.90\\)。但是，我心中想像的變項間關係更像 Figure 10.11 (a) 。也就是說，我想像有一條直線穿過資料點的中間。這條線的統計學術語是迴歸線。請注意，由於我不是統計新手，因此迴歸線一定會穿過資料散佈區域的中間地帶，絕不會認為是像 Figure 10.11 (b) 。\n\n\n\n\n\n\nFigure 10.11: 圖a展示同 Figure 14.9 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。\n\n\n\n\n這不是特別令人驚訝的。在 Figure 10.11（b）中我畫的那條線與資料的配適度不高，所以提議用它來總結資料沒有太大的意義，對吧？這是一個很簡單的觀察，但是當我們開始嘗試將一些數學理論應用在上面時，它會變得非常強大。為此，讓我們開始複習一些高中數學。一條直線的公式通常寫成以下形式：\n\n\\[y=a+bx\\]\n或者至少在我幾十年前上高中時是這樣的。這兩個變量是 \\(x\\) 和 \\(y\\)，我們有兩個係數 \\(a\\) 和 \\(b\\)。8其中 \\(a\\) 代表了直線的 \\(y\\) 截距，而 \\(b\\) 代表了直線的斜率。深入挖掘我們退化的高中記憶（對不起，對於我們中的一些人，高中已經是很久以前的事情了），我們記得截距被解釋為“當 \\(x=0\\) 時得到的 \\(y\\) 值”。同樣，斜率 \\(b\\) 意味著如果你將 \\(x\\) 值增加 1 單位，那麼 \\(y\\) 值會增加 \\(b\\) 單位，而負斜率則意味著 \\(y\\) 值會下降而不是上升。啊，是的，我現在全都回想起來了。現在我們已經記起來，所以發現我們使用完全相同的公式來計算回歸線應該也不會感到意外。如果 \\(Y\\) 是結果變量（因變量），\\(X\\) 是預測變量（自變量），那麼描述我們回歸的公式就像這樣：\n\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\n嗯，看起來這跟我們熟知的公式一模一樣，但多了些花俏的東西。讓我們來了解這些花俏的東西。首先，注意到我使用了 \\(X_i\\) 和 \\(Y_i\\)，而不是 \\(X\\) 和 \\(Y\\)，這是因為我們要記得這是實際的資料。在這個公式中，\\(X_i\\) 代表第 i 個觀察值的預測變數的值（例如在我做小實驗的第 i 天所得到的睡眠時間），而 \\(Y_i\\) 則是對應的結果變數的值（即當天的煩躁程度）。而雖然我沒有在公式中明確說明，但我們假設這個公式對資料集中的所有觀察值都適用（即對所有 i 都成立）。其次，注意到我寫的是 \\(\\hat{Y}_i\\) 而不是 \\(Y_i\\)，這是因為我們要區分實際資料 \\(Y_i\\) 與預測值 \\(\\hat{Y}_i\\)（也就是回歸線所作的預測）。第三，我將用來描述係數的字母從 a 和 b 改成 \\(b_0\\) 和 \\(b_1\\)。這是統計學家喜歡在迴歸模型中使用的方式。我不知道為什麼他們選擇了 b 這個字母，但這就是他們的選擇。無論如何，\\(b_0\\) 總是代表截距項，\\(b_1\\) 則是代表斜率。\n非常好，接下來，我不禁注意到，無論是好的迴歸線還是壞的迴歸線，數據都不完美地落在線上。換句話說，數據\\(Y_i\\)與迴歸模型的預測\\(\\hat{Y}_i\\)不完全相同。由於統計學家喜歡給一切事物都加上字母、名稱和數字，讓我們稱模型預測與實際數據之間的差異為殘差，表示為\\(\\epsilon_i\\)。9 使用數學表示，殘差被定義為：\n\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\n接著我們就可以寫出完整的線性迴歸模型：\n\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "11-Comparing-two-means.html#等級資料的平均值檢定11-translation-01",
    "href": "11-Comparing-two-means.html#等級資料的平均值檢定11-translation-01",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 等級資料的平均值檢定24",
    "text": "11.10 等級資料的平均值檢定24\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 單一樣本的Wilcoxon檢定\nWhat about the one sample Wilcoxon檢定(Wilcoxon test) (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.4.\nWhat about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.4.\n\n\n\n\nTable 11.4:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\næ­£å·®ç°å¼7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests\n\n\n\n\n\n\n11.10.2 獨立樣本的曼－惠特尼U檢定\nI’ll start by describing the 曼－惠特尼U檢定(Mann-Whitney U test), since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nI’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.5).\n\n\n\n\nTable 11.5:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 25 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for the Mann-Whitney \\(U\\) test"
  },
  {
    "objectID": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-01",
    "href": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-01",
    "title": "11  比較單一與兩組平均值",
    "section": "11.1 單一樣本z檢定2",
    "text": "11.1 單一樣本z檢定2\n在本節中，我將介紹統計學中最無用的測試之一：z 檢定。認真地說，這種測試在現實生活中幾乎從不使用。它唯一的真正用途是，在教授統計學時，它是通往 t 檢定的一個非常方便的墊腳石，而 t 檢定可能是統計學中最（過度）使用的工具。\n\n\n11.1.1 使用z檢定前的注意事項\n為了介紹 z 檢定背後的概念，讓我們舉一個簡單的例子。我的一位朋友 Zeppo 博士按曲線給他的入門統計學班級打分。假設他班級的平均分數是 \\(67.5\\)，標準差是 \\(9.5\\)。他有很多學生，其中有 20 個學生還修了心理學課程。出於好奇心，我想知道這些心理學生的成績是否傾向於與其他人獲得相同的成績（即平均分數為 \\(67.5\\)），還是他們的成績往高或往低得分？他給我發了一個 zeppo.csv 文件，我用它來查看這些學生的成績，並在 jamovi 試算表視圖中計算了 “探索” - “描述性統計” 中的平均值。3平均值為 \\(72.3\\)。\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\n嗯，也許心理學生的成績比正常情況高一些。樣本平均值 \\(\\bar{X}=72.3\\) 比假設的母體平均值 \\(\\mu=67.5\\) 要高得多，但是，另一方面，樣本大小 \\(N=20\\) 並不是很大。也許這只是純粹的巧合。\n為了回答這個問題，有助於能夠寫下我所知道的。首先，我知道樣本平均值為 \\(\\bar{X}=72.3\\)。如果我願意假設心理學生的標準差與班上其他學生的標準差相同，那麼我可以說母體標準差為 \\(\\sigma=9.5\\)。我還假設由於 Zeppo 博士是按曲線給分，心理學生的成績服從正態分布。\n接下來，要明確我希望從數據中學到什麼。在這種情況下，我的研究假設與心理學生的成績母體平均值 \\(\\mu\\) 相關，而這個值是未知的。具體而言，我想知道 \\(\\mu=67.5\\) 是否成立。鑒於這是我所知道的，我們能否設計一個假設檢定來解決我們的問題？數據以及它們被認為來自的假設分佈顯示在 Figure 11.1 中。不是非常明顯什麼是正確的答案，對嗎？為此，我們需要一些統計學知識。\n\n\n\n\n\n\nFigure 11.1: 心理學生的成績（柱形圖）應該是由理論分佈（實線）生成的。\n\n\n\n\n\n\n11.1.2 建立z檢定的假設\n構建假設檢定的第一步是明確虛無假設和對立假設是什麼。這不太難做到。我們的虛無假設 \\(H_0\\) 是，心理學生的成績母體平均值 \\(\\mu\\) 為 \\(67.5\\%\\)，而我們的對立假設是母體平均值不是 \\(67.5\\%\\)。如果我們用數學符號表示，這些假設就變成了：\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\n不過老實說，這種表示法對我們理解問題沒有太大幫助，它只是一種簡潔的寫下我們試圖從數據中學到什麼的方法。我們測試的虛無假設 \\(H_0\\) 和對立假設 \\(H_1\\) 都在 Figure 11.2 中有圖示。除了提供這些假設外，上面概述的情況還為我們提供了相當多的有用的背景知識。具體而言，有兩個特殊的信息可以添加：\n\n心理學成績服從正態分布。\n這些分數的真實標準差 \\(\\sigma\\) 已知為 9.5。\n\n暫時，我們將表現得好像這些是絕對可信的事實。在現實生活中，這種絕對可信的背景知識是不存在的，因此，如果我們想依賴這些事實，我們只能假設這些東西是真實的。但是，由於這些假設可能成立或不成立，我們可能需要檢查它們。不過，現在我們還是保持簡單。\n\n\n\n\n\n\nFigure 11.2: 單一樣本 \\(z\\) 檢定（雙側版本）所假設的虛無假設和對立假設的圖形說明。虛無假設和對立假設都假設母體分布是正態分布，並且進一步假設母體標準差已知（固定在某個值 \\(\\sigma_0\\)）。虛無假設（左）是母體平均值 \\(\\mu\\) 等於某個指定值 \\(\\mu_0\\)。對立假設（右）是母體平均值與此值不同，即 \\(\\mu \\neq \\mu_0\\)。\n\n\n\n\n下一步是找出一個好的診斷檢定統計量，這樣可以幫助我們區分 \\(H_0\\) 和 \\(H_1\\)。考慮到這些假設都涉及母體平均值 \\(\\mu\\)，你會相當有信心樣本平均值 \\(\\bar{X}\\) 是一個很有用的起點。我們可以計算樣本平均值 \\(\\bar{X}\\) 與虛無假設預測的母體平均值之間的差異。在我們的例子中，這意味著我們計算 \\(\\bar{X} - 67.5\\)。更一般地，如果讓 \\(\\mu_0\\) 指虛無假設聲稱為我們的母體平均值的值，那麼我們想要計算\n\\[\\bar{X}-\\mu_0\\]\n如果這個量等於或非常接近於0，虛無假設看起來是可接受的。如果這個量遠離0，那麼虛無假設似乎不太可能是有價值的。但是，它離0有多遠才能拒絕 H0 呢？\n為了弄清楚這一點，我們需要再狡猾一點，並且我們需要依賴我之前提到的那兩個背景知識，即原始數據服從正態分布，而且我們知道母體標準差 \\(\\sigma\\) 的值。如果虛無假設實際上是真的，真正的平均值是 \\(\\mu_0\\)，那麼這些事實一起意味著我們知道數據的完整母體分布：平均值為 \\(\\mu_0\\)，標準差為 \\(\\sigma\\) 的正態分布。4\n好的，如果這是真的，那麼我們可以關於 \\(\\bar{X}\\) 的分布說些什麼呢？嗯，正如我們之前討論過的（見 Section 8.3.3），平均數 \\(\\bar{X}\\) 的抽樣分布也是正態分布，並且具有平均值 \\(\\mu\\)。但是這個抽樣分布的標準差 \\(\\\\{se(\\bar{X})\\\\}\\)，也被稱為平均值的標準誤差，是5\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\n現在來了解這個技巧。我們可以將樣本平均數 \\(\\bar{X}\\) 轉換成標準分數（參見 Section 4.5）。這通常被寫成 z，但現在我將其稱為 \\(z_{\\bar{X}}\\)。使用這種擴展符號的原因是為了幫助您記住，我們正在計算樣本平均值的標準化版本，而不是單個觀察值的標準化版本，後者是通常指的 z 分數）。當我們這樣做時，樣本平均值的 z 分數為\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\]\n也可寫成\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n這個 z 分數就是我們的檢定統計量。使用這個作為我們的檢定統計量的好處是，像所有 z 分數一樣，它具有標準正態分布：6\n換句話說，無論原始數據所處的比例尺是什麼，z 統計量本身的解釋始終相同：它等於觀察到的樣本均值 \\(\\bar{X}\\) 與零假設所預測的母體均值 \\(\\mu_0\\) 相隔的標準誤數。更好的是，無論原始分數的母體參數是什麼，z 檢定的 5% 臨界區域始終相同，如 Figure 11.3 所示。而這意味著，在人們手動進行所有統計計算的時代，某人可以發表像 Table 11.1 這樣的表格。這反過來又意味著研究人員可以手動計算他們的 z 統計量，然後在教科書中查找臨界值。\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\n\n\nTable 11.1:  各種顯著水準的臨界值 \n\ncritical z value\n\ndesired \\(\\alpha\\) leveltwo-sided testone-sided test\n\n.11.6448541.281552\n\n.051.9599641.644854\n\n.012.5758292.326348\n\n.0013.2905273.090232\n\n\n\n\n\n\n\n11.1.3 手作z檢定\n現在，正如我之前提到的，z-test在實際應用中幾乎從不使用。這個測試在實際生活中如此罕見，以至於jamovi的基本安裝不包含內置功能。然而，這個測試是如此地簡單，以至於手動進行這個測試非常容易。讓我們回到Dr Zeppo班級的數據。在載入成績數據後，我需要做的第一件事是計算樣本均值，我已經做到了(\\(72.3\\))。我們已經有了已知的母體標準差(\\(\\sigma = 9.5\\))，零假設所指定的母體平均值(\\(\\mu_0 = 67.5\\))的值，以及樣本大小(\\(N=20\\))。\n\n\n\n\n\n\nFigure 11.3: 雙尾 z-檢定 (面板(a)) 和單尾 z-檢定 (面板(b)) 的拒絕區域\n\n\n\n\n接下來，讓我們計算（真實）標準誤（可輕鬆用計算機完成）：\n\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\n最後，我們計算我們的z分數：\n\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\n此時，我們會傳統地在臨界值表中查詢 \\(2.26\\) 的值。我們原來的假設是雙邊的（我們對心理學生在統計學上表現得比其他學生好或差沒有任何理論基礎），因此我們的假設檢驗也是雙邊的（或者是兩尾的）。從我先前顯示的小表中，我們可以看到 \\(2.26\\) 大於需要在 \\(\\alpha = .05\\) 的顯著性水平下具有顯著性的臨界值 \\(1.96\\)，但小於需要在 \\(\\alpha = .01\\) 的水平下具有顯著性的值 \\(2.58\\)。因此，我們可以得出結論，我們可以這樣寫：\n\n在心理學學生的樣本中，平均成績為 \\(73.2\\)，假定真正的人口標準差為 \\(9.5\\)，我們可以得出結論，心理學學生在統計學上的得分與班級平均分有顯著差異（\\(z = 2.26, N = 20, p<.05\\)）。\n\n\n\n\n\n11.1.4 z檢定的適用條件\n如前所述，所有統計檢驗都有其前提假設，有些假設是合理的，有些則不是。我剛剛介紹的「單樣本 z 檢定」也有三個基本的前提假設，分別為：\n\n常態性。通常所描述的z-test假設真正的母體分布是正態的。7這通常是一個相當合理的假設，如果我們感到擔憂，也是我們可以檢查的假設（請參見檢查樣本的正態性一節）。\n獨立性。測試的第二個假設是資料集中的觀察結果彼此不相關或以某種有趣的方式相關。這在統計上不是那麼容易檢查，而是有賴於良好的實驗設計。違反此假設的一個明顯（且愚蠢）的例子是將同一個觀察結果在資料文件中“複製”多次，以便您最終獲得只有一個真正觀察結果的龐大“樣本大小”。更現實的是，您必須問自己，是否真的可以想像每個觀察結果都是從您感興趣的人口中完全隨機抽樣得到的。在實踐中，此假設永遠不會被滿足，但我們會盡力設計研究來最小化相關資料的問題。\n已知標準差假設。z檢驗的第三個假設是，研究人員知道母體的真實標準差。這簡直太傻了。在沒有真正的世界數據分析問題中，你知道母體的標準差\\(\\sigma\\)，但對平均值\\(\\mu\\)卻一無所知。換句話說，這個假設永遠是錯的。\n\n鑒於假設 \\(\\alpha\\) 已知的荒謬，我們試著不使用它。這將我們帶離了 z-test 這個枯燥領域，走進了有獨角獸、仙女和小矮人的神奇王國，那就是 t-test！"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-nonparameter-rank-test",
    "href": "11-Comparing-two-means.html#sec-nonparameter-rank-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 等級資料的平均值檢定25",
    "text": "11.10 等級資料的平均值檢定25\n與此同時，當數據變得非正態時，展示一下 QQ 圖和 Shapiro-Wilk 檢定的變化可能是值得的。為此，讓我們看一下我們的澳式足球聯賽（AFL）勝利幅度數據的分佈，如果您還記得@sec-描述性統計，它看起來根本不是來自正態分佈。以下是 QQ 圖發生的情況（Figure 11.25）。\n而當我們對 AFL margins 數據執行 Shapiro-Wilk 檢定時，我們得到了 Shapiro-Wilk 正態檢定統計量的值 \\(W = 0.94\\)，p值 = \\(9.481\\)x\\(10^{-07}\\)。顯然是一個顯著的效應！\n\n\n11.10.1 單一樣本的Wilcoxon檢定\n那麼，單樣本 Wilcoxon 檢定（Wilcoxon test）（或等效地，成對樣本 Wilcoxon 檢定）呢？假設我有興趣了解上統計課對學生的幸福感有什麼影響。我的數據在 happiness.csv 文件中。這裡我測量的是每個學生在上課前和上課後的幸福感，變化分數是兩者之間的差距。就像我們在 t 檢驗中看到的那樣，在進行成對樣本檢定之前和之後，與使用變化分數進行單樣本檢定之間，沒有本質區別。與以前一樣，考慮該檢定的最簡單方法是進行標記。這次要做的是，對於正向變化的變化分數，將它們與所有完整樣本一起標記。最終，您得到的表格看起來像 Table 11.4。\n\n\n\n\n\n\nTable 11.4:  比較單樣本 Wilcoxon U 檢定中各組觀察值 \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\næ­£å·®ç°å¼7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\n這一次的探討，我們有關於Wilcoxon符號等級檢定的問題。Wilcoxon符號等級檢定是用於對兩個樣本進行比較的非參數檢定，而不需要對母體進行任何假設。本次的Wilcoxon符號等級檢定結果是\\(W=7\\)，我們有一個顯著的效果。顯然，修讀統計學對幸福感有影響。在 jamovi 中執行此測試，您可以在“一样本t-Test”分析窗口中的“Tests”下指定“Wilcoxon rank”选项，这将给出Wilcoxon \\(W=7\\)，p-value=\\(0.03711\\)的结果。轉換成配對樣本版本的測試當然不會給我們不同的答案；詳見 Figure 11.26 。\n\n\n\n\n\n\nFigure 11.26: 單樣本和成對樣本 Wilcoxon 非參數檢驗結果的 jamovi 示範畫面\n\n\n\n\n\n\n11.10.2 獨立樣本的曼－惠特尼U檢定\n我們先從曼－惠特尼U檢定(Mann-Whitney U test) 開始講起，因為它比單一樣本版本還要簡單。假設我們正在研究一個包含10個人在某個測試中得分的資料集。由於我的想像力已經完全失靈，所以讓我們假裝這是一個”超讚指數測試”，而有兩個不同的組別，分別為”A”和”B”。我很好奇哪一個組別更厲害。這些資料包含在awesome.csv檔案中，除了通常的ID變數之外，還有兩個變數：scores和group。\n只要沒有平手的情況發生（即，有人得到完全相同的超讚指數分數），那麼我們要進行的測試就非常簡單了。我們只需要建立一個表格，將組A中的每個觀察值與組B中的每個觀察值進行比較。當組A數據大於組B時，在表格中放置一個勾號(Table 11.5)。\n\n\n\n\n\n\nTable 11.5:  對於雙樣本曼－惠特尼U檢定，比較各組觀測值 \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\n我們然後計算出勾勾的數量，這就是我們的檢定統計量 W26。W的抽樣分佈實際上有些複雜，我會略過細節。對於我們的目的而言，關鍵是W的解釋在質性上與t或z的解釋相同。也就是說，如果我們需要進行雙側檢定，則當W非常大或非常小時，我們會拒絕虛無假設，但如果我們有一個單向（即單側）假設，那麼我們只使用其中之一。\n在jamovi中，如果我們將成績作為因變量，組別作為分組變量運行“獨立樣本T檢驗”，然後在“測試”選項中勾選“曼-惠特尼U檢定”選項，我們將獲得顯示\\(U=3\\)（即上面顯示的檢查記號數）和p值= 0.05556的結果。參見 Figure 11.27 。\n\n\n\n\n\n\nFigure 11.27: jamovi螢幕顯示曼-惠特尼\\(U\\)檢定的結果"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.3 jamovi的變異數分析模組",
    "text": "12.3 jamovi的變異數分析模組\n我相當確定在讀完上一節之後，您在想什麼，特別是如果您按照我的建議，用鉛筆和紙（即在試算表中）自己完成所有這些工作。自己做 ANOVA 計算很糟糕。沿途我們需要做相當多的計算，如果每次想做 ANOVA 都要一次又一次地做這些計算，會讓人厭煩。\n\n\n12.3.1 使用jamovi完成變異數分析\n為了讓您的生活更輕鬆，jamovi 可以做 ANOVA… 哈拉！ 轉到「ANOVA」-「ANOVA」分析，將 mood.gain 變量移到「依賴變量」框中，然後將 drug 變量移到「固定因子」框中。這樣應該會得到 Figure 12.3 中所示的結果。8 注意我還勾選了 ’Effect Size’選項下的 \\(\\eta^2\\) 复选框，念作“ eta 平方”，這也顯示在結果表格上。稍後我們將回到效應大小。\n\n\n\n\n\n\n\nFigure 12.3: jamovi的結果表格，用於根據施用的藥物進行情緒增益的 ANOVA。\n\n\n\n\njamovi 的結果表格顯示了平方和值、自由度以及我們現在並不真正感興趣的其他一些數量。然而，請注意，jamovi 不使用「組間」和「組內」這兩個名稱。 相反，它嘗試分配更有意義的名稱。 在我們的特定示例中，組間方差對應於藥物對結果變量的影響，組內方差對應於“剩餘”的可變性，因此它將其稱為殘差。 如果我們將這些數字與 [A worked example] 中我手工計算的數字進行比較，可以看到它們或多或少是相同的，除了四捨五入誤差。組間平方和為 \\(SS_b = 3.45\\)，組內平方和為 \\(SS_w = 1.39\\)，各自的自由度為 \\(2\\) 和 \\(15\\)。我們還得到了 F 值和 p 值，同樣，這些數字與我們在手工計算時的數字差不多相同，只是四捨五入誤差。"
  }
]