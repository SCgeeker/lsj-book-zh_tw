[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "用jamovi上手統計學",
    "section": "",
    "text": "本書內容涵蓋各大學心理學、公共衛生或社會科學大學部基礎統計的學習項目。本書同時提供以jamovi做為處理資料的工具操作指引。如同一般的統計教科書，本書從描述統計及統計圖開始，接著討論機率理論，取樣及估計，還有虛無假設檢定。理解理論概念之後，接著學習統計方法：包括列聯表分析、相關、t檢定、迴歸、變異數分析以及因素分析。最後一章將介紹貝氏統計。\n\n\n\n引用建議(英): Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15"
  },
  {
    "objectID": "16-Bayesian-statistics.html#probabilistic-reasoning-by-rational-agents",
    "href": "16-Bayesian-statistics.html#probabilistic-reasoning-by-rational-agents",
    "title": "16  貝氏統計",
    "section": "16.1 Probabilistic reasoning by rational agents",
    "text": "16.1 Probabilistic reasoning by rational agents\nFrom a Bayesian perspective statistical inference is all about belief revision. I start out with a set of candidate hypotheses h about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.\n\nI’m carrying an umbrella. Do you think it will rain?\n\nIn this problem I have presented you with a single piece of data (d = I’m carrying the umbrella), and I’m asking you to tell me your belief or hypothesis about whether it’s raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem?\n\n16.1.1 Priors: what you believed before\nThe first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia2 and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see Table 16.1). If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you I’m betting that this really is what you believe) then what I have written here is your prior distribution, written \\(P(h)\\).\n\n\n\n\nTable 16.1:  How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall \n\nHypothesisDegree of Belief\n\nRainy day0.15\n\nDry day0.85\n\n\n\n\n\n\n\n16.1.2 Likelihoods: theories about the data\nTo solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,3 and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write this out as in Table 16.2.\n\n\n\n\nTable 16.2:  How likely am I to be carrying an umbrella on rainy and dry days \n\nDataData\n\nHypothesisUmbrellaNo umbrella\n\nRainy day0.300.70\n\nDry day0.050.95\n\n\n\n\n\nIt’s important to remember that each cell in this table describes your beliefs about what data d will be observed, given the truth of a particular hypothesis \\(h\\). This “conditional probability” is written \\(P(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. In Bayesian statistics, this is referred to as the likelihood of the data \\(d\\) given the hypothesis \\(h\\).4\n\n\n16.1.3 The joint probability of data and hypothesis\nAt this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it so I’m going to try to build it up from more basic ideas.\nLet’s start out with one of the rules of probability theory. I listed it way back in Table 7.1, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I’m carrying an umbrella (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(P(d,h)\\), and you can calculate it by multiplying the prior \\(P(h)\\) by the likelihood \\(P(d|h)\\). Mathematically, we say that\n\\[P(d,h)=P(d|h)P(h)\\]\nSo, what is the probability that today is a rainy day and I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is \\(30\\%\\). So the probability that both of these things are true is calculated by multiplying the two\n\\[\n\\begin{split}\nP(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\n& = 0.30 \\times 0.15 \\\\\n& = 0.045\n\\end{split}\n\\]\nIn other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with Table 16.3.\n\n\n\n\nTable 16.3:  Four possibilities combining rain (or not) and umbrella carrying (or not) \n\nUmbrellaNo-umbrella\n\nRainy0.0450.105\n\nDry0.04250.807\n\n\n\n\n\nThis table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us Table 16.4.\n\n\n\n\nTable 16.4:  Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals \n\nUmbrellaNo-umbrellaTotal\n\nRainy0.0450.1050.15\n\nDry0.04250.8070.85\n\nTotal0.08750.9121\n\n\n\n\n\nThis is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course, as that’s our prior.5 The important thing isn’t the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.\nNow, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (Table 16.5):\n\n\n\n\nTable 16.5:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities \n\nUmbrellaNo-umbrella\n\nRainyP(Umbrella, Rainy)P(No-umbrella, Rainy)P(Rainy)\n\nDryP(Umbrella, Dry)P(No-umbrella, Dry)P(Dry)\n\nP(Umbrella)P(No-umbrella)\n\n\n\n\n\nFinally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let \\(d_1\\) refer to the possibility that you observe me carrying an umbrella, and \\(d_2\\) refers to you observing me not carrying one. Similarly, \\(h_1\\) is your hypothesis that today is rainy, and \\(h_2\\) is the hypothesis that it is not. Using this notation, the table looks like Table 16.6.\n\n\n\n\nTable 16.6:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities \n\n\\( d_1 \\)\\( d_2 \\)\n\n\\( h_1 \\)\\(P(h_1, d_1)\\)\\(P(h_1, d_2)\\)\\( P(h_1) \\)\n\n\\( h_2 \\)\\(P(h_2, d_1)\\)\\(P(h_2, d_2)\\)\\( P(h_2) \\)\n\n\\( P(d_1) \\)\\( P(d_2) \\)\n\n\n\n\n\n\n\n16.1.4 Updating beliefs using Bayes’ rule\nThe table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now know that I have an umbrella.6 To reflect this new knowledge, our revised table must have the following numbers. (see Table 16.7).\n\n\n\n\nTable 16.7:  Revising beliefs given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0\n\nDry0\n\nTotal10\n\n\n\n\n\nIn other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that \\(P(umbrella) = 1\\).\nWhat two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is slightly more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like Table 16.8, perhaps?\n\n\n\n\nTable 16.8:  Revising probabilities given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0.5140\n\nDry0.4860\n\nTotal10\n\n\n\n\n\nWhat this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The posterior probability of rain \\(P(h\\|d)\\) given that I am carrying an umbrella is 51.4%\nHow did I calculate these numbers? You can probably guess. To work out that there was a \\(0.514\\) probability of “rain”, all I did was take the \\(0.045\\) probability of “rain and umbrella” and divide it by the \\(0.0875\\) chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data \\(P(d, h)\\) by the marginal probability of the data \\(P(d)\\), and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: 7\n\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]\nHowever, remember what I said at the start of the last section, namely that the joint probability \\(P(d, h)\\) is calculated by multiplying the prior Pphq by the likelihood \\(P(d|h)\\). In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:\n\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule."
  },
  {
    "objectID": "16-Bayesian-statistics.html#bayesian-hypothesis-tests",
    "href": "16-Bayesian-statistics.html#bayesian-hypothesis-tests",
    "title": "16  貝氏統計",
    "section": "16.2 Bayesian hypothesis tests",
    "text": "16.2 Bayesian hypothesis tests\nIn Chapter 9 I described the orthodox approach to hypothesis testing. It took an entire chapter to describe, because null hypothesis testing is a very elaborate contraption that people find very hard to make sense of. In contrast, the Bayesian approach to hypothesis testing is incredibly simple. Let’s pick a setting that is closely analogous to the orthodox scenario. There are two hypotheses that we want to compare, a null hypothesis \\(h_0\\) and an alternative hypothesis \\(h_1\\). Prior to running the experiment we have some beliefs \\(P(h)\\) about which hypotheses are true. We run an experiment and obtain data d. Unlike frequentist statistics, Bayesian statistics does allow us to talk about the probability that the null hypothesis is true. Better yet, it allows us to calculate the posterior probability of the null hypothesis, using Bayes’ rule:\n\\[P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}\\]\nThis formula tells us exactly how much belief we should have in the null hypothesis after having observed the data d. Similarly, we can work out how much belief to place in the alternative hypothesis using essentially the same equation. All we do is change the subscript\n\\[P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}\\]\nIt’s all so simple that I feel like an idiot even bothering to write these equations down, since all I’m doing is copying Bayes rule from the previous section.8\n\n16.2.1 The Bayes factor\nIn practice, most Bayesian data analysts tend not to talk in terms of the raw posterior probabilities \\(P(h_0|d)\\) and \\(P(h_1|d)\\). Instead, we tend to talk in terms of the posterior odds ratio. Think of it like betting. Suppose, for instance, the posterior probability of the null hypothesis is 25%, and the posterior probability of the alternative is 75%. The alternative hypothesis is three times as probable as the null, so we say that the odds are 3:1 in favour of the alternative. Mathematically, all we have to do to calculate the posterior odds is divide one posterior probability by the other\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3\\]\nOr, to write the same thing in terms of the equations above\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}\\]\nActually, this equation is worth expanding on. There are three different terms here that you should know. On the left hand side, we have the posterior odds, which tells you what you believe about the relative plausibilty of the null hypothesis and the alternative hypothesis after seeing the data. On the right hand side, we have the prior odds, which indicates what you thought before seeing the data. In the middle, we have the Bayes factor, which describes the amount of evidence provided by the data. (Table 16.9).\n\n\n\n\nTable 16.9:  Posterior odds given the Bsyes factor and prior odds \n\n\\(\\frac{P(h_1|d)}{h_0|d}\\)\\(=\\)\\(\\frac{P(d|h_1)}{d|h_0}\\)\\(\\times \\)\\(\\frac{P(h_1)}{h_0}\\)\n\n\\(\\Uparrow\\)\\(\\Uparrow\\)\\(\\Uparrow\\)\n\nPosterior oddsBayes factorPrior odds\n\n\n\n\n\nThe Bayes factor (sometimes abbreviated as BF) has a special place in Bayesian hypothesis testing, because it serves a similar role to the p-value in orthodox hypothesis testing. The Bayes factor quantifies the strength of evidence provided by the data, and as such it is the Bayes factor that people tend to report when running a Bayesian hypothesis test. The reason for reporting Bayes factors rather than posterior odds is that different researchers will have different priors. Some people might have a strong bias to believe the null hypothesis is true, others might have a strong bias to believe it is false. Because of this, the polite thing for an applied researcher to do is report the Bayes factor. That way, anyone reading the paper can multiply the Bayes factor by their own personal prior odds, and they can work out for themselves what the posterior odds would be. In any case, by convention we like to pretend that we give equal consideration to both the null hypothesis and the alternative, in which case the prior odds equals 1, and the posterior odds becomes the same as the Bayes factor\n\n\n16.2.2 Interpreting Bayes factors\nOne of the really nice things about the Bayes factor is the numbers are inherently meaningful. If you run an experiment and you compute a Bayes factor of 4, it means that the evidence provided by your data corresponds to betting odds of 4:1 in favour of the alternative. However, there have been some attempts to quantify the standards of evidence that would be considered meaningful in a scientific context. The two most widely used are from Jeffreys (1961) and Kass & Raftery (1995). Of the two, I tend to prefer the Kass & Raftery (1995) table because it’s a bit more conservative. So here it is (Table 16.10).\n\n\n\n\nTable 16.10:  Bayes factors and strength of evidence \n\nBayes factorInterpretation\n\n1 - 3Negligible evidence\n\n3 - 20Positive evidence\n\n20 - 150Strong evidence\n\n> 150Very strong evidence\n\n\n\n\n\nAnd to be perfectly honest, I think that even the Kass & Raftery (1995) standards are being a bit charitable. If it were up to me, I’d have called the “positive evidence” category “weak evidence”. To me, anything in the range 3:1 to 20:1 is “weak” or “modest” evidence at best. But there are no hard and fast rules here. What counts as strong or weak evidence depends entirely on how conservative you are and upon the standards that your community insists upon before it is willing to label a finding as “true”.\nIn any case, note that all the numbers listed above make sense if the Bayes factor is greater than 1 (i.e., the evidence favours the alternative hypothesis). However, one big practical advantage of the Bayesian approach relative to the orthodox approach is that it also allows you to quantify evidence for the null. When that happens, the Bayes factor will be less than 1. You can choose to report a Bayes factor less than 1, but to be honest I find it confusing. For example, suppose that the likelihood of the data under the null hypothesis \\(P(d|h_0)\\) is equal to 0.2, and the corresponding likelihood \\(P(d|h_1)\\) under the alternative hypothesis is 0.1. Using the equations given above, Bayes factor here would be\n\\[BF=\\frac{P(d|h_1)}{P(d|h_0)}=\\frac{0.1}{0.2}=0.5\\]\nRead literally, this result tells is that the evidence in favour of the alternative is 0.5 to 1. I find this hard to understand. To me, it makes a lot more sense to turn the equation “upside down”, and report the amount op evidence in favour of the null. In other words, what we calculate is this\n\\[BF^{'}=\\frac{P(d|h_0)}{P(d|h_1)}=\\frac{0.2}{0.1}=2\\]\nAnd what we would report is a Bayes factor of 2:1 in favour of the null. Much easier to understand, and you can interpret this using the table above."
  },
  {
    "objectID": "16-Bayesian-statistics.html#why-be-a-bayesian",
    "href": "16-Bayesian-statistics.html#why-be-a-bayesian",
    "title": "16  貝氏統計",
    "section": "16.3 Why be a Bayesian",
    "text": "16.3 Why be a Bayesian\nUp to this point I’ve focused exclusively on the logic underpinning Bayesian statistics. We’ve talked about the idea of “probability as a degree of belief”, and what it implies about how a rational agent should reason about the world. The question that you have to answer for yourself is this: how do you want to do your statistics? Do you want to be an orthodox statistician, relying on sampling distributions and p-values to guide your decisions? Or do you want to be a Bayesian, relying on things like prior beliefs, Bayes factors and the rules for rational belief revision? And to be perfectly honest, I can’t answer this question for you. Ultimately it depends on what you think is right. It’s your call and your call alone. That being said, I can talk a little about why I prefer the Bayesian approach.\n\n16.3.1 Statistics that mean what you think they mean\n\nYou keep using that word. I do not think it means what you think it means\n– Inigo Montoya, The Princess Bride 9\n\nTo me, one of the biggest advantages to the Bayesian approach is that it answers the right questions. Within the Bayesian framework, it is perfectly sensible and allowable to refer to “the probability that a hypothesis is true”. You can even try to calculate this probability. Ultimately, isn’t that what you want your statistical tests to tell you? To an actual human being, this would seem to be the whole point of doing statistics, i.e., to determine what is true and what isn’t. Any time that you aren’t exactly sure about what the truth is, you should use the language of probability theory to say things like “there is an 80% chance that Theory A is true, but a 20% chance that Theory B is true instead”.\nThis seems so obvious to a human, yet it is explicitly forbidden within the orthodox framework. To a frequentist, such statements are a nonsense because “the theory is true” is not a repeatable event. A theory is true or it is not, and no probabilistic statements are allowed, no matter how much you might want to make them. There’s a reason why, back in Section 9.5, I repeatedly warned you not to interpret the p-value as the probability that the null hypothesis is true. There’s a reason why almost every textbook on statstics is forced to repeat that warning. It’s because people desperately want that to be the correct interpretation. Frequentist dogma notwithstanding, a lifetime of experience of teaching undergraduates and of doing data analysis on a daily basis suggests to me that most actual humans think that “the probability that the hypothesis is true” is not only meaningful, it’s the thing we care most about. It’s such an appealing idea that even trained statisticians fall prey to the mistake of trying to interpret a p-value this way. For example, here is a quote from an official Newspoll report in 2013, explaining how to interpret their (frequentist) data analysis:10\n\nThroughout the report, where relevant, statistically significant changes have been noted. All significance tests have been based on the 95 percent level of confidence. This means that if a change is noted as being statistically significant, there is a 95 percent probability that a real change has occurred, and is not simply due to chance variation. (emphasis added)\n\nNope! That’s not what p < .05 means. That’s not what 95% confidence means to a frequentist statistician. The bolded section is just plain wrong. Orthodox methods cannot tell you that “there is a 95% chance that a real change has occurred”, because this is not the kind of event to which frequentist probabilities may be assigned. To an ideological frequentist, this sentence should be meaningless. Even if you’re a more pragmatic frequentist, it’s still the wrong definition of a p-value. It is simply not an allowed or correct thing to say if you want to rely on orthodox statistical tools.\nOn the other hand, let’s suppose you are a Bayesian. Although the bolded passage is the wrong definition of a p-value, it’s pretty much exactly what a Bayesian means when they say that the posterior probability of the alternative hypothesis is greater than 95%. And here’s the thing. If the Bayesian posterior is actually the thing you want to report, why are you even trying to use orthodox methods? If you want to make Bayesian claims, all you have to do is be a Bayesian and use Bayesian tools.\nSpeaking for myself, I found this to be the most liberating thing about switching to the Bayesian view. Once you’ve made the jump, you no longer have to wrap your head around counter-intuitive definitions of p-values. You don’t have to bother remembering why you can’t say that you’re 95% confident that the true mean lies within some interval. All you have to do is be honest about what you believed before you ran the study and then report what you learned from doing it. Sounds nice, doesn’t it? To me, this is the big promise of the Bayesian approach. You do the analysis you really want to do, and express what you really believe the data are telling you.\n\n\n16.3.2 Evidentiary standards you can believe\n\nIf \\(p\\) is below .02 it is strongly indicated that the \\(null\\) hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 and consider that smaller values of \\(p\\) indicate a real discrepancy.\n– Sir Ronald Fisher (Fisher, 1925)\n\nConsider the quote above by Sir Ronald Fisher, one of the founders of what has become the orthodox approach to statistics. If anyone has ever been entitled to express an opinion about the intended function of p-values, it’s Fisher. In this passage, taken from his classic guide Statistical Methods for Research Workers, he’s pretty clear about what it means to reject a null hypothesis at p < .05. In his opinion, if we take p < .05 to mean there is “a real effect”, then “we shall not often be astray”. This view is hardly unusual. In my experience, most practitioners express views very similar to Fisher’s. In essence, the p < .05 convention is assumed to represent a fairly stringent evidential standard.\nWell, how true is that? One way to approach this question is to try to convert p-values to Bayes factors, and see how the two compare. It’s not an easy thing to do because a p-value is a fundamentally different kind of calculation to a Bayes factor, and they don’t measure the same thing. However, there have been some attempts to work out the relationship between the two, and it’s somewhat surprising. For example, Johnson (2013) presents a pretty compelling case that (for t-tests at least) the p < .05 threshold corresponds roughly to a Bayes factor of somewhere between 3:1 and 5:1 in favour of the alternative. If that’s right, then Fisher’s claim is a bit of a stretch. Let’s suppose that the null hypothesis is true about half the time (i.e., the prior probability of \\(H_0\\) is 0.5), and we use those numbers to work out the posterior probability of the null hypothesis given that it has been rejected at p < .05. Using the data from Johnson (2013), we see that if you reject the null at p ă .05, you’ll be correct about 80% of the time. I don’t know about you but, in my opinion, an evidential standard that ensures you’ll be wrong on 20% of your decisions isn’t good enough. The fact remains that, quite contrary to Fisher’s claim, if you reject at p < .05 you shall quite often go astray. It’s not a very stringent evidential threshold at all.\n\n\n16.3.3 The p-value is a lie.\n\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\nThe cake is a lie.\n– Portal11\n\nOkay, at this point you might be thinking that the real problem is not with orthodox statistics, just the p < .05 standard. In one sense, that’s true. The recommendation that Johnson (2013) gives is not that “everyone must be a Bayesian now”. Instead, the suggestion is that it would be wiser to shift the conventional standard to something like a p < .01 level. That’s not an unreasonable view to take, but in my view the problem is a little more severe than that. In my opinion, there’s a fairly big problem built into the way most (but not all) orthodox hypothesis tests are constructed. They are grossly naive about how humans actually do research, and because of this most p-values are wrong.\nSounds like an absurd claim, right? Well, consider the following scenario. You’ve come up with a really exciting research hypothesis and you design a study to test it. You’re very diligent, so you run a power analysis to work out what your sample size should be, and you run the study. You run your hypothesis test and out pops a p-value of 0.072. Really bloody annoying, right?\nWhat should you do? Here are some possibilities:\n\nYou conclude that there is no effect and try to publish it as a null result\nYou guess that there might be an effect and try to publish it as a “borderline significant” result\nYou give up and try a new study\nYou collect some more data to see if the p value goes up or (preferably!) drops below the “magic” criterion of p < .05\n\nWhich would you choose? Before reading any further, I urge you to take some time to think about it. Be honest with yourself. But don’t stress about it too much, because you’re screwed no matter what you choose. Based on my own experiences as an author, reviewer and editor, as well as stories I’ve heard from others, here’s what will happen in each case:\n\nLet’s start with option 1. If you try to publish it as a null result, the paper will struggle to be published. Some reviewers will think that p = .072 is not really a null result. They’ll argue it’s borderline significant. Other reviewers will agree it’s a null result but will claim that even though some null results are publishable, yours isn’t. One or two reviewers might even be on your side, but you’ll be fighting an uphill battle to get it through.\nOkay, let’s think about option number 2. Suppose you try to publish it as a borderline significant result. Some reviewers will claim that it’s a null result and should not be published. Others will claim that the evidence is ambiguous, and that you should collect more data until you get a clear significant result. Again, the publication process does not favour you.\nGiven the difficulties in publishing an “ambiguous” result like p = .072, option number 3 might seem tempting: give up and do something else. But that’s a recipe for career suicide. If you give up and try a new project every time you find yourself faced with ambiguity, your work will never be published. And if you’re in academia without a publication record you can lose your job. So that option is out.\nIt looks like you’re stuck with option 4. You don’t have conclusive results, so you decide to collect some more data and re-run the analysis. Seems sensible, but unfortunately for you, if you do this all of your p-values are now incorrect. All of them. Not just the p-values that you calculated for this study. All of them. All the p-values you calculated in the past and all the p-values you will calculate in the future. Fortunately, no-one will notice. You’ll get published, and you’ll have lied.\n\nWait, what? How can that last part be true? I mean, it sounds like a perfectly reasonable strategy doesn’t it? You collected some data, the results weren’t conclusive, so now what you want to do is collect more data until the the results are conclusive. What’s wrong with that?\nHonestly, there’s nothing wrong with it. It’s a reasonable, sensible and rational thing to do. In real life, this is exactly what every researcher does. Unfortunately, the theory of null hypothesis testing as I described it in Chapter 9 forbids you from doing this.12 The reason is that the theory assumes that the experiment is finished and all the data are in. And because it assumes the experiment is over, it only considers two possible decisions. If you’re using the conventional p < .05 threshold, those decisions are shown oin Table 16.11.\n\n\n\n\nTable 16.11:  Conventional Null hypothesis signicance testing (NHST) with p < .05) \n\nOutcomeAction\n\np less than .05Reject the null\n\np greater than .05Retain the null\n\n\n\n\n\nWhat you’re doing is adding a third possible action to the decision making problem. Specifically, what you’re doing is using the p-value itself as a reason to justify continuing the experiment. And as a consequence you’ve transformed the decision-making procedure into one that looks more like Table 16.12.\n\n\n\n\nTable 16.12:  Carrying on data collecting based on p-values obtained in preliminary testing \n\nOutcomeAction\n\np less than .05Stop the experiment and reject the null\n\np between .05 and .1Continue the experiment\n\np greater than .1Stop the experiment and retain the null\n\n\n\n\n\nThe “basic” theory of null hypothesis testing isn’t built to handle this sort of thing, not in the form I described in Chapter 9. If you’re the kind of person who would choose to “collect more data” in real life, it implies that you are not making decisions in accordance with the rules of null hypothesis testing. Even if you happen to arrive at the same decision as the hypothesis test, you aren’t following the decision process it implies, and it’s this failure to follow the process that is causing the problem.13 Your p-values are a lie.\nWorse yet, they’re a lie in a dangerous way, because they’re all too small. To give you a sense of just how bad it can be, consider the following (worst case) scenario. Imagine you’re a really super-enthusiastic researcher on a tight budget who didn’t pay any attention to my warnings above. You design a study comparing two groups. You desperately want to see a significant result at the \\(p < .05\\) level, but you really don’t want to collect any more data than you have to (because it’s expensive). In order to cut costs you start collecting data but every time a new observation arrives you run a t-test on your data. If the t-tests says \\(p < .05\\) then you stop the experiment and report a significant result. If not, you keep collecting data. You keep doing this until you reach your pre-defined spending limit for this experiment. Let’s say that limit kicks in at \\(N = 1000\\) observations. As it turns out, the truth of the matter is that there is no real effect to be found: the null hypothesis is true. So, what’s the chance that you’ll make it to the end of the experiment and (correctly) conclude that there is no effect? In an ideal world, the answer here should be 95%. After all, the whole point of the \\(p < .05\\) criterion is to control the Type I error rate at 5%, so what we’d hope is that there’s only a 5% chance of falsely rejecting the null hypothesis in this situation. However, there’s no guarantee that will be true. You’re breaking the rules. Because you’re running tests repeatedly, “peeking” at your data to see if you’ve gotten a significant result, all bets are off.\nSo how bad is it? The answer is shown as the solid black line in Figure 16.1, and it’s astoundingly bad. If you peek at your data after every single observation, there is a 49% chance that you will make a Type I error. That’s, um, quite a bit bigger than the 5% that it’s supposed to be. By way of comparison, imagine that you had used the following strategy. Start collecting data. Every single time an observation arrives, run Bayesian t-tests and look at the Bayes factor. I’ll assume that Johnson (2013) is right, and I’ll treat a Bayes factor of 3:1 as roughly equivalent to a p-value of .05.14 This time around, our trigger happy researcher uses the following procedure. If the Bayes factor is 3:1 or more in favour of the null, stop the experiment and retain the null. If it is 3:1 or more in favour of the alternative, stop the experiment and reject the null. Otherwise continue testing. Now, just like last time, let’s assume that the null hypothesis is true. What happens? As it happens, I ran the simulations for this scenario too, and the results are shown as the dashed line in Figure 16.1. It turns out that the Type I error rate is much much lower than the 49% rate that we were getting by using the orthodox t-test.\n\n\n\n\n\nFigure 16.1: How badly can things go wrong if you re-run your tests every time new data arrive? If you are a frequentist, the answer is very wrong\n\n\n\n\nIn some ways, this is remarkable. The entire point of orthodox null hypothesis testing is to control the Type I error rate. Bayesian methods aren’t actually designed to do this at all. Yet, as it turns out, when faced with a “trigger happy” researcher who keeps running hypothesis tests as the data come in, the Bayesian approach is much more effective. Even the 3:1 standard, which most Bayesians would consider unacceptably lax, is much safer than the p < .05 rule.\n\n\n16.3.4 Is it really this bad?\nThe example I gave in the previous section is a pretty extreme situation. In real life, people don’t run hypothesis tests every time a new observation arrives. So it’s not fair to say that the p < .05 threshold “really” corresponds to a 49% Type I error rate (i.e., \\(p = .49\\)). But the fact remains that if you want your p-values to be honest then you either have to switch to a completely different way of doing hypothesis tests or enforce a strict rule of no peeking. You are not allowed to use the data to decide when to terminate the experiment. You are not allowed to look at a “borderline” p-value and decide to collect more data. You aren’t even allowed to change your data analyis strategy after looking at data. You are strictly required to follow these rules, otherwise the p-values you calculate will be nonsense.\nAnd yes, these rules are surprisingly strict. As a class exercise a couple of years back, I asked students to think about this scenario. Suppose you started running your study with the intention of collecting \\(N = 80\\) people. When the study starts out you follow the rules, refusing to look at the data or run any tests. But when you reach \\(N = 50\\) your willpower gives in… and you take a peek. Guess what? You’ve got a significant result! Now, sure, you know you said that you’d keep running the study out to a sample size of \\(N = 80\\), but it seems sort of pointless now, right? The result is significant with a sample size of \\(N = 50\\), so wouldn’t it be wasteful and inefficient to keep collecting data? Aren’t you tempted to stop? Just a little? Well, keep in mind that if you do, your Type I error rate at \\(p < .05\\) just ballooned out to 8%. When you report \\(p < .05\\) in your paper, what you’re really saying is \\(p < .08\\). That’s how bad the consequences of “just one peek” can be.\nNow consider this. The scientific literature is filled with t-tests, ANOVAs, regressions and chi-square tests. When I wrote this book I didn’t pick these tests arbitrarily. The reason why these four tools appear in most introductory statistics texts is that these are the bread and butter tools of science. None of these tools include a correction to deal with “data peeking”: they all assume that you’re not doing it. But how realistic is that assumption? In real life, how many people do you think have “peeked” at their data before the experiment was finished and adapted their subsequent behaviour after seeing what the data looked like? Except when the sampling procedure is fixed by an external constraint, I’m guessing the answer is “most people have done it”. If that has happened, you can infer that the reported p-values are wrong. Worse yet, because we don’t know what decision process they actually followed, we have no way to know what the p-values should have been. You can’t compute a p-value when you don’t know the decision making procedure that the researcher used. And so the reported p-value remains a lie.\nGiven all of the above, what is the take home message? It’s not that Bayesian methods are foolproof. If a researcher is determined to cheat, they can always do so. Bayes’ rule cannot stop people from lying, nor can it stop them from rigging an experiment. That’s not my point here. My point is the same one I made at the very beginning of the book in Section 1.1: the reason why we run statistical tests is to protect us from ourselves. And the reason why “data peeking” is such a concern is that it’s so tempting, even for honest researchers. A theory for statistical inference has to acknowledge this. Yes, you might try to defend p-values by saying that it’s the fault of the researcher for not using them properly, but to my mind that misses the point. A theory of statistical inference that is so completely naive about humans that it doesn’t even consider the possibility that the researcher might look at their own data isn’t a theory worth having. In essence, my point is this:\n\nGood laws have their origins in bad morals.\n– Ambrosius Macrobius 15\n\nGood rules for statistical testing have to acknowledge human frailty. None of us are without sin. None of us are beyond temptation. A good system for statistical inference should still work even when it is used by actual humans. Orthodox null hypothesis testing does not.16"
  },
  {
    "objectID": "16-Bayesian-statistics.html#bayesian-t-tests",
    "href": "16-Bayesian-statistics.html#bayesian-t-tests",
    "title": "16  貝氏統計",
    "section": "16.4 Bayesian t-tests",
    "text": "16.4 Bayesian t-tests\nAn important type of statistical inference problem discussed in this book is comparing two means, discussed in some detail in Chapter 11 on t-tests. If you can remember back that far, you’ll recall that there are several versions of the t-test. I’ll talk a little about Bayesian versions of the independent samples t-tests and the paired samples t-test in this section.\n\n16.4.1 Independent samples t-test\nThe most common type of t-test is the independent samples t-test, and it arises when you have data as in the harpo.csv data set that we used in Chapter 11 on t-tests. In this data set, we have two groups of students, those who received lessons from Anastasia and those who took their classes with Bernadette. The question we want to answer is whether there’s any difference in the grades received by these two groups of students. Back in Chapter 11 I suggested you could analyse this kind of data using the Independent Samples t-test in jamovi, which gave us the results in Figure 16.2. As we obtain a p-value less than 0.05, we reject the null hypothesis.\n\n\n\n\n\nFigure 16.2: Independent Samples t-test result in jamovi\n\n\n\n\nWhat does the Bayesian version of the t-test look like? We can get the Bayes factor analysis by selecting the ‘Bayes factor’ checkbox under the ‘Tests’ option, and accepting the suggested default value for the ‘Prior’. This gives the results shown in the table in Figure 16.3. What we get in this table is a Bayes factor statistic of 1.75, meaning that the evidence provided by these data are about 1.8:1 in favour of the alternative hypothesis.\nBefore moving on, it’s worth highlighting the difference between the orthodox test results and the Bayesian one. According to the orthodox test, we obtained a significant result, though only barely. Nevertheless, many people would happily accept p = .043 as reasonably strong evidence for an effect. In contrast, notice that the Bayesian test doesn’t even reach 2:1 odds in favour of an effect, and would be considered very weak evidence at best. In my experience that’s a pretty typical outcome. Bayesian methods usually require more evidence before rejecting the null.\n\n\n\n\n\nFigure 16.3: Bayes factors analysis alongside Independent Samples t-Test\n\n\n\n\n\n\n16.4.2 Paired samples t-test\nBack in Section 11.5 I discussed the chico.csv data set in which student grades were measured on two tests, and we were interested in finding out whether grades went up from test 1 to test 2. Because every student did both tests, the tool we used to analyse the data was a paired samples t-test. Figure 16.4 shows the jamovi results table for the conventional paired t-test alongside the Bayes factor analysis. At this point, I hope you can read this output without any difficulty. The data provide evidence of about 6000:1 in favour of the alternative. We could probably reject the null with some confidence!\n\n\n\n\n\nFigure 16.4: Paired samples T-Test and Bayes Factor result in jamovi"
  },
  {
    "objectID": "16-Bayesian-statistics.html#summary",
    "href": "16-Bayesian-statistics.html#summary",
    "title": "16  貝氏統計",
    "section": "16.5 Summary",
    "text": "16.5 Summary\nThe first half of this chapter was focused primarily on the theoretical underpinnings of Bayesian statistics. I introduced the mathematics for how Bayesian inference works in the section on Probabilistic reasoning by rational agents, and gave a very basic overview of Bayesian hypothesis tests]. Finally, I devoted some space to talking about why I think Bayesian methods are worth using.\nThen I gave a practical example, with Bayesian t-tests. If you’re interested in learning more about the Bayesian approach, there are many good books you could look into. John Kruschke’s book Doing Bayesian Data Analysis is a pretty good place to start (Kruschke, 2011) and is a nice mix of theory and practice. His approach is a little different to the “Bayes factor” approach that I’ve discussed here, so you won’t be covering the same ground. If you’re a cognitive psychologist, you might want to check out Lee & Wagenmakers (2014). I picked these two because I think they’re especially useful for people in my discipline, but there’s a lot of good books out there, so look around!\n\n\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nJeffreys, H. (1961). The theory of probability (3rd ed.). Oxford.\n\n\nJohnson, V. E. (2013). Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 48, 19313–19317.\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773–795.\n\n\nKruschke, J. K. (2011). Doing Bayesian data analysis: A tutorial with R and BUGS. Academic Press.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press."
  },
  {
    "objectID": "Preface.html#history-and-license",
    "href": "Preface.html#history-and-license",
    "title": "Preface",
    "section": "History and License",
    "text": "History and License\nThis book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "Preface.html#preface-to-version-0.75",
    "href": "Preface.html#preface-to-version-0.75",
    "title": "前言",
    "section": "Preface to Version 0.75",
    "text": "Preface to Version 0.75\nIn this version we have updated the figures, images and text to maintain compatibility with latest versions of jamovi (2.2); many thanks to Peter Fisk for his help with this. Also tweaked and corrected are a few sections where improvements have been suggested by readers. This has mainly included fixing typos but also in places correcting conceptual detail, for example we have updated the information on kurtosis to reflect that it isn’t really about distribution “pointiness” and instead kurtosis is about whether data distributions have thin or fat tails. Thanks to all the readers who made suggestions, either through contacting me by email, or raising an issue on github.\nDavid Foxcroft\nFebruary 9th, 2022"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.70",
    "href": "Preface.html#preface-to-version-0.70",
    "title": "前言",
    "section": "Preface to Version 0.70",
    "text": "Preface to Version 0.70\nThis update from version 0.65 introduces some new analyses. In the ANOVA chapters we have added sections on repeated measures ANOVA and analysis of covariance (ANCOVA). In a new chapter we have introduced Factor Analysis and related techniques. Hopefully the style of this new material is consistent with the rest of the book, though eagle-eyed readers might spot a bit more of an emphasis on conceptual and practical explanations, and a bit less algebra. I’m not sure this is a good thing, and might add the algebra in a bit later. But it reflects both my approach to understanding and teaching statistics, and also some feedback I have received from students on a course I teach. In line with this, I have also been through the rest of the book and tried to separate out some of the algebra by putting it into a box or frame. It’s not that this stuff is not important or useful, but for some students they may wish to skip over it and therefore the boxing of these parts should help some readers.\nWith this version I am very grateful to comments and feedback received from my students and colleagues, notably Wakefield Morys-Carter, and also to numerous people all over the world who have sent in small suggestions and corrections - much appreciated, and keep them coming! One pretty neat new feature is that the example data files for the book can now be loaded into jamovi as an add-on module - thanks to Jonathon Love for helping with that.\nDavid Foxcroft\nFebruary 1st, 2019"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.65",
    "href": "Preface.html#preface-to-version-0.65",
    "title": "前言",
    "section": "Preface to Version 0.65",
    "text": "Preface to Version 0.65\nIn this adaptation of the excellent ‘Learning statistics with R’, by Danielle Navarro, we have replaced the statistical software used for the analyses and examples with jamovi. Although R is a powerful statistical programming language, it is not the first choice for every instructor and student at the beginning of their statistical learning. Some instructors and students tend to prefer the point-and-click style of software, and that’s where jamovi comes in. jamovi is software that aims to simplify two aspects of using R. It offers a point-and-click graphical user interface (GUI), and it also provides functions that combine the capabilities of many others, bringing a more SPSS- or SAS-like method of programming to R. Importantly, jamovi will always be free and open - that’s one of its core values - because jamovi is made by the scientific community, for the scientific community.\nWith this version I am very grateful for the help of others who have read through drafts and provided excellent suggestions and corrections, particularly Dr David Emery and Kirsty Walter.\nDavid Foxcroft\nJuly 1st, 2018"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.6",
    "href": "Preface.html#preface-to-version-0.6",
    "title": "前言",
    "section": "Preface to Version 0.6",
    "text": "Preface to Version 0.6\nThe book hasn’t changed much since 2015 when I released Version 0.5 – it’s probably fair to say that I’ve changed more than it has. I moved from Adelaide to Sydney in 2016 and my teaching profile at UNSW is different to what it was at Adelaide, and I haven’t really had a chance to work on it since arriving here! It’s a little strange looking back at this actually. A few quick comments…\n\nWeirdly, the book consistently misgenders me, but I suppose I have only myself to blame for that one :-) There’s now a brief footnote on page 12 that mentions this issue; in real life I’ve been working through a gender affirmation process for the last two years and mostly go by she/her pronouns. I am, however, just as lazy as I ever was so I haven’t bothered updating the text in the book.\nFor Version 0.6 I haven’t changed much I’ve made a few minor changes when people have pointed out typos or other errors. In particular it’s worth noting the issue associated with the etaSquared function in the lsr package (which isn’t really being maintained any more) in Section 14.4. The function works fine for the simple examples in the book, but there are definitely bugs in there that I haven’t found time to check! So please take care with that one.\nThe biggest change really is the licensing! I’ve released it under a Creative Commons licence (CC BY-SA 4.0, specifically), and placed all the source files to the associated GitHub repository, if anyone wants to adapt it.\n\nMaybe someone would like to write a version that makes use of the tidyverse… I hear that’s become rather important to R these days :-)\nBest,\nDanielle Navarro"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.5",
    "href": "Preface.html#preface-to-version-0.5",
    "title": "前言",
    "section": "Preface to Version 0.5",
    "text": "Preface to Version 0.5\nAnother year, another update. This time around, the update has focused almost entirely on the theory sections of the book. Chapters 9, 10 and 11 have been rewritten, hopefully for the better. Along the same lines, Chapter 17 is entirely new, and focuses on Bayesian statistics. I think the changes have improved the book a great deal. I’ve always felt uncomfortable about the fact that all the inferential statistics in the book are presented from an orthodox perspective, even though I almost always present Bayesian data analyses in my own work. Now that I’ve managed to squeeze Bayesian methods into the book somewhere, I’m starting to feel better about the book as a whole. I wanted to get a few other things done in this update, but as usual I’m running into teaching deadlines, so the update has to go out the way it is!\nDanielle Navarro\nFebruary 16, 2015"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.4",
    "href": "Preface.html#preface-to-version-0.4",
    "title": "前言",
    "section": "Preface to Version 0.4",
    "text": "Preface to Version 0.4\nA year has gone by since I wrote the last preface. The book has changed in a few important ways: Chapters 3 and 4 do a better job of documenting some of the time saving features of Rstudio, Chapters 12 and 13 now make use of new functions in the lsr package for running chi-square tests and t tests, and the discussion of correlations has been adapted to refer to the new functions in the lsr package. The soft copy of 0.4 now has better internal referencing (i.e., actual hyperlinks between sections), though that was introduced in 0.3.1. There’s a few tweaks here and there, and many typo corrections (thank you to everyone who pointed out typos!), but overall 0.4 isn’t massively different from 0.3.\nI wish I’d had more time over the last 12 months to add more content. The absence of any discussion of repeated measures ANOVA and mixed models more generally really does annoy me. My excuse for this lack of progress is that my second child was born at the start of 2013, and so I spent most of last year just trying to keep my head above water. As a consequence, unpaid side projects like this book got sidelined in favour of things that actually pay my salary! Things are a little calmer now, so with any luck version 0.5 will be a bigger step forward.\nOne thing that has surprised me is the number of downloads the book gets. I finally got some basic tracking information from the website a couple of months ago, and (after excluding obvious robots) the book has been averaging about 90 downloads per day. That’s encouraging: there’s at least a few people who find the book useful!\nDanielle Navarro\nFebruary 4, 2014"
  },
  {
    "objectID": "Preface.html#preface-to-version-0.3",
    "href": "Preface.html#preface-to-version-0.3",
    "title": "前言",
    "section": "Preface to Version 0.3",
    "text": "Preface to Version 0.3\nThere’s a part of me that really doesn’t want to publish this book. It’s not finished.\nAnd when I say that, I mean it. The referencing is spotty at best, the chapter summaries are just lists of section titles, there’s no index, there are no exercises for the reader, the organisation is suboptimal, and the coverage of topics is just not comprehensive enough for my liking. Additionally, there are sections with content that I’m not happy with, figures that really need to be redrawn, and I’ve had almost no time to hunt down inconsistencies, typos, or errors. In other words, this book is not finished. If I didn’t have a looming teaching deadline and a baby due in a few weeks, I really wouldn’t be making this available at all.\nWhat this means is that if you are an academic looking for teaching materials, a Ph.D. student looking to learn R, or just a member of the general public interested in statistics, I would advise you to be cautious. What you’re looking at is a first draft, and it may not serve your purposes. If we were living in the days when publishing was expensive and the internet wasn’t around, I would never consider releasing a book in this form. The thought of someone shelling out $80 for this (which is what a commercial publisher told me it would retail for when they offered to distribute it) makes me feel more than a little uncomfortable. However, it’s the 21st century, so I can post the pdf on my website for free, and I can distribute hard copies via a print-on-demand service for less than half what a textbook publisher would charge. And so my guilt is assuaged, and I’m willing to share! With that in mind, you can obtain free soft copies and cheap hard copies online, from the following webpages:\nSoft copy: www.compcogscisydney.com/learning-statistics-with-r.html\nHard copy: www.lulu.com/content/13570633\n[Ed: these links are defunct, try this instead: learningstatisticswithr.com]\nEven so, the warning still stands: what you are looking at is Version 0.3 of a work in progress. If and when it hits Version 1.0, I would be willing to stand behind the work and say, yes, this is a textbook that I would encourage other people to use. At that point, I’ll probably start shamelessly flogging the thing on the internet and generally acting like a tool. But until that day comes, I’d like it to be made clear that I’m really ambivalent about the work as it stands.\nAll of the above being said, there is one group of people that I can enthusiastically endorse this book to: the psychology students taking our undergraduate research methods classes (DRIP and DRIP:A) in 2013. For you, this book is ideal, because it was written to accompany your stats lectures. If a problem arises due to a shortcoming of these notes, I can and will adapt content on the fly to fix that problem. Effectively, you’ve got a textbook written specifically for your classes, distributed for free (electronic copy) or at near-cost prices (hard copy). Better yet, the notes have been tested: Version 0.1 of these notes was used in the 2011 class, Version 0.2 was used in the 2012 class, and now you’re looking at the new and improved Version 0.3. I’m not saying these notes are titanium plated awesomeness on a stick – though if you wanted to say so on the student evaluation forms, then you’re totally welcome to – because they’re not. But I am saying that they’ve been tried out in previous years and they seem to work okay. Besides, there’s a group of us around to troubleshoot if any problems come up, and you can guarantee that at least one of your lecturers has read the whole thing cover to cover!\nOkay, with all that out of the way, I should say something about what the book aims to be. At its core, it is an introductory statistics textbook pitched primarily at psychology students. As such, it covers the standard topics that you’d expect of such a book: study design, descriptive statistics, the theory of hypothesis testing, t-tests, χ 2 tests, ANOVA and regression. However, there are also several chapters devoted to the R statistical package, including a chapter on data manipulation and another one on scripts and programming. Moreover, when you look at the content presented in the book, you’ll notice a lot of topics that are traditionally swept under the carpet when teaching statistics to psychology students. The Bayesian/frequentist divide is openly disussed in the probability chapter, and the disagreement between Neyman and Fisher about hypothesis testing makes an appearance. The difference between probability and density is discussed. A detailed treatment of Type I, II and III sums of squares for unbalanced factorial ANOVA is provided. And if you have a look in the Epilogue, it should be clear that my intention is to add a lot more advanced content.\nMy reasons for pursuing this approach are pretty simple: the students can handle it, and they even seem to enjoy it. Over the last few years I’ve been pleasantly surprised at just how little difficulty I’ve had in getting undergraduate psych students to learn R. It’s certainly not easy for them, and I’ve found I need to be a little charitable in setting marking standards, but they do eventually get there. Similarly, they don’t seem to have a lot of problems tolerating ambiguity and complexity in presentation of statistical ideas, as long as they are assured that the assessment standards will be set in a fashion that is appropriate for them. So if the students can handle it, why not teach it? The potential gains are pretty enticing. If they learn R, the students get access to CRAN, which is perhaps the largest and most comprehensive library of statistical tools in existence. And if they learn about probability theory in detail, it’s easier for them to switch from orthodox null hypothesis testing to Bayesian methods if they want to. Better yet, they learn data analysis skills that they can take to an employer without being dependent on expensive and proprietary software.\nSadly, this book isn’t the silver bullet that makes all this possible. It’s a work in progress, and maybe when it is finished it will be a useful tool. One among many, I would think. There are a number of other books that try to provide a basic introduction to statistics using R, and I’m not arrogant enough to believe that mine is better. Still, I rather like the book, and maybe other people will find it useful, incomplete though it is.\nDanielle Navarro\nJanuary 13, 2013"
  },
  {
    "objectID": "Preface.html#沿革與版權說明",
    "href": "Preface.html#沿革與版權說明",
    "title": "前言",
    "section": "沿革與版權說明",
    "text": "沿革與版權說明\n本書改編自原作者更早開發的統計學教材~ DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\n中文化電子書\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#on-the-psychology-of-statistics",
    "href": "01-Why-do-we-learn-statistics.html#on-the-psychology-of-statistics",
    "title": "1  Why do we learn statistics",
    "section": "1.1 On the psychology of statistics",
    "text": "1.1 On the psychology of statistics\nTo the surprise of many students, statistics is a fairly significant part of a psychological education. To the surprise of no-one, statistics is very rarely the favourite part of one’s psychological education. After all, if you really loved the idea of doing statistics, you’d probably be enrolled in a statistics class right now, not a psychology class. So, not surprisingly, there’s a pretty large proportion of the student base that isn’t happy about the fact that psychology has so much statistics in it. In view of this, I thought that the right place to start might be to answer some of the more common questions that people have about stats.\nA big part of this issue at hand relates to the very idea of statistics. What is it? What’s it there for? And why are scientists so bloody obsessed with it? These are all good questions, when you think about it. So let’s start with the last one. As a group, scientists seem to be bizarrely fixated on running statistical tests on everything. In fact, we use statistics so often that we sometimes forget to explain to people why we do. It’s a kind of article of faith among scientists – and especially social scientists – that your findings can’t be trusted until you’ve done some stats. Undergraduate students might be forgiven for thinking that we’re all completely mad, because no-one takes the time to answer one very simple question:\nWhy do you do statistics? Why don’t scientists just use common sense?\nIt’s a naive question in some ways, but most good questions are. There’s a lot of good answers to it,2 but for my money, the best answer is a really simple one: we don’t trust ourselves enough. We worry that we’re human, and susceptible to all of the biases, temptations and frailties that humans suffer from. Much of statistics is basically a safeguard. Using “common sense” to evaluate evidence means trusting gut instincts, relying on verbal arguments and on using the raw power of human reason to come up with the right answer. Most scientists don’t think this approach is likely to work.\nIn fact, come to think of it, this sounds a lot like a psychological question to me, and since I do work in a psychology department, it seems like a good idea to dig a little deeper here. Is it really plausible to think that this “common sense” approach is very trustworthy? Verbal arguments have to be constructed in language, and all languages have biases – some things are harder to say than others, and not necessarily because they’re false (e.g., quantum electrodynamics is a good theory, but hard to explain in words). The instincts of our “gut” aren’t designed to solve scientific problems, they’re designed to handle day to day inferences – and given that biological evolution is slower than cultural change, we should say that they’re designed to solve the day to day problems for a different world than the one we live in. Most fundamentally, reasoning sensibly requires people to engage in “induction”, making wise guesses and going beyond the immediate evidence of the senses to make generalisations about the world. If you think that you can do that without being influenced by various distractors, well, I have a bridge in London I’d like to sell you. Heck, as the next section shows, we can’t even solve “deductive” problems (ones where no guessing is required) without being influenced by our pre-existing biases.\n\n1.1.1 The curse of belief bias\nPeople are mostly pretty smart. We’re certainly smarter than the other species that we share the planet with (though many people might disagree). Our minds are quite amazing things, and we seem to be capable of the most incredible feats of thought and reason. That doesn’t make us perfect though. And among the many things that psychologists have shown over the years is that we really do find it hard to be neutral, to evaluate evidence impartially and without being swayed by pre-existing biases. A good example of this is the belief bias effect in logical reasoning: if you ask people to decide whether a particular argument is logically valid (i.e., conclusion would be true if the premises were true), we tend to be influenced by the believability of the conclusion, even when we shouldn’t. For instance, here’s a valid argument where the conclusion is believable:\n\nAll cigarettes are expensive (Premise 1)\nSome addictive things are inexpensive (Premise 2)\nTherefore, some addictive things are not cigarettes (Conclusion)\n\nAnd here’s a valid argument where the conclusion is not believable:\n\nAll addictive things are expensive (Premise 1)\nSome cigarettes are inexpensive (Premise 2)\nTherefore, some cigarettes are not addictive (Conclusion)\n\nThe logical structure of argument #2 is identical to the structure of argument #1, and they’re both valid. However, in the second argument, there are good reasons to think that premise 1 is incorrect, and as a result it’s probably the case that the conclusion is also incorrect. But that’s entirely irrelevant to the topic at hand; an argument is deductively valid if the conclusion is a logical consequence of the premises. That is, a valid argument doesn’t have to involve true statements.\nOn the other hand, here’s an invalid argument that has a believable conclusion:\n\nAll addictive things are expensive (Premise 1)\nSome cigarettes are inexpensive (Premise 2)\nTherefore, some addictive things are not cigarettes (Conclusion)\n\nAnd finally, an invalid argument with an unbelievable conclusion:\n\nAll cigarettes are expensive (Premise 1)\nSome addictive things are inexpensive (Premise 2)\nTherefore, some cigarettes are not addictive (Conclusion)\n\nNow, suppose that people really are perfectly able to set aside their pre-existing biases about what is true and what isn’t, and purely evaluate an argument on its logical merits. We’d expect 100% of people to say that the valid arguments are valid, and 0% of people to say that the invalid arguments are valid. So if you ran an experiment looking at this, you’d expect to see data as in Table 1.1.\n\n\n\n\nTable 1.1:  Validity of arguments \n\nconclusion feels trueconclusion feels false\n\nargument is valid100\\(\\%\\) say \"valid\"100\\(\\%\\) say \"valid\"\n\nargument is invalid0\\(\\%\\) say \"valid\"0\\(\\%\\) say \"valid\"\n\n\n\n\n\nIf the psychological data looked like this (or even a good approximation to this), we might feel safe in just trusting our gut instincts. That is, it’d be perfectly okay just to let scientists evaluate data based on their common sense, and not bother with all this murky statistics stuff. However, you guys have taken psych classes, and by now you probably know where this is going.\nIn a classic study, Evans et al. (1983) ran an experiment looking at exactly this. What they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of the data, everything went the way you’d hope (Table 1.2).\n\n\n\n\nTable 1.2:  Pre-existing biases and argument validity \n\nconclusion feels trueconclusion feels false\n\nargument is valid92\\(\\%\\) say \"valid\"\n\nargument is invalid8\\(\\%\\) say \"valid\"\n\n\n\n\n\nNot perfect, but that’s pretty good. But look what happens when our intuitive feelings about the truth of the conclusion run against the logical structure of the argument (Table 1.3):\n\n\n\n\nTable 1.3:  Intuition and argument validity \n\nconclusion feels trueconclusion feels false\n\nargument is valid92\\(\\%\\) say \"valid\"46\\(\\%\\) say \"valid\"\n\nargument is invalid92\\(\\%\\) say \"valid\"8\\(\\%\\) say \"valid\"\n\n\n\n\n\nOh dear, that’s not as good. Apparently, when people are presented with a strong argument that contradicts our pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument (people only did so 46% of the time). Even worse, when people are presented with a weak argument that agrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that one wrong 92% of the time!).3\nIf you think about it, it’s not as if these data are horribly damning. Overall, people did do better than chance at compensating for their prior biases, since about 60% of people’s judgements were correct (you’d expect 50% by chance). Even so, if you were a professional “evaluator of evidence”, and someone came along and offered you a magic tool that improves your chances of making the right decision from 60% to (say) 95%, you’d probably jump at it, right? Of course you would. Thankfully, we actually do have a tool that can do this. But it’s not magic, it’s statistics. So that’s reason #1 why scientists love statistics. It’s just too easy for us to “believe what we want to believe”. So instead, if we want to “believe in the data”, we’re going to need a bit of help to keep our personal biases under control. That’s what statistics does, it helps keep us honest."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox",
    "href": "01-Why-do-we-learn-statistics.html#the-cautionary-tale-of-simpsons-paradox",
    "title": "1  Why do we learn statistics",
    "section": "1.2 The cautionary tale of Simpson’s paradox",
    "text": "1.2 The cautionary tale of Simpson’s paradox\nThe following is a true story (I think!). In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions (Table 1.4).\n\n\n\n\nTable 1.4:  Berkeley students by gender \n\nNumber of applicantsPercent admitted\n\nMales844244\\(\\%\\)\n\nFemales432135\\(\\%\\)\n\n\n\n\n\nGiven this, they were worried about being sued!4 Given that there were nearly 13,000 applicants, a difference of 9% in admission rates between males and females is just way too big to be a coincidence. Pretty compelling data, right? And if I were to say to you that these data actually reflect a weak bias in favour of women (sort of!), you’d probably think that I was either crazy or sexist.\nOddly, it’s actually sort of true. When people started looking more carefully at the admissions data they told a rather different story (Bickel et al., 1975). Specifically, when they looked at it on a department by department basis, it turned out that most of the departments actually had a slightly higher success rate for female applicants than for male applicants. Table 1.5 shows the admission figures for the six largest departments (with the names of the departments removed for privacy reasons):\n\n\n\n\nTable 1.5:  Berkeley students by gender for six largest Departments \n\nMales\n\nDepartmentApplicantsPercent admittedApplicantsPercent admitted\n\nA82562\\(\\%\\)10882\\(\\%\\)\n\nB56063\\(\\%\\)2568\\(\\%\\)\n\nC32537\\(\\%\\)59334\\(\\%\\)\n\nD41733\\(\\%\\)37535\\(\\%\\)\n\nE19128\\(\\%\\)39324\\(\\%\\)\n\nF2726\\(\\%\\)3417\\(\\%\\)\n\n\n\n\n\nRemarkably, most departments had a higher rate of admissions for females than for males! Yet the overall rate of admission across the university for females was lower than for males. How can this be? How can both of these statements be true at the same time?\nHere’s what’s going on. Firstly, notice that the departments are not equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get A>B>D>C>F>E (the “easy” departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C>E>D>F>A>B. In other words, what these data seem to be suggesting is that the female applicants tended to apply to “harder” departments. And in fact, if we look at Figure Figure 1.1 we see that this trend is systematic, and quite striking. This effect is known as Simpson’s paradox. It’s not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it’s real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are lots of subtle, counter-intuitive traps lying in wait for the unwary. That’s reason #2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.\nBefore leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves part of the problem. Remember that we started all this with the concern that Berkeley’s admissions processes might be unfairly biased against female applicants. When we looked at the “aggregated” data, it did seem like the university was discriminating against women, but when we “disaggregate” and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments. From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can’t dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce.\n\n\n\n\n\nFigure 1.1: The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from Bickel et al. (1975). Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot departments with fewer than 40 applicants\n\n\n\n\nThat was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.That was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.\nIn short there are a lot of critical questions that you can’t answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a tool to help you learn about your data. No more and no less. It’s a powerful tool to that end, but there’s no substitute for careful thought."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#statistics-in-psychology",
    "href": "01-Why-do-we-learn-statistics.html#statistics-in-psychology",
    "title": "1  Why do we learn statistics",
    "section": "1.3 Statistics in psychology",
    "text": "1.3 Statistics in psychology\nI hope that the discussion above helped explain why science in general is so focused on statistics. But I’m guessing that you have a lot more questions about what role statistics plays in psychology, and specifically why psychology classes always devote so many lectures to stats. So here’s my attempt to answer a few of them…\nWhy does psychology have so much statistics?\nTo be perfectly honest, there’s a few different reasons, some of which are better than others. The most important reason is that psychology is a statistical science. What I mean by that is that the “things” that we study are people. Real, complicated, gloriously messy, infuriatingly perverse people. The “things” of physics include objects like electrons, and while there are all sorts of complexities that arise in physics, electrons don’t have minds of their own. They don’t have opinions, they don’t differ from each other in weird and arbitrary ways, they don’t get bored in the middle of an experiment, and they don’t get angry at the experimenter and then deliberately try to sabotage the data set (not that I’ve ever done that!). At a fundamental level psychology is harder than physics.5 Basically, we teach statistics to you as psychologists because you need to be better at stats than physicists. There’s actually a saying used sometimes in physics, to the effect that “if your experiment needs statistics, you should have done a better experiment”. They have the luxury of being able to say that because their objects of study are pathetically simple in comparison to the vast mess that confronts social scientists. And it’s not just psychology. Most social sciences are desperately reliant on statistics. Not because we’re bad experimenters, but because we’ve picked a harder problem to solve. We teach you stats because you really, really need it.\nCan’t someone else do the statistics?\nTo some extent, but not completely. It’s true that you don’t need to become a fully trained statistician just to do psychology, but you do need to reach a certain level of statistical competence. In my view, there’s three reasons that every psychological researcher ought to be able to do basic statistics:\n\nFirstly, there’s the fundamental reason: statistics is deeply intertwined with research design. If you want to be good at designing psychological studies, you need to at the very least understand the basics of stats.\nSecondly, if you want to be good at the psychological side of the research, then you need to be able to understand the psychological literature, right? But almost every paper in the psychological literature reports the results of statistical analyses. So if you really want to understand the psychology, you need to be able to understand what other people did with their data. And that means understanding a certain amount of statistics.\nThirdly, there’s a big practical problem with being dependent on other people to do all your statistics: statistical analysis is expensive. If you ever get bored and want to look up how much the Australian government charges for university fees, you’ll notice something interesting: statistics is designated as a “national priority” category, and so the fees are much, much lower than for any other area of study. This is because there’s a massive shortage of statisticians out there. So, from your perspective as a psychological researcher, the laws of supply and demand aren’t exactly on your side here! As a result, in almost any real life situation where you want to do psychological research, the cruel facts will be that you don’t have enough money to afford a statistician. So the economics of the situation mean that you have to be pretty self-sufficient.\n\nNote that a lot of these reasons generalise beyond researchers. If you want to be a practicing psychologist and stay on top of the field, it helps to be able to read the scientific literature, which relies pretty heavily on statistics.\nI don’t care about jobs, research, or clinical work. Do I need statistics?\nOkay, now you’re just messing with me. Still, I think it should matter to you too. Statistics should matter to you in the same way that statistics should matter to everyone. We live in the 21st century, and data are everywhere. Frankly, given the world in which we live these days, a basic knowledge of statistics is pretty damn close to a survival tool! Which is the topic of the next section."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#statistics-in-everyday-life",
    "href": "01-Why-do-we-learn-statistics.html#statistics-in-everyday-life",
    "title": "1  Why do we learn statistics",
    "section": "1.4 Statistics in everyday life",
    "text": "1.4 Statistics in everyday life\n\n“We are drowning in information,\nbut we are starved for knowledge”\n- Various authors, original probably John Naisbitt\n\nWhen I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC news website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that I would call a statistical topic and 6 of those made a mistake. The most common error, if you’re curious, was failing to report baseline data (e.g., the article mentions that 5% of people in situation X have some characteristic Y, but doesn’t say how common the characteristic is for everyone else!). The point I’m trying to make here isn’t that journalists are bad at statistics (though they almost always are), it’s that a basic knowledge of statistics is very helpful for trying to figure out when someone else is either making a mistake or even lying to you. In fact, one of the biggest things that a knowledge of statistics does to you is cause you to get angry at the newspaper or the internet on a far more frequent basis. You can find a good example of this in Section 4.1.5 in Chapter 4. In later versions of this book I’ll try to include more anecdotes along those lines."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics",
    "href": "01-Why-do-we-learn-statistics.html#theres-more-to-research-methods-than-statistics",
    "title": "1  Why do we learn statistics",
    "section": "1.5 There’s more to research methods than statistics",
    "text": "1.5 There’s more to research methods than statistics\nSo far, most of what I’ve talked about is statistics, and so you’d be forgiven for thinking that statistics is all I care about in life. To be fair, you wouldn’t be far wrong, but research methodology is a broader concept than statistics. So most research methods courses will cover a lot of topics that relate much more to the pragmatics of research design, and in particular the issues that you encounter when trying to do research with humans. However, about 99% of student fears relate to the statistics part of the course, so I’ve focused on the stats in this discussion, and hopefully I’ve convinced you that statistics matters, and more importantly, that it’s not to be feared. That being said, it’s pretty typical for introductory research methods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite the contrary, in fact. Introductory classes focus a lot on the statistics because you almost always find yourself needing statistics before you need the other research methods training. Why? Because almost all of your assignments in other classes will rely on statistical training, to a much greater extent than they rely on other methodological tools. It’s not common for undergraduate assignments to require you to design your own study from the ground up (in which case you would need to know a lot about research design), but it is common for assignments to ask you to analyse and interpret data that were collected in a study that someone else designed (in which case you need statistics). In that sense, from the perspective of allowing you to do well in all your other classes, the statistics is more urgent.\nBut note that “urgent” is different from “important” – they both matter. I really do want to stress that research design is just as important as data analysis, and this book does spend a fair amount of time on it. However, while statistics has a kind of universality, and provides a set of core tools that are useful for most types of psychological research, the research methods side isn’t quite so universal. There are some general principles that everyone should think about, but a lot of research design is very idiosyncratic, and is specific to the area of research that you want to engage in. To the extent that it’s the details that matter, those details don’t usually show up in an introductory stats and research methods class.\n\n\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187, 398–404.\n\n\nEvans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On the conflict between logic and belief in syllogistic reasoning. Memory and Cognition, 11, 295–306."
  },
  {
    "objectID": "Epilogue.html#the-undiscovered-statistics",
    "href": "Epilogue.html#the-undiscovered-statistics",
    "title": "後記",
    "section": "The undiscovered statistics",
    "text": "The undiscovered statistics\nFirst, I’m going to talk a bit about some of the content that I wish I’d had the chance to cram into this version of the book, just so that you can get a sense of what other ideas are out there in the world of statistics. I think this would be important even if this book were getting close to a final product. One thing that students often fail to realise is that their introductory statistics classes are just that, an introduction. If you want to go out into the wider world and do real data analysis, you have to learn a whole lot of new tools that extend the content of your undergraduate lectures in all sorts of different ways. Don’t assume that something can’t be done just because it wasn’t covered in undergrad. Don’t assume that something is the right thing to do just because it was covered in an undergrad class. To stop you from falling victim to that trap, I think it’s useful to give a bit of an overview of some of the other ideas out there\n\nOmissions within the topics covered\nEven within the topics that I have covered in the book, there are a lot of omissions that I’d like to redress in future version of the book. Just sticking to things that are purely about statistics (rather than things associated with jamovi), the following is a representative but not exhaustive list of topics that I’d like to expand on at some time:\n\nOther types of correlations. In 單元 12 I talked about two types of correlation: Pearson and Spearman. Both of these methods of assessing correlation are applicable to the case where you have two continuous variables and want to assess the relationship between them. What about the case where your variables are both nominal scale? Or when one is nominal scale and the other is continuous? There are actually methods for computing correlations in such cases (e.g., polychoric correlation), and it would be good to see these included.\nMore detail on effect sizes. In general, I think the treatment of effect sizes throughout the book is a little more cursory than it should be. In almost every instance, I’ve tended just to pick one measure of effect size (usually the most popular one) and describe that. However, for almost all tests and models there are multiple ways of thinking about effect size, and I’d like to go into more detail in the future.\nDealing with violated assumptions. In a number of places in the book I’ve talked about some things you can do when you find that the assumptions of your test (or model) are violated, but I think that I ought to say more about this. In particular, I think it would have been nice to talk in a lot more detail about how you can tranform variables to fix problems. I talked a bit about this in 單元 6, but the discussion isn’t detailed enough I think.\nInteraction terms for regression. In 單元 14 I talked about the fact that you can have interaction terms in an ANOVA, and I also pointed out that ANOVA can be interpreted as a kind of linear regression model. Yet, when talking about regression in 單元 12 I made no mention of interactions at all. However, there’s nothing stopping you from including interaction terms in a regression model. It’s just a little more complicated to figure out what an “interaction” actually means when you’re talking about the interaction between two continuous predictors, and it can be done in more than one way. Even so, I would have liked to talk a little about this.\nMethod of planned comparison. As I mentioned this in 單元 14, it’s not always appropriate to be using a post hoc correction like Tukey’s HSD when doing an ANOVA, especially when you had a very clear (and limited) set of comparisons that you cared about ahead of time. I would like to talk more about this in the future.\nMultiple comparison methods. Even within the context of talking about post hoc tests and multiple comparisons, I would have liked to talk about the methods in more detail, and talk about what other methods exist besides the few options I mentioned.\n\n\n\nStatistical models missing from the book\nStatistics is a huge field. The core tools that I’ve described in this book (chi-square tests, t-tests, regression and ANOVA) are basic tools that are widely used in everyday data analysis, and they form the core of most introductory stats books. However, there are a lot of other tools out there. There are so very many data analysis situations that these tools don’t cover, and it would be great to give you a sense of just how much more there is, for example:\n\nNonlinear regression. When discussing regression in 單元 12, we saw that regression assumes that the relationship between predictors and outcomes is linear. On the other hand, when we talked about the simpler problem of correlation in 單元 4, we saw that there exist tools (e.g., Spearman correlations) that are able to assess non-linear relationships between variables. There are a number of tools in statistics that can be used to do non-linear regression. For instance, some non-linear regression models assume that the relationship between predictors and outcomes is monotonic (e.g., isotonic regression), while others assume that it is smooth but not necessarily monotonic (e.g., Lowess regression), while others assume that the relationship is of a known form that happens to be nonlinear (e.g., polynomial regression).\nLogistic regression. Yet another variation on regression occurs when the outcome variable is binary, but the predictors are continuous. For instance, suppose you’re investigating social media, and you want to know if it’s possible to predict whether or not someone is on Twitter as a function of their income, their age, and a range of other variables. This is basically a regression model, but you can’t use regular linear regression because the outcome variable is binary (you’re either on Twitter or you’re not). Because the outcome variable is binary, there’s no way that the residuals could possibly be normally distributed. There are a number of tools that statisticians can apply to this situation, the most prominent of which is logistic regression.\nThe General Linear Model (GLM). The GLM is actually a family of models that includes logistic regression, linear regression, (some) nonlinear regression, ANOVA and many others. The basic idea in the GLM is essentially the same idea that underpins linear models, but it allows for the idea that your data might not be normally distributed, and allows for nonlinear relationships between predictors and outcomes. There are a lot of very handy analyses that you can run that fall within the GLM, so it’s a very useful thing to know about.\nSurvival analysis. In 單元 2 I talked about “differential attrition”, the tendency for people to leave the study in a non-random fashion. Back then, I was talking about it as a potential methodological concern, but there are a lot of situations in which differential attrition is actually the thing you’re interested in. Suppose, for instance, you’re interested in finding out how long people play different kinds of computer games in a single session. Do people tend to play RTS (real time strategy) games for longer stretches than FPS (first person shooter) games? You might design your study like this. People come into the lab, and they can play for as long or as little as they like. Once they’re finished, you record the time they spent playing. However, due to ethical restrictions, let’s suppose that you cannot allow them to keep playing longer than two hours. A lot of people will stop playing before the two hour limit, so you know exactly how long they played. But some people will run into the two hour limit, and so you don’t know how long they would have kept playing if you’d been able to continue the study. As a consequence, your data are systematically censored: you’re missing all of the very long times. How do you analyse this data sensibly? This is the problem that survival analysis solves. It is specifically designed to handle this situation, where you’re systematically missing one “side” of the data because the study ended. It’s very widely used in health research, and in that context it is often literally used to analyse survival. For instance, you may be tracking people with a particular type of cancer, some who have received treatment A and others who have received treatment B, but you only have funding to track them for 5 years. At the end of the study period some people are alive, others are not. In this context, survival analysis is useful for determining which treatment is more effective, and telling you about the risk of death that people face over time.\nMixed models. Repeated measures ANOVA is often used in situations where you have observations clustered within experimental units. A good example of this is when you track individual people across multiple time points. Let’s say you’re tracking happiness over time, for two people. Aaron’s happiness starts at 10, then drops to 8, and then to 6. Belinda’s happiness starts at 6, then rises to 8 and then to 10. Both of these two people have the same “overall” level of happiness (the average across the three time points is 8), so a repeated measures ANOVA analysis would treat Aaron and Belinda the same way. But that’s clearly wrong. Aaron’s happiness is decreasing, whereas Belinda’s is increasing. If you want to optimally analyse data from an experiment where people can change over time, then you need a more powerful tool than repeated measures ANOVA. The tools that people use to solve this problem are called “mixed” models, because they are designed to learn about individual experimental units (e.g. happiness of individual people over time) as well as overall effects (e.g. the effect of money on happiness over time). Repeated measures ANOVA is perhaps the simplest example of a mixed model, but there’s a lot you can do with mixed models that you can’t do with repeated measures ANOVA.\nMultidimensional scaling. Factor analysis is an example of an “unsupervised learning” model. What this means is that, unlike most of the “supervised learning” tools I’ve mentioned, you can’t divide up your variables into predictors and outcomes. Regression is supervised learning whereas factor analysis is unsupervised learning. It’s not the only type of unsupervised learning model however. For example, in factor analysis one is concerned with the analysis of correlations between variables. However, there are many situations where you’re actually interested in analysing similarities or dissimilarities between objects, items or people. There are a number of tools that you can use in this situation, the best known of which is multidimensional scaling (MDS). In MDS, the idea is to find a “geometric” representation of your items. Each item is “plotted” as a point in some space, and the distance between two points is a measure of how dissimilar those items are.\nClustering. Another example of an unsupervised learning model is clustering (also referred to as classification), in which you want to organise all of your items into meaningful groups, such that similar items are assigned to the same groups. A lot of clustering is unsupervised, meaning that you don’t know anything about what the groups are, you just have to guess. There are other “supervised clustering” situations where you need to predict group memberships on the basis of other variables, and those group memberships are actually observables. Logistic regression is a good example of a tool that works this way. However, when you don’t actually know the group memberships, you have to use different tools (e.g., k-means clustering). There are even situations where you want to do something called “semi-supervised clustering”, in which you know the group memberships for some items but not others. As you can probably guess, clustering is a pretty big topic, and a pretty useful thing to know about.\nCausal models. One thing that I haven’t talked about much in this book is how you can use statistical modelling to learn about the causal relationships between variables. For instance, consider the following three variables which might be of interest when thinking about how someone died in a firing squad. We might want to measure whether or not an execution order was given (variable A), whether or not a marksman fired their gun (variable B), and whether or not the person got hit with a bullet (variable C). These three variables are all correlated with one another (e.g., there is a correlation between guns being fired and people getting hit with bullets), but we actually want to make stronger statements about them than merely talking about correlations. We want to talk about causation. We want to be able to say that the execution order (A) causes the marksman to fire (B) which causes someone to get shot (C). We can express this by a directed arrow notation: we write it as \\(A \\rightarrow B \\rightarrow C\\). This “causal chain” is a fundamentally different explanation for events than one in which the marksman fires first, which causes the shooting \\(B \\rightarrow C\\), and then causes the executioner to “retroactively” issue the execution order, \\(B \\rightarrow A\\). This “common effect” model says that A and C are both caused by B. You can see why these are different. In the first causal model, if we had managed to stop the executioner from issuing the order (intervening to change A), then no shooting would have happened. In the second model, the shooting would have happened any way because the marksman was not following the execution order. There is a big literature in statistics on trying to understand the causal relationships between variables, and a number of different tools exist to help you test different causal stories about your data. The most widely used of these tools (in psychology at least) is structural equations modelling (SEM), and at some point I’d like to extend the book to talk about it.\n\nOf course, even this listing is incomplete. I haven’t mentioned time series analysis, item response theory, market basket analysis, classification and regression trees, or any of a huge range of other topics. However, the list that I’ve given above is essentially my wish list for this book. Sure, it would double the length of the book, but it would mean that the scope has become broad enough to cover most things that applied researchers in psychology would need to use.\n\n\nOther ways of doing inference\nA different sense in which this book is incomplete is that it focuses pretty heavily on a very narrow and old-fashioned view of how inferential statistics should be done. In 單元 8 I talked a little bit about the idea of unbiased estimators, sampling distributions and so on. In 單元 9 I talked about the theory of null hypothesis significance testing and p-values. These ideas have been around since the early 20th century, and the tools that I’ve talked about in the book rely very heavily on the theoretical ideas from that time. I’ve felt obligated to stick to those topics because the vast majority of data analysis in science is also reliant on those ideas. However, the theory of statistics is not restricted to those topics and, whilst everyone should know about them because of their practical importance, in many respects those ideas do not represent best practice for contemporary data analysis. One of the things that I’m especially happy with is that I’ve been able to go a little beyond this. 單元 16 now presents the Bayesian perspective in a reasonable amount of detail, but the book overall is still pretty heavily weighted towards the frequentist orthodoxy. Additionally, there are a number of other approaches to inference that are worth mentioning:\n\nBootstrapping. Throughout the book, whenever I’ve introduced a hypothesis test, I’ve had a strong tendency just to make assertions like “the sampling distribution for BLAH is a t-distribution” or something like that. In some cases, I’ve actually attempted to justify this assertion. For example, when talking about \\(\\chi^2\\) tests in 單元 10 I made reference to the known relationship between normal distributions and \\(\\chi^2\\) distributions (see 單元 7) to explain how we end up assuming that the sampling distribution of the goodness-of-fit statistic is \\(\\chi^2\\) . However, it’s also the case that a lot of these sampling distributions are, well, wrong. The \\(\\chi^2\\) test is a good example. It is based on an assumption about the distribution of your data, an assumption which is known to be wrong for small sample sizes! Back in the early 20th century, there wasn’t much you could do about this situation. Statisticians had developed mathematical results that said that “under assumptions BLAH about the data, the sampling distribution is approximately BLAH”, and that was about the best you could do. A lot of times they didn’t even have that. There are lots of data analysis situations for which no-one has found a mathematical solution for the sampling distributions that you need. And so up until the late 20th century, the corresponding tests didn’t exist or didn’t work. However, computers have changed all that now. There are lots of fancy tricks, and some not-so-fancy, that you can use to get around it. The simplest of these is bootstrapping, and in it’s simplest form it’s incredibly simple. What you do is simulate the results of your experiment lots and lots of times, under the twin assumptions that (a) the null hypothesis is true and (b) the unknown population distribution actually looks pretty similar to your raw data. In other words, instead of assuming that the data are (for instance) normally distributed, just assume that the population looks the same as your sample, and then use computers to simulate the sampling distribution for your test statistic if that assumption holds. Despite relying on a somewhat dubious assumption (i.e., the population distribution is the same as the sample!) bootstrapping is quick and easy method that works remarkably well in practice for lots of data analysis problems.\nCross validation. One question that pops up in my stats classes every now and then, usually by a student trying to be provocative, is “Why do we care about inferential statistics at all? Why not just describe your sample?” The answer to the question is usually something like this, “Because our true interest as scientists is not the specific sample that we have observed in the past, we want to make predictions about data we might observe in the future”. A lot of the issues in statistical inference arise because of the fact that we always expect the future to be similar to but a bit different from the past. Or, more generally, new data won’t be quite the same as old data. What we do, in a lot of situations, is try to derive mathematical rules that help us to draw the inferences that are most likely to be correct for new data, rather than to pick the statements that best describe old data. For instance, given two models A and B, and a data set \\(X\\) you collected today, try to pick the model that will best describe a new data set \\(Y\\) that you’re going to collect tomorrow. Sometimes it’s convenient to simulate the process, and that’s what cross-validation does. What you do is divide your data set into two subsets, \\(X1\\) and \\(X2\\). Use the subset \\(X1\\) to train the model (e.g., estimate regression coefficients, let’s say), but then assess the model performance on the other one \\(X2\\). This gives you a measure of how well the model generalises from an old data set to a new one, and is often a better measure of how good your model is than if you just fit it to the full data set \\(X\\).\nRobust statistics. Life is messy, and nothing really works the way it’s supposed to. This is just as true for statistics as it is for anything else, and when trying to analyse data we’re often stuck with all sorts of problems in which the data are just messier than they’re supposed to be. Variables that are supposed to be normally distributed are not actually normally distributed, relationships that are supposed to be linear are not actually linear, and some of the observations in your data set are almost certainly junk (i.e., not measuring what they’re supposed to). All of this messiness is ignored in most of the statistical theory I developed in this book. However, ignoring a problem doesn’t always solve it. Sometimes, it’s actually okay to ignore the mess, because some types of statistical tools are “robust”, i.e., if the data don’t satisfy your theoretical assumptions they nevertheless still work pretty well. Other types of statistical tools are not robust, and even minor deviations from the theoretical assumptions cause them to break. Robust statistics is a branch of stats concerned with this question, and they talk about things like the “breakdown point” of a statistic. That is, how messy does your data have to be before the statistic cannot be trusted? I touched on this in places. The mean is not a robust estimator of the central tendency of a variable, but the median is. For instance, suppose I told you that the ages of my five best friends are 34, 39, 31, 43 and 4003 years. How old do you think they are on average? That is, what is the true population mean here? If you use the sample mean as your estimator of the population mean, you get an answer of 830 years. If you use the sample median as the estimator of the population mean, you get an answer of 39 years. Notice that, even though you’re “technically” doing the wrong thing in the second case (using the median to estimate the mean!) you’re actually getting a better answer. The problem here is that one of the observations is clearly, obviously, a lie. I don’t have a friend aged 4003 years. It’s probably a typo, I probably meant to type 43. But what if I had typed 53 instead of 43, or 34 instead of 43? Could you be sure if this was a typo or not? Sometimes the errors in the data are subtle, so you can’t detect them just by eyeballing the sample, but they’re still errors that contaminate your data, and they still affect your conclusions. Robust statistics is concerned with how you can make safe inferences even when faced with contamination that you don’t know about. It’s pretty cool stuff.\n\n\n\nMiscellaneous topics\n\nSuppose you’re doing a survey, and you’re interested in exercise and weight. You send data to four people. Adam says he exercises a lot and is not overweight. Briony says she exercises a lot and is not overweight. Carol says she does not exercise and is overweight. Tim says he does not exercise and refuses to answer the question about his weight. Elaine does not return the survey. You now have a missing data problem. There is one entire survey missing, and one question missing from another one, What do you do about it? Ignoring missing data is not, in general, a safe thing to do. Let’s think about Tim’s survey here. Firstly, notice that, on the basis of his other responses, he appear to be more similar to Carol (neither of us exercise) than to Adam or Briony. So if you were forced to guess his weight, you’d guess that he is closer to her than to them. Maybe you’d make some correction for the fact that Adam and Tim are males and Briony and Carol are females. The statistical name for this kind of guessing is “imputation”. Doing imputation safely is hard, but it’s important, especially when the missing data are missing in a systematic way. Because of the fact that people who are overweight are often pressured to feel poorly about their weight (often thanks to public health campaigns), we actually have reason to suspect that the people who are not responding are more likely to be overweight than the people who do respond. Imputing a weight to Tim means that the number of overweight people in the sample will probably rise from 1 out of 3 (if we ignore Tim), to 2 out of 4 (if we impute Tim’s weight). Clearly this matters. But doing it sensibly is more complicated than it sounds. Earlier, I suggested you should treat Tim like Carol, since they gave the same answer to the exercise question. But that’s not quite right. There is a systematic difference between them. She answered the question, and Tim didn’t. Given the social pressures faced by overweight people, isn’t it likely that Tim is more overweight than Carol? And of course this is still ignoring the fact that it’s not sensible to impute a single weight to Tim, as if you actually knew his weight. Instead, what you need to do it is impute a range of plausible guesses (referred to as multiple imputation), in order to capture the fact that you’re more uncertain about Tim’s weight than you are about Carol’s. And let’s not get started on the problem posed by the fact that Elaine didn’t send in the survey. As you can probably guess, dealing with missing data is an increasingly important topic. In fact, I’ve been told that a lot of journals in some fields will not accept studies that have missing data unless some kind of sensible multiple imputation scheme is followed.\nPower analysis. In 單元 9 I discussed the concept of power (i.e., how likely are you to be able to detect an effect if it actually exists) and referred to power analysis, a collection of tools that are useful for assessing how much power your study has. Power analysis can be useful for planning a study (e.g., figuring out how large a sample you’re likely to need), but it also serves a useful role in analysing data that you already collected. For instance, suppose you get a significant result, and you have an estimate of your effect size. You can use this information to estimate how much power your study actually had. This is kind of useful, especially if your effect size is not large. For instance, suppose you reject the null hypothesis at \\(p&lt; .05\\), but you use power analysis to figure out that your estimated power was only .08. The significant result means that, if the null hypothesis was in fact true, there was a 5% chance of getting data like this. But the low power means that, even if the null hypothesis is false and the effect size was really as small as it looks, there was only an 8% chance of getting data like you did. This suggests that you need to be pretty cautious, because luck seems to have played a big part in your results, one way or the other!\nData analysis using theory-inspired models. In a few places in this book I’ve mentioned response time (RT) data, where you record how long it takes someone to do something (e.g., make a simple decision). I’ve mentioned that RT data are almost invariably non-normal, and positively skewed. Additionally, there’s a thing known as the speed / accuracy trade-off: if you try to make decisions too quickly (low RT) then you’re likely to make poorer decisions (lower accuracy). So if you measure both the accuracy of a participant’s decisions and their RT, you’ll probably find that speed and accuracy are related. There’s more to the story than this, of course, because some people make better decisions than others regardless of how fast they’re going. Moreover, speed depends on both cognitive processes (i.e., time spent thinking) but also physiological ones (e.g., how fast can you move your muscles). It’s starting to sound like analysing this data will be a complicated process. And indeed it is, but one of the things that you find when you dig into the psychological literature is that there already exist mathematical models (called “sequential sampling models”) that describe how people make simple decisions, and these models take into account a lot of the factors I mentioned above. You won’t find any of these theoretically-inspired models in a standard statistics textbook. Standard stats textbooks describe standard tools, tools that could meaningfully be applied in lots of different disciplines, not just psychology. ANOVA is an example of a standard tool that is just as applicable to psychology as to pharmacology. Sequential sampling models are not, they are psychology-specific, more or less. This doesn’t make them less powerful tools. In fact, if you’re analysing data where people have to make choices quickly you should really be using sequential sampling models to analyse the data. Using ANOVA or regression or whatever won’t work as well, because the theoretical assumptions that underpin them are not well-matched to your data. In contrast, sequential sampling models were explicitly designed to analyse this specific type of data, and their theoretical assumptions are extremely well-matched to the data."
  },
  {
    "objectID": "Epilogue.html#learning-the-basics-and-learning-them-in-jamovi",
    "href": "Epilogue.html#learning-the-basics-and-learning-them-in-jamovi",
    "title": "後記",
    "section": "Learning the basics, and learning them in jamovi",
    "text": "Learning the basics, and learning them in jamovi\nOkay, that was a long list. And even that listing is massively incomplete. There really are a lot of big ideas in statistics that I haven’t covered in this book. It can seem pretty depressing to finish an almost 500-page textbook only to be told that this is only the beginning, especially when you start to suspect that half of the stuff you’ve been taught is wrong. For instance, there are a lot of people in the field who would strongly argue against the use of the classical ANOVA model, yet I’ve devoted two whole chapters to it! Standard ANOVA can be attacked from a Bayesian perspective, or from a robust statistics perspective, or even from a “it’s just plain wrong” perspective (people very frequently use ANOVA when they should actually be using mixed models). So why learn it at all?\nAs I see it, there are two key arguments. Firstly, there’s the pure pragmatism argument. Rightly or wrongly, ANOVA is widely used. If you want to understand the scientific literature, you need to understand ANOVA. And secondly, there’s the “incremental knowledge” argument. In the same way that it was handy to have seen one-way ANOVA before trying to learn factorial ANOVA, understanding ANOVA is helpful for understanding more advanced tools, because a lot of those tools extend on or modify the basic ANOVA setup in some way. For instance, although mixed models are way more useful than ANOVA and regression, I’ve never heard of anyone learning how mixed models work without first having worked through ANOVA and regression. You have to learn to crawl before you can climb a mountain.\nActually, I want to push this point a bit further. One thing that I’ve done a lot of in this book is talk about fundamentals. I spent a lot of time on probability theory. I talked about the theory of estimation and hypothesis tests in more detail than I needed to. Why did I do all this? Looking back, you might ask whether I really needed to spend all that time talking about what a probability distribution is, or why there was even a section on probability density. If the goal of the book was to teach you how to run a t-test or an ANOVA, was all that really necessary? Was this all just a huge waste of everyone’s time???\nThe answer, I hope you’ll agree, is no. The goal of an introductory stats is not to teach ANOVA. It’s not to teach t-tests, or regressions, or histograms, or p-values. The goal is to start you on the path towards becoming a skilled data analyst. And in order for you to become a skilled data analyst, you need to be able to do more than ANOVA, more than t-tests, regressions and histograms. You need to be able to think properly about data. You need to be able to learn the more advanced statistical models that I talked about in the last section, and to understand the theory upon which they are based. And you need to have access to software that will let you use those advanced tools. And this is where, in my opinion at least, all that extra time I’ve spent on the fundamentals pays off. If you understand probability theory, you’ll find it much easier to switch from frequentist analyses to Bayesian ones.\nIn short, I think that the big payoff for learning statistics this way is extensibility. For a book that only covers the very basics of data analysis, this book has a massive overhead in terms of learning probability theory and so on. There’s a whole lot of other things that it pushes you to learn besides the specific analyses that the book covers. So if your goal had been to learn how to run an ANOVA in the minimum possible time, well, this book wasn’t a good choice. But as I say, I don’t think that is your goal. I think you want to learn how to do data analysis. And if that really is your goal, you want to make sure that the skills you learn in your introductory stats class are naturally and cleanly extensible to the more complicated models that you need in real world data analysis. You want to make sure that you learn to use the same tools that real data analysts use, so that you can learn to do what they do. And so yeah, okay, you’re a beginner right now (or you were when you started this book), but that doesn’t mean you should be given a dumbed-down story, a story in which I don’t tell you about probability density, or a story where I don’t tell you about the nightmare that is factorial ANOVA with unbalanced designs. And it doesn’t mean that you should be given baby toys instead of proper data analysis tools. Beginners aren’t dumb, they just lack knowledge. What you need is not to have the complexities of real world data analysis hidden from from you. What you need are the skills and tools that will let you handle those complexities when they inevitably ambush you in the real world.\nAnd what I hope is that this book, or the finished book that this will one day turn into, is able to help you with that.\nAuthor’s note – I’ve mentioned it before, but I’ll quickly mention it again. The book’s reference list is appallingly incomplete. Please don’t assume that these are the only sources I’ve relied upon. The final version of this book will have a lot more references. And if you see anything clever sounding in this book that doesn’t seem to have a reference, I can absolutely promise you that the idea was someone else’s. This is an introductory textbook: none of the ideas are original. I’ll take responsibility for all the errors, but I can’t take credit for any of the good stuff. Everything smart in this book came from someone else, and they all deserve proper attribution for their excellent work. I just haven’t had the chance to give it to them yet."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Introduction-to-psychological-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Introduction-to-psychological-measurement",
    "title": "2  研究設計入門",
    "section": "2.1 認識心理測量",
    "text": "2.1 認識心理測量\n\n導讀簡報\n\n首先請同學理解資料分析就是一種測量。在心理學系要學的就是如何測量人類行為及心智活動。那麼到底”測量”是什麼？\n\n\n2.1.1 心理測量面面觀\n測量並不是一個實在的概念，而是泛指賦予可觀察的屬性數值、標記、或者某種清晰定義之類的表述。以下都是心理測量的實際例子：\n\n我的年齡是33歲。\n我不喜歡 小魚乾。\n我的染色體性別是男性。\n我的自我認同性別是女性。\n\n以上例子裡的粗體字是“可被測量的事物或 屬性”，斜體字就是“測量值”2。我們可以再對每個例子做些深入探討，了解每種測量的細節：\n\n我的年齡(以年計)可以是0，1，2，3…。年齡的上限是多少沒人知道，不過你可以假定我能活到150歲，雖然還沒有人類活到這麼老。\n若你問我喜歡小魚乾嗎？我可以回答我喜歡，我不喜歡，我沒有喜不喜歡，我有時候喜歡。\n我的染色體性別在多數情況下是男性(\\(XY\\))或女性(\\(XX\\))，不過凡事總有例外。我可能有克林那費爾特症候群(\\(XXY\\))，使得我有男性和女性性徵。當然還有其他性染色體變異的可能。\n我的自我認同性別也許傾向男性或女性，而這和我的染色體性別無關。我也可以認為自已不是二元性別，也可以宣稱是跨性別。\n\n如你所見，有的例子像是能間單測量數值的項目，例如年齡，但是有些研究主題的特殊考量會讓測量變得複雜。因為年齡是大家都能理解的例子，這裡先用年齡做個說明。讀到這裡的同學可能會想，用“年度”做為測量年齡的數值不就好了嗎？若是你加入發展心理學的研究，“年度”可能不是精確的測量單位，可能還有加上“月份”，例如找到兒童參與者2歲11個月大，可縮寫為”2:11”。如果研究對象是新生兒，可能要紀錄出生到現在的“天數”，甚至要用“小時”紀錄。總而言之，確認在研究主題之下，什麼是有意義的測量數值是重要的第一課。\n更仔細地探討下去，我們會發覺“年齡”並不是全然精確的概念。在一般情況，我們講“年齡”有隱含“從出生到現在的時間長度”的意思。但是並不是每次提到“年齡”就是這樣的意思。設想在一個研究新生兒控制眼球運動的實驗室，受測幼兒的”出生時間”不一定是實驗人員記錄“年齡”的有意義參照點。如果今天找到兩位“出生兩小時”的新生兒，一位是比預產期提前三週出生的早產兒，另一位比預產期晚一週出生，那麼研究人員能紀錄這兩位新生兒“年齡相同”嗎？從日常社會互動的情境來說，每個人都是以出生時間做為計算年齡的參照點，因為任何人都能接受每個人出生後都是獨立生活的個體。然而在科學研究的情境要考慮的就不只如此。將人類當成一種生物的話，最能獲得充分分析的年齡紀錄，應該從個體受孕那一刻起，紀錄成長到成熟的狀況。所以“年齡”的測量定義有兩種：從受孕時間起算的年齡，以及從出生時間起算的年齡。研究對象是成人的話或許不必計較兩種定義的區別，若研究對象是新生兒也許要考量那種定義才符合研究目的。\n由測量定義問題再討論下去，就是方法學的課題了。有關人類年齡的”測量方法“，同學們能想到或找到幾種有用的調查方法呢？以下是一些你可以參考的做法：\n\n直接問受測對象”你現在幾歲？“ 這種自陳式報告是快速、便宜又簡單的調查方法。不過必須確認受測對象已經成長到有能力回答調查問題，並且不會給不實回答。\n詢問可靠的消息來源，像是受測幼童的父母親“你們的孩子幾歲？”這種調查方法實施快速，特別是受測者的父母親就在旁邊的情境。不過如果要調查的是”受孕年齡“，大多數父母親都不清楚是什麼時候開始懷孕的。這時也許要詢問更可靠的來源，像是他們的婦產科主治醫師。\n尋找官方紀錄，像是個人的出生與死亡證明。這種方式耗時又勞力，不過如果研究對象是已往生的人是種可靠的來源。\n\n\n\n\n2.1.2 操作型定義: 定義研究的測量方法\n以上討論這麼多，是要引導同學理解操作型定義的概念。用更清楚簡潔的話來說，操作型定義是將有意義但有些模糊的概念，轉換為精確測量程序的設定過程。完整的操作型定義要在過程中考慮這些條件：\n\n清楚定義研究要測量的項目。以“年齡”為例，研究要測量的是“胎兒出生起算的時間長度”還是“母體受孕起算的時間長度”。\n確認用什麼方法進行測量。同樣以“年齡”為例，研究要用自陳式報告、詢問受測對象雙親、還是查閱官方紀錄？如果決定用自陳式報告，要如何設定問題內容？\n定義可做紀錄的測量數值範圍。請留意數值不一定要是數字！雖然年齡的測量數值無疑問題數字，依然要考慮什麼單位的數字是需要被紀錄的。是年份？月份？還是小時？像性別一類數值不是數字的測量方法，同樣也要考慮數值的範圍。像是請受測對象透過自陳報告回報性別，要設計什麼樣的選項讓人選擇？只有“男性”和“女性”兩個選項夠嗎？需不需要增加“其他”這一項？或者改用開放式回答，讓受測對象自已寫下個人認同的性別？如果確定使用開放式回答，要如何解讀各式各樣的答案？\n\n設定操作型定義有各種方法，沒有一種“唯一正解”。任何研究都是根據測量目的，透過設定操作型定義的過程，將“年齡”與”性別”等不具形式的概念，轉化為可用數值形式表達的測量值。每個科學領域都對研究對象的測量方式有基本共識。因此，要理解如何設定操作型定義，是因研究主題而異。畢竟有許多主題存在大量差異化的個別研究，有些主題則有一致的研究模式，無法用統一的方法設定操作型定義。\n進入下一節之前，讓我們整理一些之後的單元會經常遇到，與測量有關的專有名詞，這些名詞彼此之間有許多關聯：\n\n理論建構(A theoretical construct) 研究者想要測量的目標，像是”年齡”、“性別”、或”選項”。理論建構不能直接觀察記錄，都是模糊抽象的概念。\n測量程序(measure)3 測量程序是進行觀察紀錄的方法或工具。像是問卷的題目、行為觀察、腦部活動掃瞄都是一種測量程序。\n操作型定義(operationalisation) 測量程序與理論建構之間的邏輯關連條件，或是將理論建構轉換為測量程序的過程。\n變項(variable) 執行測量程序的最終成品就是“資料”，一個變項就是”資料”的集合4。\n\n在統計實務，即使是訓練過的科學家也不大會去管這些名詞的差異，但是對於正在學習的同學們，搞清楚這些名詞的涵義會很有幫助。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Scales-of-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Scales-of-measurement",
    "title": "2  研究設計入門",
    "section": "2.2 測量尺度",
    "text": "2.2 測量尺度\n前一節最後提到心理測量程序的產物就是變項。一份資料檔通常不只有一個變項，而且各種變項有本質的區別，所以這一節要好好認識幾種變項型態。同學們將認識各種測量尺度以及學會判斷資料的變項型態。\n\n\n2.2.1 名義尺度\n名義尺度變項又稱類別變項，命名理由是代表資料的每個數值之間沒有任何明確關係。名義尺度變項的資料之間，沒有那一個數值比較”大”或比較”好”，因此也無法計算這筆變項的平均值。最典型的例子是”瞳孔顏色”。人類瞳孔有藍色、綠色、棕色等各種顏色。沒有人能說瞳孔有所謂的”平均顏色”。人類性別也是典型的名義尺度變項：男性或女性沒有那一種比較好或比較不好，當然也沒有所謂的”平均性別”。總而言之，以名義尺度紀錄的資料，不同數值之間只有各自不一樣的意思而已。\n讓我們再用一個例子更深入了解名義尺度。假如今天要研究民眾如何通勤上班，我們可以設定資料變項記錄每個人是用什麼交通方式去上班。“通勤方式”這個變項可能有好幾種數值：像是“火車”，“公車”，“汽車”，“自行車”。假設調查100位民眾，得到了這四種回答，完成的紀錄就如表2-1。\n\n\n\n\n\n表2-1: 調查100位民眾本日通勤方式\n\n\n\n\n通勤方式\n人數\n\n\n\n\n(1)火車\n12\n\n\n(2)公車\n30\n\n\n(3)汽車\n48\n\n\n(4)自行車\n10\n\n\n\n那麼這裡可以找出平均的通勤方式嗎？很明顯答案不只一個，或者說這是個蠢問題。也許你會說汽車是最多人上班的交通工具，也可以說火車最沒有民眾使用，但都不能涵括調查結果。同學們也可以看一看表2-2，無論如何變動記錄表裡的項目順序，都無法讓這筆資料透露任何意義。\n\n\n\n\n\n表2-2: 調查100位民眾本日通勤方式，依上段描述更改呈現順序\n\n\n\n\n通勤方式\n人數\n\n\n\n\n(3)汽車\n48\n\n\n(1)火車\n12\n\n\n(4)自行車\n10\n\n\n(2)公車\n30\n\n\n\n\n…很明顯沒有什麼改變。\n\n\n2.2.2 次序尺度\n次序尺度變項比起名義尺度變項能呈現一點有結構的資訊，不過沒有很多。次序尺度變項使用自然有意義的方式排序資料數值，但是除了排序我們就無法再做什麼了。最典型的例子是“賽跑選手抵達終點的名次”。先抵達終點的選手毫無疑問比第二名選手快，不過紀錄看不出快多少。紀錄一場比賽的選手名次後，我們會知道第一名比第二名快，第二名比第三名快，但是從資料看不出第一名和第二名的差距，與第二名和第三名的差距有什麼差異。\n以下是個比較有心理學意義的例子。假如我想調查民眾對於氣候變遷的態度，我設計了以下幾個項目，請受訪者選出其中一個最接近個人看法的項目。\n\n氣溫確實有上升，而且是人類活動造成的\n氣溫確實有上升，但是原因不明\n氣溫確實有上升，但是與人類活動無關\n氣溫沒有變化\n\n以上四個項目的順序是根據當前已知的科學研究資訊排列，第1項看法最符合現在的研究所知，第2與第3項是尚有研究證據支持的看法，第4項看法與所有研究證據相左。若是受測者們對於已知科學證據有充分了解，以上四個項目的排序就是符合多數人的看法。如果我將選項用以下段落的方式排序，則是不符合多數人的看法。\n\n氣溫確實有上升，但是與人類活動無關\n氣溫確實有上升，而且是人類活動造成的\n氣溫沒有變化\n氣溫確實有上升，但是原因不明\n\n…這個例子說明心理測量的“結構”是資料呈現出多數人的合理回應。\n假設我成功收集了100位民眾的回應，調查結果總計如表2-3。\n\n\n\n\n\n表2-3: 氣候變遷態度調查結果\n\n\n\n\n調查項目回應\n人數\n\n\n\n\n(1)氣溫確實有上升，而且是人類活動造成的\n51\n\n\n(2)氣溫確實有上升，但是原因不明\n20\n\n\n(3)氣溫確實有上升，但是與人類活動無關\n10\n\n\n(4)氣溫沒有變化\n19\n\n\n\n資料分析能獲得幾種合理解釋，其中一種是最多民眾回應代表理性意見的(1)，(2)，(3)，或者說100位受訪中的81人至少有理解最新科學研究。另一種合理解釋是至少一半受訪者傾向不同意氣候變遷是現實的看法，因為100位受訪者中的49人選擇不同於主流科學觀點的(2)，(3)， (4)。然而也可能很難合理解釋為何有九成受訪者選擇(1)，(2)，(4)，因為這三項的排序不符合預期的順序結構，無法說明最多人選擇的三項有什麼意義。\n以上的說明是向同學們示範，次序尺度變項資料要符合預期中的”順序結構“，才會有合理的分析，而且我們不能計算數值之間的平均。如果我們用回應人數計算加權平均，雖然可以得到平均值1.97，但是這個數值對於解釋調查結果沒有任何幫助。請同學想想這個數字能不能做為報告的結論。\n\n\n\n2.2.3 等距尺度\n不同於名義尺度與次序尺度變項，等距尺度和比例尺度變項以可數的數字表數值，能獲得有意義的資訊。等距尺度變項的數值之間差異是可以推論的，但是變項數值沒有“自然的”零點。攝氏溫度是一個說明等距尺度變項的好例子。例如是，昨天氣溫是 15\\(^{\\circ}\\)，今天氣溫是 18\\(^{\\circ}\\)，所以兩日的溫差是3\\(^{\\circ}\\)。更重要的是，這個3\\(^{\\circ}\\)與氣溫7\\(^{\\circ}\\) 與 10\\(^{\\circ}\\)的差異是完全相等。簡言之，用等距尺度變項數值的做加法或減法是有意義的5。\n然而，攝氏零度並不是“量不到溫度”的意思，最早攝氏零度是根據“觀察到水開始結冰”而指定的數值。這造成溫度的數值無法相乘或相除：要說20\\(^{\\circ}\\) 比10\\(^{\\circ}\\) 熱兩倍是荒誔的，也不可能說20\\(^{\\circ}\\) 是-10\\(^{\\circ}\\) 的負兩倍。\n我們再來舉心理學的例子吧。假如我想要了解各位同學在大學四年間學習態度的變化，最好的方式是每學年或學期開始，就請同手們做一次態度調查。這樣的記錄就是等距尺度變項資料。如果我手上的紀錄有一位學生是2013年的，另一位學生的紀錄是2018年的，向你們報告說2018年學生接受調查的時間是2013年的”1.0025倍“，這樣說奇不奇怪呢？\n\n\n\n\n2.2.4 比例尺度\n最後一種變項型態是有零點的比例尺度，也就是說變項數值是可以相乘且相除的。有個不錯的心理學研究常用的比例尺度變項是反應時間(RT)。許多心理學作業都會紀錄參與者花了多少時間解決問題或給出回答，因為時間長度是作業難度的指標。假如今天有項作業，參與者A花了2.3秒回答，另一位參與者B花了3.1秒。就像等距尺度變項，比例尺度數值的相加與相減是有意義的，也就是我們可以說受試者B比起受試者A多花了3.1 - 2.3 = 0.8秒。同學也要留意反應時間數值的乘法與除法是有意義的：我們可以說受試者B比受試者A花了 3.1/2.3 = 1.35倍的時間完成回答。能做完整四則運算的原因是，反應時間有真正的”零點”~零秒就是沒有反應記錄。\n\n\n\n2.2.5 連續與間斷變項\n還有另一套變項型態是同學們需要知道的：連續變項與間斷變項。不論你要處理的測量尺度是什麼，也會具備切換為連續變項與間斷變項的條件 (見表2-4)。我將連續與間斷變項的差異整理一下：\n\n連續變項(continuous variable)的任何兩個數值之間，都能存在另一個數值，因此是連續的。\n間斷變項(discrete variable)的數值當然不是連續的。任何兩個相鄰的間斷變項數值，不可能存在其他數值。\n\n\n\n\n\n\n表2-4: 測量尺度與間斷/連續變項的關聯性。細格內的符號是jamovi的變項標示符號。6\n\n\n\n\n\n\n\n\n\n\n連續變項\n間斷變項\n\n\n\n\n名義尺度\n\n\n\n\n次序尺度\n\n\n\n\n等距尺度\n\n\n\n\n比例尺度\n\n\n\n\n\n雖然表2-5看起來有點抽象，透過一些例子就能理解如何切換。同樣用解釋比例尺度的反應時間為例，現在除了有參與者A用了2.3秒，與參與者B用了3.1秒的資料，還有一位參與者C用了3.0秒鐘，剛好記錄在前兩位之間。當然，若再有一位參與者D的記錄是3.031秒，就是在B與C之間有一筆資料。雖然真正的實驗不一定會測量得如此準確，這只是示範連續變項的主要特性是在已經存在的任何兩筆資料數值之間，都能增加一個新的資料數值。\n只要變項無法在任何兩個資料數值之間增加資料，就只能是間斷變項。像是名義尺度變項永遠都只能是間斷變項。如同火車走的鐵路與自行車道之間不可能有“切換機制”，名義數值2與3之間不可能增加一個數值2.3，所以名義尺度變項資料只能當成間斷變項資料處理。類別尺度變項也是只能切換成間斷變項，雖然”第2名”確實是在”第1名”與”第3名”之間，“第1名”與”第2名”之間沒有空間給其他數值。至於等距尺度與比例尺度變項，可以切換為間斷變項，也可以切換為連續變項。前面提到的反應時間(比例尺度)與攝氏溫度(等距尺度)都是可以切換為連續變項。不過如果是各位同學的入學年份，雖然是等距尺度變項，卻只能切換為間斷變項，像是2022年與2023年之間不能放入其他年份。還有假如今天同學們做了一份都是是非題的測驗，雖然每一題分數是等距尺度變項資料，也是只能切換為間斷變項，因為沒有5/10正確或6/10錯誤之類的回答。表2-4總結四種測量尺度變項在jamovi介面標記符號，以及能否切換為間斷變項或連續變項7。這一節特別解釋各種測量尺度與間斷/連續的切換關係，出於原作者的兩個理由：首先是有些統計教科書混淆了測量尺度與間斷/連續變項的定義，其次是經常聽到很多人，包括資深研究人員與統計學教師，提到「間斷變項」都會直覺認定是「名義尺度變項」，理解清楚的話就會知道這樣認定會誤讀資料8。\n\n\n\n2.2.6 複雜的現實\n好啦，測量尺度與資料變項之間的切換規律或許會讓一些讀者感到震驚，不過真實世界遠比這條小規律複雜許多。其實現實生活中，只有非常少的可測量指標充分符合這條規律，所以同學們必須留意，不要把切換規律硬套在任何你遇到的測量尺度。切換規律只是指引而已，提示統計使用者在實務中如何找出處置資料的最佳方式而已9。\n讓我們用一個心理學家滿常用的心理測量工具～李克特量表(Likert scale)來說明現實世界有多複雜。許多調查問卷都會使用李克特量表收集受測者的回應，讀者與同學們也許已經填過好多份使用李克特量表的問卷，搞不好曾經在某份自行設計的問卷使用過李克特量表。以下是一條假想的問卷問題：\n請從以下五個選項，挑出最符合您對於「所有韓星都很潮」10這句描述的看法？\n\n完全不同意\n部分不同意\n即非同意也非不同意\n部分同意\n完全同意\n\n這是典型的五點式李克特量表，依數值大小順序排列同意程度，讓受測者選一個數字，通常每個數字旁邊都有文字說明。不過也不必所有數字旁邊都要放文字說明，所以問卷選項也能寫成這個樣子：\n\n完全不同意\n\n\n\n完全同意\n\n李克特量表是非常好用，但是用途有限的工具。怎麼說？請同學想想用這個問題收集到的回應是什麼樣的資料變項？很明顯應該是間斷變項，因為沒有人能給2.5這種答案。這筆資料也顯然不是名義尺度，因為選項是有順序的；也顯然不是比例尺度，因為沒有自然零點。\n那麼這筆資料是次序尺度還是等距尺度？有一種說法是我們無法確定”部分同意”和”完全同意”之間的數值差異，與”部分同意”和”即非同意也非不同意”之間的數值差異是相等的。其實拿日常生活的任何事物做成調查問題，不必有數學知識，任何人都能同意李克特量表的任何一對相鄰數字之間差異是不相等的。所以說我們不應該將用李克特量表收集的資料當成次序尺度變項。另一種說法是假設受測者填答時，會以為選項數字1到5是均等平分的量尺，心理預設五個選項之間的差異如同標示在選項前的數字一樣。經常使用到今天，多數研究者都將李克特量表測得的資料當成等距尺度11。但是嚴格來說又不能算是等距尺度，所以在實務上心理學研究者常將李克特量表當成準等距尺度。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#sec-Assessing-the-reliability-of-a-measurement",
    "href": "02-A-brief-introduction-to-research-design.html#sec-Assessing-the-reliability-of-a-measurement",
    "title": "2  研究設計入門",
    "section": "2.3 測量的信度",
    "text": "2.3 測量的信度\n\n導讀簡報\n\n\n至此我們已經探討如何操作理論建構，以此創造一種心理測量。使用心理測量收集的資料，就會構成變項，而且測量的尺度決定變項的類型。所以我們可以進入下一個課題：這個心理測量夠不夠好？以下從兩個彼此有關的概念討論這個課題：信度和效度。簡而言之，一種測量的信度表示這種測量的準確度，而測量的效度告訴你這種測量的準確性。這一節的主題是信度；效度是研究的效度的主題。\n其實信度是相當容易理解的概念，所指的就是心理測量的可重複性或一致性。像是本人的體重用“體重計”測量通常非常可靠。如果我在一分鐘內不斷地站上去秤重，每次都會給我同樣的讀數。但是本人的智力程度由“問媽媽”來測量通常非常不可靠。有幾天她會說我有點遲鈍，又有幾天她又會說我完完全全是個白癡。請讀者留意，這裡所談的信度與測量結果是否正確，兩者是不同的的問題(測量結果的正確性關乎效度)。如果我扛著一袋馬鈴薯站上體重計，這樣的測量結果仍然是可靠的，重複測量還能給出一致的讀數。然而，如此高度可靠的讀數根本不符合本人的真實體重，因此這樣的測量結果是錯誤的。以心理測量的專業術語來說，這是一種可靠但無效的測量。同樣地，我媽媽給我的智力程度估計雖然有點不可靠，但是她可能是對的，也許我真的不太聰明。所以雖然她每天給我的智力估計起伏不定，綜合起來可能是正確的，這就是一種不可靠但有效的測量。當然，如果我媽媽的估計太不可靠，要弄清楚她給我的智力各種評價，哪一個是真正正確的是非常難的任務。以實用意義來說，一個非常不可靠的測量往往無法有效達到研究目標。所以許多測驗專家會說，有最起碼的信度才是確保效度的必要但非充分條件。\n好了，搞清楚信度和效度區別之後，接下來看看各種專家一致認可，用來測量信度的方法:\n\n測試-重新測試法。關乎不同時間測量結果的一致性。如果再次用相同方法測量，我們會得到相同的結果嗎？\n評分者間信度。關乎不同評分者給出的測量結果一致性。如果由其他人使用相同的方法再次測量(例如由其他人評估我的智力)，其他會給出相同的結果嗎?\n複本信度。關乎以理論的等效性所設定的測量一致性。如果我拿另一具體重計來測量我的體重，我會得到相同的讀數嗎?\n內部一致性信度。如果一套測量是由許多測量功能類似的量尺組成(例如，由圍繞相同主題的幾個問題加總，所得到的人格問卷結果)，每個部分的測量問題能測得相近的數值。我們將在 小單元 15.5 詳細地學習這種評估信度的方法。\n\n並非所有測量工具都要用到以上所有評估方法，才能確認信度。例如，學習評量可以被視為一種測量形式。原作者教授的一門課程「計算認知科學」的評量項目包含一項研究專案和一次考試(包括其他)。考試要評量的知識與研究專案所評量的學習成果並不相同，因此整體評量的內部一致性偏低。然而，考試卷裡有幾道題目，設計的目的都是測量大約相同的知識，這些問題往往會導致相似的評量結果。因此就考試這個部分的評量結果，內部一致性較高，也是很自然的。也就是說，在想要測量相同事物的情況下，我們才要有起碼的信度!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#the-role-of-variables-predictors-and-outcomes",
    "href": "02-A-brief-introduction-to-research-design.html#the-role-of-variables-predictors-and-outcomes",
    "title": "2  A brief introduction to research design",
    "section": "2.4 The “role” of variables: predictors and outcomes",
    "text": "2.4 The “role” of variables: predictors and outcomes\nI’ve got one last piece of terminology that I need to explain to you before moving away from variables. Normally, when we do some research we end up with lots of different variables. Then, when we analyse our data, we usually try to explain some of the variables in terms of some of the other variables. It’s important to keep the two roles “thing doing the explaining” and “thing being explained” distinct. So let’s be clear about this now. First, we might as well get used to the idea of using mathematical symbols to describe variables, since it’s going to happen over and over again. Let’s denote the “to be explained” variable \\(Y\\), and denote the variables “doing the explaining” as \\(X_1 , X_2\\), etc.\nWhen we are doing an analysis we have different names for \\(X\\) and \\(Y\\), since they play different roles in the analysis. The classical names for these roles are independent variable (IV) and dependent variable (DV). The IV is the variable that you use to do the explaining (i.e., \\(X\\)) and the DV is the variable being explained (i.e.,$Y $). The logic behind these names goes like this: if there really is a relationship between \\(X\\) and \\(Y\\) then we can say that \\(Y\\)depends on \\(X\\), and if we have designed our study “properly” then \\(X\\) isn’t dependent on anything else. However, I personally find those names horrible. They’re hard to remember and they’re highly misleading because (a) the IV is never actually “independent of everything else”, and (b) if there’s no relationship then the DV doesn’t actually depend on the IV. And in fact, because I’m not the only person who thinks that IV and DV are just awful names, there are a number of alternatives that I find more appealing. The terms that I’ll use in this book are predictors and outcomes. The idea here is that what you’re trying to do is use \\(X\\) (the predictors) to make guesses about \\(Y\\) (the outcomes).4 This is summarised in Table 2.5.\n\n\n\n\nTable 2.5:  Variable distinctions \n\nrole of the variableclassical namemodern name\n\n\"to be explained\"dependent variable (DV)outcome\n\n\"to do the explaining\"independent variable (IV)predictor"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#experimental-and-non-experimental-research",
    "href": "02-A-brief-introduction-to-research-design.html#experimental-and-non-experimental-research",
    "title": "2  A brief introduction to research design",
    "section": "2.5 Experimental and non-experimental research",
    "text": "2.5 Experimental and non-experimental research\nOne of the big distinctions that you should be aware of is the distinction between “experimental research” and “non-experimental research”. When we make this distinction, what we’re really talking about is the degree of control that the researcher exercises over the people and events in the study.\n\n2.5.1 Experimental research\nThe key feature of experimental research is that the researcher controls all aspects of the study, especially what participants experience during the study. In particular, the researcher manipulates or varies the predictor variables (IVs) but allows the outcome variable (DV) to vary naturally. The idea here is to deliberately vary the predictors (IVs) to see if they have any causal effects on the outcomes. Moreover, in order to ensure that there’s no possibility that something other than the predictor variables is causing the outcomes, everything else is kept constant or is in some other way “balanced”, to ensure that they have no effect on the results. In practice, it’s almost impossible to think of everything else that might have an influence on the outcome of an experiment, much less keep it constant. The standard solution to this is randomisation. That is, we randomly assign people to different groups, and then give each group a different treatment (i.e., assign them different values of the predictor variables). We’ll talk more about randomisation later, but for now it’s enough to say that what randomisation does is minimise (but not eliminate) the possibility that there are any systematic difference between groups.\nLet’s consider a very simple, completely unrealistic and grossly unethical example. Suppose you wanted to find out if smoking causes lung cancer. One way to do this would be to find people who smoke and people who don’t smoke and look to see if smokers have a higher rate of lung cancer. This is not a proper experiment, since the researcher doesn’t have a lot of control over who is and isn’t a smoker. And this really matters. For instance, it might be that people who choose to smoke cigarettes also tend to have poor diets, or maybe they tend to work in asbestos mines, or whatever. The point here is that the groups (smokers and non-smokers) actually differ on lots of things, not just smoking. So it might be that the higher incidence of lung cancer among smokers is caused by something else, and not by smoking per se. In technical terms these other things (e.g. diet) are called “confounders”, and we’ll talk about those in just a moment.\nIn the meantime, let’s consider what a proper experiment might look like. Recall that our concern was that smokers and non-smokers might differ in lots of ways. The solution, as long as you have no ethics, is to control who smokes and who doesn’t. Specifically, if we randomly divide young non-smokers into two groups and force half of them to become smokers, then it’s very unlikely that the groups will differ in any respect other than the fact that half of them smoke. That way, if our smoking group gets cancer at a higher rate than the non-smoking group, we can feel pretty confident that (a) smoking does cause cancer and (b) we’re murderers.\n\n\n2.5.2 Non-experimental research\nNon-experimental research is a broad term that covers “any study in which the researcher doesn’t have as much control as they do in an experiment”. Obviously, control is something that scientists like to have, but as the previous example illustrates there are lots of situations in which you can’t or shouldn’t try to obtain that control. Since it’s grossly unethical (and almost certainly criminal) to force people to smoke in order to find out if they get cancer, this is a good example of a situation in which you really shouldn’t try to obtain experimental control. But there are other reasons too. Even leaving aside the ethical issues, our “smoking experiment” does have a few other issues. For instance, when I suggested that we “force” half of the people to become smokers, I was talking about starting with a sample of non-smokers, and then forcing them to become smokers. While this sounds like the kind of solid, evil experimental design that a mad scientist would love, it might not be a very sound way of investigating the effect in the real world. For instance, suppose that smoking only causes lung cancer when people have poor diets, and suppose also that people who normally smoke do tend to have poor diets. However, since the “smokers” in our experiment aren’t “natural” smokers (i.e., we forced non-smokers to become smokers, but they didn’t take on all of the other normal, real life characteristics that smokers might tend to possess) they probably have better diets. As such, in this silly example they wouldn’t get lung cancer and our experiment will fail, because it violates the structure of the “natural” world (the technical name for this is an “artefactual” result).\nOne distinction worth making between two types of non-experimental research is the difference between quasi-experimental research and case studies. The example I discussed earlier, in which we wanted to examine incidence of lung cancer among smokers and non-smokers without trying to control who smokes and who doesn’t, is a quasi-experimental design. That is, it’s the same as an experiment but we don’t control the predictors (IVs). We can still use statistics to analyse the results, but we have to be a lot more careful and circumspect.\nThe alternative approach, case studies, aims to provide a very detailed description of one or a few instances. In general, you can’t use statistics to analyse the results of case studies and it’s usually very hard to draw any general conclusions about “people in general” from a few isolated examples. However, case studies are very useful in some situations. Firstly, there are situations where you don’t have any alternative. Neuropsychology has this issue a lot. Sometimes, you just can’t find a lot of people with brain damage in a specific brain area, so the only thing you can do is describe those cases that you do have in as much detail and with as much care as you can. However, there’s also some genuine advantages to case studies. Because you don’t have as many people to study you have the ability to invest lots of time and effort trying to understand the specific factors at play in each case. This is a very valuable thing to do. As a consequence, case studies can complement the more statistically-oriented approaches that you see in experimental and quasi-experimental designs. We won’t talk much about case studies in this book, but they are nevertheless very valuable tools!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#assessing-the-validity-of-a-study",
    "href": "02-A-brief-introduction-to-research-design.html#assessing-the-validity-of-a-study",
    "title": "2  A brief introduction to research design",
    "section": "2.6 Assessing the validity of a study",
    "text": "2.6 Assessing the validity of a study\nMore than any other thing, a scientist wants their research to be “valid”. The conceptual idea behind validity is very simple. Can you trust the results of your study? If not, the study is invalid. However, whilst it’s easy to state, in practice it’s much harder to check validity than it is to check reliability. And in all honesty, there’s no precise, clearly agreed upon notion of what validity actually is. In fact, there are lots of different kinds of validity, each of which raises it’s own issues. And not all forms of validity are relevant to all studies. I’m going to talk about five different types of validity:\n\nInternal validity\nExternal validity\nConstruct validity\nFace validity\nEcological validity\n\nFirst, a quick guide as to what matters here. (1) Internal and external validity are the most important, since they tie directly to the fundamental question of whether your study really works. (2) Construct validity asks whether you’re measuring what you think you are. (3) Face validity isn’t terribly important except insofar as you care about “appearances”. (4) Ecological validity is a special case of face validity that corresponds to a kind of appearance that you might care about a lot.\n\n2.6.1 Internal validity\nInternal validity refers to the extent to which you are able draw the correct conclusions about the causal relationships between variables. It’s called “internal” because it refers to the relationships between things “inside” the study. Let’s illustrate the concept with a simple example. Suppose you’re interested in finding out whether a university education makes you write better. To do so, you get a group of first year students, ask them to write a 1000 word essay, and count the number of spelling and grammatical errors they make. Then you find some third-year students, who obviously have had more of a university education than the first-years, and repeat the exercise. And let’s suppose it turns out that the third-year students produce fewer errors. And so you conclude that a university education improves writing skills. Right? Except that the big problem with this experiment is that the third-year students are older and they’ve had more experience with writing things. So it’s hard to know for sure what the causal relationship is. Do older people write better? Or people who have had more writing experience? Or people who have had more education? Which of the above is the true cause of the superior performance of the third-years? Age? Experience? Education? You can’t tell. This is an example of a failure of internal validity, because your study doesn’t properly tease apart the causal relationships between the different variables.\n\n\n2.6.2 External validity\nExternal validity relates to the generalisability or applicability of your findings. That is, to what extent do you expect to see the same pattern of results in “real life” as you saw in your study. To put it a bit more precisely, any study that you do in psychology will involve a fairly specific set of questions or tasks, will occur in a specific environment, and will involve participants that are drawn from a particular subgroup (disappointingly often it is college students!). So, if it turns out that the results don’t actually generalise or apply to people and situations beyond the ones that you studied, then what you’ve got is a lack of external validity.\nThe classic example of this issue is the fact that a very large proportion of studies in psychology will use undergraduate psychology students as the participants. Obviously, however, the researchers don’t care only about psychology students. They care about people in general. Given that, a study that uses only psychology students as participants always carries a risk of lacking external validity. That is, if there’s something “special” about psychology students that makes them different to the general population in some relevant respect, then we may start worrying about a lack of external validity.\nThat said, it is absolutely critical to realise that a study that uses only psychology students does not necessarily have a problem with external validity. I’ll talk about this again later, but it’s such a common mistake that I’m going to mention it here. The external validity of a study is threatened by the choice of population if (a) the population from which you sample your participants is very narrow (e.g., psychology students), and (b) the narrow population that you sampled from is systematically different from the general population in some respect that is relevant to the psychological phenomenon that you intend to study. The italicised part is the bit that lots of people forget. It is true that psychology undergraduates differ from the general population in lots of ways, and so a study that uses only psychology students may have problems with external validity. However, if those differences aren’t very relevant to the phenomenon that you’re studying, then there’s nothing to worry about. To make this a bit more concrete here are two extreme examples:\n\nYou want to measure “attitudes of the general public towards psychotherapy”, but all of your participants are psychology students. This study would almost certainly have a problem with external validity.\nYou want to measure the effectiveness of a visual illusion, and your participants are all psychology students. This study is unlikely to have a problem with external validity\n\nHaving just spent the last couple of paragraphs focusing on the choice of participants, since that’s a big issue that everyone tends to worry most about, it’s worth remembering that external validity is a broader concept. The following are also examples of things that might pose a threat to external validity, depending on what kind of study you’re doing:\n\nPeople might answer a “psychology questionnaire” in a manner that doesn’t reflect what they would do in real life.\nYour lab experiment on (say) “human learning” has a different structure to the learning problems people face in real life.\n\n\n\n2.6.3 Construct validity\nConstruct validity is basically a question of whether you’re measuring what you want to be measuring. A measurement has good construct validity if it is actually measuring the correct theoretical construct, and bad construct validity if it doesn’t. To give a very simple (if ridiculous) example, suppose I’m trying to investigate the rates with which university students cheat on their exams. And the way I attempt to measure it is by asking the cheating students to stand up in the lecture theatre so that I can count them. When I do this with a class of 300 students 0 people claim to be cheaters. So I therefore conclude that the proportion of cheaters in my class is 0%. Clearly this is a bit ridiculous. But the point here is not that this is a very deep methodological example, but rather to explain what construct validity is. The problem with my measure is that while I’m trying to measure “the proportion of people who cheat” what I’m actually measuring is “the proportion of people stupid enough to own up to cheating, or bloody minded enough to pretend that they do”. Obviously, these aren’t the same thing! So my study has gone wrong, because my measurement has very poor construct validity.\n\n\n2.6.4 Face validity\nFace validity simply refers to whether or not a measure “looks like” it’s doing what it’s supposed to, nothing more. If I design a test of intelligence, and people look at it and they say “no, that test doesn’t measure intelligence”, then the measure lacks face validity. It’s as simple as that. Obviously, face validity isn’t very important from a pure scientific perspective. After all, what we care about is whether or not the measure actually does what it’s supposed to do, not whether it looks like it does what it’s supposed to do. As a consequence, we generally don’t care very much about face validity. That said, the concept of face validity serves three useful pragmatic purposes:\n\nSometimes, an experienced scientist will have a “hunch” that a particular measure won’t work. While these sorts of hunches have no strict evidentiary value, it’s often worth paying attention to them. Because often times people have knowledge that they can’t quite verbalise, so there might be something to worry about even if you can’t quite say why. In other words, when someone you trust criticises the face validity of your study, it’s worth taking the time to think more carefully about your design to see if you can think of reasons why it might go awry. Mind you, if you don’t find any reason for concern, then you should probably not worry. After all, face validity really doesn’t matter very much.\nOften (very often), completely uninformed people will also have a “hunch” that your research is crap. And they’ll criticise it on the internet or something. On close inspection you may notice that these criticisms are actually focused entirely on how the study “looks”, but not on anything deeper. The concept of face validity is useful for gently explaining to people that they need to substantiate their arguments further.\nExpanding on the last point, if the beliefs of untrained people are critical (e.g., this is often the case for applied research where you actually want to convince policy makers of something or other) then you have to care about face validity. Simply because, whether you like it or not, a lot of people will use face validity as a proxy for real validity. If you want the government to change a law on scientific psychological grounds, then it won’t matter how good your studies “really” are. If they lack face validity you’ll find that politicians ignore you. Of course, it’s somewhat unfair that policy often depends more on appearance than fact, but that’s how things go.\n\n\n\n2.6.5 Ecological validity\nEcological validity is a different notion of validity, which is similar to external validity, but less important. The idea is that, in order to be ecologically valid, the entire set up of the study should closely approximate the real world scenario that is being investigated. In a sense, ecological validity is a kind of face validity. It relates mostly to whether the study “looks” right, but with a bit more rigour to it. To be ecologically valid the study has to look right in a fairly specific way. The idea behind it is the intuition that a study that is ecologically valid is more likely to be externally valid. It’s no guarantee, of course. But the nice thing about ecological validity is that it’s much easier to check whether a study is ecologically valid than it is to check whether a study is externally valid. A simple example would be eyewitness identification studies. Most of these studies tend to be done in a university setting, often with a fairly simple array of faces to look at, rather than a line up. The length of time between seeing the “criminal” and being asked to identify the suspect in the “line up” is usually shorter. The “crime” isn’t real so there’s no chance of the witness being scared, and there are no police officers present so there’s not as much chance of feeling pressured. These things all mean that the study definitely lacks ecological validity. They might (but might not) mean that it also lacks external validity."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#confounds-artefacts-and-other-threats-to-validity",
    "href": "02-A-brief-introduction-to-research-design.html#confounds-artefacts-and-other-threats-to-validity",
    "title": "2  簡易入門研究設計",
    "section": "2.7 Confounds, artefacts and other threats to validity",
    "text": "2.7 Confounds, artefacts and other threats to validity\nIf we look at the issue of validity in the most general fashion the two biggest worries that we have are confounders and artefacts. These two terms are defined in the following way:\n\nConfounder: A confounder is an additional, often unmeasured variable5 that turns out to be related to both the predictors and the outcome. The existence of confounders threatens the internal validity of the study because you can’t tell whether the predictor causes the outcome, or if the confounding variable causes it.\nArtefact: A result is said to be “artefactual” if it only holds in the special situation that you happened to test in your study. The possibility that your result is an artefact describes a threat to your external validity, because it raises the possibility that you can’t generalise or apply your results to the actual population that you care about.\n\nAs a general rule confounders are a bigger concern for non-experimental studies, precisely because they’re not proper experiments. By definition, you’re leaving lots of things uncontrolled, so there’s a lot of scope for confounders being present in your study. Experimental research tends to be much less vulnerable to confounders. The more control you have over what happens during the study, the more you can prevent confounders from affecting the results. With random allocation, for example, confounders are distributed randomly, and evenly, between different groups.\nHowever, there are always swings and roundabouts and when we start thinking about artefacts rather than confounders the shoe is very firmly on the other foot. For the most part, artefactual results tend to be a concern for experimental studies than for non-experimental studies. To see this, it helps to realise that the reason that a lot of studies are non-experimental is precisely because what the researcher is trying to do is examine human behaviour in a more naturalistic context. By working in a more real-world context you lose experimental control (making yourself vulnerable to confounders), but because you tend to be studying human psychology “in the wild” you reduce the chances of getting an artefactual result. Or, to put it another way, when you take psychology out of the wild and bring it into the lab (which we usually have to do to gain our experimental control), you always run the risk of accidentally studying something different to what you wanted to study.\nBe warned though. The above is a rough guide only. It’s absolutely possible to have confounders in an experiment, and to get artefactual results with non-experimental studies. This can happen for all sorts of reasons, not least of which is experimenter or researcher error. In practice, it’s really hard to think everything through ahead of time and even very good researchers make mistakes.\nAlthough there’s a sense in which almost any threat to validity can be characterised as a confounder or an artefact, they’re pretty vague concepts. So let’s have a look at some of the most common examples.\n\n2.7.1 History effects\nHistory effects refer to the possibility that specific events may occur during the study that might influence the outcome measure. For instance, something might happen in between a pretest and a post-test. Or in-between testing participant 23 and participant 24. Alternatively, it might be that you’re looking at a paper from an older study that was perfectly valid for its time, but the world has changed enough since then that the conclusions are no longer trustworthy. Examples of things that would count as history effects are:\n\nYou’re interested in how people think about risk and uncertainty. You started your data collection in December 2010. But finding participants and collecting data takes time, so you’re still finding new people in February 2011. Unfortunately for you (and even more unfortunately for others), the Queensland floods occurred in January 2011 causing billions of dollars of damage and killing many people. Not surprisingly, the people tested in February 2011 express quite different beliefs about handling risk than the people tested in December 2010. Which (if any) of these reflects the “true” beliefs of participants? I think the answer is probably both. The Queensland floods genuinely changed the beliefs of the Australian public, though possibly only temporarily. The key thing here is that the “history” of the people tested in February is quite different to people tested in December.\nYou’re testing the psychological effects of a new anti-anxiety drug. So what you do is measure anxiety before administering the drug (e.g., by self-report, and taking physiological measures). Then you administer the drug, and afterwards you take the same measures. In the middle however, because your lab is in Los Angeles, there’s an earthquake which increases the anxiety of the participants.\n\n\n\n2.7.2 Maturation effects\nAs with history effects, maturational effects are fundamentally about change over time. However, maturation effects aren’t in response to specific events. Rather, they relate to how people change on their own over time. We get older, we get tired, we get bored, etc. Some examples of maturation effects are:\n\nWhen doing developmental psychology research you need to be aware that children grow up quite rapidly. So, suppose that you want to find out whether some educational trick helps with vocabulary size among 3 year olds. One thing that you need to be aware of is that the vocabulary size of children that age is growing at an incredible rate (multiple words per day) all on its own. If you design your study without taking this maturational effect into account, then you won’t be able to tell if your educational trick works.\nWhen running a very long experiment in the lab (say, something that goes for 3 hours) it’s very likely that people will begin to get bored and tired, and that this maturational effect will cause performance to decline regardless of anything else going on in the experiment\n\n\n\n2.7.3 Repeated testing effects\nAn important type of history effect is the effect of repeated testing. Suppose I want to take two measurements of some psychological construct (e.g., anxiety). One thing I might be worried about is if the first measurement has an effect on the second measurement. In other words, this is a history effect in which the “event” that influences the second measurement is the first measurement itself! This is not at all uncommon. Examples of this include:\n\nLearning and practice: e.g., “intelligence” at time 2 might appear to go up relative to time 1 because participants learned the general rules of how to solve “intelligence-test-style” questions during the first testing session.\nFamiliarity with the testing situation: e.g., if people are nervous at time 1, this might make performance go down. But after sitting through the first testing situation they might calm down a lot precisely because they’ve seen what the testing looks like.\nAuxiliary changes caused by testing: e.g., if a questionnaire assessing mood is boring then mood rating at measurement time 2 is more likely to be “bored” precisely because of the boring measurement made at time 1.\n\n\n\n2.7.4 Selection bias\nSelection bias is a pretty broad term. Suppose that you’re running an experiment with two groups of participants where each group gets a different “treatment”, and you want to see if the different treatments lead to different outcomes. However, suppose that, despite your best efforts, you’ve ended up with a gender imbalance across groups (say, group A has 80% females and group B has 50% females). It might sound like this could never happen but, trust me, it can. This is an example of a selection bias, in which the people “selected into” the two groups have different characteristics. If any of those characteristics turns out to be relevant (say, your treatment works better on females than males) then you’re in a lot of trouble.\n\n\n2.7.5 Differential attrition\nWhen thinking about the effects of attrition, it is sometimes helpful to distinguish between two different types. The first is homogeneous attrition, in which the attrition effect is the same for all groups, treatments or conditions. In the example I gave above, the attrition would be homogeneous if (and only if) the easily bored participants are dropping out of all of the conditions in my experiment at about the same rate. In general, the main effect of homogeneous attrition is likely to be that it makes your sample unrepresentative. As such, the biggest worry that you’ll have is that the generalisability of the results decreases. In other words, you lose external validity.\nThe second type of attrition is heterogeneous attrition, in which the attrition effect is different for different groups. More often called differential attrition, this is a kind of selection bias that is caused by the study itself. Suppose that, for the first time ever in the history of psychology, I manage to find the perfectly balanced and representative sample of people. I start running “Dani’s incredibly long and tedious experiment” on my perfect sample but then, because my study is incredibly long and tedious, lots of people start dropping out. I can’t stop this. Participants absolutely have the right to stop doing any experiment, any time, for whatever reason they feel like, and as researchers we are morally (and professionally) obliged to remind people that they do have this right. So, suppose that “Dani’s incredibly long and tedious experiment” has a very high drop out rate. What do you suppose the odds are that this drop out is random? Answer: zero. Almost certainly the people who remain are more conscientious, more tolerant of boredom, etc., than those that leave. To the extent that (say) conscientiousness is relevant to the psychological phenomenon that I care about, this attrition can decrease the validity of my results.\nHere’s another example. Suppose I design my experiment with two conditions. In the “treatment” condition, the experimenter insults the participant and then gives them a questionnaire designed to measure obedience. In the “control” condition, the experimenter engages in a bit of pointless chitchat and then gives them the questionnaire. Leaving aside the questionable scientific merits and dubious ethics of such a study, let’s have a think about what might go wrong here. As a general rule, when someone insults me to my face I tend to get much less co-operative. So, there’s a pretty good chance that a lot more people are going to drop out of the treatment condition than the control condition. And this drop out isn’t going to be random. The people most likely to drop out would probably be the people who don’t care all that much about the importance of obediently sitting through the experiment. Since the most bloody minded and disobedient people all left the treatment group but not the control group, we’ve introduced a confound: the people who actually took the questionnaire in the treatment group were already more likely to be dutiful and obedient than the people in the control group. In short, in this study insulting people doesn’t make them more obedient. It makes the more disobedient people leave the experiment! The internal validity of this experiment is completely shot.\n\n\n2.7.6 Non-response bias\nNon-response bias is closely related to selection bias and to differential attrition. The simplest version of the problem goes like this. You mail out a survey to 1000 people but only 300 of them reply. The 300 people who replied are almost certainly not a random subsample. People who respond to surveys are systematically different to people who don’t. This introduces a problem when trying to generalise from those 300 people who replied to the population at large, since you now have a very non-random sample. The issue of non-response bias is more general than this, though. Among the (say) 300 people that did respond to the survey, you might find that not everyone answers every question. If (say) 80 people chose not to answer one of your questions, does this introduce problems? As always, the answer is maybe. If the question that wasn’t answered was on the last page of the questionnaire, and those 80 surveys were returned with the last page missing, there’s a good chance that the missing data isn’t a big deal; probably the pages just fell off. However, if the question that 80 people didn’t answer was the most confrontational or invasive personal question in the questionnaire, then almost certainly you’ve got a problem. In essence, what you’re dealing with here is what’s called the problem of missing data. If the data that is missing was “lost” randomly, then it’s not a big problem. If it’s missing systematically, then it can be a big problem.\n\n\n2.7.7 Regression to the mean\nRegression to the mean refers to any situation where you select data based on an extreme value on some measure. Because the variable has natural variation it almost certainly means that when you take a subsequent measurement the later measurement will be less extreme than the first one, purely by chance.\nHere’s an example. Suppose I’m interested in whether a psychology education has an adverse effect on very smart kids. To do this, I find the 20 psychology I students with the best high school grades and look at how well they’re doing at university. It turns out that they’re doing a lot better than average, but they’re not topping the class at university even though they did top their classes at high school. What’s going on? The natural first thought is that this must mean that the psychology classes must be having an adverse effect on those students. However, while that might very well be the explanation, it’s more likely that what you’re seeing is an example of “regression to the mean”. To see how it works, let’s take a moment to think about what is required to get the best mark in a class, regardless of whether that class be at high school or at university. When you’ve got a big class there are going to be lots of very smart people enrolled. To get the best mark you have to be very smart, work very hard, and be a bit lucky. The exam has to ask just the right questions for your idiosyncratic skills, and you have to avoid making any dumb mistakes (we all do that sometimes) when answering them. And that’s the thing, whilst intelligence and hard work are transferable from one class to the next, luck isn’t. The people who got lucky in high school won’t be the same as the people who get lucky at university. That’s the very definition of “luck”. The consequence of this is that when you select people at the very extreme values of one measurement (the top 20 students), you’re selecting for hard work, skill and luck. But because the luck doesn’t transfer to the second measurement (only the skill and work), these people will all be expected to drop a little bit when you measure them a second time (at university). So their scores fall back a little bit, back towards everyone else. This is regression to the mean.\nRegression to the mean is surprisingly common. For instance, if two very tall people have kids their children will tend to be taller than average but not as tall as the parents. The reverse happens with very short parents. Two very short parents will tend to have short children, but nevertheless those kids will tend to be taller than the parents. It can also be extremely subtle. For instance, there have been studies done that suggested that people learn better from negative feedback than from positive feedback. However, the way that people tried to show this was to give people positive reinforcement whenever they did good, and negative reinforcement when they did bad. And what you see is that after the positive reinforcement people tended to do worse, but after the negative reinforcement they tended to do better. But notice that there’s a selection bias here! When people do very well, you’re selecting for “high” values, and so you should expect, because of regression to the mean, that performance on the next trial should be worse regardless of whether reinforcement is given. Similarly, after a bad trial, people will tend to improve all on their own. The apparent superiority of negative feedback is an artefact caused by regression to the mean (see Kahneman & Tversky (1973), for discussion).\n\n\n2.7.8 Experimenter bias\nExperimenter bias can come in multiple forms. The basic idea is that the experimenter, despite the best of intentions, can accidentally end up influencing the results of the experiment by subtly communicating the “right answer” or the “desired behaviour” to the participants. Typically, this occurs because the experimenter has special knowledge that the participant does not, for example the right answer to the questions being asked or knowledge of the expected pattern of performance for the condition that the participant is in. The classic example of this happening is the case study of “Clever Hans”, which dates back to 1907 (Pfungst, 1911). Clever Hans was a horse that apparently was able to read and count and perform other human like feats of intelligence. After Clever Hans became famous, psychologists started examining his behaviour more closely. It turned out that, not surprisingly, Hans didn’t know how to do maths. Rather, Hans was responding to the human observers around him, because the humans did know how to count and the horse had learned to change its behaviour when people changed theirs.\nThe general solution to the problem of experimenter bias is to engage in double blind studies, where neither the experimenter nor the participant knows which condition the participant is in or knows what the desired behaviour is. This provides a very good solution to the problem, but it’s important to recognise that it’s not quite ideal, and hard to pull off perfectly. For instance, the obvious way that I could try to construct a double blind study is to have one of my Ph.D. students (one who doesn’t know anything about the experiment) run the study. That feels like it should be enough. The only person (me) who knows all the details (e.g., correct answers to the questions, assignments of participants to conditions) has no interaction with the participants, and the person who does all the talking to people (the Ph.D. student) doesn’t know anything. Except for the reality that the last part is very unlikely to be true. In order for the Ph.D. student to run the study effectively they need to have been briefed by me, the researcher. And, as it happens, the Ph.D. student also knows me and knows a bit about my general beliefs about people and psychology (e.g., I tend to think humans are much smarter than psychologists give them credit for). As a result of all this, it’s almost impossible for the experimenter to avoid knowing a little bit about what expectations I have. And even a little bit of knowledge can have an effect. Suppose the experimenter accidentally conveys the fact that the participants are expected to do well in this task. Well, there’s a thing called the “Pygmalion effect”, where if you expect great things of people they’ll tend to rise to the occasion. But if you expect them to fail then they’ll do that too. In other words, the expectations become a self-fulfilling prophesy.\n\n\n2.7.9 Demand effects and reactivity\nWhen talking about experimenter bias, the worry is that the experimenter’s knowledge or desires for the experiment are communicated to the participants, and that these can change people’s behaviour (Rosenthal, 1966). However, even if you manage to stop this from happening, it’s almost impossible to stop people from knowing that they’re part of a psychological study. And the mere fact of knowing that someone is watching or studying you can have a pretty big effect on behaviour. This is generally referred to as reactivity or demand effects. The basic idea is captured by the Hawthorne effect: people alter their performance because of the attention that the study focuses on them. The effect takes its name from a study that took place in the “Hawthorne Works” factory outside of Chicago (see Adair (1984)). This study, from the 1920s, looked at the effects of factory lighting on worker productivity. But, importantly, change in worker behaviour occurred because the workers knew they were being studied, rather than any effect of factory lighting.\nTo get a bit more specific about some of the ways in which the mere fact of being in a study can change how people behave, it helps to think like a social psychologist and look at some of the roles that people might adopt during an experiment but might not adopt if the corresponding events were occurring in the real world:\n\nThe good participant tries to be too helpful to the researcher. He or she seeks to figure out the experimenter’s hypotheses and confirm them.\nThe negative participant does the exact opposite of the good participant. He or she seeks to break or destroy the study or the hypothesis in some way.\nThe faithful participant is unnaturally obedient. He or she seeks to follow instructions perfectly, regardless of what might have happened in a more realistic setting.\nThe apprehensive participant gets nervous about being tested or studied, so much so that his or her behaviour becomes highly unnatural, or overly socially desirable.\n\n\n\n2.7.10 Placebo effects\nThe placebo effect is a specific type of demand effect that we worry a lot about. It refers to the situation where the mere fact of being treated causes an improvement in outcomes. The classic example comes from clinical trials. If you give people a completely chemically inert drug and tell them that it’s a cure for a disease, they will tend to get better faster than people who aren’t treated at all. In other words, it is people’s belief that they are being treated that causes the improved outcomes, not the drug.\nHowever, the current consensus in medicine is that true placebo effects are quite rare and most of what was previously considered placebo effect is in fact some combination of natural healing (some people just get better on their own), regression to the mean and other quirks of study design. Of interest to psychology is that the strongest evidence for at least some placebo effect is in self-reported outcomes, most notably in treatment of pain (Hróbjartsson & Gøtzsche, 2010).\n\n\n2.7.11 Situation, measurement and sub-population effects\nIn some respects, these terms are a catch-all term for “all other threats to external validity”. They refer to the fact that the choice of sub-population from which you draw your participants, the location, timing and manner in which you run your study (including who collects the data) and the tools that you use to make your measurements might all be influencing the results. Specifically, the worry is that these things might be influencing the results in such a way that the results won’t generalise to a wider array of people, places and measures.\n\n\n2.7.12 Fraud, deception and self-deception\n\nIt is difficult to get a man to understand something, when his salary depends on his not understanding it.\n- Upton Sinclair\n\nThere’s one final thing I feel I should mention. While reading what the textbooks often have to say about assessing the validity of a study I couldn’t help but notice that they seem to make the assumption that the researcher is honest. I find this hilarious. While the vast majority of scientists are honest, in my experience at least, some are not.6 Not only that, as I mentioned earlier, scientists are not immune to belief bias. It’s easy for a researcher to end up deceiving themselves into believing the wrong thing, and this can lead them to conduct subtly flawed research and then hide those flaws when they write it up. So you need to consider not only the (probably unlikely) possibility of outright fraud, but also the (probably quite common) possibility that the research is unintentionally “slanted”. I opened a few standard textbooks and didn’t find much of a discussion of this problem, so here’s my own attempt to list a few ways in which these issues can arise:\n\nData fabrication. Sometimes, people just make up the data. This is occasionally done with “good” intentions. For instance, the researcher believes that the fabricated data do reflect the truth, and may actually reflect “slightly cleaned up” versions of actual data. On other occasions, the fraud is deliberate and malicious. Some high-profile examples where data fabrication has been alleged or shown include Cyril Burt (a psychologist who is thought to have fabricated some of his data), Andrew Wakefield (who has been accused of fabricating his data connecting the MMR vaccine to autism) and Hwang Woo-suk (who falsified a lot of his data on stem cell research).\nHoaxes. Hoaxes share a lot of similarities with data fabrication, but they differ in the intended purpose. A hoax is often a joke, and many of them are intended to be (eventually) discovered. Often, the point of a hoax is to discredit someone or some field. There’s quite a few well known scientific hoaxes that have occurred over the years (e.g., Piltdown man) and some were deliberate attempts to discredit particular fields of research (e.g., the Sokal affair).\nData misrepresentation. While fraud gets most of the headlines, it’s much more common in my experience to see data being misrepresented. When I say this I’m not referring to newspapers getting it wrong (which they do, almost always). I’m referring to the fact that often the data don’t actually say what the researchers think they say. My guess is that, almost always, this isn’t the result of deliberate dishonesty but instead is due to a lack of sophistication in the data analyses. For instance, think back to the example of Simpson’s paradox that I discussed in the beginning of this book. It’s very common to see people present “aggregated” data of some kind and sometimes, when you dig deeper and find the raw data yourself you find that the aggregated data tell a different story to the disaggregated data. Alternatively, you might find that some aspect of the data is being hidden, because it tells an inconvenient story (e.g., the researcher might choose not to refer to a particular variable). There’s a lot of variants on this, many of which are very hard to detect.\nStudy “misdesign”. Okay, this one is subtle. Basically, the issue here is that a researcher designs a study that has built-in flaws and those flaws are never reported in the paper. The data that are reported are completely real and are correctly analysed, but they are produced by a study that is actually quite wrongly put together. The researcher really wants to find a particular effect and so the study is set up in such a way as to make it “easy” to (artefactually) observe that effect. One sneaky way to do this, in case you’re feeling like dabbling in a bit of fraud yourself, is to design an experiment in which it’s obvious to the participants what they’re “supposed” to be doing, and then let reactivity work its magic for you. If you want you can add all the trappings of double blind experimentation but it won’t make a difference since the study materials themselves are subtly telling people what you want them to do. When you write up the results the fraud won’t be obvious to the reader. What’s obvious to the participant when they’re in the experimental context isn’t always obvious to the person reading the paper. Of course, the way I’ve described this makes it sound like it’s always fraud. Probably there are cases where this is done deliberately, but in my experience the bigger concern has been with unintentional misdesign. The researcher believes and so the study just happens to end up with a built in flaw, and that flaw then magically erases itself when the study is written up for publication.\nData mining & post hoc hypothesising. Another way in which the authors of a study can more or less misrepresent the data is by engaging in what’s referred to as “data mining” (see Gelman and Loken 2014, for a broader discussion of this as part of the “garden of forking paths” in statistical analysis). As we’ll discuss later, if you keep trying to analyse your data in lots of different ways, you’ll eventually find something that “looks” like a real effect but isn’t. This is referred to as “data mining”. It used to be quite rare because data analysis used to take weeks, but now that everyone has very powerful statistical software on their computers it’s becoming very common. Data mining per se isn’t “wrong”, but the more that you do it the bigger the risk you’re taking. The thing that is wrong, and I suspect is very common, is unacknowledged data mining. That is, the researcher runs every possible analysis known to humanity, finds the one that works, and then pretends that this was the only analysis that they ever conducted. Worse yet, they often “invent” a hypothesis after looking at the data to cover up the data mining. To be clear. It’s not wrong to change your beliefs after looking at the data, and to reanalyse your data using your new “post hoc” hypotheses. What is wrong (and I suspect common) is failing to acknowledge that you’ve done. If you acknowledge that you did it then other researchers are able to take your behaviour into account. If you don’t, then they can’t. And that makes your behaviour deceptive. Bad!\nPublication bias & self-censoring. Finally, a pervasive bias is “non-reporting” of negative results. This is almost impossible to prevent. Journals don’t publish every article that is submitted to them. They prefer to publish articles that find “something”. So, if 20 people run an experiment looking at whether reading Finnegans Wake causes insanity in humans, and 19 of them find that it doesn’t, which one do you think is going to get published? Obviously, it’s the one study that did find that Finnegans Wake causes insanity.7 This is an example of a publication bias. Since no-one ever published the 19 studies that didn’t find an effect, a naive reader would never know that they existed. Worse yet, most researchers “internalise” this bias and end up self-censoring their research. Knowing that negative results aren’t going to be accepted for publication, they never even try to report them. As a friend of mine says “for every experiment that you get published, you also have 10 failures”. And she’s right. The catch is, while some (maybe most) of those studies are failures for boring reasons (e.g. you stuffed something up) others might be genuine “null” results that you ought to acknowledge when you write up the “good” experiment. And telling which is which is often hard to do. A good place to start is a paper by Ioannidis (2005) with the depressing title “Why most published research findings are false”. I’d also suggest taking a look at work by Kühberger et al. (2014) presenting statistical evidence that this actually happens in psychology.\n\nThere’s probably a lot more issues like this to think about, but that’ll do to start with. What I really want to point out is the blindingly obvious truth that real world science is conducted by actual humans, and only the most gullible of people automatically assumes that everyone else is honest and impartial. Actual scientists aren’t usually that naive, but for some reason the world likes to pretend that we are, and the textbooks we usually write seem to reinforce that stereotype."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#summary",
    "href": "02-A-brief-introduction-to-research-design.html#summary",
    "title": "2  A brief introduction to research design",
    "section": "2.8 Summary",
    "text": "2.8 Summary\nThis chapter isn’t really meant to provide a comprehensive discussion of psychological research methods. It would require another volume just as long as this one to do justice to the topic. However, in real life statistics and study design are so tightly intertwined that it’s very handy to discuss some of the key topics. In this chapter, I’ve briefly discussed the following topics:\n\nIntroduction to psychological measurement. What does it mean to operationalise a theoretical construct? What does it mean to have variables and take measurements?\nScales of measurement and types of variables. Remember that there are two different distinctions here. There’s the difference between discrete and continuous data, and there’s the difference between the four different scale types (nominal, ordinal, interval and ratio).\nAssessing the reliability of a measurement. If I measure the “same” thing twice, should I expect to see the same result? Only if my measure is reliable. But what does it mean to talk about doing the “same” thing? Well, that’s why we have different types of reliability. Make sure you remember what they are.\nThe “role” of variables: predictors and outcomes. What roles do variables play in an analysis? Can you remember the difference between predictors and outcomes? Dependent and independent variables? Etc.\nExperimental and non-experimental research designs. What makes an experiment an experiment? Is it a nice white lab coat, or does it have something to do with researcher control over variables?\nAssessing the validity of a study. Does your study measure what you want it to? How might things go wrong? And is it my imagination, or was that a very long list of possible ways in which things can go wrong?\n\nAll this should make clear to you that study design is a critical part of research methodology. I built this chapter from the classic little book by Campbell & Stanley (1963), but there are of course a large number of textbooks out there on research design. Spend a few minutes with your favourite search engine and you’ll find dozens.\n\n\n\n\nAdair, G. (1984). The hawthorne effect: A reconsideration of the methodological artifact. Journal of Applied Psychology, 69, 334–345.\n\n\nCampbell, D. T., & Stanley, J. C. (1963). Experimental and quasi-experimental designs for research. Houghton Mifflin.\n\n\nHróbjartsson, A., & Gøtzsche, P. (2010). Placebo interventions for all clinical conditions. Cochrane Database of Systematic Reviews, 1. https://doi.org//10.1002/14651858.CD003974.pub3\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Med, 2(8), 697–701.\n\n\nKahneman, D., & Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80, 237–251.\n\n\nKühberger, A., Fritz, A., & Scherndl, T. (2014). Publication bias in psychology: A diagnosis based on the correlation between effect size and sample size. Public Library of Science One, 9, 1–8.\n\n\nPfungst, O. (1911). Clever hans (the horse of mr. Von osten): A contribution to experimental animal and human psychology (C. L. Rahn, Trans.). Henry Holt.\n\n\nRosenthal, R. (1966). Experimenter effects in behavioral research. Appleton.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677–680."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#首先談談統計的心理學",
    "href": "01-Why-do-we-learn-statistics.html#首先談談統計的心理學",
    "title": "1  為什麼要學習統計",
    "section": "1.1 首先談談統計的心理學",
    "text": "1.1 首先談談統計的心理學\n先講許多第一次來上課的同學驚呆的現實，統計學在心理學課程佔有相當的份量。另外一個不令人意外的現實，是統計學很少被上過心理學課程的同學推薦。畢竟正常來說，如果是喜歡統計的同學，應該是去修統計系的課，而不是來上心理學系的課。不需要太正式的調查，正在上心理學課程的同學，有一大部分想必很不高興要學有這麼多統計。為了幫助同學渡過這段不適應，我想這本書先來談談一般人對於統計學的一些常見疑問。\n\n這個問題的很大一部分與統計學的本質有關。統計學是什麼？統計學是做什麼用的？為什麼科學家如此迷戀統計？只要你能提出這些問題，都是值得好好回答的問題。我們先從最後一個問題開始談吧。科學家集體似乎想對每件事情進行統計分析，旁人看來近乎固執。科學家確實經常使用統計學，有時候甚至忘記向一般人解釋為什麽這樣做。這是科學家之間的一種信仰，尤其是社會科學家。任何科學家沒有完成統計之前，他的發現是不會被其他科學家信任的。(新鮮人)大學生看了，可能會認為這群人都是瘋子，因為沒有科學家會花時間向一般人回答這個非常簡單的問題：\n\n為什麼當心理學家要會使用統計？為什麼科學家不能憑生活常識做研究？\n\n在某些方面，這是一個天真的問題，但大多數好問題都是這樣開始的。對此問題已經有許多有意思的回答2，不過我心中最好的回答是一個大家都懂的人性現實：人類無法充分信任自己的判斷。正是因為了解人類本身，很容易受到各種偏見、誘惑和軟弱而影響個人的判斷。在很多情況，統計學提供一種基本保障。使用”生活常識”評估證據意味著信任直覺，依靠言語推論以及使用人類原生的推理能力找出正確答案。但是大多數科學家認為這種方法不太可靠。\n\n其實好好地想一想，這很像是一個心理學家會研究的問題，既然我在心理學系工作，這似乎是個值得深入研究的好題目。真的有道理認為”靠常識”做出的研究是值得信賴的嗎？言語推論是用語言構建的，所有語言都帶有偏見 - 像是有些事情特別難說，並不一定是因為它們是錯的（例如，量子力學是一個很好的理論，但很難用言語解釋）。因為人類的”直覺”本能並不是用來解決科學問題，而是用來應付日常推論問題 - 由於生物演化速度比文化演化遲緩，我們應該說直覺是為了解決不同於我們的生活經驗的日常問題而設計的產物。科學研究的根本是理性推理，需要我們進行”歸納”，做出明智的猜测，超越用感官接收的證據，概括這個世界的樣貌。如果你認為有本事不受各種外界干擾進行理性推理，那麼我有份年薪百萬，請你去柬埔寨打工的機會想介紹給你3。哎呀，正如下一節我要說的，我们也無法解决不需要猜測的”演繹”問題，也就是那些不會受到預先存在的偏見所影響的問題。\n\n\n1.1.1 信念偏誤的詛咒\n大多數人類都很聰明。我們肯定比地球上其他物種要聰明得多（盡管可能很多人不這麽認為）。人類的思維能力是非常神奇的產物，我們似乎有能力思考和推理任何不可思議的事情。但是這並不意味著人類完美無缺。心理學家累積多年的研究已經表明，人類確實很難保持中立，公正地評估證據，而不會受到預期偏見的影響。有個很好的例子是邏輯推理中的信念偏差效應：如果你要求我判斷一個特定的論點是否在邏輯上是正確的（也就是說，如果前提是真實的，結論就是真實的），我常常會受到結論的可信度影響，即使我明知不該如此。以下是一段結論可信的有效邏輯論證：\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n再看這一段結論不可信的有效邏輯論證：\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n兩段論證的結構都是相同而且有效4。然而第二段的前提1有理由相信並不正確，所以有人會認為結論也不正確。其實無論前提的內容如何，論證的演繹有效性僅取決於前提與結論的結構。也就是說，有效論證不必然要包括真實的敘述。\n\n另一方面，無效的論證也能有讓人相信為真的結論，就像下一段論證：\n\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n\n最後來看以下這段無效演繹且結論不可信的論證：\n\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n假設人類真的完全能夠放下對於敘述真實性的預期偏見，僅憑邏輯有效性評估論點是否合理。那麼我們可以設計實驗，測試看看是否所有人都會認為有效論證是正確的，沒有人會說無效論證是正確的。我把這樣的假設製成表1.1 。\n\n\n\n\n表1.1 判斷人類能拋開偏見進行有效論證的假設結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n100\\(\\%\\) 認為“有效”\n100\\(\\%\\) 認為“有效”\n\n\n論證無效\n0\\(\\%\\) 認為“有效”\n0\\(\\%\\) 認為“有效”\n\n\n\n假如心理學研究結果就像表內的數值（或者只是接近這樣的數值），我們可能覺得完全靠人類的直覺就能做出結論。只要是像這樣的研究結果，科學家完全可以根據他們的常識評估結果數據，不用花時間處理那堆讓很多人看不懂什麼意思的統計分析。然而，你們已經至少修過心理學概論，對於這套實驗的真正結果應該略知一二。\nEvans 等 (1983) 做了一系列探討人類如何進行邏輯推論的經典實驗。他們發現只有結論敘述符合多數人的預期偏誤(也就是信念)，實驗結果才會接近人類能做有效推論的假設(表1.2)。\n\n\n\n表1.2 預期偏誤與論證有效性的實驗結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n92\\(\\%\\) 認為“有效”\n\n\n\n論證無效\n\n8\\(\\%\\) 認為“有效”\n\n\n\n\n雖然不夠完美，但是這樣的結果也算不錯了。不過看看另外兩個與一般人的直覺完全相反實驗情況，與表1.1的完美假設完全不同(表1.3)。\n\n\n\n表1.3 直覺判斷與論證有效性的實驗結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n92\\(\\%\\) 認為“有效”\n46\\(\\%\\) 認為“有效”\n\n\n論證無效\n92\\(\\%\\) 認為“有效”\n8\\(\\%\\) 認為“有效”\n\n\n\n\n哎呀，這不是好解釋的結果。實驗結果顯示似乎向一般人講述一個與既有信念互相矛盾但有邏輯效力的論點時，人們很難相信這是一個強而有力的論點（只有 46% 的人會相信）。更糟的是，向一般人講述一個與既有偏見相符但沒有邏輯效力的的論點時，幾乎沒有人能看出這個論點無效（高達 92% 的人判斷錯誤！）。5\n如果仔細想想，這並不是很糟糕的結果。總體來看，一般人的表現比隨機亂猜好，大約有60％的人做出了正確的判斷（隨機亂猜應該是50％）。即使如此，如果你是一名專業的“證據鑑識人員”，有人送你一個可以提高正確決策的機率的神奇工具，比如說從 60% 到95% ，你應會欣然接受吧？幸好我們有一種工具可以做到這一點。這種工具不是魔法，而是統計學。這就是為什麼科學家喜歡使用統計的最主要原因。我們很容易「信任我們想要相信的」。所以如果我們要做到「信任資料」，就需要一些工具幫助我們控制個人偏見。而這就是統計學的用途，它能幫助我們保持推論的誠實。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#檢核辛普森悖論",
    "href": "01-Why-do-we-learn-statistics.html#檢核辛普森悖論",
    "title": "1  為什麼要學習統計",
    "section": "1.2 檢核辛普森悖論",
    "text": "1.2 檢核辛普森悖論\n接著來談一則真實故事(我想應該是真的)。1973年，美國加州大學柏克萊分校高層擔憂該年研究所新生報考及錄取的狀況。更明白的說，他們覺得被錄取的研究生呈現性別不平等的狀況(見表1.4)。\n\n\n  表1.4 柏克萊男女新生報考人數及錄取比例\n    \n         \n        申請者人數\n        通過比例\n    \n    \n        男性\n        8442\n        44\\(\\%\\)\n    \n    \n        女性\n        4321\n        35\\(\\%\\)\n    \n\n\n當年的柏克萊校方擔心被考生告上法院！因為將近13,000名學生申請入學，男女之間錄取率相差9％，這樣的差距實在太大了，不可能是巧合。而且人數如此龐大，可說是鐵一般的事實。但是如果我對你說，這些數據實際上反映了對女性考生些微的偏袒，你可能會認為我搞錯了或者有性別歧視。\n\n\n然而，實情卻有點出人意料。仔細檢視錄取資料時，有人發現了另一個版本的故事(Bickel et al., 1975)。具體地說，當校方按照學系逐一計算錄取率時，發現多數學系的女性錄取率實際上略高於男性。表1.5 顯示了六個考生最多的學系錄取情形（為了保障隱私，以下將學系名稱省略）：\n\n\n  表1.5 1973年伯克萊大學六個學系錄取學生的性別分佈\n    \n         \n         男性\n         女性\n    \n    \n        學系\n        申請者人數\n        通過比例\n        申請者人數\n        通過比例\n    \n    \n        A\n        825\n        62\\(\\%\\)\n        108\n        82\\(\\%\\)\n    \n    \n        B\n        560\n        63\\(\\%\\)\n        25\n        68\\(\\%\\)\n    \n    \n        C\n        325\n        37\\(\\%\\)\n        593\n        34\\(\\%\\)\n    \n    \n        D\n        417\n        33\\(\\%\\)\n        375\n        35\\(\\%\\)\n    \n    \n        E\n        191\n        28\\(\\%\\)\n        393\n        24\\(\\%\\)\n    \n    \n        F\n        272\n        6\\(\\%\\)\n        341\n        7\\(\\%\\)\n    \n\n\nRemarkably, most departments had a higher rate of admissions for females than for males! Yet the overall rate of admission across the university for females was lower than for males. How can this be? How can both of these statements be true at the same time?\nHere’s what’s going on. Firstly, notice that the departments are not equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get A>B>D>C>F>E (the “easy” departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C>E>D>F>A>B. In other words, what these data seem to be suggesting is that the female applicants tended to apply to “harder” departments. And in fact, if we look at Figure Figure 1.1 we see that this trend is systematic, and quite striking. This effect is known as Simpson’s paradox. It’s not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it’s real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are lots of subtle, counter-intuitive traps lying in wait for the unwary. That’s reason #2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.\nBefore leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves part of the problem. Remember that we started all this with the concern that Berkeley’s admissions processes might be unfairly biased against female applicants. When we looked at the “aggregated” data, it did seem like the university was discriminating against women, but when we “disaggregate” and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments. From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can’t dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce.\n\n\n\n\n\nFigure 1.1: 1973年柏克萊大學招生數據，取自 Bickel et al. (1975) 的圖1。圖中85個點分別代表至少接受一位女性申請入學的學系，依不分性別錄取率與女性申請入學比例繪圖。圓圈代表申請者超過40人的學系；圓圈面積代表該系申請人數佔全校申請人數的比例。十字代表申請者未達40人的學系。\n\n\n\n\nThat was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.That was the basis for my somewhat glib remarks earlier, but that’s not exactly the whole story, is it? After all, if we’re interested in this from a more sociological and psychological perspective, we might want to ask why there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to “hard sciences” and females prefer “humanities”. And suppose further that the reason for why the humanities departments have low admission rates is because the government doesn’t want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are “useless chick stuff”. That seems pretty blatantly gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you’re interested in the overall structural effects of subtle gender biases, then you probably want to look at both the aggregated and disaggregated data. If you’re interested in the decision making process at Berkeley itself then you’re probably only interested in the disaggregated data.\nIn short there are a lot of critical questions that you can’t answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a tool to help you learn about your data. No more and no less. It’s a powerful tool to that end, but there’s no substitute for careful thought."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#心理學中的統計",
    "href": "01-Why-do-we-learn-statistics.html#心理學中的統計",
    "title": "1  為什麼要學習統計",
    "section": "1.3 心理學中的統計",
    "text": "1.3 心理學中的統計\n希望前面的討論能夠清楚解釋，為何各種科學領域都要使用統計。不過你應該還是會質疑學習心理學為何要學習統計，也會納悶為何許多心理學課程內容都與統計有關。以下是我常聽到來自學生的質疑，以及我的回應…\n\n為什麼學習心理學要學這麼多統計？\n\n坦白說，有好幾個原因，其中有些原因比較值得談。最重要的原因是心理學是一門統計科學。我的意思是，我們研究的「對象」是人–真實的人是複雜的、美妙的、混亂的、還有偏激的人總令人憤怒。物理學研究的「對象」包括電子等物體，雖然物理學問題也有各種複雜的情況，但是電子沒有自己的思想。它們沒有個人意見，電子之間沒有奇怪而且任意的差異，在實驗中不會感到無聊，不會對實驗者發脾氣，然後故意搞爛實驗（我沒有做過這樣的事情唷！）。從基本面來說，心理學比物理學更難研究6。所以說，我們以心理學家的身份教你統計學，是因為你需要比物理學家更會掌握統計學。物理學裡有句老生常談，意思是”如果你的實驗要用到統計學，那應該要設計一個更好的實驗”。他們確實有底氣這樣說，因為物理學研究對象的混亂程度與社會科學家面對的相比，是令人妒恨的簡單。而且不只是心理學。大多數社會科學都非常依賴統計學。不是因為心理學家是糟糕的實驗設計者，而是因為心理學面對的多數問題很難只靠實驗設計解決。學習統計學是因為你真的真的需要它。\n\n不能將統計外包給別人做嗎？\n\n在某些情況是可以這樣，但不能全部都交給別人做。你確實不必要成為一名受過完整訓練的統計學家，也可以研究心理學，但是你需要具備一定的統計能力。在我看來，有三個原因，每個心理學研究者都應該具備基本統計能力：\n\n\n首先是最基本的原因：統計學與研究設計密不可分。如果你想成為設計心理學研究的專家，你至少需要了解統計學的基礎。(譯註： 單元 2 就是談研究設計)\n其次，如果你想成為研究心理學的專家，那麼你就需要有能夠讀懂心理學文獻的能力，對吧？但是幾乎每篇心理學論文都有報告統計分析結果。如果你真的想徹底搞懂心理學，你就需要理解報告作者對資料做了什麼分析。也就是說你需要了解一定程度的統計學。\n第三，依靠別人做統計學有一個很實際問題：統計分析是件價格昂貴的工作。如果你曾經無聊到去查詢澳洲政府制定的大學學費標準，會發現一件有趣的事情：統計學被指定為”國家優先”項目，因此學費比任何其他學科都低得多。這是因為社會各界都需要統計學專家的協助。從心理學研究者的立場來看，我們面對的是供給遠少於需求的賣方市場！幾乎任何一個心理學研究室都能看到相同的殘酷現實，就是沒有足夠經費聘請統計專家。因此，經濟現實逼迫心理學家必須自立自強。\n\n除此之外，這些原因不只適用於從事研究的人員。如果你想成為一名應用取向的心理學家，為了掌握最新的研究進展，能夠獨立閱讀充滿各種統計報告的科學文獻也有助職涯發展。\n\n我不打算從事與心理學有關的工作、研究或臨床實務。我還需要學嗎？\n\n好吧，你快要難倒我了。總而言之，我相信統計學對你還是很重要的。對包括你在內的所有現代人類來說，統計學是很重要的基本知識。在21世紀的今天，隨處都是資料。老實說，今天要維持自已活得像現代人，基本的統計知識已經是必備生存工具了！下一節我繼續說給你聽。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#日常生活中的統計",
    "href": "01-Why-do-we-learn-statistics.html#日常生活中的統計",
    "title": "1  為什麼要學習統計",
    "section": "1.4 日常生活中的統計",
    "text": "1.4 日常生活中的統計\n\n“We are drowning in information,\nbut we are starved for knowledge”\n- Various authors, original probably John Naisbitt\n\n當我撰寫統計學講義時，我從 ABC 新聞網站找了 20 篇當時最新的新聞報導。我判斷其中 8 篇文字內容有提到統計資訊，不過有 6 篇報導內容有誤。若你想知道有什麼錯誤，最常見的錯誤是沒有報告基礎數據（例如，報導提到某個情況能觀察到 5％的人具有某種特徵 ，但沒有說明找了多少人總計出這個百分比！）。我想說的不是記者的統計素養很糟糕（雖然多數記者的統計知識真的很糟糕），而是基本的統計知識真的非常有幫助，可以幫助你瞭解別人表達的錯誤，還有是不是隨便拿些數據製造謠言(譯注：本書翻譯工程始於2022年末，有經歷疫情的同學應該對這段時間的各種不實疫情報導有感受。)。事實上，充實統計知識後，為個人帶來的最大一種改變，就是讓你更常對報紙和網路的資訊感到憤怒。之後在 單元 4 的 小單元 4.1.5 ，你會看到實際例子。本書的後續更新版本，會繼續收集更多類似的錯誤報導。"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#統計以外的研究方法",
    "href": "01-Why-do-we-learn-statistics.html#統計以外的研究方法",
    "title": "1  為什麼要學習統計",
    "section": "1.5 統計以外的研究方法",
    "text": "1.5 統計以外的研究方法\n到這裡為止，討論的研究方法大部分都是與統計學有關，你有理由相信這門課只關心和統計學有關的研究方法主題。坦白說，這麼認為並非完全錯誤，但是研究方法是比統計學更廣泛的課題。大多數研究方法課程都會涵蓋研究設計相關的實用主題，特別是進行與人類有關的研究會遇到的問題。但是，約有 99% 的學生害怕課程中統計的部分。這本書之所以著重統計學，是希望我能讓你相信統計學很重要，更重要的是，統計並不可怕。這也就是為什麼，大多數初級研究方法課程都會提到統計學。但是不是因為教師都是壞蛋，其實恰恰相反。各種入門課程之所以重視統計學，是因為同學們需要在學習各種研究方法之前，要充分了解統計學。為什麼？因為你在任何課程的所有作業都要靠統計方法才能完成，比起其他方法學工具，作業中的統計使用頻率經常居冠。心理系的作業通常不需要你從頭開始設計自己的研究（若要從頭開始，就需要了解很多關於研究設計的知識），而是要你分析和解釋別人設計的研究所收集的數據（這是你需要統計學的狀況）。就作業安排的意義來說，為了幫助你在其他課程學得好，統計學是需要優先學習的課程。\n但是要注意，“優先”與”重要”是不同的概念 - 雖然兩者都是關鍵。我想強調的是研究設計和數據分析同樣重要，本書確實花了相當多篇幅在這些主題。然而，雖然統計學具有某種普遍性，提供了一套對許多類型的心理學研究都有用的核心工具，但是不是所有研究方法都會運用統計。有些研究設計的一般原則是每個研究者都應該注意，但是許多研究設計非常特殊，並且只有在特定研究領域才會使用。基礎統計和研究方法課程考慮到細節的重要性，不一定會介紹這些特殊的研究設計。\n\n\n\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in graduate admissions: Data from Berkeley. Science, 187, 398–404.\n\n\nEvans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On the conflict between logic and belief in syllogistic reasoning. Memory and Cognition, 11, 295–306."
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html",
    "href": "01-Why-do-we-learn-statistics.html",
    "title": "1  為什麼要學習統計",
    "section": "",
    "text": "\\[ \\]"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html",
    "href": "02-A-brief-introduction-to-research-design.html",
    "title": "2  簡易入門研究設計",
    "section": "",
    "text": "“To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.”\n– Sir Ronald Fisher1\nIn this chapter, we’re going to start thinking about the basic ideas that go into designing a study, collecting data, checking whether your data collection works, and so on. It won’t give you enough information to allow you to design studies of your own, but it will give you a lot of the basic tools that you need to assess the studies done by other people. However, since the focus of this book is much more on data analysis than on data collection, I’m only giving a very brief overview. Note that this chapter is “special” in two ways. Firstly, it’s much more psychology specific than the later chapters. Secondly, it focuses much more heavily on the scientific problem of research methodology, and much less on the statistical problem of data analysis. Nevertheless, the two problems are related to one another, so it’s traditional for stats textbooks to discuss the problem in a little detail. This chapter relies heavily on Campbell & Stanley (1963) and Stevens (1946) for the discussion of scales of measurement."
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#變項的角色-預測變項與結果變項",
    "href": "02-A-brief-introduction-to-research-design.html#變項的角色-預測變項與結果變項",
    "title": "2  研究設計入門",
    "section": "2.4 變項的”角色”: 預測變項與結果變項",
    "text": "2.4 變項的”角色”: 預測變項與結果變項\n\n結束這個一節之前，我們還需要認識一些專有名稱。通常執行完一項行為科學研究後，都會得到各種變項。研究者分析資料的過程中，通常是嘗試用某些變項來解釋其他某些變項，所以區分“用來做出解釋的變項”和“被解釋的變項”是很重要的，學習統計的基本功之一就是要弄清楚如何區分。同學們首先可以從熟悉描述變項的數學符號開始，因為會一直遇到。以下用 Y 表示“要解釋的”變項，用 X1，X2 等符號表示“進行解釋的”變項。\n在各種分析程序， Y 和 Xi 會有各種不同名稱，因為兩種在分析程序裡扮演不同角色。這些角色的經典名稱是依變項(DV)和獨變項(IV)。IV 是您用於進行解釋的變項(即 Xi)，DV 是被解釋的變項(即 Y)。然而，本書原作者認為這些名詞很不好使用。它們既難記，也很容易誤導使用者，原作者推薦讀者使用其他替代名稱。在這本書，最稱使用的名稱是預測變項和結果變項，理由是分析者的工作是使用預測變項來預測結果變項。12 表2.1整理了以上的所有名稱及意義。\n\n表2.1 變項的各種名稱及意義\n\n\n變項的“角色”\n古典名稱\n現代名稱\n\n\n“要解釋的”\n依變項(DV)\n結果變項\n\n\n“推進解釋的”\n獨變項(IV)\n預測變項"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#實驗與非實驗研究",
    "href": "02-A-brief-introduction-to-research-design.html#實驗與非實驗研究",
    "title": "2  研究設計入門",
    "section": "2.5 實驗與非實驗研究",
    "text": "2.5 實驗與非實驗研究\n\n這一節要討論的是“實驗研究”和“非實驗研究”之間的區別。區分這兩類研究的關鍵，在於研究者控制參與研究過程的參與者和研究條件的程度。\n\n2.5.1 實驗研究\n實驗研究的關鍵特徵是研究者全面控制研究過程的細節，特別是參與者在研究過程的主觀體驗。具體來說，執行實驗的研究者只會操作或改變預測變項(IV)，但是允許結果變項(DV)自然變化。專業研究者的想法是透過改變預測變項條件，查看條件的改變有沒有對結果產生任何直接的影響，確認因果關係的推論。此外，為了確保預測變項之外的其他因素不會影響結果，非預測變項的所有條件都保持不變，或者運用某種方法達到條件之間的“平衡”，確保非預測變項的因素不會影響結果。在真實的研究場域，要認真考慮所有可能的影響實驗結果的因素，幾乎是不可能的任務，更不用說所有非預測變項的條件都要保持不變。公認最有效的解決方法是隨機化，也就是將每位參與者隨機分派到不同的實驗條件組，然後操作每一組的實驗條件(也就是分派不同的預測變項值)。稍後我們會討論更多隨機化方法，現在只要知道隨機化方法的功能是儘可能減少(但不能完全消除)實驗條件之間，任何可能存在的系統化誤差。\n現在來看一個非常簡單、不過完全不現實且極不道德的例子。假如你想弄清楚吸菸是否會導致肺癌，一種方法是找來一群吸菸者和一群不吸菸者，然後觀察吸菸者的肺癌發病率是否比不吸菸者高。請注意！這並不是一項合理的實驗，因為研究者無法充分控制誰吸菸誰不吸菸。例如，吸菸者的飲食習慣可能也不會顧慮個人身體健康，或者這類人士可能從事需要高度消耗體力的工作等。也就是說，只以有沒有吸菸分組，其實忽略很多其他能影響個人健康的條件差異。所以，吸菸者的肺癌發病率較高，可能是其他原因引起，並不是吸菸直接造成。這些會影響結果的其他因素(如飲食)，專業研究人員稱之為“混淆因素”，稍後會再談到混淆因素的處理方式。\n藉著以上的例子，請同學想想看合理的實驗研究應該是什麼樣子。這個例子的研究者面對的課題是，吸菸者和不吸菸者在許多方面可能完全不相同。最合理但不可行的解決方法是，只要研究者沒有道德約束，就可以控制誰吸菸誰不吸菸。具體的作法是，研究者只要將年輕的非吸菸者隨機分成兩組，強迫其中一組吸菸，那麼兩組在吸菸與否以外的任何條件可能都不會有太大差別。如此一來，研究結果真的顯示吸菸組得到癌症的比率確實高於不吸菸組，研究者可以相當有信心地提出兩個結論：(a)吸菸確實會導致癌症，(b)我們是凶手。\n\n\n2.5.2 非實驗研究\n非實驗研究泛指“任何研究者無法依研究目的控制測量條件的研究”。科學研究者顯然喜歡能控制的研究條件，但是正如前一節討論的案例，現實世界有許多狀況是無法或不應該控制的研究條件。為了確定人類是否會得癌症而強迫人吸菸，是完全沒有道德正確當性(幾乎可以肯定是種犯罪)，這個案例的情況充分說明什麼是研究者不該為了獲得良好的實驗結果，嘗試控制研究條件的底線。除了道德問題，“吸菸實驗”也存在其他問題。例如，所謂“強迫”一半參與者成為吸菸者時，我是指要求無吸菸習慣的參與者，強迫他們成為吸菸者。雖然這聽起來像是瘋狂科學家會喜歡的那種硬核又充滿惡意的實驗設計，但是調查真實世界的影響時，這可能不是一種合理的方法。例如，假設只有飲食不規律的人們吸菸才會導致肺癌，並且經常吸菸的人確實有不規律的飲食習慣。由於以上案例中討論的“吸菸者”，並不是“自然養成的”吸菸者，而是研究者強迫非吸菸者成為吸菸者，但這些人的生活習慣較一般吸菸者規律，可能有較好的飲食習慣。因此，在這個胡鬧的虛擬案例裡，這些人並不會因為吸菸而得到肺癌，這樣的實驗終將失敗，因為整個實驗的設計違反“自然”世界的規律(技術上這叫做“人為”結果)。\n非實驗研究可以區分為兩種類型:類實驗研究(quasi-experimental research)和個案研究(case studies)。如同吸菸致癌的研究案例，研究者 試圖不控制誰吸煙或誰不吸煙的條件，檢查吸煙者和不吸煙者的肺癌發生率，就是一種類實驗設計。也就是說，基本設計與實驗研究相同，但是研究者不控制預測變項。我們仍然可以使用統計方法分析結果，但是必須更加小心謹慎。\n個案研究則是儘可能詳細描述一個或少數個案。一般來說，任何統計方法都無法分析個案研究的結果，並且通常很難從少數獨立的個案資料，歸納出任何適用“一般人”的通用結論。不過在某些情況，個案研究是有價值的，特別是研究者別無選擇的某些情況。像是神經心理學經常遇到這樣的問題。有時候研究者根本找不到好幾位特定大腦區域有損傷的傷患，所以唯一能做的就是盡可能詳細且謹慎地描述確實有特定損傷的個案狀況。此外，個案研究也確實有一些真正的優點。若是研究者沒有太多研究對象，但是有能力投入大量時間和精力，仔細調查影響每個案例表現的特定因素，就是一件非常有價值的事。因此，個案研究可以補充依賴統計方法檢驗結果的實驗和類實驗設計，無法得出的看法。雖然本書不會細談個案研究，這種研究方法的也可能是你所需要的!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#研究的效度",
    "href": "02-A-brief-introduction-to-research-design.html#研究的效度",
    "title": "2  研究設計入門",
    "section": "2.6 研究的效度",
    "text": "2.6 研究的效度\n\n除了信度和良好的研究條件控制，科學研究者最希望研究是“有效的”。檢視一項研究有沒有效度非常簡單，只要問研究結果可不可以被相信？如果不能，那麼這項研究就是無效的。然而，雖然講如何讓研究具有效度容易，實際上檢驗效度要比檢驗信度困難得多。老實說，什麼樣的研究結果是有效度的，並沒有精確清晰、研究者一致認同的概念。至今為止，已經出現很多種效度指標，每一種指標都是為了處理某個層面的問題，但是並非所有效度指標是任何研究都要符合的。以下討論五種效度指標：\n\n內在效度(Internal validity)\n外在效度(External validity)\n建構效度(Construct validity)\n表面效度(Face validity)\n生態效度(Ecological validity)\n\n這裡提供一個快速指南，說明每項指標的重點。(1)內在效度和外在效度是最基本的，因為兩者都是評估研究結果是否真正有效回答主要的研究問題。(2)建構效度的評估重點是測量結果確實是研究者想要測量的對象。(3)表面效度不是太重要的指標，除非研究者在意測量結果的“外觀”。(4)生態效度是一種特殊的表面效度，特別指研究者非常在意的某種“外觀”。\n\n2.6.1 內在效度\n內在效度是指研究者在多大程度上，能正確推論變項之間的因果關係的結論。之所以稱為“內在效度”，是因為特指的是研究“內部”實體或抽象條件之間的關係。我們用一個簡單的例子來說明：假設研究者有興趣了解大學教育能否提高學生的寫作能力，研究者可以找幾位大一學生，指派每個人寫一篇1000字的文章，然後統計每篇文章裡的錯字和錯誤文法。接著再找幾位已經接受較長大學教育的大三學生，每個人也寫篇文章。研究者可以假設，大三學生的錯字和文法錯誤較大一學生少。結果符合假設的話，似乎就能推論大學教育可以提高寫作技巧？然而，這種實驗的最大問題在於，大三學生也更年長，有更多寫作經驗，所以很難確定教育是真正的原因。年長的人寫作能力更好嗎？還是個人寫作經驗？ 還是一個人接受過更多教育？大三學生表現更好的真正原因是什麼？年齡？經驗？教育？我們無法從這樣的研究結果區辨出來。這是內部效度失效的一個例子，因為這樣的研究沒有充分拆解不同變項之間的因果關係。\n\n\n2.6.2 外在效度\n外在效度關乎研究結果的普遍性(generalisability)或可用性(applicability)。這個指標看重的是，在“現實生活”中觀察到的模式與研究所觀察到的模式之相似程度。更精確地說，心理學研究者進行的任何研究都會涉及發生在特定環境的具體問題，或要表現的行為，而且通常會向特殊群體招募參與者(遺憾的是經常是大學生!)。所以，如果研究結果無法推廣或應用於研究條件之外的人和實際狀況，那麼這樣的研究就會有外部效度不足的問題。\n大部分心理學研究只會招募研究者任職機構的學生擔任參與者，是外部效度不足的典型例子。然而，研究者的研究通常不僅是關注機構內的學生，更是關注全體人類。就這一點來說，只招募心理系學生作為參與者的研究，都存在外在效度不足的風險。也就是說，如果心理學系學生在某些條件“與眾不同”，就要擔心缺乏外在效度。\n即使如此，心理學研究者也必須要意識到，只用心理系學生的研究也不完全必然存在外在效度問題。稍後會再談到這個問題，不過這是一個現在所有心理學者都會擔心的大問題，所以先在此說明。若是一項研究具有這兩個條件，特殊的參與者群體會對研究的外在效度構成威脅：(a)研究取樣的群體範圍狹窄(例如只找心理系學生);(b)研究取樣的狹窄群體在某些研究所關注的心理特微，與一般人群有系統性的差異。斜體字是研究者常忽略的部分。心理系學生確實有許多“與眾不同”的地方，所以只有心理系學生擔任參與者的研究可能存在外在效度問題。然而，如果這些差異與研究關注的目標關係不大，那麼就沒有什麼可擔心的。為了清楚說明這一點，這裡提供兩個極端的例子:\n\n研究者想評估“大眾對心理治療的態度”，但是所有參與者都是心理系學生。這種研究幾乎肯定存在外在效度問題。\n研究者想測量視覺錯覺的有效性，參與者全部都是心理系學生。這樣的研究不太可能存在外在效度問題。\n\n這裡用了幾段文字來討論選擇參與者群體的問題，因為這是所有行為科學研究者都要擔心的大問題，所以務必理解外在效度是一種更廣泛的概念。根據研究的類型，以下的例子也可能對外在效度構成威脅:\n\n參與者可能採取不同於現實生活的方式回答“心理學問卷”。\n根據實驗室研究歸納“人類學習”的實驗結果，整理出來的規律可能不同於一般人在現實生活中面臨的學習問題。\n\n\n\n2.6.3 建構效度\n建構效度的基本概念是評量研究者使用的測量工具是不是真正有效。如果一種測量的條件符合理論設定，可以說這樣的測量工具有良好的建構效度；反之如果條件不符合理論設定，則建構效度不佳。舉一個非常簡單(但是荒謬)的例子，假設有位大學老師試圖調查他的學生在期末考試作弊的比率，測量的方法是讓有作弊的學生考試現場站立，讓老師能計算人數。若是在一堂有300名學生的課堂上這麼做，不會有人公開自己有作弊。那麼這為老師能判斷，這門課作弊的學生比例是0%？這顯然有些荒謬。但是舉這個例子不是要談這種作法有多荒謬，而是說明什麼是建構效度。這位老師的測量方法問題在於，雖然他試圖測量“作弊率”，實際上只有找到“愚蠢到自己承認作弊的人”，或是“自目到假裝自己作弊的人”。很顯然，這兩種人不是這位老師想找出的目標!所以這項研究出錯了，因為這樣的測量方式建構效度非常差。\n\n\n2.6.4 表面效度\n表面效度只注著評估測量工具是否“看起來”正常發揮。如果有研究者設計一份智力測試，其他人看了說“不，這個測試並不能測量智力”，那麼這份測試就缺乏表面效度，評估表面效度就是這麼簡單。不過從嚴格的科學操作來看，表面效度顯然沒有太高價值。畢竟，研究者關注的是這樣的測量工具能否真正發揮出最初設計的功能，而不只是有沒有看起來運作正常。在研究實務，表面效度通常是研究者最不在意的。即使如此，評估表面效度在研究實務具有三種作用:\n\n有經驗的科學家有時候會有一種“直覺”，認為某種測量方式無法起作用。雖然這種直覺在嚴格的科學意義來說，並沒有證據價值，但是聽從這類建議通常是值得的。因為人類通常具有難以用言語表達的智慧，即使說不出為什麼，也可能存在值得擔憂的地方。換句話說，有經驗的研究者聽到所信任的人士批評進行中的研究缺乏表面效度，都會花點時間更仔細地考慮研究設計，找找看是否存在可能出錯的原因。當然，如果找不到任何值得擔憂的原因，那麼可能就不必擔心。畢竟，表面效度其實並不太重要。\n完全不明白你的研究在做什麼的人，通常也會有一種你的研究很爛的“直覺”。並且他們會在網路上或其他公開場合發表批評意見。仔細檢視他們的意見，很可能會注意到這些批判其實都是在計較研究的“表面”，而不是更深層的東西。洞察表面效度，能讓研究者用溫和的方式，向其他人解釋他們需要進一步說明自己的論點。\n接續前一點，如果非專業人士的看法有重要作用(例如，應用研究通常要說服有決策權的人士贊同研究成果)，那麼研究者就必須關心表面效度。只因為關鍵人士喜歡一項研究，表面效度就被很多研究者當成重要的效度替代指標。想想看研究者希望政府能依據科學心理學理由條改某條法律，那麼研究的“真實”品質並不是最重要。如果研究內容缺乏表面效度，立法者是不會理睬研究者的意見。當然，立定政策通常依賴表面而非事實是有點不公平的，但真實狀況就是如此。\n\n\n\n2.6.5 生態效度\n生態效度是另一種類似外在效度的效度指標，但是重要性較低。研究應該有生態效度的想法是，研究條件的設定應該儘可能合乎所要調查的現實世界場景。就某方面來說，生態效度是一種表面效度。也是就研究的表面“看起來”是否正確地測量要測的對象，不過更為嚴謹。為了有起碼的生態效度，評估一份研究有沒有“看起來”正確，必須根據相當具體的條件。這背後的觀念是，具備生態效度的研究可能有更好的外在效度。當然，這並不是保證。但生態效度的好處在於，與檢驗一項研究是否具有外在效度相比，檢驗是否具有生態效度要容易得多。一個簡單的例子是辨認目擊者的研究。這些研究中大多是在大學校園裡進行，通常參與者要看的臉孔數量較少，而不像警察安排的指認程序。參與者目擊“罪犯”現場，到進行指認程序找出嫌疑犯，場景之間的時間差比實際狀況更短。研究的“犯罪”現場不是真實發生的，所以目擊者不可能感到害怕，也沒有警察在場，所以感到壓力也可能小得多。這些條件設定意味著這樣的研究肯定缺乏生態效度 ，也可能(但可能不會)缺乏外在效度。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#本章小結",
    "href": "02-A-brief-introduction-to-research-design.html#本章小結",
    "title": "2  研究設計入門",
    "section": "2.8 本章小結",
    "text": "2.8 本章小結\n\n這一章的目的並不是要全面討論心理研究方法。要充分討論這個話題至少需要另外一本同樣長的書。然而，在現實生活中，統計和研究設計緊密交織在一起，所以討論一些關鍵話題非常方便。在這一章中，我簡要討論了以下內容:\n\n認識心理測量。操作化理論構想意味著什麼?變項和測量的意義是什麼?\n測量尺度和變項類型。請記住這裡有兩個不同的區分。一是離散資料和連續資料之間的區別，二是四種不同尺度類型(名義、序數、間隔和比率)之間的區別。\n測量的信度。如果我“同樣”測量兩次某事物，應該期望看到相同的結果嗎?只有當我的測量是可靠的時候。但是“同樣”的事物意味著什麼?這就是為什麼我們有不同類型的可靠性。請確保您記住了它們。\n變項的”角色”: 預測變項與結果變項。變項在分析中扮演什麼角色?您還記得預測變項和結果變項之間的區別嗎?依變項和自變項之間的區別等等。\n實驗與非實驗研究設計。是什麼讓一項實驗成為實驗?是好看的白色實驗袍，還是與研究人員對變項的控制有關?\n研究的效度。您的研究是否測量了您希望它測量的內容?可能會出什麼問題?我看錯了嗎，還是可能出錯的方式的列表非常長?\n\n所有這些都應該讓您明白，研究設計是研究方法的一個關鍵部分。我以 Campbell & Stanley (1963) 的經典小書為基礎構建了這一章，但當然還有很多研究設計教科書。在您喜歡的搜索引擎上花幾分鐘，您就會找到幾十本。\n\n\n\n\nAdair, G. (1984). The hawthorne effect: A reconsideration of the methodological artifact. Journal of Applied Psychology, 69, 334–345.\n\n\nCampbell, D. T., & Stanley, J. C. (1963). Experimental and quasi-experimental designs for research. Houghton Mifflin.\n\n\nGelman, A., & Loken, E. (2014). The statistical crisis in science. American Scientist, 102(6), 460+. https://doi.org/10.1511/2014.111.460\n\n\nHróbjartsson, A., & Gøtzsche, P. (2010). Placebo interventions for all clinical conditions. Cochrane Database of Systematic Reviews, 1. https://doi.org//10.1002/14651858.CD003974.pub3\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Med, 2(8), 697–701.\n\n\nKahneman, D., & Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80, 237–251.\n\n\nKühberger, A., Fritz, A., & Scherndl, T. (2014). Publication bias in psychology: A diagnosis based on the correlation between effect size and sample size. Public Library of Science One, 9, 1–8.\n\n\nPfungst, O. (1911). Clever hans (the horse of mr. Von osten): A contribution to experimental animal and human psychology (C. L. Rahn, Trans.). Henry Holt.\n\n\nRosenthal, R. (1966). Experimenter effects in behavioral research. Appleton.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103, 677–680."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html",
    "href": "03-Getting-started-with-jamovi.html",
    "title": "3  Getting started with jamovi",
    "section": "",
    "text": "Robots are nice to work with.\n– Roger Zelazny1\nIn this chapter I’ll discuss how to get started in jamovi. I’ll briefly talk about how to download and install jamovi, but most of the chapter will be focused on getting you started with finding your way around the jamovi GUI. Our goal in this chapter is not to learn any statistical concepts: we’re just trying to learn the basics of how jamovi works and get comfortable interacting with the system. To do this we’ll spend a bit of time looking at datasets and variables. In doing so, you’ll get a bit of a feel for what it’s like to work in jamovi.\nHowever, before going into any of the specifics, it’s worth talking a little about why you might want to use jamovi at all. Given that you’re reading this you’ve probably got your own reasons. However, if those reasons are “because that’s what my stats class uses”, it might be worth explaining a little why your lecturer has chosen to use jamovi for the class. Of course, I don’t really know why other people choose jamovi so I’m really talking about why I use it.\nThose are the main reasons I use jamovi. It’s not without its flaws, though. It’s relatively new2 so there is not a huge set of textbooks and other resources to support it, and it has a few annoying quirks that we’re all pretty much stuck with, but on the whole I think the strengths outweigh the weakness; more so than any other option I’ve encountered so far."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#installing-jamovi",
    "href": "03-Getting-started-with-jamovi.html#installing-jamovi",
    "title": "3  Getting started with jamovi",
    "section": "3.1 Installing jamovi",
    "text": "3.1 Installing jamovi\nOkay, enough with the sales pitch. Let’s get started. Just as with any piece of software, jamovi needs to be installed on a “computer”, which is a magical box that does cool things and delivers free ponies. Or something along those lines; I may be confusing computers with the iPad marketing campaigns. Anyway, jamovi is freely distributed online and you can download it from the jamovi homepage, which is: https://www.jamovi.org/\nAt the top of the page, under the heading “Download”, you’ll see separate links for Windows users, Mac users, and Linux users. If you follow the relevant link you’ll see that the online instructions are pretty self-explanatory. As of this writing, the current version of jamovi is 2.3, but they usually issue updates every few months, so you’ll probably have a newer version.3\n\n3.1.1 Starting up jamovi\nOne way or another, regardless of what operating system you’re using, it’s time to open jamovi and get started. When first starting jamovi you will be presented with a user interface which looks something like Figure 3.1.\n\n\n\n\n\nFigure 3.1: jamovi starts up!\n\n\n\n\nTo the left is the spreadsheet view, and to the right is where the results of statistical tests appear. Down the middle is a bar separating these two regions and this can be dragged to the left or the right to change their sizes.\nIt is possible to simply begin typing values into the jamovi spreadsheet as you would in any other spreadsheet software. Alternatively, existing data sets in the CSV (.csv) file format can be opened in jamovi. Additionally, you can easily import SPSS, SAS, Stata and JASP files directly into jamovi. To open a file select the File tab (three horizontal lines signify this tab) at the top left hand corner, select ‘Open’ and then choose from the files listed on ‘Browse’ depending on whether you want to open an example or a file stored on your computer."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#analyses",
    "href": "03-Getting-started-with-jamovi.html#analyses",
    "title": "3  Getting started with jamovi",
    "section": "3.2 Analyses",
    "text": "3.2 Analyses\nAnalyses can be selected from the analysis ribbon or menu along the top. Selecting an analysis will present an ‘options panel’ for that particular analysis, allowing you to assign different variables to different parts of the analysis, and select different options. At the same time, the results for the analysis will appear in the right ‘Results panel’ and will update in real-time as you make changes to the options.\nWhen you have the analysis set up correctly you can dismiss the analysis options by clicking the arrow to the top right of the optional panel. If you wish to return to these options, you can click on the results that were produced. In this way, you can return to any analysis that you (or say, a colleague) created earlier.\nIf you decide you no longer need a particular analysis, you can remove it with the results context menu. Right-clicking on the analysis results will bring up a menu and by selecting ‘Analysis’ and then ‘Remove’ the analysis can be removed. But more on this later. First, let’s take a more detailed look at the spreadsheet view."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#the-spreadsheet",
    "href": "03-Getting-started-with-jamovi.html#the-spreadsheet",
    "title": "3  Getting started with jamovi",
    "section": "3.3 The spreadsheet",
    "text": "3.3 The spreadsheet\nIn jamovi data is represented in a spreadsheet with each column representing a ‘variable’ and each row representing a ‘case’ or ‘participant’.\n\n3.3.1 Variables\nThe most commonly used variables in jamovi are ‘Data Variables’, these variables simply contain data either loaded from a data file, or ‘typed in’ by the user. Data variables can be one of several measurement levels (Figure 3.2).\n\n\n\n\n\nFigure 3.2: measurement levels\n\n\n\n\nThese levels are designated by the symbol in the header of the variable’s column. The ID variable type is unique to jamovi. It’s intended for variables that contain identifiers that you would almost never want to analyse. For example, a persons name, or a participant ID. Specifying an ID variable type can improve performance when interacting with very large data sets.\nNominal variables are for categorical variables which are text labels, for example a column called Gender with the values Male and Female would be nominal. So would a person’s name. Nominal variable values can also have a numeric value. These variables are used most often when importing data which codes values with numbers rather than text. For example, a column in a dataset may contain the values 1 for males, and 2 for females. It is possible to add nice ‘human-readable’ labels to these values with the variable editor (more on this later).\nOrdinal variables are like Nominal variables, except the values have a specific order. An example is a Likert scale with 3 being ‘strongly agree’ and -3 being ‘strongly disagree’.\nContinuous variables are variables which exist on a continuous scale. Examples might be height or weight. This is also referred to as ‘Interval’ or ‘Ratio scale’.\nIn addition, you can also specify different data types: variables have a data type of either ‘Text’, ‘Integer’ or ‘Decimal’.\nWhen starting with a blank spreadsheet and typing values in the variable type will change automatically depending on the data you enter. This is a good way to get a feel for which variable types go with which sorts of data. Similarly, when opening a data file jamovi will try and guess the variable type from the data in each column. In both cases this automatic approach may not be correct, and it may be necessary to manually specify the variable type with the variable editor.\nThe variable editor can be opened by selecting ‘Setup’ from the data tab or by double-clicking on the variable column header. The variable editor allows you to change the name of the variable and, for data variables, the variable type, the order of the levels, and the label displayed for each level. Changes can be applied by clicking the ‘tick’ to the top right. The variable editor can be dismissed by clicking the ‘Hide’ arrow.\nNew variables can be inserted or appended to the data set using the ‘add’ button from the data ribbon. The ‘add’ button also allows the addition of computed variables.\n\n\n3.3.2 Computed variables\nComputed Variables are those which take their value by performing a computation on other variables. Computed Variables can be used for a range of purposes, including log transforms, z-scores, sum-scores, negative scoring and means.\nComputed variables can be added to the data set with the ‘add’ button available on the data tab. This will produce a formula box where you can specify the formula. The usual arithmetic operators are available. Some examples of formulas are:\nA + B\nLOG10(len)\nMEAN(A, B)\n(len - VMEAN(len)) / VSTDEV(len)\nIn order, these are the sum of A and B, a log (base 10) transform of len, the mean of A and B, and the z-score of the variable len4. Figure 3.3 shows the jamovi screen for the new variable computed as the z-score of len (from the ‘Tooth Growth’ example data set).\n\n\n\n\n\nFigure 3.3: A newly computed variable, the z-score of ‘dose’\n\n\n\n\n\n3.3.2.1 V-functions\nSeveral functions are already available in jamovi and available from the drop down box labelled fx. A number of functions appear in pairs, one prefixed with a V and the other not. V functions perform their calculation on a variable as a whole, where as non-V functions perform their calculation row by row. For example, MEAN(A, B) will produce the mean of A and B for each row. Where as VMEAN(A) gives the mean of all the values in A.\n\n\n\n3.3.3 Copy and Paste\njamovi produces nice American Psychological Association (APA) formatted tables and attractive plots. It is often useful to be able to copy and paste these, perhaps into a Word document, or into an email to a colleague. To copy results right click on the object of interest and from the menu select exactly what you want to copy. The menu allows you to choose to copy only the image or the entire analysis. Selecting “copy” copies the content to the clipboard and this can be pasted into other programs in the usual way. You can practice this later on when we do some analyses.\n\n\n3.3.4 Syntax mode\njamovi also provides an “R Syntax Mode”. In this mode jamovi produces equivalent R code for each analysis. To change to syntax mode, select the Application menu to the top right of jamovi (a button with three vertical dots) and click the “Syntax mode” checkbox there. You can turn off syntax mode by clicking this a second time.\nIn syntax mode analyses continue to operate as before but now they produce R syntax, and ‘ascii output’ like an R session. Like all results objects in jamovi, you can right click on these items (including the R syntax) and copy and paste them, for example into an R session. At present, the provided R syntax does not include the data import step and so this must be performed manually in R. There are many resources explaining how to import data into R and if you are interested we recommend you take a look at these; just search on the interweb."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#loading-data-in-jamovi",
    "href": "03-Getting-started-with-jamovi.html#loading-data-in-jamovi",
    "title": "3  Getting started with jamovi",
    "section": "3.4 Loading data in jamovi",
    "text": "3.4 Loading data in jamovi\nThere are several different types of files that are likely to be relevant to us when doing data analysis. There are two in particular that are especially important from the perspective of this book:\n\njamovi files are those with a .omv file extension. This is the standard kind of file that jamovi uses to store data, and variables and analyses.\nComma separated value (csv) files are those with a .csv file extension. These are just regular old text files and they can be opened with many different software programs. It’s quite typical for people to store data in csv files, precisely because they’re so simple.\n\nThere are also several other kinds of data file that you might want to import into jamovi. For instance, you might want to open Microsoft Excel spreadsheets (.xls files), or data files that have been saved in the native file formats for other statistics software, such as SPSS or SAS. Whichever file formats you are using, it’s a good idea to create a folder or folders especially for your jamovi data sets and analyses and to make sure you keep these backed up regularly.\n\n3.4.1 Importing data from csv files\nOne quite commonly used data format is the humble “comma separated value” file, also called a csv file, and usually bearing the file extension .csv. csv files are just plain old-fashioned text files and what they store is basically just a table of data. This is illustrated in Figure 3.4, which shows a file called booksales.csv that I’ve created. As you can see, each row represents the book sales data for one month. The first row doesn’t contain actual data though, it has the names of the variables.\n\n\n\n\n\nFigure 3.4: The booksales.csv data file. On the left I have opened the file using a spreadsheet program (OpenOffice), which shows that the file is basically a table. On the right the same file is open in a standard text editor (the TextEdit program on a Mac), which shows how the file is formatted. The entries in the table are wrapped in quote marks and separated by commas\n\n\n\n\nIt’s easy to open csv files in jamovi. From the top left menu (the button with three parallel lines) choose ‘Open’ and browse to where you have stored the csv file on your computer. If you’re on a Mac, it’ll look like the usual Finder window that you use to choose a file; on Windows it looks like an Explorer window. An example of what it looks like on a Mac is shown in Figure 3.5. I’m assuming that you’re familiar with your own computer, so you should have no problem finding the csv file that you want to import! Find the one you want, then click on the “Open” button.\n\n\n\n\n\nFigure 3.5: A dialog box on a Mac asking you to select the csv file jamovi should try to import. Mac users will recognise this immediately, it’s the usual way in which a Mac asks you to find a file. Windows users won’t see this, instead they’ll see the usual explorer window that Windows always gives you when it wants you to select a file\n\n\n\n\nThere are a few things that you can check to make sure that the data gets imported correctly:\n\nHeading. Does the first row of the file contain the names for each variable - a ‘header’ row? The booksales.csv file has a header, so that’s a yes.\nDecimal. What character is used to specify the decimal point? In English speaking countries this is almost always a period (i.e., .). That’s not universally true though, many European countries use a comma.\nQuote. What character is used to denote a block of text? That’s usually going to be a double quote mark (“). It is for the booksales.csv file."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#importing-unusual-data-files",
    "href": "03-Getting-started-with-jamovi.html#importing-unusual-data-files",
    "title": "3  Getting started with jamovi",
    "section": "3.5 Importing unusual data files",
    "text": "3.5 Importing unusual data files\nThroughout this book I’ve assumed that your data are stored as a jamovi .omv file or as a “properly” formatted csv file. However, in real life that’s not a terribly plausible assumption to make so I’d better talk about some of the other possibilities that you might run into.\n\n3.5.1 Loading data from text files\nThe first thing I should point out is that if your data are saved as a text file but aren’t quite in the proper csv format then there’s still a pretty good chance that jamovi will be able to open it. You just need to try it and see if it works. Sometimes though you will need to change some of the formatting. The ones that I’ve often found myself needing to change are:\n\nheader. A lot of the time when you’re storing data as a csv file the first row actually contains the column names and not data. If that’s not true then it’s a good idea to open up the csv file in a spreadsheet programme such as Open Office and add the header row manually.\nsep. As the name “comma separated value” indicates, the values in a row of a csv file are usually separated by commas. This isn’t universal, however. In Europe the decimal point is typically written as , instead of . and as a consequence it would be somewhat awkward to use , as the separator. Therefore it is not unusual to use ; instead of , as the separator. At other times, I’ve seen a TAB character used.\nquote. It’s conventional in csv files to include a quoting character for textual data. As you can see by looking at the booksales.csv file, this is usually a double quote character, “. But sometimes there is no quoting character at all, or you might see a single quote mark ’ used instead.\nskip. It’s actually very common to receive CSV files in which the first few rows have nothing to do with the actual data. Instead, they provide a human readable summary of where the data came from, or maybe they include some technical info that doesn’t relate to the data.\nmissing values. Often you’ll get given data with missing values. For one reason or another, some entries in the table are missing. The data file needs to include a “special” value to indicate that the entry is missing. By default jamovi assumes that this value is 995, for both numeric and text data, so you should make sure that, where necessary, all missing values in the csv file are replaced with 99 (or -9999; whichever you choose) before opening / importing the file into jamovi. Once you have opened / imported the file into jamovi all the missing values are converted to blank or greyed out cells in the jamovi spreadsheet view. You can also change the missing value for each variable as an option in the Data - Setup view.\n\n\n\n3.5.2 Loading data from SPSS (and other statistics packages)\nThe commands listed above are the main ones we’ll need for data files in this book. But in real life we have many more possibilities. For example, you might want to read data files in from other statistics programs. Since SPSS is probably the most widely used statistics package in psychology, it’s worth mentioning that jamovi can also import SPSS data files (file extension .sav). Just follow the instructions above for how to open a csv file, but this time navigate to the .sav file you want to import. For SPSS files, jamovi will regard all values as missing if they are regarded as “system missing” files in SPSS. The ‘Default missings’ value does not seem to work as expected when importing SPSS files, so be aware of this - you might need another step: import the SPSS file into jamovi, then export as a csv file before re-opening in jamovi.6\nAnd that’s pretty much it, at least as far as SPSS goes. As far as other statistical software goes, jamovi can also directly open / import SAS and STATA files.\n\n\n3.5.3 Loading Excel files\nA different problem is posed by Excel files. Despite years of yelling at people for sending data to me encoded in a proprietary data format, I get sent a lot of Excel files. The way to handle Excel files is to open them up first in Excel or another spreadsheet programme that can handle Excel files, and then export the data as a csv file before opening / importing the csv file into jamovi."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#sec-Changing-data-from-one-level-to-another",
    "href": "03-Getting-started-with-jamovi.html#sec-Changing-data-from-one-level-to-another",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.6 更動資料變項性質",
    "text": "3.6 更動資料變項性質\n為了方便排序數值，或是匯入的資料型態不正確，你會想要手動改變變項級別。像是匯入後的數字被判定為名義變項；日期被當成一般文字；受測者編號被當成連續變項。這些時候你會想把變項型態壓製成真正的樣子。\n這時請擅用”測量級別設定面板” (參考 圖 3.2 )，改變“Measurement Type”的選項，調整變項為你要的尺度。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#installing-add-on-modules-into-jamovi",
    "href": "03-Getting-started-with-jamovi.html#installing-add-on-modules-into-jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.7 Installing add-on modules into jamovi",
    "text": "3.7 Installing add-on modules into jamovi\nA really great feature of jamovi is the ability to install add-on modules from the jamovi library. These add-on modules have been developed by the jamovi community, i.e., jamovi users and developers who have created special software add-ons that do other, usually more advanced, analyses that go beyond the capabilities of the base jamovi program.\nTo install add-on modules, just click on the large \\(+\\) in the top right of the jamovi window, select “jamovi-library” and then browse through the various add-on modules that are available. Choose the one(s) you want, and then install them, as in Figure 3.6. It’s that easy. The newly installed modules can then be accessed from the “Analyses” button bar. Try it…useful add-on modules to install include “scatr” (added under “Descriptives”) and \\(R_j\\).\n\n\n\n\n\nFigure 3.6: Installing add-on modules in jamovi"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#quitting-jamovi",
    "href": "03-Getting-started-with-jamovi.html#quitting-jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.8 Quitting jamovi",
    "text": "3.8 Quitting jamovi\nThere’s one last thing I should cover in this chapter: how to quit jamovi. It’s not hard, just close the program the same way you would any other program. However, what you might want to do before you quit is save your work! There are two parts to this: saving any changes to the data set, and saving the analyses that you ran.\nIt is good practice to save any changes to the data set as a new data set. That way you can always go back to the original data. To save any changes in jamovi, select ‘Export’…‘Data’ from the main jamovi menu (button with three horizontal bars in the top left) and create a new file name for the changed data set.\nAlternatively, you can save both the changed data and any analyses you have undertaken by saving as a jamovi file. To do this, from the main jamovi menu select ‘Save as’ and type in a file name for this ‘jamovi file (.omv)’. Remember to save the file in a location where you can find it again later. I usually create a new folder for specific data sets and analyses."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#summary",
    "href": "03-Getting-started-with-jamovi.html#summary",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nEvery book that tries to teach a new statistical software program to novices has to cover roughly the same topics, and in roughly the same order. Ours is no exception, and so in the grand tradition of doing it just the same way everyone else did it, this chapter covered the following topics:\n\n[Installing jamovi]. We downloaded and installed jamovi, and started it up.\n[Analyses]. We very briefly oriented to the part of jamovi where analyses are done and results appear, but then deferred this until later in the book.\n[The spreadsheet]. We spent more time looking at the spreadsheet part of jamovi, and considered different variable types, and how to compute new variables.\n[Loading data in jamovi]. We also saw how to load data files in jamovi.\n[Importing unusual data files]. Then we figured out how to open other data files, from different file types.\nChanging data from one level to another. And saw that sometimes we need to coerce data from one type to another.\nInstalling add-on modules into jamovi. Installing add-on modules from the jamovi community really extends jamovi capabilities.\nQuitting jamovi. Finally, we looked at good practice in terms of saving your data set and analyses when you have finished and are about to quit jamovi.\n\nWe still haven’t arrived at anything that resembles data analysis. Maybe the next Chapter will get us a bit closer!"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#混淆人為結果以及各種降低效度的因素",
    "href": "02-A-brief-introduction-to-research-design.html#混淆人為結果以及各種降低效度的因素",
    "title": "2  研究設計入門",
    "section": "2.7 混淆、人為結果、以及各種降低效度的因素",
    "text": "2.7 混淆、人為結果、以及各種降低效度的因素\n\n(以下為AI初翻，尚待校稿)\n如果我們以最廣泛的方式看待效度問題，我們最擔心的兩個大問題是混淆因素和人為結果。這兩個術語的定義如下:\n\n混淆因素:混淆因素是額外的、通常未測量的變項13，最終與預測變項和結果變項都相關。混淆因素的存在會威脅研究的內在效度，因為您無法確定是預測變項導致結果，還是混淆變項導致結果。\n人為結果:如果研究結果僅在您測試的特殊情況下成立，則稱結果為“人為的”。您的結果可能是人為的，這描述了對您的外在效度的威脅，因為它提出了您無法將結果推廣或應用到您所關心的實際人群的可能性。\n\n總的來說，混淆因素更多地使非實驗研究感到擔憂，正是因為它們不是適當的實驗。按定義，您將許多事情都未受控制，所以您的研究中存在大量混淆因素的可能性。實驗研究往往更不容易受到混淆因素的影響。您對研究過程的控制程度越高，就越能防止混淆因素影響結果。例如，通過隨機分配，混淆因素在不同組之間隨機且均勻地分佈。\n但是，萬事都有利弊。當我們開始考慮人為結果而不是混淆因素時，情況剛好完全相反。人為結果往往更使實驗研究而不是非實驗研究感到擔憂。為了理解這一點，認識到許多研究之所以是非實驗的，正是因為研究人員正試圖在更自然的環境中檢查人類行為。通過在更加真實的環境中操作，您會失去實驗控制(使您容易受到混淆因素的影響)，但因為您傾向於在“野外”學習人類心理學，所以您減少了獲得人為結果的可能性。或者，以另一種方式說，當您將心理學從野外帶入實驗室(我們通常必須這樣做以獲得實驗控制)時，您總是有意外研究與您想要研究的內容不同的風險。\n但是，以上只是一個大致的指南。實驗中絕對有可能出現混淆因素，並且非實驗研究也可能會產生人為結果。由於各種原因，這種情况可能會發生，其中不 least 的是研究人員或研究人員的错误。在實踐中，提前考慮所有事情真的很困難，即使非常出色的研究人員也會犯錯誤。\n儘管幾乎可以將任何威脅效度的事情表述為混淆因素或人為結果，但它們是相當模糊的概念。所以，讓我們看一下一些最常見的示例。\n\n2.7.1 歷史效應\n歷史效應指的是研究過程中可能發生特定事件，從而可能影響結果測量的可能性。例如，在預測結果和事後結果之間可能會發生一些事情。或者在測試第23位參與者和第24位參與者之間。或者，您可能正在查看舊的完全有效的研究文獻，但自那以後世界已經發生了足夠的變化，以致結論不再值得信賴。屬於歷史效應的事情示例包括:\n\n您對人們如何看待風險和不確定性感興趣。您於2010年12月開始收集數據。但是找到參與者和收集數據需要時間，所以您在2011年2月仍在尋找新的人。不幸的是(對您和其他人來說)，2011年1月昆士蘭洪水導致數十億美元的損失和許多人死亡。自然地，2011年2月測試的人對處理風險的信念與2010年12月測試的人有很大不同。這其中(如果有的話)哪個反映了參與者的“真實”信念?我認為答案可能都是。昆士蘭洪水確實改變了澳大利亞公眾的信念，儘管可能只是暫時的。這裡的關鍵是，2月受測者的“歷史”與12月受測者的情況截然不同。\n您正在測試一種新的抗焦慮藥物的心理影響。所以，在給藥前，您通過自我報告和生理測量來測量焦慮。然後您給藥，事後採取相同的測量。但是，在此過程中，因為您的實驗室在洛杉磯，發生了地震，增加了參與者的焦慮。\n\n\n\n2.7.2 發展成熟效應\n與歷史效應一樣，發展成熟效應根本上與時間的變化有關。然而，成熟效應不是對特定事件的反應。相反，它們與人們隨時間自行變化有關。我們會變老、疲憊、厭倦等。成熟效應的一些示例包括:\n\n在進行發展心理學研究時，您需要意識到兒童成長得相當迅速。因此，假設您想弄清某些教育技巧是否可以幫助3歲兒童的詞彙量。您需要知道的一件事是，僅憑自己，這個年齡的兒童的詞彙量以難以置信的速度增長(每天多個詞)。如果在設計研究時沒有考慮這種成熟效應，那麼您將無法確定您的教育技巧是否有效。\n當在實驗室中運行非常長時間的實驗(比如3小時)時，人們很可能會開始感到厭倦和疲憊，並且這種成熟效應會導致表現下降，而與實驗中發生的任何其他事情無關。\n\n\n\n2.7.3 重覆測試效應\n歷史效應的一種重要類型是重覆測試效應。假設我想對某些心理構造(例如焦慮)進行兩次測量。我可能會擔心的是，第一次測量是否會對第二次測量產生影響。換句話說，這是一種歷史效應，其中影響第二次測量的“事件”本身就是第一次測量!這並不少見。這方面的示例包括:\n\n學習和實踐:例如，由於參與者在第一次測試期間學會了如何解決“智力測試類”問題的一般規則，所以第二次的“智力”可能看起來比第一次高。\n測試情景的熟悉度:例如，如果人們在第一次時感到緊張，這可能會使表現下降。但是在度過了第一次測試情況後，他們可能會變得很平靜，就是因為他們已經看到了測試的樣子。\n測試導致的輔助變化:例如，如果衡量情緒的問卷很枯燥，那麼第二次測量時報告的情緒更有可能是“厭倦”，確切地是由于第一次測量的枯燥。\n\n\n\n2.7.4 選擇偏誤\n選擇偏誤是一個相當廣泛的術語。假設您運行一項實驗，其中有兩組參與者，每組得到不同的“處理”，並且您想看看不同的處理是否會導致不同的結果。然而，假設儘管您已經盡了最大努力，但在兩組之間仍存在性別失衡(例如，A組為80%的女性和B組為50%的女性)。聽起來這可能永遠不會發生，但相信我，它會發生。這是選擇偏誤的一個例子，其中被“選擇進入”兩組的人具有不同的特點。如果這些特點中的任何一個被證明相關(例如，您的處理對女性的效果比對男性更好)，那麼您會遇到大麻煩。\n\n\n2.7.5 個體特質差異\n在考慮淘汰效應時，區分兩種不同類型有時很有幫助。第一種是同質淘汰，其中淘汰效應對所有組、處理或條件都是相同的。在我上面給出的示例中，如果(且僅當)容易感到厭倦的參與者以相似的速度從我的實驗中的所有條件中退出，則該淘汰將是同質的。通常，同質淘汰的主要影響可能是使您的樣本不代表性。因此，您最大的擔心是結果的普適性降低。換句話說，您失去了外在效度。\n第二種淘汰類型是異質淘汰，其中淘汰效應對不同組有所不同。更常被稱為差異淘汰，這是一種由研究本身引起的選擇偏誤。假設，在心理學歷史上第一次，我成功找到完美平衡和代表性的人樣本。我開始在我的完美樣本上運行“丹妮難以置信的冗長和乏味的實驗”，但由于我的研究非常冗長和乏味，很多人開始退出。我無法阻止這種情況。參與者絕對有權利在任何時候基於任何理由停止任何實驗，作為研究人員，我們在道德和職業上有義務提醒人們他們確實有這個權利。所以，假設“丹妮難以置信的冗長和乏味的實驗”有非常高的退出率。您認為這種退出是隨機的可能性有多大?答案:零。幾乎可以肯定，留下來的人比退出的人更盡責、更能忍受無聊等。在某種程度上(例如)，盡責感與我關心的心理現象相關，這種淘汰會降低我研究結果的有效性。\n這裡有另一個示例。假設我設計了一項有兩種條件的實驗。在“治療”條件下，實驗者侮辱參與者，然後給他們一份旨在測量順從性的問卷。在“控制”條件下，實驗者參與了一些無意義的閒聊，然後給他們問卷。不考慮這樣一項研究的可疑科學價值和可疑倫理性，讓我們思考一下這裡可能出什麼問題。 一般而言，當有人當著我的面侮辱我時，我會變得合作性極低。所以，很有可能會有更多人從治療組而不是對照組中退出。這種退出不會是隨機的。最有可能退出的人很可能是那些不太關心完全順從地參加完整實驗的重要性的人。由於最頑固和不順從的人都退出了治療組而沒有退出控制組，我們引入了一個混淆:在治療組實際上完成了問卷的人已經比對照組中的人更有可能是富有責任感和順從的。簡而言之，在這項研究中侮辱人並沒有使他們更加順從。而是使更不順從的人退出了實驗!這項實驗的內部效度完全被破壞。\n\n\n2.7.6 無回應偏誤\n無回應偏誤與選擇偏誤和差異淘汰密切相關。這個問題最簡單的版本如下。您向1000人發出調查，但只有300人回复。回复的300人幾乎肯定不是隨機子樣本。回复調查的人與不回复的人在系統上是不同的。這在嘗試從回复者將結論推廣至更廣泛的人群時會引入問題，因為您現在擁有的是一個非常不隨機的樣本。無回應偏誤的問題比這更廣泛。在(比如說)回覆調查的300人中，您可能會發現並非所有人都回答了每個問題。 如果(比如說)80人選擇不回答您的一個問題，這會引入問題嗎?與往常一樣，答案是:可能。如果未回答的問題位於問卷的最後一頁，而那80份調查的最後一頁遺失，則失踪數據可能不是什麼大問題;可能只是頁面脫落了。然而，如果那80人未回答的是調查中最具對抗性或最具侵入性的個人問題，那麼您幾乎肯定面臨問題。 實質上，您在這裡面臨的是所謂的失踪數據問題。如果失踪的數據是“隨機遺失的”，那麼這不是什麼大問題。如果它系統性遺失，那麼它可能是個大問題。\n\n\n2.7.7 趨向平均數的迴歸\n趨向平均數的迴歸指的是您根據某項測量上的極端值選擇數據的任何情況。由于變項本身有自然變異，這幾乎肯定意味著當您進行後續測量時，後續測量將比第一次測量的結果更不極端，純粹是偶然的。\n這裡有一個示例。假設我對心理學教育是否對非常聰明的孩子有不良影響感興趣。為此，我找到20名大學一年級心理學專業成績最好的學生，並觀察他們在大學的表現如何。結果發現，他們的表現比平均水平好得多，但他們在大學並未高居榜首，儘管他們在高中曾經是頂尖生。這是怎麼回事?最自然的第一個想法是，這一定意味著心理學課程對這些學生產生了不良影響。但是，儘管這很有可能是解釋，但您看到的更可能是“趨向平均數的迴歸”的一個示例。為了理解它的工作原理，讓我們花點時間思考一下在高中或大學課堂上獲得最高分需要什麼條件，而不考慮課堂本身。當一個班級很大時，會有很多非常聰明的人。 要獲得最高分，您必須非常聰明，非常努力，還有一點點運氣。考試必須恰好考到您的特殊技能，而在回答時，您也必須避免犯任何愚蠢的錯誤(我們都會有這種時候)。關鍵在于，雖然智力和努力可以從一門課轉移到下一門課，但運氣不可以。 在高中運氣好的人不會與大學運氣好的人相同。這就是“運氣”的含義。 這個結果是，當您根據某項測量的極端值(前20名學生)選擇人時，您選擇的是努力工作、技能和運氣。 但由於運氣不會轉移到第二次測量(只有技能和工作)，所以預期這些人在第二次測量(大學)時的表現都會有所下降。 因此，他們的分數稍有下降，回歸到其他所有人的平均水平。 這就是趨向平均數的迴歸。\n令人驚訝的是，趨向平均數的迴歸非常普遍。例如，如果兩個身材高大的人生孩子，他們的孩子往往會比平均水平高，但不如父母高。對於身材矮小的父母則情況相反。兩個身材矮小的父母往往會生矮小的孩子，但這些孩子往往會比父母高。這也可能非常微妙。例如，曾有研究表明，人們從負面反饋中學習效果更好，而不是從正面反饋中。然而，人們嘗試展示這一點的方式是，在人們表現良好時給予正面強化，在人們表現不好時給予負面強化。您會發現，在正面強化後，人們的表現趨向于變差，但在負面強化後，人們的表現趨向于變好。但請注意，這裡存在選擇偏誤!當人們表現非常出色時，您選擇的是“高”值，所以您應該預期，由于趨向平均數的迴歸，無論是否給予強化，下一次試驗的表現都應該變差。同樣，在一次糟糕的試驗後，人們本身就會有改進的趨勢。負面反饋的明顯優越性是由于趨向平均數的迴歸而引起的結果偏差(見 Kahneman & Tversky (1973) 的討論)。\n\n\n2.7.8 實驗者偏誤\n實驗者偏誤可以有多種形式。其基本思想是，儘管實驗者存有最好的意圖，但可能會不經意地通過微妙地向參與者傳達“正確答案”或“期望的行為”而最終影響實驗結果。通常，這是因為實驗者擁有參與者所没有的特殊知識，例如問題的正確答案或對參與者所在條件的預期表現模式的了解。這種情況的典型例子是 1907年“聰明的漢斯”(Pfungst (1911))的個案研究。聰明的漢斯是一隻表面上能夠閱讀、計算和執行其他人類智力技能的馬。 在聰明的漢斯走紅後，心理學家開始更仔細地檢查他的行為。事實證明，漢斯不令人驚訝地并不會數學。而是因為人類了解如何計數，當人類改變行為時，馬學會了改變自己的行為。\n解決實驗者偏誤問題的一般方法是進行雙盲研究，其中實驗者和參與者都不知道參與者所在的條件或期望的行為。這為該問題提供了一個非常好的解決方案，但重要的是要認識到這並不完美，也很難完美實現。例如，我嘗試構建雙盲研究的顯而易見的方法是讓我的一名博士生(不了解該實驗的學生)來運行實驗。這看起來應該足夠了。唯一知道所有細節(例如問題的正確答案、參與者與條件的分配)的人(我)與參與者没有交流，與人進行交流的人(博士生)什麼也不知道。除了現實情况是最后一部分不太可能是真的。為了使博士生能夠有效地運行研究，他們需要由我這個研究人員進行指導。而事實上，博士生也了解我，並了解一些我對人和心理學的一般信念(例如，我傾向於認為人類比心理學家給他們的信任更聰明)。由此，實驗者幾乎不可能完全避免知道一點點我的期望。即使一點點知識也會產生影響。假設實驗者不經意地傳遞了參與者預期在這項任務中表現良好的事實。好吧，有一個叫“毕加索效应”的东西，如果你对别人有很高的期望，他们就会努力不负所望。 但是，如果您期望他們失敗，他們也會這樣做。 換句話說，期望成為了自我實現的預言。\n好的，注意到原文中有些特殊格式的内容不需要翻译。我翻译了最后四个小节的正文内容，结果如下:\n\n\n2.7.9 需求效應與反應性\n當談到實驗者偏誤時，人們擔心的是實驗者對實驗的知識或期望被傳達給了參與者，並且這些都可能改變人們的行為(Rosenthal, 1966)。然而，即使您成功阻止了這種情況的發生，幾乎不可能阻止人們意識到自己是心理學研究的一部分。並且仅仅知道有人在觀察或研究您就可能對行為產生相當大的影響。這通常被稱為反應性或需求效應。其基本思想是霍桑效應:人們由于研究專注于他們而改變了自己的表現。這個效應的名字來自一項在芝加哥外的“霍桑工廠”進行的研究(見 Adair (1984))。這項20世紀20年代的研究觀察了工廠照明對工人生產力的影響。但重要的是，工人行為的變化是由于工人知道自己被研究了，而不是工廠照明的任何效果。\n為了更具體地說明作為一項研究的一部分會如何改變人們的行為，思考一下社會心理學家的觀點并查看人們在實驗中可能會但在現實生活中卻不會採取的一些角色:\n\n好的參與者嘗試對研究人員過於樂于助人。他或她試圖找出實驗者的假設並證實它們。\n消極參與者與好參與者完全相反。他或她試圖以某種方式破壞或否定研究或假設。\n忠實參與者異常地順從。無論在更真實的環境中可能發生什麼，他或她都試圖完美遵循說明。\n戒慎的參與者對被測試或研究感到緊張，以至於他或她的行為變得非常不自然，或過度符合社會期望。\n\n\n\n2.7.10 安慰劑效應\n安慰劑效應是我們非常擔心的一種特定類型的需求效應。它指的是僅僅被治療的事實就會導致結果的改善。典型的例子來自臨床試驗。如果您給予人們一種完全化學惰性藥物並告訴他們這是某種疾病的治療，那么與完全未治療的人相比，他們的康复速度會更快。換句話說，人們認为自己正在接受治療是导致結果改善的原因，而不是藥物本身。\n然而，醫學界的現在共識是真正的安慰劑效應非常罕見，此前被認為是安慰劑效應的大部分事實上是自然痊愈(有些人自己會好轉)、趨向平均數的迴歸和其他研究設計怪癖的組合。 對心理學感興趣的是，至少存在一些安慰劑效應的最有力證據來自自我報告的結果，最顯著的是治療疼痛 (Hróbjartsson & Gøtzsche, 2010)。\n\n\n2.7.11 情境、測量、及小群體效應\n從某些方面講，這些術語是“所有其他外部效度威脅”的通稱。它們指的是您從中抽取參與者的子人群的選擇、運行研究的位置、時間和方式(包括誰收集數據)以及您用於測量的工具都可能會影響結果。 具體而言，人們擔心的是，這些都可能以某種方式影響結果，從而結果無法推廣至更廣泛的人群、地點和測量中。\n\n\n2.7.12 詐欺、欺暪與自我欺暪\n\n當一個人的薪水依賴於他不理解某件事情時，很難讓他理解這件事。\n- 厄普頓·辛克萊\n\n有一件事我覺得我應該提到。在閱讀文本中關於評估一項研究效度的內容時，我無法不注意到它們似乎假定研究人員是誠實的。我覺得這很有趣。儘管絕大多數科學家都是誠實的，但在我的經驗中，至少也有一些不是。14 不僅如此，正如我前面所提到的，科學家也無法免疫信念偏見。研究人員很容易最終愚弄自己相信錯誤的事情，這可能導致他們進行稍有缺陷的研究，然後在撰寫時隱藏這些缺陷。所以您不僅需要考慮公然欺詐的(可能性不大的)可能性，還需要考慮研究無意中“有偏見”的(可能相當常見的)可能性。我翻開了幾本標準教科書，卻沒有找到對此問題的太多討論，所以這裡是我自己嘗試列出這些問題可能出現的幾種方式:\n\n資料造假。有时候，人們會捏造數據。這有時會“本著好意”進行。例如，研究人員認為，捏造的數據確實反映了事實，並且實際上可能反映了實際數據的“略為清理”版本。在其他情況下，欺詐是故意和惡意的。一些被指控或被證明數據造假的高調例子包括:西里爾·伯特(一位據信捏造了部分數據的心理學家)、安德魯·韋克菲爾德(被指控捏造MMR疫苗與自閉症之間聯繫的數據)和黃禹錫(偽造了大量幹細胞研究數據)。\n惡作劇。惡作劇與數據造假有很多相似之處，但它們的目的不同。惡作劇通常是玩笑，其中許多都旨在(最終)被發現。惡作劇的重點通常是為了詆毀某人或某個領域。多年來發生了很多眾所周知的科學惡作劇(例如皮爾當人)，有些是故意試圖抹黑特定領域的研究(例如索卡爾事件)。\n資料不實。雖然欺詐佔據了大部分頭條，但在我的經驗中，看到數據被歪曲的情況更為常見。我這麼說不是指報紙報導錯誤(他們確實幾乎總是這樣)，而是指出事實上數據並不總是研究人員認為它們所說的那樣。我猜，幾乎總是如此，這不是蓄意不誠實的結果，而是由於數據分析缺乏成熟。例如，回想一下我在這本書開頭討論的辛普森佯謬的示例。人們展示某種“匯總”數據非常常見，有時如果你更深入地找到原始數據，你會發現匯總數據與未匯總數據講述了不同的故事。或者，您可能會發現某些數據被隱藏了，因為它們講述了一個不方便的故事(例如，研究人員可能選擇不提及某個特定變項)。這方面有很多變體，其中許多非常難以檢測。\n“不良”研究設計。好吧，這一點很微妙。基本上，這裡的問題在於研究人員設計了一項內在缺陷的研究，而這些缺陷在論文中從未報告。報告的數據是完全真實的並正確分析的，但它們是由一個實際上組織錯誤的研究產生的。研究人員真的想找到特定效應，所以研究的設定方式使其“容易”人為地觀察到該結果。為了幫助您學習欺詐，我可以給你一個小竅門，那就是設計一項實驗，讓參與者很明顯知道他們「應該」做什麼，然後讓反應性為你發揮魔力。如果您要的話，可以添加所有雙盲實驗的外觀，但這並没有什麼區別，因為研究材料本身就在微妙地告訴人們您希望他們做什麼。當您寫出結果時，欺詐對讀者並不明顯。當參與者身臨其境時對他們很明顯的東西，對閱讀論文的人來說並不總是那麼明顯。當然，我描述這點的方式讓它聽起來總是欺詐。可能確實有一些情況是故意這樣做的，但在我的經驗中，更大的擔憂來自於非故意的錯誤設計。研究人員確實相信某些事情，所以研究恰好最終出現了內在缺陷，而當研究被撰寫並發表時，這些缺陷就神奇地抹去了。\n資料探勘與後設假設檢定。研究作者或多或少歪曲數據的另一種方式是從事所謂的“數據挖掘”(見 Gelman & Loken (2014) 的更廣泛討論，作為統計分析中的“岔路花園”的一部分)。正如我們稍後將討論的那樣，如果您不斷嘗試以許多不同方式分析數據，最終您會找到某些“看起來像”真實結果但實際上不是的東西。這被稱為“數據挖掘”。由於數據分析曾經需要幾周時間，所以它曾經相當罕見，但現在每個人的電腦上都有非常強大的統計軟件，所以它正在變得越來越普遍。數據探勘本身並不是“錯誤的”，但您探勘得越多，風險就越大。這裡錯誤並且我懷疑很常見的是未承認的數據探勘。也就是說，研究人員進行了人類所知的每種可能的分析，找到了起作用的，然後假裝這是他們唯一進行過的分析。更糟糕的是，他們通常會在查看數據後“發明”一個假設來掩蓋數據挖掘。但我想澄清的是，看到數據後改變你的信念並不是錯誤的，並基於新的“後設”假設來重新分析數據也不是錯誤的。錯誤(並且我懷疑很常見)的是未承認你這樣做了。如果您承認自己這樣做了，其他研究人員就能考慮到您的行為。如果你不承認，他們就不能。這使您的行為具有欺騙性。很糟糕!\n發表偏誤與自我審查。最后，一种无所不在的偏见就是不報告負面結果。這几乎是不可能預防的。期刊不會發表提交給他們的每一篇文章。他們喜歡發表那些發現了“某事物”的文章。所以，如果有20人做一次實驗，研究閱讀《芬尼根的守靈夜》是否會導致人類瘋狂，其中19人發現不會，你認為哪一篇文章會被發表?顯然，它是那一項確實發現《芬尼根的守靈夜》會導致瘋狂的研究。15 這是發表偏誤的一個示例。由於沒有人發表那19項沒有發現效應的研究，一個天真的讀者永遠不會知道它們的存在。更糟糕的是，大多數研究人員“內化”了這種偏見，並最終會自我審查他們的研究。知道負面結果不會被接受發表，他們甚至從未試圖報告它們。正如我的一位朋友所說“每一項您發表的實驗，您還有10個失敗”。她是對的。關鍵是，雖然其中一些(可能大多數)研究由於枯燥的原因而失敗(例如，您搞砸了某些事情)，但其他研究可能是真正的“零”結果，您在撰寫“良好”實驗時應該承認它們。區分兩者通常很難做到。一個很好的起點是 Ioannidis (2005) 的一篇題為“為什麼大多數已發表的研究結果是錯誤的”的論文。我也建議您看看 Kühberger et al. (2014) 的工作，他們提供了這種情況在心理學中確實發生的統計證據。\n\n關於這方面還有很多問題需要思考，但这對開始來說已經足夠了。我真正想指出的是一個顯而易見的事實，那就是現實世界的科學是由真實的人類進行的，只有最天真的人才會自動假定其他每個人都是誠實和公正的。實際的科學家通常不那麼天真，但由於某些原因，世界喜歡假裝我們很天真，而我們通常編寫的教科書似乎加強了這種刻板印象。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#measures-of-central-tendency",
    "href": "04-Descriptive-statistics.html#measures-of-central-tendency",
    "title": "4  Descriptive statistics",
    "section": "4.1 Measures of central tendency",
    "text": "4.1 Measures of central tendency\nDrawing pictures of the data, as I did in Figure 4.2, is an excellent way to convey the “gist” of what the data is trying to tell you. It’s often extremely useful to try to condense the data into a few simple “summary” statistics. In most situations, the first thing that you’ll want to calculate is a measure of central tendency. That is, you’d like to know something about where the “average” or “middle” of your data lies. The three most commonly used measures are the mean, median and mode. I’ll explain each of these in turn, and then discuss when each of them is useful.\n\n4.1.1 The mean\nThe mean of a set of observations is just a normal, old-fashioned average. Add all of the values up, and then divide by the total number of values. The first five AFL winning margins were 56, 31, 56, 8 and 32, so the mean of these observations is just:\n\\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\] Of course, this definition of the mean isn’t news to anyone. Averages (i.e., means) are used so often in everyday life that this is pretty familiar stuff. However, since the concept of a mean is something that everyone already understands, I’ll use this as an excuse to start introducing some of the mathematical notation that statisticians use to describe this calculation, and talk about how the calculations would be done in jamovi.\nThe first piece of notation to introduce is \\(N\\), which we’ll use to refer to the number of observations that we’re averaging (in this case \\(N = 5\\)). Next, we need to attach a label to the observations themselves. It’s traditional to use X for this, and to use subscripts to indicate which observation we’re actually talking about. That is, we’ll use \\(X_1\\) to refer to the first observation, \\(X_2\\) to refer to the second observation, and so on all the way up to \\(X_N\\) for the last one. Or, to say the same thing in a slightly more abstract way, we use \\(X_i\\) to refer to the i-th observation. Just to make sure we’re clear on the notation, Table 4.1 lists the 5 observations in the afl.margins variable, along with the mathematical symbol used to refer to it and the actual value that the observation corresponds to.\n\n\n\n\nTable 4.1:  Observations in the afl.margins variable \n\nthe observationits symbolthe observed value\n\nwinning margin, game 1\\( X_1 \\)56 points\n\nwinning margin, game 2\\( X_2 \\)31 points\n\nwinning margin, game 3\\( X_3 \\)56 points\n\nwinning margin, game 4\\( X_4 \\)8 points\n\nwinning margin, game 5\\( X_5 \\)32 points\n\n\n\n\n\n[Additional technical detail2]\n\n\n4.1.2 Calculating the mean in jamovi\nOkay, that’s the maths. So how do we get the magic computing box to do the work for us? When the number of observations starts to become large it’s much easier to do these sorts of calculations using a computer. To calculate the mean using all the data we can use jamovi. The first step is to click on the ‘Exploration’ button and then click ‘Descriptives’. Then you can highlight the afl.margins variable and click the ‘right arrow’ to move it across into the ‘Variables box’. As soon as you do that a Table appears on the right hand side of the screen containing default ‘Descriptives’ information; see Figure 4.3.\n\n\n\n\n\nFigure 4.3: Default descriptives for the AFL 2010 winning margin data (the afl.margins variable)\n\n\n\n\nAs you can see in Figure 4.3, the mean value for the afl.margins variable is 35.30. Other information presented includes the total number of observations (N=176), the number of missing values (none), and the Median, Minimum and Maximum values for the variable.\n\n\n4.1.3 The median\nThe second measure of central tendency that people use a lot is the median, and it’s even easier to describe than the mean. The median of a set of observations is just the middle value. As before let’s imagine we were interested only in the first 5 AFL winning margins: \\(56\\), \\(31\\), \\(56\\), \\(8\\) and \\(32\\). To figure out the median we sort these numbers into ascending order:\n8, 31, 32, 56, 56\nFrom inspection, it’s obvious that the median value of these 5 observations is 32 since that’s the middle one in the sorted list (I’ve put it in bold to make it even more obvious). Easy stuff. But what should we do if we are interested in the first 6 games rather than the first 5? Since the sixth game in the season had a winning margin of 14 points, our sorted list is now\n8, 31, 32, 56, 56\nand there are two middle numbers, 31 and 32. The median is defined as the average of those two numbers, which is of course 31.5. As before, it’s very tedious to do this by hand when you’ve got lots of numbers. In real life, of course, no-one actually calculates the median by sorting the data and then looking for the middle value. In real life we use a computer to do the heavy lifting for us, and jamovi has provided us with a Median value of 30.50 for the afl.margins variable (Figure 4.3).\n\n\n4.1.4 Mean or median? What’s the difference?\nKnowing how to calculate means and medians is only a part of the story. You also need to understand what each one is saying about the data, and what that implies for when you should use each one. This is illustrated in Figure 4.4. The mean is kind of like the “centre of gravity” of the data set, whereas the median is the “middle value” in the data. What this implies, as far as which one you should use, depends a little on what type of data you’ve got and what you’re trying to achieve. As a rough guide:\n\nIf your data are nominal scale you probably shouldn’t be using either the mean or the median. Both the mean and the median rely on the idea that the numbers assigned to values are meaningful. If the numbering scheme is arbitrary then it’s probably best to use the Mode instead.\nIf your data are ordinal scale you’re more likely to want to use the median than the mean. The median only makes use of the order information in your data (i.e., which numbers are bigger) but doesn’t depend on the precise numbers involved. That’s exactly the situation that applies when your data are ordinal scale. The mean, on the other hand, makes use of the precise numeric values assigned to the observations, so it’s not really appropriate for ordinal data.\nFor interval and ratio scale data either one is generally acceptable. Which one you pick depends a bit on what you’re trying to achieve. The mean has the advantage that it uses all the information in the data (which is useful when you don’t have a lot of data). But it’s very sensitive to extreme, outlying values.\n\nLet’s expand on that last part a little. One consequence is that there are systematic differences between the mean and the median when the histogram is asymmetric (Skew and kurtosis). This is illustrated in Figure 4.4. Notice that the median (right hand side) is located closer to the “body” of the histogram, whereas the mean (left hand side) gets dragged towards the “tail” (where the extreme values are). To give a concrete example, suppose Bob (income $50,000), Kate (income $60,000) and Jane (income $65,000) are sitting at a table. The average income at the table is $58,333 and the median income is $60,000. Then Bill sits down with them (income $100,000,000). The average income has now jumped to $25,043,750 but the median rises only to $62,500. If you’re interested in looking at the overall income at the table the mean might be the right answer. But if you’re interested in what counts as a typical income at the table the median would be a better choice here.\n\n\n\n\n\nFigure 4.4: An illustration of the difference between how the mean and the median should be interpreted. The mean is basically the ‘centre of gravity’ of the data set. If you imagine that the histogram of the data is a solid object, then the point on which you could balance it (as if on a see-saw) is the mean. In contrast, the median is the middle observation, with half of the observations smaller and half of the observations larger\n\n\n\n\n\n\n4.1.5 A real life example\nTo try to get a sense of why you need to pay attention to the differences between the mean and the median let’s consider a real life example. Since I tend to mock journalists for their poor scientific and statistical knowledge, I should give credit where credit is due. This is an excellent article on the ABC news website3 from 24 September, 2010:\n\nSenior Commonwealth Bank executives have travelled the world in the past couple of weeks with a presentation showing how Australian house prices, and the key price to income ratios, compare favourably with similar countries. “Housing affordability has actually been going sideways for the last five to six years,” said Craig James, the chief economist of the bank’s trading arm, CommSec.\n\nThis probably comes as a huge surprise to anyone with a mortgage, or who wants a mortgage, or pays rent, or isn’t completely oblivious to what’s been going on in the Australian housing market over the last several years. Back to the article:\n\nCBA has waged its war against what it believes are housing doomsayers with graphs, numbers and international comparisons. In its presentation, the bank rejects arguments that Australia’s housing is relatively expensive compared to incomes. It says Australia’s house price to household income ratio of 5.6 in the major cities, and 4.3 nationwide, is comparable to many other developed nations. It says San Francisco and New York have ratios of 7, Auckland’s is 6.7, and Vancouver comes in at 9.3.\n\nMore excellent news! Except, the article goes on to make the observation that:\n\nMany analysts say that has led the bank to use misleading figures and comparisons. If you go to page four of CBA’s presentation and read the source information at the bottom of the graph and table, you would notice there is an additional source on the international comparison – Demographia. However, if the Commonwealth Bank had also used Demographia’s analysis of Australia’s house price to income ratio, it would have come up with a figure closer to 9 rather than 5.6 or 4.3\n\nThat’s, um, a rather serious discrepancy. One group of people say 9, another says 4-5. Should we just split the difference and say the truth lies somewhere in between? Absolutely not! This is a situation where there is a right answer and a wrong answer. Demographia is correct, and the Commonwealth Bank is wrong. As the article points out:\n\n[An] obvious problem with the Commonwealth Bank’s domestic price to income figures is they compare average incomes with median house prices (unlike the Demographia figures that compare median incomes to median prices). The median is the mid-point, effectively cutting out the highs and lows, and that means the average is generally higher when it comes to incomes and asset prices, because it includes the earnings of Australia’s wealthiest people. To put it another way: the Commonwealth Bank’s figures count Ralph Norris’ multi-million dollar pay packet on the income side, but not his (no doubt) very expensive house in the property price figures, thus understating the house price to income ratio for middle-income Australians.\n\nCouldn’t have put it better myself. The way that Demographia calculated the ratio is the right thing to do. The way that the Bank did it is incorrect. As for why an extremely quantitatively sophisticated organisation such as a major bank made such an elementary mistake, well… I can’t say for sure since I have no special insight into their thinking. But the article itself does happen to mention the following facts, which may or may not be relevant:\n\n[As] Australia’s largest home lender, the Commonwealth Bank has one of the biggest vested interests in house prices rising. It effectively owns a massive swathe of Australian housing as security for its home loans as well as many small business loans.\n\nMy, my.\n\n\n4.1.6 Mode\nThe mode of a sample is very simple. It is the value that occurs most frequently. We can illustrate the mode using a different AFL variable: who has played in the most finals? Open the aflsmall finalists file and take a look at the afl.finalists variable, see Figure 4.5. This variable contains the names of all 400 teams that played in all 200 finals matches played during the period 1987 to 2010.\nWhat we could do is read through all 400 entries and count the number of occasions on which each team name appears in our list of finalists, thereby producing a frequency table. However, that would be mindless and boring: exactly the sort of task that computers are great at. So let’s use jamovi to do this for us. Under ‘Exploration’ - ‘Descriptives’ click the small check box labelled ‘Frequency tables’ and you should get something like Figure 4.6.\nNow that we have our frequency table we can just look at it and see that, over the 24 years for which we have data, Geelong has played in more finals than any other team. Thus, the mode of the afl.finalists data is “Geelong”. We can see that Geelong (39 finals) played in more finals than any other team during the 1987-2010 period. It’s also worth noting that in the ‘Descriptives’ Table no results are calculated for Mean, Median, Minimum or Maximum. This is because the afl.finalists variable is a nominal text variable so it makes no sense to calculate these values.\n\n\n\n\n\nFigure 4.5: A screenshot of jamovi showing the variables stored in the aflsmall finalists.csv file\n\n\n\n\n\n\n\n\n\nFigure 4.6: A screenshot of jamovi showing the frequency table for the afl.finalists variable\n\n\n\n\nOne last point to make regarding the mode. Whilst the mode is most often calculated when you have nominal data, because means and medians are useless for those sorts of variables, there are some situations in which you really do want to know the mode of an ordinal, interval or ratio scale variable. For instance, let’s go back to our afl.margins variable. This variable is clearly ratio scale (if it’s not clear to you, it may help to re-read Section 2.2), and so in most situations the mean or the median is the measure of central tendency that you want. But consider this scenario: a friend of yours is offering a bet and they pick a football game at random. Without knowing who is playing you have to guess the exact winning margin. If you guess correctly you win $50. If you don’t you lose $1. There are no consolation prizes for “almost” getting the right answer. You have to guess exactly the right margin. For this bet, the mean and the median are completely useless to you. It is the mode that you should bet on. To calculate the mode for the afl.margins variable in jamovi, go back to that data set and on the ‘Exploration’ - ‘Descriptives’ screen you will see you can expand the section marked ‘Statistics’. Click on the checkbox marked ‘Mode’ and you will see the modal value presented in the ‘Descriptives’ Table, as in Figure 4.7. So the 2010 data suggest you should bet on a 3 point margin.\n\n\n\n\n\nFigure 4.7: A screenshot of jamovi showing the modal value for the afl.margins variable"
  },
  {
    "objectID": "04-Descriptive-statistics.html#sec-Measures-of-variability",
    "href": "04-Descriptive-statistics.html#sec-Measures-of-variability",
    "title": "4  描述統計",
    "section": "4.2 變異量數",
    "text": "4.2 變異量數\n至此有關集中量數的說明先告一段落。除了資料的“中心點”或“次數最多”的部分，描述統計也要說明資料的變異程度(variability)。像是資料有多“離散”？測量到的數值離平均值或中位數有多遠？這一節的只有談等距或等比尺度資料的變異量數，所以繼續使用afl.margins資料示範，還有說明各種變異量數的優缺點。\n\n\n4.2.1 全距\n計算變項資料的全距非常容易，就是最大值減去最小值。以AFL勝隊得分的例子來說，最大值是116，最小值是0。儘管全距是所有變異量數裡計算方法最簡單，卻也是最不可靠的。還記得平均值的說明曾提到，統計量數最好要穩定(robust)。如果變項資料裡有一兩個極端數值，會影響全距的大小。就像以下數列有個非常極端的數值：\n-100, 2, 3, 4, 5, 6, 7, 8, 9, 10\n很明顯這筆資料的全距完全被極端值主宰：沒有去掉極端值的全距是110，但是移除之後變成8。\n\n\n\n4.2.2 四分位數間距\n四分位數間距(interquartile range)類似全距，不過是改成計算資料內位於25%與75%兩個百分位數之間的差異。若是同學還不知道什麼是百分位數(percentile)，可以想成一筆資料x的10%百分位數，是資料內小於這個數值的比例佔10%。其實前面我們就已經學過這個概念，因為中位數就是50%百分位數！jamovi的預設描述統計功能可以直接算出一筆資料的25%、50%、75%三個百分位數。請在jamovi描述統計選單內勾選“Quartiles”的選項試試看。\n圖 4.8 展示的50%百分位數與中位數毫不意外是相同的。同時也能得到2010年AFL勝隊得分的IQR是37.5 (50.50 - 12.75 = 37.75)。理解IQR的簡單方式是：這個數值代表一筆資料“中間一半”跨越的範圍。小於25%百分位數的四分之一數值，以及大於75%百分位數的四分之一數值都在這“中間一半”之外。只有IQR內的數值在這段範圍之內。\n\n\n\n\n\n\n圖 4.8: jamovi計算資料變項afl.margins的三個百分位數。\n\n\n\n\n\n\n4.2.3 平均絕對差\n全距與IQR都是利用資料內的百分位數，估計資料的變異程度。不過這不是估計變異程度的唯一方法。另一種方法是先決定一個有意義的參考點(像是平均值或中位數)，再計算整筆資料與參考點的“典型”差異值。那麼，什麼是“典型”差異值？通常是總和所有資料與參考點的差異值。實務上有兩種變異量數可用：“平均絕對差”(mean absolute deviation，平均值是參考點)與“中位絕對差”(median absolute deviation，中位數是參考點)。就原作者研讀過的書藉，“中位絕對差”是較理想的變異程度評估指標，不過心理學領域不常見到。\n經過前一段的提要，我們來看看如何計算平均絕對差。同樣透過手算AFL勝隊得分來學習，就用前五場得分紀錄56, 31, 56, 8, 32來示範計算步驟。之前我們已經算出平均值是36.6 (記為 \\(\\bar{X} = 36.6\\) )。每個數值減去平均，可以得到個別的差異值( 記為 \\(X_i - \\bar{X}\\) )，接著取每個差異值的絕對值( 記為 \\(\\mid X_i - \\bar{X} \\mid\\) )。建議同學按步驟照著表4-2 依序紀錄計算結果4。\n\n\n\n\n\n表4-2: 使用離均差絕對值計算資料變異程度\n\n\n\n\n口語\n符號\n數值\n離均差\n離均差絕對值\n\n\n\n\n符號\n\\(i\\)\n\\(X_i\\)\n\\(X_i - \\bar{X}\\)\n\\(|X_i - \\bar{X}|\\)\n\n\n\n1\n56\n19.4\n19.4\n\n\n\n2\n31\n-5.6\n5.6\n\n\n\n3\n56\n19.4\n19.4\n\n\n\n4\n8\n-28.6\n28.6\n\n\n\n5\n32\n-4.6\n4.6\n\n\n\n現在只要計算所有差異值的絕對值，就是平均絕對差。\n\n\\[\n\\frac{19.4 + 5.6 + 19.4 + 28.6 + 4.6}{5} = 15.52\n\\]\n至此我們算出5個數值的平均絕對差是15.52。\n[其他技術細節5]\n\n\n\n4.2.4 變異數\n雖然平均絕對差有其用處，但是並非評估變異程度的最好指標。從數學原理的角度來看，差異值平方比差異值絕對值是更合適的指標。這裡介紹的變異數(variance)會省去許多統計細節6，主要著墨討論心理學研究累積的巨大缺陷。首先我們定義一筆資料(\\(X\\))的變異數記號是 Var(\\(X\\))，通常也被寫成 \\(s^2\\)，其中原因之後會說明。\n[更多技術細節7]\n\n有了基本概念，就用實際資料熟悉變異數的運算。同樣用AFL前五場比賽紀錄練習，請同學用前一節計算平均絕對差異步驟，完成表4-3的手算步驟8。\n\n\n\n\n\n表4-3: 使用離均差平方計算資料變異程度\n\n\n\n\n口語\n符號\n數值\n離均差\n離均差平方\n\n\n\n\n符號\n\\(i\\)\n\\(X_i\\)\n\\(X_i - \\bar{X}\\)\n\\((X_i - \\bar{X})^2\\)\n\n\n\n1\n56\n19.4\n376.36\n\n\n\n2\n31\n-5.6\n31.36\n\n\n\n3\n56\n19.4\n376.36\n\n\n\n4\n8\n-28.6\n817.96\n\n\n\n5\n32\n-4.6\n21.16\n\n\n\n最後一欄是五筆資料的差異值平方，接著依公式用手或計算機計算平均，我們會得到變異數是\\(324.64\\)。好像大功告成了是不是？那這樣的變異數代表什麼意思？進一步解釋前，請用jamovi計算看看，建議重新匯入afl.margins資料變項，並且設定只計算前五項紀錄，然後開啟描述統計模組選單，這次要增加勾選”Variance”計算變異數(見 圖 4.9 )，然後你會發現jamovi的計算結果是\\(405.80\\)，難到表4-3* 的計算步驟是錯誤的？\n\n\n\n\n\n\n圖 4.9: 使用jamovi計算資料變項afl.margins前五筆的變異數。\n\n\n\n\n其實按公式筆算是正確的，jamovi的計算也沒問題9。解釋jamovi用什麼公式計算變異數很簡單，但是為什麼jamovi要用這個公式就需要花點功夫說明。我們先了解jamovi用什麼公式。前面提到的變異數公式是差異值平方總和除以資料個數\\(N\\)，不過jamovi計算的變異數是除以\\(N - 1\\)。\n[更多技術細節10]\n那麼為什麼jamovi是用 \\(N - 1\\)而非 \\(N\\)？畢竟變異數是差異值平方的平均不是嗎？為何不是用資料數值個數\\(N\\)做為分母呢？這樣想雖然沒有錯，不過在 單元 8 我們會討論“描述樣本”與“根據樣本推測母群性質”兩種統計操作的區別。平均數的計算方法在“描述樣本”與“推測母群性質”都是一樣的。但是對於變異數、標準差、還有許多估計變異程度的量數，“樣本“與”母群”的計算方法是不一樣的。正在學習的同學們從這裡開始，必須察覺多數統計實務計算的變異數其實是樣本變異數。樣本只是揭露真實世界的一部分而己。這一章學習的描述統計方法其實是“整理樣本的方法”，真實目的是要要估計“母群參數”。不過這樣的轉念對很多同學來說可能要求太高了，我們先接受jamovi計算的統計量數都是正確的，到了 單元 8 我們會再回來討論這些問題。\n好啦，這一節讀起來有些像是燒腦的偵探小說。先請同學們擱置具體理由，接受只要是用jamovi計算的變異數都是用\\(N-1\\)除以平方差總和的結果。不過還有一件事需要討論：怎麼解讀變異數的意思？描述統計的功能畢竟是用數字說明資料裡的故事，而且變異數就明明白白的顯示在報表上。但是目前還不能直接用變異數解釋資料的變異程度，原因在於變異數是總計差異值的平方，而任何數值平方之後，就失去測量尺度的數值差異。就拿表4-3紀錄的第一場比賽分數，差異值平方是\\(324.64\\)來說，這個數字無法說明這筆資料與平均值的差異究竟有多大，因為平方後的數值喪失了原來測量尺度的意義。另外，你也可以想想看，你知道了全班同學的身高平方總和有什麼意義？\n\n\n\n\n4.2.5 標準差\n若是同學學到了這裡，應該都能接受變異數的數學特性，不過要撰寫人類看得到的報告，我們還是要呈現能具體描述變異程度的量數。那要怎麼改造變異數呢？方法很簡單，就是將變異數開根號取得標準差(standard deviation)。統計學術語還有“差異平均值之平方根”(mean squared deviation)，或者簡稱RMSD。標準差的數值能以資料尺度解釋變異程度。以AFL資料的實務報告來說，通常會寫“標準差18.01”而不是”變異數324.68“。專業報告規範會提示樣本資料的標準差寫法，像是用小寫s，sd，還有“std dev”等。\n[更多技術細節11]\n不過在前一節變異數的介紹中，我們已經知道jamovi使用的樣本變異數計算公式是除以 \\(N - 1\\)，所以樣本標準差同樣是來自這個公式計算結果的開根號。\n[更多技術細節12]\n使用標準差解讀變異程度的數學原理很複雜，在此先學一套簡單的規則：一筆資料裡與平均值相差一個標準差的資料大約佔68%；相差兩個標準差的資料大約佔95%；相差三個標準差的資料大約佔99.7%。只要資料繪制出的直方圖接近對稱或”鐘形曲線“，大致能用這套規則解讀資料的變異程度。但是從AFL資料的直方圖( 圖 4.2 )來看，似乎不大適用這套規則。根據 圖 4.10 的解說，實際上只有65.3%的得分紀錄，落在AFL資料平均值的前後一個標準差之內。\n\n\n\n4.2.6 應該用那種變異量數?\n這一節學到了幾種變異量數：全距，四分位數間距，平均絕對差，變異數，以及標準差。各種量數優缺點不同，以下做個小結：\n\n全距 呈現資料的完整變異範圍。極端值無可避免地會決定全距數值大小，除非我們在意資料裡的極端值，報告全距才有用處。\n四分位數間距 呈現資料中間一半涵蓋的範圍。數值穩定且與中位數互補。多數研究者愛用。\n平均絕對差 呈現所有資料遠離平均值的平均距離。容易解讀但是有些這一節未談到的問題，讓這項量數比較不如標準差受歡迎。研究文獻中不常見到。\n變異數 呈現所有資料遠離平均值的平均距離的平方。在數學意義上能優雅解釋資料與平均值的變異，但是因為尺度單位改變，導致不易解讀。除非有數學工具不然不常在報告裡呈現，不過許多統計軟體的後台程序都會計算變異數。\n標準差 變異數的開根號。具備同樣的數學特性，但是保留測量尺度所以容易解讀。平均值做為集中量數的報告時常搭配呈現，是最常在研究文獻裡看到的變異數數。\n\n簡而言之，四分位數間距與標準差是最容易解讀而且最常見的變異量數。不過其他量數也有適用的時機。在此介紹以備同學們說不定那一天會真的用到。\n\n\n\n\n\n\n\n圖 4.10: 直方圖著色範圍與AFL勝隊得分平均值相差一個標準差，範圍面積約佔65.3%，與正文內提到的68%有些微差異。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#skew-and-kurtosis",
    "href": "04-Descriptive-statistics.html#skew-and-kurtosis",
    "title": "4  Descriptive statistics",
    "section": "4.3 Skew and kurtosis",
    "text": "4.3 Skew and kurtosis\nThere are two more descriptive statistics that you will sometimes see reported in the psychological literature: skew and kurtosis. In practice, neither one is used anywhere near as frequently as the measures of central tendency and variability that we’ve been talking about. Skew is pretty important, so you do see it mentioned a fair bit, but I’ve actually never seen kurtosis reported in a scientific article to date.\n\n\n\n\n\nFigure 4.11: An illustration of skewness. On the left we have a negatively skewed data set, in the middle we have a data set with no skew, and on the right we have a positively skewed data set\n\n\n\n\nSince it’s the more interesting of the two, let’s start by talking about the skewness. Skewness is basically a measure of asymmetry and the easiest way to explain it is by drawing some pictures. As Figure 4.11 illustrates, if the data tend to have a lot of extreme small values (i.e., the lower tail is “longer” than the upper tail) and not so many extremely large values (left panel) then we say that the data are negatively skewed. On the other hand, if there are more extremely large values than extremely small ones (right panel) we say that the data are positively skewed. That’s the qualitative idea behind skewness. If there are relatively more values that are far greater than the mean, the distribution is positively skewed or right skewed, with a tail stretching to the right. Negative or left skew is the opposite. A symmetric distribution has a skewness of 0. The skewness value for a positively skewed distribution is positive, and a negative value for a negatively skewed distribution.\n[Additional technical detail11]\nPerhaps more helpfully, you can use jamovi to calculate skewness: it’s a check box in the ‘Statistics’ options under ‘Exploration’ - ‘Descriptives’. For the afl.margins variable, the skewness figure is \\(0.780\\). If you divide the skewness estimate by the Std. error for skewness you have an indication of how skewed the data is. Especially in small samples (N \\(<\\) 50), one rule of thumb suggests that a value of 2 or less can mean that the data is not very skewed, and a value of over 2 that there is sufficient skew in the data to possibly limit its use in some statistical analyses. Though there is no clear agreement on this interpretation. That said, this does indicate that the AFL winning margins data is somewhat skewed (\\(\\frac{0.780}{0.183} = 4.262\\)).\nThe final measure that is sometimes referred to, though very rarely in practice, is the kurtosis of a data set. Put simply, kurtosis is a measure of how thin or fat the tails of a distribution are, as illustrated in Figure 4.12. By convention, we say that the “normal curve” (black lines) has zero kurtosis, so the degree of kurtosis is assessed relative to this curve.\n\n\n\n\n\nFigure 4.12: An illustration of kurtosis. On the left, we have a ‘platykurtic’ distribution (kurtosis = -.95) meaning that the distribution has ‘thin’ or flat tails. In the middle we have a ‘mesokurtic’ distribution (kurtosis is almost exactly 0) which means that the tails are neither thin or fat. Finally, on the right, we have a ‘leptokurtic’ distribution (kurtosis = 2.12) indicating that the distribution has ‘fat’ tails. Note that kurtosis is measured with respect to a normal curve (black line)\n\n\n\n\nIn this Figure, the data on the left have a pretty flat distribution, with thin tails, so the kurtosis is negative and we call the data platykurtic. The data on the right have a distribution with fat tails, so the kurtosis is positive and we say that the data is leptokurtic. But the data in the middle have neither think or fat tails, so we say that it is mesokurtic and has kurtosis zero. This is summarised in Table 4.4:\n\n\n\n\nTable 4.4:  Thin to fat tails to illustrate kurtosis \n\nEnglishinformal termkurtosis value\n\n\"tails too thin\"platykurticnegative\n\n\"tails neither thin or fat\"mesokurticzero\n\n\"tails too fat\"leptokurticpositive\n\n\n\n\n\n[Additional technical detail12]\nMore to the point, jamovi has a check box for kurtosis just below the check box for skewness, and this gives a value for kurtosis of \\(0.101\\) with a standard error of \\(0.364\\). This means that the AFL winning margins data has only a small kurtosis, which is ok."
  },
  {
    "objectID": "04-Descriptive-statistics.html#descriptive-statistics-separately-for-each-group",
    "href": "04-Descriptive-statistics.html#descriptive-statistics-separately-for-each-group",
    "title": "4  Descriptive statistics",
    "section": "4.4 Descriptive statistics separately for each group",
    "text": "4.4 Descriptive statistics separately for each group\nIt is very commonly the case that you find yourself needing to look at descriptive statistics broken down by some grouping variable. This is pretty easy to do in jamovi. For instance, let’s say I want to look at the descriptive statistics for some clinical trial data, broken down separately by therapy type. This is a new data set, one that you’ve never seen before. The data is stored in the clinicaltrial.csv file and we’ll use it a lot later on in Chapter 13 (you can find a complete description of the data at the start of that chapter). Let’s load it and see what we’ve got (Figure 4.13):\nEvidently there were three drugs: a placebo, something called “anxifree” and something called “joyzepam”, and there were 6 people administered each drug. There were 9 people treated using cognitive behavioural therapy (CBT) and 9 people who received no psychological treatment. And we can see from looking at the ‘Descriptives’ of the mood.gain variable that most people did show a mood gain (\\(mean = 0.88\\)), though without knowing what the scale is here it’s hard to say much more than that. Still, that’s not too bad. Overall I feel that I learned something from that.\nWe can also go ahead and look at some other descriptive statistics, and this time separately for each type of therapy. In jamovi, check Std. deviation, Skewness and Kurtosis in the ‘Statistics’ options. At the same time, transfer the therapy variable into the ‘Split by’ box, and you should get something like Figure 4.14.\n\n\n\n\n\nFigure 4.13: A screenshot of jamovi showing the variables stored in the clinicaltrial.csv file\n\n\n\n\nWhat if you have multiple grouping variables? Suppose you want to look at the average mood gain separately for all possible combinations of drug and therapy. It is possible to do this by adding another variable, drug, into the ‘Split by’ box. Easy peasy, though sometimes if you split too much there isn’t enough data in each breakdown combination to make meaningful calculations. In this case jamovi tells you this by stating something like NaN or Inf. 13\n\n\n\n\n\nFigure 4.14: A screenshot of jamovi showing Descriptives split by therapy type"
  },
  {
    "objectID": "04-Descriptive-statistics.html#sec-Standard-scores",
    "href": "04-Descriptive-statistics.html#sec-Standard-scores",
    "title": "4  描述統計",
    "section": "4.5 標準分數",
    "text": "4.5 標準分數\n想像同學們今天一起做了一份“壞脾氣量表”，量表一供有50項二選一單選題，答“是”就得1分。假設這項量表已經有非常龐大的常模樣本，常模資料分佈相當接近常態分佈，平均得分為17分，標準差是5。今天某位同學的得分是35分，那麼他的壞脾氣程度有多少？也許有人會說可以用 \\(\\frac{35}{50}\\) 或者 70% 代表這位同學的壞脾氣程度。但是想一想用這些數值代表一個人的壞脾氣都有點奇怪。只要編製測驗的人改一下題目敘述，受測者的答題反應就會改變，全部樣本的平均分數也會變高或變低。因此70%只是代表這位同學在某版本的壞脾氣量表得到的分數，所以得分比例並不能提供什麼有用的資訊。\n比較這位同學的分數和其他人的分數，是個簡單的描述壞脾氣程度的方法。如果編製者的常模是收集了一百萬份樣本，只有159人的壞脾氣得分高於35分，也就是說這位同學的壞脾氣排序是前0.016%。這麼一來就比較能解釋資料了。轉換原始分數為可相對比較的分數，這種程序稱為“標準化”(standardisation)。將原始分數轉換為相對百分比相當簡單，但是百分比有個限制：如果常模樣本只有1000人，平均值是16，標準差是5，那麼35分是所有樣本得分的第一高分，無法說明這位同學的脾氣有多壞。\n不過還不用絕望。我們可以將原始評量得分轉換為標準分數(standard score)，也是一般報告稱呼的z分數。標準分數的定義是原始分數與平均分數的差異相當於多少標準差。這句話可以寫成像以下的數學公式：\n\\[\n\\text{標準分數} = \\frac{\\text{原始分數} - 平均數}{\\text{標準差}}\n\\]\n[更多技術細節16]\n現在我們可以將這位同學的評量得分轉換為標準化壞脾氣分數。\n\\[ z =\\frac{35 - 17}{5} = 3.6 \\] 要解釋這個分數的意義，請回憶 小單元 4.2.5 提到的簡易判斷法則：在常態分佈的資料中，比平均值大於3個標準差的數值，大約佔99.7%。所以3.6代表這位同學真的是位壞脾氣的人物。如果有興趣詳細探究，3.6代表在樣本中高於99.98%的受測者。\n除了能了解原始分數在龐大樣本裡的相對高低，標準分數還有另一種有用的功能：可以比較同一位受測者的不同份量表分數。想像有這位同學後來做了另一份”外向度量表“，一共有24題。外向度量表的常模樣本平均值是13，樣本標準差是4，而這位同學的原始外向度得分是2。這個分數無法直接與他的壞脾氣量表得分35分，硬要比的話就變成拿蘋果和橘子比較了。\n如果用標準分數就不同了。我們已經知道他的壞脾氣標準分數\\((z = \\frac{(35-17)}{5}=3.6)\\) 還有外向度標準分數 \\((z = \\frac{(2-13)}{4}=-2.75)\\)。兩項標準分數是指向原始數值在各自常模樣本的相對位置，才可以互相比較17。這位同學明顯比大多數人不外向(\\(z = -2.75\\))，脾氣也比大多數人壞 (\\(z=3.6\\))。而且標準分數的絕對值說明，這位同學給人壞脾氣的印象(\\(3.6\\))，明顯高於他不夠外向(\\(2.75\\))。我們能這樣斷言，是因為z分數是指出資料在變項代表的群體之間相對位置，如此才能比較不同變項的標準分數。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#summary",
    "href": "04-Descriptive-statistics.html#summary",
    "title": "4  Descriptive statistics",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nCalculating some basic descriptive statistics is one of the very first things you do when analysing real data, and descriptive statistics are much simpler to understand than inferential statistics, so like every other statistics textbook I’ve started with descriptives. In this chapter, we talked about the following topics:\n\nMeasures of central tendency. Broadly speaking, central tendency measures tell you where the data are. There’s three measures that are typically reported in the literature: the mean, median and mode.\nMeasures of variability. In contrast, measures of variability tell you about how “spread out” the data are. The key measures are: range, standard deviation, and interquartile range.\nSkew and kurtosis. We also looked at assymetry in a variable’s distribution (skew) and thin or fat tailed distributions (kurtosis).\nDescriptive statistics separately for each group. Since this book focuses on doing data analysis in jamovi, we spent a bit of time talking about how descriptive statistics are computed for different subgroups.\nStandard scores. The z-score is a slightly unusual beast. It’s not quite a descriptive statistic, and not quite an inference. Make sure you understand this section. It’ll come up again later.\n\nIn the next Chapter we’ll move on to a discussion of how to draw pictures! Everyone loves a pretty picture, right? But before we do, I want to end on an important point. A traditional first course in statistics spends only a small proportion of the class on descriptive statistics, maybe one or two lectures at most. The vast majority of the lecturer’s time is spent on inferential statistics because that’s where all the hard stuff is. That makes sense, but it hides the practical everyday importance of choosing good descriptives. With that in mind…"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#安裝jamovi",
    "href": "03-Getting-started-with-jamovi.html#安裝jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.1 安裝jamovi",
    "text": "3.1 安裝jamovi\n好啦，推銷結束。開始上課吧。就像任何應用軟體，jamovi需要安裝能執行各種軟體工具還有免費遊戲的”電腦”。這樣說好像”電腦”和各家平板沒什麼分別，不過如果你有使用套裝軟體的經驗，你就會懂我的意思。無論如何，jamovi可從網路免費取得，現誰你可以從jamovi官網https://www.jamovi.org/下載安裝程式2。\n\n首先在官網首頁按下”jamovi Desktop”的大標題按鈕，讀者會看到給Windows, Mac, 還有Linux作業系統使用者的各種安裝程式超連結。只要點選符合同學用的作業系統規格，就能按照視窗指示完成下載安裝程序。翻譯這本電子書時，jamovi版本已經來到2.3.21，請留意每隔幾個月會有新版本上架，有需要的話得要安裝新版本。3\n\n\n\n3.1.1 啟動jamovi\n不論讀者的作業系統是什麼，安裝成功就可以啟動jamovi開始第一次接觸。首次啟動的jamovi視窗介面應該像 圖 3.1。\n\n\n\n\n\n\n圖 3.1: 啟動jamovi!\n\n\n\n\n視窗左半區塊很像Excel試算表(本書名稱”試算表介面”），右半區塊是顯示統計測試結果的地方（以下稱”報表介面”）。中間邊界線區隔區塊範圍，使用者可以水平拖曳改變左右區塊的面積。\n\n使用者可以在試算表區域的任意格子內輸入任何值，就像使用試算表軟體一樣。還有，已存在電腦裡的資料檔案是CSV (.csv)的話，可以直接載入jamovi。另外，jamovi可以直接載入SPSS, SAS, Stata, 還有JASP等軟體的資料格式檔案。要開啟檔案請先選擇管理面板的File(按左上角三條橫線開啟)，再選擇’Open’，然後從’Browse’視窗選擇你要載入的檔案，這種操作可以開啟範例檔案或已存在的資料檔案。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#分析模組",
    "href": "03-Getting-started-with-jamovi.html#分析模組",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.2 分析模組",
    "text": "3.2 分析模組\n如 圖 3.1 界面上方的一排圖示是可以選擇使用的分析方法模組。選好要使用的分析方法，就會出現該模組的“選項視窗”。“選項視窗”可以指定要分析的變項，選擇分析方法的設定。任何在“選項視窗”的有效動作，會在報表介面顯示分析結果，每次增加或修改設定都會即時更新報表介面內容。\n\n只要你認為報表介面顯示的結果已經足夠，按下“選項視窗”右上角巨大的右箭頭，就能關閉“選項視窗”。若是想再重新調整選項，只要在報表介面任何一處點一下，就可以重新開啟“選項視窗”。這樣的操作讓你或你的同伴回顧報表結果如何生成。\n\n若是報表介面內的某項分析結果不再需要了，你也可以透過副選單移除這項分析結果。以滑鼠右鍵點擊要移除的結果，就會跳出副選單。選擇副選單的Analysis，再選擇Remove就能移除結果。這些就是jamovi的基本操作，接著我們會看到更多。首先來了解試算表介面有什麼功能可以操作。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#資料試算表",
    "href": "03-Getting-started-with-jamovi.html#資料試算表",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.3 資料試算表",
    "text": "3.3 資料試算表\n載入jamovi的資料是以試算表的模樣呈現在試算表介面，每一欄代表“變項”，每一列代表一筆“個案”或“受測者”。\n\n\n3.3.1 變項\njamovi試算表介面最常見的變項是資料變項(Data Variables)4，純粹展示從資料檔匯入的數值，或者使用者自行輸入的數值。每個資料變項能設定為各種測量級別(見 圖 3.2 )\n\n\n\n\n\n\n圖 3.2: 測量級別設定面板\n\n\n\n\n變項標題旁的圖示提示使用者每個變項的測量級別。ID是jamovi特有的變項型態。這個型態用來表示不會放進分析模組的變項。像是參與者姓名或者編號。在處理規模巨大的資料集時，指定某些變項為ID能提供運算效率。\n\n名義變項(Nominal)是表示數值是文字標籤的類別變項，像是性別的標籤是”男”,“女”,“跨性別”等名義數值。人名也是一樣。名義變項數值也可以用數字表現。某些資料集的名義變項會用數字而不是文字做為標籤，像是男生表示為1，女生表示為2。稍後介紹的“變項編輯器”，可以將數字轉換為“人類可讀的”標籤。\n\n次序變項(Ordinal)很像名義變項，除了數值之間有順序性。例如李克特七點量表的數值3代表”非常同意”，數值-3代表”非常不同意”。\n\n連續變項(Continuous)表示來自連續量表的數值，像是身高或體重。又稱”等距尺度“(Interval scale)或”等比尺度“(Ratio scale)。\n\n除此之外，使用者還可以設定資料型態：包括”文字”(Text)，“整數”(Integer)，“十進制小數”(Decimal)。\n\n在空白試算表內任意輸入任何數值，jamovi會根據你輸入的資料自動判斷變項的資料型態。你可以借助這套功能掌握什麼樣的資料適用何種變項尺度。開啟一份資料檔時，jamovi也會自動判斷每一欄的資料型態。不過自動判斷的資料型態不一定是正確的，有必要的話你需要使用變項編輯器手動設定正確的資料型態。\n\n在資料面板(Data)點選Setup圖示或雙擊變項標題都能開啟變項編輯器。在變項編輯器的介面，你可以更改變項名稱、更動變項型態、標籤順序、還有標籤文字。任何變動馬上會在試算表介面看到改變的效果。要關閉變項編輯器，只要點選右上角的向上箭頭。\n\n要增加或插入新變項，只要點選資料面板上方的Add圖示。不只是增加資料變項，也能增加計算變項(computed variables)\n\n\n\n3.3.2 自訂計算變項\n計算變項存儲其他變項的計算結果。可存儲的數值相當多樣，像是log轉換，z分數，變項數值總和，正負轉換與跨變項平均值。\n\n在資料面板上方按下Add圖示，就能在已匯入的資料變項之間插入計算變項。接著會出現方程式編輯區讓你設定計算公式。常用的四則運算符號都能輸入，以下是一些可輸入的公式：\n\nA + B\nLOG10(len)\nMEAN(A, B)\n(len - VMEAN(len)) / VSTDEV(len)\n以上公式依序是變項A與變項B的總和，變項len的log轉換(底數為10)，變項A與變項B的平均值，以及變項len的z分數5。 圖 3.3 示範建立新變項存儲變項len的z分數(這份資料來自Tooth Growh練習資料集)。\n\n\n\n\n\n\n圖 3.3: 設定計算變項zscore-len存放變項len的z分數。\n\n\n\n\n\n3.3.2.1 垂直計算函式\n圖 3.3 的面板裡有個可開啟捲動清單的 \\(f_x\\) 圖示，能選擇已內建的函式。其中有些以V開頭的函式是垂直計算函式。相對於沒有V開頭的同名函式，垂直計算函式用於計算資料變項的總計數值。例如函式MEAN(A, B)是計算變項A與變項B同一列的平均值，而函式VMEAN(A)製造的計算變項內容都是變項A的平均值。\n\n\n\n\n3.3.3 複製貼上\njamovi產生的報表內表格及統計圖符合美國心理學會(APA)建議的學術發表格式。使用者可以使用內建的複製貼上功能，將圖表加到WORD文件、或者email裡給同事。直要對想複製的圖表以滑鼠右鍵點選，開啟副選單即可選擇複製功能。透過副選單能指定要複製報表裡的某張圖或整個分析表格。jamovi複製的操作與其他應用程式的操作完全相同。在稍後的分析示範裡你可以練習看看。\n\n\n\n\n3.3.4 程式碼模式\njamovi提供”R程式碼模式”(R Syntax Mode)。開啟此模式能檢視分析模組後台的R程式碼。從jamovi主畫面右上方開啟系統設定面板(圖示是三個垂直小點)，勾選”Syntax Mode)旁邊的核取方塊就能開啟或關閉。\n“R程式碼模式”開啟時運作方式沒有不同，除了每份報表會增加R程式碼，還有表格會以ASCII碼的模式輸出，如同R語言的終端介面。如果你有支援編譯R語言的軟體，可以直接將R程式碼複製到編譯介面測試看看。至此還沒有正式載入任何資料，所以你看到的R程式碼複製到編譯介面，並不會產生任何結果。如果你有興趣測試的話，網路上有許多學習資源，等著你去發掘。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#載入資料檔案",
    "href": "03-Getting-started-with-jamovi.html#載入資料檔案",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.4 載入資料檔案",
    "text": "3.4 載入資料檔案\n在實際的資料分析場景裡，我們通常要處理好幾份不同格式的檔案。本書示範的資料分析所處理的檔案主要有兩種格式：\n\njamovi專案檔(.omv) 這是jamovi儲存資料，變項及分析結果的標準檔案格式。\n逼號分隔值文字檔(.csv) 歷史最久的標準文字檔案格式，可以處理文字檔案的軟體幾乎都可以開啟。由於格式簡單，專業統計分析人員首選以csv格式儲存資料。\n\njamovi也能載入其他格式的資料檔案。最新版本支援開啟微軟Excel試算表檔案(.xls)，還有市場佔有率高的商用套裝統計軟體，如SPSS、SAS的資料格式檔案。無論你要處理的資料是用那種格式儲存，強烈建議先新增一個或一群資料夾，分別儲存資料檔案或jamovi檔，以便定期備份。\n\n\n3.4.1 匯入csv格式資料檔\n在現代資料分析軟體發展史裡，到處都能見到csv檔的身影。csv檔可儲存任何表格化的資料，如同 圖 3.4 展示的本章示範資料檔 booksales.csv 。每一列記錄每個月書籍銷售資料，不過第一列不是資料，而是變項名稱。\n\n\n\n\n\n\n\n圖 3.4: booksales.csv的資料內容。畫面左方是以試算表軟體OpenOffice開啟的視窗，可見到清楚的表格標記。畫面右方是以純文字編輯器開啟的視窗，可見到逗號標記。試算表裡的每一格在csv檔是以逗號間隔。\n\n\n\n\njamovi能輕鬆開啟csv檔。按下主畫面左上角三條平行線的圖示鈕開啟面板，點選Open，再點選This Device，就能從個人電腦資料夾裡選擇要開啟的csv檔6。 圖 3.5 是以Mac作業系統展示瀏覽和選取檔案的畫面。我相信讀者應該熟悉自己使用的設備，應該能找到自己想匯入的csv檔。請放手試試看吧。\n\n\n\n\n\n\n圖 3.5: Mac作業系統的檔案瀏覽及對話視窗，相信Mac使用者都很熟悉。Windows系統有類似的檔案總管，選取檔案的操作大同小異。\n\n\n\n\n這裡有些標記提示，能幫助你開啟的csv資料檔案能順利進行分析：\n\n變項名稱。確認csv檔第一列是各變項的名稱。如同 booksales.csv 的示範。\n小數點。確認數字資料的小數點是以英文句號(.)表示。在英語系國家是慣例，但是部分如東歐國家是以英文逼號(,)表示小數點。\n引號。變項名稱與文字資料都會放在兩個引號(“)之間，jamovi會以引號自動判斷變項資料的型態。請參考 booksales.csv 的示範。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#匯入不常見格式的資料檔",
    "href": "03-Getting-started-with-jamovi.html#匯入不常見格式的資料檔",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.5 匯入不常見格式的資料檔",
    "text": "3.5 匯入不常見格式的資料檔\nThroughout this book I’ve assumed that your data are stored as a jamovi .omv file or as a “properly” formatted csv file. However, in real life that’s not a terribly plausible assumption to make so I’d better talk about some of the other possibilities that you might run into.\n\n3.5.1 資料檔是純文字格式\nThe first thing I should point out is that if your data are saved as a text file but aren’t quite in the proper csv format then there’s still a pretty good chance that jamovi will be able to open it. You just need to try it and see if it works. Sometimes though you will need to change some of the formatting. The ones that I’ve often found myself needing to change are:\n\nheader. A lot of the time when you’re storing data as a csv file the first row actually contains the column names and not data. If that’s not true then it’s a good idea to open up the csv file in a spreadsheet programme such as Open Office and add the header row manually.\nsep. As the name “comma separated value” indicates, the values in a row of a csv file are usually separated by commas. This isn’t universal, however. In Europe the decimal point is typically written as , instead of . and as a consequence it would be somewhat awkward to use , as the separator. Therefore it is not unusual to use ; instead of , as the separator. At other times, I’ve seen a TAB character used.\nquote. It’s conventional in csv files to include a quoting character for textual data. As you can see by looking at the booksales.csv file, this is usually a double quote character, “. But sometimes there is no quoting character at all, or you might see a single quote mark ’ used instead.\nskip. It’s actually very common to receive CSV files in which the first few rows have nothing to do with the actual data. Instead, they provide a human readable summary of where the data came from, or maybe they include some technical info that doesn’t relate to the data.\nmissing values. Often you’ll get given data with missing values. For one reason or another, some entries in the table are missing. The data file needs to include a “special” value to indicate that the entry is missing. By default jamovi assumes that this value is 995, for both numeric and text data, so you should make sure that, where necessary, all missing values in the csv file are replaced with 99 (or -9999; whichever you choose) before opening / importing the file into jamovi. Once you have opened / imported the file into jamovi all the missing values are converted to blank or greyed out cells in the jamovi spreadsheet view. You can also change the missing value for each variable as an option in the Data - Setup view.\n\n\n\n3.5.2 套裝軟體專用格式(如SPSS)\nThe commands listed above are the main ones we’ll need for data files in this book. But in real life we have many more possibilities. For example, you might want to read data files in from other statistics programs. Since SPSS is probably the most widely used statistics package in psychology, it’s worth mentioning that jamovi can also import SPSS data files (file extension .sav). Just follow the instructions above for how to open a csv file, but this time navigate to the .sav file you want to import. For SPSS files, jamovi will regard all values as missing if they are regarded as “system missing” files in SPSS. The ‘Default missings’ value does not seem to work as expected when importing SPSS files, so be aware of this - you might need another step: import the SPSS file into jamovi, then export as a csv file before re-opening in jamovi.6\nAnd that’s pretty much it, at least as far as SPSS goes. As far as other statistical software goes, jamovi can also directly open / import SAS and STATA files.\n\n\n3.5.3 微軟Excel資料格式\nA different problem is posed by Excel files. Despite years of yelling at people for sending data to me encoded in a proprietary data format, I get sent a lot of Excel files. The way to handle Excel files is to open them up first in Excel or another spreadsheet programme that can handle Excel files, and then export the data as a csv file before opening / importing the csv file into jamovi."
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#安裝jamovi擴充模組",
    "href": "03-Getting-started-with-jamovi.html#安裝jamovi擴充模組",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.7 安裝jamovi擴充模組",
    "text": "3.7 安裝jamovi擴充模組\njamovi的最大賣點是使用者能依實際需要，自行從擴充模組庫安排需要的模組。模組都是由jamovi社區成員熱情開發貢獻。當你有需要進行較複雜的統計分析，可以找找模組庫裡有沒有符合目的的模組。\n安裝模組的操作很簡單，只要按一下主畫面右上方的大十字，就能開啟如 圖 3.6 的清單視窗。接著如同逛購物網站一樣，找到想用的模組，按下”Install”按鈕，等幾秒後，主畫面”Analysis”面板上方顯示模組圖示，就能開始使用。推薦安裝的模組有”scatr”(安裝後加強”Descriptives”模組功能)，“lsj-data”(本書示範檔案集合，安裝後開啟檔案”Open”選單增加”Data Library”，以及\\(R_j\\)(R程式碼編譯介面)。10\n\n\n\n\n\n\n圖 3.6: jamovi擴充模組庫管理面板"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#關閉jamovi",
    "href": "03-Getting-started-with-jamovi.html#關閉jamovi",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.8 關閉jamovi",
    "text": "3.8 關閉jamovi\n本章結束前來談談如何正確關閉jamovi。操作不難，就像關閉任何應用程式一樣。但是，關閉前一定要記得存檔！統計軟體的存檔其實分成兩種：儲存更新過的資料檔；儲存資料分析結果。\n更新的資料檔最好另存新檔，保留原始檔案能避免悲劇發生。步驟是從檔案管理面板(主畫面左上角三條橫線)開啟，選擇Export，再選擇檔案格式(建議csv)，取好新檔案名稱後，按Enter鍵或點按右上方的”Export”都能完成另存新檔。\n另外，儲存為jamovi專案檔(.omv)就會包括資料及分析結果兩部分。步驟是從檔案管理面板(主畫面左上角三條橫線)開啟，選擇Save as，取好檔案名稱後，按Enter鍵或點按右上方的”Save”都能完成存檔。請記得要為存檔的資料夾找個容易找到的地方，特定的檔案用特定資料夾區分。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#本章小節",
    "href": "03-Getting-started-with-jamovi.html#本章小節",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 本章小節",
    "text": "3.9 本章小節\n到這裡還沒有真的開始處理資料，下一章才是真正的開始。\nEvery book that tries to teach a new statistical software program to novices has to cover roughly the same topics, and in roughly the same order. Ours is no exception, and so in the grand tradition of doing it just the same way everyone else did it, this chapter covered the following topics:\n\n安裝jamovi. We downloaded and installed jamovi, and started it up.\n分析模組. We very briefly oriented to the part of jamovi where analyses are done and results appear, but then deferred this until later in the book.\n資料試算表 We spent more time looking at the spreadsheet part of jamovi, and considered different variable types, and how to compute new variables.\n[Loading data in jamovi]. We also saw how to load data files in jamovi.\n[Importing unusual data files]. Then we figured out how to open other data files, from different file types.\n[Changing data from one level to another]. And saw that sometimes we need to coerce data from one type to another.\n[Installing add-on modules into jamovi]. Installing add-on modules from the jamovi community really extends jamovi capabilities.\n[Quitting jamovi]. Finally, we looked at good practice in terms of saving your data set and analyses when you have finished and are about to quit jamovi.\n\nWe still haven’t arrived at anything that resembles data analysis. Maybe the next Chapter will get us a bit closer!"
  },
  {
    "objectID": "05-Drawing-graphs.html#sec-Histograms",
    "href": "05-Drawing-graphs.html#sec-Histograms",
    "title": "5  繪製統計圖",
    "section": "5.1 直方圖",
    "text": "5.1 直方圖\n直方圖(Histograms)是製作方式最簡單且易懂的資料視覺化工具，主要用途是概覽等距尺度或比例尺度資料變項(例如 單元 4 示範的afl.margins資料集)的分佈趨勢。同學們可能在某些課程或網路媒體見過用直方圖解釋的研究證據，現在要學會如何製作直方圖。製作方式是先設定變項數值分成數個間值(bins)，接著計算每個間值之間有多少資料落入。每個間值之間的資料個數稱為次數(frequency)或密度(density)，並繪製對應高度的長條就完成了。 單元 4 的 圖 4.2 展示的直方圖最左邊的長條對應勝隊得分小於10分的場次總數，一共有33場。這份直方圖是用R語言套件製作的，超出本書的學習範圍，所以在此說明如何使用jamovi繪製接近該作品的直方圖。請開啟描述統計模組選單(Exploration-Descriptives)，將變項放到”Variable”視窗後展開最下方plot次選單，勾選histogram就會看到如 圖 5.1 的畫面。jamovi預設的直方圖y軸是密度，x軸是變項名稱。間值是jamovi自動指定，y值並不是顯示各間值之間的資料數目。雖然如此，這份直方圖已充分展示這筆資料變項的分佈形狀，你覺得這筆資料是常態分佈還是有偏態或峰度呢？分析實務對資料的第一印象就是從繪製直方圖(Histograms)開始。\n\n\n\n\n\n\n圖 5.2: 使用jamovi繪製直方圖示範畫面\n\n\n\n\n值得一提jamovi有繪製”密度”曲線的功能。只要開啟”Histogram”選項之下的”Density“方塊，並取消勾選”Histogram”，就可以得到 圖 5.3 的作品。只要資料數值是連續的，密度曲線能將分佈趨勢視覺化。密度曲線是直方圖的變形，jamvoi採用kernel smoothing演算法繪製，原理是將資料中的雜訊平滑化，使得曲線看起來平滑。使用密度曲線判斷資料變項分佈趨勢，比直方圖更佳，因為曲線不受間值設定影響。如果這份資料只用4個間值繪製直方圖，看起來絕對和用20個間值繪製的直方圖不一樣。\n\n\n\n\n\n\n圖 5.3: 用Jamovi繪製資料變項afl.margins的密度曲線。\n\n\n\n\n雖然這裡示範的直方圖和密度曲線還需要一些加工才能用在報告，至少已經為完善描述統計工作給出清楚方向。直方圖或密度曲線的最大用處是顯示資料的變異趨勢，讓分析者掌握資料特性。直方圖的劣勢是無法有效簡化資料訊息，本書稍後會展示將20多個直方圖塞在一張圖裡有多讓人眼花燎亂。最後請注意，直方圖不能將名義尺度資料視覺化。"
  },
  {
    "objectID": "05-Drawing-graphs.html#boxplots",
    "href": "05-Drawing-graphs.html#boxplots",
    "title": "5  繪製統計圖",
    "section": "5.2 Boxplots",
    "text": "5.2 Boxplots\nAnother alternative to histograms is a boxplot, sometimes called a “box and whiskers” plot. Like histograms they’re most suited to interval or ratio scale data. The idea behind a boxplot is to provide a simple visual depiction of the median, the interquartile range, and the range of the data. And because they do so in a fairly compact way boxplots have become a very popular statistical graphic, especially during the exploratory stage of data analysis when you’re trying to understand the data yourself. Let’s have a look at how they work, again using the afl.margins data as our example.\n\n\n\n\n\nFigure 5.4: A box plot of the afl.margins variable plotted in jamovi\n\n\n\n\nThe easiest way to describe what a boxplot looks like is just to draw one. Click on the ‘Box plot’ check box and you will get the plot shown on the lower right of Figure 5.4. jamovi has drawn the most basic boxplot possible. When you look at this plot this is how you should interpret it: the thick line in the middle of the box is the median; the box itself spans the range from the 25th percentile to the 75th percentile; and the “whiskers” go out to the most extreme data point that doesn’t exceed a certain bound. By default, this value is 1.5 times the interquartile range (IQR), calculated as 25th percentile - (1.5*IQR) for the lower boundary, and 75th percentile + (1.5*IQR) for the upper boundary. Any observation whose value falls outside this range is plotted as a circle or dot instead of being covered by the whiskers, and is commonly referred to as an outlier. For our AFL margins data there are two observations that fall outside this range, and these observations are plotted as dots (the upper boundary is 107, and looking over the data column in the spreadsheet there are two observations with values higher than this, 108 and 116, so these are the dots).\n\n5.2.1 Violin plots\n\n\n\n\n\nFigure 5.5: A violin plot of the afl.margins variable plotted in jamovi, also showing a box plot and data points\n\n\n\n\nA variation to the traditional box plot is the violin plot. Violin plots are similar to box plots except that they also show the kernel probability density of the data at different values. Typically, violin plots will include a marker for the median of the data and a box indicating the interquartile range, as in standard box plots. In jamovi you can achieve this sort of functionality by checking both the ‘Violin’ and the ‘Box plot’ check boxes. See Figure 5.5, which also has the ‘Data’ check box turned on to show the actual data points on the plot. This does tend to make the graph a bit too busy though, in my opinion. Clarity is simplicity, so in practice it might be better to just use a simple box plot.\n\n\n5.2.2 Drawing multiple boxplots\nOne last thing. What if you want to draw multiple boxplots at once? Suppose, for instance, I wanted separate boxplots showing the AFL margins not just for 2010 but for every year between 1987 and 2010. To do that the first thing we’ll have to do is find the data. These are stored in the aflmarginbyyear.csv file. So let’s load it into jamovi and see what is in it. You will see that it is a pretty big data set. It contains 4296 games and the variables that we’re interested in. What we want to do is have jamovi draw boxplots for the margin variable, but plotted separately for each year. The way to do this is to move the year variable across into the ‘Split by’ box, as in Figure 5.6.\n\n\n\n\n\nFigure 5.6: jamovi screen shot showing the ‘Split by’ window\n\n\n\n\nThe result is shown in Figure 5.7. This version of the box plot, split by year, gives a sense of why it’s sometimes useful to choose box plots instead of histograms. It’s possible to get a good sense of what the data look like from year to year without getting overwhelmed with too much detail. Now imagine what would have happened if I’d tried to cram 24 histograms into this space: no chance at all that the reader is going to learn anything useful.\n\n\n\n\n\nFigure 5.7: Multiple boxplots plotted in jamovi, for the margin by year variables\n\n\n\n\n\n\n5.2.3 Using box plots to detect outliers\nBecause the boxplot automatically separates out those observations that lie outside a certain range, depicting them with a dot in jamovi, people often use them as an informal method for detecting outliers: observations that are “suspiciously” distant from the rest of the data. Here’s an example. Suppose that I’d drawn the boxplot for the AFL margins data and it came up looking like Figure 5.8. It’s pretty clear that something funny is going on with two of the observations. Apparently, there were two games in which the margin was over 300 points! That doesn’t sound right to me. Now that I’ve become suspicious it’s time to look a bit more closely at the data. In jamovi you can quickly find out which of these observations are suspicious and then you can go back to the raw data to see if there has been a mistake in data entry. One way to do this is to tell jamovi to label the outliers, by checking the box next to the Box plot check box. This adds a row number label next to the outlier in the boxplot, so you can go look at that row and find the extreme value. Another, more flexible way, is to set up a filter so that only those observations with values over a certain threshold are included. In our example, the threshold is over 300, so that is the filter we will create. First, click on the ‘Filters’ button at the top of the jamovi window, and then type ‘margin > 300’ into the filter field, as in Figure 5.9.\n\n\n\n\n\nFigure 5.8: A boxplot showing two very suspicious outliers!\n\n\n\n\nThis filter creates a new column in the spreadsheet view where only those observations that pass the filter are included. One neat way to quickly identify which observations these are is to tell jamovi to produce a ‘Frequency table’ (in the ‘Exploration’ - ‘Descriptives’ window) for the ID variable (which must be a nominal variable otherwise the Frequency table is not produced). In Figure 5.10 you can see that the ID values for the observations where the margin was over 300 are 14 and 134. These are suspicious cases, or observations, where you should go back to the original data source to find out what is going on.\n\n\n\n\n\nFigure 5.9: The jamovi filter screen\n\n\n\n\n\n\n\n\n\nFigure 5.10: Frequency table for ID showing the ID numbers for the two suspicious outliers, 14 and 134\n\n\n\n\nUsually you find that someone has just typed in the wrong number. Whilst this might seem like a silly example, I should stress that this kind of thing actually happens a lot. Real world data sets are often riddled with stupid errors, especially when someone had to type something into a computer at some point. In fact, there’s actually a name for this phase of data analysis and in practice it can take up a huge chunk of our time: data cleaning. It involves searching for typing mistakes (“typos”), missing data and all sorts of other obnoxious errors in raw data files.\nFor less extreme values, even if they are flagged in a a boxplot as outliers, the decision about whether to include outliers or exclude them in any analysis depends heavily on why you think the data look they way they do and what you want to use the data for. You really need to exercise good judgement here. If the outlier looks legitimate to you, then keep it. In any case, I’ll return to the topic again in Section 12.10 in Chapter 12."
  },
  {
    "objectID": "05-Drawing-graphs.html#sec-Bar-graphs",
    "href": "05-Drawing-graphs.html#sec-Bar-graphs",
    "title": "5  繪製統計圖",
    "section": "5.3 柱狀圖",
    "text": "5.3 柱狀圖\n柱狀圖(bar graph)也是同學們常見到的資料視覺化作品，主要用來展示名義尺度變項的分佈趨勢。我們用 小單元 4.1.6 這一節示範眾數的資料變項afl.finalists，來示範如何繪製。只要將出現在變項裡的隊名擺在x軸，再將每一隊打入季後賽的次數繪製成對應高度的柱子就完成了。這筆資料有很多隊伍，我們只抓四支隊伍做個示範，他們是Brisbane, Carlton, Fremantle 以及 Richmond。請同學先點選主介面左下角的漏斗圖示，開啟jamovi的Filters功能選單，複製下列視窗內的所有文字與符號(點一下視窗右側的圖示即可複製)，再貼到Filters選單對話視窗的”=“之後，關閉選單即可生效4。\n\nafl.finalists == 'Brisbane' or afl.finalists == 'Carlton' or afl.finalists == 'Fremantle' or afl.finalists == 'Richmond'\n\n在試算表介面你會看到有的數值被反白，代表該項資料已經被過濾了。接著照舊開啟描述統計模組選單，這次勾選Bar plot選項(請記得變項要放到Variables視窗)，就會看到如 圖 5.11 的柱狀圖成品。\n\n\n\n\n\n\n圖 5.11: 只抓四支AFL球隊資料繪製的柱狀圖示範畫面。"
  },
  {
    "objectID": "05-Drawing-graphs.html#saving-image-files-using-jamovi",
    "href": "05-Drawing-graphs.html#saving-image-files-using-jamovi",
    "title": "5  繪製統計圖",
    "section": "5.4 Saving image files using jamovi",
    "text": "5.4 Saving image files using jamovi\nHold on, you might be thinking. What’s the good of being able to draw pretty pictures in jamovi if I can’t save them and send them to friends to brag about how awesome my data is? How do I save the picture? Simples. Just right click on the plot image and export it to a file, either as ‘png’, ‘eps’, ‘svg’ or ‘pdf’. These formats all produce nice images that you can then send to your friends, or include in your assignments or papers."
  },
  {
    "objectID": "05-Drawing-graphs.html#summary",
    "href": "05-Drawing-graphs.html#summary",
    "title": "5  繪製統計圖",
    "section": "5.5 Summary",
    "text": "5.5 Summary\nPerhaps I’m a simple minded person, but I love pictures. Every time I write a new scientific paper one of the first things I do is sit down and think about what the pictures will be. In my head an article is really just a sequence of pictures linked together by a story. All the rest of it is just window dressing. What I’m really trying to say here is that the human visual system is a very powerful data analysis tool. Give it the right kind of information and it will supply a human reader with a massive amount of knowledge very quickly. Not for nothing do we have the saying “a picture is worth a thousand words”. With that in mind, I think that this is one of the most important chapters in the book. The topics covered were:\n\nCommon plots. Much of the chapter was focused on standard graphs that statisticians like to produce: Histograms, Boxplots and Bar graphs\nSaving image files using jamovi. Importantly, we also covered how to export your pictures.\n\nOne final thing to point out. Whilst jamovi produces some really neat default graphics, editing the plots is currently not possible. For more advanced graphics and plotting capability the packages available in R are much more powerful. One of the most popular graphics systems is provided by the ggplot2 package (see https://ggplot2.tidyverse.org/), which is loosely based on “The grammar of graphics” (Wilkinson et al., 2006). It’s not for novices. You need to have a pretty good grasp of R before you can start using it, and even then it takes a while to really get the hang of it. But when you’re ready it’s worth taking the time to teach yourself, because it’s a much more powerful and cleaner system.\n\n\n\n\n\nWilkinson, L., Wills, D., Rope, D., Norton, A., & Dubbs, R. (2006). The grammar of graphics. Springer."
  },
  {
    "objectID": "04-Descriptive-statistics.html#集中量數",
    "href": "04-Descriptive-statistics.html#集中量數",
    "title": "4  描述統計",
    "section": "4.1 集中量數",
    "text": "4.1 集中量數\n如同 圖 4.2 展示的統計繪圖，是一種向人展現資料“要點”的絕佳方式。統計圖在精鍊資料為簡化的”總成”統計資訊相同有用。在許多實務場合的統計工作，首先要做的總成統計是計算集中量數。 也就是要呈現資料裡的“平均值”或“中位數”。以下依序介紹最常見的平均值、中位數、以及眾數，還有說明這些量數的用途。\n\n\n4.1.1 平均值\n一組資料的平均值通常指算術平均值。計算方法是將所有數值加起來，除以數值的數目。以下是拿AFL資料前五場比賽勝隊得分56, 31, 56, 8, 32，代入公式計算的平均值：\n\n\\[\n\\frac{56 + 31 + 56 + 8 + 32}{5} = \\frac{183}{5} = 36.60\n\\]\n會用這本書學習統計的同學應該對以上定義耳熟能詳。平均值(也稱平均數)在許多日常場合都會用到。儘管很多同學都熟悉如何計算，在此我們用這套公式學習一些統計學家常用的數學記號，這樣能讓我們了解如何使用jamovi進行算術運算。\n\n首先來認識數學記號\\(N\\)(大寫N)，用於表示要計算平均值的觀察值個數(以上的例子 \\(N = 5\\))。接著是表示一組觀察值的記號，慣例用大寫X表示，再加上底標數字記號就代表個別觀察值。也就是說，\\(X_1\\)是第一筆觀察值，\\(X_2\\)是第二筆觀察值，以此類推\\(X_N\\)是最後一筆觀察值。觀察值記號用比較抽象簡化的方式使用，\\(X_i\\)是資料裡的第i筆觀察值。為了幫同學了解符號的用法，我們將afl.margins的前五場比賽紀錄，連同數學記號整理在表4-1。\n\n\n\n\n\n表4-1: afl.margins的觀察資料資訊。\n\n\n\n\n觀察值資訊\n數學記號\n觀察值數值\n\n\n\n\n勝隊得分。第1場\n\\(X_1\\)\n56分\n\n\n勝隊得分。第2場\n\\(X_2\\)\n31分\n\n\n勝隊得分。第3場\n\\(X_3\\)\n56分\n\n\n勝隊得分。第4場\n\\(X_4\\)\n8分\n\n\n勝隊得分。第5場\n\\(X_5\\)\n32分\n\n\n\n[更多技術細節2]\n\n\n\n4.1.2 平均值計算示範\n好啦，以上是純數學的說明，那要如何使用jamovi幫我們完成計算工作？特別是資料變項有成千上百個觀察值，還是使用電腦計算平均值比較簡單。第一步是點選Analysis面板上的’Exploration’按鈕，再點選’Descriptives’，就會出現如同 圖 4.3 的選單畫面。要完成如同畫面中顯示的結果，只要點選左邊方框裡的afl.margins，將變項移動到”Variables”方框裡，馬上就能在報表介面看到顯示統計量數的表格。請同學自行試看看能否得到一樣的結果。\n\n\n\n\n\n\n\n圖 4.3: 2010年AFL例行賽勝隊得分的預設描述統計報告。\n\n\n\n\n圖 4.3 的報表顯示變項afl.margins的平均值是35.30。其他一起呈現的資訊有比賽場次數目(N = 176)，遺漏值數目(無)，以及中位數、最小值、最大值。\n\n\n\n4.1.3 中位數\n第二種常見的集中量數是中位數，計算方式甚至比平均值更簡單。中位數就是一組觀察值排序在中間的數值。以下同樣用AFL前五場比賽紀錄：\\(56\\), \\(31\\), \\(56\\), \\(8\\), \\(32\\)說明如何計算中位數。首先將所由數值由小到大升冪排序：\n\n8, 31, 32, 56, 56\n表面看來，以上五個數值的中位數是32，因為這個數值剛好排在中央位(以粗體字標示)。但是如果要整理的是前六場比賽紀錄的話呢？由於第6場比賽的勝隊得分是14，排序的數列如下：\n\n8, 14, 31, 32, 56, 56\n位在中間的數值有兩個，31與32。中位數就變成兩個數值的平均，也就是31.5。就像前面的例子，到這裡我們都可以用手計算。但是在真正的統計實務裡，我們不可能排序所有數值再找出中位數，還是要使用電腦完成繁複的工作。就像 圖 4.3 的示範，jamovi已經算出afl.margins變項所有數值的中位數是30.50。\n\n\n\n4.1.4 應該要計算平均值還是中位數?\n只知道怎麼計算平均值與中位數還不能下課。我們還要知道這兩種量數指出了一組資料的什麼特徵，這也能讓我們曉得各自的正確使用時機。請看 圖 4.4 的圖解。平均值顯示一筆資料的“重心”；中位數則是一筆資料的“中心點”。這也就是說，要用那種量數描述資料特徵，要看資料的種類，以及解釋這筆資料的目的是什麼。以下是簡要原則：\n\n名義尺度(nomral scale)資料不應使用平均值或中位數。因為兩種量數的計算要有意義，都有資料數值有大小順序的條件。如果數值之間的大小順序不明確，最好使用眾數。\n次序尺度(ordinal scale)資料應使用中位數而非平均值。中位數只會表示資料的順序資訊，不會指出數值間的差異比例。收集次序尺度資料之前已經知道不需測量觀察值之間的差異。只有測量尺度包括數值之間的差異時，才能使用平均值表現資料特徵。\n等距(interval)與等比(ratio)尺度資料都能使用平均值表達資料特徵。實際使用那一種尺度，全賴實務目的。平均值能掌握這類尺度測量的觀察值所有資訊特徵。不過請切記，平均值對於位於極端、遠離重心的數值非常敏感。\n\n關於最後一條，我們再多談一些。如 圖 4.4 的圖解，資料直方圖呈現非對稱時，平均值與中位數必定是不一樣的值(見偏態與峰度)。圖中右邊的直方圖內的中位數接近資料密集的“軀幹”，左邊的直方圖內中位數則落在資料稀少的”尾巴“。用實際的例子來說，有三位朋友同桌聚會，Bob年收入有50,000澳幣、Kate年收入有60,000澳幣、Jane年收入有65,000澳幣。三人的收入平均值是58,333澳幣，中位數是60,000。接著年收入高達100,000,000澳幣的Bill加入他們，平均值馬上提高到25,043,750澳幣，不過中位數只有升到62,500澳幣。你也許會覺得這一桌朋友的總收入應該能用平均值總結，但是考慮到只有四人，中位數也許是更好的選擇。\n\n\n\n\n\n\n\n圖 4.4: 圖解使用平均值與中位數解釋資料特徵的差異。平均值是一筆資料的“重心”。如果構成資料直方圖的成份是有重量的積木，平均值是能保持積木平衡不動的支點。中位數則是所有資料平分切開，代表其中一半剛好小於另一半的中間數值。\n\n\n\n\n\n\n4.1.5 真實案例\n為了弄清楚為什麼你需要注意平均值和中位數的區別，讓我們看一個現實生活中的例子。雖然我喜歡看媒體記者因為缺乏科學和統計知識而鬧的笑話，有好報導還是應該給予讚揚。請看2010年9月24日澳洲廣播公司新聞網站的這篇優秀報導3：\n\n在過去的幾個星期，澳大利亞聯邦銀行的資深高層到世界各地旅行，準備了一份簡報說明與發達程度接近的國家相比，澳大利亞房價及主要價格收入比率比較有利。『實際上過去五、六年，澳大利亞的住宅負擔能力出現偏差』聯邦銀行的首席經濟學家Craig James說道。\n\n這對於任何有按時付房貸，想要申請房貸，支付租金，或對於過去幾年澳大利亞房地產市場發生的事情全然不知情的民眾來說是重大消息。讓我們繼續看下去：\n\n澳大利亞聯邦銀行以圖表、數字還有比較各國狀況，反擊房屋末日論。銀行官員用這份簡報裡的數據，否定與澳大利亞一般家庭收入比較，住宅負擔相對昂貴的論點。簡報裡提到澳洲主要城市的房屋價格與家庭收入的比值為5.6，全國平均為4.3，與其他已開發國家的城市相比，美國舊金山和紐約的比值都是7，紐西蘭奧克蘭6.7，加拿大溫哥華9.3。\n\n聽起來很棒! 不過這篇報導接下來評論了簡報內容：\n\n許多分析家指出聯邦銀行誤用了圖表及數字，做出的結論是錯誤的。看一下簡報第四頁的表格，表格註腳提到其他國家的數值是來自一個網站——國際住宅負擔調查報告。然而，如果聯邦銀行也引用該網站對澳大利亞房價與收入比值的分析，數字應該是9，而不是5.6或4.3。\n\n嗯，出現一個相當嚴重的歧異。一群專家說是9，另一群專家說是4或5。我們能說雙方都有理，宣稱真實數字就在兩者之間？絕對不行！房價和收入的調查結果只會有一個。正如報導接著指出，國際住宅負擔調查報告的數字是正確的，澳洲聯邦銀行提供的數字是錯誤的：\n\n澳洲聯邦銀行提供的國內房價與收入數字存在一個明顯的問題，他們比較的是收入平均值與房價中位數（國際住宅負擔調查報告比較的是收入中位數與房價中位數）。中位數是數據資料中間的點，能有效剔除了最高數值和最低數值，這意味著當涉及到收入和資產價格時，平均值會被拉高，因為整體收入數據包括澳大利亞最有錢的人。換句話說：聯邦銀行計算的收入數據包括執行長拉爾夫-諾里斯（Ralph Norris）的數百萬美元年薪，但是沒有把他的豪宅算在房產價格數據裡，因此低估了澳大利亞中產受薪階級的房價與收入比。\n\n這段報導還是我自己(原作者)來寫好了。“(簡單說，)國際住宅負擔調查報告的比值計算方式是正確的，但是聯邦銀行的算法是錯的。”至於為什麽經常處理複雜數據的大銀行會犯這種低級錯誤，嗯…我沒什麼話可說，因為我對這間機構沒什麼了解。不過這篇報導剛好提到一個事實，可能與銀行出這種包有關，也可能無關:\n\n作為澳大利亞最大的房屋貸款機構，房價上漲會讓聯邦銀行有最大獲益。全澳大利亞大多數購屋族以及小型企業都是向聯邦銀行貸款。\n\n同學們，這就是社會現實呀！\n\n\n\n\n4.1.6 眾數\n從一組樣本找出眾數非常容易，就是個數最多的數值是也。我們用另一個AFL資料變項說明：那個球隊在歷年季後賽出場次數最多？請參考 圖 4.5 開啟AFL Finallist這個檔案，看一下變項afl.finalists。這個變項的數值是包括1987到2010年200場季後賽的400支球隊隊名。\n大家可以直接掃過全部400項，然後計數每支球隊的出場次數，如此能製造一張次數表(frequency table)。不過這樣的工作相當耗神且無聊，還是交給電腦吧。同樣也是點選開啟”Exploration”的“Descriptives”選單，這次要勾選”Frequency tables”這個方塊。如同 圖 4.6 的示範。\n從次數表可以看到，在這24年的季後賽，Geelong隊的出場次數遠多其他球隊，所以變項afl.finalists的眾數是Geelong。眾數說明Geelong(39次)是1987到2010季後賽出場最多的球隊，也能看到”Descriptives”表格並沒有呈現平均值、中位數、最小值、還有最大值。這是因為變項afl.finalists是名義尺度資料，無法計算這些量數。\n\n\n\n\n\n\n\n圖 4.5: aflsmall finalists.csv匯入jamovi的變項顯示畫面。\n\n\n\n\n\n\n\n\n\n圖 4.6: 變項afl.finalists的次數表在jamovi的輸出畫面。\n\n\n\n\n關於眾數還有一點要了解。儘管眾數常用來計算名義尺度資料的集中量數，是因為這類資料無法計算平均值和中位數，還是有些情況是不論資料尺度是什麼，我們都想知道眾數。我們回頭看前面的示範資料變項afl.margins，這是等比尺度變項，通常我們用平均值或中位數代表這筆資料的集中量數。不過想想這個情況：某天你的朋友想買運動彩卷賭下一場比賽的勝隊是幾分。若是不知道下一場是那隊出賽，只能隨便猜任何一個贏球分數。如果賭對了就能拿到50澳幣，賭輸了買彩卷的一塊就沒了。彩卷規則沒有所謂的“幾乎猜中”，只能賭勝隊實際會贏幾分。這時平均值或中位數都不能幫你朋友做決定，只有眾數才有參考價值。這時可以回到afl.margins的jamovi檔案，我們在”Exploration”的“Descriptives”選單勾選”Mode”，就能在描述統計報表看到眾數是3，如同 圖 4.7 的示範。\n\n\n\n\n\n\n\n圖 4.7: 計算變項afl.margins眾數的jamovi示範畫面。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#偏態與峰度",
    "href": "04-Descriptive-statistics.html#偏態與峰度",
    "title": "4  描述統計",
    "section": "4.3 偏態與峰度",
    "text": "4.3 偏態與峰度\n有些心理學文獻還會報告兩項描述統計項目：偏態(skew)與峰度(kurtosis)。實務上任何一項出現在報告裡的機會，都不比以上討論過的集中量數與變異量數。偏態稍微重要，所以會在某些領域的文獻經常看到(例如反應時間的測量)，峰度則相當罕見。\n\n\n\n\n\n\n圖 4.11: 圖解資料分佈的偏態類型。左圖是負偏態，中間是無偏態，右圖是正偏態。\n\n\n\n\n在此我們多認識一些偏態在解讀資料的意義。偏態是解釋資料不對稱程度的最常用指標。以 圖 4.11 的展示來看，負偏態是多數資料的數值偏高所造成；而正偏態則是多數資料的數值偏低才會出現的現象。因此一筆資料大於平均值的數值佔多數，分析者會自然想到資料分佈是正偏態，偏態指數會是正值；相反的情況就會是負偏態，偏態指數會是負值。資料是對稱分佈的偏態指數則為0。\n[更多技術細節13]\n同學們也可以使用jamovi計算偏態，同樣在描述統計模組選單就有”skweness”可以勾選。資料變項afl.margins的偏態指數是0.780。使用偏態指數除以標準誤(Std. error)得到的數值，可以表示一筆資料的“偏態程度”。有個簡易判斷法則(特別是個數少於50的資料)：這個數值小於或等於2低表示偏態並不明顯，大於2就表示偏態程度會影響統計分析的結果。雖然這樣的判斷法則還不是學界共識，不過AFL的這筆資料偏態程度是 \\(\\frac{0.780}{0.183} = 4.262\\) 。\n峰度的報告雖然相當罕見，不過還是值得了解一下。峰度指標是描述資料分佈曲線的寬窄程度，如同 圖 4.12 的圖解，常態分佈曲線的峰度指數是0，峰度指數是正是負顯示曲線是低闊峰(左圖)或高狹峰(右圖)。\n\n\n\n\n\n\n圖 4.12: 圖解資料分佈的峰度。左圖的低闊峰分佈呈現資料往兩側分佈(峰度指數= -.95)；中間的常峰態分佈呈現資料分佈貼合常態分佈曲線(峰度指數= 0)；右圖的高狹峰分佈呈現資料往中央集中(峰度指數= 2.12)。峰度指數是根據各資料點偏離圖中黑色曲線的程度估算。\n\n\n\n\n圖 4.12 的左圖是平坦的資料分佈，稱為”低闊峰”是因為資料往雙側分佈而造成，峰度指數是負值。右圖的資料分佈兩側很寬，稱為”高狹峰”是因為資料往雙側分佈而造成，峰度指數是正值。中間的資料分佈不窄也不寬，所以稱為”常峰態”，峰度指數是0。 表4-4 整理三種峰度的稱呼與及指數數值說明：\n\n\n\n\n\n表4-4: 峰度的稱呼與指數數值說明\n\n\n\n\n口語\n術語\n峰度指數\n\n\n\n\n分佈尾端太廋\n低闊峰(platykurtic)\n指數為負值\n\n\n分佈尾端均勻\n常態峰(mesokurtic)\n指數為零\n\n\n分佈尾端太肥\n高狹峰(leptokurtic)\n指數為正值\n\n\n\n[更多技術細節14]\n最後不免俗地提醒同學，jamovi的描述統計模組選單有計算峰度“kurtosis”的選項，只要開啟就會在報表看到AFL資料變項的峰度是0.101，除以標準誤的話是0.364，嚴格來說不算是”高狹峰”。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#分組描述統計",
    "href": "04-Descriptive-statistics.html#分組描述統計",
    "title": "4  描述統計",
    "section": "4.4 分組描述統計",
    "text": "4.4 分組描述統計\n往後許多實例要根據某個分組變項，對其他變項進行分組描述統計。使用jamovi分組描述統計只是小菜一碟。例如根據臨床治療方式，對收集到的臨床試驗資料進行分組描述統計。在這一節我們用另一批資料學習。請開啟學習資料庫裡的”Clinical Trial”資料集，之後在 單元 13 會詳細這一批資料的來歷。開啟後的試算表介面如同 圖 4.13 :\n先看一下名義變項drug的數值：有安慰劑(placebo)，還有”anxifree”和“joyzepam”，三種藥各有六個受測者。另一個名義變項therapy代表治療方法，其中有九位受測者接受認知行為治療(CBT)，另外九位並未接受任何心理治療。比照之前的操作，在描述統計模組選單裡，單獨將連續尺度變項mood.gain放到Variables框裡，就會看到報表介面出現這個變項的平均值0.88。雖然我們還不知道這個變項的測量尺度是什麼意義，至少看起來能做些統計工作。\n接著可以試試看分組描述統計，請將therapy放到”Split by”，並且勾選Std. deviation, Skewness, Kurtosis等項目，看看會不會出現如同 圖 4.14 的結果。\n\n\n\n\n\n\n圖 4.13: 匯入 clinicaltrial.csv 的jamovi試算表介面擷圖。\n\n\n\n\n如果用不只一種分組變項會看到什麼結果呢？試試組合drug與theropy兩個變項的分組描述統計結果會是什麼。只要將drug放到”Split by”就知道了。分組描述統計的操作很容易，不過如果資料不多，分析結果很難說有什麼意義。有時候因為資料太少，報表會出現”NaN”或“Inf”等記號15。\n\n\n\n\n\n\n圖 4.14: 使用jamovi以治療方式進行分組描述統計的擷圖。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#本章小結",
    "href": "04-Descriptive-statistics.html#本章小結",
    "title": "4  描述統計",
    "section": "4.6 本章小結",
    "text": "4.6 本章小結\n任何資料分析作業都是從最基本的描述統計開始，而且比起推論統計學習門檻較低。因此本書比照其他教科書由描述統計開始。這一章的各主題有以下重點：\n\n集中量數 集中量數顯示資料在幾何空間的所在位置。各種論文通常會報告平均值、中位數或眾數其中一種集中量數。\n變異量數 變異量數顯示資料在幾何空間的離散範圍。常用的變異量數有全距、四分位數間距、平均絕對差、變異數、標準差。\n偏態與峰度 分析作業也會關切變項資料分配的對稱程度(偏態)，以及寬窄程度(峰度)。\n分組描述統計 分組統計是jamovi的基本功能，值得學習如何設定分組用變項。\n標準分數 z分數放在這一章有些微妙，因為能用於描述統計，也能用於推論統計。請務必學好這一節的示範重點，之後的章節還會用到。\n\n下一章我們將學習如何繪製統計圖，似乎是輕鬆有趣的單元。結束前我想提醒一點，傳統統計課給描述統計的講授時間比例，大約只有推論統計的1/8或1/9，這是因為後者內容比較複雜且難學。不過這樣的安排很容易讓同學輕忽安排易讀的描述統計，也是重要的實務工作。請切記…"
  },
  {
    "objectID": "Preface.html",
    "href": "Preface.html",
    "title": "前言",
    "section": "",
    "text": "This book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA."
  },
  {
    "objectID": "04-Descriptive-statistics.html",
    "href": "04-Descriptive-statistics.html",
    "title": "4  描述統計",
    "section": "",
    "text": "Any time that you get a new data set to look at one of the first tasks that you have to do is find ways of summarising the data in a compact, easily understood fashion. This is what descriptive statistics (as opposed to inferential statistics) is all about. In fact, to many people the term “statistics” is synonymous with descriptive statistics. It is this topic that we’ll consider in this chapter, but before going into any details, let’s take a moment to get a sense of why we need descriptive statistics. To do this, let’s open the aflsmall_margins file and see what variables are stored in the file, see Figure 4.1.\nIn fact, there is just one variable here, afl.margins. We’ll focus a bit on this variable in this chapter, so I’d better tell you what it is. Unlike most of the data sets in this book, this is actually real data, relating to the Australian Football League (AFL).1 The afl.margins variable contains the winning margin (number of points) for all 176 home and away games played during the 2010 season.\nThis output doesn’t make it easy to get a sense of what the data are actually saying. Just “looking at the data” isn’t a terribly effective way of understanding data. In order to get some idea about what the data are actually saying we need to calculate some descriptive statistics (this chapter) and draw some nice pictures (Chapter 5). Since the descriptive statistics are the easier of the two topics I’ll start with those, but nevertheless I’ll show you a histogram of the afl.margins data since it should help you get a sense of what the data we’re trying to describe actually look like, see Figure 4.2. We’ll talk a lot more about how to draw histograms in Section 5.1 in the next chapter. For now, it’s enough to look at the histogram and note that it provides a fairly interpretable representation of the afl.margins data."
  },
  {
    "objectID": "05-Drawing-graphs.html",
    "href": "05-Drawing-graphs.html",
    "title": "5  繪製統計圖",
    "section": "",
    "text": "Above all else show the data.\n– Edward Tufte1\nVisualising data is one of the most important tasks facing the data analyst. It’s important for two distinct but closely related reasons. Firstly, there’s the matter of drawing “presentation graphics”, displaying your data in a clean, visually appealing fashion makes it easier for your reader to understand what you’re trying to tell them. Equally important, perhaps even more important, is the fact that drawing graphs helps you to understand the data. To that end, it’s important to draw “exploratory graphics” that help you learn about the data as you go about analysing it. These points might seem pretty obvious but I cannot count the number of times I’ve seen people forget them.\nTo give a sense of the importance of this chapter, I want to start with a classic illustration of just how powerful a good graph can be. To that end, Figure 5.1 shows a redrawing of one of the most famous data visualisations of all time. This is John Snow’s 1854 map of cholera deaths. The map is elegant in its simplicity. In the background we have a street map which helps orient the viewer. Over the top we see a large number of small dots, each one representing the location of a cholera case. The larger symbols show the location of water pumps, labelled by name. Even the most casual inspection of the graph makes it very clear that the source of the outbreak is almost certainly the Broad Street pump. Upon viewing this graph Dr Snow arranged to have the handle removed from the pump and ended the outbreak that had killed over 500 people. Such is the power of a good data visualisation.\nThe goals in this chapter are twofold. First, to discuss several fairly standard graphs that we use a lot when analysing and presenting data, and second to show you how to create these graphs in jamovi. The graphs themselves tend to be pretty straightforward, so in one respect this chapter is pretty simple. Where people usually struggle is learning how to produce graphs, and especially learning how to produce good graphs. Fortunately, learning how to draw graphs in jamovi is reasonably simple as long as you’re not too picky about what your graph looks like. What I mean when I say this is that jamovi has a lot of very good default graphs, or plots, that most of the time produce a clean, high-quality graphic. However, on those occasions when you do want to do something non-standard, or if you need to make highly specific changes to the figure, then the graphics functionality in jamovi is not yet capable of supporting advanced work or detail editing."
  },
  {
    "objectID": "06-Pragmatic-matters.html",
    "href": "06-Pragmatic-matters.html",
    "title": "6  實務課題",
    "section": "",
    "text": "The garden of life never seems to confine itself to the plots philosophers have laid out for its convenience. Maybe a few more tractors would do the trick.\n– Roger Zelazny1\nThis is a somewhat strange chapter, even by my standards. My goal in this chapter is to talk a bit more honestly about the realities of working with data than you’ll see anywhere else in the book. The problem with real world data sets is that they are messy. Very often the data file that you start out with doesn’t have the variables stored in the right format for the analysis you want to do. Sometimes there might be a lot of missing values in your data set. Sometimes you only want to analyse a subset of the data. Et cetera. In other words, there’s a lot of data manipulation that you need to do just to get the variables in your data set into the format that you need it. The purpose of this chapter is to provide a basic introduction to these pragmatic topics. Although the chapter is motivated by the kinds of practical issues that arise when manipulating real data, I’ll stick with the practice that I’ve adopted through most of the book and rely on very small, toy data sets that illustrate the underlying issue. Because this chapter is essentially a collection of techniques and doesn’t tell a single coherent story, it may be useful to start with a list of topics:\nAs you can see, the list of topics that the chapter covers is pretty broad, and there’s a lot of content there. Even though this is one of the longest and hardest chapters in the book, I’m really only scratching the surface of several fairly different and important topics. My advice, as usual, is to read through the chapter once and try to follow as much of it as you can. Don’t worry too much if you can’t grasp it all at once, especially the later sections. The rest of the book is only lightly reliant on this chapter so you can get away with just understanding the basics. However, what you’ll probably find is that later on you’ll need to flick back to this chapter in order to understand some of the concepts that I refer to here."
  },
  {
    "objectID": "06-Pragmatic-matters.html#sec-Tabulating-and-cross-tabulating-data",
    "href": "06-Pragmatic-matters.html#sec-Tabulating-and-cross-tabulating-data",
    "title": "6  實務課題",
    "section": "6.1 製作次數表及列聯表",
    "text": "6.1 製作次數表及列聯表\n幾乎每項統計分析實務都要建立次數表(frequency table)或資料變項列聯表(cross tabulation)。這一節示範如何運用jamovi完成。\n\n\n\n6.1.1 製表示範\n此處以原作者某天晚上照顧剛出生的小孩的時候，偶然看了一集深夜節目，隨手紀錄節目中出現的角色及台詞，整理成”說話者(speaker)“與”台詞(utterance)“兩個變項。這份記錄存於jamovi示範檔案Night Garden，開啟後兩個變項的內容如下：\n\n資料變項’說話者’: upsy-daisy upsy-daisy upsy-daisy upsy-daisy tombliboo tombliboo makka-pakka makka-pakka makka-pakka makka-pakka 資料變項’台詞’: pip pip onk onk ee oo pip pip onk onk\n那一晚原作者的腦袋發生了什麽事？讓我們將每個角色的說話次數做成表格吧，只要用將資料變項’speaker’放到jamovi述統計模組選單的Variables視窗，再勾選啟動’Frequency tables’，報表介面就會出現像是 表 6.1 的次數表。\n\n\n\n\n\n\n表 6.1: 資料變項’speaker’的次數表\n\n\n角色名字\n發言次數\n發言百分比\n累積百分比\n\n\n\n\nmakka-pakka\n4\n40\n40\n\n\ntombliboo\n2\n20\n60\n\n\nupsy-daisy\n4\n40\n100\n\n\n\n\n\n\n\n\n次數表表格第一行是speaker資料變項的各種基本統計資料的欄位名稱。像是”Levels”這欄列出紀錄在變項裡的所有角色名字，“Counts”這欄之中的數字是每位角色說了幾次台詞。\njamovi的”Frequency tables”功能只能製作一個變項的次數表。若是要製作能展示兩個變項的列聯表，像是計算每個角色’speaker’講了各種台詞’utterance’幾次，就要使用Frequencies模組之中的Contigency Tables-Independent Samples功能。功能選單如同 圖 6.1 ，請按照畫面範，製造兩個變項的列聯表。\n\n\n\n\n\n\n\n圖 6.1: 資料變項speaker 與 utterances的列聯表示範畫面。\n\n\n\n\n各位先不必管報表最下面的”\\(\\chi^2\\) 檢定”，這是 單元 10 的學習項目。解讀列聯表的關鍵是表格中每個數字是計數，像是第一列第二欄的”2”是指Makka-Pakka這個角色說了”onk”這個台詞2次。\n\n\n\n\n6.1.2 列聯表裡的百分比\n圖 6.1 的列聯表呈現的是原始計數，也就是兩個變項內各層次資料組合的總次數。不過統計實務通常也要呈現次數百分比。在這個例子，只要從’Contingency Tables’功能選單下的’Cells’次選單勾選啟動Percentages之下的方塊。 圖 6.2 示範啟動Row，自動計算每個原始計數在各列之內的百分比。\n\n\n\n\n\n\n\n圖 6.2: 列聯表呈現資料變項speaker 與 utterances的次數及列內百分比。\n\n\n\n\n列內百分比讓我們知道每位角色說的台詞次數百分比，所以Makka-Pakka講的台詞有50%是”pip”，另外的50%是”onk”。若是改成計算每欄之內的百分比(取消Row並改成Column)，列聯表就會變成如 圖 6.3 的樣子。這樣的列聯表告訴我們那個台詞被那些角色講了幾次。像是”ee” 100% 是Tombliboo講的。\n\n\n\n\n\n\n\n圖 6.3: 列聯表呈現資料變項speaker 與 utterances的次數及欄內百分比。"
  },
  {
    "objectID": "06-Pragmatic-matters.html#logical-expressions-in-jamovi",
    "href": "06-Pragmatic-matters.html#logical-expressions-in-jamovi",
    "title": "6  實務課題",
    "section": "6.2 Logical expressions in jamovi",
    "text": "6.2 Logical expressions in jamovi\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 Assessing mathematical truths\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 Logical operations\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 Applying logical operation to text\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable",
    "href": "06-Pragmatic-matters.html#sec-Transforming-and-recoding-a-variable",
    "title": "6  實務課題",
    "section": "6.3 資料變項的轉換與編碼",
    "text": "6.3 資料變項的轉換與編碼\n\n在現實世界的數據分析中,發現您的一個變量與您真正需要的變量不完全等價的情况并不罕見。例如,通常將一個連續值變量(例如年齡)分解為數量較少的類別(例如更年輕、中等和更老)很方便。在其他時候,您可能需要將數值變量轉換為不同的數值變量(例如,您可能希望分析原始變量的絕對值)。在本節中,我將描述您可以在 jamovi 中完成這些操作的一些關鍵方法。\n\n6.3.1 轉換資料數值\n\n首先要討論的技巧是轉換變項的概念。從字面上看,對變項執行的任何操作都是轉換,但在實踐中,它通常的意思是您對原始變項應用相對簡單的數學函數以創建新變項,新變項要么(a)以更好的方式描述您實際感興趣的事物,要么(b)與您想要執行的統計測試的假設更為一致。由于在這一階段我還沒有談到統計測試或它們的假設,所以我將給您展示第一種情况的示例。\n假設我進行了一項短期研究,向10人提出了一個問題:\n在1(強烈不同意)到7(強烈同意)的量表上,您在多大程度上同意“恐龍很棒”的主張?\n現在讓我們加載資料并查看。omv檔案likert.omv包含了這10個人的單個變項,其中包含原始李克特量表響應。然而,仔細想想,這並不是表示這些響應的最佳方式。由于我們建立響應量表的相當對稱的方式,在某種意義上,量表的中位數應編碼為0(沒有意見),而兩個端點應為\\`3(強烈同意)和 ́3(強烈不同意)。通過這種方式重新編碼資料,它在一定程度上更能反映我們對響應的真實想法。這裡重新編碼很簡單,只需從原始分數中減去4。在jamovi中,您可以通過計算新變項來完成此操作:點擊“資料”- “計算”按鈕,您將看到電子表格中添加了一個新變項。讓我們將此新變項命名為likert.centred(前往並輸入該名稱),然后在公式框中添加以下內容,如 圖 6.4 所示:“likert.raw - 4”\n\n\n\n\n\n圖 6.4: 在 jamovi 中創建新的計算變項\n\n\n\n\n\n以這種格式保存資料的一個原因是,在很多情况下,您可能更喜歡單獨分析意見的力度而不是意見的方向。我們可以對這個likert.centred變項執行兩種不同的轉換,以區分這兩個不同的概念。首先,為了計算opinion.strength變項,我們想采取中心化資料的絕對值(使用ABS函數)。6在jamovi中,使用“計算”按鈕再創建一個新變項。將變項命名為opinion.strength,這次點擊公式框旁邊的fx按鈕。這將顯示可以添加到“公式”框中的不同“函數”和“變量”,所以雙擊“ABS”,然後雙擊“likert.centred”,您將看到“公式”框被填充為ABS(likert.centred),並且在電子表格視圖中創建了一個新變項,如 圖 6.5 所示。\n\n\n\n\n\n圖 6.5: 使用 \\(f\\_x\\) 按鈕選擇函數和變項\n\n\n\n\n\n其次,為了計算仅包含意見方向並忽略力度的變項,我們想計算變量的“符號”。在jamovi中,我們可以使用IF函數來實現。使用“計算”按鈕再創建一個新變量,將此變量命名為opinion.sign,然後在函數框中鍵入以下內容:\nIF(likert.centred \\(==\\) 0, 0, likert.centred / opinion.strength) 完成後,您會看到likert.centred變量中的所有負數都轉換為-1,所有正數都轉換為1,零保持為0,如下所示:\n-1 1 -1 0 0 0 -1 1 1 1\n讓我們分解這個‘IF’命令的工作原理。在jamovi中,’IF’語句有三個部分,寫為‘IF(expression,value,else)’。第一部分“expression”可以是邏輯或數學語句。在我們的示例中,我們已經指定了‘likert.centred \\(==\\) 0’,這對於likert.centred為零的值為真。下一部分“value”是第一部分中的運算式為真時的新值。在我們的示例中,對於所有likert.centred為零的值,我們說保持它們為零。在下一部分“else”中,我們可以輸入另一個邏輯或數學語句,如果第一部分評估為假,即likert.centred不為零,則使用該語句。在我們的示例中,我們將likert.centred除以opinion.strength以給出“-1”或“+1”,具體取決於likert.centred中原始值的符號。7\n就這些了。我們現在有三個閃亮的新變量,它們都是原始likert.raw資料的有用轉換。\n\n\n6.3.2 轉換連續變項為間斷變項\n\n\n一個非常實用的任務是將變項歸併為更少數量的離散層次或類別的問題。例如,假設我有興趣查看社交聚會上人們的年齡分佈:\n60,58,24,26,34,42,31,30,33,2,9\n在某些情况下,將這些資料分組為少量類別會非常有幫助。例如,我們可以將資料分為三大類:年輕(0-20)、成年(21-40)和老年(41-60)。這是一種相當粗糙的分類,並且我給出的標籤僅在本資料集上下文中有意義(例如,從更廣泛的角度來看,42歲的人不會認為自己是“老年人”)。我們可以使用已經使用過的 jamovi ’IF’函數輕鬆地劃分這個變項。這次我們必須指定巢狀的 ’IF’語句,這僅僅意味著如果第一個邏輯運算式為 TRUE,則插入第一個值,但如果第二個邏輯運算式為 TRUE,則插入第二個值,但是如果第三個邏輯運算式為 TRUE,則插入第三個值。這可以寫為:\nIF(Age &gt;= 0 和 Age &lt;= 20,1,IF(Age &gt;= 21 和 Age &lt;= 40,2,IF(Age &gt;= 41 和 Age &lt;= 60,3)))\n請注意,巢狀使用了三個左括號,所以整個語句必須以三個右括號結束,否則您將得到錯誤消息。 這個資料操作的jamovi截圖以及隨附的頻率表如 圖 6.6 所示。\n\n\n\n\n\n圖 6.6: 使用 jamovi“IF”函數將變量歸併為更少數量的離散層次\n\n\n\n\n\n\n花時間確定結果類別在您的研究項目中是否有任何意義非常重要。 如果對您來說這些類別並沒有任何有意義的區分,那麼使用這些類別的任何資料分析可能同樣沒有意義。 更廣泛地說,在實踐中,我注意到人們非常渴望將他們的(連續和混亂的)資料劃分為少量(離散和簡單的)類別,然後使用分類後的資料而不是原始資料進行分析。8 我不會說這本身就是一個糟糕的主意,但有時確實有一些相當嚴重的缺點,所以如果您正在考慮這樣做,我會建議您保持謹慎。\n\n\n6.3.3 設計用途多重的轉換功能\n\n有時您想對多個變量應用相同的轉換,例如,當您有多個問卷項目都需要以相同的方式重新計算或重新編碼時。jamovi 的一個很棒的功能是,您可以使用 “資料” - “轉換”按鈕創建轉換,然后將其保存並應用於多個變量。讓我們回到上面的第一個示例,使用包含10人原始李克特量表響應的單個變量的 omv 檔案 likert.omv。要創建可以保存然后應用於多個變量的轉換(假設您的数据檔中有更多这样的變量),首先在电子表格編輯器中選擇(即點擊)您要用於最初創建轉換的變量。 在我們的示例中,這是 likert.raw。 下一步,點擊 jamovi“資料”功能區中的“轉換”按鈕,您會看到類似 圖 6.7 的界面。\n給您的新變量命名,我們將其命名為 opinion.strength,然后點擊“使用轉換”選擇框,并選擇“創建新轉換…”。這是您將創建和命名可以重新應用於任意多個變量的轉換的地方。 該轉換會自動為我們命名為“轉換1”(有創意,哈哈。如果您願意,可以更改此名稱)。 然后在函數文本框中鍵入運算式“ABS($source - 4)”,如 圖 6.8 所示,在鍵盤上按 Enter 或 Return 鍵,耶,您已經創建了一個新轉換并將其應用於 likert.raw 變量! 不錯吧。 請注意,在運算式中我們没有使用變量標籤,而是使用了“$source”。 這樣我們就可以將相同的轉換與任意多的不同變量一起使用 - jamovi 要求您使用“$source”來引用您正在轉換的源變量。 您的轉換也已經保存並可以在任何時候重用(只要您將資料集保存為“.omv”檔,否則您會失去它!)。\n您也可以通過我們查看的第二個示例創建轉換,即社交聚會上人們的年齡分佈。 前往吧,我知道你想試試! 記住我們將這個變量歸併為三組:年輕、成年和老年。 这次我們將達到相同的目的,但是使用 jamovi 的“轉換” - “添加條件”按鈕。 使用這個資料集(返回或如果您没有保存请重新創建),設置一個新的變量轉換。 將轉換後的變量命名為 AgeCats,您將創建的轉換命名為 Agegroupings。 然后點擊函數框旁邊的大“\\(+\\)”符號。 這是“添加條件”按鈕,我在 圖 6.9 中加上了一個大紅色箭頭,以便您可以準確看到這在哪裡。 重新創建 圖 6.9 所示的轉換,完成后,您將在電子表格窗口中看到出現新值。 更重要的是,Age groupings 轉換已經保存並可以在任何時候重用。 好吧,所以我知道您不太可能有多於一個“年齡”變量,但是您現在已經了解了如何在 jamovi 中設置轉換,所以您可以跟隨這一想法處理其他類型的變量。 這種典型的情況是,當您有一個問卷量表,比如有 20 個項目(變量),並且每個項目最初得分從 1 分到 6 分,但是由於某些原因或數據的怪癖,您決定將所有項目重新編碼為 1 到 3。 您可以通過在 jamovi 中創建然后為您要重新編碼的每個變量重新應用您的轉換來輕鬆完成此操作。\n\n\n\n\n\n圖 6.7: 使用 jamovi“轉換”命令創建新變項轉換\n\n\n\n\n\n\n\n\n\n圖 6.8: 在 jamovi 中指定轉換以保存為名為“轉換 1”的轉換\n\n\n\n\n\n\n\n\n\n圖 6.9: 使用jamovi內建函式 將變項轉換為三個年齡類別,使用“添加條件”按鈕\n\n\n\n\n\n\n\n\n表 6.5: 一些數學運算示範\n\n\nX.nbsp.\n函式\n輸入範例\n輸出\n\n\n\n\n開平方根\nSQRT(x)\nSQRT(25)\n5\n\n\n求絕對值\nABS(x)\nABS(-23)\n23\n\n\n求對數(底數為10)\nLOG10(x)\nLOG10(1000)\n3\n\n\n求對數(底數為自然數e)\nLN(x)\nLN(1000)\n6.91\n\n\n求自然指數\nEXP(x)\nEXP(6.908)\n1e+03\n\n\nBox-Cox轉換\nBOXCOX(x, lamda)\nBOXCOX(6.908, 3)\n110"
  },
  {
    "objectID": "06-Pragmatic-matters.html#a-few-more-mathematical-functions-and-operations",
    "href": "06-Pragmatic-matters.html#a-few-more-mathematical-functions-and-operations",
    "title": "6  實務課題",
    "section": "6.4 A few more mathematical functions and operations",
    "text": "6.4 A few more mathematical functions and operations\nIn the section on Transforming and recoding a variable I discussed the ideas behind variable transformations and showed that a lot of the transformations that you might want to apply to your data are based on fairly simple mathematical functions and operations. In this section I want to return to that discussion and mention several other mathematical functions and arithmetic operations that are actually quite useful for a lot of real world data analysis. Table 6.5 gives a brief overview of the various mathematical functions I want to talk about here, or later.9 Obviously this doesn’t even come close to cataloguing the range of possibilities available, but it does cover a range of functions that are used regularly in data analysis and that are available in jamovi.\n\n6.4.1 Logarithms and exponentials\nAs I’ve mentioned earlier, jamovi has an useful range of mathematical functions built into it and there really wouldn’t be much point in trying to describe or even list all of them. For the most part, I’ve focused only on those functions that are strictly necessary for this book. However I do want to make an exception for logarithms and exponentials. Although they aren’t needed anywhere else in this book, they are everywhere in statistics more broadly. And not only that, there are a lot of situations in which it is convenient to analyse the logarithm of a variable (i.e., to take a “log-transform” of the variable). I suspect that many (maybe most) readers of this book will have encountered logarithms and exponentials before, but from past experience I know that there’s a substantial proportion of students who take a social science statistics class who haven’t touched logarithms since high school, and would appreciate a bit of a refresher.\nIn order to understand logarithms and exponentials, the easiest thing to do is to actually calculate them and see how they relate to other simple calculations. There are three jamovi functions in particular that I want to talk about, namely LN(), LOG10() and EXP(). To start with, let’s consider LOG10(), which is known as the “logarithm in base 10”. The trick to understanding a logarithm is to understand that it’s basically the “opposite” of taking a power. Specifically, the logarithm in base 10 is closely related to the powers of 10. So let’s start by noting that 10-cubed is 1000. Mathematically, we would write this:\n\\[10^3=1000\\]\nThe trick to understanding a logarithm is to recognise that the statement that “10 to the power of 3 is equal to 1000” is equivalent to the statement that “the logarithm (in base 10) of 1000 is equal to 3”. Mathematically, we write this as follows,\n\\[log_{10}(1000)=3\\]\nOkay, since the LOG10() function is related to the powers of 10, you might expect that there are other logarithms (in bases other than 10) that are related to other powers too. And of course that’s true: there’s not really anything mathematically special about the number 10. You and I happen to find it useful because decimal numbers are built around the number 10, but the big bad world of mathematics scoffs at our decimal numbers. Sadly, the universe doesn’t actually care how we write down numbers. Anyway, the consequence of this cosmic indifference is that there’s nothing particularly special about calculating logarithms in base 10. You could, for instance, calculate your logarithms in base 2. Alternatively, a third type of logarithm, and one we see a lot more of in statistics than either base 10 or base 2, is called the natural logarithm, and corresponds to the logarithm in base e. Since you might one day run into it, I’d better explain what e is. The number e, known as Euler’s number, is one of those annoying “irrational” numbers whose decimal expansion is infinitely long, and is considered one of the most important numbers in mathematics. The first few digits of e are:\n\\[e = 2.718282 \\]\nThere are quite a few situation in statistics that require us to calculate powers of \\(e\\), though none of them appear in this book. Raising e to the power \\(x\\) is called the exponential of \\(x\\), and so it’s very common to see \\(e^x\\) written as exppxq. And so it’s no surprise that jamovi has a function that calculates exponentials, called EXP(). Because the number e crops up so often in statistics, the natural logarithm (i.e., logarithm in base e) also tends to turn up. Mathematicians often write it as \\(log_e(x)\\) or \\(ln(x)\\). In fact, jamovi works the same way: the LN() function corresponds to the natural logarithm.\nAnd with that, I think we’ve had quite enough exponentials and logarithms for this book!"
  },
  {
    "objectID": "06-Pragmatic-matters.html#extracting-a-subset-of-the-data",
    "href": "06-Pragmatic-matters.html#extracting-a-subset-of-the-data",
    "title": "6  實務課題",
    "section": "6.5 Extracting a subset of the data",
    "text": "6.5 Extracting a subset of the data\nOne very important kind of data handling is being able to extract a particular subset of the data. For instance, you might be interested only in analysing the data from one experimental condition, or you may want to look closely at the data from people over 50 years in age. To do this, the first step is getting jamovi to filter the subset of the data corresponding to the observations that you’re interested in.\nThis section returns to the nightgarden.csv data set. If you’re reading this whole chapter in one sitting, then you should already have this data set loaded into a jamovi window. For this section, let’s focus on the two variables speaker and utterance (see [Tabulating and cross-tabulating data]) if you’ve forgotten what those variables look like). Suppose that what I want to do is pull out only those utterances that were made by Makka-Pakka. To that end, we need to specify a filter in jamovi. First open up a filter window by clicking on ‘Filters’ on the main jamovi ‘Data’ toolbar. Then, in the ‘Filter 1’ text box, next to the ‘=’ sign, type the following:\nspeaker == ‘makka-pakka’\n\n\n\n\n\nFigure 6.10: Creating a subset of the nightgarden data using the jamovi ‘Filters’ option\n\n\n\n\nWhen you have done this, you will see that a new column has been added to the spreadsheet window (see Figure 6.10), labelled ‘Filter 1’, with the cases where speaker is not ‘makka-pakka’ greyed-out (i.e., filtered out) and, conversely, the cases where speaker is ‘makka-pakka’ have a green check mark indicating they are filtered in. You can test this by running ‘Exploration’ - ‘Descriptives’ - ‘Frequency tables’ for the speaker variable and seeing what that shows. Go on, try it!\nFollowing on from this simple example, you can also build up more complex filters using logical expressions in jamovi. For instance, suppose I wanted to keep only those cases when the utterance is either “pip” or “oo”. In this case in the ‘Filter 1’ text box, next to the ‘=’ sign, you would type the following:\nutterance == ‘pip’ or utterance == ‘oo’"
  },
  {
    "objectID": "06-Pragmatic-matters.html#summary",
    "href": "06-Pragmatic-matters.html#summary",
    "title": "6  Pragmatic matters",
    "section": "6.6 Summary",
    "text": "6.6 Summary\nObviously, there’s no real coherence to this chapter. It’s just a grab bag of topics and tricks that can be handy to know about, so the best wrap up I can give here is just to repeat this list:\n\nTabulating and cross-tabulating data\nLogical expressions in jamovi\nTransforming and recoding a variable\nA few more mathematical functions and operations\nExtracting a subset of the data"
  },
  {
    "objectID": "05-Drawing-graphs.html#箱型圖",
    "href": "05-Drawing-graphs.html#箱型圖",
    "title": "5  繪製統計圖",
    "section": "5.2 箱型圖",
    "text": "5.2 箱型圖\n箱型圖(boxplot)是與直方圖功能相同，適用於等距尺度或比例尺度變項的另一種資料視覺化方法，又被稱為“盒鬚圖”。箱形圖的基本概念是將變項資料的中位數、四分位數間距、以及全距用視覺標記呈現。如此簡潔的構成讓箱形圖變成最常使用的統計圖，特別是用在初步探索資料趨勢的時候。以下同樣使用afl.margins資料集做為示範。\n\n\n\n\n\n\n圖 5.4: 使用jamovi繪製afl.margins變項的箱形圖。\n\n\n\n\n了解如何用箱形圖解釋資料的最簡單方式，就是親手繪製一份。與繪製直方圖一樣的步驟，只是改成勾選”Box plot”，就會在jamovi報表介面得到如 圖 5.4 的成品。看著圖中的特徵，你能辨識出重要訊息：箱子中大的粗線是中位數；箱子的上下邊界距離是25%到75%的四分位數間距；箱子之外的”觸鬚”長度，只要不會超過“限制邊界”，就能延伸到資料的最小值及最大值。預設的限制邊界是四分位數間距的1.5倍，也就是說觸鬚向下延伸只能到25%的四分位數 減去1.5倍的四分位數間距，以及向上延伸最多到75%的四分位數加上1.5倍的四分位數間距。任何落在觸鬚或限制邊界之外的數值，一般稱為極端值(outlier)。afl.margins這筆變項有兩個極端值，因為上邊界值是107，試算表裡可以找出是第108行及第116行的數值。\n\n\n5.2.1 小提琴圖\n\n\n\n\n\n圖 5.5: 使用jamovi繪製afl.margins變項的小提琴圖，同時繪製資料點與箱形圖。\n\n\n\n\n小提琴圖(violin plot)是傳統箱形圖的變形。小提琴圖類似箱形圖，曲線代表每個數值在整筆資料的機率密度。通常小提琴圖都要同時呈現箱形圖的主要視覺標記，包括標記中位數的粗線，以及代表四分位數間距的箱子。用jamovi完成 圖 5.5 作品的繪製方法是只要將”Box plot”選項之下的”Violin”與”Data”都一起勾選。因為統計圖儘可能簡潔易懂，小提琴圖有太多視覺元素，原作者喜歡使用箱形圖比小提琴圖多一些。\n\n\n\n5.2.2 使用多重箱形圖\n最後，如果我們需要繪製不只一個箱形圖要怎麼做呢？像是我們要將每一年AFL勝隊得分的資料都會成一份箱形圖。首先把能達到這個目的的資料匯入jamovi，請開啟示範資料庫的”AFL Margins By Year”這份檔案。這份檔案一共有4296場比賽紀錄，還有一個變項儲存年份。要用jamovi繪製每一年勝隊得分margins的箱形圖，請依照 圖 5.6 的示範，把變項year放到”Split by”視窗裡。\n\n\n\n\n\n\n圖 5.6: 使用”Split by”視窗的示範畫面。\n\n\n\n\n繪製成品如 圖 5.7 。由於每一年都有一份箱形圖，比直方圖更能讓我們看到逐年趨勢，而且不會被年份的連續性干擾解讀2。如果這裡改成繪製24份直方圖，各位可以自行試試看容不容易解讀。\n\n\n\n\n\n\n圖 5.7: 使用jamovi繪製多重箱形圖，每份箱形圖代表各年份的勝隊得分分佈。\n\n\n\n\n\n\n5.2.3 使用箱形圖辨認極端值\njamovi繪製箱形圖會自動標記超出限制邊界的資料點，實務上通常用繪製箱形圖偵測資料裡的極端值：泛指離多數資料遠得”可疑”的數值。我們用 圖 5.8 的AFL勝隊得分箱形圖裡的兩個資料點來說明為何應該懷疑極端值的存在：因為這兩場勝隊得分超過300分！實在太不尋常了3。接著我們試著使用jamovi的功能，仔細檢視這兩場紀錄。只要勾選”Box Plot”選項時，一併啟動之下的”Label outliers”，jamovi就會在箱形圖標示極端值資料的在第幾列，讓我們能回到試算表介面看個仔細。另一種方式是使用jmaovi的過濾器(Filter)功能：按下主介面左下角的漏斗圖示，就會開啟如 圖 5.9 Filter選單，接著比照該圖示範，在視窗裡輸入或複製貼上 ‘margin &gt; 300’。\n\n\n\n\n\n\n圖 5.8: 這份箱形圖顯示兩筆非常可疑的極端值！\n\n\n\n\n過濾器啟動後，試算面介面最左邊會多一個欄位，欄位下的細格會標記有那些資料通過稍早設定的條件。這種方法能快速辨識極端值資料在那裡。更進一步還可以開啟jamovi描述統計選單的”Frequency table”，如同 圖 5.10 的示範。報表顯示超過300分的比賽紀錄是第14與第134場。這些資料探索能讓我們決定，是不是要回去看看資料檔案裡，這些不尋常的極端值到底是怎麼回事。\n\n\n\n\n\n\n\n圖 5.9: jamovi Filter視窗操作示範畫面\n\n\n\n\n\n\n\n\n\n圖 5.10: 以次數表展示兩筆極端值ID ~ 176與202。\n\n\n\n\n通常造常極端值的問題是登打紀錄的人手誤。雖然這是個不細心而犯的錯，但是這種事情在統計實務層出不窮。真實世界的各種資料充滿這類錯誤，尤其是透過人工輸入電腦的資料。其實實務上有個專有名詞叫「資料清理」：泛指在正式進行資料分析前，找出錯誤或可疑數值的一切工作。找出並清理原始資料裡一切輸入錯誤、遺漏值、或者各種希奇古怪問題的工作都是資料清理的項目。\n至於比較沒那麼極端，但是在箱形圖裡被標為極端值的資料，要不要納入分析或排除這些數值，全看你要如何看待這筆資料，以及利用這筆資料的想法。上統計課就是自我訓練判斷如何運用這類資料的能力。若是你認為這些極端值應該納入分析，就保留它們。到了 ?sec-Model-checking 我們會繼續學習更多判斷極端值要不要保留的策略。"
  },
  {
    "objectID": "05-Drawing-graphs.html#匯出統計圖",
    "href": "05-Drawing-graphs.html#匯出統計圖",
    "title": "5  繪製統計圖",
    "section": "5.4 匯出統計圖",
    "text": "5.4 匯出統計圖\n快要下課了，不過認真的同學應該會想問：如果我很滿意用jamovi畫出來的統計圖，想要存起來分享給朋友看的話，難道他們也要有jamovi才看得到嗎？可以只將統計圖單獨存檔嗎？很簡單，在你想存檔的統計圖上按一下滑鼠右鍵，就會出現匯出存檔選單，你可以選擇要存檔的格式，目前有’png’，‘eps’，‘svg’，’pdf’等可以選擇。只要存成你覺得合適的圖檔格式，就可以用電子郵件或社交軟體分享給朋友，或者將圖檔放到你的作業報告裡。"
  },
  {
    "objectID": "05-Drawing-graphs.html#本章小結",
    "href": "05-Drawing-graphs.html#本章小結",
    "title": "5  繪製統計圖",
    "section": "5.5 本章小結",
    "text": "5.5 本章小結\n本書原作者自敘個人寫作學術報告的習慣，都是從思考要放什麼圖開始。因為每個人都對搭配有敘事順序的圖畫故事有興趣，想清楚要放什麼圖，報告的其他部分都是點綴。因為人類天生會用眼睛探索世界的傾向，統計圖也是一種分析資料的工具。佈局精密的統計圖能幫助讀者從海量資訊裡立刻看到關鍵，也就是許多人都聽過的「一圖抵萬言」。希望讀過並操作過本章範例的同學，能將這個想法記在心裡。本章涵蓋的主題有：\n\n常用統計圖. 本章重點介紹與示範最基本的統計圖，包括直方圖, 箱型圖 以及 柱狀圖\n匯出統計圖 完成統計圖後，別忘了匯出到你的正式報告裡。\n\n最後提醒一點。jamovi(還有大多數套裝軟體)輸出的統計圖都是最基本的樣式，不一定符合報告呈現的需要。如果有心製作報告需要的統計圖，建議學習使用R語言及製作統計圖的套件。最多R語言使用者喜歡用ggplot2，有許多參考資源可以拿來自我學習及參照(例如 Wilkinson et al., 2006)。不過如果你還是統計初學者，你需要先花些時間掌握R語法。本書不會談到R語言，當你認為有必要使用時，可以運用本書的範例，做為學習R的入門資源(請記得jamovi有R程式碼模式)。\n\n\n\n\n\n\nWilkinson, L., Wills, D., Rope, D., Norton, A., & Dubbs, R. (2006). The grammar of graphics. Springer."
  },
  {
    "objectID": "07-Introduction-to-probability.html",
    "href": "07-Introduction-to-probability.html",
    "title": "7  Introduction to probability",
    "section": "",
    "text": "[God] has afforded us only the twilight … of Probability.\n– John Locke\nUp to this point in the book we’ve discussed some of the key ideas in experimental design, and we’ve talked a little about how you can summarise a data set. To a lot of people this is all there is to statistics: collecting all the numbers, calculating averages, drawing pictures, and putting them all in a report somewhere. Kind of like stamp collecting but with numbers. However, statistics covers much more than that. In fact, descriptive statistics is one of the smallest parts of statistics and one of the least powerful. The bigger and more useful part of statistics is that it provides information that lets you make inferences about data.\nOnce you start thinking about statistics in these terms, that statistics is there to help us draw inferences from data, you start seeing examples of it everywhere. For instance, here’s a tiny extract from a newspaper article in the Sydney Morning Herald (30 Oct 2010):\nThis kind of remark is entirely unremarkable in the papers or in everyday life, but let’s have a think about what it entails. A polling company has conducted a survey, usually a pretty big one because they can afford it. I’m too lazy to track down the original survey so let’s just imagine that they called 1000 New South Wales (NSW) voters at random, and 230 (23%) of those claimed that they intended to vote for the Australian Labor Party (ALP). For the 2010 Federal election the Australian Electoral Commission reported 4,610,795 enrolled voters in NSW, so the opinions of the remaining 4,609,795 voters (about 99.98% of voters) remain unknown to us. Even assuming that no-one lied to the polling company the only thing we can say with 100% confidence is that the true ALP primary vote is somewhere between 230/4610795 (about 0.005%) and 4610025/4610795 (about 99.83%). So, on what basis is it legitimate for the polling company, the newspaper, and the readership to conclude that the ALP primary vote is only about 23%?\nThe answer to the question is pretty obvious. If I call 1000 people at random, and 230 of them say they intend to vote for the ALP, then it seems very unlikely that these are the only 230 people out of the entire voting public who actually intend to vote ALP. In other words, we assume that the data collected by the polling company is pretty representative of the population at large. But how representative? Would we be surprised to discover that the true ALP primary vote is actually 24%? 29%? 37%? At this point everyday intuition starts to break down a bit. No-one would be surprised by 24%, and everybody would be surprised by 37%, but it’s a bit hard to say whether 29% is plausible. We need some more powerful tools than just looking at the numbers and guessing.\nInferential statistics provides the tools that we need to answer these sorts of questions, and since these kinds of questions lie at the heart of the scientific enterprise, they take up the lions share of every introductory course on statistics and research methods. However, the theory of statistical inference is built on top of probability theory. And it is to probability theory that we must now turn. This discussion of probability theory is basically background detail. There’s not a lot of statistics per se in this chapter, and you don’t need to understand this material in as much depth as the other chapters in this part of the book. Nevertheless, because probability theory does underpin so much of statistics, it’s worth covering some of the basics."
  },
  {
    "objectID": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "href": "07-Introduction-to-probability.html#how-are-probability-and-statistics-different",
    "title": "7  Introduction to probability",
    "section": "7.1 How are probability and statistics different?",
    "text": "7.1 How are probability and statistics different?\nBefore we start talking about probability theory, it’s helpful to spend a moment thinking about the relationship between probability and statistics. The two disciplines are closely related but they’re not identical. Probability theory is “the doctrine of chances”. It’s a branch of mathematics that tells you how often different kinds of events will happen. For example, all of these questions are things you can answer using probability theory:\n\nWhat are the chances of a fair coin coming up heads 10 times in a row?\nIf I roll a six sided dice twice, how likely is it that I’ll roll two sixes?\nHow likely is it that five cards drawn from a perfectly shuffled deck will all be hearts?\nWhat are the chances that I’ll win the lottery?\n\nNotice that all of these questions have something in common. In each case the “truth of the world” is known and my question relates to the “what kind of events” will happen. In the first question I know that the coin is fair so there’s a 50% chance that any individual coin flip will come up heads. In the second question I know that the chance of rolling a 6 on a single die is 1 in 6. In the third question I know that the deck is shuffled properly. And in the fourth question I know that the lottery follows specific rules. You get the idea. The critical point is that probabilistic questions start with a known model of the world, and we use that model to do some calculations. The underlying model can be quite simple. For instance, in the coin flipping example we can write down the model like this:\n\\[P(head)=0.5\\]\nwhich you can read as “the probability of heads is 0.5”. As we’ll see later, in the same way that percentages are numbers that range from 0% to 100%, probabilities are just numbers that range from 0 to 1. When using this probability model to answer the first question I don’t actually know exactly what’s going to happen. Maybe I’ll get 10 heads, like the question says. But maybe I’ll get three heads. That’s the key thing. In probability theory the model is known but the data are not.\nSo that’s probability. What about statistics? Statistical questions work the other way around. In statistics we do not know the truth about the world. All we have is the data and it is from the data that we want to learn the truth about the world. Statistical questions tend to look more like these:\n\nIf my friend flips a coin 10 times and gets 10 heads are they playing a trick on me?\nIf five cards off the top of the deck are all hearts how likely is it that the deck was shuffled?\nIf the lottery commissioner’s spouse wins the lottery how likely is it that the lottery was rigged?\n\nThis time around the only thing we have are data. What I know is that I saw my friend flip the coin 10 times and it came up heads every time. And what I want to infer is whether or not I should conclude that what I just saw was actually a fair coin being flipped 10 times in a row, or whether I should suspect that my friend is playing a trick on me. The data I have look like this:\nH H H H H H H H H H H\nand what I’m trying to do is work out which “model of the world” I should put my trust in. If the coin is fair then the model I should adopt is one that says that the probability of heads is 0.5, that is P(heads) = 0.5. If the coin is not fair then I should conclude that the probability of heads is not 0.5, which we would write as \\(P(heads)\\ne{0.5}\\). In other words, the statistical inference problem is to figure out which of these probability models is right. Clearly, the statistical question isn’t the same as the probability question, but they’re deeply connected to one another. Because of this, a good introduction to statistical theory will start with a discussion of what probability is and how it works."
  },
  {
    "objectID": "07-Introduction-to-probability.html#what-does-probability-mean",
    "href": "07-Introduction-to-probability.html#what-does-probability-mean",
    "title": "7  機率入門",
    "section": "7.2 What does probability mean?",
    "text": "7.2 What does probability mean?\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n7.2.1 The frequentist view\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the frequentist view and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 7.1:  Coin flips and proportion of heads \n\nnumber of flips12345678910\n\nnumber of heads0123444567\n\nproportion00.50.670.750.80.670.570.630.670.7\n\nnumber of flips11121314151617181920\n\nnumber of heads88910101010101011\n\nproportion0.730.670.690.710.670.630.590.560.530.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 7.1. As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.1 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 8.5).\n\n\n\n\n\nFigure 7.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n7.2.2 The Bayesian view\nThe Bayesian view of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n7.2.3 What’s the difference? And who is right?\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in Chapter 16."
  },
  {
    "objectID": "07-Introduction-to-probability.html#basic-probability-theory",
    "href": "07-Introduction-to-probability.html#basic-probability-theory",
    "title": "7  機率入門",
    "section": "7.3 Basic probability theory",
    "text": "7.3 Basic probability theory\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n7.3.1 Introducing probability distributions\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 7.2 shows an example of a probability distribution.\n\n\n\n\nTable 7.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see Section 5.3) to visualise this distribution, as shown in Figure 7.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 7.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 7.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\nTable 7.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-binomial-distribution",
    "title": "8  機率入門",
    "section": "8.4 二項分佈",
    "text": "8.4 二項分佈\n\n可想而知現在用於統計分析的機率分佈非常多,還有很 多分佈是只有純數學家才會研究的，不過不是所有機率分佈都必須要認識。本書介紹的統計方法主要依靠五種機率分佈運作:二項分佈(binomial distribution)、常態分佈(normal distribution)、 t 分佈(t distribution)、卡方分佈( \\(\\chi^2\\) distribution )以及 F 分佈( F distribution )。所以這個單元後半部就是簡要介紹這五種機率分佈,二項分佈和常態分佈的篇幅會比較多。我們先從二項分佈開始,因為這是五種之中最簡單的機率分佈。\n\n8.4.1 二項分佈入門\n\n機率論的源起是分析博奕遊戲的過程，所以擲骰子或擲幣的實驗經常用做認識二項分佈的教材。現在想像一個簡單的“實驗”：你面前的莊家手裡拿著20個相同的六面骰子，每個骰子的其中一面是骷髏圖案,其他五面都是空白的。如果莊家接著擲出所有骰子,你會看到其中4個骰子的骷髏圖案那面朝上的機率是多少?假如骰子是公平的,我們能假定任何一個骰子擲出骷髏的機率是 1/6。也就是說,擲出一個骷髏機率約為.167。 這些資訊足以回答請你想像的問題,讓我們用機率分佈計算看看。\n如同說明本人會穿什麼褲子出門的機率分佈一樣,這裡先要介紹一些名詞和符號。在這個例子裡，我們用\\(N\\)表示骰子擲出次數,通常被稱為二項分佈的大小參數(size parameter?)；還有用\\(\\theta\\)代表一個骰子擲出骷髏的機率,這個參數通常稱為二項式的成功機率4。\n我們將使用\\(N\\)表示我們實驗中的骰子擲出次數,這通常被稱為我們二項分佈的大小參數。 我們也將使用\\(\\theta\\)引用單個骰子出現骷髏的機率,這個量通常被稱為二項式的成功機率5。 最後一個參數 \\(X\\) 代表這次實驗擲出的幾個骷髏朝上的骰子。因為\\(X\\)的數值多少是由機率決定的,有個通用名稱隨機變數。有了這些專有名詞和符號,現在能準確地陳述要計算的問題。 我們已經知道 \\(\\theta = .167\\) 還有 \\(N = 20\\)，要計算 \\(X = 4\\) 的機率。只要知道要用什麼機率分佈計算，任何事件發生機率的通用“數學式”一律可以寫為\n\\[P(X|\\theta,N)\\]\n表示我們要算出的是 \\(X = 4, \\theta = .167\\) 和 \\(N = 20\\) 的機率值。\n[附加技術細節 6]\n我知道很多讀者走到這裡會想：怎麼又是一堆符號，能不用知道那麼多嗎？這本書是為不想學太多數學符號的讀者而設計的，所以接下來會直接談如何運用二項分佈計算機率。不過有些比較好學的讀者可能想知道細節，所以我將詳細公式說明寫在腳注^07-introduction-to-probability-3_1]。因為多數讀者沒興趣知道如何用公式計算機率，本書也不要求讀者要了解公式才能學如何計算，只要看看符合實驗條件 的二項分佈長什麼樣子。\n請看 圖 8.3 。這個柱狀圖展示這個擲骰實驗所有可能結果 \\(X\\) 的出現機率，包括\\(X = 0\\)(没有一個骷髏) 到\\(X = 20\\)(全部都是骷髏)。讀者只要知道，解讀這個柱狀圖的方式與 圖 8.2 一樣，水平軸的值代表所有可能的實驗結果,垂直軸的值代表看到各種實驗結果的機率。所以,擲出 \\(4\\) 個骷髏的機率是約 \\(0.20\\)(實際答案是 \\(0.2022036\\),我們很快會學到怎麼算)。也就是說，若你試著重做這個時驗，看到相同結果的機率大約是20%。\n\n\n\n\n\n圖 8.3: 大小參數為 \\(N = 20\\),基礎成功機率為 \\(\\theta = \\frac{1}{6}\\) 的二項分佈。 每個柱子對應一個特定结果(即X的一個可能值)的機率。 因為這是一個機率分佈,所以每個事件的機率值必須在 0 到 1 之間,所有柱子的高度之和必須為 1\n\n\n\n\n\n因為二項分佈柱狀圖的形狀會隨\\(\\theta\\) 和 \\(N\\)改變，為了讓讀者好好感受，現在我們把實驗材料改成一面是骷髏頭像硬幣。這次的實驗是持續投擲一枚公平的硬幣，我會紀錄會有幾次骷髏的那面朝上，所以設定這個實驗每次結果的成功機率\\(\\theta = \\frac{1}{2}\\)，做完實驗一共擲硬幣 \\(N = 20\\) 次。只有成功機率改變的話，二項分佈有會有什麼變化？你有稍微研究一下公式的話，會預期如 圖 8.4 (a) 展示的柱狀圖一樣，分佈的中心點位置移動到水平軸中央了。那麼連續擲\\(N = 100\\) 次呢? 如同 圖 8.4(b) ，分佈依然位於中間，但是可能的實驗結果(柱子)數量增加了。\n\n\n\n\n\n圖 8.4: 兩個擲硬幣實驗的二項分佈,假如是公平硬幣,基礎成功機率是 \\(\\theta = \\frac{1}{2}\\)。 圖(a)是連續擲 \\(N = 20\\) 次硬幣的所有可能結果。 圖(b)是連續擲 \\(N = 100\\) 次硬幣的所有可能結果。"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "href": "07-Introduction-to-probability.html#sec-The-normal-distribution",
    "title": "8  機率入門",
    "section": "8.5 常態分佈",
    "text": "8.5 常態分佈\n\n雖然二項分佈是最好懂的機率分佈，不過並不是統計學裡最重要的分佈，運作各種統計方法要依賴的其實是常態分佈,又稱“鐘形曲線”或“高斯分佈”。常態分佈用兩個參數描述分佈的形狀：平均值 μ 和標準差\\(\\sigma\\)。用隨機變數符號 \\(X\\) 來表現的話就是:\n\\[X \\sim \\text{Normal}(\\mu,\\sigma)\\]\n[附加技術細節 7]\n接著來談談為何常態分佈的視覺化長得如同 圖 8.5 的樣子。圖中的常態分佈參數是平均值 \\(\\mu = 0\\) 、標準差 \\(\\sigma = 1\\) 。從外形就能了解又稱為“鐘形曲線”的原因;真得看起來有點像中世紀時期的鐘。那麼常態分佈和二項分佈有何不同？首先 圖 8.5 的常態分佈呈現光滑的曲線，而不是“柱狀圖”。這不是隨便畫的,因為常態分佈是連續隨機變數,而二項分佈是離散隨機變數。用上一節的擲骰子範例來解釋的話,每次會獲得3個骷髏或4個骷髏,但是不可能會獲得3.9個骷髏。如同 圖 8.3 呈現的事實：\\(X = 3\\) 有一個長條, \\(X = 4\\) 處也有一個長條,但是兩者之間没有。連續隨機變數没有這樣的限制。以天氣溫度來說，某個春天的日子氣溫可能是23度、24度、23.9度、或介於任兩個數值之間的任何數字,因為溫度是連續變數。這也是為什麼，常態分佈很適合描述春季的氣溫8。\n\n\n\n\n\n圖 8.5: 平均值 \\(\\mu = 0\\) 和標準差 \\(\\sigma = 1\\) 的常態分佈。 x 軸對應某個隨機變數的值,y 軸的值代表測得某數值的可能性有多大。請留意 y 軸標籤是機率密度(probability density)而不是機率。 連續分佈具有一些微妙且不太好懂的特徵,使得 y 軸的數值不能直觀理解—這條曲條的高度並不是觀測到特定 x 值的機率，曲線的高度只是顯示那些 x 值特別容易觀測到(最高點之下的那一小塊!)。 這些需要花多一點時間學習的細節,請見機率密度。\n\n\n\n\n\n\n\n\n\n圖 8.6: 展示更改常態分佈的平均值後會改變什麼。 實線是平均值為 \\(\\mu = 4\\) 的常態分佈。 虛線是平均值為 \\(\\mu = 7\\) 的常態分佈。 兩個分佈的標準差都是 \\(\\sigma = 1\\)。 兩個分佈的形狀相同並不讓人意外,只是虛線部分站得更右邊。\n\n\n\n\n\n讓我們試著用直觀的方式瞭解常態分佈為何是連續隨機變數，首先看一下改變分佈的參數會改變什麼。圖 8.6 並列平均值不同，但是標準差相同的常態分佈。讀者可以預期兩個分佈都具有相同的“寬度”，唯一區別是其中一個的最高點在左邊、另一個在右邊。兩者的其他特徵都是相同的。如果保持平均值不變、增加標準差的話,新的分佈最高點還是在同樣的位置,不過分佈曲線變得更寬,如同 圖 8.7 的展示。留意一下分佈寬度變大時,最高點的高度會跟著降低。這是必然的,如同離散二項分佈的所有柱子，高度相加起來必須為1,常態分佈曲線覆蓋的面積也必須等於1。 除此之外，常態分佈還有一個重要特徵需要讀者知道：無論實際平均值和標準差是多少,從平均值向左右延伸1個標準差覆蓋了 \\(68.3\\%\\) 的面積；從平均值向左右延伸2個標準差覆蓋了 \\(95.4\\%\\) ；從平均值向左右延伸3個標準差覆蓋了 \\(99.7\\%\\) 。 請參考@fig-fig7-8和 圖 8.9 的說明。\n\n\n\n\n\n圖 8.7: 說明變更常態分佈的標準差會發生什麼事。 圖中的兩個分佈平均值相同 \\(\\mu = 5\\),只有標準差不一樣。 實線是標準差為 \\(\\sigma = 1\\) 的分佈,虛線顯示了一個標準差為 \\(\\sigma = 2\\) 的分佈。 因此,這兩個分佈均以相同的位置“置中”,只是虛線比實線寬。\n\n\n\n\n\n\n\n\n\n圖 8.8: 曲線下的陰影面積表示觀察值落在特定範圍内的機率。 實線是根據平均值 \\(\\mu = 0\\) 和標準差 \\(\\sigma = 1\\) 的常態分佈繪製。 灰色區域呈現了指定標準差的“曲線下陰影面積”。 圖(a)顯示觀察值落在距離平均值一個標準差内的機率為 68.3%。 圖(b),顯示觀察值落在距離平均值兩個標準差内的機率為 95.4%。\n\n\n\n\n\n\n\n\n\n圖 8.9: 另外兩個說明“曲線下面積”的例子。圖(a)展示 觀察值小於或等於距離平均值一個標準差之外的機率為 15.9%；圖(b)展示觀察值落在距離平均值一個標準差和平均值之間的機率為 34.1%。 留意一下，把這兩個圖的面積加在一起,就是 15.9% + 34.1% = 50%。也就是說，對於符合常態分佈的資料,觀察值低於平均值的機率為 50%。 這也表示高於平均值的觀察值出現機率為 50%。\n\n\n\n\n\n8.5.1 機率密度\n\n\n以上常態分佈的介紹之中，其實一直避而不談機率密度，許多基礎統計教科書也會省略不提。不談機率密度也是是對的，因為這概念的本質既奇怪又違反多數人的直覺，雖然現代統計方法的許多使用標準是採用機率密度設定的。幸好如果謮者只是想要學完基礎統計的所有內容，有沒有弄懂機率密度並非必要，只有之後要學習更進階的統計課程才有必要。如果讀完這一節還是弄不懂的話也不必灰心，就算只記得一部分內容，也能讓你更能掌握如何學習使用各種統計方法\n在介紹常態分佈的視覺化統計圖裡,讀者可能已經注意到這些圖中的 y 軸標記為“機率密度”而不是密度。還有若是有仔細看常態分佈公式的話,應該會注意到符號是 \\(p(X)\\) 而不是 \\(P(X)\\)。\n這是因為圖中呈現數值的並不是實際的機率,而是某種代理指標。要理解那些y軸的數值是什麼意思,需要花一些時間了解 \\(X\\) 是連續隨機變數是什麼意思。比如我們正在聊現在戶外氣溫有幾度。溫度計顯示 \\(23\\) 度,但我知道這並不是真的。因為不是剛好 \\(23\\) 度，我心裡想也可能是 \\(23.1\\) 度。但我知道這也並不是真的,因為它實際上可能是 \\(23.09\\) 度。但是我知道……好啦,我講這麼一大串，是讓你了解連續隨機變數的真正棘手之處是，沒有工具能測出準確的數值。\n現在來想一想如何計算連續隨機變數的機率。假設要預測明天的最高溫度，就從從平均值為 \\(23\\) 度、標準差為 1 的常態分佈中隨機選出一個數值。溫度正好是 \\(23\\) 度的機率是多少?答案是“零”,或者可能是“一個如此接近零以至於可以視為零的數字”。為什麼會這樣?這就像往一個無限小的飛鏢靶心丟飛鏢。無論你多會瞄準,你永遠不會命中(除非你是某個娛樂作品的主角)。在現實生活中,永遠不會出現剛好是 \\(23\\) 度的值，真實數值總是會像 \\(23.1\\) 或 \\(22.99998\\)等一類的數值。換句話說,想要知道溫度正好是 \\(23\\) 度的機率是多少是完全没有意義的。然而,日常的溝通我跟你說戶外是 \\(23\\) 度,就算實際上是 \\(22.9998\\) 度,你也不會認為我在騙你。因為在日常溝通的情境,\\(23\\) 度通常涵蓋“ \\(22.5\\) 度及 \\(23.5\\) 度之間的某個位數值”。雖然計較溫度正好是 \\(23\\) 度的機率沒有不太意義,但是評估溫度在 \\(22.5\\) 度和 \\(23.5\\) 度之間，或者 \\(20\\) 度和 \\(30\\) 度之間，或者任何兩個數值範圍之間的機率相當合理。\n這段討論的重點是,當我們使用連續機率分佈時,計算某個特定值的機率並没有意義。然而,我們可以評估某個數值落在那段數值範圍內的機率。為了找出某個數值範圍的機率,要做的就是計算“曲線下的面積”。我們已經在 圖 8.8 看到這個概念的視覺化,其中顯示的陰影區域代表真實機率，像是其中的圖(a)顯示了觀察到的值落在離平均值 1 個標準差範圍內的機率。\n到這裡已經解釋了一部分被省略的情節，以上說明了為何知道一段數值範圍覆蓋的曲線下面積，是解讀連續機率分佈的面積關鍵。那麼這一節開始提到的 \\(p(x)\\) 在 公式裡是什麼角色?很 顯然\\(p(x)\\) 不是代表一個機率值,那究竟是什麼?這個符號 \\(p(x)\\) 的正式名稱是機率密度。在這個單元展示的曲線圖，就是y軸的數值，對應曲線的高度。密度本身並没有任何意義,但是經過“調整”，就可確保曲線下的部分面積可以代表真實機率。老實說,這就是現在你需要學習有關機率密度的全部知識。9"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "href": "07-Introduction-to-probability.html#sec-Other-useful-distributions",
    "title": "8  機率入門",
    "section": "8.6 其他常見機率分佈",
    "text": "8.6 其他常見機率分佈\n\n常態分佈是統計學最常使用的機率分佈(原因很快就會討論),二項分佈在很多狀況也會用到。 因為使用統計都是遇到各式各樣機率分佈,我們將在後面的單元會遇到其中一些。 特別是另外三個還沒介紹的t 分佈、\\(\\chi^2\\) 分佈和 F 分佈。 我也不會跟你解釋這些分佈的公式,也不會多寫幾段話討論這些分佈,讀者只要透過這些視覺化圖像感受他們:圖 8.10、圖 8.11 和 圖 8.12。\n\n\n\n\n\n圖 8.10: 自由度為 3 的 t 分佈(實線)。 看起來很像常態分佈,但完全是另位一種機率分佈。 請比較虛線繪製的標準常態分佈。\n\n\n\n\n\nt 分佈是一種與常態分佈非常相似的連續分佈,請見 圖 8.10。 仔細看t 分佈的“尾巴”比常態分佈的尾巴更“重”(尾巴曲線稍微高一些)。這是兩者之間的重要區別，當你認為資料的分佈符合常態分佈但並不知道平均值或標準差，其實應該是符合t分佈。 單元 12 將學習使用t分佈的統計分析方法。\n\n\n\n\n\n\n圖 8.11: 自由度為 3 的 \\(\\chi^2\\) 分佈。 請注意,所有觀察值都是大於零,並且該分佈形狀相當歪斜。 這些是 \\(\\chi^2\\) 分佈的關鍵特徵\n\n\n\n\n\n\\(\\chi^2\\) 分佈也是經常在許多文獻報告裡看到的機率分佈。 最常使用的場景是 單元 11 介紹的分類資料分析,實際上這個分佈無處不在。如果讀者有興趣深入研究機率分佈的數學原理(如果你上完這個課程開竅的話?),會發現 \\(\\chi^2\\) 分佈隨處可見的主要原因是,如果要處理不只一個符合常態分佈的變項,計算出平方和之總和後(稱為計算“平方和”),形成的隨機變數就是 \\(\\chi^2\\) 分佈。知道這個原理會提昇你運用統計的能力。 現在先感受一下 \\(\\chi^2\\) 分佈的視覺特徵: 圖 8.11。\n\n\n\n\n\n\n圖 8.12: 自由度為 3 及 5 的 F 分佈。 以關鍵特徵來看,非常像 \\(\\chi^2\\) 分佈,但他們完全是兩種不同的機率分佈。\n\n\n\n\n\n\nF 分佈看起來有點像 \\(\\chi^2\\) 分佈,並且需要比較個 \\(\\chi^2\\) 分佈就會遇到。 雖然任何有頭腦的人都不會想挖這個坑跳進去,但這種比較在資料分析實務非常重要。 還記得 \\(\\chi^2\\) 分佈是處理“平方和”會用到的機率分佈嗎?這也就是說如果要處理的統計問題是比較兩個“平方和”，就要使用F分佈。當然，我們目前還沒遇到有平方和的範例，這是 單元 13 的學習課題，我們將在那個單元好好認識F分佈。現在請看 圖 8.12 感受F分佈的特徵。\n\n這個單元終於要結束了。讀者們又看到三種機率分佈:\\(\\chi^2\\)、t 和 F。 這些都是連續分佈,並且都與常態分佈有密切關聯。不想深究數學原理的讀者，只要知道這些基本事實就足夠了。在後面的單元，我們將遭遇遵循常態分佈或至少假定遵循常態分佈的資料。讀者現在只要記住，如果假定要處理的資料遵循常態分佈，其實應該使用\\(\\chi^2\\)、t 或 F 分佈的話，不必感到驚慌了。"
  },
  {
    "objectID": "07-Introduction-to-probability.html#summary",
    "href": "07-Introduction-to-probability.html#summary",
    "title": "7  機率入門",
    "section": "7.7 Summary",
    "text": "7.7 Summary\nIn this chapter we’ve talked about probability. We’ve talked about what probability means and why statisticians can’t agree on what it means. We talked about the rules that probabilities have to obey. And we introduced the idea of a probability distribution and spent a good chunk of the chapter talking about some of the more important probability distributions that statisticians work with. The section by section breakdown looks like this:\n\nProbability theory versus statistics: [How are probability and statistics different?]\nThe frequentist view versus The Bayesian view of probability\nBasic probability theory\nThe binomial distribution, The normal distribution, and Other useful distributions\n\nAs you’d expect, my coverage is by no means exhaustive. Probability theory is a large branch of mathematics in its own right, entirely separate from its application to statistics and data analysis. As such, there are thousands of books written on the subject and universities generally offer multiple classes devoted entirely to probability theory. Even the “simpler” task of documenting standard probability distributions is a big topic. I’ve described five standard probability distributions in this chapter, but sitting on my bookshelf I have a 45-chapter book called “Statistical Distributions” (Evans et al., 2011) that lists a lot more than that. Fortunately for you, very little of this is necessary. You’re unlikely to need to know dozens of statistical distributions when you go out and do real world data analysis, and you definitely won’t need them for this book, but it never hurts to know that there’s other possibilities out there.\nPicking up on that last point, there’s a sense in which this whole chapter is something of a digression. Many undergraduate psychology classes on statistics skim over this content very quickly (I know mine did), and even the more advanced classes will often “forget” to revisit the basic foundations of the field. Most academic psychologists would not know the difference between probability and density, and until recently very few would have been aware of the difference between Bayesian and frequentist probability. However, I think it’s important to understand these things before moving onto the applications. For example, there are a lot of rules about what you’re “allowed” to say when doing statistical inference and many of these can seem arbitrary and weird. However, they start to make sense if you understand that there is this Bayesian vs. frequentist distinction. Similarly, in Chapter 11 we’re going to talk about something called the t-test, and if you really want to have a grasp of the mechanics of the t-test it really helps to have a sense of what a t-distribution actually looks like. You get the idea, I hope.\n\n\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical Transactions of the Royal Society A, 222, 309–368.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A methodological paradox. Philosophy of Science, 34, 103–115."
  },
  {
    "objectID": "06-Pragmatic-matters.html#本章小結",
    "href": "06-Pragmatic-matters.html#本章小結",
    "title": "6  實務課題",
    "section": "6.6 本章小結",
    "text": "6.6 本章小結\n這一章是原作者分享實務經驗會用到的各種不起眼，但是會影響描述統計品質的小技巧或注意項目。最後再次回顧本章的課題\n\n製作次數表及列聯表\n邏輯運算\n資料變項的轉換與編碼\n數學函式及運算子\n篩選部分資料"
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯表達",
    "href": "06-Pragmatic-matters.html#邏輯表達",
    "title": "6  實務課題",
    "section": "6.2 邏輯表達",
    "text": "6.2 邏輯表達\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 判斷算式真假值\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 邏輯運算子\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 在文字內嵌入邏輯運算子\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#機率和統計有什麼不一樣",
    "href": "07-Introduction-to-probability.html#機率和統計有什麼不一樣",
    "title": "8  機率入門",
    "section": "8.1 機率和統計有什麼不一樣？",
    "text": "8.1 機率和統計有什麼不一樣？\n\n\n開始談論機率論之前,先花點時間思考機率與統計學之間的關係是很有幫助的。這兩門學科關係密切,但是兩者是不一樣的。機率論是“隨機事件的理論”，是數學的一個分支,用來探討不同種類的事件發生的次數/頻率。例如,使用機率論可以回答以下這些問題:\n\n一枚公平的硬幣連續擲出正面10次的機率是多少?\n如果我擲兩次6面骰子,擲出兩次都是6點的可能性有多大?\n從完美洗牌的撲克牌組中抽出5張全部都是紅心的牌可能性有多大?\n這一期我買的彩劵中獎的機率是多少?\n\n這些問題都有一些共同點。在所有可預料會發生的結果,“事實宇宙”是可知的,問題的答案與“什麼樣的事件”會發生有關。 第一個問題,我知道硬幣是公平的,所以每次硬幣正面朝上都有50%的機率。第二個問題,我知道一枚骰子擲到6的機率為1/6。第三個問題,我知道洗牌後的撲克牌，抽到那一張的機率都是1/52。第四個問題,我知道彩劵中獎遵循發行商訂下的規則。共同關鍵是每個機率問題都是來自已知的世界模型,使用模型做點計算，我們就可以知道答案。\n機率模型可以非常簡單。以擲硬幣的問題來說,模型可以寫成這樣:\n\\[P(\\text{正面})=0.5\\]\n我們可以用白話講“擲出正面的機率是0.5”。 正如稍後會看到的,就像表達百分比只有用0%到100%之間的任何一個數字,機率只用從0到1之間的任何數字。 使用這個機率模型回答第一個問題時,其實我並不知道會發生什麼事。也許會像問題所問的,我真的會擲出10次正面，但也許我會擲出3次正面。這就是關鍵所在：在機率論的世界,模型是已知的,但是資料是未知的。\n那麼統計學呢?統計學解決問題的原則剛好相反。在統計學世界裡,我們並不知道世界的真實樣貌。唯一能掌握的只有資料,我們想要從資料中獲得世界的真實樣貌。用統計學問同樣的四道問題，會像是這樣:\n\n如果我的朋友在我的面前擲了10次硬幣全是正面,這是在耍我嗎?\n如果莊家今天用的6面骰子，連續兩次都是6點，這顆骰子有機關的可能性有多大?\n如果撲克牌組最前面的五張牌都是紅心,洗牌不夠徹底的可能性有多大?\n如果彩劵行的老闆娘中了頭獎，這期開獎有人作假的可能性有多大?\n\n在這裡我們唯一擁有的就只有資料。我能知道的是,我看到我的朋友擲硬幣10次,每次都是正面。我能做推論的是我應不應該相信眼前看到的是使用公平硬幣而出現的結果，還是應該懷疑我的朋友在耍我。這個場景我得到的資料如下:\n正面 正面 正面 正面 正面 正面 正面 正面 正面 正面\n我要努力弄清楚我應該相信哪種“世界模型”。 如果硬幣是公平的,那麼我應該相信的模型是正面機率為0.5,即 \\(P(\\text{正面}) = 0.5\\)。 如果硬幣是有機關的,那麼我應該判斷出現正面的機率不是0.5,可以寫成 \\(P(\\text{正面})\\neq0.5\\)。 換句話說,使用推論統計的目標是要弄清楚那一種機率模型是正確的。統計問題與機率問題顯然不是一樣的,但是兩者之間有深刻的關聯。因此,要充分學懂統計理論，就要從認識什麼是機率及運算機率的定理開始。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#samples-populations-and-sampling",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.1 Samples, populations and sampling",
    "text": "8.1 Samples, populations and sampling\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 Defining a population\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 Simple random samples\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 Most samples are not simple random samples\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 How much does it matter if you don’t have a simple random sample?\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 Population parameters and sample statistics\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about Estimating population parameters using your sample statistics and also Estimating a confidence interval but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#the-law-of-large-numbers",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.2 The law of large numbers",
    "text": "8.2 The law of large numbers\nIn the previous section I showed you the results of one fictitious IQ experiment with a sample size of N = 100. The results were somewhat encouraging as the true population mean is 100 and the sample mean of 98.5 is a pretty reasonable approximation to it. In many scientific studies that level of precision is perfectly acceptable, but in other situations you need to be a lot more precise. If we want our sample statistics to be much closer to the population parameters, what can we do about it? The obvious answer is to collect more data. Suppose that we ran a much larger experiment, this time measuring the IQs of 10,000 people. We can simulate the results of this experiment using jamovi. The IQsim.omv file is a jamovi data file. In this file I have generated 10,000 random numbers sampled from a normal distribution for a population with mean = 100 and sd = 15. This was done by computing a new variable using the = NORM(100,15) function. A histogram and density plot shows that this larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics. The mean IQ for the larger sample turns out to be 99.68 and the standard deviation is 14.90. These values are now very close to the true population. See Figure 8.5.\n\n\n\n\n\nFigure 8.5: A random sample drawn from a normal distribution using jamovi\n\n\n\n\nI feel a bit silly saying this, but the thing I want you to take away from this is that large samples generally give you better information. I feel silly saying it because it’s so bloody obvious that it shouldn’t need to be said. In fact, it’s such an obvious point that when Jacob Bernoulli, one of the founders of probability theory, formalised this idea back in 1713 he was kind of a jerk about it. Here’s how he described the fact that we all share this intuition:\n\nFor even the most stupid of men, by some instinct of nature, by himself and without any instruction (which is a remarkable thing), is convinced that the more observations have been made, the less danger there is of wandering from one’s goal (Stigler, 1986, p. 65).\n\nOkay, so the passage comes across as a bit condescending (not to mention sexist), but his main point is correct. It really does feel obvious that more data will give you better answers. The question is, why is this so? Not surprisingly, this intuition that we all share turns out to be correct, and statisticians refer to it as the law of large numbers. The law of large numbers is a mathematical law that applies to many different sample statistics but the simplest way to think about it is as a law about averages. The sample mean is the most obvious example of a statistic that relies on averaging (because that’s what the mean is… an average), so let’s look at that. When applied to the sample mean what the law of large numbers states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as \\(N \\longrightarrow \\infty\\)), the sample mean approaches the population mean \\(\\bar{X} \\longrightarrow \\mu\\))3\nI don’t intend to subject you to a proof that the law of large numbers is true, but it’s one of the most important tools for statistical theory. The law of large numbers is the thing we can use to justify our belief that collecting more and more data will eventually lead us to the truth. For any particular data set the sample statistics that we calculate from it will be wrong, but the law of large numbers tells us that if we keep collecting more data those sample statistics will tend to get closer and closer to the true population parameters."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sampling-distributions-and-the-central-limit-theorem",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.3 Sampling distributions and the central limit theorem",
    "text": "8.3 Sampling distributions and the central limit theorem\nThe law of large numbers is a very powerful tool but it’s not going to be good enough to answer all our questions. Among other things, all it gives us is a “long run guarantee”. In the long run, if we were somehow able to collect an infinite amount of data, then the law of large numbers guarantees that our sample statistics will be correct. But as John Maynard Keynes famously argued in economics, a long run guarantee is of little use in real life.\n\n[The] long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task, if in tempestuous seasons they can only tell us, that when the storm is long past, the ocean is flat again. (Keynes, 1923, p. 80).\n\nAs in economics, so too in psychology and statistics. It is not enough to know that we will eventually arrive at the right answer when calculating the sample mean. Knowing that an infinitely large data set will tell me the exact value of the population mean is cold comfort when my actual data set has a sample size of \\(N = 100\\). In real life, then, we must know something about the behaviour of the sample mean when it is calculated from a more modest data set!\n\n8.3.1 Sampling distribution of the mean\nWith this in mind, let’s abandon the idea that our studies will have sample sizes of 10,000 and consider instead a very modest experiment indeed. This time around we’ll sample \\(N = 5\\) people and measure their IQ scores. As before, I can simulate this experiment in jamovi = NORM(100,15) function, but I only need 5 participant IDs this time, not 10,000. These are the five numbers that jamovi generated:\n90 82 94 99 110\nThe mean IQ in this sample turns out to be exactly 95. Not surprisingly, this is much less accurate than the previous experiment. Now imagine that I decided to replicate the experiment. That is, I repeat the procedure as closely as possible and I randomly sample 5 new people and measure their IQ. Again, jamovi allows me to simulate the results of this procedure, and generates these five numbers:\n78 88 111 111 117\nThis time around, the mean IQ in my sample is 101. If I repeat the experiment 10 times I obtain the results shown in Table 8.1, and as you can see the sample mean varies from one replication to the next.\n\n\n\n\nTable 8.1:  Ten replications of the IQ experiment, each with a sample size of ( N = 5 ) \n\nPerson 1Person 2Person 3Person 4Person 5Sample Mean\n\nRep. 19082949911095.0\n\nRep. 27888111111117101.0\n\nRep. 3111122919886101.6\n\nRep. 4989611999107103.8\n\nRep. 510511310310398104.4\n\nRep. 68189938511492.4\n\nRep. 71009310898133106.4\n\nRep. 810710010511785102.8\n\nRep. 98611910873116100.4\n\nRep. 109512611212076105.8\n\n\n\n\n\nNow suppose that I decided to keep going in this fashion, replicating this “five IQ scores” experiment over and over again. Every time I replicate the experiment I write down the sample mean. Over time, I’d be amassing a new data set, in which every experiment generates a single data point. The first 10 observations from my data set are the sample means listed in Table 8.1, so my data set starts out like this:\n95.0 101.0 101.6 103.8 104.4 …\nWhat if I continued like this for 10,000 replications, and then drew a histogram. Well that’s exactly what I did, and you can see the results in Figure 8.6. As this picture illustrates, the average of 5 IQ scores is usually between 90 and 110. But more importantly, what it highlights is that if we replicate an experiment over and over again, what we end up with is a distribution of sample means! (Table 8.1)) This distribution has a special name in statistics, it’s called the sampling distribution of the mean.\n\n\n\n\n\nFigure 8.6: The sampling distribution of the mean for the ‘five IQ scores experiment’. If you sample 5 people at random and calculate their average IQ you’ll almost certainly get a number between 80 and 120, even though there are quite a lot of individuals who have IQs above 120 or below 80. For comparison, the black line plots the population distribution of IQ scores\n\n\n\n\nSampling distributions are another important theoretical idea in statistics, and they’re crucial for understanding the behaviour of small samples. For instance, when I ran the very first “five IQ scores” experiment, the sample mean turned out to be 95. What the sampling distribution in Figure 8.6 tells us, though, is that the “five IQ scores” experiment is not very accurate. If I repeat the experiment, the sampling distribution tells me that I can expect to see a sample mean anywhere between 80 and 120.\n\n\n8.3.2 Sampling distributions exist for any sample statistic!\nOne thing to keep in mind when thinking about sampling distributions is that any sample statistic you might care to calculate has a sampling distribution. For example, suppose that each time I replicated the “five IQ scores” experiment I wrote down the largest IQ score in the experiment. This would give me a data set that started out like this:\n110 117 122 119 113 …\nDoing this over and over again would give me a very different sampling distribution, namely the sampling distribution of the maximum. The sampling distribution of the maximum of 5 IQ scores is shown in Figure 8.7. Not surprisingly, if you pick 5 people at random and then find the person with the highest IQ score, they’re going to have an above average IQ. Most of the time you’ll end up with someone whose IQ is measured in the 100 to 140 range.\n\n\n\n\n\nFigure 8.7: The sampling distribution of the maximum for the ‘five IQ scores experiment’. If you sample 5 people at random and select the one with the highest IQ score you’ll probably see someone with an IQ between 100 and 140\n\n\n\n\n\n\n\n\n\nFigure 8.8: An illustration of the how sampling distribution of the mean depends on sample size. In each panel I generated 10,000 samples of IQ data and calculated the mean IQ observed within each of these data sets. The histograms in these plots show the distribution of these means (i.e., the sampling distribution of the mean). Each individual IQ score was drawn from a normal distribution with mean 100 and standard deviation 15, which is shown as the solid black line. In panel (a), each data set contained only a single observation, so the mean of each sample is just one person’s IQ score. As a consequence, the sampling distribution of the mean is of course identical to the population distribution of IQ scores. However, when we raise the sample size to 2 the mean of any one sample tends to be closer to the population mean than any one person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. By the time we raise the sample size to 10 (panel (c)), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean\n\n\n\n\n\n\n8.3.3 The central limit theorem\nAt this point I hope you have a pretty good sense of what sampling distributions are, and in particular what the sampling distribution of the mean is. In this section I want to talk about how the sampling distribution of the mean changes as a function of sample size. Intuitively, you already know part of the answer. If you only have a few observations, the sample mean is likely to be quite inaccurate. If you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. If you replicate a large experiment and recalculate the sample mean you’ll probably get the same answer you got last time, so the sampling distribution will be very narrow. You can see this visually in Figure 8.8, showing that the bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the standard error. The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the picture, as the sample size \\(N\\) increases, the SEM decreases.\nOkay, so that’s one part of the story. However, there’s something I’ve been glossing over so far. All my examples up to this point have been based on the “IQ scores” experiments, and because IQ scores are roughly normally distributed I’ve assumed that the population distribution is normal. What if it isn’t normal? What happens to the sampling distribution of the mean? The remarkable thing is this, no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this I ran some simulations. To do this, I started with the “ramped” distribution shown in the histogram in Figure 8.9. As you can see by comparing the triangular shaped histogram to the bell curve plotted by the black line, the population distribution doesn’t look very much like a normal distribution at all. Next, I simulated the results of a large number of experiments. In each experiment I took \\(N = 2\\) samples from this distribution, and then calculated the sample mean. Figure 8.9 (b) plots the histogram of these sample means (i.e., the sampling distribution of the mean for \\(N = 2\\)). This time, the histogram produces a \\(\\chi^2\\)-shaped distribution. It’s still not normal, but it’s a lot closer to the black line than the population distribution in Figure 8.9 (a). When I increase the sample size to \\(N = 4\\), the sampling distribution of the mean is very close to normal (Figure 8.9 (c)), and by the time we reach a sample size of N = 8 it’s almost perfectly normal. In other words, as long as your sample size isn’t tiny, the sampling distribution of the mean will be approximately normal no matter what your population distribution looks like!\n\n\n\n\n\nFigure 8.9: A demonstration of the central limit theorem. In panel (a), we have a non-normal population distribution, and panels (b)-(d) show the sampling distribution of the mean for samples of size 2,4 and 8 for data drawn from the distribution in panel (a). As you can see, even though the original population distribution is non-normal the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations\n\n\n\n\nOn the basis of these figures, it seems like we have evidence for all of the following claims about the sampling distribution of the mean.\n\nThe mean of the sampling distribution is the same as the mean of the population\nThe standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases\nThe shape of the sampling distribution becomes normal as the sample size increases\n\nAs it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the central limit theorem. Among other things, the central limit theorem tells us that if the population distribution has mean µ and standard deviation σ, then the sampling distribution of the mean also has mean µ and the standard error of the mean is\n\\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]\nBecause we divide the population standard deviation σ by the square root of the sample size N, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.4\nThis result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#estimating-population-parameters",
    "title": "8  Estimating unknown quantities from a sample",
    "section": "8.4 Estimating population parameters",
    "text": "8.4 Estimating population parameters\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.5 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 Estimating the population mean\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 Estimating the population standard deviation\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.6 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on Estimating population parameters, the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in Estimating population parameters. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).7\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-a-confidence-interval",
    "title": "8  運用樣本估計未知量數",
    "section": "8.5 母群參數的區間估計",
    "text": "8.5 母群參數的區間估計\n\n學習統計的最大收穫是永遠不再說「我肯定這就是答案」\n– 來源不明9\n\n這一章到目前為止，已經概述了統計學家常取用的基本取樣理論，用來根樣本資料猜測母群參數。正如己經解說的各節主題，我們需要取樣理論的原因之一是每個資料集都會有一些不確定性，因此估計值永遠不會百分之百準確。這一節我們要學的是量化估計不確定程度的方法，因為只報告像是大學心理學專業學生平均智商為115這要的點估計值並不足夠，我們還要能用數值表達猜測程度。例如：有95％ 的可能性，真實平均值介於109和121之間。這種表達方式被稱做平均值的信賴區間。\n充分理解取樣分佈，計算平均值的信賴區間其實相當簡單。這裡解說計算方法：假設母群平均值為 \\(\\mu\\)，標準差為 \\(\\sigma\\)。我剛完成了一項有 N 名參與者的研究，這些參與者的智力測驗分數的平均值是 \\(\\bar{X}\\)。根據中央極限定理，我們可以知道平均值的取樣分佈接近常態分佈。經由 小單元 8.5 關於常態分佈的學習，我們也知道一個服從常態分佈的取樣分佈，在平均值周圍相距約兩個標準差之內，有95％左右的量數落在此範圍內。\n更精確的正確答案是：以一個符合常態分佈的取樣分佈估計母群平均值，估計值有95% 的機率落在真正母群平均值的1.96個標準差內。接下來，請記住取樣分配的標準差正式名稱為標準誤(standar error)，因此平均值的標準誤差可簡寫為SEM(Standard Error of Measurement, 測量標準誤)。當我們合成這些數字時，就能得知有95%的可能性確定由樣本資料計算得到的樣本平均值 \\(\\bar{X}\\) ，與母群平均值差距在1.96倍的標準誤差之內。\n當然，1.96並不是什麼神奇數字。如果你想要計算一個95％的信賴區間，這就是經常需要使用的乘數。若是我想要70％的置信區間，我會使用1.04作為神奇數字而不是1.96。\n[更多技術細節10]\n\n\n8.5.1 解讀信賴區間\n學習信賴區間最困難的地方是理解其意義。很多學生第一次接觸信賴區間時，總是憑直覺說“真實平均值有95％的機率落在信賴區間內”。這種說法很簡單，似乎捕捉到“我有95％的自信”的常識想法。不幸的是，這並不完全正確。直覺定義非常依賴個人對母群平均值的個人價值觀。我之所以會說我有95％的自信，是因為這樣的說法是基於我的個人信仰。在日常生活這樣講沒什麼問題，但是回顧一下 如何解讀機率？你會注意到談論個人信念和自信心的統計觀點是貝氏統計的中心思想。然而，信賴區間不是基於貝氏觀點開發出來的工具。就像這一章各節介紹的其他估計方法，信賴區間是次數主義學派開發的工具；如果要使用次數主義學派的方法，用貝氏觀點解釋就不對盤了！\n好吧，如果這樣子不是正確回答，那麼要怎麼回答呢？記得討論次數主義如何解讀機率的時候，唯一“陳述機率”的合法方式是列舉一系列可觀察的機率事件，再計算各種事件的出現次數。由此看來，解釋95％信賴區間必須根據次數。具體而言，如果我們反覆執行同樣的實驗程序，並計算每次實驗結果的95％信賴區間，大約會有95％的實驗結果計算的區間會包含真正的平均值。更進一步的說法是，使用這樣的實驗程序得到的所有資料樣本信賴區間，應該會有95％包括真正的母群平均值 。圖 8.12 用模擬實驗結果的視覺化說明這樣的概念，兩幅圖各顯示50個信賴區間，分別表示“10位受測者的智力測驗分數”（圖a）和“25位受測者的智力測驗分數”（圖b）。有點幸運的是，在這100次模擬實驗結果裡，恰好有95次包含了真實平均值。\n這與貝氏觀點的關鍵差異在於，貝氏認為我們能用一個機率值，表達我們看不到的母群平均值有多麼不確定，但是次數主義學派不允許這樣解讀機率，因為沒有人能“復刻”母群！次數主義學派主張母群平均值是固定的，不能母群做任何機率式的陳述。不過，資料樣本的信賴區間是可重覆取得及計算的，所以我們可以不斷重做實驗。\n因此，次數主義學派允許使用信賴區間（一種隨機變數），表達包括真實平均值的機率，但不允許估計真實母群平均值（不可重覆觀察的事件）落在置信區間內的概率。我知道這似乎有點拘泥於細節，但這確實很重要。它之所以重要是因為解釋方式的差異導致了數學方法的差異。貝氏統計有一種替代信賴區間的方法，稱為可信區間(Credible intervals)。在大多數情況下，可信區間與信賴區間的數值範圍非常接近，但在其他情況下可能會截然不同。在本書 單元 16 ，我們將學習更多貝氏統計觀點。\n\n\n\n\n\n\n\n圖 8.12: 95％信賴區間視覺化。圖（a）顯示50次模擬實驗的結果，每次實驗測量10人的智力測驗分數。一個點表示一次實驗的樣本平均值，一個線條表示95％信賴區間。其中有47個信賴區間包含期望的平均值（即100），標示星號的三個區間則沒有包含。圖（b）展示另一項類似的模擬實驗，每次實驗測量25人的智力測驗分數。\n\n\n\n\n\n\n8.5.2 計算信賴區間\njamovi 的描述統計模組內建計算信賴區間的簡單設定。開啟’Descriptives’面板的’Statistics’子選單，最下面有兩個勾選框，分別是’Std. Error of Mean’（平均值標準誤差）和’Confidence interval for the mean’（平均值信賴區間），同學可以使用這些功能計算每個平均值的95％信賴區間（預設值）。例如，載入 IQsim.omv 檔案並勾選’Confidence interval for the mean’，就可以得到與智力測驗模擬實驗平均分數的信賴區間：下限95％CI = 99.39 和上限95％CI = 99.97。也就是大樣本數據（N=10,000）的模擬結果顯示，平均智商得分為99.68，95％ CI 為99.39至99.97。\n假如要使用 jamovi 繪製信賴區間的統計圖，可以指定平均值作為箱形圖選項之一。此外，在學習特定統計方法時（例如 單元 13 的變異數分析），我們還可以將信賴區間作為分析報表的一部分。這很酷，稍後我們會學習如何做到。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#summary",
    "title": "8  從樣本估計未知量數",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter I’ve covered two main topics. The first half of the chapter talks about sampling theory, and the second half talks about how we can use sampling theory to construct estimates of the population parameters. The section breakdown looks like this:\n\nBasic ideas about [Samples, populations and sampling]\nStatistical theory of sampling: [The law of large numbers] and [Sampling distributions and the central limit theorem]\n[Estimating population parameters]. Means and standard deviations\n[Estimating a confidence interval]\n\nAs always, there’s a lot of topics related to sampling and estimation that aren’t covered in this chapter, but for an introductory psychology class this is fairly comprehensive I think. For most applied researchers you won’t need much more theory than this. One big question that I haven’t touched on in this chapter is what you do when you don’t have a simple random sample. There is a lot of statistical theory you can draw on to handle this situation, but it’s well beyond the scope of this book.\n\n\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan; Company.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard University Press."
  },
  {
    "objectID": "07-Introduction-to-probability.html#如何解讀機率",
    "href": "07-Introduction-to-probability.html#如何解讀機率",
    "title": "7  機率入門",
    "section": "7.2 如何解讀機率？",
    "text": "7.2 如何解讀機率？\nLet’s start with the first of these questions. What is “probability”? It might seem surprising to you but while statisticians and mathematicians (mostly) agree on what the rules of probability are, there’s much less of a consensus on what the word really means. It seems weird because we’re all very comfortable using words like “chance”, “likely”, “possible” and “probable”, and it doesn’t seem like it should be a very difficult question to answer. But if you’ve ever had that experience in real life you might walk away from the conversation feeling like you didn’t quite get it right, and that (like many everyday concepts) it turns out that you don’t really know what it’s all about.\nSo I’ll have a go at it. Let’s suppose I want to bet on a soccer game between two teams of robots, Arduino Arsenal and C Milan. After thinking about it, I decide that there is an 80% probability of Arduino Arsenal winning. What do I mean by that? Here are three possibilities:\n\nThey’re robot teams so I can make them play over and over again, and if I did that Arduino Arsenal would win 8 out of every 10 games on average.\nFor any given game, I would agree that betting on this game is only “fair” if a $1 bet on C Milan gives a $5 payoff (i.e. I get my $1 back plus a $4 reward for being correct), as would a $4 bet on Arduino Arsenal (i.e., my $4 bet plus a $1 reward).\nMy subjective “belief” or “confidence” in an Arduino Arsenal victory is four times as strong as my belief in a C Milan victory.\n\nEach of these seems sensible. However, they’re not identical and not every statistician would endorse all of them. The reason is that there are different statistical ideologies (yes, really!) and depending on which one you subscribe to, you might say that some of those statements are meaningless or irrelevant. In this section I give a brief introduction the two main approaches that exist in the literature. These are by no means the only approaches, but they’re the two big ones.\n\n7.2.1 次數主義觀點\nThe first of the two major approaches to probability, and the more dominant one in statistics, is referred to as the 次數主義觀點(frequentist view) and it defines probability as a long-run frequency. Suppose we were to try flipping a fair coin over and over again. By definition this is a coin that has \\(P(H) = 0.5\\). What might we observe? One possibility is that the first 20 flips might look like this:\nT,H,H,H,H,T,T,H,H,H,H,T,H,H,T,T,T,T,T,H\nIn this case 11 of these 20 coin flips (55%) came up heads. Now suppose that I’d been keeping a running tally of the number of heads (which I’ll call \\(N_H\\)) that I’ve seen, across the first N flips, and calculate the proportion of heads \\(\\frac{N_H}{N}\\) every time. Table 7.1 shows what I’d get (I did literally flip coins to produce this!):\n\n\n\n\nTable 7.1:  Coin flips and proportion of heads \n\nnumber of flips12345678910\n\nnumber of heads0123444567\n\nproportion00.50.670.750.80.670.570.630.670.7\n\nnumber of flips11121314151617181920\n\nnumber of heads88910101010101011\n\nproportion0.730.670.690.710.670.630.590.560.530.55\n\n\n\n\n\nNotice that at the start of the sequence the proportion of heads fluctuates wildly, starting at \\(.00\\) and rising as high as \\(.80\\). Later on, one gets the impression that it dampens out a bit, with more and more of the values actually being pretty close to the “right” answer of \\(.50\\). This is the frequentist definition of probability in a nutshell. Flip a fair coin over and over again, and as N grows large (approaches infinity, denoted \\(N \\rightarrow \\infty\\) ) the proportion of heads will converge to 50%. There are some subtle technicalities that the mathematicians care about, but qualitatively speaking that’s how the frequentists define probability. Unfortunately, I don’t have an infinite number of coins or the infinite patience required to flip a coin an infinite number of times. However, I do have a computer and computers excel at mindless repetitive tasks. So I asked my computer to simulate flipping a coin 1000 times and then drew a picture of what happens to the proportion \\(\\frac{N_H}{N}\\) as \\(N\\) increases. Actually, I did it four times just to make sure it wasn’t a fluke. The results are shown in Figure 7.1. As you can see, the proportion of observed heads eventually stops fluctuating and settles down. When it does, the number at which it finally settles is the true probability of heads.\nThe frequentist definition of probability has some desirable characteristics. First, it is objective. The probability of an event is necessarily grounded in the world. The only way that probability statements can make sense is if they refer to (a sequence of) events that occur in the physical universe.1 Secondly, it is unambiguous. Any two people watching the same sequence of events unfold, trying to calculate the probability of an event, must inevitably come up with the same answer.\nHowever, it also has undesirable characteristics. First, infinite sequences don’t exist in the physical world. Suppose you picked up a coin from your pocket and started to flip it. Every time it lands it impacts on the ground. Each impact wears the coin down a bit. Eventually the coin will be destroyed. So, one might ask whether it really makes sense to pretend that an “infinite” sequence of coin flips is even a meaningful concept, or an objective one. We can’t say that an “infinite sequence” of events is a real thing in the physical universe, because the physical universe doesn’t allow infinite anything. More seriously, the frequentist definition has a narrow scope. There are lots of things out there that human beings are happy to assign probability to in everyday language, but cannot (even in theory) be mapped onto a hypothetical sequence of events. For instance, if a meteorologist comes on TV and says “the probability of rain in Adelaide on 2 November 2048 is 60%” we humans are happy to accept this. But it’s not clear how to define this in frequentist terms. There’s only one city of Adelaide, and only one 2 November 2048. There’s no infinite sequence of events here, just a one-off thing. Frequentist probability genuinely forbids us from making probability statements about a single event. From the frequentist perspective it will either rain tomorrow or it will not. There is no “probability” that attaches to a single non-repeatable event. Now, it should be said that there are some very clever tricks that frequentists can use to get around this. One possibility is that what the meteorologist means is something like “There is a category of days for which I predict a 60% chance of rain, and if we look only across those days for which I make this prediction, then on 60% of those days it will actually rain”. It’s very weird and counter-intuitive to think of it this way, but you do see frequentists do this sometimes. And it will come up later in this book (e.g. in Section 8.5).\n\n\n\n\n\nFigure 7.1: An illustration of how frequentist probability works. If you flip a fair coin over and over again the proportion of heads that you’ve seen eventually settles down and converges to the true probability of \\(0.5\\). Each panel shows four different simulated experiments. In each case we pretend we flipped a coin \\(1000\\) times and kept track of the proportion of flips that were heads as we went along. Although none of these sequences actually ended up with an exact value of \\(.5\\), if we’d extended the experiment for an infinite number of coin flips they would have\n\n\n\n\n\n\n7.2.2 貝氏觀點\n貝氏觀點(The Bayesian view) of probability is often called the subjectivist view, and although it has been a minority view among statisticians it has been steadily gaining traction for the last several decades. There are many flavours of Bayesianism, making it hard to say exactly what “the” Bayesian view is. The most common way of thinking about subjective probability is to define the probability of an event as the degree of belief that an intelligent and rational agent assigns to that truth of that event. From that perspective, probabilities don’t exist in the world but rather in the thoughts and assumptions of people and other intelligent beings.\nHowever, in order for this approach to work we need some way of operationalising “degree of belief”. One way that you can do this is to formalise it in terms of “rational gambling”, though there are many other ways. Suppose that I believe that there’s a 60% probability of rain tomorrow. If someone offers me a bet that if it rains tomorrow then I win $5, but if it doesn’t rain I lose $5. Clearly, from my perspective, this is a pretty good bet. On the other hand, if I think that the probability of rain is only 40% then it’s a bad bet to take. So we can operationalise the notion of a “subjective probability” in terms of what bets I’m willing to accept.\nWhat are the advantages and disadvantages to the Bayesian approach? The main advantage is that it allows you to assign probabilities to any event you want to. You don’t need to be limited to those events that are repeatable. The main disadvantage (to many people) is that we can’t be purely objective. Specifying a probability requires us to specify an entity that has the relevant degree of belief. This entity might be a human, an alien, a robot, or even a statistician. But there has to be an intelligent agent out there that believes in things. To many people this is uncomfortable, it seems to make probability arbitrary. Whilst the Bayesian approach requires that the agent in question be rational (i.e., obey the rules of probability), it does allow everyone to have their own beliefs. I can believe the coin is fair and you don’t have to, even though we’re both rational. The frequentist view doesn’t allow any two observers to attribute different probabilities to the same event. When that happens then at least one of them must be wrong. The Bayesian view does not prevent this from occurring. Two observers with different background knowledge can legitimately hold different beliefs about the same event. In short, where the frequentist view is sometimes considered to be too narrow (forbids lots of things that that we want to assign probabilities to), the Bayesian view is sometimes thought to be too broad (allows too many differences between observers).\n\n\n7.2.3 觀點之間的差異是什麼？何者正確？\nNow that you’ve seen each of these two views independently it’s useful to make sure you can compare the two. Go back to the hypothetical robot soccer game at the start of the section. What do you think a frequentist and a Bayesian would say about these three statements? Which statement would a frequentist say is the correct definition of probability? Which one would a Bayesian opt for? Would some of these statements be meaningless to a frequentist or a Bayesian? If you’ve understood the two perspectives you should have some sense of how to answer those questions.\nOkay, assuming you understand the difference then you might be wondering which of them is right? Honestly, I don’t know that there is a right answer. As far as I can tell there’s nothing mathematically incorrect about the way frequentists think about sequences of events, and there’s nothing mathematically incorrect about the way that Bayesians define the beliefs of a rational agent. In fact, when you dig down into the details Bayesians and frequentists actually agree about a lot of things. Many frequentist methods lead to decisions that Bayesians agree a rational agent would make. Many Bayesian methods have very good frequentist properties.\nFor the most part, I’m a pragmatist so I’ll use any statistical method that I trust. As it turns out, that makes me prefer Bayesian methods for reasons I’ll explain towards the end of the book. But I’m not fundamentally opposed to frequentist methods. Not everyone is quite so relaxed. For instance, consider Sir Ronald Fisher, one of the towering figures of 20th century statistics and a vehement opponent to all things Bayesian, whose paper on the mathematical foundations of statistics referred to Bayesian probability as “an impenetrable jungle [that] arrests progress towards precision of statistical concepts” (Fisher, 1922, p. 311). Or the psychologist Paul Meehl, who suggests that relying on frequentist methods could turn you into “a potent but sterile intellectual rake who leaves in his merry path a long train of ravished maidens but no viable scientific offspring” (Meehl, 1967, p. 114). The history of statistics, as you might gather, is not devoid of entertainment.\nIn any case, whilst I personally prefer the Bayesian view, the majority of statistical analyses are based on the frequentist approach. My reasoning is pragmatic. The goal of this book is to cover roughly the same territory as a typical undergraduate stats class in psychology, and if you want to understand the statistical tools used by most psychologists you’ll need a good grasp of frequentist methods. I promise you that this isn’t wasted effort. Even if you end up wanting to switch to the Bayesian perspective, you really should read through at least one book on the “orthodox” frequentist view. Besides, I won’t completely ignore the Bayesian perspective. Every now and then I’ll add some commentary from a Bayesian point of view, and I’ll revisit the topic in more depth in Chapter 16."
  },
  {
    "objectID": "07-Introduction-to-probability.html#基本機率理論",
    "href": "07-Introduction-to-probability.html#基本機率理論",
    "title": "7  機率入門",
    "section": "7.3 基本機率理論",
    "text": "7.3 基本機率理論\nIdeological arguments between Bayesians and frequentists notwithstanding, it turns out that people mostly agree on the rules that probabilities should obey. There are lots of different ways of arriving at these rules. The most commonly used approach is based on the work of Andrey Kolmogorov, one of the great Soviet mathematicians of the 20th century. I won’t go into a lot of detail, but I’ll try to give you a bit of a sense of how it works. And in order to do so I’m going to have to talk about my trousers.\n\n7.3.1 機率分佈入門\nOne of the disturbing truths about my life is that I only own 5 pairs of trousers. Three pairs of jeans, the bottom half of a suit, and a pair of tracksuit pants. Even sadder, I’ve given them names: I call them \\(X_1\\), \\(X_2\\), \\(X_3\\), \\(X_4\\) and \\(X_5\\). I really have, that’s why they call me Mister Imaginative. Now, on any given day, I pick out exactly one of pair of trousers to wear. Not even I’m so stupid as to try to wear two pairs of trousers, and thanks to years of training I never go outside without wearing trousers anymore. If I were to describe this situation using the language of probability theory, I would refer to each pair of trousers (i.e., each \\(X\\)) as an elementary event. The key characteristic of elementary events is that every time we make an observation (e.g., every time I put on a pair of trousers) then the outcome will be one and only one of these events. Like I said, these days I always wear exactly one pair of trousers so my trousers satisfy this constraint. Similarly, the set of all possible events is called a sample space. Granted, some people would call it a “wardrobe”, but that’s because they’re refusing to think about my trousers in probabilistic terms. Sad.\nOkay, now that we have a sample space (a wardrobe), which is built from lots of possible elementary events (trousers), what we want to do is assign a probability of one of these elementary events. For an event \\(X\\), the probability of that event \\(P(X)\\) is a number that lies between 0 and 1. The bigger the value of \\(P(X)\\), the more likely the event is to occur. So, for example, if \\(P(X) = 0\\) it means the event \\(X\\) is impossible (i.e., I never wear those trousers). On the other hand, if \\(P(X) = 1\\) it means that event \\(X\\) is certain to occur (i.e., I always wear those trousers). For probability values in the middle it means that I sometimes wear those trousers. For instance, if \\(P(X) = 0.5\\) it means that I wear those trousers half of the time.\nAt this point, we’re almost done. The last thing we need to recognise is that “something always happens”. Every time I put on trousers, I really do end up wearing trousers (crazy, right?). What this somewhat trite statement means, in probabilistic terms, is that the probabilities of the elementary events need to add up to 1. This is known as the law of total probability, not that any of us really care. More importantly, if these requirements are satisfied then what we have is a probability distribution. For example, Table 7.2 shows an example of a probability distribution.\n\n\n\n\nTable 7.2:  A probability distribution for trouser wearing \n\nWhich trousers?LabelProbability\n\nBlue jeans\\(X_1 \\)\\(P(X_1)=.5 \\)\n\nGrey jeans\\(X_2 \\)\\(P(X_2)=.3 \\)\n\nBlack jeans\\(X_3 \\)\\(P(X_3)=.1 \\)\n\nBlack suit\\(X_4 \\)\\(P(X_4)=0 \\)\n\nBlue tracksuit\\(X_5 \\)\\(P(X_5)=.1 \\)\n\n\n\n\n\nEach of the events has a probability that lies between 0 and 1, and if we add up the probability of all events they sum to 1. Awesome. We can even draw a nice bar graph (see Section 5.3) to visualise this distribution, as shown in Figure 7.2. And, at this point, we’ve all achieved something. You’ve learned what a probability distribution is, and I’ve finally managed to find a way to create a graph that focuses entirely on my trousers. Everyone wins! The only other thing that I need to point out is that probability theory allows you to talk about non elementary events as well as elementary ones. The easiest way to illustrate the concept is with an example. In the trousers example it’s perfectly legitimate to refer to the probability that I wear jeans. In this scenario, the “Dani wears jeans” event is said to have happened as long as the elementary event that actually did occur is one of the appropriate ones. In this case “blue jeans”, “black jeans” or “grey jeans”. In mathematical terms we defined the “jeans” event \\(E\\) to correspond to the set of elementary events \\((X1, X2, X3)\\). If any of these elementary events occurs then \\(E\\) is also said to have occurred. Having decided to write down the definition of the E this way, it’s pretty straightforward to state what the probability P(E) and, since the probabilities of blue, grey and black jeans respectively are \\(.5\\), \\(.3\\) and \\(.1\\), the probability that I wear jeans is equal to \\(.9\\). is: we just add everything up. In this particular case \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\] At this point you might be thinking that this is all terribly obvious and simple and you’d be right. All we’ve really done is wrap some basic mathematics around a few common sense intuitions. However, from these simple beginnings it’s possible to construct some extremely powerful mathematical tools. I’m definitely not going to go into the details in this book, but what I will do is list, in Table 7.3, some of the other rules that probabilities satisfy. These rules can be derived from the simple assumptions that I’ve outlined above, but since we don’t actually use these rules for anything in this book I won’t do so here.\n\n\n\n\n\nFigure 7.2: A visual depiction of the ‘trousers’ probability distribution. There are five ‘elementary events’, corresponding to the five pairs of trousers that I own. Each event has some probability of occurring - this probability is a number between 0 to 1. The sum of these probabilities is 1\n\n\n\n\n\n\n\n\nTable 7.3:  Some rules that probabilities satisfy \n\nEnglishNotationFormula\n\nnot A\\(P (\\neg A) \\)\\(1-P(A) \\)\n\nA or B\\(P(A \\cup B) \\)\\(P(A) + P(B) - P(A \\cap B) \\)\n\nA and B\\(P(A \\cap B) \\)\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "07-Introduction-to-probability.html#本章小結",
    "href": "07-Introduction-to-probability.html#本章小結",
    "title": "8  機率入門",
    "section": "8.7 本章小結",
    "text": "8.7 本章小結\n\n在本章中,我們討論了機率。我們討論了機率的意義以及為什麼統計學家無法就其意義達成共識。我們討論了機率必須遵守的規則。並且我們介紹了機率分佈的概念,並在本章中花了很大篇幅討論統計學家使用的一些更重要的機率分佈。逐節概述如下:\n\n機率論與統計學之間的區別:機率和統計有什麼不一樣？\n機率的次數主義觀點與貝氏觀點\n基本機率論\n二項分佈、常態分佈和其他常見機率分佈\n\n如您所料,我的內容絕不詳盡。機率論本身就是數學的一個巨大分支,與其在統計和資料分析中的應用完全分開。因此,這個主題上已經寫了上千本書,大學通常提供多門完全致力於機率論的課程。即使是記錄標準機率分佈這樣“更簡單”的任務也是一個大課題。我在本章中描述了五種標準機率分佈,但在我的書架上有一本45章的書,名為“統計分佈” (Evans et al., 2011),其中列出了很多更多分佈。幸運的是,您需要的很少。當您走出去做現實世界的資料分析時,您不太可能需要知道幾十種統計分佈,在本書中您肯定不需要它們,但是瞭解還有其他可能性永遠不會有害。\n關於最後一點,在某種意義上,整章可以說是額外的內容。許多本科心理學統計課程會很快減少這部分內容(我知道我的課程就是這樣),即使是更高級的課程也經常會“忘記”重新訪問該領域的基礎。大多數學術心理學家不會區分機率和密度,直到最近,很少有人意識到貝式和次數主義機率之間的區別。然而,我認為在繼續應用之前理解這些很重要。例如,在進行統計推論時,有很多關於您“允許”說什麼的規則,其中許多看起來很武斷和奇怪。然而,如果您瞭解貝式與次數主義的區別,它們就會變得有意義。同樣,在 單元 12 中,我們將談論所謂的 t 檢定,如果您真的想完全理解 t 檢定的機制,那麼瞭解 t 分佈的實際情況真的會有幫助。我希望您理解我的意思。\n\n\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical distributions (3rd ed). Wiley.\n\n\nFisher, R. A. (1922). On the mathematical foundation of theoretical statistics. Philosophical Transactions of the Royal Society A, 222, 309–368.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A methodological paradox. Philosophy of Science, 34, 103–115."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群取樣",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群取樣",
    "title": "8  從樣本估計未知量數",
    "section": "8.1 樣本、母群、取樣",
    "text": "8.1 樣本、母群、取樣\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 定義何謂母群\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 簡單隨機樣本\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 母群參數與樣本統計\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about [Estimating population parameters] using your sample statistics and also [Estimating a confidence interval] but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#大數法則",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#大數法則",
    "title": "8  運用樣本估計未知量數",
    "section": "8.2 大數法則",
    "text": "8.2 大數法則\n在前一節中，我們展示了一個樣本數本數是 N = 100 的虛構智力測驗實驗結果。這個結果有些令人振奮，因為真實母群的平均值是 100，而樣本平均值 98.5 是一個相當合理的近似值。在許多科學研究中，這種精確度是可以接受的，但在其他情況下，我們希望更加精確。如果我們希望樣本統計量數更接近母群參數，可以怎麼做呢？顯然需要收集更多的資料。假定我們進行了一個更大型的實驗，這次測量了 10,000 人的智商。同學們可以使用 jamovi 模擬這個實驗的結果。在示範檔案庫裡的IQsim.omv ，我們生成了 10,000 個從平均值為 100、標準差為 15 的常態分佈中隨機取樣的數字。這是通過使用計算變項函式 = NORM(100, 15) 生成的。參見圖 圖 8.5 的直方圖和密度圖，模擬結果顯示這個更大的樣本比較小的樣本更近似真實母群分佈。這也反映在樣本統計量數。更大樣本的平均智力分數是 99.68，標準差為 14.90。這些值現在非常接近真實母群。\n\n\n\n\n\n\n\n圖 8.5: 使用jamovi產生由符合常態分佈的母群隨機取樣之結果。\n\n\n\n\n希望同學能從這個示範得到一點啟示，雖然有點不好意思，因為大樣本能提供更有品質的訊息，是顯而易見的，似乎不需要特別說明。其實這個觀點非常直觀，以至於機率理論的創始人之一雅各布·伯努利在1713年發表大數法則的論文時，曾經用尖酸刻薄的語氣描述這個人人都有的直覺：\n\n即使是最蠢的人，也憑藉本能的直覺，不靠他人教導(相當了不起)就能獨自明白：觀察的次數越多，結果就越不容易偏離目標。 (Stigler, 1986, p. 第65頁)。\n\n嗯，這段話聽起來有點自大（而且還有點性別歧視），但他的主要觀點是正確的。事實上，更多數據確實會產生更好的結果。問題是，為什麼會這樣呢？不出所料，所有人類都會直覺地認為的這樣的看法是正確的，統計學家稱之為大數法則。大數法則是一條適用於許多不同樣本統計量的數學法則，但最簡單的想法就是關於平均數的法則。樣本平均值是一個最明顯的例子，因為它是算述平均的產物（因為算述平均就是一個平均值）。大數法則應用於算述平均的意思是，隨著樣本數增加，算述平均的結果越趨近於真實的母群平均值。或者更精確一點地說，當樣本數“趨近”於無窮大（寫為\\(N \\longrightarrow \\infty\\)）時，樣本平均值趨近於母群平均值（\\(\\bar{X} \\longrightarrow \\mu\\)）4。\n我並不打算向同學示範如何證明大數法則，不過它是統計理論中最重要的工具之一。大數法則可以用來證明，收集越多的資料，最終將接近真相的數學工具。對於任何特定的資料集，所計算出來的樣本統計量數都可能是錯誤的，但是大數法則告訴我們，只要繼續收集更多的數據，這些樣本統計量數將趨近於真實的母群參數。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本分佈與中央極限定理",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本分佈與中央極限定理",
    "title": "8  運用樣本估計未知量數",
    "section": "8.3 樣本分佈與中央極限定理",
    "text": "8.3 樣本分佈與中央極限定理\n大數法則是非常強大的工具，但它並不能回答所有現實生活的問題。它給我們的只是一個“長期保證”。長期而言，如果我們能夠收集無限量的資料，那麼大數法則保證我們的樣本統計數據是正確的。但是正如經濟學大師凱因斯的著名論點，長期保證在我們的現實生活中幾乎沒有用處。\n\n長期保證對當前問題的指引是一個誤導，(因為問題解決時)，我們(可能)都已經死去。如果經濟學家只能告訴我們，當暴風雨過去後，海洋會再次變平靜，那麼他們的任務就太容易也太無用了，特別是在風雨飄搖的時刻。 (Keynes, 1923, p. 80)\n\n就像經濟學一樣，心理學和統計學也是如此。僅僅知道最終計算出的樣本平均值會趨近於母體平均值是不夠的。一個無限大的資料集終將揭露實際的母群平均值，只是精神上的慰藉，但是如果實際的資料樣本數只有 \\(N = 100\\) ，那麼這樣的知識在現實生活中是不夠用的。在心理學的實際研究場景，我們通常要從一個樣本數更小的資料計算樣本平均值，描述這群樣本的行為！\n\n\n8.3.1 平均值的樣本分佈\n考慮到現實情況，讓我們不得不放棄這樣的想法，也就是不要期待我們的研究會收集到一萬人，改成設計一個規模非常小的實驗。這次只找 \\(N=5\\) 個人，測量他們的智力分數。和前面的示範一樣，我們可以在 jamovi 中使用 NORM(100,15) 函式模擬這個實驗，但是這一次我只需要5個參與者，而不是10,000個。以下是 jamovi 隨機生成的五個數值(應該和你的操作結果不同)：\n\n90 82 94 99 110\n這批樣本的平均智力分數恰好是95。不出所料，比起前面示範的實驗結果不準確得多。現在想像一下，我決定重現(replicate)這項實驗。也就是說，我會盡可能地重做原來的實驗程序，隨機選擇另外5個人並測量他們的智力分數。同樣地，Jamovi可以模擬這個過程的結果，生成以下5個數字(也應該和你的操作結果不同)：\n\n78 88 111 111 117\n這次樣本的平均智力分數是101。如果我重做這個實驗10次，可能得到的結果如 表8-1 ，可以看到每次實驗的樣本平均值都不一樣。\n\n表8-1：重現智力測驗實驗十次的結果，每次樣本數都是 \\(( N = 5 )\\) 。\n\n\n\n\n\n\n\n\n\n\n\n\n\n參與者 1\n參與者 2\n參與者 3\n參與者 4\n參與者 5\n樣本平均值\n\n\n\n\n實驗批次 1\n90\n82\n94\n99\n110\n95.0\n\n\n實驗批次 2\n78\n88\n111\n111\n117\n101.0\n\n\n實驗批次 3\n111\n122\n91\n98\n86\n101.6\n\n\n實驗批次 4\n98\n96\n119\n99\n107\n103.8\n\n\n實驗批次 5\n105\n113\n103\n103\n98\n104.4\n\n\n實驗批次 6\n81\n89\n93\n85\n114\n92.4\n\n\n實驗批次 7\n100\n93\n108\n98\n133\n106.4\n\n\n實驗批次 8\n107\n100\n105\n117\n85\n102.8\n\n\n實驗批次 9\n86\n119\n108\n73\n116\n100.4\n\n\n實驗批次 10\n95\n126\n112\n120\n76\n105.8\n\n\n\n假如現在我決定繼續以這種方式，繼續重做這個「五個智力分數」的實驗。每次完成實驗，我都會記錄下樣本平均值。隨著實驗的進行，我會累積一個新的資料集，在這個資料集中，每次實驗都有產生一個資料點。我的資料集的前10個觀察值就是 表8-1 列出的樣本平均值，因此我的資料集前幾個數值是這樣的：\n\n95.0 101.0 101.6 103.8 104.4 …\n假如我繼續重做實驗10,000次，然後用每次實驗的樣本平均值繪製直方圖，那會怎麼樣呢？這正是 圖 8.6 所展示的，5個智力分數的平均值大部分落在90到110之間。更重要的是：如果我們一遍又一遍地重做一個實驗，我們最終得到的是一個樣本平均值的分佈！（見表8-1）。統計學的正式名稱叫做樣本平均值的取樣分佈(sampling distribution of the mean)。\n\n\n\n\n\n\n圖 8.6: 「五個智力分數實驗」的樣本平均值分配。假如你隨機找五個人並計算他們的平均智力測驗分數，幾乎會得到一個介於80和120之間的數字，即使很多人測得的智力分數是高於120或低於80。為了比較，黑線表示智力分數的母群分佈。\n\n\n\n\n取樣分佈是另一個重要的統計理論概念，對於理解小樣本的行為非常關鍵。例如，當我進行第一個“五個智力分數”的實驗時，樣本平均值是95。但是 圖 8.6 的取樣分佈告訴我們，“五個智力分數”的實驗不是很準確。如果我重複進行實驗，取樣分佈告訴我，我可以預期大部分的樣本平均值會是80到120之間。\n\n\n\n8.3.2 任何數值皆有樣本分佈！\n針對取樣分佈的概念，需要注意的一點是，任何你可能想要計算的樣本統計量數都有其對應的取樣分佈。例如，假設每次我重複進行 “五個智力分數” 的實驗時，我都記錄其中最高的智力分數。這會產生一個資料集，一開始的數值可能如下：\n\n110 117 122 119 113 …\n反覆進行這個實驗過程會給我一個非常不同的取樣分佈，也就是最大值的取樣分佈。五個智力分數的最大值的取樣分佈顯示在 圖 8.7 。不出所料，如果你隨機選擇五個人，然後找出智力分數最高的人，他們的智力分數很有可能會高於平均水準。大多數情況下，你會得到智力分數在100到140之間的結果。\n\n\n\n\n\n\n圖 8.7: 「五個智力分數實驗」最大值的取樣分佈。如果您隨機選取5個人，然後紀錄最高的智力分數，您可能會看到多數智力分數落在100到140之間。\n\n\n\n\n\n\n8.3.3 中央極限定理\n\n\n\n\n\n圖 8.8: 這是一個說明樣本數如何影響樣本平均數的取樣分佈的例子。每張圖的直方圖是由10,000組智力分數樣本平值構成。圖中的直方圖顯示了這些平均值的分佈（即平均值的取樣分佈）。每個單獨的智力分數都是從平均值為100，標準差為15的常態分佈選機選取的，這在圖中以實線表示。圖(a)的每個資料集僅包含一個觀察值，因此每個樣本的平均值就是一個人的智力分數。因此，平均數的取樣分佈當然與智力分數的母群分佈相同。然而，當我們增加樣本數到2，任何一個樣本的平均值都比任何一個人的智力分數更接近母體平均值，因此直方圖（即取樣分佈）比母群分佈更窄。當樣本數提高到10（圖(c)），可以看到樣本平均值的分佈會緊密地聚集在真實的母體平均值周圍。\n\n\n\n\n至此希望同學對取樣分佈有充分的理解，特別是有關平均值的取樣分佈。在這一節，我想談談平均值的取樣分佈如何隨著樣本數而改變。以直觀來說，你應該已經知道一部分的答案。如果你只有幾個觀察值，樣本平均值可能相當不準確。假如你持續重做一個小樣本實驗並計算平均值，你會得到一個非常不同的答案。換句話說，取樣分佈的變異範圍非常寬。如果你持續重做一個大樣本實驗並計算樣本平均值，你有可能會得到和上次實驗一樣的結果，因此取樣分佈的變異範圍會非常窄。你可以在 圖 8.8 看到這個直觀敘述的效果，由左到右的圖表顯示樣本數越大，取樣分佈越窄。我們可以通過計算取樣分佈的標準差來量化這種變化，統計學名詞為標準誤差(standard error)。統計學報告裡的標準誤差通常寫成SE，由於最常報告的標準誤差是樣本平均值的，因此我們通常使用縮寫SEM(standard error of the sample mean)。從 圖 8.8 可以看出，隨著樣本數 \\(N\\) 的增加，SEM會減少。\n好的，走到這一節，我有一個到目前為止一直省略的部分。至此示範的模擬實驗都是基於”智力測驗分數”，是因為智力測驗分數大致呈現常態分佈，所以我假設母群的分佈也是常態。如果母群的分佈不是常態分佈，那麼樣本平均數的取樣分佈會變成什麼樣子？令初次學習的同學驚訝的是，無論母群分佈是什麼形狀，當樣本數 \\(N\\) 增加時，樣本平均數的取樣分佈會越來越像是常態分佈。為了讓同學了解，我進行了一些模擬。我們首先從 圖 8.9 (a)，像“斜坡”的母群分佈開始。比較看起來像三角形的直方圖和黑色的鐘形曲線，同學可以看出母群分佈看起來根本不像常態分佈。接下來，我做了大量模擬的實驗。在每次實驗，我從母群隨機選取 \\(N=2\\) 個樣本，然後計算樣本平均值。圖 8.9 (b) 繪製了這些樣本平均值的直方圖（即 \\(N=2\\) 時的樣本平均數的取樣分佈）。累積的直方圖近似 \\(\\chi^2\\) 分佈。雖然不是常態分佈，但是比起 圖 8.9（a）的母群分佈更接近黑線。當我將樣本大小增加到 \\(N=4\\) 時，樣本平均數的取樣分佈就非常接近常態分佈（圖 8.9（c）），當樣本數達到 \\(N=8\\) 時，它幾乎完全等於常態分佈。換句話說，只要你的樣本數不是太小，無論你的母群分佈長什麼樣子，樣本平均值的取樣分佈都會近似於常態分佈！\n\n\n\n\n\n\n圖 8.9: 中央極限定理的視覺化示範。圖(a)是一個非常態分佈的母群分佈，而圖 (b) - (d) 展示從圖 (a) 取得樣本數分別為 2、4 和 8 的樣本平均值的取樣分佈。正如您所看到的，即使原始母群分佈不是常態分佈，隨著樣本數增加，樣本平均值的取樣分佈會趨近常態分佈。\n\n\n\n\n根據以上視覺化展示，關於樣本平均值的取樣分佈，我們可以得出以下結論：\n\n取樣分佈的平均值與母群的平均值相同。\n隨著樣本數的增加，取樣分佈的標準差（即標準誤）越來越小。\n隨著樣本數的增加，取樣分佈的形狀變得越來越接近常態分佈。\n\n中央極限定理（Central Limit Theorem）是統計學中的一個著名定理，除了證明上述所有說法都是正確的。中央極限定理也告訴我們，假設母群的平均值為\\(\\mu\\)，標準差為\\(\\sigma\\)，那麼樣本平均值的取樣分佈的平均值也是\\(\\mu\\)，而平均值的標準誤則是：\n\\[SEM=\\frac{\\sigma}{\\sqrt{N}}\\]\n因為是母群標準差\\(\\sigma\\)除以樣本大小 N 的平方根，因此SEM會隨著樣本數的增加而變小。中央極限定理還告訴我們，樣本平均值的取樣分佈形狀會變成常態分佈。5\n中央極限定理在處理各種問題都很有用。它告訴我們為什麼大樣本實驗比小樣本實驗更可靠，而且因為有標準誤的明確公式，所以它還告訴我們大樣本實驗的可靠性有多高。它也解釋了為什麼常態分佈是正常的。在真正的實驗中，我們想要測量的許多事物實際上是綜合各種不同數值指標的平均值（例如，智力測驗所測量的“普遍”智力是很多“具體”技能和能力的平均值），遇到這種情況時，各種指標的平均值應該遵循常態分佈。因為中央極限定理，常態分佈在各種真實數據隨處可見。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#估計母群參數",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#估計母群參數",
    "title": "8  從樣本估計未知量數",
    "section": "8.4 估計母群參數",
    "text": "8.4 估計母群參數\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.5 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 母群平均值\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 母群標準差\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.6 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on [Estimating population parameters], the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in [Estimating population parameters]. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).7\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "href": "09-Hypothesis-testing.html#a-menagerie-of-hypotheses",
    "title": "9  Hypothesis testing",
    "section": "9.1 A menagerie of hypotheses",
    "text": "9.1 A menagerie of hypotheses\nEventually we all succumb to madness. For me, that day will arrive once I’m finally promoted to full professor. Safely ensconced in my ivory tower, happily protected by tenure, I will finally be able to take leave of my senses (so to speak) and indulge in that most thoroughly unproductive line of psychological research, the search for extrasensory perception (ESP).3\nLet’s suppose that this glorious day has come. My first study is a simple one in which I seek to test whether clairvoyance exists. Each participant sits down at a table and is shown a card by an experimenter. The card is black on one side and white on the other. The experimenter takes the card away and places it on a table in an adjacent room. The card is placed black side up or white side up completely at random, with the randomisation occurring only after the experimenter has left the room with the participant. A second experimenter comes in and asks the participant which side of the card is now facing upwards. It’s purely a one-shot experiment. Each person sees only one card and gives only one answer, and at no stage is the participant actually in contact with someone who knows the right answer. My data set, therefore, is very simple. I have asked the question of N people and some number \\(X\\) of these people have given the correct response. To make things concrete, let’s suppose that I have tested \\(N = 100\\) people and \\(X = 62\\) of these got the answer right. A surprisingly large number, sure, but is it large enough for me to feel safe in claiming I’ve found evidence for ESP? This is the situation where hypothesis testing comes in useful. However, before we talk about how to test hypotheses, we need to be clear about what we mean by hypotheses.\n\n9.1.1 Research hypotheses versus statistical hypotheses\nThe first distinction that you need to keep clear in your mind is between research hypotheses and statistical hypotheses. In my ESP study my overall scientific goal is to demonstrate that clairvoyance exists. In this situation I have a clear research goal: I am hoping to discover evidence for ESP. In other situations I might actually be a lot more neutral than that, so I might say that my research goal is to determine whether or not clairvoyance exists. Regardless of how I want to portray myself, the basic point that I’m trying to convey here is that a research hypothesis involves making a substantive, testable scientific claim. If you are a psychologist then your research hypotheses are fundamentally about psychological constructs. Any of the following would count as research hypotheses:\n\nListening to music reduces your ability to pay attention to other things. This is a claim about the causal relationship between two psychologically meaningful concepts (listening to music and paying attention to things), so it’s a perfectly reasonable research hypothesis.\nIntelligence is related to personality. Like the last one, this is a relational claim about two psychological constructs (intelligence and personality), but the claim is weaker: correlational not causal\nIntelligence is speed of information processing. This hypothesis has a quite different character. It’s not actually a relational claim at all. It’s an ontological claim about the fundamental character of intelligence (and I’m pretty sure this one actually. It’s usually easier to think about how to construct experiments to test research hypotheses of the form “does \\(X\\) affect \\(Y\\)?” than it is to address claims like “what is \\(X\\)?” And in practice what usually happens is that you find ways of testing relational claims that follow from your ontological ones. For instance, if I believe that intelligence is speed of information processing in the brain, my experiments will often involve looking for relationships between measures of intelligence and measures of speed. As a consequence most everyday research questions do tend to be relational in nature, but they’re almost always motivated by deeper ontological questions about the state of nature.\n\nNotice that in practice, my research hypotheses could overlap a lot. My ultimate goal in the ESP experiment might be to test an ontological claim like “ESP exists”, but I might operationally restrict myself to a narrower hypothesis like “Some people can ‘see’ objects in a clairvoyant fashion”. That said, there are some things that really don’t count as proper research hypotheses in any meaningful sense:\n\nLove is a battlefield. This is too vague to be testable. Whilst it’s okay for a research hypothesis to have a degree of vagueness to it, it has to be possible to operationalise your theoretical ideas. Maybe I’m just not creative enough to see it, but I can’t see how this can be converted into any concrete research design. If that’s true then this isn’t a scientific research hypothesis, it’s a pop song. That doesn’t mean it’s not interesting. A lot of deep questions that humans have fall into this category. Maybe one day science will be able to construct testable theories of love, or to test to see if God exists, and so on. But right now we can’t, and I wouldn’t bet on ever seeing a satisfying scientific approach to either.\nThe first rule of tautology club is the first rule of tautology club. This is not a substantive claim of any kind. It’s true by definition. No conceivable state of nature could possibly be inconsistent with this claim. We say that this is an unfalsifiable hypothesis, and as such it is outside the domain of science. Whatever else you do in science your claims must have the possibility of being wrong.\nMore people in my experiment will say “yes” than “no”. This one fails as a research hypothesis because it’s a claim about the data set, not about the psychology (unless of course your actual research question is whether people have some kind of “yes” bias!). Actually, this hypothesis is starting to sound more like a statistical hypothesis than a research hypothesis.\n\nAs you can see, research hypotheses can be somewhat messy at times and ultimately they are scientific claims. Statistical hypotheses are neither of these two things. Statistical hypotheses must be mathematically precise and they must correspond to specific claims about the characteristics of the data generating mechanism (i.e., the “population”). Even so, the intent is that statistical hypotheses bear a clear relationship to the substantive research hypotheses that you care about! For instance, in my ESP study my research hypothesis is that some people are able to see through walls or whatever. What I want to do is to “map” this onto a statement about how the data were generated. So let’s think about what that statement would be. The quantity that I’m interested in within the experiment is \\(P(correct)\\), the true-but-unknown probability with which the participants in my experiment answer the question correctly. Let’s use the Greek letter \\(\\theta\\) (theta) to refer to this probability. Here are four different statistical hypotheses:\n\nIf ESP doesn’t exist and if my experiment is well designed then my participants are just guessing. So I should expect them to get it right half of the time and so my statistical hypothesis is that the true probability of choosing correctly is \\(\\theta=0.5\\) .\nAlternatively, suppose ESP does exist and participants can see the card. If that’s true people will perform better than chance and the statistical hypothesis is that \\(\\theta > 0.5\\).\nA third possibility is that ESP does exist, but the colours are all reversed and people don’t realise it (okay, that’s wacky, but you never know). If that’s how it works then you’d expect people’s performance to be below chance. This would correspond to a statistical hypothesis that \\(\\theta < 0.5\\).\nFinally, suppose ESP exists but I have no idea whether people are seeing the right colour or the wrong one. In that case the only claim I could make about the data would be that the probability of making the correct answer is not equal to 0.5. This corresponds to the statistical hypothesis that \\(\\theta \\neq 0.5\\).\n\nAll of these are legitimate examples of a statistical hypothesis because they are statements about a population parameter and are meaningfully related to my experiment.\nWhat this discussion makes clear, I hope, is that when attempting to construct a statistical hypothesis test the researcher actually has two quite distinct hypotheses to consider. First, he or she has a research hypothesis (a claim about psychology), and this then corresponds to a statistical hypothesis (a claim about the data generating population). In my ESP example these might be as shown in Table 9.1.\n\n\n\n\nTable 9.1:  Research and statistical hypotheses \n\nDani's research hypothesis:\"ESP exists\"\n\nDani's statistical hypothesis:\\( \\theta \\neq 0.5 \\)\n\n\n\n\n\nAnd a key thing to recognise is this. A statistical hypothesis test is a test of the statistical hypothesis, not the research hypothesis. If your study is badly designed then the link between your research hypothesis and your statistical hypothesis is broken. To give a silly example, suppose that my ESP study was conducted in a situation where the participant can actually see the card reflected in a window. If that happens I would be able to find very strong evidence that \\(\\theta \\neq 0.5\\), but this would tell us nothing about whether “ESP exists”.\n\n\n9.1.2 Null hypotheses and alternative hypotheses\nSo far, so good. I have a research hypothesis that corresponds to what I want to believe about the world, and I can map it onto a statistical hypothesis that corresponds to what I want to believe about how the data were generated. It’s at this point that things get somewhat counter-intuitive for a lot of people. Because what I’m about to do is invent a new statistical hypothesis (the “null” hypothesis, \\(H_0\\) ) that corresponds to the exact opposite of what I want to believe, and then focus exclusively on that almost to the neglect of the thing I’m actually interested in (which is now called the “alternative” hypothesis, H1). In our ESP example, the null hypothesis is that \\(\\theta = 0.5\\), since that’s what we’d expect if ESP didn’t exist. My hope, of course, is that ESP is totally real and so the alternative to this null hypothesis is \\(\\theta \\neq 0.5\\). In essence, what we’re doing here is dividing up the possible values of \\(\\theta\\) into two groups: those values that I really hope aren’t true (the null), and those values that I’d be happy with if they turn out to be right (the alternative). Having done so, the important thing to recognise is that the goal of a hypothesis test is not to show that the alternative hypothesis is (probably) true. The goal is to show that the null hypothesis is (probably) false. Most people find this pretty weird.\nThe best way to think about it, in my experience, is to imagine that a hypothesis test is a criminal trial4, the trial of the null hypothesis. The null hypothesis is the defendant, the researcher is the prosecutor, and the statistical test itself is the judge. Just like a criminal trial, there is a presumption of innocence. The null hypothesis is deemed to be true unless you, the researcher, can prove beyond a reasonable doubt that it is false. You are free to design your experiment however you like (within reason, obviously!) and your goal when doing so is to maximise the chance that the data will yield a conviction for the crime of being false. The catch is that the statistical test sets the rules of the trial and those rules are designed to protect the null hypothesis, specifically to ensure that if the null hypothesis is actually true the chances of a false conviction are guaranteed to be low. This is pretty important. After all, the null hypothesis doesn’t get a lawyer, and given that the researcher is trying desperately to prove it to be false someone has to protect it."
  },
  {
    "objectID": "09-Hypothesis-testing.html#two-types-of-errors",
    "href": "09-Hypothesis-testing.html#two-types-of-errors",
    "title": "9  Hypothesis testing",
    "section": "9.2 Two types of errors",
    "text": "9.2 Two types of errors\nBefore going into details about how a statistical test is constructed it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky. For instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence for a conclusion that the coin is biased, but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we always have to accept that there’s a chance that we made a mistake. As a consequence the goal behind statistical hypothesis testing is not to eliminate errors, but to minimise them.\nAt this point, we need to be a bit more precise about what we mean by “errors”. First, let’s state the obvious. It is either the case that the null hypothesis is true or that it is false, and our test will either retain the null hypothesis or reject it.5 So, as Table 9.2 illustrates, after we run the test and make our choice one of four things might have happened:\n\n\n\n\nTable 9.2:  Null hypothesis statistical testing (NHST) \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\nAs a consequence there are actually two different types of error here. If we reject a null hypothesis that is actually true then we have made a type I error. On the other hand, if we retain the null hypothesis when it is in fact false then we have made a type II error.\nRemember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidential rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant, as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way. Punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same. The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted \\(\\alpha\\), is called the significance level of the test. And I’ll say it again, because it is so central to the whole set-up, a hypothesis test is said to have significance level \\(\\alpha\\) if the type I error rate is no larger than \\(\\alpha\\).\nSo, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by \\(\\beta\\). However, it’s much more common to refer to the power of the test, that is the probability with which we reject a null hypothesis when it really is false, which is \\(1 - \\beta\\). To help keep this straight, here’s the same table again but with the relevant numbers added (Table 9.3):\n\n\n\n\nTable 9.3:  Null hypothesis statistical testing (NHST) - additional detail \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\nA “powerful” hypothesis test is one that has a small value of \\(\\beta\\), while still keeping \\(\\alpha\\) fixed at some (small) desired level. By convention, scientists make use of three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) and \\(.001\\). Notice the asymmetry here; the tests are designed to ensure that the \\(\\alpha\\) level is kept small but there’s no corresponding guarantee regarding \\(\\beta\\). We’d certainly like the type II error rate to be small and we try to design tests that keep it small, but this is typically secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one”. To be honest, I don’t know that I agree with this philosophy. There are situations where I think it makes sense, and situations where I think it doesn’t, but that’s neither here nor there. It’s how the tests are built."
  },
  {
    "objectID": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "href": "09-Hypothesis-testing.html#test-statistics-and-sampling-distributions",
    "title": "9  Hypothesis testing",
    "section": "9.3 Test statistics and sampling distributions",
    "text": "9.3 Test statistics and sampling distributions\nAt this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the form of the data is that \\(X\\) out of \\(N\\) people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true, that ESP doesn’t exist and the true probability that anyone picks the correct colour is exactly \\(\\theta = 0.5\\). What would we expect the data to look like? Well, obviously we’d expect the proportion of people who make the correct response to be pretty close to \\(50\\%\\). Or, to phrase this in more mathematical terms, we’d say that \\(\\frac{X}{N}\\) is approximately \\(0.5\\). Of course, we wouldn’t expect this fraction to be exactly \\(0.5\\). If, for example, we tested \\(N = 100\\) people and \\(X = 53\\) of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if \\(X = 99\\) of our participants got the question right then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only \\(X = 3\\) people got the answer right we’d be similarly confident that the null was wrong. Let’s be a little more technical about this. We have a quantity \\(X\\) that we can calculate by looking at our data. After looking at the value of \\(X\\) we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a test statistic.\nHaving chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so we need to determine what the sampling distribution of the test statistic would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section 8.3.1. Why do we need this? Because this distribution tells us exactly what values of X our null hypothesis would lead us to expect. And, therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.\nHow do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter \\(\\theta\\) is just the overall probability that people respond correctly when asked the question, and our test statistic \\(X\\) is the count of the number of people who did so out of a sample size of N. We’ve seen a distribution like this before, in Section 7.4, and that’s exactly what the binomial distribution describes! So, to use the notation and terminology that I introduced in that section, we would say that the null hypothesis predicts that \\(X\\) is binomially distributed, which is written\n\\[X \\sim Binomial(\\theta,N)\\]\nSince the null hypothesis states that \\(\\theta = 0.5\\) and our experiment has \\(N = 100\\) people, we have the sampling distribution we need. This sampling distribution is plotted in Figure 9.1. No surprises really, the null hypothesis says that \\(X = 50\\) is the most likely outcome, and it says that we’re almost certain to see somewhere between \\(40\\) and \\(60\\) correct responses.\n\n\n\n\n\nFigure 9.1: The sampling distribution for our test statistic \\(X\\) when the null hypothesis is true. For our ESP scenario this is a binomial distribution. Not surprisingly, since the null hypothesis says that the probability of a correct response is \\(\\theta = .5\\), the sampling distribution says that the most likely value is 50 (out of 100) correct responses. Most of the probability mass lies between 40 and 60"
  },
  {
    "objectID": "09-Hypothesis-testing.html#making-decisions",
    "href": "09-Hypothesis-testing.html#making-decisions",
    "title": "9  Hypothesis testing",
    "section": "9.4 Making decisions",
    "text": "9.4 Making decisions\nOkay, we’re very close to being finished. We’ve constructed a test statistic \\((X)\\) and we chose this test statistic in such a way that we’re pretty confident that if \\(X\\) is close to \\(\\frac{N}{2}\\) then we should retain the null, and if not we should reject it. The question that remains is this. Exactly which values of the test statistic should we associate with the null hypothesis, and exactly which values go with the alternative hypothesis? In my ESP study, for example, I’ve observed a value of \\(X = 62\\). What decision should I make? Should I choose to believe the null hypothesis or the alternative hypothesis?\n\n9.4.1 Critical regions and critical values\nTo answer this question we need to introduce the concept of a critical region for the test statistic X. The critical region of the test corresponds to those values of X that would lead us to reject null hypothesis (which is why the critical region is also sometimes called the rejection region). How do we find this critical region? Well, let’s consider what we know:\n\n\\(X\\) should be very big or very small in order to reject the null hypothesis\nIf the null hypothesis is true, the sampling distribution of \\(X\\) is \\(Binomial(0.5, N)\\)\nIf \\(\\alpha = .05\\), the critical region must cover 5% of this sampling distribution.\n\nIt’s important to make sure you understand this last point. The critical region corresponds to those values of \\(X\\) for which we would reject the null hypothesis, and the sampling distribution in question describes the probability that we would obtain a particular value of \\(X\\) if the null hypothesis were actually true. Now, let’s suppose that we chose a critical region that covers \\(20\\%\\) of the sampling distribution, and suppose that the null hypothesis is actually true. What would be the probability of incorrectly rejecting the null? The answer is of course \\(20\\%\\). And, therefore, we would have built a test that had an α level of \\(0.2\\). If we want \\(\\alpha = .05\\), the critical region is only allowed to cover 5% of the sampling distribution of our test statistic.\nAs it turns out those three things uniquely solve the problem. Our critical region consists of the most extreme values, known as the tails of the distribution. This is illustrated in Figure 9.2. If we want \\(\\alpha = .05\\) then our critical regions correspond to \\(X \\leq 40\\) and \\(X \\geq 60\\).6 That is, if the number of people saying “true” is between 41 and 59, then we should retain the null hypothesis. If the number is between \\(0\\) to \\(40\\), or between \\(60\\) to \\(100\\), then we should reject the null hypothesis. The numbers \\(40\\) and \\(60\\) are often referred to as the critical values since they define the edges of the critical region\n\n\n\n\n\nFigure 9.2: The critical region associated with the hypothesis test for the ESP study, for a hypothesis test with a significance level of \\(\\alpha = .05\\). The plot shows the sampling distribution of \\(X\\) under the null hypothesis (i.e., same as Figure 9.1) . The grey bars correspond to those values of \\(X\\) for which we would retain the null hypothesis. The blue (darker shaded) bars show the critical region, those values of \\(X\\) for which we would reject the null. Because the alternative hypothesis is two sided (i.e., allows both \\(\\theta < .5\\) and \\(\\theta > .5\\), the critical region covers both tails of the distribution. To ensure an \\(\\alpha\\) level of \\(.05\\), we need to ensure that each of the two regions encompasses \\(2.5\\%\\) of the sampling distribution\n\n\n\n\nAt this point, our hypothesis test is essentially complete:\n\nWe choose an α level (e.g., \\(\\alpha = .05\\));\nCome up with some test statistic (e.g., \\(X\\)) that does a good job (in some meaningful sense) of comparing \\(H_0\\) to \\(H_1\\);\nFigure out the sampling distribution of the test statistic on the assumption that the null hypothesis is true (in this case, binomial); and then\nCalculate the critical region that produces an appropriate α level (0-40 and 60-100).\n\nAll that we have to do now is calculate the value of the test statistic for the real data (e.g., X = 62) and then compare it to the critical values to make our decision. Since 62 is greater than the critical value of 60 we would reject the null hypothesis. Or, to phrase it slightly differently, we say that the test has produced a statistically significant result.\n\n\n9.4.2 A note on statistical “significance”\n\nLike other occult techniques of divination, the statistical method has a private jargon deliberately contrived to obscure its methods from non-practitioners.\n– Attributed to G. O. Ashley 7\n\nA very brief digression is in order at this point, regarding the word “significant”. The concept of statistical significance is actually a very simple one, but has a very unfortunate name. If the data allow us to reject the null hypothesis, we say that “the result is statistically significant”, which is often shortened to “the result is significant”. This terminology is rather old and dates back to a time when “significant” just meant something like “indicated”, rather than its modern meaning which is much closer to “important”. As a result, a lot of modern readers get very confused when they start learning statistics because they think that a “significant result” must be an important one. It doesn’t mean that at all. All that “statistically significant” means is that the data allowed us to reject a null hypothesis. Whether or not the result is actually important in the real world is a very different question, and depends on all sorts of other things.\n\n\n9.4.3 The difference between one sided and two sided tests\nThere’s one more thing I want to point out about the hypothesis test that I’ve just constructed. If we take a moment to think about the statistical hypotheses I’ve been using, \\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] we notice that the alternative hypothesis covers both the possibility that \\(\\theta < .5\\) and the possibility that \\(\\theta \\> .5.\\) This makes sense if I really think that ESP could produce either better-than chance performance or worse-than-chance performance (and there are some people who think that). In statistical language this is an example of a two-sided test. It’s called this because the alternative hypothesis covers the area on both “sides” of the null hypothesis, and as a consequence the critical region of the test covers both tails of the sampling distribution (2.5% on either side if α = .05), as illustrated earlier in Figure 9.2. However, that’s not the only possibility. I might only be willing to believe in ESP if it produces better than chance performance. If so, then my alternative hypothesis would only covers the possibility that \\(\\theta > .5\\), and as a consequence the null hypothesis now becomes \\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] When this happens, we have what’s called a one-sided test and the critical region only covers one tail of the sampling distribution. This is illustrated in Figure 9.3.\n\n\n\n\n\nFigure 9.3: The critical region for a one sided test. In this case, the alternative hypothesis is that \\(\\theta \\geq .5\\) so we would only reject the null hypothesis for large values of \\(X\\). As a consequence, the critical region only covers the upper tail of the sampling distribution, specifically the upper \\(5\\%\\) of the distribution. Contrast this to the two-sided version in Figure 9.2"
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "href": "09-Hypothesis-testing.html#sec-The-p-value-of-a-test",
    "title": "9  假設檢定",
    "section": "9.5 統計檢定的p值",
    "text": "9.5 統計檢定的p值\n在某種意義上，我們已經完成假設檢定程序。我們已經建立一個統計檢定值，設定好如果虛無假設符合事實的取樣分佈，並為檢定結果決策設定棄卻域。然而，其實我還省略了一個最重要的數值 – p值。p 值有兩種不同的解釋版本，一種是由羅納德·費雪爵士(Sir Ronald Fisher)提出，另一種是由傑茲·尼曼 (Jerzy Neyman) 提出。兩種版本都是統計學家接受的解釋方法，雖然兩位大師的假設檢定思維彼此不相同。大多數統計學教科書只會講費雪的版本，但我認為這有點可惜。我認為尼曼的版本更簡潔，更能反映虛無假設檢定的邏輯。當然，也許讀者會有不同意見，以下兩種版本都會介紹。我先從尼曼的版本開始介紹。\n\n\n9.5.1 運用p值做決策的簡單理由\n前面描述的假設檢定程序有一個問題，就是並沒有區分“剛好顯著”和“非常顯著”的結果。像是在我的ESP研究案例裡，所獲得的資料只是剛好落在棄卻域的邊緣，讓我我確實得到了一個顯著結果，但這個結論其實非常微妙。若是我另外進行了一項研究，\\(N=100\\) 參與者中有 \\(X=97\\) 人回答正確，顯然這個結果也是顯著的，但是顯著性的程度要大得多，沒有任何模糊空間。前面描述的程序都沒有區分這兩種情況，若是我採用慣例做法，只以 \\(\\alpha=.05\\) 做為我可以接受的型一錯誤率，那麼這兩個結果都是顯著的。\n這裡就是p值派上用場的地方了。為了容易理解其中的原理，讓我們想像對同一組資料做了好幾次假設檢定，但是每次設定不一樣的顯著水準 \\(\\alpha\\) 。對我所得到的 ESP 資料( \\(X=62\\) )進行好幾次檢定後，得到的結論大致如 表 9.4 。\n\n\n\n\n\n\n表 9.4: 以不同顯著水準執行假設檢定的結果\n\n\nα值\nX0.05\nX0.04\nX0.03\nX0.02\nX0.01\n\n\n\n\n是否拒絕虛無假設\n是\n是\n是\n否\n否\n\n\n\n\n\n\n\n\n對我的ESP資料（100次觀察中有62次成功）進行幾次檢定後，以\\(\\alpha=.03\\)及以上的顯著水準決策，都是會拒絕虛無假設。以\\(\\alpha=.02\\)及以下的水準，都是是會保留虛無假設。因此，在\\(.02\\)和\\(.03\\)之間必定有一個最小的\\(\\alpha\\)值，讓我們可以拒絕虛無假設，這個值就是p值。最後我得到這筆ESP資料的p值為\\(.021\\)。簡而言之，p值被定義為如果你想要拒絕虛無假設的話，你必須願意容忍的最小型一錯誤率(\\(\\alpha\\))。\n如果p值顯示的決策錯誤率是大到你無法接受，那麼你必須保留虛無假設。如果你對等於p值的決策錯誤率感到滿意，那麼你可以拒絕虛無假設並支持偏好的對立假設。\n總而言之，p值是對以所有可能的顯著水準\\(\\alpha\\)，所執行的假設檢定結果總結。因此p值有”軟化”決策難度的效果。檢定結果的p值比 \\(\\alpha\\) 小的話，我們會拒絕虛無假設；而檢定結果的p值比 \\(\\alpha\\) 大的話，我們會保留虛無假設。由於我的 ESP 實驗結果是 \\(X = 62\\)，因此 p = .021，要宣稱人類有ESP的話，我必須容忍 \\(2.1%\\) 的型一錯誤率。另一方面，若是我的實驗結果是 \\(X = 97\\)，那麼p值會是多少？這次縮小為 \\(p = 1.36 \\times 10^{-25}\\) 8，這是一個非常、非常微小的型一錯誤率。根據第二次實驗結果，我會更有信心地拒絕虛無假設，因為我只需要 “願意” 容忍大約十分之一兆兆兆兆的型一錯誤率，我的結論會是正確的。\n\n\n\n\n9.5.2 獲得極端資料的機率\n第二種p值的定義來自羅納德·費雪爵士，大多數入門統計學的教科書採用這個定義解釋p值。留意一下設定棄卻域時，是不是對應到取樣分佈的尾部，也就是分佈所涵蓋的量數之極端值？這並不是巧合，幾乎所有「好的」檢定都有這種特徵（所謂「好」是指型二錯誤率 \\(\\beta\\) 被最小化）。這是因為好的棄卻域幾乎總是對應到虛無假設成立時最不可能觀察到的統計檢定值。如果這條規則成立，那麼我們可以定義p值是我們會觀察到這個檢定統計值，其極端程度至少達到我們實際上得到該統計值的機率。換句話說，如果根據虛無假設解釋資料的正確性非常的低，那麼虛無假設應該是錯誤的。\n\n\n\n9.5.3 常見的錯誤解讀\n好了，我們看到了兩種相當不同但是統計學家公認合理的解釋p值的定義，一種基於尼曼對假設檢定的想法，另一種基於費雪的可能性思考。不幸的是，還有第三種解釋，有些學生在初次學習統計學會遇到某些講師這樣介紹，但是完全錯誤的解讀。這種錯誤的定義方法是把p值稱為“虛無假設為真的機率”。這是一種直觀上很容易吸收的想法，但是有兩個關鍵錯誤。首先，虛無假設檢定是一種次數主義工具，而次數主義方法對機率的基本立場是，不允許研究者給虛無假設設定機率分佈。根據這種觀點，虛無假設要麼是真的，要麼不是真的，不可能存在“有\\(5%\\)的機率是真的”這種說法。其次，即使是貝氏方法這一派，雖然會允許研究者給假設設定機率分佈，p值也不會對應虛無假設為真的機率，這派主張與計算p值的數學原理是完全不一樣的。總之，儘管這種定義方式非常直覺，但是沒有任何公認的學術方法能充分證明p值是某種假設為真的機率。請同學千萬要謹慎分辨。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "href": "09-Hypothesis-testing.html#reporting-the-results-of-a-hypothesis-test",
    "title": "9  Hypothesis testing",
    "section": "9.6 Reporting the results of a hypothesis test",
    "text": "9.6 Reporting the results of a hypothesis test\nWhen writing up the results of a hypothesis test there’s usually several pieces of information that you need to report, but it varies a fair bit from test to test. Throughout the rest of the book I’ll spend a little time talking about how to report the results of different tests (see Section 10.1.9 for a particularly detailed example, so that you can get a feel for how it’s usually done. However, regardless of what test you’re doing, the one thing that you always have to do is say something about the \\(p\\) value and whether or not the outcome was significant.\nThe fact that you have to do this is unsurprising, it’s the whole point of doing the test. What might be surprising is the fact that there is some contention over exactly how you’re supposed to do it. Leaving aside those people who completely disagree with the entire framework underpinning null hypothesis testing, there’s a certain amount of tension that exists regarding whether or not to report the exact \\(p\\) value that you obtained, or if you should state only that \\(p < \\alpha\\) for a significance level that you chose in advance (e.g., \\(p < .05\\)).\n\n9.6.1 The issue\nTo see why this is an issue, the key thing to recognise is that p values are terribly convenient. In practice, the fact that we can compute a p value means that we don’t actually have to specify any \\(\\alpha\\) level at all in order to run the test. Instead, what you can do is calculate your p value and interpret it directly. If you get \\(p = .062\\), then it means that you’d have to be willing to tolerate a Type I error rate of \\(6.2\\%\\) to justify rejecting the null. If you personally find \\(6.2\\%\\) intolerable then you retain the null. Therefore, the argument goes, why don’t we just report the actual \\(p\\) value and let the reader make up their own minds about what an acceptable Type I error rate is? This approach has the big advantage of “softening” the decision making process. In fact, if you accept the Neyman definition of the p value, that’s the whole point of the p value. We no longer have a fixed significance level of \\(\\alpha = .05\\) as a bright line separating “accept” from “reject” decisions, and this removes the rather pathological problem of being forced to treat \\(p = .051\\) in a fundamentally different way to \\(p = .049\\).\nThis flexibility is both the advantage and the disadvantage to the \\(p\\) value. The reason why a lot of people don’t like the idea of reporting an exact \\(p\\) value is that it gives the researcher a bit too much freedom. In particular, it lets you change your mind about what error tolerance you’re willing to put up with after you look at the data. For instance, consider my ESP experiment. Suppose I ran my test and ended up with a \\(p\\) value of \\(.09\\). Should I accept or reject? Now, to be honest, I haven’t yet bothered to think about what level of Type I error I’m “really” willing to accept. I don’t have an opinion on that topic. But I do have an opinion about whether or not ESP exists, and I definitely have an opinion about whether my research should be published in a reputable scientific journal. And amazingly, now that I’ve looked at the data I’m starting to think that a \\(9\\%\\) error rate isn’t so bad, especially when compared to how annoying it would be to have to admit to the world that my experiment has failed. So, to avoid looking like I just made it up after the fact, I now say that my \\(\\alpha\\) is .1, with the argument that a \\(10\\%\\) type I error rate isn’t too bad and at that level my test is significant! I win.\nIn other words, the worry here is that I might have the best of intentions, and be the most honest of people, but the temptation to just “shade” things a little bit here and there is really, really strong. As anyone who has ever run an experiment can attest, it’s a long and difficult process and you often get very attached to your hypotheses. It’s hard to let go and admit the experiment didn’t find what you wanted it to find. And that’s the danger here. If we use the “raw” p-value, people will start interpreting the data in terms of what they want to believe, not what the data are actually saying and, if we allow that, why are we even bothering to do science at all? Why not let everyone believe whatever they like about anything, regardless of what the facts are? Okay, that’s a bit extreme, but that’s where the worry comes from. According to this view, you really must specify your \\(\\alpha\\) value in advance and then only report whether the test was significant or not. It’s the only way to keep ourselves honest\n\n\n\n\nTable 9.5:  Typical translations of p value levels \n\nUsual notationSignif. starsEnglish translationThe null is...\n\np > .05The test wasn't significantRetained\n\np < .05*The test was significant at \\( \\alpha \\) = .05 but not at \\( \\alpha \\) = .01 or \\( \\alpha \\) = .001.Rejected\n\np < .01**The test was significant at \\( \\alpha \\) = .05  and \\( \\alpha \\) = .01 but not at \\( \\alpha \\) = .001.Rejected\n\np < .001***The test was significant at all levelsRejected\n\n\n\n\n\n\n\n9.6.2 Two proposed solutions\nIn practice, it’s pretty rare for a researcher to specify a single α level ahead of time. Instead, the convention is that scientists rely on three standard significance levels: \\(.05\\), \\(.01\\) and \\(.001\\). When reporting your results, you indicate which (if any) of these significance levels allow you to reject the null hypothesis. This is summarised in Table 9.5. This allows us to soften the decision rule a little bit, since \\(p < .01\\) implies that the data meet a stronger evidential standard than \\(p < .05\\) would. Nevertheless, since these levels are fixed in advance by convention, it does prevent people choosing their α level after looking at the data\nNevertheless, quite a lot of people still prefer to report exact p values. To many people, the advantage of allowing the reader to make up their own mind about how to interpret p = .06 outweighs any disadvantages. In practice, however, even among those researchers who prefer exact p values it is quite common to just write \\(p < .001\\) instead of reporting an exact value for small p. This is in part because a lot of software doesn’t actually print out the p value when it’s that small (e.g., SPSS just writes \\(p = .000\\) whenever \\(p < .001\\)), and in part because a very small p value can be kind of misleading. The human mind sees a number like .0000000001 and it’s hard to suppress the gut feeling that the evidence in favour of the alternative hypothesis is a near certainty. In practice however, this is usually wrong. Life is a big, messy, complicated thing, and every statistical test ever invented relies on simplifications, approximations and assumptions. As a consequence, it’s probably not reasonable to walk away from any statistical analysis with a feeling of confidence stronger than \\(p < .001\\) implies. In other words, \\(p < .001\\) is really code for “as far as this test is concerned, the evidence is overwhelming.”\nIn light of all this, you might be wondering exactly what you should do. There’s a fair bit of contradictory advice on the topic, with some people arguing that you should report the exact p value, and other people arguing that you should use the tiered approach illustrated in Table 9.1. As a result, the best advice I can give is to suggest that you look at papers/reports written in your field and see what the convention seems to be. If there doesn’t seem to be any consistent pattern, then use whichever method you prefer."
  },
  {
    "objectID": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "href": "09-Hypothesis-testing.html#running-the-hypothesis-test-in-practice",
    "title": "9  Hypothesis testing",
    "section": "9.7 Running the hypothesis test in practice",
    "text": "9.7 Running the hypothesis test in practice\nAt this point some of you might be wondering if this is a “real” hypothesis test, or just a toy example that I made up. It’s real. In the previous discussion I built the test from first principles, thinking that it was the simplest possible problem that you might ever encounter in real life. However, this test already exists. It’s called the binomial test, and it’s implemented by jamovi as one of the statistical analyses available when you hit the ‘Frequencies’ button. To test the null hypothesis that the response probability is one-half \\(p = .5\\),9 and using data in which \\(x =62\\) of \\(n = 100\\) people made the correct response, available in the binomialtest.omv data file, we get the results shown in Figure 9.4.\n\n\n\n\n\nFigure 9.4: Binomial test analysis and results in jamovi\n\n\n\n\nRight now, this output looks pretty unfamiliar to you, but you can see that it’s telling you more or less the right things. Specifically, the p-value of \\(0.02\\) is less than the usual choice of \\(\\alpha = .05\\), so you can reject the null. We’ll talk a lot more about how to read this sort of output as we go along, and after a while you’ll hopefully find it quite easy to read and understand."
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "href": "09-Hypothesis-testing.html#sec-Effect-size-sample-size-and-power",
    "title": "9  假設檢定",
    "section": "9.8 效果量、樣本量、考驗力",
    "text": "9.8 效果量、樣本量、考驗力\n在前面的章節，我(原作者)一再強調統計假設檢定背後的主要設計原則，那就是要儘可能有效控制型一錯誤率。當我們設定 \\(\\alpha = .05\\) 時，就要嘗試確保虛無假設主張成立的話，最多只有 \\(5%\\) 的機率被錯誤拒絕。然而，這並不是說我們不用關心型二錯誤。從研究者進行統計實務的角度來看，當虛無假設實際不成立，卻不能拒絕所造成的錯誤非常令人困擾。考慮這一點，假設檢定的次要目標就是要儘可能讓型二錯誤率 \\(\\beta\\) 維持在最低水準，儘管我們通常不會用「最小化型二錯誤」來形容這個目標。相反的，我們常用「最大化考驗力」這樣的描述。因為考驗力的定義是 \\(1-\\beta\\)，所以兩者意思是相同的。\n\n\n9.8.1 圖解考驗力\n\n\n\n\n\n圖 9.5: 對立假設主張母群參數 \\(\\theta=0.55\\) 的取樣分佈。圖中藍色區域代表可合理拒絕虛無假設的棄卻域範圍。\n\n\n\n\n讓我們花一點時間來好好理解什麼是型二錯誤。當對立假設成立，但我們仍然無法拒絕虛無假設時，做出的決策錯誤就是型二錯誤(再看一下 表 9.2 )。較理想的情況是我們可以設定一個代表型二錯誤率的數值 \\(\\beta\\)，就像我們可以將 \\(\\alpha = .05\\) 設定為型一錯誤率一樣。不幸的是，實際的做法非常棘手。仔細看一下ESP 研究案例的對立假設主張，其實對應許多可能的 \\(\\theta\\) 值，只要是除了 0.5 之外，每個 \\(\\theta\\) 值都可以支持對立假設。若是某位參與者 選擇正確答案的真實機率為 55%（也就是 \\(\\theta = .55\\)）。若是如此， \\(X\\) 的取樣分佈就不是虛無假設所預測的分佈，因為 \\(X\\) 的最有可能的數值是 \\(100\\) 次中有 \\(55\\) 次。不僅如此，整個取樣分佈也偏移了，如同 圖 9.5 的展示。當然，棄絕域並不會改變。根據定義，棄卻域是根據虛無假設的主張所設定的。從 圖 9.5 看到的是，當虛無假設不成立， \\(\\theta = .55\\) 的取樣分佈內有很高的比例屬於棄卻域。 的確，當虛無假設主張不成立時，拒絕虛無假設的機率相對更大！然而，\\(\\theta = .55\\) 並不是唯一符合對立假設的可能值。現在讓我們看看 \\(\\theta\\) 的真實數值是 \\(0.7\\)，取樣分佈會變成什麼樣子？如同 圖 9.6 的展示，當 \\(\\theta = 0.7\\) 時，幾乎整個取樣分佈都落在棄卻域裡。因此，如果 \\(\\theta = 0.7\\)，我們正確拒絕虛無假設的機率（即檢定力）比 \\(\\theta = 0.55\\) 要大得多。簡而言之，雖然 \\(\\theta = .55\\) 和 \\(\\theta = .70\\) 都是支持對立假設的實驗結果，但型二錯誤率是不相等的。\n\n\n\n\n\n\n圖 9.6: 對立假設主張母群參數 \\(\\theta=0.70\\) 的取樣分佈，幾乎整個分佈都在棄絕域內。\n\n\n\n\n這樣討論下來，我們知道假設檢定的考驗力（即\\(1-\\beta\\)）取決於 \\(\\theta\\) 的真實數值。為了清楚說明這一點，我已經算出所有 \\(\\theta\\) 值可拒絕虛無假設的預期機率，繪製出 圖 9.7 。圖中的曲線通常稱為假設檢定的考驗力函數。這是一個能評估檢定方法品質的好工具，因為函數列出所有可能的 \\(\\theta\\) 值可達到的考驗力（\\(1-\\beta\\)）。正如圖中顯示，當 \\(\\theta\\) 的真實值越接近 \\(0.5\\) 時，檢定的考驗力會急劇降低，遠離 \\(0.5\\) 的話，考驗力就越大。\n\n\n\n\n\n\n圖 9.7: 如果真正的\\(\\theta\\)值不同於虛無假設所主 張的值（\\(\\theta=0.5\\)），我們能拒絕虛無假設的機率，也就是檢定的考驗力。可以明顯看出，當\\(\\theta\\)與\\(\\theta=0.5\\)差異越大，假設檢定的考驗力更高（正確拒絕虛無假設的機率更高）。需要注意的是，當\\(\\theta\\)確實等於\\(0.5\\)時（以黑點表示），虛無假設實際上是成立的，此時拒絕虛無假設所犯的錯誤就是型一錯誤。\n\n\n\n\n\n\n9.8.2 估計考驗力的功用\n\nSince all models are wrong the scientist must be alert to what is importantly wrong. It is inappropriate to be concerned with mice when there are tigers abroad\n- George Box (Box 1976, p. 792)\n\n圖 9.7 展示了我們能執行假設檢定的最基本條件 。如果實際狀態和虛無假設預測的狀態非常不同，那麼考驗力會非常高；但是如果實際狀態和虛無假設預測的非常相似（但不完全相同），那麼考驗力會非常低。所以，為了能讓科學家們能方便估計考驗力，統計學家發展出一些方法量化實際狀態與虛無假設之間的“相似程度”。這種統計方法就是效果量(effect size)的測量（例如 Cohen (1988); Ellis (2010)）。效果量在不同的研究中的定義可能略有不同（因此，本節僅討論一般性的定義），但是各種定義嘗試捕捉的質性想法始終一致（參考 表 9.6 ）。其中一種就像本章討論的ESP案例：真實的母群參數數值與虛無假設的參數數值之間差異有多大？如果讓 \\(\\theta_0 = 0.5\\) 表示虛無假設的參數值， \\(\\theta\\) 表示母群參數的真實數值，那麼測量效果量的方式就是計算兩種數值之間的差異（即\\(\\theta - \\theta_0\\)），或者數值差異的絕對值，\\(abs(\\theta - \\theta_0)\\)。\n\n\n\n\n\n表 9.6: 關於統計顯著性和效果量之間關係的簡單解讀指南。如果你沒有得到顯著的結果，基本上效果量多少就沒什麼意義，因為你沒有表明真的有效果的任何證據。另一方面，如果你得到一個顯著的效果，但是效果量很小，那麼你的研究結果（雖然是真實的）可能並不太值得注意。不過，由於這只是非常粗略的指南，結果有沒有意思主要取決於研究的具體內容。在某些情況下，即使效果量很小，仍可能具有重大的實用意義。因此，不要太過認真地看待這份指南，只是讓初學者有一個粗略的概念而已。\n\n\nX.\n效果量大\n效果量小\n\n\n\n\n有顯著結果\n確實發現差異，且有實用意義\n確實發現差異，但未必有實用意義\n\n\n無顯著結果\n未發現差異\n未發現差異\n\n\n\n\n\n\n\n\n為什麼要計算效果量？假設你已經進行實驗，收集完資料，進行假設檢定而得到了一個顯著性的結果。那麼你宣稱得到了一個明顯的效果，研究就結束嗎？這不就是假設檢定的功能嗎？當然不完全是。同學們現在應該會同意，進行假設檢定的目的是試圖證明虛無假設是錯誤的，但這並不是我們做研究唯一關注的事情。如果虛無假設主張\\(\\theta=0.5\\)，然後我們證實這是錯誤的，我們只是講完了一半的故事。拒絕虛無假設表示我們相信\\(\\theta \\neq 0.5\\)，但是\\(\\theta=0.51\\)和\\(\\theta=0.8\\)之間存在很大的差異。如果實驗結果是\\(\\theta=0.8\\)，那麼虛無假設不僅僅是錯了，還錯得非常離譜。另一方面，若是我們成功地拒絕虛無假設，但是\\(\\theta\\)的真實數值似乎只有0.51（這只有在規模非常大的研究中才可能發生）。這個情況的虛無假設當然是錯的，但是我們並不確定這個結果是否值得注意，因為效果量非常地小。以我創造的ESP研究案例來說，我們可能仍然感到興趣，因為證實人類真的有超能力是相當酷的事10，但是在大多數研究的狀況，即使結果顯示有\\(1%\\)的顯著差異，就算是真的有差異，通常也不是值得注意的發現。例如，假如我們今天研究了男女高中生的考試成績差異，結果發現女學生的平均成績比男生高\\(1%\\)。若是我有來自成千上萬名學生的資料，那麼這種差異幾乎肯定是有統計顯著性，但是無論p值有多小，這種差異都沒什麼意思。沒有人敢根據如此微小的差異，宣稱一個國家出現了青少年教育危機吧？正是因為這個原因，越來越多的研究者使用某種效果量的標準尺度，與假設檢定的結果並列報告。假設檢定只有告訴我們，應不應該相信所觀察到的效果是不是真實的（即不僅僅是偶然的），而效果量則透露是不是應該關心這種效果。\n\n\n\n9.8.3 增加研究考驗力的方案\n毫不意外地，現代科學家非常關心如何最大幅度提高實驗的考驗力。我們都希望實驗能夠成功，因此期望虛無假設不成立的話，能夠最大幅度地增加拒絕的機會（當然，我們通常希望虛無假設是錯誤的！）。正如前一節討論的，影響考驗力的因素之一是效果量。因此，增加考驗力的首選方案就是增加效果量。這表示在研究實務，我們所設計的研究能使效果量放到最大。以ESP研究為例，我也許認為超感官能力在安靜、昏暗的房間裡，且要儘量減少干擾，才能得到最佳實驗效果。因此，我會嘗試在這樣的環境裡進行實驗。如果我的實驗方式確實能增強人類的ESP能力，那麼\\(\\theta\\)的真實數值就會增加11，就能提高效果量。簡而言之，巧妙的實驗設計是提高考驗力的一種方式，因為實驗設計可以改變效果量。\n不幸的是，即使擁有最佳的實驗設計，大部分的研究只能得到微小效果。例如，超感官知覺也許確實存在，但是即使在最佳的設計條件測試，效果量也非常微弱。遇到這種情況，增加樣本量是增加考驗力的最佳方案。一般來說，收集越多觀察值，就越有可能區分兩種假設。比如說我找了10位參與者進行超感官知覺實驗，其中7人正確答出隱藏卡片的顏色，這樣的結果可能不會令人印象深刻。但是如果我找了10,000位參與者進行實驗，其中7,000人猜對了，這樣的結果更有可能讓人覺得是件大事。換句話說，樣本量增加，考驗力也會增加。圖 9.8 展示了兩者的關係，對於參數真實數值 \\(\\theta = 0.7\\) ，該圖顯示從1到100的所有樣本量\\(N\\)的檢定力，其中虛無假設預測 \\(\\theta_0 = 0.5\\) 。\n\n\n\n\n\n\n\n圖 9.8: ESP研究的二項式檢定考驗力與樣本量 \\(N\\) 函式關係視覺化。此圖的繪製條件還有 \\(\\theta\\) 的真實數值為0.7，但是虛無假設主張 \\(\\theta = 0.5\\) 。總體來說，樣本數 \\(N\\) 越大，考驗力越高。(註：曲線裡的鋸齒是因為 \\(\\theta\\) ， \\(\\alpha\\) ，以及二項分佈是間斷機率分佈的本質等條件，交互運作而造成的，不過整體來說不影響我想表達的目的。)\n\n\n\n\n因為考驗力很重要，規劃一個實驗的執行細節時，知道可能有多少考驗力會非常有用。當然，因為無法確定真正的效果量有多少，我們無法得知考驗力會有多高，但是有時候我們可以評估考驗力應該要有多大。能做到的話，我們就可以評估需要多大的樣本量！這種方案叫做考驗力分析，做得好的話，對於設計實驗會非常有用。有用的考驗力分析結果包括能成功執行實驗所需要的時間或金錢是否足夠等資訊。現在越來越多的研究者同意，考驗力分析應該成為實驗設計的必要部分，因此值得好好了解如何執行。然而，本書沒有打算介紹考驗力分析。有部分理由是講出來很無聊，還有部分是實質性的理由。無聊的理由是原作者還沒有時間去寫關於考驗力分析的介紹。實質理由是原作者對於考驗力分析的方法還有些懷疑的地方。作為一位研究者，我幾乎沒做過考驗力分析。我認為可能是因為 (a) 我的實驗經常有些非常規條件 ，不曉得如何正確地定義效果量，還有 (b) 我對研究結果的效果量實在是一無所知，不知道如何解釋分析結果。此外，我常與一位擔任統計諮詢師廣泛交流（其實就是我老婆），她實際接過的案子裡，只有要撰寫申請補助經費的計畫書的客戶才會詢問怎麼做考驗力分析。換句話說，在現實的研究場景，科學家們似乎只有被特別要求時才需要進行考驗力分析，而且考驗力分析並不是日常科研工作的一部分。簡而言之，我同意考驗力是一個重要的概念，但是考驗力分析並不像人們所說的那麼有用，除非在少數情況。像是（a）已經有方法能計算你要執行的實驗設計考驗力，（b）你對要測量的效果量可能大小有很好的想法12。也許其他研究者比我幸運，但我個人從未遇到過(a)和(b)都成立的情況。未來我的想法也許會改變，這本書的更新版本就會有章節介紹詳細考驗力分析，但是以目前來說，這是我編寫這個主題最心安理得的處理方式。13"
  },
  {
    "objectID": "09-Hypothesis-testing.html#some-issues-to-consider",
    "href": "09-Hypothesis-testing.html#some-issues-to-consider",
    "title": "9  Hypothesis testing",
    "section": "9.9 Some issues to consider",
    "text": "9.9 Some issues to consider\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n9.9.1 Neyman versus Fisher\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually13 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n9.9.2 Bayesians versus frequentists\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 7) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in Chapter 16, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n9.9.3 Traps\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,14 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#summary",
    "href": "09-Hypothesis-testing.html#summary",
    "title": "9  Hypothesis testing",
    "section": "9.10 Summary",
    "text": "9.10 Summary\nNull hypothesis testing is one of the most ubiquitous elements to statistical theory. The vast majority of scientific papers report the results of some hypothesis test or another. As a consequence it is almost impossible to get by in science without having at least a cursory understanding of what a p-value means, making this one of the most important chapters in the book. As usual, I’ll end the chapter with a quick recap of the key ideas that we’ve talked about:\n\nA menagerie of hypotheses. Research hypotheses and statistical hypotheses. Null and alternative hypotheses.\nTwo types of errors. Type I and Type II.\nTest statistics and sampling distributions.\nHypothesis testing for Making decisions\nThe p value of a test. p-values as “soft” decisions\nReporting the results of a hypothesis test\nRunning the hypothesis test in practice\nEffect size, sample size and power\nSome issues to consider regarding hypothesis testing\n\nLater in the book, in Chapter 16, I’ll revisit the theory of null hypothesis tests from a Bayesian perspective and introduce a number of new tools that you can use if you aren’t particularly fond of the orthodox approach. But for now, though, we’re done with the abstract statistical theory, and we can start discussing specific data analysis tools.\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results. Cambridge University Press.\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60, 328–331.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation of classical statistics. Springer."
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯表達功能",
    "href": "06-Pragmatic-matters.html#邏輯表達功能",
    "title": "6  實務課題",
    "section": "6.2 邏輯表達功能",
    "text": "6.2 邏輯表達功能\nA key concept that a lot of data transformations in jamovi rely on is the idea of a logical value. A logical value is an assertion about whether something is true or false. This is implemented in jamovi in a pretty straightforward way. There are two logical values, namely TRUE and FALSE. Despite the simplicity, logical values are very useful things. Let’s see how they work.\n\n6.2.1 判斷算式真假值\nIn George Orwell’s classic book 1984 one of the slogans used by the totalitarian Party was “two plus two equals five”. The idea being that the political domination of human freedom becomes complete when it is possible to subvert even the most basic of truths. It’s a terrifying thought, especially when the protagonist Winston Smith finally breaks down under torture and agrees to the proposition. “Man is infinitely malleable”, the book says. I’m pretty sure that this isn’t true of humans2 and it’s definitely not true of jamovi. jamovi is not infinitely malleable, it has rather firm opinions on the topic of what is and isn’t true, at least as regards basic mathematics. If I ask it to calculate \\(2 + 2\\)3, it always gives the same answer, and it’s not bloody 5!\nOf course, so far jamovi is just doing the calculations. I haven’t asked it to explicitly assert that \\(2 + 2 = 4\\) is a true statement. If I want jamovi to make an explicit judgement, I can use a command like this: \\(2 + 2 == 4\\)\nWhat I’ve done here is use the equality operator, \\(==\\), to force jamovi to make a “true or false” judgement.4 Okay, let’s see what jamovi thinks of the Party slogan, so type this into the compute new variable ‘formula’ box:\n\\[2 + 2 == 5\\]\nAnd what do you get? It should be a whole set of ‘false’ values in the spreadsheet column for your newly computed variable. Booyah! Freedom and ponies for all! Or something like that. Anyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\nAnyway, it was worth having a look at what happens if I try to force jamovi to believe that two plus two is five by making a statement like \\(2 + 2 = 5\\). I know that if I do this in another program, say R, then it throws up an error message. But wait, if you do this in jamovi you get a whole set of ‘false’ values. So what is going on? Well, it seems that jamovi is being pretty smart and realises that you are testing whether it is TRUE or FALSE that \\(2 + 2 = 5\\), regardless of whether you use the correct equality operator, \\(==\\), or the equals sign “\\(=\\)”.\n\n\n6.2.2 邏輯運算子\nSo now we’ve seen logical operations at work. But so far we’ve only seen the simplest possible example. You probably won’t be surprised to discover that we can combine logical operations with other operations and functions in a more complicated way, like this: \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) or this \\(SQRT(25) == 5\\)\nNot only that, but as Table 6.2 illustrates, there are several other logical operators that you can use corresponding to some basic mathematical concepts. Hopefully these are all pretty self-explanatory. For example, the less than operator < checks to see if the number on the left is less than the number on the right. If it’s less, then jamovi returns an answer of TRUE, but if the two numbers are equal, or if the one on the right is larger, then jamovi returns an answer of FALSE.\nIn contrast, the less than or equal to operator \\(<=\\) will do exactly what it says. It returns a value of TRUE if the number of the left hand side is less than or equal to the number on the right hand side. At this point I hope it’s pretty obvious what the greater than operator \\(<\\) and the greater than or equal to operator \\(<=\\) do!\nNext on the list of logical operators is the not equal to operator != which, as with all the others, does what it says it does. It returns a value of TRUE when things on either side are not identical to each other. Therefore, since \\(2 + 2\\) isn’t equal to \\(5\\), we would get ‘true’ as the value for our newly computed variable. Try it and see:\n\\[2 + 2 \\text{ != } 5\\]\nWe’re not quite done yet. There are three more logical operations that are worth knowing about, listed in Table 6.3. These are the not operator !, the and operator and, and the or operator or. Like the other logical operators, their behaviour is more or less exactly what you’d expect given their names. For instance, if I ask you to assess the claim that “either \\(2 + 2 = 4\\) or \\(2 + 2 = 5\\)” you’d say that it’s true. Since it’s an “either-or” statement, all we need is for one of the two parts to be true. That’s what the or operator does:5\n\n\n\n\nTable 6.2:  Some logical operators \n\noperationoperatorexample inputanswer\n\nless than<2  <  3TRUE\n\nless than or equal to<2 < = 2TRUE\n\ngreater than>2 > 3FALSE\n\ngreater than or equal to> =2 > = 2TRUE\n\nequal to= =2 = = 3FALSE\n\nnot equal to!=2 != 3TRUE\n\n\n\n\n\n\n\n\n\nTable 6.3:  Some more logical operators \n\noperationoperatorexample inputanswer\n\nnotNOTNOT(1==1)FALSE\n\noror(1==1) or (2==3)TRUE\n\nandand(1==1) and (2==3)FALSE\n\n\n\n\n\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\nOn the other hand, if I ask you to assess the claim that “both \\(2 + 2 = 4\\) and \\(2 + 2 = 5\\)” you’d say that it’s false. Since this is an and statement we need both parts to be true. And that’s what the and operator does:\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\nFinally, there’s the not operator, which is simple but annoying to describe in English. If I ask you to assess my claim that “it is not true that \\(2 + 2 = 5\\)” then you would say that my claim is true, because actually my claim is that “\\(2 + 2 = 5\\) is false”. And I’m right. If we write this in jamovi we use this:\n\\[NOT(2+2 == 5)\\]\nIn other words, since \\(2+2 == 5\\) is a FALSE statement, it must be the case that \\(NOT(2+2 == 5)\\) is a TRUE one. Essentially, what we’ve really done is claim that “not false” is the same thing as “true”. Obviously, this isn’t really quite right in real life. But jamovi lives in a much more black or white world. For jamovi everything is either true or false. No shades of grey are allowed.\nOf course, in our \\(2 + 2 = 5\\) example, we didn’t really need to use the “not” operator \\(NOT\\) and the “equals to” operator \\(==\\) as two separate operators. We could have just used the “not equals to” operator \\(!=\\) like this:\n\\[2+2 \\text{ != } 5\\]\n\n\n6.2.3 在報告中表達邏輯運算子\nI also want to briefly point out that you can apply these logical operators to text as well as to logical data. It’s just that we need to be a bit more careful in understanding how jamovi interprets the different operations. In this section I’ll talk about how the equal to operator \\(==\\) applies to text, since this is the most important one. Obviously, the not equal to operator != gives the exact opposite answers to \\(==\\) so I’m implicitly talking about that one too, but I won’t give specific commands showing the use of \\(!=\\).\nOkay, let’s see how it works. In one sense, it’s very simple. For instance, I can ask jamovi if the word “cat” is the same as the word “dog”, like this:\n“cat” \\(==\\) “dog” That’s pretty obvious, and it’s good to know that even jamovi can figure that out. Similarly, jamovi does recognise that a “cat” is a “cat”: “cat” \\(==\\) “cat” Again, that’s exactly what we’d expect. However, what you need to keep in mind is that jamovi is not at all tolerant when it comes to grammar and spacing. If two strings differ in any way whatsoever, jamovi will say that they’re not equal to each other, as with the following: ” cat” \\(==\\) “cat” “cat” \\(==\\) “CAT” “cat” \\(==\\) “c a t”\nYou can also use other logical operators too. For instance jamovi also allows you to use the > and > operators to determine which of two text ‘strings’ comes first, alphabetically speaking. Sort of. Actually, it’s a bit more complicated than that, but let’s start with a simple example:\n“cat” \\(<\\) “dog”\nIn jamovi, this example evaluates to ‘true’. This is because “cat” does does come before “dog” alphabetically, so jamovi judges the statement to be true. However, if we ask jamovi to tell us if “cat” comes before “anteater” then it will evaluate the expression as false. So far, so good. But text data is a bit more complicated than the dictionary suggests. What about “cat” and “CAT”? Which of these comes first? Try it and find out:\n“CAT” \\(<\\) “cat”\nThis in fact evaluates to ‘true’. In other words, jamovi assumes that uppercase letters come before lowercase ones. Fair enough. No-one is likely to be surprised by that. What you might find surprising is that jamovi assumes that all uppercase letters come before all lowercase ones. That is, while “anteater” \\(<\\) “zebra” is a true statement, and the uppercase equivalent “ANTEATER” \\(<\\) “ZEBRA” is also true, it is not true to say that “anteater” \\(<\\) “ZEBRA”, as the following extract illustrates. Try this:\n“anteater” \\(<\\) “ZEBRA”\nThis evaluates to ‘false’, and this may seem slightly counter-intuitive. With that in mind, it may help to have a quick look at Table 6.4 which lists various text characters in the order that jamovi processes them.\n\n\n\n\nTable 6.4:  Text characters in the order that jamovi processes them \n\n\\( \\text{!} \\)\\( \\text{\"} \\)\\( \\# \\)\\( \\text{\\$} \\)\\( \\% \\)\\( \\& \\)\\( \\text{'} \\)\\( \\text{(} \\)\n\n\\( \\text{)} \\)\\( \\text{*} \\)\\( \\text{+} \\)\\( \\text{,} \\)\\( \\text{-} \\)\\( \\text{.} \\)\\( \\text{/} \\)0\n\n12345678\n\n9\\( \\text{:} \\)\\( \\text{;} \\)<\\( \\text{=} \\)>\\( \\text{?} \\)\\( \\text{@} \\)\n\nABCDEFGH\n\nIJKLMNOP\n\nQRSTUVWX\n\nYZ\\( \\text{[} \\)\\( \\backslash \\)\\( \\text{]} \\)\\( \\hat{} \\)\\( \\_ \\)\\( \\text{`} \\)\n\nabcdeghi\n\njklmnopq\n\nrstuvwxy\n\nz\\(\\text{\\{}\\)\\(\\text{|}\\)\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#其他常用數學函式及運算子",
    "href": "06-Pragmatic-matters.html#其他常用數學函式及運算子",
    "title": "6  實務課題",
    "section": "6.4 其他常用數學函式及運算子",
    "text": "6.4 其他常用數學函式及運算子\nIn the section on [Transforming and recoding a variable] I discussed the ideas behind variable transformations and showed that a lot of the transformations that you might want to apply to your data are based on fairly simple mathematical functions and operations. In this section I want to return to that discussion and mention several other mathematical functions and arithmetic operations that are actually quite useful for a lot of real world data analysis. Table 6.5 gives a brief overview of the various mathematical functions I want to talk about here, or later.9 Obviously this doesn’t even come close to cataloguing the range of possibilities available, but it does cover a range of functions that are used regularly in data analysis and that are available in jamovi.\n\n6.4.1 對數與指數\nAs I’ve mentioned earlier, jamovi has an useful range of mathematical functions built into it and there really wouldn’t be much point in trying to describe or even list all of them. For the most part, I’ve focused only on those functions that are strictly necessary for this book. However I do want to make an exception for logarithms and exponentials. Although they aren’t needed anywhere else in this book, they are everywhere in statistics more broadly. And not only that, there are a lot of situations in which it is convenient to analyse the logarithm of a variable (i.e., to take a “log-transform” of the variable). I suspect that many (maybe most) readers of this book will have encountered logarithms and exponentials before, but from past experience I know that there’s a substantial proportion of students who take a social science statistics class who haven’t touched logarithms since high school, and would appreciate a bit of a refresher.\nIn order to understand logarithms and exponentials, the easiest thing to do is to actually calculate them and see how they relate to other simple calculations. There are three jamovi functions in particular that I want to talk about, namely LN(), LOG10() and EXP(). To start with, let’s consider LOG10(), which is known as the “logarithm in base 10”. The trick to understanding a logarithm is to understand that it’s basically the “opposite” of taking a power. Specifically, the logarithm in base 10 is closely related to the powers of 10. So let’s start by noting that 10-cubed is 1000. Mathematically, we would write this:\n\\[10^3=1000\\]\nThe trick to understanding a logarithm is to recognise that the statement that “10 to the power of 3 is equal to 1000” is equivalent to the statement that “the logarithm (in base 10) of 1000 is equal to 3”. Mathematically, we write this as follows,\n\\[log_{10}(1000)=3\\]\nOkay, since the LOG10() function is related to the powers of 10, you might expect that there are other logarithms (in bases other than 10) that are related to other powers too. And of course that’s true: there’s not really anything mathematically special about the number 10. You and I happen to find it useful because decimal numbers are built around the number 10, but the big bad world of mathematics scoffs at our decimal numbers. Sadly, the universe doesn’t actually care how we write down numbers. Anyway, the consequence of this cosmic indifference is that there’s nothing particularly special about calculating logarithms in base 10. You could, for instance, calculate your logarithms in base 2. Alternatively, a third type of logarithm, and one we see a lot more of in statistics than either base 10 or base 2, is called the natural logarithm, and corresponds to the logarithm in base e. Since you might one day run into it, I’d better explain what e is. The number e, known as Euler’s number, is one of those annoying “irrational” numbers whose decimal expansion is infinitely long, and is considered one of the most important numbers in mathematics. The first few digits of e are:\n\\[e = 2.718282 \\]\nThere are quite a few situation in statistics that require us to calculate powers of \\(e\\), though none of them appear in this book. Raising e to the power \\(x\\) is called the exponential of \\(x\\), and so it’s very common to see \\(e^x\\) written as exppxq. And so it’s no surprise that jamovi has a function that calculates exponentials, called EXP(). Because the number e crops up so often in statistics, the natural logarithm (i.e., logarithm in base e) also tends to turn up. Mathematicians often write it as \\(log_e(x)\\) or \\(ln(x)\\). In fact, jamovi works the same way: the LN() function corresponds to the natural logarithm.\nAnd with that, I think we’ve had quite enough exponentials and logarithms for this book!"
  },
  {
    "objectID": "06-Pragmatic-matters.html#篩選部分資料",
    "href": "06-Pragmatic-matters.html#篩選部分資料",
    "title": "6  實務課題",
    "section": "6.5 篩選部分資料",
    "text": "6.5 篩選部分資料\n\n一種非常重要的數據處理是能够提取特定子集的数据。例如,您可能僅對分析來自一個實驗條件的数据感興趣,或者您可能想仔細查看 50 歲以上人群的数据。為此,第一步是讓 jamovi 過濾與您感興趣的觀測值對應的數據子集。\n本節返回到 nightgarden.csv 資料集。如果您一次坐着閱讀整章內容,則該資料集應已加載到 jamovi 窗口中。 對於本節,讓我們集中討論 speaker 和 utterance 兩個變量(如果您忘記了它們的外觀,請參見[建立次數分佈表])。 假設我想要做的是只提取 Makka-Pakka 發出的語句。為此,我們需要在 jamovi 中指定過濾器。首先通過點擊主 jamovi “資料”工具欄上的“過濾器”打開過濾器窗口。 然后,在“過濾器 1”文本框中,在“=”號旁邊,鍵入以下內容:\nspeaker == ‘makka-pakka’\n\n\n\n\n\n圖 6.10: 使用’篩選器`保留nightgarden資料集內，要做資料分析的部分\n\n\n\n\n\n\n完成後,您會看到電子表格窗口中添加了一個新列(見 圖 6.10),標籤為“過濾器 1”,其中 speaker 不是 ‘makka-pakka’ 的情况被灰化(即過濾掉),相反地,speaker 是 ‘makka-pakka’ 的情况下有一個綠色的勾號,表示它們被過濾。您可以通過運行“勘探” - “描述統計” - “次數分佈表”的 speaker 變量并查看顯示的內容來測試這一點。 前往試試!\n在此簡單示例的基礎上,您還可以使用 jamovi 中的邏輯運算式構建更複雜的過濾器。例如,假設我只想保留語句為“pip”或“oo”的情况。在這種情况下,在“過濾器 1”文本框中,在“=”號旁邊,您將鍵入以下內容:\nutterance == ‘pip’ or utterance == ‘oo’"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html",
    "title": "8  從樣本估計未知量數",
    "section": "",
    "text": "At the start of the last chapter I highlighted the critical distinction between descriptive statistics and inferential statistics. As discussed in Chapter 4, the role of descriptive statistics is to concisely summarise what we do know. In contrast, the purpose of inferential statistics is to “learn what we do not know from what we do”. Now that we have a foundation in probability theory we are in a good position to think about the problem of statistical inference. What kinds of things would we like to learn about? And how do we learn them? These are the questions that lie at the heart of inferential statistics, and they are traditionally divided into two “big ideas”: estimation and hypothesis testing. The goal in this chapter is to introduce the first of these big ideas, estimation theory, but I’m going to witter on about sampling theory first because estimation theory doesn’t make sense until you understand sampling. As a consequence, this chapter divides naturally into two parts, the first three sections are focused on sampling theory, and the last two sections make use of sampling theory to discuss how statisticians think about estimation."
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#母群參數的點估計",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#母群參數的點估計",
    "title": "8  運用樣本估計未知量數",
    "section": "8.4 母群參數的點估計",
    "text": "8.4 母群參數的點估計\nIn all the IQ examples in the previous sections we actually knew the population parameters ahead of time. As every undergraduate gets taught in their very first lecture on the measurement of intelligence, IQ scores are defined to have mean 100 and standard deviation 15. However, this is a bit of a lie. How do we know that IQ scores have a true population mean of 100? Well, we know this because the people who designed the tests have administered them to very large samples, and have then “rigged” the scoring rules so that their sample has mean 100. That’s not a bad thing of course, it’s an important part of designing a psychological measurement. However, it’s important to keep in mind that this theoretical mean of 100 only attaches to the population that the test designers used to design the tests. Good test designers will actually go to some lengths to provide “test norms” that can apply to lots of different populations (e.g., different age groups, nationalities etc).\nThis is very handy, but of course almost every research project of interest involves looking at a different population of people to those used in the test norms. For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning in Port Pirie, a South Australian industrial town with a lead smelter. Perhaps you decide that you want to compare IQ scores among people in Port Pirie to a comparable sample in Whyalla, a South Australian industrial town with a steel refinery.6 Regardless of which town you’re thinking about, it doesn’t make a lot of sense simply to assume that the true population mean IQ is 100. No-one has, to my knowledge, produced sensible norming data that can automatically be applied to South Australian industrial towns. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?\n\n8.4.1 母群平均值\nSuppose we go to Port Pirie and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be \\(\\bar{X}=98.5\\). So what is the true mean IQ for the entire population of Port Pirie? Obviously, we don’t know the answer to that question. It could be 97.2, but it could also be 103.5. Our sampling isn’t exhaustive so we cannot give a definitive answer. Nevertheless, if I was forced at gunpoint to give a “best guess” I’d have to say 98.5. That’s the essence of statistical estimation: giving a best guess.\nIn this example estimating the unknown poulation parameter is straightforward. I calculate the sample mean and I use that as my estimate of the population mean. It’s pretty simple, and in the next section I’ll explain the statistical justification for this intuitive answer. However, for the moment what I want to do is make sure you recognise that the sample statistic and the estimate of the population parameter are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if the true population mean is denoted \\(\\mu\\), then we would use \\(\\hat{mu}\\) to refer to our estimate of the population mean. In contrast, the sample mean is denoted \\(\\bar{X}\\) or sometimes m. However, in simple random samples the estimate of the population mean is identical to the sample mean. If I observe a sample mean of \\(\\bar{X}=98.5\\) then my estimate of the population mean is also \\(\\hat{\\mu}=98.5\\). To help keep the notation clear, here’s a handy table (Table 8.2):\n\n\n\n\nTable 8.2:  Notation for the mean \n\nSymbolWhat is it?Do we know what it is?\n\n\\( \\hat{X} \\)Sample meanYes, calculated from the raw data\n\n\\( \\mu \\)True population meanAlmost never known for sure\n\n\\( \\hat{\\mu} \\)Estimate of the population meanYes, identical to the sample mean in simple random samples\n\n\n\n\n\n\n\n8.4.2 母群標準差\nSo far, estimation seems pretty simple, and you might be wondering why I forced you to read through all that stuff about sampling theory. In the case of the mean our estimate of the population parameter (i.e. \\(\\hat{\\mu}\\)) turned out to identical to the corresponding sample statistic (i.e. \\(\\bar{X}\\)). However, that’s not always true. To see this, let’s have a think about how to construct an estimate of the population standard deviation, which we’ll denote \\(\\hat{\\sigma}\\). What shall we use as our estimate in this case? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.\nHere’s why. Suppose I have a sample that contains a single observation. For this example, it helps to consider a sample where you have no intuitions at all about what the true population values might be, so let’s use something completely fictitious. Suppose the observation in question measures the cromulence of my shoes. It turns out that my shoes have a cromulence of \\(20\\). So here’s my sample:\nThis is a perfectly legitimate sample, even if it does have a sample size of \\(N = 1\\). It has a sample mean of \\(20\\) and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0. As a description of the sample this seems quite right, the sample contains a single observation and therefore there is no variation observed within the sample. A sample standard deviation of \\(s = 0\\) is the right answer here. But as an estimate of the population standard deviation it feels completely insane, right? Admittedly, you and I don’t know anything at all about what “cromulence” is, but we know something about data. The only reason that we don’t see any variability in the sample is that the sample is too small to display any variation! So, if you have a sample size of \\(N = 1\\) it feels like the right answer is just to say “no idea at all”.\nNotice that you don’t have the same intuition when it comes to the sample mean and the population mean. If forced to make a best guess about the population mean it doesn’t feel completely insane to guess that the population mean is \\(20\\). Sure, you probably wouldn’t feel very confident in that guess because you have only the one observation to work with, but it’s still the best guess you can make.\nLet’s extend this example a little. Suppose I now make a second observation. My data set now has \\(N = 2\\) observations of the cromulence of shoes, and the complete sample now looks like this:\n\\[20, 22\\]\nThis time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed! For our new data set, the sample mean is \\(\\bar{X} = 21\\), and the sample standard deviation is \\(s = 1\\). What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean. If forced to guess we’d probably guess that the population mean cromulence is \\(21\\). What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, and if you’re at all like me you probably have the intuition that, with only two observations we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong, after all with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.\nThis intuition feels right, but it would be nice to demonstrate this somehow. There are in fact mathematical proofs that confirm this intuition, but unless you have the right mathematical background they don’t help very much. Instead, what I’ll do is simulate the results of some experiments. With that in mind, let’s return to our IQ studies. Suppose the true population mean IQ is \\(100\\) and the standard deviation is \\(15\\). First I’ll conduct an experiment in which I measure \\(N = 2\\) IQ scores and I’ll calculate the sample standard deviation. If I do this over and over again, and plot a histogram of these sample standard deviations, what I have is the sampling distribution of the standard deviation. I’ve plotted this distribution in Figure 8.10. Even though the true population standard deviation is 15 the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found in Figure 8.8 (b) when we plotted the sampling distribution of the mean, where the population mean is \\(100\\) and the average of the sample means is also \\(100\\).\n\n\n\n\n\nFigure 8.10: The sampling distribution of the sample standard deviation for a ‘two IQ scores’ experiment. The true population standard deviation is 15 (dashed line), but as you can see from the histogram the vast majority of experiments will produce a much smaller sample standard deviation than this. On average, this experiment would produce a sample standard deviation of only 8.5, well below the true value! In other words, the sample standard deviation is a biased estimate of the population standard deviation\n\n\n\n\nNow let’s extend the simulation. Instead of restricting ourselves to the situation where \\(N=2\\), let’s repeat the exercise for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in Figure 8.11. On the left hand side (panel (a)) I’ve plotted the average sample mean and on the right hand side (panel (b)) I’ve plotted the average standard deviation. The two plots are quite different:on average, the average sample mean is equal to the population mean. It is an unbiased estimator, which is essentially the reason why your best estimate for the population mean is the sample mean.7 The plot on the right is quite different: on average, the sample standard deviation \\(s\\) is smaller than the population standard deviation \\(\\sigma\\). It is a biased estimator. In other words, if we want to make a “best guess” \\(\\hat{\\sigma}\\) about the value of the population standard deviation \\(\\hat{\\sigma}\\) we should make sure our guess is a little bit larger than the sample standard deviation \\(s\\).\n\n\n\n\n\nFigure 8.11: An illustration of the fact that the sample mean is an unbiased estimator of the population mean (panel a), but the sample standard deviation is a biased estimator of the population standard deviation (panel b). For the figure I generated \\(10,000\\) simulated data sets with 1 observation each, \\(10,000\\) more with 2 observations, and so on up to a sample size of 10. Each data set consisted of fake IQ data, that is the data were normally distributed with a true population mean of 100 and standard deviation 15. On average, the sample means turn out to be 100, regardless of sample size (panel a). However, the sample standard deviations turn out to be systematically too small (panel b), especially for small sample sizes\n\n\n\n\nThe fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation let’s look at the variance. If you recall from the section on [Estimating population parameters], the sample variance is defined to be the average of the squared deviations from the sample mean. That is: \\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\] The sample variance \\(s^2\\) is a biased estimator of the population variance \\(\\sigma^2\\). But as it turns out, we only need to make a tiny tweak to transform this into an unbiased estimator. All we have to do is divide by \\(N-1\\) rather than by \\(N\\).\nThis is an unbiased estimator of the population variance \\(\\sigma\\). Moreover, this finally answers the question we raised in [Estimating population parameters]. Why did jamovi give us slightly different answers for variance? It’s because jamovi calculates \\(\\hat{\\sigma}^2 \\text{ not } s^2\\), that’s why. A similar story applies for the standard deviation. If we divide by \\(N - 1\\) rather than \\(N\\) our estimate of the population standard deviation is unbiased, and when we use jamovi’s built in standard deviation function, what it’s doing is calculating \\(\\hat{\\sigma}\\) not \\(s\\).8\nOne final point. In practice, a lot of people tend to refer to \\(\\hat{\\sigma}\\) (i.e., the formula where we divide by \\(N - 1\\)) as the sample standard deviation. Technically, this is incorrect. The sample standard deviation should be equal to s (i.e., the formula where we divide by N). These aren’t the same thing, either conceptually or numerically. One is a property of the sample, the other is an estimated characteristic of the population. However, in almost every real life application what we actually care about is the estimate of the population parameter, and so people always report \\(\\hat{\\sigma}\\) rather than s. This is the right number to report, of course. It’s just that people tend to get a little bit imprecise about terminology when they write it up, because “sample standard deviation” is shorter than “estimated population standard deviation”. It’s no big deal, and in practice I do the same thing everyone else does. Nevertheless, I think it’s important to keep the two concepts separate. It’s never a good idea to confuse “known properties of your sample” with “guesses about the population from which it came”. The moment you start thinking that \\(s\\) and \\(\\hat{\\sigma}\\) are the same thing, you start doing exactly that.\nTo finish this section off, here’s another couple of tables to help keep things clear (Table 8.3 and Table 8.4).\n\n\n\n\nTable 8.3:  Notation for standard deviation \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s \\)Sample standard deviationYes, calculated from the raw data\n\n\\( \\sigma  \\)Population standard deviationAlmost never known for sure\n\n\\( \\hat{\\sigma } \\)Estimate of the population  standard deviationYes, but not the same as the  sample standard deviation\n\n\n\n\n\n\n\n\n\nTable 8.4:  Notation for variance \n\nSymbolWhat is it?Do we know what it is?\n\n\\( s^2 \\)Sample varianceYes, calculated from the raw data\n\n\\( \\sigma^2  \\)Population varianceAlmost never known for sure\n\n\\( \\hat{\\sigma }^2 \\)Estimate of the population  varianceYes, but not the same as the  sample variance"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#本章小結",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#本章小結",
    "title": "8  運用樣本估計未知量數",
    "section": "8.6 本章小結",
    "text": "8.6 本章小結\n這一章涵蓋兩大主題。前半部談取樣理論，後半部談運用取樣理論估計母群參數的方法。兩大主題分為幾個小主題：\n\n有關樣本、母群、取樣的基本概念11\n取樣的機率理論：大數法則、樣本分佈與中央極限定理12\n母群參數的點估計討論平均值及標準差的機率意義。13\n母群參數的區間估計14\n\n還有許多取樣與估計的主題尚未談到，本章對於心理學及統計的初學者應該是能夠理解且能吸收的。而且應用取向的心理學者不大需要深入了解統計理論。唯一你可能需要了解但本章未探討的問題，是分析的資料不是來自隨機樣本的狀況。其實已經有許多處理非隨機樣本的統計理論，不過已經超出本書範圍了。\n\n\n\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan; Company.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard University Press."
  },
  {
    "objectID": "09-Hypothesis-testing.html",
    "href": "09-Hypothesis-testing.html",
    "title": "9  假設檢定",
    "section": "",
    "text": "The process of induction is the process of assuming the simplest law that can be made to harmonize with our experience. This process, however, has no logical foundation but only a psychological one. It is clear that there are no grounds for believing that the simplest course of events will really happen. It is an hypothesis that the sun will rise tomorrow: and this means that we do not know whether it will rise.\n– Ludwig Wittgenstein 1\nIn the last chapter I discussed the ideas behind estimation, which is one of the two “big ideas” in inferential statistics. It’s now time to turn our attention to the other big idea, which is hypothesis testing. In its most abstract form, hypothesis testing is really a very simple idea. The researcher has some theory about the world and wants to determine whether or not the data actually support that theory. However, the details are messy and most people find the theory of hypothesis testing to be the most frustrating part of statistics. The structure of the chapter is as follows. First, I’ll describe how hypothesis testing works in a fair amount of detail, using a simple running example to show you how a hypothesis test is “built”. I’ll try to avoid being too dogmatic while doing so, and focus instead on the underlying logic of the testing procedure.2 Afterwards, I’ll spend a bit of time talking about the various dogmas, rules and heresies that surround the theory of hypothesis testing."
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設大觀園",
    "href": "09-Hypothesis-testing.html#假設大觀園",
    "title": "9  假設檢定",
    "section": "9.1 假設大觀園",
    "text": "9.1 假設大觀園\n我們從一個原作者虛構的狂想開始談吧：我認為最終，人類都會屈服於瘋狂。我真正想做的研究，將在我升任正教授的時候開始。那一天到來時，我在象牙塔中會受到終身職的保障，我總算能夠拋棄理智，完全投入那個最沒有成效的心理研究領域：證實人類有超感官知覺（ESP）。3\n假如這一天終於來臨了，我的第一個研究是一項簡單的測試透視力實驗。每個參與者坐在桌子前，由實驗者向他展示一張卡片。這張卡片的一面是黑色的，另一面是白色的。實驗者把卡片拿走後，將它放在隔壁房間的桌子上。在實驗者與參與者離開房間後，卡片將被第二位實驗者隨機地翻到黑面或白面朝上，然後第二位實驗者進入房間，詢問參與者現在卡片的哪一面朝上。這只是一次性的實驗。每個人只會看到一張卡片，只回答一個問題，而參與者在任何時候都沒有與知道正確答案的人接觸。因此，我的資料紀錄非常簡單：我問了N個人的問題，其中有\\(X\\)個人給出了正確的答案。為了具體描述，假如我測試了100個人，其中有62個人回答正確。這會是一個令人驚訝的大數字，但是這個數字是否足夠讓我聲稱發現了ESP的證據呢？這就是檢驗假設有無效用的情況。然而，在我們談論如何檢驗假設之前，我們需要明白假設是什麼。\n\n\n\n9.1.1 研究假設還是統計假設\n首先需要清楚區別的是研究假設和統計假設之間的差別。在我設想的ESP研究中，我的整體科學目標是證實人類有透視能力。在這樣的場景，我有一個清晰的研究目標：我希望發現ESP的證據。在其他場景，我的想法可能會比較中立，也就是我可能會說我的研究目標是確定人類是否有透視能力。無論如何描述我的目標，我想傳達給各位同學的基本觀點是，制定研究假設是提出一個實際的、可測試的科學主張。如果你是一名心理學家，那麼你的研究假設基本上是關於心理學構念的。以下任何一種案例都可說是研究假設：\n\n聆聽音樂會降低你對其他事物的注意力能力。這是一種關於兩個有心理學意義的構念之間有因果關係的主張（聆聽音樂和注意力），因此這是一個非常合理的研究假設。\n智力與個性有關。和前一個一樣，這個假設主張兩個有心理學意義的構念之間存在關係性（智力和個性），但這個主張立埸比較弱：探討相關性而不是因果關係。\n智力是訊息處理速度。這個假設與前面兩個很不一樣。實際上這並不是一個因果關係或關聯性的假設，而是關於智力基本特性的本體論主張（我相當確定是這樣的）。通常來說，設計實驗測試像是“\\(X\\)是否影響\\(Y\\)？”，比回答“\\(X\\)是什麼？”這樣的問題要容易得多。在實際情況通常是你會找到方法，測試基本特性所形成的關聯性假設。例如，如果我相信智力的本質是大腦中訊息處理速度，我就會設計實驗探討智力和訊息處理之間的關係。因此，大多數日常生活中想到的研究問題雖然都與本質有關，但是通常是基於好奇關於自然界本體論問題的更深層動機。\n\n請注意在真實的實驗室，我會設定幾個互相重疊的研究假設。儘管我設計ESP實驗的終極目標是測試“人類有ESP”這樣的本體論主張，但是實際操作會限制自己只測試目標更狹窄的假設，像是“某些人可以用透視‘看見’物體”。話雖如此，有一些看似目標明確的主張，在任何意義上都不算是合適的研究假設：\n\n愛情就像戰場。這個假設過於模糊，無法進行測試。雖然研究假設可以有一定程度的模糊性，但是必須能夠將理論觀念具體化。也許我不夠有創造力，想不到能將這個假設轉化為具體研究設計的方式。如果真的有辦法，那麼應該不是科學的研究假設，而是一首流行歌曲。這並不是說這樣的假設不有趣，而是要指出許多人能想到的深刻問題都是屬於這種類別。也許有一天科學家能夠構建關於愛情的可測試理論，或者測試上帝是否存在等等。但是現在的我們還做不到，我不會指望看到一個令人滿意的科學方法來解決這些問題。\n套套邏輯俱樂部的第一條規則就是套套邏輯俱樂部的第一條規則。這是不具備任何實質意義的主張，儘管形式上是符合邏輯的。因為在任何自然狀態都不能提出與此主張相反的看法，我們會說這是一個不可證偽的假設，因此這樣的主張不屬於科學研究的領域。在科學研究，無論你想研究的問題是什麼，你提出的主張都必須有可能是錯誤的。\n在我的實驗裡，較多的參與者會說‘是’，而不是‘否’。這並不是一個有意義的研究假設，因為重點的是資料本身而非心理學問題（當然，除非真正要研究的問題是關於多數人是不是有回答“是”的偏好！）。實際上，這個假設看起來更像是一個統計假設而非研究假設。\n\n正如同學所見，有的研究假設的主張可能會有些混亂，不過都是各樣科學主張的一種。統計假設則不是一種主張。統計假設必須具有數學精確性，並且必須對資料的生成機制（也就是“母群”）的特徵提出具體的條件。即便如此，統計假設的內在意圖必須有與真正的研究假設有一個明確的關係！例如，在我的ESP研究中，我的研究假設是有些人能夠透視牆壁看到隔壁的物體。我要做的是將這樣的研究假設對應到產生資料的方法陳述。因此，現在我們考慮一下要如何表達這樣的陳述。我感興趣的實驗數值是 \\(P(correct)\\)，也就是實驗參與者正確回答問題的理論上為真但未知之機率。讓我們使用希臘字母\\(\\theta\\)（theta）來表示這項機率。以下是四種不同的統計假設：\n\n如果ESP不存在，而我的實驗設計沒有偏誤，那麼參與者的回答只是猜測。因此，我應該期望有一半參與者的回答是正確的，所以我的統計假設是，回答正確的理論機率是 \\(\\theta=0.5\\)。\n或者，假設ESP存在並且參與者真的能夠看到卡片。如果實驗結果真的是這樣，參與者回答的正確率會高於只是猜測，所以統計假設是 \\(\\theta > 0.5\\)。\n第三種可能是ESP確實存在，但是參與者並沒有意識到透視看到的物體顏色是相反的（好吧，這有些荒謬，但我們永遠無法知道）。如果是這樣的實驗結果，我會期望參與者回答的正確率會低於只是猜測。所以統計假設是 \\(\\theta < 0.5\\)。\n最後，如果人類確實有ESP，但是我不知道參與者是否看到了正確的顏色。在這種情況下，我只能期望參與者回答的正確率不等於0.5。所以統計假設是 \\(\\theta \\neq 0.5\\)。\n\n以上例子都是合乎科學研究目標的統計假設，因為每條陳述都有定義母群參數，並且緊密扣連我的實驗目的。\n我希望這些例子可以讓同學清楚了解，當研究者要構建一個統計假設檢定程序時，實際上要考慮兩種不同層次的假設。首先，研究者要有一個研究假設（關於心理學的主張），能對應到一個統計假設（關於數據生成母群的主張）。以我的ESP實驗來說，會像 Table 9.1 的表達。\n\n\n\n\n\n\nTable 9.1:  原作者狂想的研究假設與統計假設 \n \n  \n    研究假設 \n    人類有超感官知覺 \n  \n \n\n  \n    統計假設 \n     \n\n  \n\n\n\n\n\n\n小結一下兩種假設的差別。統計假設檢定的測試對象是統計假設，而非研究假設。假如你的研究設計不良，會造成研究假設和統計假設之間的斷裂。舉個有點荒謬的狀況，要是我的ESP研究是在參與者可以從窗戶反光看到卡片的環境裡進行，那麼我肯定能得到非常強的證據證明 \\(\\theta \\neq 0.5\\)，但是這並不能告訴我們”人類真的有ESP”。\n\n\n\n9.1.2 虛無假設與對立假設\n到目前為止還算順利。我有一個研究假設，對應我想相信的世界，還有映射到一個對應於資料生成方式的統計假設。接下來我要創造一個新的統計假設（“虛無假設”，\\(H_0\\)），這對很多人來說有些違反直覺。因為”虛無假設”對應與我想相信的事情完全相反，然後專注於驗證這條統計假設，並且忽略實際關心的事情（現在被稱為”對立假設”，\\(H_1\\)）。在我的ESP研究裡，虛無假設是 \\(\\theta = 0.5\\)，因為如果人類沒有ESP，我會期望看到這個結果。當然，我期望ESP是真的，所以這個虛無假設對立的假設就是 \\(\\theta \\neq 0.5\\)。實際上，我們是在將 \\(\\theta\\) 涵括的可能數值分成兩類：我真心期望不是真的那些數值（虛無假設），以及如果ESP被證實是存在的，我會很高興的那些數值（對立假設）。完成這些設定之後，需要意識到的關鍵是，假設檢定的真正目標不是證實對立假設（可能）是真的，而是證實虛無假設（可能）是假的。很多初學的同學會覺得這樣的邏輯很奇怪。\n就我的學習經驗，最好的比喻是把假設檢定當作刑事法庭審判4。虛無假設就是被告，研究者就像檢察官，統計檢定程序是法官。像真正的刑事審判程序一樣，一開始我們要以無罪推定原則看待虛無假設，也就是說它的主張應被認為是真實的，直到位研究者能夠明確證實它的主張是錯的。研究者可以憑自由意志設計實驗（當然也要合理），目的就是要用可能性最大的資料證實虛無假設是錯的。然而，統計檢定(法官)設定了審判的規則，而這些規則是為了保護虛無假設而設計的，這些規則是要確保如果虛無假設的主張確實是真的，讓法官誤判的機會保持在很低的水準。這非常重要，畢竟虛無假設沒有律師幫忙辨護，而研究者卻在拼命地嘗試證實它的主張是錯的，所以必須有一方提供虛無假設一些保護。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#兩種決策失誤型態",
    "href": "09-Hypothesis-testing.html#兩種決策失誤型態",
    "title": "9  假設檢定",
    "section": "9.2 兩種決策失誤型態",
    "text": "9.2 兩種決策失誤型態\nBefore going into details about how a statistical test is constructed it’s useful to understand the philosophy behind it. I hinted at it when pointing out the similarity between a null hypothesis test and a criminal trial, but I should now be explicit. Ideally, we would like to construct our test so that we never make any errors. Unfortunately, since the world is messy, this is never possible. Sometimes you’re just really unlucky. For instance, suppose you flip a coin 10 times in a row and it comes up heads all 10 times. That feels like very strong evidence for a conclusion that the coin is biased, but of course there’s a 1 in 1024 chance that this would happen even if the coin was totally fair. In other words, in real life we always have to accept that there’s a chance that we made a mistake. As a consequence the goal behind statistical hypothesis testing is not to eliminate errors, but to minimise them.\nAt this point, we need to be a bit more precise about what we mean by “errors”. First, let’s state the obvious. It is either the case that the null hypothesis is true or that it is false, and our test will either retain the null hypothesis or reject it.5 So, as Table 9.2 illustrates, after we run the test and make our choice one of four things might have happened:\n\n\n\n\nTable 9.2:  Null hypothesis statistical testing (NHST) \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is truecorrect decisionerror (type I)\n\n\\( H_0 \\) is falseerror (type II)correct decision\n\n\n\n\n\nAs a consequence there are actually two different types of error here. If we reject a null hypothesis that is actually true then we have made a type I error. On the other hand, if we retain the null hypothesis when it is in fact false then we have made a type II error.\nRemember how I said that statistical testing was kind of like a criminal trial? Well, I meant it. A criminal trial requires that you establish “beyond a reasonable doubt” that the defendant did it. All of the evidential rules are (in theory, at least) designed to ensure that there’s (almost) no chance of wrongfully convicting an innocent defendant. The trial is designed to protect the rights of a defendant, as the English jurist William Blackstone famously said, it is “better that ten guilty persons escape than that one innocent suffer.” In other words, a criminal trial doesn’t treat the two types of error in the same way. Punishing the innocent is deemed to be much worse than letting the guilty go free. A statistical test is pretty much the same. The single most important design principle of the test is to control the probability of a type I error, to keep it below some fixed probability. This probability, which is denoted \\(\\alpha\\), is called the significance level of the test. And I’ll say it again, because it is so central to the whole set-up, a hypothesis test is said to have significance level \\(\\alpha\\) if the type I error rate is no larger than \\(\\alpha\\).\nSo, what about the type II error rate? Well, we’d also like to keep those under control too, and we denote this probability by \\(\\beta\\). However, it’s much more common to refer to the power of the test, that is the probability with which we reject a null hypothesis when it really is false, which is \\(1 - \\beta\\). To help keep this straight, here’s the same table again but with the relevant numbers added (Table 9.3):\n\n\n\n\nTable 9.3:  Null hypothesis statistical testing (NHST) - additional detail \n\nretain \\( H_0 \\)reject  \\( H_0 \\)\n\n\\( H_0 \\) is true1-\\( \\alpha \\) (probability of correct retention)\\(\\alpha\\)  (type I error rate)\n\n\\( H_0 \\) is false\\(\\beta\\) (type II error rate)\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\nA “powerful” hypothesis test is one that has a small value of \\(\\beta\\), while still keeping \\(\\alpha\\) fixed at some (small) desired level. By convention, scientists make use of three different \\(\\alpha\\) levels: \\(.05\\), \\(.01\\) and \\(.001\\). Notice the asymmetry here; the tests are designed to ensure that the \\(\\alpha\\) level is kept small but there’s no corresponding guarantee regarding \\(\\beta\\). We’d certainly like the type II error rate to be small and we try to design tests that keep it small, but this is typically secondary to the overwhelming need to control the type I error rate. As Blackstone might have said if he were a statistician, it is “better to retain 10 false null hypotheses than to reject a single true one”. To be honest, I don’t know that I agree with this philosophy. There are situations where I think it makes sense, and situations where I think it doesn’t, but that’s neither here nor there. It’s how the tests are built."
  },
  {
    "objectID": "09-Hypothesis-testing.html#運用樣本分佈檢測統計值",
    "href": "09-Hypothesis-testing.html#運用樣本分佈檢測統計值",
    "title": "9  假設檢定",
    "section": "9.3 運用樣本分佈檢測統計值",
    "text": "9.3 運用樣本分佈檢測統計值\n現在我們需要開始討論如何建立一個假設檢定的具體步驟。為此，讓我們回到剛才的靈異感知（ESP）的例子。暫且忽略我們實際獲得的資料，只談談實驗的結構。無論數據是什麼，資料的形式都是 \\(N\\) 個人當中有 \\(X\\) 個人正確地辨認出了隱藏卡牌的顏色。此外，假設虛無假設（null hypothesis）確實是真的，也就是說靈異感知不存在，每個人正確辨認出卡牌顏色的真實機率 \\(\\theta\\) 等於 \\(0.5\\)。在這種情況下，我們預期的資料是什麼樣子呢？很明顯，我們期望作出正確反應的人的比例接近 \\(50%\\)。換言之，在更加數學化的語言中，我們可以說 \\(\\frac{X}{N}\\) 大約等於 \\(0.5\\)。當然，我們不會期望這個比例完全等於 \\(0.5\\)。例如，如果我們測試了 \\(N=100\\) 個人，其中有 \\(X=53\\) 人回答正確，我們可能不得不承認這個數據與虛無假設是相當一致的。另一方面，如果有 \\(X=99\\) 個參與者回答正確，我們會非常有信心地認為虛無假設是錯的。同樣地，如果只有 \\(X=3\\) 人回答正確，我們也會非常有信心地認為虛無假設是錯的。現在讓我們更加技術地描述這一點。我們有一個數量 \\(X\\)，可以通過觀察資料計算出來。觀察了 \\(X\\) 值之後，我們就必須決定是相信虛無假設還是拒絕虛無假設，接受另一種假設。我們用來指導我們作出這個決定的量被稱為檢定統計量。\n在選擇了一個統計值後，下一步是明確地說明哪些統計值將導致我們拒絕虛無假設，哪些統計值將導致我們接受它。為了做到這一點，我們需要確定當虛無假設實際成立時，該統計值的樣本分佈是什麼（我們在@sec-Sampling-distribution-of-the-mean中討論過樣本分佈）。為什麼我們需要這個？因為這個分佈告訴我們，如果虛無假設是正確的，我們將期望哪些X的值。因此，我們可以使用這個分佈作為評估虛無假設與我們的資料是否一致的工具。\n如何確定統計值的樣本分佈？對於很多假設檢定來說，這個步驟通常相當複雜，甚至有些假設檢定我自己都不是很懂，稍後在本書中你會看到我對某些測試有些含糊其辭。但有時，這個步驟是非常簡單的。幸運的是，在ESP的例子中，我們可以提供其中最簡單的案例。我們的母群參數 \\(\\theta\\) 就是人們回答問題時的整體機率，而統計值 \\(X\\) 則是在樣本大小 \\(N\\) 中回答正確的人數。我們之前在「二項分佈」一節中已經見過這樣的分佈，那正是二項分佈所描述的內容！因此，為了使用在那個節中介紹的符號和術語，我們會說虛無假設預測 \\(X\\) 的分佈是二項式分佈，寫作\nAt this point we need to start talking specifics about how a hypothesis test is constructed. To that end, let’s return to the ESP example. Let’s ignore the actual data that we obtained, for the moment, and think about the structure of the experiment. Regardless of what the actual numbers are, the form of the data is that \\(X\\) out of \\(N\\) people correctly identified the colour of the hidden card. Moreover, let’s suppose for the moment that the null hypothesis really is true, that ESP doesn’t exist and the true probability that anyone picks the correct colour is exactly \\(\\theta = 0.5\\). What would we expect the data to look like? Well, obviously we’d expect the proportion of people who make the correct response to be pretty close to \\(50\\%\\). Or, to phrase this in more mathematical terms, we’d say that \\(\\frac{X}{N}\\) is approximately \\(0.5\\). Of course, we wouldn’t expect this fraction to be exactly \\(0.5\\). If, for example, we tested \\(N = 100\\) people and \\(X = 53\\) of them got the question right, we’d probably be forced to concede that the data are quite consistent with the null hypothesis. On the other hand, if \\(X = 99\\) of our participants got the question right then we’d feel pretty confident that the null hypothesis is wrong. Similarly, if only \\(X = 3\\) people got the answer right we’d be similarly confident that the null was wrong. Let’s be a little more technical about this. We have a quantity \\(X\\) that we can calculate by looking at our data. After looking at the value of \\(X\\) we make a decision about whether to believe that the null hypothesis is correct, or to reject the null hypothesis in favour of the alternative. The name for this thing that we calculate to guide our choices is a test statistic.\nHaving chosen a test statistic, the next step is to state precisely which values of the test statistic would cause is to reject the null hypothesis, and which values would cause us to keep it. In order to do so we need to determine what the sampling distribution of the test statistic would be if the null hypothesis were actually true (we talked about sampling distributions earlier in Section 8.3.1. Why do we need this? Because this distribution tells us exactly what values of X our null hypothesis would lead us to expect. And, therefore, we can use this distribution as a tool for assessing how closely the null hypothesis agrees with our data.\nHow do we actually determine the sampling distribution of the test statistic? For a lot of hypothesis tests this step is actually quite complicated, and later on in the book you’ll see me being slightly evasive about it for some of the tests (some of them I don’t even understand myself). However, sometimes it’s very easy. And, fortunately for us, our ESP example provides us with one of the easiest cases. Our population parameter \\(\\theta\\) is just the overall probability that people respond correctly when asked the question, and our test statistic \\(X\\) is the count of the number of people who did so out of a sample size of N. We’ve seen a distribution like this before, in Section 7.4, and that’s exactly what the binomial distribution describes! So, to use the notation and terminology that I introduced in that section, we would say that the null hypothesis predicts that \\(X\\) is binomially distributed, which is written\n\\[X \\sim Binomial(\\theta,N)\\]\n因為虛無假設聲明 \\(\\theta = 0.5\\)，而我們的實驗有 \\(N=100\\) 人，所以我們已經擁有所需的樣本分佈。這個樣本分佈在@fig-fig9-1中繪製。沒有什麼驚喜，虛無假設說 \\(X=50\\) 是最有可能的結果，並且它說我們幾乎一定會看到 \\(40\\) 到 \\(60\\) 個正確的回答。\nSince the null hypothesis states that \\(\\theta = 0.5\\) and our experiment has \\(N = 100\\) people, we have the sampling distribution we need. This sampling distribution is plotted in Figure 9.1. No surprises really, the null hypothesis says that \\(X = 50\\) is the most likely outcome, and it says that we’re almost certain to see somewhere between \\(40\\) and \\(60\\) correct responses.\n\n\n\n\n\nFigure 9.1: 這是當虛無假設為真時，我們測試統計值 \\(X\\) 的樣本分佈。對於 ESP 的情境，這是一個二項式分佈。毫不意外的，因為虛無假設說明正確回答的機率是 \\(\\theta = 0.5\\)，所以樣本分佈顯示 50 (在 100 次測試中) 正確回答的比數最有可能發生。大部分的機率分佈在 40 到 60 之間。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#統計推論的決策要素",
    "href": "09-Hypothesis-testing.html#統計推論的決策要素",
    "title": "9  假設檢定",
    "section": "9.4 統計推論的決策要素",
    "text": "9.4 統計推論的決策要素\n我們已經非常接近最後一步了。前一步設定了一個統計檢定值 \\((X)\\)，並且選擇了我們相當有信心的檢定值數值。如果 \\(X\\) 接近 \\(\\frac{N}{2}\\)，我們應該保留虛無假設，否則我們就應該拒絕虛無假設。剩下的問題是什麼呢？確切地說，我們應該設定那些統計檢定值是對應虛無假設，那些統計檢定值對應對立假設？以ESP研究案為例，假如我觀察到一個值 \\(X=62\\)。我應該做出什麼決策呢？我應該相信虛無假設還是對立假設？\n\n\n9.4.1 棄卻域與臨界值\n要回答這個問題，我需要向各位介紹統計檢定值 \\(X\\) 的棄卻域（critical region）。檢定的棄卻域對應那些會讓我們拒絕虛無假設的 \\(X\\) 數值集合（這就是為什麼棄卻域有時也被稱為拒絕域）。我們如何找到這個棄卻域呢？嗯，讓我們想一想已知的條件：\n\n為了拒絕虛無假設，\\(X\\) 應該非常大或非常小\n如果虛無假設為真，\\(X\\) 的取樣分佈會是 \\(Binomial(0.5, N)\\)\n如果 \\(\\alpha = .05\\)，則棄卻域必須包含這個取樣分佈的 5%。\n\n最後一點非常重要。棄卻域的範圍是指那些會導致我們拒絕虛無假設的 \\(X\\) 數值範圍，而這個範圍是經由取樣分佈代換後的機率質量所決定的。如果我們選擇了一個其涵蓋 \\(20%\\) 機率質量的棄卻域，且虛無假設是符合事實的，那麼拒絕虛無假設的錯誤機率就是 \\(20%\\)。換言之，我們完成了一個顯著水準為 \\(0.2\\) 的檢驗。如果我們要求顯著水準是 \\(\\alpha = .05\\)，那麼棄卻域只能涵蓋統計檢定量取樣分佈的 \\(5%\\) 機率質量。\n\n我們在此總結解決完成假設檢定程序的三個要點。我們的棄卻域包括了機率分佈的最極端數值，也就是機率分佈的尾部。 Figure 9.2 展示了這個概念的視覺化。如果我們希望 \\(\\alpha = .05\\)，那麼對應的棄卻域是 \\(X \\leq 40\\) 和 \\(X \\geq 60\\)。6也就是說，如果回答正確的人數在 41 到 59 之間，那麼我們應該保留虛無假設。如果回答正確的人數在 0 到 40 或 60 到 100 之間，那麼我們就應該拒絕虛無假設。數字 40 和 60 通常被稱為臨界值(critical values)，因為這些數值定義了棄卻域的邊界。\n\n\n\n\n\n\nFigure 9.2: 這張圖與 Figure 9.1 的虛無假設 \\(X\\) 取樣分佈一樣，進一步展示ESP研究的假設檢定的棄卻域，假設檢定的顯著水準為\\(\\alpha = .05\\) 。灰色的柱子表示我們會保留虛無假設的 \\(X\\) 數值集合。深藍色的柱子表示棄卻域，也就是我們會拒絕虛無假設的 \\(X\\) 值。由於對立假設的主張是雙側的（即允許 \\(\\theta < .5\\) 和 \\(\\theta > .5\\)），因此棄卻域涵蓋分佈的兩個尾部。為確保 \\(\\alpha\\) 水準為 \\(.05\\)，我們需要確保左右區域各涵蓋了取樣分佈的 \\(2.5%\\)。\n\n\n\n\n最後，總結一下完成假設檢驗的主要步驟：\n\n選擇一個顯著水準 (例如，\\(\\alpha = .05\\))；\n選擇一個適當的統計檢定值 (例如，\\(X\\))，並且設定有比較意義的\\(H_0\\)和\\(H_1\\);\n假設虛無假設是符合事實的，找出該統計檢定值的取樣分佈（在ESP案例為二項分佈）；\n計算會產生符合 \\(\\alpha\\) 的棄卻域（0-40和60-100）。\n\n現在我們所要做的就是用實際資料計算統計檢定值（例如 \\(X=62\\)），然後比較檢定值與臨界值做出決策。由於 \\(62\\) 大於臨界值 \\(60\\)，我們可以拒絕虛無假設。也可以說，我們根據檢定結果得到一個在統計顯著的結論。\n\n\n\n9.4.2 小心使用統計“顯著”\n\n統計學和其他占卜術一樣，擁有一套專門術語，故意設計成讓非專業人員無法從字面理解術語的意思。 – G. O. Ashley 7\n\n在此需要講個關於 “significant”(常見中文說法“顯著”) 這個詞怎麼來的題外話。“significant” 在統計學中的概念其實很簡單，但這樣的命名並不夠好。如果實際資料能讓我們拒絕虛無假設，我們會說 “the result is statistically significant”(常見中文說法”結果有統計顯著性”)，通常簡單寫成 “the result is significant”(常見中文說法”有顯著結果”)。這個英文詞彙其實由來已久，來源可以追溯到 “significant” 的意思只是表達 “indicated”(已確認)的時代，並沒有現代英語的 “重要” 之類的含義。因此，今天許多讀者在開始學習統計學時會感到非常困惑，因為他們認為 “significant result” 必定是一個重要的結果。實際上，這並不是最早統計學家開始使用這個詞的意思。所有用”statistically significant”表達的主張， 只是表示資料允許我們拒絕一個虛無假設。至於結果是不是真的重要，則是另一個完全不同的問題，並且有其他各種因素的影響。\n\n\n\n9.4.3 單側與雙側檢定的不同\n還有一件事情要提醒各位同學，就是虛無假設與對立假設的設定方式。假如前面我所使用的統計假設是：\\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] 我們會發現對立假設涵蓋了 \\(\\theta < .5\\) 和 \\(\\theta > .5\\) 這兩種可能的數值集合。這代表我認為超感官知覺可能造成優於純粹猜測的表現，也可能產生比純粹猜測還差的表現（有些人就是會這麼認為），那麼這樣的設定是有意義的。在統計學的語彙庫，這稱為雙側檢定(two-sided test)。這是因為對立假設涵蓋了 無假設兩側的數值集合，因此檢定的棄卻域覆蓋取樣分佈的兩側尾部（如果 \\(\\alpha = .05\\)，則每側尾部佔取樣分佈的2.5％），如同 Figure 9.2 的展示。不過，這不是唯一的可能結果。如果我只在乎超感官知覺能夠產生優於純粹猜測的表現時，才願意相信這是事實，那麼對立假設就只會涵蓋 \\(\\theta > .5\\) 的數值集合。因此，虛無假設和對立假設就會變成\\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta > 0.5\\] 這種檢定條件 就是所謂的單側檢定(one-sided test)，此時檢定的棄卻域只有覆蓋取樣分佈的右側尾部，如同 Figure 9.3 的展示。\n\n\n\n\n\n\nFigure 9.3: 單側檢定的臨界區間。此狀況的對立假設是\\(\\theta \\geq .5\\)，當\\(X\\)的值很大，我們才能拒絕虛無假設。因此，臨界區間僅覆蓋取樣分佈的較大數值，具體來說是分佈的右側的 \\(5%\\) 。比較一下 Figure 9.2 的雙側檢定狀況。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設檢定的報告格式",
    "href": "09-Hypothesis-testing.html#假設檢定的報告格式",
    "title": "9  假設檢定",
    "section": "9.6 假設檢定的報告格式",
    "text": "9.6 假設檢定的報告格式\n在假設檢定結果的報告，通常需要報告幾項資訊，各種檢定方法的報告格式也各有特別之處。本書的後半部將介紹如何報告各種的檢定結果（特別詳細的例子請參考 小單元 10.1.9 ），讓同學對常用檢定方法的報告格式有所了解。不過，無論是做那一種檢定，都是要報告p值和結果是否有統計顯著性。\n報告裡要說明p值是否表示有顯著性，這一點並不令人意外，因為這正是執行假設檢定程序的目的。會讓人意外的是，學術界對於要如何報告假設檢定結果一直存在各種爭議。先不管完全反對在報告中呈現虛無假設檢定結果的聲音，報告確切的p值和僅聲明\\(p &lt; \\alpha\\)（例如，\\(p &lt; .05\\)），這兩種報告規範一直互相拉鋸。\n\n\n9.6.1 一些爭議\n要了解為什麼有這些爭議，首先要認識報告p值的便利性。在統計實務，只要能夠計算出p值，即使事先沒有指定任何顯著水準 \\(\\alpha\\) ，任何人都可以進行檢定。也就是說，人人可以直接計算p值並直接做解釋。如果今天得到p = .062，這代表著研究者 必須願意容忍 \\(6.2%\\) 的型一錯誤率，才能證實虛無假設不符合事實。如果研究者無法接受 \\(6.2%\\) 的型一錯誤率，那麼就要接受虛無假設。因此爭議在於，為什麼不報告實際的p值，讓讀者自己決定什麼樣的型一錯誤率是可以接受的？這種報告方式的優勢在於 “軟化” 決策責任。若是你傾向接受尼曼定義p值的方法，這就是p值的充分意義。我們不必再依賴一個固定的顯著水準，像是 \\(\\alpha = .05\\) ， 做為 “接受” 或 “否決” 之間的明確界線，如此就消除了用 p = .051 和 p = .049等邊緣數值，做為決策依據的詭異問題。\n這種解讀的彈性是p值的優勢和劣勢。許多人不喜歡報告精確的p值原因是，研究人員因此有太多解釋的彈性空間。特別是每次查看資料後，p值能讓研究者改變願意容忍多少錯誤率的想法。以我虛構的 ESP 實驗為例，若是我進行了一次實驗，得到了一個\\(0.09\\)的p值。我應該要接受還是拒絕虛無假設？說實話，我甚至還沒有想到我“真正”願意接受的型一錯誤率。我對方法學的問題沒有意見，但是我對 ESP 是否存在肯定有預先立場，而且我肯定會在乎我的研究是否能夠在一個有影響力的科學期刊上發表。最匪夷所思的是，在我看了初期資料後，開始覺得 \\(9%\\) 的錯誤率還算可以，特別是與向全世界承認我的實驗失敗的煩惱相比，似乎是幸運的。因此，為了避免看起來像是事後編造，我就在報告裡說我的 \\(\\alpha\\) 是 \\(0.1\\)，理由是型一錯誤率 \\(10%\\) 並不算太糟糕，而且以這個條件來說，我的實驗結果是顯著的！我贏了。\n換句話說，這個故事情節透露，即使研究者的意圖是好的，態度也夠誠實，有時也很難抵擋在某些方面“淡化”研究難度的誘惑。任何曾經做過實驗的人都知道，這是一個漫長而且困難的過程，你常常會對你的假設情有獨鍾，很難讓人放下，並承認實驗沒有找到你想找到的結果，而這就是決策風險所在。如果我們報告“原始的”p值，每個人會朝他們想要相信的方向來解讀數據，而不是從數據看出事實，如果我們允許這樣做，那為什麼科學家們還要繼續進行科學研究呢？為什麼不讓每個人在任何事情上相信他們喜歡的立場，而不去管事實是什麼？好吧，這麼說有點極端了，但這就是爭議的來源。如果同學同意這樣的觀點，就必須事先指定顯著水準\\(\\alpha\\)，然後只報告檢定結果是否顯著。這是保持誠實的唯一途徑。\n\n\n\n\n\n\n表 9.5: p值水準的一般報告方法\n\n\nUsual notation\nSignif. stars\nEnglish translation\nThe null is...\n\n\np &gt; .05\n\nThe test wasn't significant\nRetained\n\n\np &lt; .05\n*\nThe test was significant at \\( \\alpha \\) = .05 but not at \\( \\alpha \\) = .01 or \\( \\alpha \\) = .001.\nRejected\n\n\np &lt; .01\n**\nThe test was significant at \\( \\alpha \\) = .05 and \\( \\alpha \\) = .01 but not at \\( \\alpha \\) = .001.\nRejected\n\n\np &lt; .001\n***\nThe test was significant at all levels\nRejected\n\n\n\n\n\n\n\n\n\n\n9.6.2 兩種實務建議\n在許多領域的統計實務，研究人員很少只指定一種顯著水準 \\(\\alpha\\) 。很多科學家會根據三種顯著水準做為決策標準：\\(.05\\)、\\(.01\\) 和 \\(.001\\)。結果報告裡要明確指出根據那些顯著水準，能拒絕虛無假設。陳述檢定結果的報告範例整理在 表 9.5 。這樣可以稍微放開決策規則，因為以符合 \\(p &lt; .01\\) 的資料做為證據，強度明顯大過 \\(p &lt; .05\\) 的資料。除此之外，由於這些顯著水準是事先按慣例設定的，可以避免研究人員在查看數據後，才選擇顯著水準 \\(\\alpha\\) 。\n儘管如此，許多研究者仍然喜歡報告精確的p值。對許多人來說，讓讀者自行決定如何解讀\\(p=0.06\\)的好處多過壞處。然而，在許多實務場合，即使是那些偏好報告精確p值的研究人員，通常也只會寫\\(p&lt;0.001\\)，而不是報告超微小p值的確切數值。這部分是因為許多軟體在p值太小時並不會印出完整數值（例如，SPSS算出\\(p&lt;0.001\\)的話，只會印出\\(p=0.000\\)），另一方面，非常小的p值可能會誤導讀者 。人類大腦看到像\\(0.0000000001\\)這樣的數字時，很難壓抑直覺認為支持對立假設的結果是不可動搖的鐵證。然而，在真實世界，這樣的想法通常是錯誤的。生命是複雜的，每一個被發明出來的統計檢驗方法都是根據簡化的、逼近的和假設的模型。因此，很難抗拒從任何統計分析得到的\\(p&lt;0.001\\)強烈信心感。換句話說，\\(p&lt;0.001\\)實際只代表“就這個檢驗方法而言，證據是壓倒性的”。\n有鑑於此，同學可能好奇應該要怎麼做。有許多處理這個問題的建議，存在相當多相互矛盾的地方。有些人認為應該報告精確的p值，也有人認為檢定報告應該如同 表 9.1 ，明確寫出研究假設與統計假設。對此，我能給同學們的最好建議是，大量閱讀你的主修領域已經發表的文獻報告，看看有什麼慣例。如果看不出有什麼一致的報告模式，那麼就使用你偏好的任何方法。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設檢定實作須知",
    "href": "09-Hypothesis-testing.html#假設檢定實作須知",
    "title": "9  假設檢定",
    "section": "9.7 假設檢定實作須知",
    "text": "9.7 假設檢定實作須知\n介紹到這裡，可能有些同學會想知道ESP研究是一個“真正”的假設檢定，還是只為了教學而編造的虛構範例，其實我們真的可以跑檢定程序。在前面的介紹，我(原作者)根據 小單元 9.4 介紹的決策要素設定這個檢定程序，因為我認為這是大家在現實世界裡會遇到的最簡單的研究問題。不過，統計學家早就發明對應這種問題的檢定方法，正式名稱是二項式檢定，並且能用 jamovi 的內建模組”Frequencies”執行，在”Analyses”面板開啟模組選單，選擇“2 Outcomes”就能執行。若是設定虛無假設是回答機率為一半，即 \\(p = 0.5\\)，9。從示範資料庫裡開啟binomialtest.omv這個檔案，檢驗\\(n=100\\)位參與者裡 \\(x=62\\) 位回答正確的資料能拒絕或接受虛無假設，檢定結果如同 圖 9.4 。\n\n\n\n\n\n\n圖 9.4: 使用jamovi執行二項式檢定\n\n\n\n\n報表內容對初學的同學來說可能還很陌生，但你應該可以看出一些前面談過，要在報告裡呈現的資訊。最具體的一項是p = 0.02 小於常用的顯著水準 \\(\\alpha = 0.05\\)，因此我們可以拒絕虛無假設。隨著後續章節的學習，同學們會逐漸了解如何閱讀報表內容，希望學過之後，大家會發現報表內容的呈現方式，在閱讀和理解報告的好處。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#值得多了解的觀點",
    "href": "09-Hypothesis-testing.html#值得多了解的觀點",
    "title": "9  假設檢定",
    "section": "9.9 值得多了解的觀點",
    "text": "9.9 值得多了解的觀點\nWhat I’ve described to you in this chapter is the orthodox framework for null hypothesis significance testing (NHST). Understanding how NHST works is an absolute necessity because it has been the dominant approach to inferential statistics ever since it came to prominence in the early 20th century. It’s what the vast majority of working scientists rely on for their data analysis, so even if you hate it you need to know it. However, the approach is not without problems. There are a number of quirks in the framework, historical oddities in how it came to be, theoretical disputes over whether or not the framework is right, and a lot of practical traps for the unwary. I’m not going to go into a lot of detail on this topic, but I think it’s worth briefly discussing a few of these issues.\n\n9.9.1 尼曼與費雪\nThe first thing you should be aware of is that orthodox NHST is actually a mash-up of two rather different approaches to hypothesis testing, one proposed by Sir Ronald Fisher and the other proposed by Jerzy Neyman (see Lehmann (2011) for a historical summary). The history is messy because Fisher and Neyman were real people whose opinions changed over time, and at no point did either of them offer “the definitive statement” of how we should interpret their work many decades later. That said, here’s a quick summary of what I take these two approaches to be.\nFirst, let’s talk about Fisher’s approach. As far as I can tell, Fisher assumed that you only had the one hypothesis (the null) and that what you want to do is find out if the null hypothesis is inconsistent with the data. From his perspective, what you should do is check to see if the data are “sufficiently unlikely” according to the null. In fact, if you remember back to our earlier discussion, that’s how Fisher defines the p-value. According to Fisher, if the null hypothesis provided a very poor account of the data then you could safely reject it. But, since you don’t have any other hypotheses to compare it to, there’s no way of “accepting the alternative” because you don’t necessarily have an explicitly stated alternative. That’s more or less all there is to it.\nIn contrast, Neyman thought that the point of hypothesis testing was as a guide to action and his approach was somewhat more formal than Fisher’s. His view was that there are multiple things that you could do (accept the null or accept the alternative) and the point of the test was to tell you which one the data support. From this perspective, it is critical to specify your alternative hypothesis properly. If you don’t know what the alternative hypothesis is, then you don’t know how powerful the test is, or even which action makes sense. His framework genuinely requires a competition between different hypotheses. For Neyman, the \\(p\\) value didn’t directly measure the probability of the data (or data more extreme) under the null, it was more of an abstract description about which “possible tests” were telling you to accept the null, and which “possible tests” were telling you to accept the alternative.\nAs you can see, what we have today is an odd mishmash of the two. We talk about having both a null hypothesis and an alternative (Neyman), but usually13 define the \\(p\\) value in terms of exreme data (Fisher), but we still have \\(\\alpha\\) values (Neyman). Some of the statistical tests have explicitly specified alternatives (Neyman) but others are quite vague about it (Fisher). And, according to some people at least, we’re not allowed to talk about accepting the alternative (Fisher). It’s a mess, but I hope this at least explains why it’s a mess.\n\n\n9.9.2 貝氏統計與次數統計\nEarlier on in this chapter I was quite emphatic about the fact that you cannot interpret the p value as the probability that the null hypothesis is true. NHST is fundamentally a frequentist tool (see Chapter 7) and as such it does not allow you to assign probabilities to hypotheses. The null hypothesis is either true or it is not. The Bayesian approach to statistics interprets probability as a degree of belief, so it’s totally okay to say that there is a \\(10\\%\\) chance that the null hypothesis is true. That’s just a reflection of the degree of confidence that you have in this hypothesis. You aren’t allowed to do this within the frequentist approach. Remember, if you’re a frequentist, a probability can only be defined in terms of what happens after a large number of independent replications (i.e., a long run frequency). If this is your interpretation of probability, talking about the “probability” that the null hypothesis is true is complete gibberish: a null hypothesis is either true or it is false. There’s no way you can talk about a long run frequency for this statement. To talk about “the probability of the null hypothesis” is as meaningless as “the colour of freedom”. It doesn’t have one!\nMost importantly, this isn’t a purely ideological matter. If you decide that you are a Bayesian and that you’re okay with making probability statements about hypotheses, you have to follow the Bayesian rules for calculating those probabilities. I’ll talk more about this in Chapter 16, but for now what I want to point out to you is the p value is a terrible approximation to the probability that \\(H_0\\) is true. If what you want to know is the probability of the null, then the p value is not what you’re looking for!\n\n\n9.9.3 決策陷阱\nAs you can see, the theory behind hypothesis testing is a mess, and even now there are arguments in statistics about how it “should” work. However, disagreements among statisticians are not our real concern here. Our real concern is practical data analysis. And while the “orthodox” approach to null hypothesis significance testing has many drawbacks, even an unrepentant Bayesian like myself would agree that they can be useful if used responsibly. Most of the time they give sensible answers and you can use them to learn interesting things. Setting aside the various ideologies and historical confusions that we’ve discussed, the fact remains that the biggest danger in all of statistics is thoughtlessness. I don’t mean stupidity, I literally mean thoughtlessness. The rush to interpret a result without spending time thinking through what each test actually says about the data, and checking whether that’s consistent with how you’ve interpreted it. That’s where the biggest trap lies.\nTo give an example of this, consider the following example (see Gelman & Stern (2006)). Suppose I’m running my ESP study and I’ve decided to analyse the data separately for the male participants and the female participants. Of the male participants, \\(33\\) out of \\(50\\) guessed the colour of the card correctly. This is a significant effect (\\(p = .03\\)). Of the female participants, \\(29\\) out of \\(50\\) guessed correctly. This is not a significant effect (\\(p = .32\\)). Upon observing this, it is extremely tempting for people to start wondering why there is a difference between males and females in terms of their psychic abilities. However, this is wrong. If you think about it, we haven’t actually run a test that explicitly compares males to females. All we have done is compare males to chance (binomial test was significant) and compared females to chance (binomial test was non significant). If we want to argue that there is a real difference between the males and the females, we should probably run a test of the null hypothesis that there is no difference! We can do that using a different hypothesis test,14 but when we do that it turns out that we have no evidence that males and females are significantly different (\\(p = .54\\)). Now do you think that there’s anything fundamentally different between the two groups? Of course not. What’s happened here is that the data from both groups (male and female) are pretty borderline. By pure chance one of them happened to end up on the magic side of the \\(p = .05\\) line, and the other one didn’t. That doesn’t actually imply that males and females are different. This mistake is so common that you should always be wary of it. The difference between significant and not-significant is not evidence of a real difference. If you want to say that there’s a difference between two groups, then you have to test for that difference!\nThe example above is just that, an example. I’ve singled it out because it’s such a common one, but the bigger picture is that data analysis can be tricky to get right. Think about what it is you want to test, why you want to test it, and whether or not the answers that your test gives could possibly make any sense in the real world."
  },
  {
    "objectID": "09-Hypothesis-testing.html#本章小結",
    "href": "09-Hypothesis-testing.html#本章小結",
    "title": "9  假設檢定",
    "section": "9.10 本章小結",
    "text": "9.10 本章小結\n虛無假設檢定是統計理論最普遍的一種應用，科學研究報告結果都會呈現某種假設的檢定。現代科學家幾乎都多少要了解p值的意義，否則不能被認為是合格的科學研究者，所以這一章是本書最重要的部分。以下幫助讀者及學生快速回顧本章重點：\n\n假設的層次 研究假設與統計假設。虛無假設與對立假設。\n兩種決策失誤 型一與型二錯誤15\n運用取樣分佈檢測統計值.\n統計推論的決策要素16\n統計檢定的p值 為何用p值做決策是”模擬兩可”。\n假設檢定的報告格式\n假設檢定實作須知17\n效果量、樣本量、考驗力18\n一些值得繼續學習的主題\n\n到了本書最後一章 單元 16 ，我們會從貝氏統計觀點回顧統計理論與虛無假設檢定，還有介紹幾套非次數主義取向的統計工具。不過我們先暫別抽象的統計理論，接下來的第五部分將學習實用的統計分析方法。\n\n\n\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes: Statistical power, meta-analysis, and the interpretation of research results. Cambridge University Press.\n\n\nGelman, A., & Stern, H. (2006). The difference between “significant” and “not significant” is not itself statistically significant. The American Statistician, 60, 328–331.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation of classical statistics. Springer."
  },
  {
    "objectID": "10-Categorical-data-analysis.html",
    "href": "10-Categorical-data-analysis.html",
    "title": "10  類別資料分析",
    "section": "",
    "text": "Now that we’ve covered the basic theory behind hypothesis testing it’s time to start looking at specific tests that are commonly used in psychology. So where should we start? Not every textbook agrees on where to start, but I’m going to start with “\\(\\chi^2\\) tests” (this chapter, pronounced “chi-square”1 and “t-tests” in Chapter 11). Both of these tools are very frequently used in scientific practice, and whilst they’re not as powerful as “regression” and “analysis of variance” which we cover in later chapters, they’re much easier to understand.\nThe term “categorical data” is just another name for “nominal scale data”. It’s nothing that we haven’t already discussed, it’s just that in the context of data analysis people tend to use the term “categorical data” rather than “nominal scale data”. I don’t know why. In any case, categorical data analysis refers to a collection of tools that you can use when your data are nominal scale. However, there are a lot of different tools that can be used for categorical data analysis, and this chapter covers only a few of the more common ones."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-chi2-chi-square-goodness-of-fit-test",
    "href": "10-Categorical-data-analysis.html#the-chi2-chi-square-goodness-of-fit-test",
    "title": "10  Categorical data analysis",
    "section": "10.1 The \\(\\chi^2\\) (chi-square) goodness-of-fit test",
    "text": "10.1 The \\(\\chi^2\\) (chi-square) goodness-of-fit test\nThe \\(\\chi^2\\) goodness-of-fit test is one of the oldest hypothesis tests around. It was invented by Karl Pearson around the turn of the century (Pearson, 1900), with some corrections made later by Sir Ronald Fisher (Fisher, 1922). It tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution. For example, suppose a group of patients has been undergoing an experimental treatment and have had their health assessed to see whether their condition has improved, stayed the same or worsened. A goodness-of-fit test could be used to determine whether the numbers in each category - improved, no change, worsened - match the numbers that would be expected given the standard treatment option. Let’s think about this some more, with some psychology.\n\n10.1.1 The cards data\nOver the years there have been many studies showing that humans find it difficult to simulate randomness. Try as we might to “act” random, we think in terms of patterns and structure and so, when asked to “do something at random”, what people actually do is anything but random. As a consequence, the study of human randomness (or non-randomness, as the case may be) opens up a lot of deep psychological questions about how we think about the world. With this in mind, let’s consider a very simple study. Suppose I asked people to imagine a shuffled deck of cards, and mentally pick one card from this imaginary deck “at random”. After they’ve chosen one card I ask them to mentally select a second one. For both choices what we’re going to look at is the suit (hearts, clubs, spades or diamonds) that people chose. After asking, say, \\(N = 200\\) people to do this, I’d like to look at the data and figure out whether or not the cards that people pretended to select were really random. The data are contained in the randomness.csv file in which, when you open it up in jamovi and take a look at the spreadsheet view, you will see three variables. These are: an id variable that assigns a unique identifier to each participant, and the two variables choice_1 and choice_2 that indicate the card suits that people chose.\nFor the moment, let’s just focus on the first choice that people made. We’ll use the Frequency tables option under ‘Exploration’ - ‘Descriptives’ to count the number of times that we observed people choosing each suit. This is what we get (Table 10.1):\n\n\n\n\nTable 10.1:  Number of times each suit was chosen \n\nclubsdiamondsheartsspades\n\n35516450\n\n\n\n\n\nThat little frequency table is quite helpful. Looking at it, there’s a bit of a hint that people might be more likely to select hearts than clubs, but it’s not completely obvious just from looking at it whether that’s really true, or if this is just due to chance. So we’ll probably have to do some kind of statistical analysis to find out, which is what I’m going to talk about in the next section.\nExcellent. From this point on, we’ll treat this table as the data that we’re looking to analyse. However, since I’m going to have to talk about this data in mathematical terms (sorry!) it might be a good idea to be clear about what the notation is. In mathematical notation, we shorten the human-readable word “observed” to the letter \\(O\\), and we use subscripts to denote the position of the observation. So the second observation in our table is written as \\(O_2\\) in maths. The relationship between the English descriptions and the mathematical symbols are illustrated in Table 10.2.\n\n\n\n\nTable 10.2:  Relationship between English descriptions and mathematical symbols \n\nlabelindex, imath. symbolthe value\n\nclubs, \\( \\clubsuit \\)1\\( O_1 \\)35\n\ndiamonds, \\( \\diamondsuit \\)2\\( O_2 \\)51\n\nhearts, \\( \\heartsuit \\)3\\( O_3 \\)64\n\nspades, \\( \\spadesuit \\)4\\( O_4 \\)50\n\n\n\n\n\nHopefully that’s pretty clear. It’s also worth noting that mathematicians prefer to talk about general rather than specific things, so you’ll also see the notation \\(O_i\\), which refers to the number of observations that fall within the i-th category (where i could be 1, 2, 3 or 4). Finally, if we want to refer to the set of all observed frequencies, statisticians group all observed values into a vector 2, which I’ll refer to as \\(O\\).\n\\[O = (O_1, O_2, O_3, O_4)\\]\nAgain, this is nothing new or interesting. It’s just notation. If I say that \\(O = (35, 51, 64, 50)\\) all I’m doing is describing the table of observed frequencies (i.e., observed), but I’m referring to it using mathematical notation.\n\n\n10.1.2 The null hypothesis and the alternative hypothesis\nAs the last section indicated, our research hypothesis is that “people don’t choose cards randomly”. What we’re going to want to do now is translate this into some statistical hypotheses and then construct a statistical test of those hypotheses. The test that I’m going to describe to you is Pearson’s \\(\\chi^2\\) (chi-square) goodness-of-fit test, and as is so often the case we have to begin by carefully constructing our null hypothesis. In this case, it’s pretty easy. First, let’s state the null hypothesis in words:\n\\[H_0: \\text{ All four suits are chosen with equal probability}\\]\nNow, because this is statistics, we have to be able to say the same thing in a mathematical way. To do this, let’s use the notation \\(P_j\\) to refer to the true probability that the j-th suit is chosen. If the null hypothesis is true, then each of the four suits has a 25% chance of being selected. In other words, our null hypothesis claims that \\(P_1 = .25\\), \\(P_2 = .25\\), \\(P3 = .25\\) and finally that \\(P_4 = .25\\) . However, in the same way that we can group our observed frequencies into a vector O that summarises the entire data set, we can use P to refer to the probabilities that correspond to our null hypothesis. So if I let the vector \\(P = (P_1, P_2, P_3, P_4)\\) refer to the collection of probabilities that describe our null hypothesis, then we have:\n\\[H_0: P =(.25, .25, .25, .25)\\]\nIn this particular instance, our null hypothesis corresponds to a vector of probabilities P in which all of the probabilities are equal to one another. But this doesn’t have to be the case. For instance, if the experimental task was for people to imagine they were drawing from a deck that had twice as many clubs as any other suit, then the null hypothesis would correspond to something like \\(P = (.4, .2, .2, .2)\\). As long as the probabilities are all positive numbers, and they all sum to 1, then it’s a perfectly legitimate choice for the null hypothesis. However, the most common use of the goodness-of-fit test is to test a null hypothesis that all of the categories are equally likely, so we’ll stick to that for our example.\nWhat about our alternative hypothesis, \\(H_1\\)? All we’re really interested in is demonstrating that the probabilities involved aren’t all identical (that is, people’s choices weren’t completely random). As a consequence, the “human friendly” versions of our hypotheses look like this:\n\\(H_0: \\text{ All four suits are chosen with equal probability}\\)\n\\(H_1: \\text{ At least one of the suit-choice probabilities isn’t 0.25}\\)\n…and the “mathematician friendly” version is:\n\\(H_0: P= (.25, .25, .25, .25)\\)\n\\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n10.1.3 The “goodness-of-fit” test statistic\nAt this point, we have our observed frequencies O and a collection of probabilities P corresponding to the null hypothesis that we want to test. What we now want to do is construct a test of the null hypothesis. As always, if we want to test \\(H_0\\) against \\(H_1\\), we’re going to need a test statistic. The basic trick that a goodness-of-fit test uses is to construct a test statistic that measures how “close” the data are to the null hypothesis. If the data don’t resemble what you’d “expect” to see if the null hypothesis were true, then it probably isn’t true. Okay, if the null hypothesis were true, what would we expect to see? Or, to use the correct terminology, what are the expected frequencies. There are \\(N = 200\\) observations, and (if the null is true) the probability of any one of them choosing a heart is \\(P_3 = .25\\), so I guess we’re expecting \\(200 \\times .25 = 50\\) hearts, right? Or, more specifically, if we let Ei refer to “the number of category i responses that we’re expecting if the null is true”, then\n\\[E_i=N \\times P_i\\]\nThis is pretty easy to calculate.If there are 200 observations that can fall into four categories, and we think that all four categories are equally likely, then on average we’d expect to see 50 observations in each category, right?\nNow, how do we translate this into a test statistic? Clearly, what we want to do is compare the expected number of observations in each category (\\(E_i\\)) with the observed number of observations in that category (\\(O_i\\)). And on the basis of this comparison we ought to be able to come up with a good test statistic. To start with, let’s calculate the difference between what the null hypothesis expected us to find and what we actually did find. That is, we calculate the “observed minus expected” difference score, \\(O_i - E_i\\) . This is illustrated in Table 10.3.\n\n\n\n\nTable 10.3:  Expected and observed frequencies \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)50505050\n\nobserved frequency \\( O_i\\)35516450\n\ndifference score \\( O_i-E_i\\)-151140\n\n\n\n\n\nSo, based on our calculations, it’s clear that people chose more hearts and fewer clubs than the null hypothesis predicted. However, a moment’s thought suggests that these raw differences aren’t quite what we’re looking for. Intuitively, it feels like it’s just as bad when the null hypothesis predicts too few observations (which is what happened with hearts) as it is when it predicts too many (which is what happened with clubs). So it’s a bit weird that we have a negative number for clubs and a positive number for hearts. One easy way to fix this is to square everything, so that we now calculate the squared differences, \\((E_i - O_i)^2\\) . As before, we can do this by hand (Table 10.4).\n\n\n\n\nTable 10.4:  Squaring the difference scores \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n22511960\n\n\n\n\n\nNow we’re making progress. What we’ve got now is a collection of numbers that are big whenever the null hypothesis makes a bad prediction (clubs and hearts), but are small whenever it makes a good one (diamonds and spades). Next, for some technical reasons that I’ll explain in a moment, let’s also divide all these numbers by the expected frequency Ei , so we’re actually calculating \\(\\frac{(E_i-O_i)^2}{E_i}\\) . Since \\(E_i = 50\\) for all categories in our example, it’s not a very interesting calculation, but let’s do it anyway (Table 10.5).\n\n\n\n\nTable 10.5:  Dividing the squared difference scores by the expected frequency to provide an ‘error’ score \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n4.500.023.920.00\n\n\n\n\n\nIn effect, what we’ve got here are four different “error” scores, each one telling us how big a “mistake” the null hypothesis made when we tried to use it to predict our observed frequencies. So, in order to convert this into a useful test statistic, one thing we could do is just add these numbers up. The result is called the goodness-of-fit statistic, conventionally referred to either as \\(\\chi^2\\) (chi-square) or GOF. We can calculate it as in Table 10.6.\n\\[\\sum( (observed - expected)^2 / expected )\\]\nThis gives us a value of 8.44.\n[Additional technical detail 3]\nAs we’ve seen from our calculations, in our cards data set we’ve got a value of \\(\\chi^2\\) = 8.44. So now the question becomes is this a big enough value to reject the null?\n\n\n10.1.4 The sampling distribution of the GOF statistic\nTo determine whether or not a particular value of \\(\\chi^2\\) is large enough to justify rejecting the null hypothesis, we’re going to need to figure out what the sampling distribution for \\(\\chi^2\\) would be if the null hypothesis were true. So that’s what I’m going to do in this section. I’ll show you in a fair amount of detail how this sampling distribution is constructed, and then, in the next section, use it to build up a hypothesis test. If you want to cut to the chase and are willing to take it on faith that the sampling distribution is a \\(\\chi^2\\) (chi-square) distribution with \\(k - 1\\) degrees of freedom, you can skip the rest of this section. However, if you want to understand why the goodness-of-fit test works the way it does, read on.\nOkay, let’s suppose that the null hypothesis is actually true. If so, then the true probability that an observation falls in the i-th category is \\(P_i\\) . After all, that’s pretty much the definition of our null hypothesis. Let’s think about what this actually means. This is kind of like saying that “nature” makes the decision about whether or not the observation ends up in category i by flipping a weighted coin (i.e., one where the probability of getting a head is \\(P_j\\) ). And therefore we can think of our observed frequency \\(O_i\\) by imagining that nature flipped N of these coins (one for each observation in the data set), and exactly \\(O_i\\) of them came up heads. Obviously, this is a pretty weird way to think about the experiment. But what it does (I hope) is remind you that we’ve actually seen this scenario before. It’s exactly the same set up that gave rise to Section 7.4 in Chapter 7. In other words, if the null hypothesis is true, then it follows that our observed frequencies were generated by sampling from a binomial distribution:\n\\[O_i \\sim Binomial(P_i,N) \\]\nNow, if you remember from our discussion of Section 8.3.3 the binomial distribution starts to look pretty much identical to the normal distribution, especially when \\(N\\) is large and when \\(P_i\\) isn’t too close to 0 or 1. In other words as long as \\(N^P_i\\) is large enough. Or, to put it another way, when the expected frequency Ei is large enough then the theoretical distribution of \\(O_i\\) is approximately normal. Better yet, if \\(O_i\\) is normally distributed, then so is \\((O_i-E_i)/\\sqrt{(E_i)}\\) . Since \\(E_i\\) is a fixed value, subtracting off Ei and dividing by ? Ei changes the mean and standard deviation of the normal distribution but that’s all it does. Okay, so now let’s have a look at what our goodness-of-fit statistic actually is. What we’re doing is taking a bunch of things that are normally-distributed, squaring them, and adding them up. Wait. We’ve seen that before too! As we discussed in the section on Section 7.6, when you take a bunch of things that have a standard normal distribution (i.e., mean 0 and standard deviation 1), square them and then add them up, the resulting quantity has a chi-square distribution. So now we know that the null hypothesis predicts that the sampling distribution of the goodness-of-fit statistic is a chi-square distribution. Cool.\nThere’s one last detail to talk about, namely the degrees of freedom. If you remember back to Section 7.6, I said that if the number of things you’re adding up is k, then the degrees of freedom for the resulting chi-square distribution is k. Yet, what I said at the start of this section is that the actual degrees of freedom for the chi-square goodness-of-fit test is \\(k - 1\\). What’s up with that? The answer here is that what we’re supposed to be looking at is the number of genuinely independent things that are getting added together. And, as I’ll go on to talk about in the next section, even though there are k things that we’re adding only \\(k - 1\\) of them are truly independent, and so the degrees of freedom is actually only \\(k - 1\\). That’s the topic of the next section4.\n\n\n10.1.5 Degrees of freedom\n\n\n\n\n\nFigure 10.1: \\(\\chi^2\\) (chi-square) distributions with different values for the ‘degrees of freedom’\n\n\n\n\nWhen I introduced the chi-square distribution in Section 7.6, I was a bit vague about what “degrees of freedom” actually means. Obviously, it matters. Looking at Figure 10.1, you can see that if we change the degrees of freedom then the chi-square distribution changes shape quite substantially. But what exactly is it? Again, when I introduced the distribution and explained its relationship to the normal distribution, I did offer an answer: it’s the number of “normally distributed variables” that I’m squaring and adding together. But, for most people, that’s kind of abstract and not entirely helpful. What we really need to do is try to understand degrees of freedom in terms of our data. So here goes.\nThe basic idea behind degrees of freedom is quite simple. You calculate it by counting up the number of distinct “quantities” that are used to describe your data and then subtracting off all of the “constraints” that those data must satisfy.5 This is a bit vague, so let’s use our cards data as a concrete example. We describe our data using four numbers, \\(O1, O2, O3\\) and O4 corresponding to the observed frequencies of the four different categories (hearts, clubs, diamonds, spades). These four numbers are the random outcomes of our experiment. But my experiment actually has a fixed constraint built into it: the sample size \\(N\\). 6 That is, if we know\nhow many people chose hearts, how many chose diamonds and how many chose clubs, then we’d be able to figure out exactly how many chose spades. In other words, although our data are described using four numbers, they only actually correspond to \\(4 - 1 = 3\\) degrees of freedom. A slightly different way of thinking about it is to notice that there are four probabilities that we’re interested in (again, corresponding to the four different categories), but these probabilities must sum to one, which imposes a constraint. Therefore the degrees of freedom is \\(4 - 1 = 3\\). Regardless of whether you want to think about it in terms of the observed frequencies or in terms of the probabilities, the answer is the same. In general, when running the \\(\\chi^2\\)(chi-square) goodness-of-fit test for an experiment involving \\(k\\) groups, then the degrees of freedom will be \\(k - 1\\).\n\n\n10.1.6 Testing the null hypothesis\nThe final step in the process of constructing our hypothesis test is to figure out what the rejection region is. That is, what values of \\(\\chi^2\\) would lead us to reject the null hypothesis. As we saw earlier, large values of \\(\\chi^2\\) imply that the null hypothesis has done a poor job of predicting the data from our experiment, whereas small values of \\(\\chi^2\\) imply that it’s actually done pretty well. Therefore, a pretty sensible strategy would be to say there is some critical value such that if \\(\\chi^2\\) is bigger than the critical value we reject the null, but if \\(\\chi^2\\) is smaller than this value we retain the null. In other words, to use the language we introduced in Chapter 9 the chi-square goodness-of-fit test is always a one-sided test. Right, so all we have to do is figure out what this critical value is. And it’s pretty straightforward. If we want our test to have significance level of \\(\\alpha = .05\\) (that is, we are willing to tolerate a Type I error rate of \\(5%\\)), then we have to choose our critical value so that there is only a 5% chance that \\(\\chi^2\\) could get to be that big if the null hypothesis is true. This is illustrated in Figure 10.2.\n\n\n\n\n\nFigure 10.2: Illustration of how the hypothesis testing works for the \\(\\chi^2\\) (chi-square) goodness of-fit test\n\n\n\n\nAh but, I hear you ask, how do I find the critical value of a chi-square distribution with \\(k-1\\) degrees of freedom? Many many years ago when I first took a psychology statistics class we used to look up these critical values in a book of critical value tables, like the one in Figure 10.3. Looking at this Figure, we can see that the critical value for a \\(\\chi^2\\) distribution with 3 degrees of freedom, and p=0.05 is 7.815.\n\n\n\n\n\nFigure 10.3: Table of critical values for the chi-square distribution\n\n\n\n\nSo, if our calculated \\(\\chi^2\\) statistic is bigger than the critical value of \\(7.815\\), then we can reject the null hypothesis (remember that the null hypothesis, \\(H_0\\), is that all four suits are chosen with equal probability). Since we actually already calculated that before (i.e., \\(\\chi^2\\) = 8.44) we can reject the null hypothesis. And that’s it, basically. You now know “Pearson’s \\(\\chi^2\\) test for the goodness-of-fit”. Lucky you.\n\n\n10.1.7 Doing the test in jamovi\nNot surprisingly, jamovi provides an analysis that will do these calculations for you. Let’s use the Randomness.omv file. From the main ‘Analyses’ toolbar select ‘Frequencies’ - ‘One Sample Proportion Tests’ - ‘\\(N\\) Outcomes’. Then in the analysis window that appears move the variable you want to analyse (choice 1 across into the ‘Variable’ box. Also, click on the ‘Expected counts’ check box so that these are shown on the results table. When you have done all this, you should see the analysis results in jamovi as in Figure 10.4. No surprise then that jamovi provides the same expected counts and statistics that we calculated by hand above, with a \\(\\chi^2\\) value of \\((8.44\\) with \\(3\\) d.f. and \\(p=0.038\\). Note that we don’t need to look up a critical p-value threshold value any more, as jamovi gives us the actual p-value of the calculated \\(\\chi^2\\) for \\(3\\) d.f.\n\n\n\n\n\nFigure 10.4: A \\(\\chi^2\\) One Sample Proportion Test in jamovi, with table showing both observed and expected frequencies and proportions\n\n\n\n\n\n\n10.1.8 Specifying a different null hypothesis\nAt this point you might be wondering what to do if you want to run a goodness-of-fit test but your null hypothesis is not that all categories are equally likely. For instance, let’s suppose that someone had made the theoretical prediction that people should choose red cards \\(60\\%\\) of the time, and black cards \\(40\\%\\) of the time (I’ve no idea why you’d predict that), but had no other preferences. If that were the case, the null hypothesis would be to expect \\(30\\%\\) of the choices to be hearts, \\(30\\%\\) to be diamonds, \\(20\\%\\) to be spades and \\(20\\%\\) to be clubs. In other words we would expect hearts and diamonds to appear 1.5 times more often than spades and clubs (the ratio \\(30\\%\\) : \\(20\\%\\) is the same as 1.5 : 1). This seems like a silly theory to me, and it’s pretty easy to test this explicitly specified null hypothesis with the data in our jamovi analysis. In the analysis window (labelled ‘Proportion Test (N Outcomes)’ in Figure 10.4 you can expand the options for ‘Expected Proportions’. When you do this, there are options for entering different ratio values for the variable you have selected, in our case this is choice 1. Change the ratio to reflect the new null hypothesis, as in Figure 10.5, and see how the results change.\n\n\n\n\n\nFigure 10.5: Changing the expected proportions in the \\(\\\\chi^2\\) One Sample Proportion Test in jamovi\n\n\n\n\nThe expected counts are now shown in Table 10.6.\n\n\n\n\nTable 10.6:  Expected counts for a different null hypothesis \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)40606040\n\n\n\n\n\nand the \\(\\chi^2\\) statistic is 4.74, 3 d.f., \\(p = 0.182\\). Now, the results of our updated hypotheses and the expected frequencies are different from what they were last time. As a consequence our \\(\\chi^2\\) test statistic is different, and our p-value is different too. Annoyingly, the p-value is \\(.182\\), so we can’t reject the null hypothesis (look back at Section 9.5 to remind yourself why). Sadly, despite the fact that the null hypothesis corresponds to a very silly theory, these data don’t provide enough evidence against it.\n\n\n10.1.9 How to report the results of the test\nSo now you know how the test works, and you know how to do the test using a wonderful jamovi flavoured magic computing box. The next thing you need to know is how to write up the results. After all, there’s no point in designing and running an experiment and then analysing the data if you don’t tell anyone about it! So let’s now talk about what you need to do when reporting your analysis. Let’s stick with our card-suits example. If I wanted to write this result up for a paper or something, then the conventional way to report this would be to write something like this:\n\nOf the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness-of-fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were significant (\\(\\chi^2(3) = 8.44, p< .05)\\), suggesting that people did not select suits purely at random.\n\nThis is pretty straightforward and hopefully it seems pretty unremarkable. That said, there’s a few things that you should note about this description:\n\nThe statistical test is preceded by the descriptive statistics. That is, I told the reader something about what the data look like before going on to do the test. In general, this is good practice. Always remember that your reader doesn’t know your data anywhere near as well as you do. So, unless you describe it to them properly, the statistical tests won’t make any sense to them and they’ll get frustrated and cry.\nThe description tells you what the null hypothesis being tested is. To be honest, writers don’t always do this but it’s often a good idea in those situations where some ambiguity exists, or when you can’t rely on your readership being intimately familiar with the statistical tools that you’re using. Quite often the reader might not know (or remember) all the details of the test that your using, so it’s a kind of politeness to “remind” them! As far as the goodness-of-fit test goes, you can usually rely on a scientific audience knowing how it works (since it’s covered in most intro stats classes). However, it’s still a good idea to be explicit about stating the null hypothesis (briefly!) because the null hypothesis can be different depending on what you’re using the test for. For instance, in the cards example my null hypothesis was that all the four suit probabilities were identical (i.e., \\(P1 = P2 = P3 = P4 = 0.25\\)), but there’s nothing special about that hypothesis. I could just as easily have tested the null hypothesis that \\(P_1 = 0.7\\) and \\(P2 = P3 = P4 = 0.1\\) using a goodness-of-fit test. So it’s helpful to the reader if you explain to them what your null hypothesis was. Also, notice that I described the null hypothesis in words, not in maths. That’s perfectly acceptable. You can describe it in maths if you like, but since most readers find words easier to read than symbols, most writers tend to describe the null using words if they can.\nA “stat block” is included. When reporting the results of the test itself, I didn’t just say that the result was significant, I included a “stat block” (i.e., the dense mathematical looking part in the parentheses) which reports all the “key” statistical information. For the chi-square goodness-of-fit test, the information that gets reported is the test statistic (that the goodness-of-fit statistic was 8.44), the information about the distribution used in the test (\\(\\chi^2\\) with 3 degrees of freedom which is usually shortened to \\(\\chi^2\\)(3)), and then the information about whether the result was significant (in this case \\(p< .05\\)). The particular information that needs to go into the stat block is different for every test, and so each time I introduce a new test I’ll show you what the stat block should look like.7 However the general principle is that you should always provide enough information so that the reader could check the test results themselves if they really wanted to.\nThe results are interpreted. In addition to indicating that the result was significant, I provided an interpretation of the result (i.e., that people didn’t choose randomly). This is also a kindness to the reader, because it tells them something about what they should believe about what’s going on in your data. If you don’t include something like this, it’s really hard for your reader to understand what’s going on.8\n\nAs with everything else, your overriding concern should be that you explain things to your reader. Always remember that the point of reporting your results is to communicate to another human being. I cannot tell you just how many times I’ve seen the results section of a report or a thesis or even a scientific article that is just gibberish, because the writer has focused solely on making sure they’ve included all the numbers and forgotten to actually communicate with the human reader.\n\nSatan delights equally in statistics and in quoting scripture9 – H.G. Wells"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-chi2-test-of-independence-or-association",
    "href": "10-Categorical-data-analysis.html#the-chi2-test-of-independence-or-association",
    "title": "10  Categorical data analysis",
    "section": "10.2 The \\(\\chi^2\\) test of independence (or association)",
    "text": "10.2 The \\(\\chi^2\\) test of independence (or association)\n\nGUARDBOT 1: Halt!\nGUARDBOT 2: Be you robot or human?\nLEELA: Robot…we be.\nFRY: Uh, yup! Just two robots out roboting it up! Eh?\nGUARDBOT 1: Administer the test.\nGUARDBOT 2: Which of the following would you most prefer? A: A puppy, B: A pretty flower from your sweetie, or C: A large properly-formatted data file?\nGUARDBOT 1: Choose!\nFuturama, “Fear of a Bot Planet”\n\nThe other day I was watching an animated documentary examining the quaint customs of the natives of the planet Chapek 9. Apparently, in order to gain access to their capital city a visitor must prove that they’re a robot, not a human. In order to determine whether or not a visitor is human, the natives ask whether the visitor prefers puppies, flowers, or large, properly formatted data files. “Pretty clever,” I thought to myself “but what if humans and robots have the same preferences? That probably wouldn’t be a very good test then, would it?” As it happens, I got my hands on the testing data that the civil authorities of Chapek 9 used to check this. It turns out that what they did was very simple. They found a bunch of robots and a bunch of humans and asked them what they preferred. I saved their data in a file called chapek9.omv, which we can now load into jamovi. As well as the ID variable that identifies individual people, there are two nominal text variables, species and choice. In total there are 180 entries in the data set, one for each person (counting both robots and humans as “people”) who was asked to make a choice. Specifically, there are 93 humans and 87 robots, and overwhelmingly the preferred choice is the data file. You can check this yourself by asking jamovi for Frequency Tables, under the ‘Exploration’ - ‘Descriptives’ button. However, this summary does not address the question we’re interested in. To do that, we need a more detailed description of the data. What we want to do is look at the choices broken down by species. That is, we need to cross-tabulate the data (see Section 6.1). In jamovi we do this using the ‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’ analysis, and we should get a table something like Table 10.7.\n\n\n\n\nTable 10.7:  Cross-tabulating the data \n\nRobotHumanTotal\n\nPuppy131528\n\nFlower301343\n\nData4465109\n\nTotal8793180\n\n\n\n\n\nFrom this, it’s quite clear that the vast majority of the humans chose the data file, whereas the robots tended to be a lot more even in their preferences. Leaving aside the question of why the humans might be more likely to choose the data file for the moment (which does seem quite odd, admittedly), our first order of business is to determine if the discrepancy between human choices and robot choices in the data set is statistically significant.\n\n10.2.1 Constructing our hypothesis test\nHow do we analyse this data? Specifically, since my research hypothesis is that “humans and robots answer the question in different ways”, how can I construct a test of the null hypothesis that “humans and robots answer the question the same way”? As before, we begin by establishing some notation to describe the data (Table 10.8).\n\n\n\n\nTable 10.8:  Notation to describe the data \n\nRobotHumanTotal\n\nPuppy\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nFlower\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nData\\(O_{31}\\)\\(O_{32}\\)\\(R_{3}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)N\n\n\n\n\n\nIn this notation we say that \\(O_{ij}\\) is a count (observed frequency) of the number of respondents that are of species j (robots or human) who gave answer i (puppy, flower or data) when asked to make a choice. The total number of observations is written \\(N\\), as usual. Finally, I’ve used \\(R_i\\) to denote the row totals (e.g., \\(R_1\\) is the total number of people who chose the flower), and \\(C_j\\) to denote the column totals (e.g., \\(C_1\\) is the total number of robots).10\nSo now let’s think about what the null hypothesis says. If robots and humans are responding in the same way to the question, it means that the probability that “a robot says puppy” is the same as the probability that “a human says puppy”, and so on for the other two possibilities. So, if we use \\(P_{ij}\\) to denote “the probability that a member of species j gives response i” then our null hypothesis is that:\n\\[\n\\begin{aligned}\nH_0 &: \\text{All of the following are true:} \\\\\n&P_{11} = P_{12}\\text{ (same probability of saying “puppy”),} \\\\\n&P_{21} = P_{22}\\text{ (same probability of saying “flower”), and} \\\\\n&P_{31} = P_{32}\\text{ (same probability of saying “data”).}\n\\end{aligned}\n\\]\nAnd actually, since the null hypothesis is claiming that the true choice probabilities don’t depend on the species of the person making the choice, we can let Pi refer to this probability, e.g., P1 is the true probability of choosing the puppy.\nNext, in much the same way that we did with the goodness-of-fit test, what we need to do is calculate the expected frequencies. That is, for each of the observed counts \\(O_{ij}\\) , we need to figure out what the null hypothesis would tell us to expect. Let’s denote this expected frequency by \\(E_{ij}\\). This time, it’s a little bit trickier. If there are a total of \\(C_j\\) people that belong to species \\(j\\), and the true probability of anyone (regardless of species) choosing option \\(i\\) is \\(P_i\\) , then the expected frequency is just:\n\\[E_{ij}=C_j \\times P_i\\]\nNow, this is all very well and good, but we have a problem. Unlike the situation we had with the goodness-of-fit test, the null hypothesis doesn’t actually specify a particular value for Pi .\nIt’s something we have to estimate (see Chapter 8) from the data! Fortunately, this is pretty easy to do. If 28 out of 180 people selected the flowers, then a natural estimate for the probability of choosing flowers is \\(\\frac{28}{180}\\), which is approximately \\(.16\\). If we phrase this in mathematical terms, what we’re saying is that our estimate for the probability of choosing option i is just the row total divided by the total sample size:\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\nTherefore, our expected frequency can be written as the product (i.e. multiplication) of the row total and the column total, divided by the total number of observations:11\n\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]\n[Additional technical detail 12]\nAs before, large values of \\(X^2\\) indicate that the null hypothesis provides a poor description of the data, whereas small values of \\(X^2\\) suggest that it does a good job of accounting for the data. Therefore, just like last time, we want to reject the null hypothesis if \\(X^2\\) is too large.\nNot surprisingly, this statistic is \\(\\chi^2\\) distributed. All we need to do is figure out how many degrees of freedom are involved, which actually isn’t too hard. As I mentioned before, you can (usually) think of the degrees of freedom as being equal to the number of data points that you’re analysing, minus the number of constraints. A contingency table with r rows and c columns contains a total of \\(r^{c}\\) observed frequencies, so that’s the total number of observations. What about the constraints? Here, it’s slightly trickier. The answer is always the same\n\\[df=(r-1)(c-1)\\]\nbut the explanation for why the degrees of freedom takes this value is different depending on the experimental design. For the sake of argument, let’s suppose that we had honestly intended to survey exactly 87 robots and 93 humans (column totals fixed by the experimenter), but left the row totals free to vary (row totals are random variables). Let’s think about the constraints that apply here. Well, since we deliberately fixed the column totals by Act of Experimenter, we have \\(c\\) constraints right there. But, there’s actually more to it than that. Remember how our null hypothesis had some free parameters (i.e., we had to estimate the Pi values)? Those matter too. I won’t explain why in this book, but every free parameter in the null hypothesis is rather like an additional constraint. So, how many of those are there? Well, since these probabilities have to sum to 1, there’s only \\(r - 1\\) of these. So our total degrees of freedom is:\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\nAlternatively, suppose that the only thing that the experimenter fixed was the total sample size N. That is, we quizzed the first 180 people that we saw and it just turned out that 87 were robots and 93 were humans. This time around our reasoning would be slightly different, but would still lead us to the same answer. Our null hypothesis still has \\(r - 1\\) free parameters corresponding to the choice probabilities, but it now also has \\(c - 1\\) free parameters corresponding to the species probabilities, because we’d also have to estimate the probability that a randomly sampled person turns out to be a robot.13 Finally, since we did actually fix the total number of observations N, that’s one more constraint. So, now we have rc observations, and \\((c-1)+(r-1)+1\\) constraints. What does that give?\n\\[\\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\n\\] Amazing.\n\n\n10.2.2 Doing the test in jamovi\nOkay, now that we know how the test works let’s have a look at how it’s done in jamovi. As tempting as it is to lead you through the tedious calculations so that you’re forced to learn it the long way, I figure there’s no point. I already showed you how to do it the long way for the goodness-of-fit test in the last section, and since the test of independence isn’t conceptually any different, you won’t learn anything new by doing it the long way. So instead I’ll go straight to showing you the easy way. After you have run the test in jamovi (‘Frequencies’ - ‘Contingency Tables’ - ‘Independent Samples’), all you have to do is look underneath the contingency table in the jamovi results window and there is the \\(\\chi^2\\) statistic for you. This shows a \\(\\chi^2\\) statistic value of 10.72, with 2 d.f. and p-value = 0.005.\nThat was easy, wasn’t it! You can also ask jamovi to show you the expected counts - just click on the check box for ‘Counts’ - ‘Expected’ in the ‘Cells’ options and the expected counts will appear in the contingency table. And whilst you are doing that, an effect size measure would be helpful. We’ll choose Cramér’s \\(V\\), and you can specify this from a check box in the ‘Statistics’ options, and it gives a value for Cramér’s \\(V\\) of \\(0.24\\). See Figure 10.6. We will talk about this some more in just a moment.\n\n\n\n\n\nFigure 10.6: Independent samples \\(\\chi^2\\) test in jamovi using the Chapek 9 data\n\n\n\n\nThis output gives us enough information to write up the result:\n\nPearson’s \\(\\chi^2\\) revealed a significant association between species and choice (\\(\\chi^2(2) = 10.7, p< .01)\\). Robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data.\n\nNotice that, once again, I provided a little bit of interpretation to help the human reader understand what’s going on with the data. Later on in my discussion section I’d provide a bit more context. To illustrate the difference, here’s what I’d probably say later on:\n\nThe fact that humans appeared to have a stronger preference for raw data files than robots is somewhat counter-intuitive. However, in context it makes some sense, as the civil authority on Chapek 9 has an unfortunate tendency to kill and dissect humans when they are identified. As such it seems most likely that the human participants did not respond honestly to the question, so as to avoid potentially undesirable consequences. This should be considered to be a substantial methodological weakness.\n\nThis could be classified as a rather extreme example of a reactivity effect, I suppose. Obviously, in this case the problem is severe enough that the study is more or less worthless as a tool for understanding the difference preferences among humans and robots. However, I hope this illustrates the difference between getting a statistically significant result (our null hypothesis is rejected in favour of the alternative), and finding something of scientific value (the data tell us nothing of interest about our research hypothesis due to a big methodological flaw)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-continuity-correction",
    "href": "10-Categorical-data-analysis.html#the-continuity-correction",
    "title": "10  Categorical data analysis",
    "section": "10.3 The continuity correction",
    "text": "10.3 The continuity correction\nOkay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the Yates correction. Remember what I pointed out earlier: the \\(\\chi^2\\) test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large \\(N\\). One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a \\(2 \\times 2\\) contingency table). The main reason for this is that the true sampling distribution for the \\(X^{2}\\) statistic is actually discrete (because you’re dealing with categorical data!) but the \\(\\chi^2\\) distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when \\(df = 1\\), the goodness-of-fit statistic tends to be “too big”, meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).\nAs far as I can tell from reading Yates’ paper14, the correction is basically a hack. It’s not derived from any principled theory. Rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the ‘Statistics’ options, where it is called ‘\\(\\chi^2\\) continuity correction’."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#effect-size",
    "href": "10-Categorical-data-analysis.html#effect-size",
    "title": "10  Categorical data analysis",
    "section": "10.4 Effect size",
    "text": "10.4 Effect size\nAs we discussed earlier in Section 9.8, it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?\nThere are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them but will instead focus on the most commonly reported measures of effect size.\nBy default, the two measures that people tend to report most frequently are the \\(\\phi\\) statistic and the somewhat superior version, known as Cramér’s \\(V\\) .\n[Additional technical detail 15]\nAnd you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly. With Cramér’s \\(V\\), you know that the value really does range from 0 (no association at all) to 1 (perfect association)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#assumptions-of-the-tests",
    "href": "10-Categorical-data-analysis.html#assumptions-of-the-tests",
    "title": "10  Categorical data analysis",
    "section": "10.5 Assumptions of the test(s)",
    "text": "10.5 Assumptions of the test(s)\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:\n\nExpected frequencies are sufficiently large. Remember how in the previous section we saw that the \\(\\chi^2\\) sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter 7 this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran (1954)) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative (Larntz, 1978).\nData are independent of one another. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that everyone selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic” (Tversky & Kahneman, 1974). This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.\n\nIf you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#the-fisher-exact-test",
    "href": "10-Categorical-data-analysis.html#the-fisher-exact-test",
    "title": "10  Categorical data analysis",
    "section": "10.6 The Fisher exact test",
    "text": "10.6 The Fisher exact test\nWhat should you do if your cell counts are too small, but you’d still like to test the null hypothesis that the two variables are independent? One answer would be “collect more data”, but that’s far too glib There are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, Fisher (1922) kindly provided the right answer to the question. To illustrate the basic idea let’s suppose that we’re analysing data from a field experiment looking at the emotional status of people who have been accused of Witchcraft, some of whom are currently being burned at the stake.16 Unfortunately for the scientist (but rather fortunately for the general populace), it’s actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. A contingency table of the salem.csv data illustrates the point (Table 10.9).\n\n\n\n\nTable 10.9:  Contingency table of the salem.csv data \n\nhappyFALSETRUE\n\non.fireFALSE310\n\nTRUE30\n\n\n\n\n\nLooking at this data, you’d be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. So, speaking as someone who doesn’t want to be set on fire, I’d really like to be able to get a better answer than this. This is where Fisher’s exact test (Fisher, 1922) comes in very handy.\nThe Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn’t have a test statistic, but it calculates the p-value “directly”. I’ll explain the basics of how the test works for a \\(2 \\times 2\\) contingency table. As before, let’s have some notation (Table 10.10).\n\n\n\n\nTable 10.10:  Notation for the Fisher exact test \n\nHappySadTotal\n\nSet on fire\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nNot set on fire\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)\\(N\\)\n\n\n\n\n\nIn order to construct the test Fisher treats both the row and column totals \\((R_1, R_2, C_1 \\text{ and } C_2)\\) as known, fixed quantities and then calculates the probability that we would have obtained the observed frequencies that we did \\((O_{11}, O_{12}, O_{21} \\text{ and } O_{22})\\) given those totals. In the notation that we developed in Chapter 7 this is written:\n\\[P(O_{11}, O_{12}, O_{21}, O_{22}  \\text{ | } R_1, R_2, C_1, C_2)\\] and as you might imagine, it’s a slightly tricky exercise to figure out what this probability is. But it turns out that this probability is described by a distribution known as the hypergeometric distribution. What we have to do to calculate our p-value is calculate the probability of observing this particular table or a table that is “more extreme”. 17 Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it’s pretty easy as long as the tables aren’t too big and the sample size isn’t too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more “extreme” than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the p-value.\nYou can specify this test in jamovi from a check box in the ‘Statistics’ options of the ‘Contingency Tables’ analysis. When you do this with the data from the salem.csv file, the Fisher exact test statistic is shown in the results. The main thing we’re interested in here is the p-value, which in this case is small enough (p = .036) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire. See Figure 10.7.\n\n\n\n\n\nFigure 10.7: Fisher exact test analysis in jamovi"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-The-McNemar-test",
    "href": "10-Categorical-data-analysis.html#sec-The-McNemar-test",
    "title": "10  類別資料分析",
    "section": "10.7 麥內瑪檢定",
    "text": "10.7 麥內瑪檢定\n想像一下你是澳大利亞全民黨(AGPP)的基層黨工19，被上級交辦要分析AGPP投放的政治宣傳有效程度，因此你找來總共\\(N = 100\\)有投票權的公民，請他們觀看AGPP的宣傳廣告。在播放宣傳內容之前，你先問他們是否打算投票給AGPP，播放廣告之後再問他們一次，看看有沒有人會改變主意。如果你是非常專業的幕僚，還會做更多事情，現在讓我們先看一下這個簡單實驗的結果，一種描述資料的方法就是建立如同 表 10.11 的列聯表。\n\n\n\n\n\n\n表 10.11: 測試AGPP宣傳廣告效果的列聯表\n\n\n\nBefore\nAfter\nTotal\n\n\nYes\n30\n10\n40\n\n\nNo\n70\n90\n160\n\n\nTotal\n100\n100\n200\n\n\n\n\n\n\n\n\n也許你可能會認為這種問題適合用皮爾森\\(\\chi^2\\)獨立性檢定處理，不過仔細想一下就會發現這樣做不切實際：雖然有100名參與者，全部資料卻有200個觀察值，這是因為每個人在”看廣告之前”和”看廣告之後”都有給出回應，也就是說這200個觀察值彼此之間並不獨立。如果選民A第一次說“是”，選民B說“否”，可以預期選民A第二次比選民B更可能說“是”！。因為違反了樣本獨立性的條件，\\(\\chi^2\\)檢定的結果就會非常不可靠。如果這是一個相當罕見的狀況，我不會特別編寫一個小單元來討論，但是這種狀況並不罕見。最尷尬的是，這個例子是一個標準的重複測量設計，已介紹至此的任何檢定方法都無法處理。\n能解決這種問題的檢定方法來自McNemar(1947)這篇論文，訣竅是先用稍微不同的方式整理列聯表，就像 表 10.12。\n\n\n\n\n\n表 10.12: 資料是重複測量所得的話，加上測量條件所整理的列聯表\n\n\n\nBefore: Yes\nBefore: No\nTotal\n\n\nAfter: Yes\n5(a)\n5(b)\n10\n\n\nAfter: No\n25(c)\n65(d)\n90\n\n\nTotal\n30\n70\n100\n\n\n\n\n\n\n\n\n接著重新設定這個問題的虛無假設：“看宣傳內容之前”及“看宣傳內容之後”，兩種條件的測試有相同比例的人會回應：“是的，我會投票支持AGPP。”因為列聯表已經重新整理，這代表我們假設各行總次數和各列總次數都是來自相同的樣本分佈，也就是說麥內瑪檢定的虛無假設代表樣本資料的邊際同質性(marginal homogeneity)：第一行總次數等於第一列總次數\\(P_a + P_b = P_a + P_c\\)，以及第二行總次數等於第二列總次數\\(P_c + P_d = P_b + P_d\\)。請注意，這表示虛無假設只要設定為\\(P_b = P_c\\)，所以設定麥內瑪檢定程序，重要的只有列聯表中的對角格內數值。了解這一點後，執行麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity)應注意的適用條件與卡方檢定完全一樣。經過葉氏校正後，統計值的公式就是：\n\\[\\chi^2=\\frac{(|b-c|-0.5)^2}{b+c}\\]\n或者用這個單元一開始使用的公式改造：\n\\[\\chi^2=\\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}\\]\n這個統計值逼近自由度為1(df = 1)的\\(\\chi^2\\)分佈（近似），自由度df = 1。不過務必記得，如同正規的\\(\\chi^2\\)檢定，這只是一個近似值，只有觀察次數夠大才不需要使用校正後的統計值。\n\n\n\n10.7.1 實作麥內瑪檢定\n現在您已經了解麥內瑪檢驗的所有內容，讓我們實際運行一個。agpp.csv 文件包含了我之前討論過的原始資料。agpp 資料集包含三個變項，一個id變項標記資料集中的每個參與者（我們將在片刻之後看到這有什麼用），一個response_before 變項記錄了當他們第一次被問到這個問題時的答案，以及一個response_after變項顯示他們在第二次被問到同樣問題時給出的答案。注意每個參與者在這個資料集中只出現一次。在jamovi中，轉到 ‘Analyses’ - ‘Frequencies’ - ‘Contingency Tables’ - ‘Paired Samples’ 分析，並將response_before 放入 ‘Rows’ 框，將response_after 放入 ‘Columns’ 框。然後，您將在結果窗口中獲得一個列聯表，麥內瑪檢驗的統計資料就在它下面，參見 圖 10.8 。\n\n\n\n\n\n\n\n圖 10.8: jamovi中的麥內瑪檢驗輸出\n\n\n\n\n我們完成了。我們剛剛運行了一個麥內瑪檢驗，以確定人們在廣告後是否和廣告前一樣有可能投票支持AGPP。檢驗是顯著的（\\(\\chi^2(1)= 12.03, p&lt; .001\\)），表明他們並非如此。事實上，看起來廣告產生了負面影響：人們在看過廣告後，投票支持AGPP的可能性更低。考慮到典型政治廣告的質量，這是很合理的。\n\n\n\n10.7.2 與獨立性檢定有可分別?\n讓我們回到本章的開頭，再次查看卡片資料集。如果您回憶一下，我描述的實際實驗設計涉及人們進行兩次選擇。因為我們有關於每個人第一次選擇和第二次選擇的信息，我們可以構建以下列聯表，用於將第一次選擇與第二次選擇交叉列聯（表 10.13）。\n\n\n\n\n\n\n表 10.13: 用Randomness.omv（卡片）資料交叉列聯第一次與第二次選擇\n\n\n\nBefore: Yes\nBefore: No\nTotal\n\n\nAfter: Yes\n\\(a \\)\n\\(b \\)\n\\(a + b \\)\n\n\nAfter: No\n\\(c \\)\n\\(d \\)\n\\(c + d \\)\n\n\nTotal\n\\(a+c \\)\n\\(b+d \\)\n\\(n \\)\n\n\n\n\n\n\n\n\n假設我想知道第二次選擇是否取決於第一次選擇。這是獨立性檢驗有用的地方，我們試圖做的是看看這個表格的行和列之間是否存在某種關係。\n另外，假設我想知道平均而言，第二次選擇的花色頻率是否與第一次選擇不同。在這種情況下，我真正想做的是看看行總數是否不同於列總數。這就是您使用麥內瑪檢驗的時候。\n這些不同分析產生的不同統計資料顯示在 圖 10.9 中。注意結果是不同的！這些檢驗並不相同。\n\n\n\n\n\n\n\n圖 10.9: Randomness.omv（卡片）資料中的獨立與成對（麥內瑪）"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#whats-the-difference-between-mcnemar-and-independence",
    "href": "10-Categorical-data-analysis.html#whats-the-difference-between-mcnemar-and-independence",
    "title": "10  Categorical data analysis",
    "section": "10.8 What’s the difference between McNemar and independence?",
    "text": "10.8 What’s the difference between McNemar and independence?\nLet’s go all the way back to the beginning of the chapter and look at the cards data set again. If you recall, the actual experimental design that I described involved people making two choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice (Table 10.13).\n\n\n\n\nTable 10.13:  Cross-tabulating first against second choice with the Randomness.omv (cards) data \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes\\(a \\)\\(b \\)\\(a + b \\)\n\nAfter: No\\(c  \\)\\(d  \\)\\(c + d  \\)\n\nTotal\\(a+c  \\)\\(b+d  \\)\\(n  \\)\n\n\n\n\n\nSuppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we’re trying to do is see if there’s some relationship between the rows and columns of this table.\nAlternatively, suppose I wanted to know if on average, the frequencies of suit choices were different the second time than the first time. In that situation, what I’m really trying to see is if the row totals are different from the column totals. That’s when you use the McNemar test.\nThe different statistics produced by these different analyses are shown in Figure 10.9. Notice that the results are different! These aren’t the same test.\n\n\n\n\n\nFigure 10.9: Independent vs. Paired (McNemar) with the Randomness.omv (cards) data"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#summary",
    "href": "10-Categorical-data-analysis.html#summary",
    "title": "10  類別資料分析",
    "section": "10.8 Summary",
    "text": "10.8 Summary\nThe key ideas discussed in this chapter are:\n\n[The \\(\\chi^2\\) (chi-square) goodness-of-fit test] is used when you have a table of observed frequencies of different categories, and the null hypothesis gives you a set of “known” probabilities to compare them to.\n[The \\(\\chi^2\\) test of independence (or association)] is used when you have a contingency table (cross-tabulation) of two categorical variables. The null hypothesis is that there is no relationship or association between the variables.\n[Effect size] for a contingency table can be measured in several ways. In particular we noted the Cramér’s \\(V\\) statistic.\nBoth versions of the Pearson test rely on two assumptions: that the expected frequencies are sufficiently large, and that the observations are independent ([Assumptions of the test(s)]. [The Fisher exact test] can be used when the expected frequencies are small. [The McNemar test] can be used for some kinds of violations of independence.\n\nIf you’re interested in learning more about categorical data analysis a good first choice would be Agresti (1996) which, as the title suggests, provides an Introduction to Categorical Data Analysis. If the introductory book isn’t enough for you (or can’t solve the problem you’re working on) you could consider Agresti (2002), Categorical Data Analysis. The latter is a more advanced text, so it’s probably not wise to jump straight from this book to that one.\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nCramer, H. (1946). Mathematical methods of statistics. Princeton University Press.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "11-Comparing-two-means.html",
    "href": "11-Comparing-two-means.html",
    "title": "11  比較兩組平均值",
    "section": "",
    "text": "In Chapter 10 we covered the situation when your outcome variable is nominal scale and your predictor variable is also nominal scale. Lots of real world situations have that character, and so you’ll find that chi-square tests in particular are quite widely used. However, you’re much more likely to find yourself in a situation where your outcome variable is interval scale or higher, and what you’re interested in is whether the average value of the outcome variable is higher in one group or another. For instance, a psychologist might want to know if anxiety levels are higher among parents than non-parents, or if working memory capacity is reduced by listening to music (relative to not listening to music). In a medical context we might want to know if a new drug increases or decreases blood pressure. An agricultural scientist might want to know whether adding phosphorus to Australian native plants will kill them.1 In all these situations our outcome variable is a fairly continuous, interval or ratio scale variable, and our predictor is a binary “grouping” variable. In other words, we want to compare the means of the two groups.\nThe standard answer to the problem of comparing means is to use a t-test, of which there are several varieties depending on exactly what question you want to solve. As a consequence, the majority of this chapter focuses on different types of t-test: one sample t-tests, independent samples t-tests and paired samples t-tests. We’ll then talk about one sided tests and, after that, we’ll talk a bit about Cohen’s d, which is the standard measure of effect size for a t-test. The later sections of the chapter focus on the assumptions of the t-tests, and possible remedies if they are violated. However, before discussing any of these useful things, we’ll start with a discussion of the z-test."
  },
  {
    "objectID": "11-Comparing-two-means.html#the-one-sample-z-test",
    "href": "11-Comparing-two-means.html#the-one-sample-z-test",
    "title": "11  Comparing two means",
    "section": "11.1 The one-sample z-test",
    "text": "11.1 The one-sample z-test\nIn this section I’ll describe one of the most useless tests in all of statistics: the z-test. Seriously – this test is almost never used in real life. Its only real purpose is that, when teaching statistics, it’s a very convenient stepping stone along the way towards the t-test, which is probably the most (over)used tool in all statistics.\n\n11.1.1 The inference problem that the test addresses\nTo introduce the idea behind the z-test, let’s use a simple example. A friend of mine, Dr Zeppo, grades his introductory statistics class on a curve. Let’s suppose that the average grade in his class is \\(67.5\\), and the standard deviation is \\(9.5\\). Of his many hundreds of students, it turns out that 20 of them also take psychology classes. Out of curiosity, I find myself wondering if the psychology students tend to get the same grades as everyone else (i.e., mean \\(67.5\\)) or do they tend to score higher or lower? He emails me the zeppo.csv file, which I use to look at the grades of those students, in the jamovi spreadsheet view,and then calculate the mean in ‘Exploration’ - ‘Descriptives’ 2. The mean value is \\(72.3\\).\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\nHmm. It might be that the psychology students are scoring a bit higher than normal. That sample mean of \\(\\bar{X} = 72.3\\) is a fair bit higher than the hypothesised population mean of \\(\\mu = 67.5\\) but, on the other hand, a sample size of \\(N = 20\\) isn’t all that big. Maybe it’s pure chance.\nTo answer the question, it helps to be able to write down what it is that I think I know. Firstly, I know that the sample mean is \\(\\bar{X} = 72.3\\). If I’m willing to assume that the psychology students have the same standard deviation as the rest of the class then I can say that the population standard deviation is \\(\\sigma = 9.5\\). I’ll also assume that since Dr Zeppo is grading to a curve, the psychology student grades are normally distributed.\nNext, it helps to be clear about what I want to learn from the data. In this case my research hypothesis relates to the population mean \\(\\mu\\) for the psychology student grades, which is unknown. Specifically, I want to know if \\(\\mu = 67.5\\) or not. Given that this is what I know, can we devise a hypothesis test to solve our problem? The data, along with the hypothesised distribution from which they are thought to arise, are shown in Figure 11.1. Not entirely obvious what the right answer is, is it? For this, we are going to need some statistics.\n\n\n\n\n\nFigure 11.1: The theoretical distribution (solid line) from which the psychology student grades (bars) are supposed to have been generated\n\n\n\n\n\n\n11.1.2 Constructing the hypothesis test\nThe first step in constructing a hypothesis test is to be clear about what the null and alternative hypotheses are. This isn’t too hard to do. Our null hypothesis, \\(H_0\\), is that the true population mean \\(\\mu\\) for psychology student grades is \\(67.5\\%\\), and our alternative hypothesis is that the population mean isn’t \\(67.5\\%\\). If we write this in mathematical notation, these hypotheses become:\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\nthough to be honest this notation doesn’t add much to our understanding of the problem, it’s just a compact way of writing down what we’re trying to learn from the data. The null hypotheses \\(H_0\\) and the alternative hypothesis \\(H_1\\) for our test are both illustrated in Figure 11.2. In addition to providing us with these hypotheses, the scenario outlined above provides us with a fair amount of background knowledge that might be useful. Specifically, there are two special pieces of information that we can add:\n\nThe psychology grades are normally distributed.\nThe true standard deviation of these scores \\(\\sigma\\) is known to be 9.5.\n\nFor the moment, we’ll act as if these are absolutely trustworthy facts. In real life, this kind of absolutely trustworthy background knowledge doesn’t exist, and so if we want to rely on these facts we’ll just have make the assumption that these things are true. However, since these assumptions may or may not be warranted, we might need to check them. For now though, we’ll keep things simple.\n\n\n\n\n\nFigure 11.2: Graphical illustration of the null and alternate hypotheses assumed by the one sample \\(z\\)-test (the two sided version, that is). The null and alternate hypotheses both assume that the population distribution is normal, and additionally assumes that the population standard deviation is known (fixed at some value \\(\\$sigma_0\\)). The null hypothesis (left) is that the population mean \\(\\mu\\) is equal to some specified value \\(\\mu_0\\). The alternative hypothesis (right) is that the population mean differs from this value, \\(\\mu \\neq \\mu_0\\)\n\n\n\n\nThe next step is to figure out what we would be a good choice for a diagnostic test statistic, something that would help us discriminate between \\(H_0\\) and \\(H_1\\). Given that the hypotheses all refer to the population mean \\(\\mu\\), you’d feel pretty confident that the sample mean \\(\\bar{X}\\) would be a pretty useful place to start. What we could do is look at the difference between the sample mean \\(\\bar{X}\\) and the value that the null hypothesis predicts for the population mean. In our example that would mean we calculate \\(\\bar{X} - 67.5\\). More generally, if we let \\(\\mu_0\\) refer to the value that the null hypothesis claims is our population mean, then we’d want to calculate\n\\[\\bar{X}-\\mu_0\\]\nIf this quantity equals or is very close to 0, things are looking good for the null hypothesis. If this quantity is a long way away from 0, then it’s looking less likely that the null hypothesis is worth retaining. But how far away from zero should it be for us to reject H0?\nTo figure that out we need to be a bit more sneaky, and we’ll need to rely on those two pieces of background knowledge that I wrote down previously; namely that the raw data are normally distributed and that we know the value of the population standard deviation \\(\\sigma\\). If the null hypothesis is actually true, and the true mean is \\(\\mu_0\\), then these facts together mean that we know the complete population distribution of the data: a normal distribution with mean \\(\\mu_0\\) and standard deviation \\(\\sigma\\).3\nOkay, if that’s true, then what can we say about the distribution of \\(\\bar{X}\\)? Well, as we discussed earlier (see Section 8.3.3), the sampling distribution of the mean \\(\\bar{X}\\) is also normal, and has mean \\(\\mu\\). But the standard deviation of this sampling distribution \\(\\\\{se(\\bar{X})\\\\}\\), which is called the standard error of the mean, is 4\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\nNow comes the trick. What we can do is convert the sample mean \\(\\bar{X}\\) into a standard score (see Section 4.5). This is conventionally written as z, but for now I’m going to refer to it as \\(z_{\\bar{X}}\\). The reason for using this expanded notation is to help you remember that we’re calculating a standardised version of a sample mean, not a standardised version of a single observation, which is what a z-score usually refers to). When we do so the z-score for our sample mean is\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\] or, equivalently \\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\nThis z-score is our test statistic. The nice thing about using this as our test statistic is that like all z-scores, it has a standard normal distribution:5\nIn other words, regardless of what scale the original data are on, the z-statistic itself always has the same interpretation: it’s equal to the number of standard errors that separate the observed sample mean \\(\\bar{X}\\) from the population mean \\(\\mu_0\\) predicted by the null hypothesis. Better yet, regardless of what the population parameters for the raw scores actually are, the 5% critical regions for the z-test are always the same, as illustrated in Figure 11.3. And what this meant, way back in the days where people did all their statistics by hand, is that someone could publish a table like Table 11.1. This, in turn, meant that researchers could calculate their z-statistic by hand and then look up the critical value in a text book.\n\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\nTable 11.1:  Critical values for different alpha levels \n\ncritical z value\n\ndesired \\(\\alpha\\) leveltwo-sided testone-sided test\n\n.11.6448541.281552\n\n.051.9599641.644854\n\n.012.5758292.326348\n\n.0013.2905273.090232\n\n\n\n\n\n\n\n11.1.3 A worked example, by hand\nNow, as I mentioned earlier, the z-test is almost never used in practice. It’s so rarely used in real life that the basic installation of jamovi doesn’t have a built in function for it. However, the test is so incredibly simple that it’s really easy to do one manually. Let’s go back to the data from Dr Zeppo’s class. Having loaded the grades data, the first thing I need to do is calculate the sample mean, which I’ve already done (\\(72.3\\)). We already have the known population standard deviation (\\(\\sigma = 9.5\\)), and the value of the population mean that the null hypothesis specifies (\\(\\mu_0 = 67.5\\)), and we know the sample size (\\(N=20\\)).\n\n\n\n\n\nFigure 11.3: Rejection regions for the two-sided z-test (panel (a)) and the one-sided z-test (panel (b))\n\n\n\n\nNext, let’s calculate the (true) standard error of the mean (easily done with a calculator):\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\nAnd finally, we calculate our z-score:\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\nAt this point, we would traditionally look up the value \\(2.26\\) in our table of critical values. Our original hypothesis was two-sided (we didn’t really have any theory about whether psych students would be better or worse at statistics than other students) so our hypothesis test is two-sided (or two-tailed) also. Looking at the little table that I showed earlier, we can see that \\(2.26\\) is bigger than the critical value of \\(1.96\\) that would be required to be significant at \\(\\alpha = .05\\), but smaller than the value of \\(2.58\\) that would be required to be significant at a level of \\(\\alpha = .01\\). Therefore, we can conclude that we have a significant effect, which we might write up by saying something like this:\n\nWith a mean grade of \\(73.2\\) in the sample of psychology students, and assuming a true population standard deviation of \\(9.5\\), we can conclude that the psychology students have significantly different statistics scores to the class average (\\(z = 2.26, N = 20, p<.05\\)).\n\n\n\n11.1.4 Assumptions of the z-test\nAs I’ve said before, all statistical tests make assumptions. Some tests make reasonable assumptions, while other tests do not. The test I’ve just described, the one sample z-test, makes three basic assumptions. These are:\n\nNormality. As usually described, the z-test assumes that the true population distribution is normal.6 This is often a pretty reasonable assumption, and it’s also an assumption that we can check if we feel worried about it (see Section on Checking the normality of a sample).\nIndependence. The second assumption of the test is that the observations in your data set are not correlated with each other, or related to each other in some funny way. This isn’t as easy to check statistically, it relies a bit on good experimental design. An obvious (and stupid) example of something that violates this assumption is a data set where you “copy” the same observation over and over again in your data file so that you end up with a massive “sample size”, which consists of only one genuine observation. More realistically, you have to ask yourself if it’s really plausible to imagine that each observation is a completely random sample from the population that you’re interested in. In practice this assumption is never met, but we try our best to design studies that minimise the problems of correlated data.\nKnown standard deviation. The third assumption of the z-test is that the true standard deviation of the population is known to the researcher. This is just stupid. In no real world data analysis problem do you know the standard deviation σ of some population but are completely ignorant about the mean \\(\\mu\\). In other words, this assumption is always wrong.\n\nIn view of the stupidity of assuming that \\(\\alpha\\) is known, let’s see if we can live without it. This takes us out of the dreary domain of the z-test, and into the magical kingdom of the t-test, with unicorns and fairies and leprechauns!"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-The-one-sample-t-test",
    "href": "11-Comparing-two-means.html#sec-The-one-sample-t-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.2 單一樣本t檢定",
    "text": "11.2 單一樣本t檢定\n經過深思熟慮，我（原作者）認為無法輕易假定Zeppo教授班上心理學系學生的成績標準差與其他科系學生相同。如果我們已預設不同科系學生的平均成績存在差異，那麼為何我們要假定他們的標準差相等呢？基於這個理由，我認為不應該預先假定自己知道真實的\\(\\sigma\\)是多少，這樣就違反了z檢定的前提條件。這麼一來，我們的分析工作似乎回到了原點。然而，我們還是有其他方法可用。最重要的是，原始資料依然是一樣的，這些資料提供了母群標準差的估計值，即9.52。換言之，雖然我不能假定\\(\\sigma = 9.5\\)，但我可以提出\\(\\hat{\\sigma} = 9.52\\)。\n這是一個不錯的開頭。你可能會很自然地想到，也許能使用樣本估計的標準差9.52來進行z檢定，而不是依賴於對\\(\\sigma = 9.5\\)的假定，這種方法依然能夠得到顯著的結果。這樣的檢定方法接近正確，但是並非完全無懈可擊。由於現在是用母群標準差的估計值計算統計值，因此需要調整一下面對真實母群標準差不確定性的看法：或許手頭上的資料僅是一次偶發事件……或許真實的母群標準差其實是\\(11\\)，如果這是真的，假定\\(\\sigma=11\\)進行z檢定，分析結果可能不會是顯著的。這是一個需要解決的問題。\n\n\n\n\n\n\n圖 11.4: 圖解單一樣本雙側t檢定的虛無假設和對立假設。留意t檢定和z檢定的相似之處（請見 圖 11.2 ），就是虛無假設的母群平均值\\(\\mu\\)等於某個特定值\\(\\mu_0\\)，對立假設不等於\\(\\mu_0\\)。與z檢定一樣，t檢定也有資料服從常態分佈的適用條件，但是不必事先已知母群標準差\\(\\sigma\\)。\n\n\n\n\n\n11.2.1 深入認識單一樣本t檢定\n這段歷史性的難題在 1908 年由 William Sealy Gosset 解決了。當時他在吉尼斯啤酒公司擔任專職化學分析師（見 Box (1987)），吉尼斯有規定禁止員工公開發表有關統計分析的研究，因為公司認為這屬於商業機密，因此Gosset 用了筆名“學生”發表他的研究成果。這是為什麼今天t檢定的正式名稱是「學生t檢定」(Student’s t-test)。Gosset 的關鍵貢獻是，他發現即使我們不完全確定母群標準差的確切數值，如何確保取樣分佈充分匹配真實資料，解決方法是適當調整取樣分佈。在t檢定中，我們所使用的檢定統計值是t統計值(t statistic)，如同之前z檢定的計算方法，我們假定虛無假設的真實平均值是\\(\\mu\\)，然而樣本平均值是\\(\\bar{X}\\)，根據母群標準差的估計值\\(\\hat{\\sigma}\\)，t統計值的計算公式就是：\n\n\\[\nt=\\frac{\\bar{X}-\\mu}{\\frac{\\hat{\\sigma}}{\\sqrt{N}}}\n\\]\n\n\n\n\n\n圖 11.5: 具有2個自由度（左）和10個自由度（右）的t分布，標準常態分佈（即，均值為0，標準差為1）以虛線繪製，以進行比較。請注意，t分布的尾部比常態分佈重（高峰度），當自由度非常小時，這種效應會被極大地夸大，但對於較大的值可以忽略不計。換句話說，對於大的自由度，t分布基本上與常態分佈相同。\n\n\n\n\nt統計值的公式與z統計值公式的唯一不同處是我們使用母群標準差的估計值 \\(\\hat{\\sigma}\\) ，取代真實的母群標準差\\(\\sigma\\)。如果這個估計值是由 N 個觀察值所構成，那麼樣本分佈就會符合一個自由度是\\(N-1\\)的 t 分佈。 t 分佈的長相和常態分佈非常類似，但是兩側密度高一點。這個特性在 小單元 8.6 曾經提到，也呈現在 圖 11.5 。不過，各位可注意到自由度越大，t 分佈會越來越和標準常態分佈一致，而這就是許多研究者希望的，若是樣本量有\\(N = 70,000,000\\)，母群標準差的「估計值」就幾乎等於真實的數值了？所以我們可以期待分析大樣本資料，使用t 檢定的結果完全等於z檢定，用數學推導來看也是如此！\n\n\n\n11.2.2 實作單一樣本t檢定\n正如你所預期的，t檢定的機制幾乎與z檢定的機制完全相同。因此，沒有必要繁瑣地展示如何使用低級命令進行計算。它與我們之前所做的計算幾乎相同，只是使用了估計的標準差，然後使用t分佈而不是常態分佈來測試假設。因此，我將直接跳到顯示如何實際進行t檢定。jamovi配備了一個專門用於t檢定的分析，非常靈活（它可以運行許多不同類型的t檢定）。它非常簡單易用；您需要做的只是指定“分析”-“T檢定”-“單樣本T檢定”，將您感興趣的變量（X）移動到“變量”框中，並在“假設”-“測試值”框中輸入零假設的平均值（‘67.5’）。很容易。見@fig-fig11-6，除了我們稍後會提到的其他事項之外，它還提供了t檢定統計量=2.25，自由度為19，相關的p值為\\(0.036\\)。\n\n\n\n\n\n\n圖 11.6: jamovi執行單一樣本t檢定示範\n\n\n\n\n此外，報告了其他兩個可能會引起關注的指標：95％置信區間和效應大小的測量值（我們稍後會談論效應大小）。看起來相當簡單。現在我們該如何處理這個輸出呢？既然我們假裝真的在乎我們的例子，我們很高興地發現結果在統計上是顯著的（即p值低於0.05）。我們可以通過這樣的報告方式來表達結果：\n\n心理學生的平均成績為 \\(72.3\\)，略高於平均成績\\(67.5\\) (\\(t(19) = 2.25\\)，\\(p = .036\\))。平均分差為\\(4.80\\)，\\(95\\%\\)置信區間為\\(0.34\\)至\\(9.26\\)。\n\n…其中 \\(t(19)\\) 是一個簡寫符號，代表具有 \\(19\\) 自由度的 t 統計量。話雖如此，人們通常不會報告信賴區間，或者使用比我在此展示的更簡短的形式進行報告。例如，看到信賴區間被包含在報告均值差之後的統計資訊中也不是很少見，就像這樣：\n\\[t(19)=2.25, p = .036, CI_{95} = [0.34, 9.26]\\]\n在這半行內塞這麼多專業術語，你就知道這必定是非常聰明的。7\n\n\n\n11.2.3 單一樣本t檢定的適用條件\n好的，那麼單樣本 t 檢驗有什麼假設呢？因為 t 檢驗基本上就是一個移除了已知標準差假設的 z 檢驗，你不應該感到意外，看到它做出了與 z 檢驗相同的假設，減去了已知標準差的假設，那就是：\n\n正態性。我們仍然假設母群分布是正態的 8，如前所述，有標準的工具可以用來檢查這種假設是否符合（檢查樣本正態性），如果這種假設被違反，還有其他的測試方法可以替代（檢驗非正態資料）。\n獨立性。再次，我們必須假設樣本中的觀察值是相互獨立的產生的。詳細討論請參見之前有關 z 檢驗的討論（z 檢驗的假設）。\n\n總的來說，這兩個假設並不是非常不合理的，因此單樣本 t 檢定在實踐中被廣泛用於比較樣本平均數與假定的母群平均數。"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-student-test",
    "href": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-student-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.3 獨立樣本t檢定(學生t檢定)",
    "text": "11.3 獨立樣本t檢定(學生t檢定)\n儘管單樣本 t 檢驗有其用處，但它並不是 t 檢驗的最典型範例。9更常見的情況是當你有兩個不同的觀測組時。在心理學中，這往往對應於研究中的兩個不同條件，每個條件對應於一個不同的參與者組。對於研究中的每個人，您測量一些感興趣的結果變量，而您要問的研究問題是這兩組是否具有相同的母群平均值。這就是獨立樣本 t 檢驗所設計的情況。\n\n\n11.3.1 使用獨立t檢定的狀況\n假設我們有 33 名學生參加 Harpo 博士的統計學講座，而 Harpo 博士沒有按曲線分數。實際上，Harpo 博士的評分有點神秘，因此我們對整個課程的平均成績一無所知。該課程有兩名導師，Anastasia 和 Bernadette。Anastasia 的輔導課有 \\(N_1=15\\) 名學生，Bernadette 的輔導課有 \\(N_2=18\\) 名學生。我感興趣的研究問題是 Anastasia 和 Bernadette 誰是更好的導師，或者他們的教學效果沒有太大的差異。Harpo 博士將課程成績發送給我，存在 harpo.csv 文件中。像往常一樣，我會將文件加載到 jamovi 中，查看它包含哪些變量 - 有三個變量，ID、grade 和 tutor。grade 變量包含每個學生的成績，但它未按正確的測量層級屬性導入到 jamovi 中，因此我需要更改它以使其被視為連續變量（參見 小單元 3.6）。tutor 變量是一個因子，表示每個學生的導師是 Anastasia 還是 Bernadette。\n我們可以使用 ‘Exploration’ - ‘descriptives’ 分析計算平均值和標準差，這裡是一個不錯的摘要表格（表 11.2）。\n\n\n\n\n\n\n表 11.2: 兩位統計助教帶的學生成績描述統計\n\n\n\nmean\nstd dev\nN\n\n\nAnastasia's students\n74.53\n9.00\n15\n\n\nBernadette's students\n69.06\n5.77\n18\n\n\n\n\n\n\n\n\n為了讓你更清楚知道正在發生什麼，我在 jamovi 中繪製了盒形圖和小提琴圖，並在圖表中添加了平均分數的小實心方塊。這些圖表顯示了兩位助教的學生成績分佈 (圖 11.7)。\n\n\n\n11.3.2 深入認識獨立t檢定\n獨立樣本 t 檢定有兩種不同的形式，學生 t 檢定和韋爾奇 t 檢定。原始的學生 t 檢定是兩者中比較簡單的一種，但比韋爾奇 t 檢定依賴更為嚴格的假設。假設你現在要進行雙側檢定，目標是確定兩個「獨立樣本」是否來自具有相同平均值（虛無假設）或不同平均值（對立假設）的母群。當我們說「獨立樣本」時，我們真正意思是這兩個樣本之間沒有特殊的關係。現在可能還不太明白，但等到我們談到成對樣本 t 檢定時，這一點就會更清楚了。現在，讓我們指出，如果我們有一個實驗設計，其中參與者是隨機分配到兩組中的一組，而我們想比較這兩組對某個結果度量的平均表現，那麼獨立樣本 t 檢定（而不是成對樣本 t 檢定）就是我們所需要的。\n\n\n\n\n\n\n圖 11.7: 盒形圖和小提琴圖顯示Anastasia和Bernadette的課程學生成績分布。從視覺上看，這些圖表暗示著Anastasia的學生平均成績可能會更好，但它們也似乎更加變化不定。\n\n\n\n\n好的，讓我們讓 \\(\\mu_1\\) 代表第一組的真實母群平均值（例如 Anastasia 的學生），而 \\(\\mu_2\\) 則是第二組的真實母群平均值（例如 Bernadette 的學生）10，通常我們讓 \\(\\bar{X_1}\\) 和 \\(\\bar{X_2}\\) 代表這兩個組別的觀察樣本平均值。我們的虛無假設是兩個母群平均值是相同的（\\(\\mu_1 = \\mu_2\\)），而對立假設則是它們不相同（\\(\\mu_1 \\neq \\mu_2\\)）（圖 11.8）。以數學術語來說，這是：\n\n\n\n\n\n\n圖 11.8: 學生t檢定所假設的零假設和替代假設的圖形示意。零假設假設兩組的平均值 \\(\\mu\\) 相同，而替代假設則假設兩組的平均值 \\(\\mu_1\\) 和 \\(\\mu_2\\) 不同。請注意，假設母群分配為正態分配，且儘管替代假設允許兩組平均值不同，但假設它們具有相同的標準差。\n\n\n\n\n建立一個能夠處理這種情況的假設檢驗，我們開始注意到，如果零假設成立，那麼兩個母群平均數之間的差異就是 正好 為零，\\(\\mu_1-\\mu_2 = 0\\)。因此，診斷性的檢定統計量將基於兩個樣本平均數之間的差異。因為如果零假設成立，那麼我們預期 \\(\\bar{X}_1 - \\bar{X}_2\\) 要非常接近於零。然而，就像我們在單樣本測試中看到的那樣（即單樣本 z 檢定和單樣本 t 檢定），我們必須精確地確定這種差異應該接近於零。解決問題的方法幾乎相同。我們計算一個標準誤差估計值（SE），然後將平均值之間的差異除以這個估計值。因此，我們的 t 檢定統計量 將具有以下形式：\n\\[t=\\frac{\\bar{X_1}-\\bar{X_2}}{SE}\\]\n我們需要找出這個標準誤差估計值是什麼。這比我們之前介紹過的兩個檢定複雜得多，因此我們需要更仔細地研究它的運作方式。\n\n\n\n11.3.3 標準差的合併估計\n在原始的「Student t 檢定」中，我們假設兩個群體有相同的母群標準差。換句話說，無論母群平均數是否相同，我們假設母群標準差是相同的，\\(\\sigma_1 = \\sigma_2\\)。由於我們假設兩個標準差相同，因此我們省略下標，將它們都稱為 \\(\\sigma\\)。我們該如何估計這個值？當我們有兩個樣本時，該如何建構標準差的單一估計值？答案是，基本上我們將它們加以平均。好吧，有點。實際上，我們取一個加權平均的變異數估計值，它被用作我們的合併變異數的估計值。分配給每個樣本的權重等於該樣本中的觀察數量減去 1。\n[其他技術細節11]"
  },
  {
    "objectID": "11-Comparing-two-means.html#completing-the-test",
    "href": "11-Comparing-two-means.html#completing-the-test",
    "title": "11  Comparing two means",
    "section": "11.4 Completing the test",
    "text": "11.4 Completing the test\nRegardless of which way you want to think about it, we now have our pooled estimate of the standard deviation. From now on, I’ll drop the silly p subscript, and just refer to this estimate as \\(\\hat{\\sigma}\\). Great. Let’s now go back to thinking about the bloody hypothesis test, shall we? Our whole reason for calculating this pooled estimate was that we knew it would be helpful when calculating our standard error estimate. But standard error of what? In the one-sample t-test it was the standard error of the sample mean, \\(se(\\bar{X})\\), and since \\(se(\\bar{X}) = \\frac{\\sigma}{\\sqrt{N}}\\) that’s what the denominator of our t-statistic looked like. This time around, however, we have two sample means. And what we’re interested in, specifically, is the the difference between the two \\(\\bar{X}_1-\\bar{X}_2\\) As a consequence, the standard error that we need to divide by is in fact the standard error of the difference between means.\n[Additional technical detail 13]\nJust as we saw with our one-sample test, the sampling distribution of this t-statistic is a t-distribution (shocking, isn’t it?) as long as the null hypothesis is true and all of the assumptions of the test are met. The degrees of freedom, however, is slightly different. As usual, we can think of the degrees of freedom to be equal to the number of data points minus the number of constraints. In this case, we have N observations (\\(N_1\\) in sample 1, and \\(N_2\\) in sample 2), and 2 constraints (the sample means). So the total degrees of freedom for this test are \\(N - 2\\).\n\n11.4.1 Doing the test in jamovi\nNot surprisingly, you can run an independent samples t-test easily in jamovi. The outcome variable for our test is the student grade, and the groups are defined in terms of the tutor for each class. So you probably won’t be too surprised that all you have to do in jamovi is go to the relevant analysis (‘Analyses’ - ‘T-Tests’ - ‘Independent Samples T-Test’) and move the grade variable across to the ‘Dependent Variables’ box, and the tutor variable across into the ‘Grouping Variable’ box, as shown in Figure 11.10.\n\n\n\n\n\nFigure 11.10: Independent t-test in jamovi, with options checked for useful results\n\n\n\n\nThe output has a very familiar form. First, it tells you what test was run, and it tells you the name of the dependent variable that you used. It then reports the test results. Just like last time the test results consist of a t-statistic, the degrees of freedom, and the p-value. The final section reports two things: it gives you a confidence interval and an effect size. I’ll talk about effect sizes later. The confidence interval, however, I should talk about now.\nIt’s pretty important to be clear on what this confidence interval actually refers to. It is a confidence interval for the difference between the group means. In our example, Anastasia’s students had an average grade of \\(74.53\\), and Bernadette’s students had an average grade of \\(69.06\\), so the difference between the two sample means is \\(5.48\\). But of course the difference between population means might be bigger or smaller than this. The confidence interval reported in Figure 11.10 tells you that there’s a if we replicated this study again and again, then \\(95\\%\\) of the time the true difference in means would lie between \\(0.20\\) and \\(10.76\\). Look back at Section 8.5 for a reminder about what confidence intervals mean.\nIn any case, the difference between the two groups is significant (just barely), so we might write up the result using text like this:\n\nThe mean grade in Anastasia’s class was \\(74.5\\%\\) (std dev = \\(9.0\\)), whereas the mean in Bernadette’s class was \\(69.1\\%\\) (std dev = \\(5.8\\)). A Student’s independent samples t-test showed that this \\(5.4\\%\\) difference was significant \\((t(31) = 2.1, p<.05, CI_{95} = [0.2, 10.8], d = .74)\\), suggesting that a genuine difference in learning outcomes has occurred.\n\nNotice that I’ve included the confidence interval and the effect size in the stat block. People don’t always do this. At a bare minimum, you’d expect to see the t-statistic, the degrees of freedom and the p value. So you should include something like this at a minimum: \\(t(31) = 2.1, p< .05\\). If statisticians had their way, everyone would also report the confidence interval and probably the effect size measure too, because they are useful things to know. But real life doesn’t always work the way statisticians want it to so you should make a judgement based on whether you think it will help your readers and, if you’re writing a scientific paper, the editorial standard for the journal in question. Some journals expect you to report effect sizes, others don’t. Within some scientific communities it is standard practice to report confidence intervals, in others it is not. You’ll need to figure out what your audience expects. But, just for the sake of clarity, if you’re taking my class, my default position is that it’s usually worth including both the effect size and the confidence interval.\n\n\n11.4.2 Positive and negative t values\nBefore moving on to talk about the assumptions of the t-test, there’s one additional point I want to make about the use of t-tests in practice. The first one relates to the sign of the t-statistic (that is, whether it is a positive number or a negative one). One very common worry that students have when they start running their first t-test is that they often end up with negative values for the t-statistic and don’t know how to interpret it. In fact, it’s not at all uncommon for two people working independently to end up with results that are almost identical, except that one person has a negative t values and the other one has a positive t value. Assuming that you’re running a two-sided test then the p-values will be identical. On closer inspection, the students will notice that the confidence intervals also have the opposite signs. This is perfectly okay. Whenever this happens, what you’ll find is that the two versions of the results arise from slightly different ways of running the t-test. What’s happening here is very simple. The t-statistic that we calculate here is always of the form\n\\[t=\\frac{\\text{mean 1-mean 2}}{SE}\\]\nIf “mean 1” is larger than “mean 2” the t statistic will be positive, whereas if “mean 2” is larger then the t statistic will be negative. Similarly, the confidence interval that jamovi reports is the confidence interval for the difference “(mean 1) minus (mean 2)”, which will be the reverse of what you’d get if you were calculating the confidence interval for the difference “(mean 2) minus (mean 1)”.\nOkay, that’s pretty straightforward when you think about it, but now consider our t-test comparing Anastasia’s class to Bernadette’s class. Which one should we call “mean 1” and which one should we call “mean 2”. It’s arbitrary. However, you really do need to designate one of them as “mean 1” and the other one as “mean 2”. Not surprisingly, the way that jamovi handles this is also pretty arbitrary. In earlier versions of the book I used to try to explain it, but after a while I gave up, because it’s not really all that important and to be honest I can never remember myself. Whenever I get a significant t-test result, and I want to figure out which mean is the larger one, I don’t try to figure it out by looking at the t-statistic. Why would I bother doing that? It’s foolish. It’s easier just to look at the actual group means since the jamovi output actually shows them!\nHere’s the important thing. Because it really doesn’t matter what jamovi shows you, I usually try to report the t-statistic in such a way that the numbers match up with the text. Suppose that what I want to write in my report is: Anastasia’s class had higher grades than Bernadette’s class. The phrasing here implies that Anastasia’s group comes first, so it makes sense to report the t-statistic as if Anastasia’s class corresponded to group 1. If so, I would write Anastasia’s class had higher grades than Bernadette’s class \\((t(31) = 2.1, p = .04)\\).\n(I wouldn’t actually underline the word “higher” in real life, I’m just doing it to emphasise the point that “higher” corresponds to positive t values). On the other hand, suppose the phrasing I wanted to use has Bernadette’s class listed first. If so, it makes more sense to treat her class as group 1, and if so, the write up looks like this: Bernadette’s class had lower grades than Anastasia’s class \\((t(31) = -2.1, p = .04)\\).\nBecause I’m talking about one group having “lower” scores this time around, it is more sensible to use the negative form of the t-statistic. It just makes it read more cleanly.\nOne last thing: please note that you can’t do this for other types of test statistics. It works for t-tests, but it wouldn’t be meaningful for chi-square tests, F-tests or indeed for most of the tests I talk about in this book. So don’t over-generalise this advice! I’m really just talking about t-tests here and nothing else!\n\n\n11.4.3 Assumptions of the test\nAs always, our hypothesis test relies on some assumptions. So what are they? For the Student t-test there are three assumptions, some of which we saw previously in the context of the one sample t-test (see Assumptions of the one sample t-test):\n\nNormality. Like the one-sample t-test, it is assumed that the data are normally distributed. Specifically, we assume that both groups are normally distributed14. In the section on Checking the normality of a sample we’ll discuss how to test for normality, and in Testing non-normal data we’ll discuss possible solutions.\nIndependence. Once again, it is assumed that the observations are independently sampled. In the context of the Student test this has two aspects to it. Firstly, we assume that the observations within each sample are independent of one another (exactly the same as for the one-sample test). However, we also assume that there are no cross-sample dependencies. If, for instance, it turns out that you included some participants in both experimental conditions of your study (e.g., by accidentally allowing the same person to sign up to different conditions), then there are some cross sample dependencies that you’d need to take into account.\nHomogeneity of variance (also called “homoscedasticity”). The third assumption is that the population standard deviation is the same in both groups. You can test this assumption using the Levene test, which I’ll talk about later on in the book (in Section 13.6.1). However, there’s a very simple remedy for this assumption if you are worried, which I’ll talk about in the next section."
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-welch-test",
    "href": "11-Comparing-two-means.html#sec-the-independent-samples-t-test-welch-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.5 獨立樣本t檢定(Welch t檢定)",
    "text": "11.5 獨立樣本t檢定(Welch t檢定)\n在實際應用中，使用學生 t 檢定的最大問題是前一節所列的第三個假設。它假設兩組具有相同的標準差，但在現實生活中，這種情況很少發生。如果兩個樣本的平均值不同，為什麼我們期望它們具有相同的標準差呢？實際上，我們沒有理由期望這個假設成立。我們稍後會談到如何檢查這個假設，因為它在幾個不同的地方會出現，不僅僅是 t 檢定。但現在我會談論一下不依賴這個假設的 t 檢定 (Welch, 1947) 的不同形式。圖 11.10 以圖形方式說明了 Welch t 檢定對資料的假設，以與 圖 11.8 中的學生 t 檢定進行對比。我承認在談診斷之前談治療有點奇怪，但事實上 Welch 檢定可以在 jamovi 的「獨立樣本 t 檢定」選項中指定，所以這可能是談論它的最好地方。\n\n\n\n\n\n\n圖 11.10: 圖形說明了 Welch t 檢定所假設的虛無假說和替代假說。和 Student t-test（見圖@fig-fig11-9）一樣，我們假設兩個樣本都是從一個常態分佈的母群中抽取的; 但是，替代假說不再需要兩個母群具有相等的變異數。\n\n\n\n\nWelch 檢定與 Student 檢定非常相似。例如，在 Welch 檢定中，我們使用的 t 統計量的計算方式與 Student 檢定非常相似。也就是說，我們取樣本平均數之間的差異，然後除以其標準誤的估計：\n\\[t=\\frac{\\bar{X}_1-\\bar{X}_2}{SE(\\bar{X}_1-\\bar{X}_2)}\\]\n主要的差異在於標準誤的計算方式不同。如果兩個母群具有不同的標準差，那麼嘗試計算合併的標準差估計是完全荒謬的，因為你正在平均蘋果和橘子。14\n[其他技術細節 15]\n\\[SE(\\bar{X}_1-\\bar{X}_2)=\\sqrt{\\frac{\\hat{\\sigma}_1^2}{N_1}+\\frac{\\hat{\\sigma}_2^2}{N_2}}\\]\n為什麼是這樣計算的原因超出了本書的範圍。對我們而言重要的是，從 Welch t-test 得出的 t 統計量實際上與從 Student t-test 得出的 t 統計量略有不同。\nWelch 與 Student 之間的第二個差異在於，計算自由度的方式非常不同。在 Welch 測試中，「自由度」不再必須是一個整數，並且它不太符合「資料點數量減去約束數量」的啟發式方法，這是我到目前為止一直在使用的。\n\n\n11.5.1 jamovi實作\n如果你在上面的分析中勾選了 Welch test 的核取方塊，則會得到以下結果（見@fig-fig11-11）。\n\n\n\n\n\n\n圖 11.11: jamovi並列學生t檢定與Welch t檢定結果。\n\n\n\n\n這個輸出結果的解釋應該很明顯。您可以像讀取學生t檢驗的結果一樣讀取Welch檢驗的輸出。你有你的描述統計資料，測試結果和其他信息。所以這一切都相當容易。\n除了，除了… 我們的結果不再顯著。 當我們執行Student t-test時，我們得到了一個顯著的效應，但在相同的資料集上進行的Welch測試並不顯著 \\((t(23.02) = 2.03, p = .054)\\)。 這是什麼意思？ 我們應該恐慌嗎？ 天空是否燃燒？ 可能不是。 一個測試顯著，而另一個測試不顯著本身並沒有什麼意義，特別是因為我有點編造了這些資料。 通常，不要試圖解釋或解釋\\(.049\\)和\\(.051\\)之間的p值差異。 如果這種情況在現實生活中發生，這些p值之間的差異幾乎肯定是由於偶然性引起的。 重要的是，您在選擇使用哪種測試時要仔細思考。 Student測試和Welch測試各有優勢和劣勢。 如果兩個母群確實具有相等的變異數，則Student測試比Welch測試略具備更高的功效（較低的II型錯誤率）。 但是，如果它們的變異數不同，則Student測試的假設就會被違反，您可能無法信任它。 您可能會得到更高的I型錯誤率。 因此，這是一個權衡。 但是，在現實生活中，我通常更喜歡使用Welch測試，因為幾乎沒有人認為母群變異數是相同的。\n\n\n\n11.5.2 Welch檢定的適用條件\nWelch檢定的假設與學生t檢定的假設非常相似（請參閱獨立樣本平均數檢定的適用條件），唯一不同的是Welch檢定不需要假定變異數相同。這只剩下正態性和獨立性的假設，這些假設的細節對於Welch檢定和學生t檢定來說是相同的。"
  },
  {
    "objectID": "11-Comparing-two-means.html#the-paired-samples-t-test",
    "href": "11-Comparing-two-means.html#the-paired-samples-t-test",
    "title": "11  Comparing two means",
    "section": "11.6 The paired-samples t-test",
    "text": "11.6 The paired-samples t-test\nRegardless of whether we’re talking about the Student test or the Welch test, an independent samples t-test is intended to be used in a situation where you have two samples that are, well, independent of one another. This situation arises naturally when participants are assigned randomly to one of two experimental conditions, but it provides a very poor approximation to other sorts of research designs. In particular, a repeated measures design, in which each participant is measured (with respect to the same outcome variable) in both experimental conditions, is not suited for analysis using independent samples t-tests. For example, we might be interested in whether listening to music reduces people’s working memory capacity. To that end, we could measure each person’s working memory capacity in two conditions: with music, and without music. In an experimental design such as this one, 17 each participant appears in both groups. This requires us to approach the problem in a different way, by using the paired samples t-test.\n\n11.6.1 The data\nThe data set that we’ll use this time comes from Dr Chico’s class.18 In her class students take two major tests, one early in the semester and one later in the semester. To hear her tell it, she runs a very hard class, one that most students find very challenging. But she argues that by setting hard assessments students are encouraged to work harder. Her theory is that the first test is a bit of a “wake up call” for students. When they realise how hard her class really is, they’ll work harder for the second test and get a better mark. Is she right? To test this, let’s import the chico.csv file into jamovi. This time jamovi does a good job during the import of attributing measurement levels correctly. The chico data set contains three variables: an id variable that identifies each student in the class, the grade_test1 variable that records the student grade for the first test, and the grade_test2 variable that has the grades for the second test.\nIf we look at the jamovi spreadsheet it does seem like the class is a hard one (most grades are between 50% and 60%), but it does look like there’s an improvement from the first test to the second one.\nIf we take a quick look at the descriptive statistics, in Figure 11.13, we see that this impression seems to be supported. Across all 20 students the mean grade for the first test is 57%, but this rises to 58% for the second test. Although, given that the standard deviations are 6.6% and 6.4% respectively, it’s starting to feel like maybe the improvement is just illusory; maybe just random variation. This impression is reinforced when you see the means and confidence intervals plotted in Figure 11.14 (a). If we were to rely on this plot alone, looking at how wide those confidence intervals are, we’d be tempted to think that the apparent improvement in student performance is pure chance.\n\n\n\n\n\nFigure 11.13: Descriptives for the two grade test variables in the chico data set\n\n\n\n\nNevertheless, this impression is wrong. To see why, take a look at the scatterplot of the grades for test 1 against the grades for test 2, shown in Figure 11.14 (b). In this plot each dot corresponds to the two grades for a given student. If their grade for test 1 (x co-ordinate) equals their grade for test 2 (y co-ordinate), then the dot falls on the line. Points falling above the line are the students that performed better on the second test. Critically, almost all of the data points fall above the diagonal line: almost all of the students do seem to have improved their grade, if only by a small amount. This suggests that we should be looking at the improvement made by each student from one test to the next and treating that as our raw data. To do this, we’ll need to create a new variable for the improvement that each student makes, and add it to the chico data set. The easiest way to do this is to compute a new variable, with the expression grade test2 - grade test1.\n\n\n\n\n\nFigure 11.14: Mean grade for test 1 and test 2, with associated 95% confidence intervals (panel (a)). Scatterplot showing the individual grades for test 1 and test 2 (panel (b)). Histogram showing the improvement made by each student in Dr Chico’s class (panel (c)). In panel (c), notice that almost the entire distribution is above zero - the vast majority of students did improve their performance from the first test to the second one\n\n\n\n\nOnce we have computed this new improvement variable we can draw a histogram showing the distribution of these improvement scores, shown in Figure 11.14 (c). When we look at the histogram, it’s very clear that there is a real improvement here. The vast majority of the students scored higher on test 2 than on test 1, reflected in the fact that almost the entire histogram is above zero.\n\n\n11.6.2 What is the paired samples t-test?\nIn light of the previous exploration, let’s think about how to construct an appropriate t test. One possibility would be to try to run an independent samples t-test using grade_test1 and grade_test2 as the variables of interest. However, this is clearly the wrong thing to do as the independent samples t-test assumes that there is no particular relationship between the two samples. Yet clearly that’s not true in this case because of the repeated measures structure in the data. To use the language that I introduced in the last section, if we were to try to do an independent samples t-test, we would be conflating the within subject differences (which is what we’re interested in testing) with the between subject variability (which we are not).\nThe solution to the problem is obvious, I hope, since we already did all the hard work in the previous section. Instead of running an independent samples t-test on grade_test1 and grade_test2, we run a one-sample t-test on the within-subject difference variable, improvement. To formalise this slightly, if \\(X_{i1}\\) is the score that the i-th participant obtained on the first variable, and \\(X_{i2}\\) is the score that the same person obtained on the second one, then the difference score is:\n\\[D_i=X_{i1}-X_{i2}\\]\nNotice that the difference scores is variable 1 minus variable 2 and not the other way around, so if we want improvement to correspond to a positive valued difference, we actually want “test 2” to be our “variable 1”. Equally, we would say that \\(\\mu_D = \\mu_1 - \\mu_2\\) is the population mean for this difference variable. So, to convert this to a hypothesis test, our null hypothesis is that this mean difference is zero and the alternative hypothesis is that it is not\n\\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]\nThis is assuming we’re talking about a two-sided test here. This is more or less identical to the way we described the hypotheses for the one-sample t-test. The only difference is that the specific value that the null hypothesis predicts is 0. And so our t-statistic is defined in more or less the same way too. If we let \\(\\bar{D}\\) denote the mean of the difference scores, then\n\\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\] which is \\[t=\\frac{\\bar{D}}{\\frac{\\hat{\\sigma}_D}{\\sqrt{N}}}\\]\nwhere \\(\\hat{\\sigma}_D\\) is the standard deviation of the difference scores. Since this is just an ordinary, one-sample t-test, with nothing special about it, the degrees of freedom are still \\(N - 1\\). And that’s it. The paired samples t-test really isn’t a new test at all. It’s a one-sample t-test, but applied to the difference between two variables. It’s actually very simple. The only reason it merits a discussion as long as the one we’ve just gone through is that you need to be able to recognise when a paired samples test is appropriate, and to understand why it’s better than an independent samples t test.\n\n\n11.6.3 Doing the test in jamovi\nHow do you do a paired samples t-test in jamovi? One possibility is to follow the process I outlined above. That is, create a “difference” variable and then run a one sample t-test on that. Since we’ve already created a variable called improvement, let’s do that and see what we get, Figure 11.15.\n\n\n\n\n\nFigure 11.15: Results showing a one sample t-test on paired difference scores\n\n\n\n\nThe output shown in Figure 11.15 is (obviously) formatted exactly the same was as it was the last time we used the one-sample t-Test analysis (Section 11.2), and it confirms our intuition. There’s an average improvement of \\(1.4\\%\\) from test 1 to test 2, and this is significantly different from \\(0\\) \\((t(19) = 6.48, p< .001)\\).\nHowever, suppose you’re lazy and you don’t want to go to all the effort of creating a new variable. Or perhaps you just want to keep the difference between one-sample and paired samples tests clear in your head. If so, you can use the jamovi ‘Paired Samples T-Test’ analysis, getting the results shown in Figure 11.16.\n\n\n\n\n\nFigure 11.16: Results showing a paired sample t-test. Compare with Figure 11.15\n\n\n\n\nThe numbers are identical to those that come from the one sample test, which of course they have to be given that the paired samples t-test is just a one sample test under the hood."
  },
  {
    "objectID": "11-Comparing-two-means.html#one-sided-tests",
    "href": "11-Comparing-two-means.html#one-sided-tests",
    "title": "11  Comparing two means",
    "section": "11.7 One-sided tests",
    "text": "11.7 One-sided tests\nWhen introducing the theory of null hypothesis tests, I mentioned that there are some situations when it’s appropriate to specify a one-sided test (see Section 9.4.3). So far all of the t-tests have been two-sided tests. For instance, when we specified a one sample t-test for the grades in Dr Zeppo’s class the null hypothesis was that the true mean was \\(67.5\\%\\). The alternative hypothesis was that the true mean was greater than or less than \\(67.5\\%\\). Suppose we were only interested in finding out if the true mean is greater than \\(67.5\\%\\), and have no interest whatsoever in testing to find out if the true mean is lower than \\(67.5\\%\\). If so, our null hypothesis would be that the true mean is \\(67.5\\%\\) or less, and the alternative hypothesis would be that the true mean is greater than \\(67.5\\%\\). In jamovi, for the ‘One Sample T-Test’ analysis, you can specify this by clicking on the ‘\\(>\\) Test Value’ option, under ‘Hypothesis’. When you have done this, you will get the results as shown in Figure 11.17.\n\n\n\n\n\nFigure 11.17: jamovi results showing a ‘One Sample T-Test’ where the actual hypothesis is one sided, i.e. that the true mean is greater than \\(67.5\\%\\)\n\n\n\n\nNotice that there are a few changes from the output that we saw last time. Most important is the fact that the actual hypothesis has changed, to reflect the different test. The second thing to note is that although the t-statistic and degrees of freedom have not changed, the p-value has. This is because the one-sided test has a different rejection region from the two-sided test. If you’ve forgotten why this is and what it means, you may find it helpful to read back over Chapter 9, and Section 9.4.3 in particular. The third thing to note is that the confidence interval is different too: it now reports a “one-sided” confidence interval rather than a two-sided one. In a two-sided confidence interval we’re trying to find numbers a and b such that we’re confident that, if we were to repeat the study many times, then \\(95\\%\\) of the time the mean would lie between a and b. In a one-sided confidence interval, we’re trying to find a single number a such that we’re confident that \\(95\\%\\) of the time the true mean would be greater than a (or less than a if you selected Measure 1 < Measure 2 in the ‘Hypothesis’ section).\nSo that’s how to do a one-sided one sample t-test. However, all versions of the t-test can be one-sided. For an independent samples t test, you could have a one-sided test if you’re only interested in testing to see if group A has higher scores than group B, but have no interest in finding out if group B has higher scores than group A. Let’s suppose that, for Dr Harpo’s class, you wanted to see if Anastasia’s students had higher grades than Bernadette’s. For this analysis, in the ‘Hypothesis’ options, specify that ‘Group 1 > Group2’. You should get the results shown in Figure 11.18.\n\n\n\n\n\nFigure 11.18: jamovi results showing an ‘Independent Samples t-Test’ where the actual hypothesis is one sided, i.e. that Anastasia’s students had higher grades than Bernadette’s\n\n\n\n\nAgain, the output changes in a predictable way. The definition of the alternative hypothesis has changed, the p-value has changed, and it now reports a one-sided confidence interval rather than a two-sided one.\nWhat about the paired samples t-test? Suppose we wanted to test the hypothesis that grades go up from test 1 to test 2 in Dr Zeppo’s class, and are not prepared to consider the idea that the grades go down. In jamovi you would do this by specifying, under the ‘Hypotheses’ option, that grade_test2 (‘Measure 1’ in jamovi, because we copied this first into the paired variables box) > grade test1 (‘Measure 2’ in jamovi). You should get the results shown in Figure 11.19.\n\n\n\n\n\nFigure 11.19: jamovi results showing a ‘Paired Samples T-Test’ where the actual hypothesis is one sided, i.e. that grade test2 (‘Measure 1’) \\(>\\) grade test1 (‘Measure 2’)\n\n\n\n\nYet again, the output changes in a predictable way. The hypothesis has changed, the p-value has changed, and the confidence interval is now one-sided."
  },
  {
    "objectID": "11-Comparing-two-means.html#effect-size",
    "href": "11-Comparing-two-means.html#effect-size",
    "title": "11  Comparing two means",
    "section": "11.8 Effect size",
    "text": "11.8 Effect size\nThe most commonly used measure of effect size for a t-test is Cohen’s d (Cohen, 1988). It’s a very simple measure in principle, with quite a few wrinkles when you start digging into the details. Cohen himself defined it primarily in the context of an independent samples t-test, specifically the Student test. In that context, a natural way of defining the effect size is to divide the difference between the means by an estimate of the standard deviation. In other words, we’re looking to calculate something along the lines of this:\n\\[d=\\frac{(\\text{mean 1})-(\\text{mean 2})}{\\text{std dev}}\\]\nand he suggested a rough guide for interpreting \\(d\\) in Table 11.3.\n\n\n\n\nTable 11.3:  A (very) rough guide to interpreting Cohen’s d. My personal recommendation is to not use these blindly. The d statistic has a natural interpretation in and of itself. It re-describes the difference in means as the number of standard deviations that separates those means. So it’s generally a good idea to think about what that means in practical terms. In some contexts a ‘small’ effect could be of big practical importance. In other situations a ‘large’ effect may not be all that interesting \n\nd-valuerough interpretation\n\nabout 0.2\"small\" effect\n\nabout 0.5\"moderate\" effect\n\nabout 0.8\"large\" effect\n\n\n\n\n\nYou’d think that this would be pretty unambiguous, but it’s not. This is largely because Cohen wasn’t too specific on what he thought should be used as the measure of the standard deviation (in his defence he was trying to make a broader point in his book, not nitpick about tiny details). As discussed by McGrath & Meyer (2006), there are several different versions in common usage, and each author tends to adopt slightly different notation. For the sake of simplicity (as opposed to accuracy), I’ll use d to refer to any statistic that you calculate from the sample, and use \\(\\delta\\) to refer to a theoretical population effect. Obviously, that does mean that there are several different things all called d.\nMy suspicion is that the only time that you would want Cohen’s d is when you’re running a t-test, and jamovi has an option to calculate the effect size for all the different flavours of t-test it provides.\n\n11.8.1 Cohen’s d from one sample\nThe simplest situation to consider is the one corresponding to a one-sample t-test. In this case, this is the one sample mean \\(\\bar{X}\\) and one (hypothesised) population mean \\(\\mu_0\\) to compare it to. Not only that, there’s really only one sensible way to estimate the population standard deviation. We just use our usual estimate \\(\\hat{\\sigma}\\). Therefore, we end up with the following as the only way to calculate \\(d\\)\n\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]\nWhen we look back at the results in Figure 11.6, the effect size value is Cohen’s \\(d = 0.50\\). Overall, then, the psychology students in Dr Zeppo’s class are achieving grades (\\(mean = 72.3\\%\\)) that are about .5 standard deviations higher than the level that you’d expect (\\(67.5\\%\\)) if they were performing at the same level as other students. Judged against Cohen’s rough guide, this is a moderate effect size.\n\n\n11.8.2 Cohen’s d from a Student’s t test\nThe majority of discussions of Cohen’s \\(d\\) focus on a situation that is analogous to Student’s independent samples t test, and it’s in this context that the story becomes messier, since there are several different versions of \\(d\\) that you might want to use in this situation. To understand why there are multiple versions of \\(d\\), it helps to take the time to write down a formula that corresponds to the true population effect size \\(\\delta\\). It’s pretty straightforward,\n\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\]\nwhere, as usual, \\(\\mu_1\\) and \\(\\mu_2\\) are the population means corresponding to group 1 and group 2 respectively, and \\(\\sigma\\) is the standard deviation (the same for both populations). The obvious way to estimate \\(\\delta\\) is to do exactly the same thing that we did in the t-test itself, i.e., use the sample means as the top line and a pooled standard deviation estimate for the bottom line\n\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\nwhere \\(\\hat{\\sigma}_p\\) is the exact same pooled standard deviation measure that appears in the t-test. This is the most commonly used version of Cohen’s d when applied to the outcome of a Student t-test, and is the one provided in jamovi. It is sometimes referred to as Hedges’ \\(g\\) statistic (Hedges, 1981).\nHowever, there are other possibilities which I’ll briefly describe. Firstly, you may have reason to want to use only one of the two groups as the basis for calculating the standard deviation. This approach (often called Glass’ \\(\\triangle\\), pronounced delta) only makes most sense when you have good reason to treat one of the two groups as a purer reflection of “natural variation” than the other. This can happen if, for instance, one of the two groups is a control group. Secondly, recall that in the usual calculation of the pooled standard deviation we divide by \\(N - 2\\) to correct for the bias in the sample variance. In one version of Cohen’s d this correction is omitted, and instead we divide by \\(N\\). This version makes sense primarily when you’re trying to calculate the effect size in the sample rather than estimating an effect size in the population. Finally, there is a version called Hedge’s g, based on Hedges & Olkin (1985), who point out there is a small bias in the usual (pooled) estimation for Cohen’s d.19\nIn any case, ignoring all those variations that you could make use of if you wanted, let’s have a look at the default version in jamovi. In Figure 11.10 Cohen’s \\(d = 0.74\\), indicating that the grade scores for students in Anastasia’s class are, on average, \\(0.74\\) standard deviations higher than the grade scores for students in Bernadette’s class. For a Welch test, the estimated effect size is the same (Figure 11.12).\n\n\n11.8.3 Cohen’s d from a paired-samples test\nFinally, what should we do for a paired samples t-test? In this case, the answer depends on what it is you’re trying to do. jamovi assumes that you want to measure your effect sizes relative to the distribution of difference scores, and the measure of d that you calculate is:\n\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\nwhere \\(\\hat{\\sigma}_D\\) is the estimate of the standard deviation of the differences. In Figure 11.16 Cohen’s \\(d = 1.45\\), indicating that the time 2 grade scores are, on average, \\(1.45\\) standard deviations higher than the time 1 grade scores.\nThis is the version of Cohen’s \\(d\\) that gets reported by the jamovi ‘Paired Samples T-Test’ analysis. The only wrinkle is figuring out whether this is the measure you want or not. To the extent that you care about the practical consequences of your research, you often want to measure the effect size relative to the original variables, not the difference scores (e.g., the 1% improvement in Dr Chico’s class over time is pretty small when measured against the amount of between-student variation in grades), in which case you use the same versions of Cohen’s d that you would use for a Student or Welch test. It’s not so straightforward to do this in jamovi; essentially you have to change the structure of the data in the spreadsheet view so I won’t go into that here20, but the Cohen’s d for this perspective is quite different: it is \\(0.22\\) which is quite small when assessed on the scale of the original variables."
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-Checking-the-normality-of-a-sample",
    "href": "11-Comparing-two-means.html#sec-Checking-the-normality-of-a-sample",
    "title": "11  比較單一與兩組平均值",
    "section": "11.9 檢測樣本常態性",
    "text": "11.9 檢測樣本常態性\n迄今為止，在本章中討論的所有檢驗都假定資料呈常態分佈。這個假設通常是相當合理的，因為中心極限定理（見 小單元 8.3.3 ）確實傾向於確保許多現實世界的數量呈常態分佈。任何時候你懷疑你的變量實際上是很多不同事物的平均值時，它很可能呈常態分佈，或者至少接近正態，以便你可以使用t檢驗。然而，生活中沒有保證，而且還有很多方法可以讓你得到非常非正態的變量。例如，任何時候你認為你的變量實際上是很多不同事物的最小值，它很可能會變得非常偏斜。在心理學中，反應時間（RT）資料就是一個很好的例子。如果你認為有很多事情可能觸發一個人類參與者的反應，那麼實際的反應將在這些觸發事件中的第一次發生時發生。20這意味著RT資料是系統性地非正態的。好吧，那麼，如果正態性是所有檢驗的假設，並且在現實世界的資料中大多數（至少近似地）得到滿足，那麼我們如何檢查樣本的正態性呢？在本節中，我將討論兩種方法：分位數對分位數圖(Q-Q plot)和Shapiro-Wilk檢驗。\n\n\n11.9.1 分位數對分位數圖\n檢查樣本是否違反正態性假設的一種方法是繪製“Q-Q圖”（分位數對分位數圖，Quantile-Quantile plot）。這可以讓您直觀地檢查是否有任何系統性違規行為。在QQ圖中，每個觀察值都作為一個單獨的點繪製。x坐標是觀察值在資料常態分佈（用樣本估計均值和方差）的情況下應該落入的理論分位數，y坐標是資料在樣本中的實際分位數。如果資料是正態的，點應該形成一條直線。例如，讓我們看看如果我們通過從常態分佈中抽樣生成資料，然後繪製QQ圖會發生什麼。結果顯示在@fig-fig11-20。\n\n\n\n\n\n\n圖 11.20: 直方圖（圖板(a)）和正態QQ圖（圖板(b)）顯示 normal.data 是具有 \\(100\\) 個觀測值的常態分佈樣本。與這些資料相關的 Shapiro-Wilk 統計量是 \\(W = .99\\)，表示未檢測到顯著偏離正態性（\\(p = .54\\)）。\n\n\n\n\n如您所見，這些資料形成了一條非常直線；這並不奇怪，因為我們從常態分佈中抽樣得到它們！相比之下，請看@fig-fig11-21中顯示的兩個資料集。上面的面板顯示了一個高度偏斜的資料集的直方圖和QQ圖：QQ圖向上彎曲。下面的面板顯示了同樣的圖，但對於一個尾部較重（即高峰度）的資料集：在這種情況下，QQ圖在中間變平並在兩端急劇彎曲。\n\n\n\n\n\n\n圖 11.21: 在第一行，顯示有偏的資料集的 \\(100\\) 個觀測值的直方圖和正態QQ圖。這裡的資料偏度為 \\(1.88\\)，並在向上彎曲的QQ圖中反映出來。因此，Shapiro-Wilk統計量為 \\(W = .80\\)，反映了與正態性的顯著偏差（\\(p &lt; .001\\)）。底部的行顯示了同樣的圖，但對於尾部較重的資料集，同樣包含100個觀測值。在這種情況下，資料中的重尾產生了高峰度（\\(6.57\\)），並使QQ圖在中間變平，在兩側急劇彎曲。由此產生的Shapiro-Wilk統計量為 \\(W = .75\\)，同樣反映了顯著的非正態性（\\(p &lt; .001\\)）。\n\n\n\n\n\n\n11.9.2 t檢定的分位數對分位數圖\n在之前的分析中，我們展示了如何在 jamovi 中進行獨立 t 檢驗（圖 11.10）和配對樣本 t 檢驗（圖 11.16）。對於這些分析，jamovi 提供了顯示差異得分的 QQ 圖選項（jamovi 稱之為「殘差」），這是檢查正態性假設的更好方法。當我們選擇這些分析的此選項時，我們將分別獲得 圖 11.22 和 圖 11.23 中顯示的 QQ 圖。我的解釋是，這兩個圖都表明差異得分基本上呈常態分佈，所以我們可以繼續了！\n\n\n\n\n\n\n圖 11.22: 獨立t檢驗分析的QQ圖 參考 圖 11.10\n\n\n\n\n\n\n\n\n\n圖 11.23: 配對樣本t檢驗分析的QQ圖 參考 圖 11.16\n\n\n\n\n\n\n11.9.3 Shapiro-Wilk檢定\nQQ圖提供了一種非正式檢查資料正態性的好方法，但有時您可能需要做更正式的檢查，Shapiro-Wilk檢定(Shapiro & Wilk, 1965)可能正是您正在尋找的。21如您所料，被檢驗的虛無假設是一組\\(N\\)個觀測值是常態分佈的。\n[其他技術細節 22]\n\n\n\n\n\n\n圖 11.24: 在樣本大小為10、20和50的情況下，據擷取的Shapiro-Wilk \\(W\\)統計量分布，據此推出空假設是資料呈常態分佈。注意，\\(W\\)值較小表示偏離常態分佈。\n\n\n\n\n該檢驗統計量通常表示為\\(W\\)，其計算方法如下。首先，我們按照大小遞增的順序對觀測值進行排序，讓\\(\\bar{X_1}\\)成為樣本中最小的值，\\(X_2\\)成為第二小的值，依此類推。然後\\(W\\)的值由以下公式給出\\[W=\\frac{(\\sum_{i=1}^N a_iX_i)^2}{\\sum_{i=1}^N(X_i-\\bar{X})^2}\\]其中\\(\\bar{X}\\)是觀測值的平均值，\\(a_i\\)值是…咕嚕咕嚕…比較複雜，超出了入門級教材的範疇。\n\n\n\n11.9.4 檢測樣本常態性的範例\n與此同時，當資料變得非正態時，展示一下 QQ 圖和 Shapiro-Wilk 檢定的變化可能是值得的。為此，讓我們看一下我們的澳式足球聯賽（AFL）勝利幅度資料的分佈，如果您還記得@sec-描述性統計，它看起來根本不是來自常態分佈。以下是 QQ 圖發生的情況（圖 11.25）。\n\n\n\n\n\n\n圖 11.25: 展示 AFL 勝利幅度資料非正態性的 QQ 圖\n\n\n\n\n而當我們對 AFL margins 資料執行 Shapiro-Wilk 檢定時，我們得到了 Shapiro-Wilk 正態檢定統計量的值 \\(W = 0.94\\)，p值 = \\(9.481\\)x\\(10^{-07}\\)。顯然是一個顯著的效應！"
  },
  {
    "objectID": "11-Comparing-two-means.html#testing-non-normal-data",
    "href": "11-Comparing-two-means.html#testing-non-normal-data",
    "title": "11  Comparing two means",
    "section": "11.10 Testing non-normal data",
    "text": "11.10 Testing non-normal data\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 Two sample Mann-Whitney U test\nI’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.4).\n\n\n\n\nTable 11.4:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 24 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for the Mann-Whitney \\(U\\) test\n\n\n\n\n\n\n11.10.2 One sample Wilcoxon test\nWhat about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5.\n\n\n\n\nTable 11.5:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\npositive differences7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests"
  },
  {
    "objectID": "11-Comparing-two-means.html#summary",
    "href": "11-Comparing-two-means.html#summary",
    "title": "11  Comparing two means",
    "section": "11.11 Summary",
    "text": "11.11 Summary\n\nThe one-sample t-test is used to compare a single sample mean against a hypothesised value for the population mean.\nAn independent samples t-test is used to compare the means of two groups, and tests the null hypothesis that they have the same mean. It comes in two forms: The independent samples t-test (Student test) assumes that the groups have the same standard deviation, The independent samples t-test (Welch test) does not.\nThe paired-samples t-test is used when you have two scores from each person, and you want to test the null hypothesis that the two scores have the same mean. It is equivalent to taking the difference between the two scores for each person, and then running a one sample t-test on the difference scores.\nOne-sided tests are perfectly legitimate as long as they are pre-planned (like all tests!).\nEffect size calculations for the difference between means can be calculated via the Cohen’s d statistic.\nChecking the normality of a sample using QQ plots and the Shapiro-Wilk test.\nIf your data are non-normal, you can use Mann-Whitney or Wilcoxon tests instead of t-tests for Testing non-normal data.\n\n\n\n\n\nBox, J. F. (1987). Guinness, gosset, fisher, and small samples. Statistical Science, 2, 45–52.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nHedges, L. V. (1981). Distribution theory for glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6, 107–128.\n\n\nHedges, L. V., & Olkin, I. (1985). Statistical methods for meta-analysis. Academic Press.\n\n\nMcGrath, R. E., & Meyer, G. J. (2006). When effect sizes disagree: The case of \\(r\\) and \\(d\\). Psychological Methods, 11, 386–401.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52, 591–611.\n\n\nStudent, A. (1908). The probable error of a mean. Biometrika, 6, 1–2.\n\n\nWelch, B. L. (1947). The generalization of “Student’s” problem when several different population variances are involved. Biometrika, 34, 28–35."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方適合度檢定",
    "href": "10-Categorical-data-analysis.html#卡方適合度檢定",
    "title": "10  類別資料分析",
    "section": "10.1 卡方適合度檢定",
    "text": "10.1 卡方適合度檢定\n卡方適合度檢定(goodness-of-fit test)是統計學者們最早開發出來的假設檢定方法之一。主要發明者是20世紀初的統計學者卡爾．皮爾森( Karl Pearson , 皮爾森相關係數以他的名字命名) (Pearson, 1900)，之後羅納德·費雪爵士(Sir Ronald Fisher) (Fisher, 1922) 做了一些改良，這個方法是檢定名義變項的觀察次數分佈是否符合預期次數分佈。像是有一組病人接受了實驗性治療，根據治療後的健康狀況分類，醫師紀錄病情是否有改善、保持不變或惡化，接著使用適合度檢定可以確定每個類別的人次，是否匹配標準治療後的預期人次。以下用一些心理學研究案例學習如何使用卡方適合度檢定。\n\n\n10.1.1 撲克牌花色隨機選擇資料\n過去幾十年許多人類模擬隨機現象的研究顯示，這是人類很難學成的能力。雖然每個人或多或少會試表“隨機行動”，依照模式和結構進行思考仍然是一般人難以擺脫的習慣，就算是被要求“隨機做某件事情”，實際上完全無法隨機行事，所以這類研究反過來揭露了許多人類的非隨機行為，其中透露我們如何看待世界的深刻心理問題。這一節的範例啟發自隨機行為研究，以此虛構一個非常簡單的研究。假如研究人員要求參與者想像一副已經洗好的撲克牌，然後從這副牌裡“隨機”選出一張牌。參與者選好想像中的牌之後，再選出第二張牌。參與者選擇好之後，再請他們說出選擇的花色（紅心、梅花、黑桃或方塊）。假如這個研究最後收集了 \\(N = 200\\) 個人選出的牌，研究人員想分析資料，確認一般人假裝選擇的撲克牌是否真的隨機。這份資料存在lsj檔案庫的 Randomness ，當讀者從 jamovi 的lsj檔案庫開啟這份資料的試算表，會看到三個變項：為每個參與者分配辨識碼id ，以及紀錄每個人先後選出的兩種撲克牌花色 choice_1 和 choice_2。\n現在先看參與者第一次選擇的花色。開啟“Explore” - “Descriptives”的設定視窗，點選Frequency table選項計算每個花色被參與者們選擇的次數。得到的結果就如同 表 10.1 ：\n\n\n\n\n\n\n表 10.1: 的第一次選擇撲克牌花色的次數紀錄\n\n\nclubs\ndiamonds\nhearts\nspades\n\n\n35\n51\n64\n50\n\n\n\n\n\n\n\n\n這份小小的次數表非常有用。其中數字似乎暗示，參與者們可能偏好選擇紅心而且比較不想選擇梅花。只看表面數字並不能判斷這樣的差距是不是巧合，因此需要進行統計分析來找出答案，這就是下一節要學習的功課。\n沒問題的話，接下來要分析 表 10.1 的資料囉。然後，這裡開始不得不用些數學符號討論這些資料，所以最好先認識一下每個符號的意義。首先是至目前為止一直提到“觀察值”，將用大寫字母\\(O\\)，而字母的下標表示觀察值在表格裡的位置。像是 表 10.1 的第二個觀察值可寫為\\(O_2\\)。表 10.2 說明每個花色的報告次數與代表符號之間的對應 。\n\n\n\n\n\n\n表 10.2: 英語描述與數學符號之間的關係\n\n\nlabel\nindex, i\nmath. symbol\nthe value\n\n\nclubs, \\( \\clubsuit \\)\n1\n\\( O_1 \\)\n35\n\n\ndiamonds, \\( \\diamondsuit \\)\n2\n\\( O_2 \\)\n51\n\n\nhearts, \\( \\heartsuit \\)\n3\n\\( O_3 \\)\n64\n\n\nspades, \\( \\spadesuit \\)\n4\n\\( O_4 \\)\n50\n\n\n\n\n\n\n\n\n希望這樣整理能讓讀者搞清楚。同時提醒一下，數學家更喜歡用符號討論而不是直接談論具體事項，因此接著會一直看到\\(O_i\\)之類的符號，這是指在第 i 類別的觀察次數（其中 i 可能是 1，2，3 或 4）。最後，如果我們要在報告裡提及所有觀察次數，統計學家習慣將所有觀察值構成一個向量 2，本書以 \\(O\\) 之類的大寫字母表示。\n\\[O = (O_1, O_2, O_3, O_4)\\]\n同樣的，這裡沒有什麼新奇有趣之處，一切只是符號。如果說 \\(O = (35, 51, 64, 50)\\)，就只是將描述觀察值的次數表，改用數學符號來表示而已。\n\n\n\n\n10.1.2 虛無假設與對立假設\n正如一開始的範例說明，研究者的假設是“一般人不會隨機選出想像中的撲克牌”。接著要做的是將概念中的假設，轉換為相互對立的統計假設，然後決定測試這些統計假設的統計方法。這個範例要使用的統計方法就是需要使用這一節要學習皮爾森 \\(\\chi^2\\) 適合度檢定，規劃適合度檢定的第一步是設定虛無假設，撲克牌範例的虛無假設是很簡單地。我們先用文字說明虛無假設：\n\\[H_0: \\text{ 四種花色被選擇的機率相等}\\]\n修習統計學課程的學生要學會的功課之一，就是用數學符號表達虛無假設，這裡使用 \\(P_j\\) 表示第j種花色被參與者 選擇的真實機率。如果研究結果符合虛無假設，那麼任何一種花色都有 25% 的機率被選中。換句話說，以上的虛無假設說明用數學符號表達的方式是\\(P_1 = .25\\)，\\(P_2 = .25\\)，\\(P_3 = .25\\)， \\(P_4 = .25\\)。因為研究人員習慣用向量符號涵括同一個變項的資料，在此用 \\(P\\) 表示虛無假設涵括的所有機率事件。也就是向量 \\(P = (P_1, P_2, P_3, P_4)\\) 表示虛無假設的機率事件集合，這麼一來可將虛無假設寫成：\n\\[H_0: P =(.25, .25, .25, .25)\\]\n對應虛無假設的向量 \\(P\\) ，涵括的所有事件發生機率剛好相等，不過真實的研究條件不一定會是這樣。如同這個範例的實驗任務是讓參與者從想像中的撲克牌組抽一張牌，若是這副牌的梅花數量是其他花色的兩倍，那麼虛無假設就要寫成\\(P = (.4, .2, .2, .2)\\)。只要機率值都是正數，且總和為 1，就能構成合法的虛無假設。因為許多使用適合度檢定的場景，是用來處理所有類別事件發生機率相等的虛無假設，以下討論繼續使用四種花色被挑選機率相等的虛無假設。\n那麼對立假設 \\(H_1\\)是什麼呢？這道統計假設表示研究者感興趣的研究結果，也就是認為參與者的選擇不是完全隨機，因此四種花色的發生機率並不相等。所以兩則統計假設的白話版本是：\n\\(H_0: \\text{ 四種花色被選擇的機率相等}\\) \\(H_1: \\text{ 至少有一個選擇花色的機率不是 0.25}\\)\n…或者可寫成“數學家偏好的”版本：\n\n\\(H_0: P= (.25, .25, .25, .25)\\) \\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n10.1.3 適合度檢定統計程序\n到了這一步，我們手上有一組觀察次數 \\(O\\) 以及一組對應虛無假設的機率 \\(P\\)，接著就是規劃虛無假設的檢定程序。如同 單元 9 的說明，要檢測 \\(H_0\\) 和 \\(H_1\\)，就需要計算檢測統計值，構成適合度檢定的統計值是衡量資料與虛無假設之間的“接近程度”。如果資料不相符虛無假設的期望機率，那麼虛無假設呈現的情況可能不是真的。那麼，如果虛無假設是真的，檢測結果會是什麼樣子呢？或者專有名詞來問，期望次數是什麼？總共有 \\(N = 200\\) 次觀察，若是虛無假設為真，任何一位參與者選擇紅心的機率是 \\(P_3 = .25\\)，所以紅心的期望次數是 \\(200 \\times .25 = 50\\) ，對吧？更具體的方式是用\\(E_i\\)代表“虛無假設為真時，研究人員期望觀察到第 i 類反應的次數”，也就是這個數學公式：\n\\[E_i=N \\times P_i\\]\n這個式子不難用手計算。就像現在的200 個觀察值是分為四個類別，研究人員認為參與者會選擇任何一個類別的可能性相等，那麼每個類別應該都有50人次，對吧？\n接著要如何將觀察次數和期望次數轉換為檢測統計值呢？只要比較每個類別的期望觀察次數（\\(E_i\\)）與各自的實際觀察次數（\\(O_i\\)），就能得出一個有用的檢測統計值。首先要計算虛無假設的期望次數與實際總計出的次數之間差異也就是計算“觀察次數減去期望次數”的差異值，\\(O_i - E_i\\)。詳細計算請見 表 10.3 。\n\n\n\n\n\n表 10.3: 各種花色的期望次數(expected frequencu)、實際次數(observed frequency)和差異分數(difference score)\n\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\nexpected frequency \\( E_i\\)\n50\n50\n50\n50\n\n\nobserved frequency \\( O_i\\)\n35\n51\n64\n50\n\n\ndifference score \\( O_i-E_i\\)\n-15\n1\n14\n0\n\n\n\n\n\n\n\n\n根據計算結果，選擇紅心的人數顯然比虛無假設預測的多，選擇梅花的人較少。不過稍微想一下就會發現，最後一列的差異值有負的數值！這裡的奇怪之處在於虛無假設所做的預測少於觀察次數(像示範資料的紅心)，和預測多於觀察次數(像示範資料的梅花)一樣糟糕。最簡單的解決方法是將所有數字平方，如此就可以計算平方差，\\((E_i - O_i)^2\\)。這樣就能算出如 表 10.4 的數值。\n\n\n\n\n\n表 10.4: 四種花色差異次數平方\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\n225\n1\n196\n0\n\n\n\n\n\n\n\n\n現在的算出來的這組數字有明顯的特色，與虛無假設預測差異太大的類別（梅花和紅心），數字都很大；預測接近的類別（方塊和黑桃），數字都很小。為了解釋稍後的步驟，我們要將這些平方差除以期望次數 \\(E_i\\) ，也就是\\(\\frac{(E_i-O_i)^2}{E_i}\\)。因為這個例子裡所有類別的 \\(E_i = 50\\)，用手一個一個算還挺無聊的，不過算完後會得到 表 10.5 的結果。\n\n\n\n\n\n表 10.5: 四種花色’誤差’分數~差異平方除以期望次數\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\n4.50\n0.02\n3.92\n0.00\n\n\n\n\n\n\n\n\n這裡我們算出了四種’誤差’分數，每個分數代表用虛無假設預測實際次數，所造成的“錯誤”程度。為了轉換為檢測統計值，接下來要做的事就是將這些數字加起來。最後加起來的值就是適合度，通常在報告裡寫成\\(\\chi^2\\)（卡方）或 適合值(GOF)。所有步驟總結如以下公式：3\n\\[\\sum( (觀察次數 - 期望次數)^2 / 期望次數 )\\]\n最後得到適合度的卡方統計值 8.44。\n[額外的技術細節 4]\n根據以上的計算，這項研究資料分析結果得到了 \\(\\chi^2\\) = 8.44。那麼現在的問題就是，這個值是否大到足夠拒絕虛無假設？\n\n\n\n10.1.4 適合度檢定的樣本分佈\n要確定某個 \\(\\chi^2\\) 統計值是否大到能充分拒絕虛無假設，接著要確定如果虛無假設為真，\\(\\chi^2\\) 的樣本分佈會是什麼，在這一節中要學習的，就是如何規劃符合適合度檢定的樣本分佈，然後下一節學習如何使用這樣的樣本分佈構建假設檢程序。如果讀者願意相信樣本分佈是具有 \\(k - 1\\) 自由度的 \\(\\chi^2\\)（卡方）分佈，其實可以跳過本節。然而，如果讀者需要了解為什麼適合度檢定程序要如此安排，請繼續閱讀下去。\n先假設虛無假設的說法是真的，那麼觀察值落入第 i 類的真實機率就是 \\(P_i\\)，這麼一來就符合對應虛無假設的向量，那這樣代表什麼意思呢？這像是擲出一枚特製的硬幣，觀察擲出那一面以決定落入類別i，而擲出那一面的的機率是 \\(P_j\\)。所以觀察到某一類別的實際次數 \\(O_i\\)，就像是擲出這枚特製硬幣N次，每次紀錄一個觀察值，最後有 \\(O_i\\) 次紀錄到硬幣的那一面。雖然這樣的實驗很奇怪，這樣的說明只是希望讀者了解，我們已經在前面的單元見識類似的場景，像是 單元 8 的 小單元 8.4 所示範的實驗設定。換句話說，如果虛無假設是真的，那麼多次實驗的觀察次數會符合二項分佈所構成的機率分佈：\n\\[O_i \\sim Binomial(P_i,N) \\]\n若是讀者還記得 小單元 8.3.3 提到的中央極限定理，當 \\(N\\) 較大且 \\(P_i\\) 剛好在0與1中間時，二項分佈看起來幾乎與常態分佈一模一樣。換句話說，只要 \\(N^P_i\\) 足夠大，或者說，當期望次數 \\(E_i\\) 足夠大時，理論上 \\(O_i\\) 的機率分佈接近常態分佈。更棒的是，如果 \\(O_i\\) 是常態分佈的，那麼 \\((O_i-E_i)/\\sqrt{E_i}\\) 也會接近常態分佈。因為任何一個 \\(E_i\\) 數值是固定的，所以實際觀察次數減去期望次數並除以期望值的開根號，相當於改變了常態分佈的平均值和標準差。為何這樣的統計值公式能計算適合度呢？因為這個公式將所有觀察值平方後加總，形成的樣本統計值可用近似的常態分佈計算可能發生的機率。等等，這就像 小單元 8.6 提到的，當我們手上有很多符合標準常態分佈的資料（平均值為 0 且標準差為 1），將所有數值平方然後加起來時，所得到的樣本數值符合卡方分佈。所以現在我們知道虛無假設所預測適合度統計值的樣本分佈符合卡方分佈。太棒了。\n最後還要提一個細節，就是自由度。 小單元 8.6 曾提到把k個數值加總起來，所構成的卡方分佈的自由度就是 k。但是卡方適合度檢定的實際自由度是 \\(k - 1\\)，倒底是怎麼一回事呢？因為真正能加總的數值必須是獨立的。下一節我們將討論，為何全部有k個觀察值，卻只有 \\(k - 1\\) 個是真正獨立的，所以自由度實際上只有 \\(k - 1\\)。這就是下一節的主題5。\n\n\n\n10.1.5 自由度\n\n\n\n\n\n圖 10.1: 不同“自由度”的卡方分佈\n\n\n\n\n在 小單元 8.6 我初次向讀者介紹卡方分佈，只有稍微講一下什麼是自由度，強調這是很重要的概念。請看一下 圖 10.1 不同的卡方分佈，可知只要自由度改變了，那麼形狀劇烈改變。那麼自由度究竟是什麼呢？其實在解釋自由度與常態態分佈的關係時，就提到了一種解釋：自由度是可平方並可加成的常態分佈變項數量。當然，這樣的解釋相當抽象，對多數讀者沒有幫助。接著讓我們透過真實的資料，來嘗試理解自由度吧。\n自由度的基本概念其實相當簡單，計算方法是將先算出資料的“總數”，然後減去“不可變動”的資料數目。6這樣說有點籠統，所以這裡用想像實驗的撲克牌資料再解釋一下。首先有\\(O1, O2, O3, O4\\)四個分別代表選出一種花色（紅心，梅花，方塊，黑桃）觀察次數的數字。這四個數字是經過實驗得到的隨機結果，其實還有一個不可變動的樣本量 \\(N\\)。7也就是說，如果我們知道有多少人選擇紅心，多少人選擇方塊，多少人選擇梅花，那麼就能確實知道還有多少人選擇黑桃。換句話說，雖然總體資料量是四個，實際上只有 \\(4 - 1 = 3\\) 個自由度。另一種角度的思理解方式是，我們想要估算四種類別的機率，但是全部類別的機率加起來必須等於一，那麼其中一個類別的機率不可變動。因此自由度是 \\(4 - 1 = 3\\)。無論您想用觀察次數的角度，還是機率的角度理解自由度，答案都是一樣的。通常要執行 \\(k\\) 組的卡方適合度檢定時，自由度都是 \\(k - 1\\)。\n\n\n\n10.1.6 檢定虛無假設\n一個完整的假設檢定程序，最後要規劃的是棄卻域，也就是決定那些 \\(\\chi^2\\) 值能拒絕虛無假設。我們已經知道越大的\\(\\chi^2\\)數值代表虛無假設的預測與真正的實驗結果差異越大，\\(\\chi^2\\)數值越小則代表虛無假設的預測越接近真正的實驗結果。所以明智的策略要設定一個臨界值，如果 \\(\\chi^2\\) 數值大於臨界值，就拒絕虛無假設；如果 \\(\\chi^2\\) 小於臨界值，就保留虛無假設。用 單元 9 介紹過的詞彙，卡方適合度檢定是一種單側檢定。好了，現在要做的就是訂下臨界值是多少。方法很簡單，如果研究人員願意容忍犯下型一錯誤的機率最多\\(5%\\)，可以顯著水準\\(\\alpha = .05\\)決定臨界值，如此一來，在虛無假設預測為真的前提下，能得到與臨界值一樣大的 \\(\\chi^2\\) 數值之機率應該只有\\(5%\\)。如同 圖 10.2 的圖解。\n\n\n\n\n\n\n圖 10.2: \\(\\chi^2\\)（卡方）適合度檢定的假設檢定如何運作的示意圖\n\n\n\n\n上到這裡，有些同學會問，要如何找到自由度是 \\(k-1\\) 的卡方分佈臨界值？如果是多年前的用實體教科書上統計課，都可以在書的附錄裡看到像 圖 10.3 的表格，用來查找臨界值。在這個表中能找到自由度是3的卡方分佈臨界值，\\(\\alpha = 0.05\\)對應的數值是 7.815。\n\n\n\n\n\n\n\n圖 10.3: 查找卡方分佈臨界值的表格\n\n\n\n\n所以，若是真正算出的 \\(\\chi^2\\) 統計值大於 7.815，就可以拒絕虛無假設（預測每一種花色被選擇的機率相等）。即然已經算出統計值是8.44，那麼可以拒絕虛無假設。至此我們已經走完“皮爾森卡方適合度檢定”的過程，真不錯。\n\n\n\n10.1.7 jamovi實作\n毫不意外地，jamovi 提供了一個分析工具，可以幫你完成這些計算。讓我們使用 Randomness.omv 文件。在主要的“分析”工具欄中，選擇“頻率” - “單樣本比例檢驗” - “\\(N\\) 個結果”。然後在出現的分析視窗中將要分析的變項（從選擇 1 開始）移到“變項”框中。此外，單擊“預期計數”復選框，以便將這些資料顯示在結果表中。完成所有這些操作後，你應該會在 jamovi 中看到分析結果，如 圖 10.4。然後不出所料，jamovi 提供了與我們上面手動計算相同的預期計數和統計資料，\\(\\chi^2\\) 值為 \\((8.44\\)，自由度為 \\(3\\)，\\(p=0.038\\)。注意，我們不再需要查找臨界 p 值閾值，因為 jamovi 給出了在 \\(3\\) 自由度下計算得出的 \\(\\chi^2\\) 的實際 p 值。\n\n\n\n\n\n\n\n圖 10.4: jamovi 中的 \\(\\chi^2\\) 單樣本比例檢驗，表格顯示觀察到的頻率和比例以及期望的頻率和比例\n\n\n\n\n\n\n10.1.8 另一種虛無假設\n此時，你可能會想知道如果你想進行適合度檢驗，但你的虛無假設不是所有類別的機率都相等該怎麼辦。例如，假設有人提出了這樣的理論預測，即人們應該以 \\(60\\%\\) 的機率選擇紅色牌，以 \\(40\\%\\) 的機率選擇黑色牌（我不知道為什麼你會這樣預測），但沒有其他偏好。如果是這樣，虛無假設將期望選擇愛心的比例為 \\(30\\%\\)，選擇方塊的比例為 \\(30\\%\\)，選擇黑桃的比例為 \\(20\\%\\)，選擇梅花的比例為 \\(20\\%\\)。換句話說，我們期望愛心和方塊的出現次數是黑桃和梅花的 1.5 倍（\\(30\\%\\) : \\(20\\%\\) 的比例與 1.5 : 1 相同）。對我來說，這似乎是一個愚蠢的理論，但是用我們的 jamovi 分析可以很容易地測試這個明確指定的虛無假設。在分析視窗中（標記為“比例檢驗（N個結果）”的 圖 10.4 中，你可以展開“預期比例”的選項。當你這樣做時，將會出現一些選項，讓你為選定的變項輸入不同的比例值，在我們的案例中，這個變項是 choice 1。將比例更改為反映新的虛無假設，如 圖 10.5 所示，並觀察結果如何變化。\n\n\n\n\n\n\n圖 10.5: 在 jamovi 中更改 \\(\\\\chi^2\\) 單樣本比例檢驗的預期比例\n\n\n\n\n預期計數現在顯示在 表 10.6 中。\n\n\n\n\n\n表 10.6: 不同虛無假設的預期計數\n\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\nexpected frequency \\( E_i\\)\n40\n60\n60\n40\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) 統計量為 4.74，自由度為 3，\\(p = 0.182\\)。現在，我們更新的假設和預期頻率與上次的結果有所不同。因此，我們的 \\(\\chi^2\\) 檢驗統計量和 p 值也有所不同。令人惱火的是，p 值為 \\(.182\\)，因此我們不能拒絕虛無假設（回顧 小單元 9.5 提醒自己為什麼）。可悲的是，儘管虛無假設對應著一個非常愚蠢的理論，這些資料並沒有提供足夠的證據來反駁它。\n\n\n\n10.1.9 適合度檢定的報告寫作\n現在你知道了這個測試的運作方式，也知道如何使用神奇的jamovi計算盒來進行測試。接下來你需要知道的是如何撰寫結果。畢竟，設計和執行實驗，然後分析資料，如果不告訴別人結果是沒有意義的！所以讓我們來談談在報告分析時需要做的事情。讓我們繼續以撲克牌花色為例。如果我想將這個結果寫成一篇論文之類的東西，那麼慣常的報告方式是這樣寫的：\n\n在實驗的200名參與者中，有64人首選紅心，51人選擇方塊，50人選擇黑桃，35人選擇梅花。進行了卡方適合度檢驗以測試四種花色的選擇機率是否相同。結果顯著（\\(\\chi^2(3) = 8.44, p&lt; .05\\)），這表明人們在選擇花色時並非完全隨機。\n\n這相當直接，希望它看起來很不起眼。儘管如此，你應該注意到這個描述中的幾點內容：\n\n描述統計資料在統計檢驗之前。也就是說，在進行檢驗之前，我告訴讀者有關資料的一些信息。通常，這是一個很好的做法。永遠記住，你的讀者對你的資料了解得遠不如你。因此，除非你妥善地向他們描述，否則統計檢驗對他們來說毫無意義，他們會感到沮喪和哭泣。\n描述告訴你正在測試的虛無假設是什麼。老實說，作者並不總是這樣做，但在存在一定歧義的情況下，或者在你不能依賴你的讀者非常熟悉你正在使用的統計工具時，這通常是一個好主意。很多時候讀者可能不知道（或記不起）你正在使用的檢驗的所有細節，所以提醒他們是一種禮貌！對於適合度檢驗來說，你通常可以依賴科學觀眾了解它的運作方式（因為它涵蓋在大多數入門統計課程中）。然而，明確陳述虛無假設（簡要地！）仍然是一個好主意，因為虛無假設可能因你使用檢驗的目的而有所不同。例如，在撲克牌的例子中，我的虛無假設是四個花色的機率相同（即，\\(P1 = P2 = P3 = P4 = 0.25\\)），但這個假設並沒有什麼特別的。我可以同樣使用適合度檢驗測試虛無假設，即\\(P_1 = 0.7\\)和\\(P2 = P3 = P4 = 0.1\\)。所以，向讀者解釋你的虛無假設是有幫助的。另外，注意到我用文字而不是數學描述虛無假設。這是完全可以接受的。你可以用數學描述它，但是因為大多數讀者發現文字比符號更容易閱讀，所以大多數作者傾向於用文字描述虛無假設（如果可以的話）。\n包括”統計塊”。在報告檢驗結果本身時，我不僅僅說結果顯著，還包括了一個“統計塊”（即括號內密集的數學部分），其中報告了所有“關鍵”統計信息。對於卡方適應度檢驗，報告的信息包括檢驗統計量（即適應度統計量為8.44）、用於檢驗的分布信息（具有3個自由度的\\(\\chi^2\\)，通常縮寫為\\(\\chi^2\\)(3)），然後是結果是否顯著（在本例中為\\(p&lt; .05\\)）。每個檢驗所需的統計塊中的特定信息各不相同，因此每次我介紹一個新檢驗時，我都會向您展示統計塊應該是什麼樣子。8 但是，一般原則是您應該始終提供足夠的信息，以便讀者在需要時可以自己檢查測試結果。\n對結果進行解釋。除了指出結果顯著之外，我還提供了結果的解釋（即，人們沒有隨機選擇）。這對讀者也是一種友善，因為它告訴他們關於資料中發生了什麼事的一些信息。如果不包括這樣的東西，讀者很難理解發生了什麼事。9\n\n正如其他所有事物一樣，你應該首要關注的是向讀者解釋事物。永遠記住，報告結果的目的是與另一個人溝通。我無法告訴您我看過多少次報告、論文甚至科學文章的結果部分就是胡言亂語，因為作者只關注確保包含所有數字，卻忘記了與人類讀者真正交流。\n\n撒旦在統計和引用經文中同樣感到高興10 – H.G. 威爾斯"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方獨立性檢定",
    "href": "10-Categorical-data-analysis.html#卡方獨立性檢定",
    "title": "10  類別資料分析",
    "section": "10.2 卡方獨立性檢定",
    "text": "10.2 卡方獨立性檢定\n\n守衛機器人 1：停！ 守衛機器人 2：你們是機器人還是人類？ 莉娜：我們是…機器人。 弗萊：呃，對！就是在機器人世界裡像機器人一樣生活的兩個機器人！呃？\n守衛機器人 1：測試！ 守衛機器人 2：你最喜歡哪一個東西？A：一隻小狗，B：來自心上人的漂亮鮮花，還是C：以正確格式儲存的大量資料檔案？ 守衛機器人 1：選擇！ 《飛出個未來》第一季第5集”Fear of a Bot Planet”台詞(1999~2003於美國福斯電視網播映的喜劇動畫片；台灣無代理商引進播映)\n\n某天我看了動畫片《飛出個未來》某一集，描述名叫Chapek 9的外星球原住民古怪風俗，能進入星球首都的訪客必須是機器器，絕不能是人類。守衛為了確認訪客是不是人類，會詢問訪客是喜歡小狗、鮮花還是格式正確的大量資料檔案。我心想：“這是相當聰明的問題，但是如果人類和機器人有相同的喜好呢？那就不是一個好的測試問題了吧？”其實，我偶然間取得了Chapek 9首都市政府用來檢查這個問題有沒有效的測試資料。他們做的測試非常簡單，就是找來一群機器人和一群人類，問他們喜歡什麼。所有資料都儲存在chapek9.omv這個檔案裡，有安裝本書資料庫模組的話，可以直接從jamovi資料庫匯入這個檔案(Chapek 9)。除了識別參與者的變項ID，還有兩個名義尺度變項，species和choice，這份資料檔案一共有180筆參與者的反應。全部資料包括93個人類和87個機器人，絕大多數參與者選擇了資料檔案。只要從’Exploration’ - ‘Descriptives’開啟描述統計設定視窗，建立次數表(frequency table)就能確認。不過，只有描述統計還無法檢測這樣的問題能不能有效區別機器人和人類，我們需要對資料進行更詳細的描述。我們要按照種族區分的各種選擇的次數，也就是建立這份資料的列聯表（cross tabulation，見 小單元 6.1 的介紹）。啟動’Frequencies’ - ‘Contingency Tables’ - ’Independent Samples’的設定視窗就能建立列聯表，成果會如同 表 10.7 。\n\n\n\n\n\n\n表 10.7: Chapek 9資料列聯表\n\n\n\nRobot\nHuman\nTotal\n\n\nPuppy\n13\n15\n28\n\n\nFlower\n30\n13\n43\n\n\nData\n44\n65\n109\n\n\nTotal\n87\n93\n180\n\n\n\n\n\n\n\n\n這份列聯表清楚展現絕大多數人類參與者回答喜歡資料檔案，而機器人參與者對每一項的回答相對平衡。先不管為什麼人類可能更喜歡資料檔案（這確實看起來有點奇怪，承認吧），我們現在的目標是確定這份資料的人類和機器人的選擇差異有沒有統計顯著性。\n\n\n10.2.1 獨立性假設檢定程序\n要如何分析這樣的資料？由於一開始的研究假設是”人類和機器人回答問題的方式並不一樣”，假設檢定程序要測試的虛無假設應該設定為”人類和機器人回答問題的方式是一樣的”？與適合度檢定一樣，首先要定義一些描述資料的符號（表 10.8）。\n\n\n\n\n\n表 10.8: 獨立性假設檢定範例各項資料的符號表\n\n\n\nRobot\nHuman\nTotal\n\n\nPuppy\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(R_{1}\\)\n\n\nFlower\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(R_{2}\\)\n\n\nData\n\\(O_{31}\\)\n\\(O_{32}\\)\n\\(R_{3}\\)\n\n\nTotal\n\\(C_{1}\\)\n\\(C_{2}\\)\nN\n\n\n\n\n\n\n\n\n根據符號表，每個 \\(O_{ij}\\) 代表其中一個種族的受測者j（機器人或人類）所做的選擇 i（小狗，鮮花或資料）之總計次數（觀察次數）。總計次數通常用\\(N\\)表示。然後，\\(R_i\\) 表示各項選擇的總人數，像是\\(R_1\\) 代表選擇鮮花的受測者人數，\\(C_j\\) 表示各種族受測者人數，像是\\(C_1\\) 代表機器人的總數。11\n接著來想想虛無假設的設定。如果機器人和人類對這個問題的回答是一樣的，也就是“機器人選小狗”的機率與“人類選小狗”的機率相同，其他兩個選項的機率也是如此。所以用符號\\(P_{ij}\\) 表示種族j的受測者回答選項i的機率，因此虛無假設就是：\n\\[\n\\begin{aligned}\nH_0 &: \\text{實驗結果符合以下三項：} \\\\\n&P_{11} = P_{12}\\text{ （選擇“小狗”的機率相同），} \\\\\n&P_{21} = P_{22}\\text{ （選擇“鮮花”的機率相同），還有} \\\\\n&P_{31} = P_{32}\\text{ （選擇“資料”的機率相同）}\n\\end{aligned}\n\\]\n其實，因為虛無假設所設定的真實機率不必限定受測者的種族，符號可以再簡化用\\(P_i\\)代表做某個選擇的機率，例如\\(P_1\\)代表選擇小狗的真實機率。\n接下來的程序就和適合度檢定一樣，就是計算期望次數。對應每個觀察次數\\(O_{ij}\\)，需要先搞清楚虛無假設預測每個觀察次數是多少，因此用\\(E_{ij}\\) 表示每個期望次數。這個問題的狀況有點棘手，如果種族 \\(j\\) 有 \\(C_j\\) 人，無論是那個種族的受測者做出什麼選項\\(i\\)的真實機率是\\(P_i\\)，那麼期望次數就是：\n\\[E_{ij}=C_j \\times P_i\\]\n到這一步還算順利，但是遇到了一個問題。與適合度檢定程序不同的是，這裡的虛無假設實際上並未指定\\(P_i\\)的數值。\n用資料估計未知量數是必須的步驟(需要複習的話請回 單元 8 )！幸運的是，這不難做到。若是180位參與者裡有28 位選擇了鮮花，那麼選擇鮮花的機率很自然的估計值就是 \\(\\frac{28}{180}\\)，大約是 \\(0.16\\)。若是用數學式呈現估計選擇i的機率，就是用每行總次數除以總樣本次數：\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\n因此，期望次數可以改寫為各行次數與各列次數的乘積，再除以總觀察次數：12\n\n[額外的技術細節13]\n與前一個檢定程序一樣，\\(\\chi^2\\) 的數值越大，表示虛無假設對資料的解釋越差，而 \\(\\chi^2\\) 的數值越大，表示虛無假設對資料的解釋越好。所以如同前一種檢定程序，如果 \\(\\chi^2\\) 數值太大，就有可能拒絕虛無假設。\n不出聰明的讀者所料，這個檢定統計值遵循 \\(\\chi^2\\) 分佈。現在要做的就是弄清楚有多少自由度，實際上這並不難知道。如同之前提到的，研究人員通常可以將自由度當成正在分析的資料點總數量，減去不可變動的資料點數量。具備 r 行和 c 列的列聯表總共有 \\(r^{c}\\) 個觀察次數，所以這是觀察次數的全部數量。那不可變動的有多少呢？這裡的狀況稍微複雜一些，但是答案始終是相同的\n\\[df=(r-1)(c-1)\\]\n不過要解釋為什麼自由度是這樣算，需要考慮實驗設計。為了方便說明，假如真實資料真的有 87 台機器人和 93 位人類，不過因為選擇是隨機的，讓每行的總次數自由變化，就可以考慮這裡有多少不可變動的資料點。由於題目情境已經限制了每列的總次數，所以有c 個不可變動的資料點。其實還有更多不可變動的資料點，記得虛無假設提到了需要估計的參數\\(P_i\\)，本書雖然不會解釋為什麼這些參數是需要考慮的，現在讀者只要知道虛無假設所列出的參數都是不可變動，如此一來，問題就簡化成這種參數有多少呢？其實很簡單，因為所有機率必須加起來等於 1，所以只有 \\(r - 1\\) 個。因此，卡方獨立性檢定的自由度是：\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\n另一種解釋方式是，假如研究人員唯一確定的數值只有總樣本量 \\(N\\)。也就是說，研究人員對180位受測者進行問卷調查，結果發現 87 位是機器人，93 位是人類。現在的推論方式會不大相同，但仍然會得到相同的答案。虛無假設仍然有 \\(r - 1\\) 個待估計的參數，對應每個回答項目被選擇的機率值，現在要加上\\(c - 1\\) 個待估計的參數，對應受測者被確認是機器人的機率值。14還有，我們確定觀察值的總次數 \\(N\\)，這是另一個不可變動的參數。所以一共有\\(rc\\)項資料點，其中有 \\((c-1)+(r-1)+1\\) 個不可變動的資料點。那正確答案是多少呢？\n這真是太神奇了。\n\n\n\n\n10.2.2 獨立性檢定實作\n好吧，既然我們知道了檢驗是如何進行的，讓我們看看如何在 jamovi 中完成它。雖然讓您長時間地經歷繁瑣的計算以便被迫學習可能很有吸引力，但我認為這是沒有意義的。在上一節中，我已經向您展示了如何針對適合度檢驗進行長時間的操作，而且由於獨立性檢驗在概念上沒有任何不同，所以您不會通過長時間的操作學到任何新的東西。因此，我將直接向您展示簡單的方法。在 jamovi 中運行檢驗（“頻率” - “列聯表” - “獨立樣本”）之後，您只需查看 jamovi 結果窗口中列聯表下方，那裡就是 \\(\\chi^2\\) 統計量。這顯示了一個 \\(\\chi^2\\) 統計值為 10.72，2 d.f.，p-value = 0.005。\n那很簡單，不是嗎？您還可以要求 jamovi 顯示預期計數 - 只需單擊“Cells”選項中的“Counts” - “Expected”複選框，預期計數將出現在列聯表中。同時，在此操作中，效果大小度量會有所幫助。我們將選擇 Cramér’s \\(V\\)，您可以在“Statistics”選項中的複選框中指定它，它會給出 Cramér’s \\(V\\) 的值為 \\(0.24\\)。參見 圖 10.6。我們稍後會再談論這個問題。\n\n\n\n\n\n\n\n圖 10.6: 在 jamovi 中使用 Chapek 9 資料進行獨立樣本 \\(\\chi^2\\) 檢驗\n\n\n\n\n這個輸出為我們提供了足夠的信息來寫出結果：\n\nPearson 的 \\(\\chi^2\\) 顯示了物種和選擇之間存在顯著關聯（\\(\\chi^2(2) = 10.7, p&lt; .01\\)）。機器人似乎更傾向於說他們喜歡花，而人類更傾向於說他們喜歡資料。\n\n注意，再次，我提供了一些解釋，以幫助人類讀者理解資料發生的情況。稍後在我的討論部分，我會提供更多的上下文。舉例來說，這是我可能會在之後說的：\n\n人類似乎比機器人更喜歡原始資料文件，這有點反直覺。但在某種程度上，它是有道理的，因為 Chapek 9 上的民事當局往往在發現人類時會將其殺死並解剖。因此，最有可能的是，人類參與者並未如實回答問題，以避免可能產生不良後果。這應該被認為是一個嚴重的方法論缺陷。\n\n我想，這可以被歸類為反應效應的一個極端例子。顯然，在這種情況下，問題嚴重到研究幾乎毫無價值，作為理解人類和機器人之間的差異偏好的工具。然而，我希望這能夠說明在獲得統計顯著結果（我們拒絕虛無假設，轉而接受替代假設）和找到具有科學價值的東西（由於嚴重的方法論缺陷，資料對我們研究假設的興趣一無所知）之間的區別。"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#連續性校正",
    "href": "10-Categorical-data-analysis.html#連續性校正",
    "title": "10  類別資料分析",
    "section": "10.3 連續性校正",
    "text": "10.3 連續性校正\nOkay, time for a little bit of a digression. I’ve been lying to you a little bit so far. There’s a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It’s called the “continuity correction”, or sometimes the Yates correction. Remember what I pointed out earlier: the \\(\\chi^2\\) test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large \\(N\\). One problem with this is that it often doesn’t quite work, especially when you’ve only got 1 degree of freedom (e.g., when you’re doing a test of independence on a \\(2 \\times 2\\) contingency table). The main reason for this is that the true sampling distribution for the \\(X^{2}\\) statistic is actually discrete (because you’re dealing with categorical data!) but the \\(\\chi^2\\) distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when \\(df = 1\\), the goodness-of-fit statistic tends to be “too big”, meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).\nAs far as I can tell from reading Yates’ paper14, the correction is basically a hack. It’s not derived from any principled theory. Rather, it’s based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the ‘Statistics’ options, where it is called ‘\\(\\chi^2\\) continuity correction’."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#效果量",
    "href": "10-Categorical-data-analysis.html#效果量",
    "title": "10  類別資料分析",
    "section": "10.4 效果量",
    "text": "10.4 效果量\nAs we discussed earlier in Section 9.8, it’s becoming commonplace to ask researchers to report some measure of effect size. So, let’s suppose that you’ve run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?\nThere are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won’t discuss all of them but will instead focus on the most commonly reported measures of effect size.\nBy default, the two measures that people tend to report most frequently are the \\(\\phi\\) statistic and the somewhat superior version, known as Cramér’s \\(V\\) .\n[Additional technical detail 15]\nAnd you’re done. This seems to be a fairly popular measure, presumably because it’s easy to calculate, and it gives answers that aren’t completely silly. With Cramér’s \\(V\\), you know that the value really does range from 0 (no association at all) to 1 (perfect association)."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#檢定方法的適用條件",
    "href": "10-Categorical-data-analysis.html#檢定方法的適用條件",
    "title": "10  類別資料分析",
    "section": "10.5 檢定方法的適用條件",
    "text": "10.5 檢定方法的適用條件\nAll statistical tests make assumptions, and it’s usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:\n\nExpected frequencies are sufficiently large. Remember how in the previous section we saw that the \\(\\chi^2\\) sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in Chapter 7 this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I’ve been able to discover (e.g., Cochran (1954)) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative (Larntz, 1978).\nData are independent of one another. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here’s what I mean. Suppose I’m interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I’d actually walked into the same ward 10 times and in fact I’d only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence “stuffs things up”. Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let’s consider what would happen if I’d done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that everyone selects one heart, one club, one diamond and one spade (in keeping with the “representativeness heuristic” (Tversky & Kahneman, 1974). This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.\n\nIf you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we’ll discuss) or the Cochran test (which we won’t). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn."
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-The-Fisher-test",
    "href": "10-Categorical-data-analysis.html#sec-The-Fisher-test",
    "title": "10  類別資料分析",
    "section": "10.6 費雪精確檢定",
    "text": "10.6 費雪精確檢定\n若是每個觀察次數太小，但還是需要檢驗兩個變項是否獨立的虛無假設，該怎麼辦？一種辦法是“收集更多資料”，但這樣做太隨便了。許多行為科學研究要收集比計畫目標更多的資料不是不可行，就是會違反研究倫理。統計學者有必要幫助研究人員解決倫理困境，提供一種更好的檢定方法。 Fisher (1922) 提出的精確檢定剛好能解決這個況的問題。為了方便說明，我們設定正在分析一個田野實驗的資料，研究對象是被指控為巫師，被綁在火刑柱上的情緒狀態，而且有些柱子已經開始燒起來了。17，雖然鄉民們會因為巫師被燒死而安心，但是研究人員很難找到正在被燒的巫師。如果真的要收集，觀察次數會非常小。salem.csv(lsj資料集Salem)儲存的 資料列聯表表現了這一點（整理結果參見 表 10.9 ）。\n\n\n\n\n\n\n表 10.9: 以Salem 原始資料整理的列聯表\n\n\n\nhappy\nFALSE\nTRUE\n\n\non.fire\nFALSE\n3\n10\n\n\n\nTRUE\n3\n0\n\n\n\n\n\n\n\n\n看了列聯表，研究人員很難不懷疑還沒被火燒的人，他們感覺的情緒可能比正在被火燒的人快樂。不過，因為樣本數很小，卡方檢定很難適用。做為一個不想被處火刑的嫌犯，非常希望有更肯定的答案，這就是費雪精確檢定（Fisher’s exact test）(Fisher, 1922) 能派上用場的地方。\n費雪精確檢定不同於卡方檢定，還有這本書介紹的其他假設檢定方法，因為這套方法沒有檢定統計值，而是“直接”計算 p 值。以下說明費雪精確檢定用在分析 \\(2 \\times 2\\) 列聯表的基本原理，與之前的單元一樣，我們要先定義一些符號（表 10.10）。\n\n\n\n\n\n\n表 10.10: 費雪精確檢定的符號\n\n\n\nHappy\nSad\nTotal\n\n\nSet on fire\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(R_{1}\\)\n\n\nNot set on fire\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(R_{2}\\)\n\n\nTotal\n\\(C_{1}\\)\n\\(C_{2}\\)\n\\(N\\)\n\n\n\n\n\n\n\n\n為了設定檢定程序，費雪將各行及各列的總次數（\\(R_1, R_2, C_1\\) 和 \\(C_2\\)）都視為已知的固定值，然後根據這些總次數不變的前提，計算研究人員會得到實際觀察次數（\\(O_{11}, O_{12}, O_{21}\\) 和 \\(O_{22}\\)）的機率。運用我們在 單元 8 學到的機率符號表達規則，寫成的公式就是：\n\\[P(O_{11}, O_{12}, O_{21}, O_{22} \\text{ | } R_1, R_2, C_1, C_2)\\]\n大部分讀者要搞清楚這是什麼樣的機率分佈是有點困難的任務，實際地說，這個機率函數符合所謂的超幾何分佈(hypergeometric distribution)。要計算 p 值，首先要計算觀察到這個表格內的數值，或者一個數值更“極端”的表格之機率。18在 20 世紀 20 年代，即使是狀況最單純的研究問題，要做到這樣的計算是相當令人害怕的工作，不過現在只要表格不是太大，樣本數不是太龐大，一般計算設備都能輕易處理。真正棘手的問題是，要弄清楚一個列聯表比另一個列聯表更“極端”，是怎樣的概念？最簡單的解決方案是，出現機率最低的表格是最極端的，我們只要知道這個表格的 p 值。\n從 jamovi Analysis界面開啟Frequencies-Independent Samples開啟Contingency Tables設定視窗，其中Statistics子選項裡有Fisher's excat test。若是已經載入Salem資料並設定好變項，就能在報表看到費雪精確檢定的統計結果。因為這種檢定方法只有輸出p值，由於這個範例的p 值足夠小（p = .036），足以拒絕那些正在被處刑的人和還沒有被處刑的人一樣快樂的虛無假設。輸出結果請參考 圖 10.7。\n\n\n\n\n\n\n\n\n圖 10.7: jamovi 費雪精確檢定分析結果"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的校正",
    "href": "10-Categorical-data-analysis.html#卡方檢定的校正",
    "title": "10  類別資料分析",
    "section": "10.3 卡方檢定的校正",
    "text": "10.3 卡方檢定的校正\n好了，是時候稍微離題一下下，其實談到這裡，我隱瞞了一點細節沒有交待。自由度只有 1 的時候，需要稍微改變計算方法。這被稱為 “連續性修正(continuity correction)”，或者葉氏修正(Yates correction)。記得稍早提到：\\(\\chi^2\\) 檢定的統計值是一種近似值，嚴格來說是假設當 \\(N\\)的值夠大時，二項分佈會逼近常態分佈。然這依賴這條假設的問題是，現實的研究問題很少遇到這種條件，尤其是自由度只有 1 的時候，像是用\\(2 \\times 2\\)規模的列聯表進行獨立性檢定。無法保證這種資料的機率分佈逼近常態分佈，原因是處理的資料是分類資料！所以\\(\\chi^2\\)統計值的真實樣本分佈其實是離散的，但是\\(\\chi^2\\) 分佈是連續的。如此會導致系統性問題：當\\(N\\)很小且 \\(df = 1\\) 時，適合度統計值往往”過大”，需要比原來預設更大的\\(\\alpha\\)，或者更小的p值，才能正確拒絕虛無假設。\n根據描述葉氏修正的論文主張，這種修正其實是一種操作方法的破解。15因為修正方法不是基於任何機率或統計理論，而是檢討多數分析人員的操作行為，經過測試發現經過校正後的統計值似乎比較正確。讀者可以在 jamovi 卡方檢定模組的操作視窗， 於’Statistics’這個部分勾選$\\chi^2$ continuity correction，就能輸出校正後的統計值。"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的效果量",
    "href": "10-Categorical-data-analysis.html#卡方檢定的效果量",
    "title": "10  類別資料分析",
    "section": "10.4 卡方檢定的效果量",
    "text": "10.4 卡方檢定的效果量\n稍早在 小單元 9.8 提到，現在各種要求研究人員在報告中說明效果量測量指標的規範越來越普遍。若是現在已經完成了卡方檢定，結果顯示有統計顯著性，表示我們能肯定所探討的變項之間存在某種關聯（獨立性檢定）或特定條件之間存在某種差異（適合度檢定）。那麼要如何報告其中的效應量強度，也就是變項條件之間關聯性或差異程度？\n有好幾種不同的測量指標可在報告裡呈現，而且有好幾種公式和程式能做計算。這裡不會介紹所有可用的測量指標，只重點介紹最多報告使用的效果量測量指標。最常見於研究報告，以及最多統計軟體預設計算的指標是 \\(\\phi\\)，以及克拉默氏\\(V\\)(Cramér’s V)。\n[額外的技術細節16]\n讀者很容易就能算出兩種效果量測量值。這兩種效果量指標相當受歡迎，可能是因為容易計算，又不會讓人覺得有虛應故事的感覺。用了Cramér’s ，還能確認變項之間的相關程度確實在 0（完全無關聯）和 1（完全關聯）。"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#卡方檢定的適用條件",
    "href": "10-Categorical-data-analysis.html#卡方檢定的適用條件",
    "title": "10  類別資料分析",
    "section": "10.5 卡方檢定的適用條件",
    "text": "10.5 卡方檢定的適用條件\n所有統計檢定都有適用條件，檢查手上的資料是否符合適用條件是一個好主意。至此討論過的卡方適合度檢定及獨立性檢定，適用條件包括：\n\n期望次數足夠大。還記得 小單元 10.2 提到 \\(\\chi^2\\) 統計值的機率分佈是如何產生的嗎？因為二項分佈非常逼近常態分佈，正如 單元 8 所提到的，只有在觀察次數足夠多的狀況下才會成立，這表示所有可用卡方檢定的資料期望次數都需要大到一個合理程度。那什麼是合理程度？目前學術界意見分歧，不過最起碼的共識是列聯表裡的所有期望次數至少要大於5。對於規模較大的表格，還可以期待至少有80%的期望次數大於5，並且沒有一個項目的期望次數小於1。然而，就原作者所找到的文獻（像是 Cochran (1954) ），這些條件設定似乎只是粗略的指導原則，而非硬性規定，並且主張似乎有些保守 (Larntz, 1978)。\n資料彼此獨立。另一個卡方檢定稍微不嚴格的適用條件 是，研究人員必須相信每一項觀察結果是彼此獨立的。例如，我偶然得知某家醫院出生的嬰兒性別比例似乎很懸殊，我私下走訪這家醫院的產房，紀錄到20名女嬰和10名男嬰，看起來性別比例差異很大對吧？其實這個紀錄是我分別在十個不同的日子去參觀，每次都只看到了2名女嬰和1名男嬰。這樣的紀錄不再那麼令人信服了，是吧？表面上的30筆觀察完全不彼此獨立，實際上只相尚於3個彼此獨立的觀察結果。這顯然是一個非常極端而且容易看出缺陷的例子，但其中指出了獨立性的基本問題。有時違反獨立性的資料，就像前述的走訪醫院的例子那樣，會導致研究人員錯誤地拒絕虛無假設，但是也可能朝相反的方向發展。為了幫助讀者認識不容易看出缺陷的狀況，這裡用 小單元 10.1 提到的撲克牌實驗做一些修改來說明。假如研究人員不是要求200個人各自想像隨機抽出一張撲克牌，而是要求50個人各自選出4張撲克牌，其中一種可能的結果是每個人都會善用“代表性捷思法”(Tversky & Kahneman, 1974)，抽出一張紅心、一張梅花、一張方塊和一張黑桃。這是人類自主意識造成的非隨機行為，但在這種實驗狀況，研究人員能預期四種花色都會出現50次觀察次數。因為參與者的四次選擇彼此關聯，破壞每次觀察結果的獨立性，導致統計檢定結果將錯誤地保留了虛無假設。\n\n如果讀者懷疑手上的資料違反獨立性的條件，可以使用稍後會介紹的麥內瑪檢定( McNemar Test)或者本書不會介紹的 Cochran 檢定加以確認。若是覺得期望次數太小，可以使用費雪精確檢定。這一章最後兩個部分將簡單介紹費雪精確檢定與麥內瑪檢定。"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#本章小結",
    "href": "10-Categorical-data-analysis.html#本章小結",
    "title": "10  類別資料分析",
    "section": "10.8 本章小結",
    "text": "10.8 本章小結\n本章的學習重點有：\n\n卡方適合度檢定用於你的表列資料是來自不同類別的觀察次數，虛無假設是可相互比較的已知機率。\n卡方獨立性或關聯性檢定用於你的資料是能化為列聯表的觀察次數。虛無假設是兩種變項之間沒有關聯性。\n列聯表的效果量有多種測量方法。在此介紹最常見的Cramér’s V。\n上述的卡方檢定法有兩種適用條件：期望值次數夠大，觀察值彼此獨立。如果期望值次數不夠大，可以使用費雪精確檢定；如果觀察值並非彼此獨立，可以使用麥內瑪檢定。\n\n如果想學習更多類別資料分析方法，推薦閱讀 Agresti (1996) 的專書”類別資料分析導論”。如果基礎教科書無法滿足你的需要，或者未提供解決手上問題的方法，可以參考 Agresti (2002) 的進階書藉。因為是進階書，建議先充分理解導論再來學習進階教科書。\n\n\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "11-Comparing-two-means.html#單一樣本z檢定",
    "href": "11-Comparing-two-means.html#單一樣本z檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.1 單一樣本z檢定",
    "text": "11.1 單一樣本z檢定\n\n這一節要介紹統計學裡最無用的一種統計檢定方法：z 檢定。嚴格來說，這種檢定方法幾乎沒什麼研究人員會使用。學習z檢定的真正用途是，這個方法是學習 t 檢定前相當方便的墊腳石，但是 t 檢定可能是最被過度使用的統計學工具。\n\n\n11.1.1 使用z檢定前的注意事項\n我們先用一個簡單的例子介紹 z 檢定如何運作的概念：我有一位朋友 Zeppo 老師準備按照常態分佈曲線為他的基礎統計學課程修課學生打分數。假如這門課程的平均分數是 \\(67.5\\)，標準差是 \\(9.5\\)，修課學生有數百人，其中有 20 位還修了心理學課程。出於好奇心，我想知道這些學生的成績是否與其他學生的成績一致，也就是這20位學生的平均分數也是 \\(67.5\\)，還是他們的成績比全體平均分數更高或更低？這些學生的成績存於Zeppo老師分享給我的 zeppo.csv 資料檔(lsj資料集Zeppo)，開啟 jamovi 的 ‘Exploration’ - ’Descriptives’的設定視窗，按照 單元 4 學過的方法，就能得到平均值為 \\(72.3\\)。2\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\n看起來有修心理學課程的學生成績比全部同學的好一些。樣本平均值 \\(\\bar{X}=72.3\\) 比假設的母群平均值 \\(\\mu=67.5\\) 要高了不少分，但是，樣本量 \\(N=20\\) 其實並不是很大，也許這只是純粹的巧合。\n為了能做出合理的評估，寫出我所知道的細節能幫助讀者了解。首先知道的是樣本平均值為 \\(\\bar{X}=72.3\\)，如果我相信這20位學生的標準差與班上其他學生的標準差相同，那麼母群標準差就是 \\(\\sigma=9.5\\)。同時我還相信 Zeppo 老師是按常態分佈曲線給分，這20位學生的成績分佈也應該符合常態分佈。\n接著我要具體呈現如何從資料取得能做判斷的資訊。我對此問題的假設可用這20位學生來源母群平均值 \\(\\mu\\) 表示，而這個值是未知的。在有資料的當下，我想知道是不是 \\(\\mu=67.5\\) ？因為這是我目前所知道的，需要設定一個假設檢定程序來解答這個問題。樣本資料以及可能的來源母群分佈顯示在 圖 11.1 。畫出統計圖還不能解答我的問題嗎？ 現在需要學習更進一步的統計知識了。\n\n\n\n\n\n\n圖 11.1: 20位學生的統計課成績（直方圖）與理論的來源母群分佈（實線）。\n\n\n\n\n\n\n11.1.2 z檢定的統計假設\n假設檢定的第一步是確定虛無假設和對立假設分表是代表什麼結果，以這個問題來說不難了解。這裡的虛無假設 \\(H_0\\) 是，心理系學生的成績母群平均值 \\(\\mu\\) 等於 \\(67.5\\%\\)，對立假設是母群平均值不等於 \\(67.5\\%\\)。若是用數學符號表示，這些假設的白話說明就變成了：\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\n不過，使用數學記號表示，其實不大能讓大家更好理解統計假設，這只是呈現完成資料分析後會得到什麼簡記方法。要被檢定的虛無假設 \\(H_0\\) 和對立假設 \\(H_1\\) 的視覺圖像都呈現在 圖 11.2 。配合上一段的背景說明，視覺圖像除了能我們了解統計假設的重點在於結果是符合那個平均值(或者說期望值)，還有提供一些有用的資訊，特別是以下兩點：：\n\n20位學生成績分佈符合常態分佈。\n20位學生成績分佈的真實標準差 \\(\\sigma\\) 等於 9.5。\n\n我們暫時先將這些條件當成是絕對沒錯的事實，許多現實世界的研究問題，這類絕對可信的背景條件通常是不可能成立的。若是要靠這些條件執行假設檢定，只能當做所有條件都是成立的。要確認這些條件是否成立，其實需要做些檢查程序，只是為了方便解說z檢定程序，目前先暫時跳過。\n\n\n\n\n\n\n圖 11.2: 以單一樣本 \\(z\\) 檢定檢測的虛無假設和對立假設的視覺圖解，此例進行雙側檢定。虛無假設和對立假設都根據母群分佈是常態分佈，以及母群標準差（ \\(\\sigma_0\\)）為已知數值等條件成立。虛無假設（左）的 期望值 （\\(\\mu\\)） 等於母群平均值 （\\(\\mu_0\\)）。對立假設（右）的期望值不等於母群平均值( \\(\\mu \\neq \\mu_0\\))。\n\n\n\n\n接下來是設定一個可靠的檢定統計量，用來診斷成績分佈是符合\\(H_0\\) 還是 \\(H_1\\)。因為兩種假設都與母群平均值 \\(\\mu\\)有關，樣本平均值 \\(\\bar{X}\\)是我們能用來檢定的絕佳參考點，也就是計算樣本平均值 \\(\\bar{X}\\) 與虛無假設預測的母群平均值之間的差異，在這個例子是 \\(\\bar{X} - 67.5\\)。若要類推到同樣的分析問題，以\\(\\mu_0\\) 代指虛無假設主張的母群平均值，這樣一來要計算的就是\n\\[\\bar{X}-\\mu_0\\]\n如果這個差異值等於或非常接近於0，虛無假設似乎是可接受的結論。如果這個差異值完全不等於0，那麼虛無假設似乎不太可能是有意義的結論。不過，這個差異離0多遠才有可能拒絕 \\(H_0\\) 呢？\n為了搞清楚，我們需要耍一點小手段，就是之前提到的樣本資料服從常態分佈，以及母群標準差 \\(\\sigma\\) 的值是已知的。若是成績分佈確實符合虛無假設，真正的平均值確實是 \\(\\mu_0\\)，這些事實支持母群分布是平均值為 \\(\\mu_0\\)，標準差為 \\(\\sigma\\) 的常態分佈。3\n好啦，若真的那麼順利， \\(\\bar{X}\\) 的分佈能告訴我們什麼事情呢？根據 小單元 8.3.3 討論過的中央極限定理，平均數 \\(\\bar{X}\\) 的取樣分佈也是常態分佈，並且等於母群平均值 \\(\\mu\\)。只是這個取樣分佈的標準差 \\(se(\\bar{X})\\)–也被稱為平均值的標準誤差–真面目是4\n\\[se(\\bar{X})=\\frac{\\sigma}{\\sqrt{N}}\\]\n讓我們進一步了解這個技巧。運用 小單元 4.5 學過的標準分數，樣本平均數 \\(\\bar{X}\\)可以轉換成標準分數，因為通常被寫成z，這裡可以改寫為 \\(z_{\\bar{X}}\\)。改寫符號是為了方便記住，這裡正在計算的是樣本平均值的標準分數，不是單一觀察值的標準分數。如此一來，樣本平均值的 z 分數\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\]\n也可以寫成\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n這個 z 分數就是這裡要用的檢定統計值，使用這個檢定統計值的好處是，像所有 z 分數一樣，它符合標準常態分佈：5\n\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n換句話說，無論原始資料的測量尺度是什麼，解讀z統計值的方法都是一樣的：z值代表觀察到的樣本平均值 \\(\\bar{X}\\) 與虛無假設預測的母群平均值 \\(\\mu_0\\)相差了多少標準誤。更棒的是，無論原始分數的母群參數是多少，z 檢定的 5% 棄絕域始終不變，如同 圖 11.3 的圖解。所以在以前大多數研究者和學生都要用手算統計的時代，可以從 表 11.1 找出要報告的p值。也可以反過來使用手算得出的z統計值，從教科書的附表找出臨界值。\n\n\n\n\n\n\n表 11.1: 各種顯著水準的臨界值\n\n\n\ncritical z value\n\n\ndesired \\(\\alpha\\) level\ntwo-sided test\none-sided test\n\n\n.1\n1.644854\n1.281552\n\n\n.05\n1.959964\n1.644854\n\n\n.01\n2.575829\n2.326348\n\n\n.001\n3.290527\n3.090232\n\n\n\n\n\n\n\n\n\n\n11.1.3 手作z檢定\n現在，正如我之前提到的，z-test在實際應用中幾乎從不使用。這個測試在實際生活中如此罕見，以至於jamovi的基本安裝不包含內置功能。然而，這個測試是如此地簡單，以至於手動進行這個測試非常容易。讓我們回到Dr Zeppo課程的資料。在載入成績資料後，我需要做的第一件事是計算樣本均值，我已經做到了(\\(72.3\\))。我們已經有了已知的母群標準差(\\(\\sigma = 9.5\\))，零假設所指定的母群平均值(\\(\\mu_0 = 67.5\\))的值，以及樣本大小(\\(N=20\\))。\n\n\n\n\n\n\n圖 11.3: 雙尾 z-檢定 (面板(a)) 和單尾 z-檢定 (面板(b)) 的拒絕區域\n\n\n\n\n接下來，讓我們計算（真實）標準誤（可輕鬆用計算機完成）：\n\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\n最後，我們計算我們的z分數：\n\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\n此時，我們會傳統地在臨界值表中查詢 \\(2.26\\) 的值。我們原來的假設是雙邊的（我們對心理學生在統計學上表現得比其他學生好或差沒有任何理論基礎），因此我們的假設檢驗也是雙邊的（或者是兩尾的）。從我先前顯示的小表中，我們可以看到 \\(2.26\\) 大於需要在 \\(\\alpha = .05\\) 的顯著性水平下具有顯著性的臨界值 \\(1.96\\)，但小於需要在 \\(\\alpha = .01\\) 的水平下具有顯著性的值 \\(2.58\\)。因此，我們可以得出結論，我們可以這樣寫：\n\n在心理學學生的樣本中，平均成績為 \\(73.2\\)，假定真正的人口標準差為 \\(9.5\\)，我們可以得出結論，心理學學生在統計學上的得分與課程平均分有顯著差異（\\(z = 2.26, N = 20, p&lt;.05\\)）。\n\n\n\n\n\n11.1.4 z檢定的適用條件\n正如先前所述，所有統計檢定皆建立於特定的假設條件之上，這些假設有的是基於合理的考量，有的則可能不太合適。這裡介紹的「單樣本z檢定」也有以下三條基礎假設條件：\n\n常態分佈假設。一般來說，z檢定必須假定母群分佈符合常態分佈。6這是一個相對合理的假設，若有疑慮，我們有方法可以進行驗證（請參考 小單元 11.9 ）。\n獨立性假設。第二條要求構成資料集的觀察值互不相關，或者說，觀察值之間的相關性沒有某種特定模式。這一點不容易直接用統計方法檢驗，而是要依賴嚴謹的實驗設計確保此條件成立。一個明顯違反此條件的不合理例子是，在資料集多次「複製貼上」同一個觀察值，導致得到的資料是基於單一測量結果的巨大“樣本”。實際上，每一個觀察值應該是從目標群體裡完全隨機選出，分別評估而得到的。這項條件在有些研究實務難以完全滿足，但是能透過精心安排的研究設計儘可能減少觀察資料的相關性對結果造成的影響。\n母群標準差已知。z檢定的第三個條件是，研究者必須知道母群的真實標準差。這個條件實在是難以滿足。在現實世界的資料分析中，極少遇到可知母群標準差\\(\\sigma\\)，卻對母群平均值\\(\\mu\\)一無所知的情況。也就是這個條件基本上難以成立。\n\n有鑒於假設 \\(\\sigma\\) 已知的荒謬性，我們需要知道z檢定之外的統計方法。接著我們要離開z檢定這個枯燥領域，走入有獨角獸、仙女和小矮人的神奇王國，就是t檢定！"
  },
  {
    "objectID": "11-Comparing-two-means.html#平均數檢定的更多細節",
    "href": "11-Comparing-two-means.html#平均數檢定的更多細節",
    "title": "11  比較單一與兩組平均值",
    "section": "11.4 平均數檢定的更多細節",
    "text": "11.4 平均數檢定的更多細節\n無論你想以哪種方式思考，現在我們已經有了我們的樣本標準差的匯集估計值。從現在開始，我會忽略那個奇怪的p小標，只稱這個估計值為\\(\\hat{\\sigma}\\)。很好。現在讓我們回到考慮這個該死的假設檢驗，好嗎？我們計算這個匯集估計的整個原因是我們知道它將在計算標準誤時非常有用。但標準誤是什麼？在單樣本t檢驗中，它是樣本平均值的標準誤，\\(se(\\bar{X})\\)，因為\\(se(\\bar{X}) = \\frac{\\sigma}{\\sqrt{N}}\\)，這就是我們的t統計量的分母。然而，這一次，我們有兩個樣本均值。而我們特別感興趣的是兩者之間的差異\\(\\bar{X}_1-\\bar{X}_2\\)，因此，我們需要除以的標準誤實際上是均值差的標準誤。\n[其他技術細節12]\n正如我們在單樣本檢定中所看到的那樣，只要虛無假設為真且測試的所有假設都符合，這個 t 統計量的取樣分佈就是一個 t 分佈。然而，自由度略有不同。通常，我們可以把自由度視為資料點數減去約束數。在這個情況下，我們有 N 個觀察值（\\(N_1\\) 在樣本 1 中，\\(N_2\\) 在樣本 2 中），和 2 個約束（樣本均值）。因此，這個檢定的總自由度為 \\(N-2\\)。\n\n\n11.4.1 jamovi實作\n不出所料，您可以很容易地在 jamovi 中進行獨立樣本 t 檢定。我們測試的結果變量是學生成績，而組是根據每個課程的導師定義的。因此，您可能不會對 jamovi 中的相應分析（“分析”-“T 檢定”-“獨立樣本 T 檢定”）感到意外，只需將成績變量移動到“依變變量”框中，將導師變量移動到“分組變量”框中，如 圖 11.9 所示。\n\n\n\n\n\n\n圖 11.9: jamovi執行獨立樣本t檢定示範，請留意圖中勾選的項目。\n\n\n\n\n這裡的輸出形式非常熟悉。首先，它告訴您運行的是什麼測試，以及使用的依變量的名稱。然後報告測試結果。與上次一樣，測試結果包括 t 統計量、自由度和 p 值。最後一部分報告了兩件事：它給出了置信區間和效應大小。我會稍後談論效應大小。然而，現在我應該談論置信區間。\n很重要要清楚這個信賴區間到底是什麼意思，它是兩組平均數之間的差異的信賴區間。在這個例子中，Anastasia的學生平均分為74.53，而Bernadette的學生平均分為69.06，因此兩組平均數之間的差異是5.48。但是，兩個母群的平均數差異可能比這個還大或者還小。在@fig-fig11-10中報告的信賴區間告訴你，如果我們重複進行這個研究，那麼在95％的時間裡，真正的平均數差異會在0.20到10.76之間。回顧一下@sec-Estimating-a-confidence-interval了解信賴區間的意思。\n在任何情況下，兩組之間的差異是顯著的（僅僅）。因此，我們可以使用以下文本編寫結果:\n\nAnastasia 課程的平均成績為 \\(74.5\\%\\)（標準差為 \\(9.0\\)），而 Bernadette 課程的平均成績為 \\(69.1\\%\\)（標準差為 \\(5.8\\)）。 學生的獨立樣本 t 檢驗顯示此 \\(5.4\\%\\) 差異是顯著的 \\((t(31) = 2.1, p&lt;.05, CI_{95} = [0.2, 10.8], d = .74)\\)，表明學習成果存在真正的差異。\n\n注意到我在統計區塊中包含了信心區間和效應大小。人們並不總是這樣做。至少，您應該期望看到t統計量、自由度和p值。所以您應該至少包含像這樣的內容：\\(t(31) = 2.1, p&lt; .05\\)。如果統計學家得到了他們想要的結果，那麼每個人都會報告信心區間，可能還有效應大小測量，因為它們是有用的知識。但現實並不總是按照統計學家的期望運作，所以您應該根據您認為會幫助讀者的情況進行判斷，如果您正在撰寫科學論文，那麼應根據期刊的編輯標準進行決策。有些期刊希望您報告效應大小，而有些則不是。在某些科學社區中，報告信心區間是標準做法，在其他社區中則不是。您需要找出您的觀眾期望什麼。但是，僅僅為了清晰起見，如果您正在上我的課，我的默認立場是通常值得包括效應大小和信心區間。\n\n\n\n11.4.2 t統計值正負的意義\n在談到t-test的假設之前，我們先來討論t-test在實務上的使用上還有一個重要的觀點。第一個觀點是關於t統計量的符號（即它是正數還是負數）。當學生第一次進行t-test時，他們常常會擔心結果出現負值，不知道如何解釋。實際上，當兩個人獨立地進行實驗時，獲得的結果幾乎相同，但其中一個人的t值是負的，而另一個人的t值是正的，這種情況是很常見的。假設您正在進行雙側檢定，那麼p值是相同的。仔細檢查後，學生們會發現置信區間也是相反的。這是完全正常的。每當這種情況發生時，你會發現兩種不同的結果是由稍微不同的t-test方法引起的。這裡發生的事情非常簡單。我們在這裡計算的t統計量總是具有以下形式\n\\[t=\\frac{\\text{平均值1-平均值2}}{SE}\\]\n如果 “平均值 1” 大於 “平均值 2”，則 t 統計量將為正數，而如果 “平均值 2” 大於 “平均值 1”，則 t 統計量將為負數。同樣地，jamovi 報告的置信區間是 “(平均值 1) 減去 (平均值 2)” 差異的置信區間，這將是計算 “(平均值 2) 減去 (平均值 1)” 差異置信區間的相反。\n好的，當你考慮這件事時，其實很簡單，但現在考慮比較阿納斯塔西亞班和伯納德班的 t 檢定。我們應該把哪一個稱為 “mean 1”，哪一個稱為 “mean 2”。這是任意的。然而，你真的需要將其中之一指定為 “mean 1”，另一個為 “mean 2”。不出所料，jamovi 處理這個的方式也相當任意。在本書早期的版本中，我試圖解釋這一點，但過了一段時間後，我放棄了，因為這並不是真正重要的事情，老實說我自己也記不清楚。每當我得到一個顯著的 t 檢定結果，並且我想找出哪個均值較大，我不會嘗試通過查看 t 統計量來找出答案。我為什麼要這麼做？這是愚蠢的。最簡單的方法就是查看實際的組均值，因為 jamovi 的輸出實際上就顯示了它們！\n這是重要的一點。因為 jamovi 向您顯示的內容實際上並不重要，所以我通常會嘗試以使數字與文本相符的方式報告 t 統計量。假設我想在報告中寫道：「阿納斯塔西亞的課程的成績高於伯納德的課程。」這種措辭意味著阿納斯塔西亞的組排在第一位，因此把阿納斯塔西亞的課程對應到第一組是有道理的。如果是這樣，我會寫成 阿納斯塔西亞的班级成绩比伯纳德的班级更高\\((t(31) = 2.1，p = .04)\\)。\n（在現實中我不會真的用下劃線來強調 “higher”，我只是這樣做是為了強調 “higher” 與正的 t 值對應）。另一方面，假設我想使用的措辭是 Bernadette 的課程排在第一位，如果這樣的話，把她的課程作為第一組更有意義，這樣的話報告應該寫成：Bernadette 的課程的成績比 Anastasia 的課程更低 \\((t(31) = -2.1, p = .04)\\)。\n最後要注意的是，這種寫法只適用於t檢定，對於卡方檢定、F檢定或本書中提到的大多數檢定而言並無意義。因此不要過度將這個建議泛化！這裡我真的只是在談論t檢定，而不是其他任何東西！\n\n\n\n11.4.3 獨立樣本平均數檢定的適用條件\n和單樣本t檢定一樣，學生t檢定也有三個假設，其中一些在單一樣本t檢定的適用條件已經提到：\n\n常態性。和單樣本t檢定一樣，需要假設資料是常態分佈。具體來說，需要假設兩組資料都符合常態分佈。13在[檢查樣本正態性]一節中，我們將討論如何檢查正態性，而在[檢驗非正態資料]一節中，我們將討論可能的解決方案。\n獨立性。再次需要假設觀測值是獨立抽樣的。在學生t檢定的情況下，有兩個方面需要考慮。首先，需要假設每個樣本內的觀測值是相互獨立的（和單樣本t檢定一樣）。但是，我們還需要假設兩個樣本之間沒有相互依賴關係。例如，如果實驗中同一個人被不小心分配到不同的條件下（例如，同一個人不小心被允許報名參加不同的條件下的實驗），那麼就存在一些跨樣本的依賴關係需要考慮。\n方差的同質性（也稱為“等變異性”）。第三個假設是，兩組的母群標準差是相同的。可以使用Levene檢定來測試這個假設，我們稍後會在[檢查同質變異性的假設]中談到。然而，如果您擔心這個假設，有一個非常簡單的解決方法，我們將在下一節中討論。"
  },
  {
    "objectID": "11-Comparing-two-means.html#相依樣本t檢定",
    "href": "11-Comparing-two-means.html#相依樣本t檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.6 相依樣本t檢定",
    "text": "11.6 相依樣本t檢定\n不論是談論學生 t 檢驗或韋爾奇 t 檢驗，獨立樣本 t 檢驗都適用於具有兩個互相獨立樣本的情況。當參與者被隨機分配到其中一個實驗條件時，自然會出現這種情況，但這對於其他類型的研究設計提供了一個非常差的近似。特別是，在重複測量設計中，每個參與者都在兩個實驗條件下測量（對於相同的結果變量），並不適合使用獨立樣本 t 檢驗進行分析。例如，我們可能會對聆聽音樂是否降低人們的工作記憶容量感興趣。為此，我們可以在兩種情況下測量每個人的工作記憶容量：有音樂和無音樂。在這樣的實驗設計中，18每個參與者出現在 兩個 組中。這需要我們以不同的方式來解決問題，即使用配對樣本 t 檢驗。\n\n\n11.6.1 示範資料\n這次我們要使用的數據集來自Chico博士的班級。19 在她的課堂上，學生要參加兩次主要考試，一次在學期初，一次在學期後。據她所說，她開的課很難，大多數學生都覺得很有挑戰性。但她認為，通過設置困難的評估，學生會被鼓勵更加努力地學習。她的理論是，第一次考試對學生來說是一個“提醒”，當他們意識到她的課有多難時，他們會為第二次考試更加努力，取得更好的成績。她的觀點正確嗎？為了測試這個問題，讓我們將chico.csv文件導入到jamovi中。這次，jamovi在導入期間做了一個好工作，正確地分類了變量的測量水平。chico數據集包含三個變量：一個id變量，用於識別班級中的每個學生，grade_test1變量記錄第一次考試的學生成績，grade_test2變量則是第二次考試的成績。\n如果我們看一下 jamovi 的試算表，似乎這個班級很難（大多數的成績都在50%到60%之間），但從第一次測驗到第二次測驗似乎有進步的趨勢。\n如果我們快速查看一下描述性統計，在@fig-fig11-12中，我們可以看到這種印象似乎得到了支持。在所有20個學生中，第一次測驗的平均成績為57%，但第二次測驗的平均成績為58%。但是，考慮到標準差分別為6.6%和6.4%，這種進步感覺起來可能只是虛假的，也可能只是隨機變異。當你看到在@fig-fig11-13a中繪製的平均值和置信區間時，這種印象得到了加強。如果我們僅僅依靠這個圖表，看看這些置信區間有多寬，我們會認為學生表現的明顯改善純粹是偶然的。\n\n\n\n\n\n\nFigure 11.12: chico資料集中的兩次測驗成績資料及描述統計\n\n\n\n\n然而，這種印象是錯的。要知道原因，請看圖 Figure 11.13 (b) 中顯示的評分1和評分2的散佈圖。在這個圖中，每個點對應於一個給定學生的兩個成績。如果他們的評分1（x座標）等於他們的評分2（y座標），那麼該點就會落在直線上。在線上方的點是第二次測試表現更好的學生。重要的是，幾乎所有的數據點都在對角線以上：幾乎所有的學生似乎都有一些提高，即使只有一點點。這表明我們應該關注每個學生在一次測試和下一次測試中所取得的進步，並將其作為我們的原始數據。為此，我們需要創建一個新變量，用於表示每個學生所取得的進步，並將其添加到 chico 數據集中。最簡單的方法是計算一個新變量，使用表達式 grade test2 - grade test1。\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 11.13: 第一次測驗和第二次測驗的平均分數，以及相應的95％置信區間(a)。顯示第一次測驗和第二次測驗的個別分數的散點圖(b)。 \n\n\n我們一旦計算了這個新的改進變量，就可以繪製一個直方圖，顯示這些改進分數的分布，如@fig-fig11-14所示。當我們觀察這個直方圖時，很明顯這裡有真正的進步。絕大多數的學生在第二次測試中得分比第一次高，這反映在幾乎整個直方圖都在零以上。\n\n\n\n\nFigure 11.14: 柱狀圖顯示了 Chico 博士班上每位學生的成績提升。注意到幾乎整個分布都在 0 的上方——大多數學生在第二次考試中的表現確實有所提升。\n\n\n\n\n11.6.2 深入認識相依樣本t檢定\n根據先前的探索，讓我們思考如何建立一個適當的 t 檢定。一個可能的方法是嘗試使用grade_test1和grade_test2作為感興趣的變量進行獨立樣本 t 檢定。然而，這顯然是錯誤的，因為獨立樣本 t 檢定假定兩個樣本之間沒有特定的關係。然而，由於數據中的重複測量結構，這顯然是不正確的。使用我在上一節中介紹的語言，如果我們試圖進行獨立樣本 t 檢定，我們會混淆 樣本內 差異（這是我們想要測試的）和 樣本間 變異性（這是我們不想要的）。\n解決這個問題的方法很明顯，我希望，因為我們已經在前一節中完成了所有的艱苦工作。我們不是對grade_test1和grade_test2進行獨立樣本t檢驗，而是對內部差異變量improvement進行單樣本t檢驗。稍微形式化一下，如果\\(X_{i1}\\)是第i個參與者在第一個變量上獲得的分數，\\(X_{i2}\\)是同一個人在第二個變量上獲得的分數，那麼差異分數是：\n\\[D_i=X_{i1}-X_{i2}\\]\n請注意，差異分數是變量1減去變量2，而不是反過來，因此如果我們希望改進對應到一個正值的差異，我們實際上需要將「測試2」作為「變量1」。同樣，我們會說 \\(\\mu_D = \\mu_1 - \\mu_2\\) 是此差異變量的母體平均值。因此，為了將其轉換為假設檢驗，我們的虛無假設是此平均差異為零，而對立假設是它不是\n\\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]\n這假設我們進行的是雙邊檢定。這與我們描述一樣，進行一樣的假設檢定。唯一的不同之處在於零假設所預測的特定值為0。因此，我們的 t 統計量可以用類似的方式來定義。如果我們讓 \\(\\bar{D}\\) 代表差異得分的平均值，那麼\n\\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\]\n其中 \\(\\hat{\\sigma}_D\\) 是差異得分的標準差。因為這只是一個普通的單樣本 t 檢定，沒有什麼特別的地方，所以自由度仍然是 \\(N - 1\\)。這就是全部。實際上，配對樣本 t 檢定並不是一個新的檢定，它是一個應用在兩個變數之間差異上的單樣本 t 檢定。它實際上非常簡單。它需要進行長時間的討論的唯一原因是，您需要能夠辨認何時適用配對樣本 t 檢定，以及為什麼它比獨立樣本 t 檢定更好。\n\n\n\n11.6.3 實作相依樣本t檢定\n如何在jamovi中執行配對樣本t檢定？一種可能的方法是按照我上面概述的過程進行。即，創建一個“差異”變量，然後對其進行單樣本t檢定。因為我們已經創建了一個名為improvement的變量，讓我們這麼做並看看我們得到什麼，如@fig-fig11-15所示。\n\n\n\n\n\n\nFigure 11.15: 配對差異分數的單一樣本t檢定結果表。\n\n\n\n\n在@fig-fig11-15中顯示的輸出與上一次使用單樣本t檢定分析時（Section 11.2）完全相同的格式，並確認了我們的直覺。從第一次測試到第二次測試，平均改善了1.4％，並且這與0顯著不同\\((t(19)=6.48,p<.001)\\)。\n然而，假設您很懶，不想花費大量精力創建新變量。或者也許您只想保持單樣本和配對樣本測試之間的差異清晰。如果是這樣，您可以使用jamovi的“配對樣本t檢定”分析，獲得@fig-fig11-16中顯示的結果。\n\n\n\n\n\n\nFigure 11.16: 相依樣本t檢定結果表。請與 Figure 11.15 比較看看。\n\n\n\n\n結果跟進行單樣本t檢定時的結果相同，這當然是因為成對樣本t檢定在實質上就是對差異變數執行單樣本t檢定。"
  },
  {
    "objectID": "11-Comparing-two-means.html#單尾檢定",
    "href": "11-Comparing-two-means.html#單尾檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.7 單尾檢定",
    "text": "11.7 單尾檢定\n當介紹零假設檢定理論時，我提到有一些情況適合指定單邊檢定（請參閱 Section 9.4.3 ）。到目前為止，所有的t檢定都是雙邊檢定。例如，當我們為Zeppo博士課堂上的成績指定單一樣本t檢定時，零假設是真實均值為\\(67.5\\%\\)。對立假設是真實均值高於或低於\\(67.5\\%\\)。假設我們只想知道真實均值是否高於\\(67.5\\%\\)，並且完全沒有興趣測試真實均值是否低於\\(67.5\\%\\)。如果是這樣，我們的零假設將是真實均值為\\(67.5\\%\\)或更低，對立假設將是真實均值高於\\(67.5\\%\\)。在jamovi中，對於“單一樣本t檢定”分析，您可以在“假設”下點擊“> Test Value”選項進行指定。這樣做後，您將得到@fig-fig11-17中顯示的結果。\n\n\n\n\n\n\nFigure 11.17: jamovi生成的「單一樣本t檢定」結果，其中實際假設是單側的，即真實平均數大於 \\(67.5%\\)\n\n\n\n\n注意到輸出與上次看到的輸出有一些變化。最重要的是實際假設已更改，以反映不同的測試。第二件要注意的事情是儘管t統計量和自由度沒有改變，但p值已經改變。這是因為單側測試具有與雙側測試不同的拒絕區域。如果您忘記了這是為什麼以及它意味著什麼，您可以回顧一下@sec-Hypothesis-testing，特別是@sec-The-difference-between-one-sided-and-two-sided-tests。第三件要注意的是置信區間也不同：現在報告了一個“單側”的置信區間，而不是一個雙側的置信區間。在雙側置信區間中，我們試圖找到數字a和b，使得如果我們重複進行該研究多次，那麼有\\(95\\%\\)的機會均值會落在a和b之間。在單側置信區間中，我們試圖找到一個數字a，使得我們有信心有\\(95\\%\\)的機會真實均值會大於a（如果您在“假設”部分中選擇了測量1 < 測量2則為小於a）。\n所有版本的t檢定都可以是單側的。對於獨立樣本t檢定，如果您只想測試A組的得分是否比B組高，但對於是否要找出B組的得分是否高於A組沒有興趣，那麼您可以進行單側檢定。假設對於Harpo博士的課程，您想知道Anastasia的學生是否比Bernadette的學生成績更好。對於此分析，在“假設”選項中指定“Group 1 > Group2”。您應該可以得到如圖@fig-fig11-18所示的結果。\n\n\n\n\n\n\nFigure 11.18: jamovi生成的’獨立樣本t檢定’分析，實際假設為單尾檢定，即 Anastasia 的學生的成績高於 Bernadette 的學生\n\n\n\n\n再次強調，輸出結果有可預測的變化。替代假設的定義已改變，p值已變化，現在報告的是單邊信賴區間，而不是雙邊信賴區間。\n那麼，配對樣本t檢驗呢？假設我們想要測試 Dr Zeppo 課堂上考試成績是否從第一次測試到第二次測試有所提高，並且不考慮成績下降的可能性。在 jamovi 中，您可以通過在「假設」選項下指定 grade_test2 （在 jamovi 中為「測量1」，因為我們首先將其複製到了配對變量框中）> grade_test1 （在 jamovi 中為「測量2」）來實現這一點。您應該可以看到 Figure 11.19 中的結果。\n\n\n\n\n\n\n\nFigure 11.19: jamovi生成的’相依樣本t檢定’分析，實際假設為單尾檢定，即測驗1成績 > 測驗2成績。\n\n\n\n\n這次的輸出和之前一樣，以可預測的方式改變。假設已經改變，p值也改變，並且置信區間現在是單邊的。"
  },
  {
    "objectID": "11-Comparing-two-means.html#t檢定的效果量",
    "href": "11-Comparing-two-means.html#t檢定的效果量",
    "title": "11  比較單一與兩組平均值",
    "section": "11.8 t檢定的效果量",
    "text": "11.8 t檢定的效果量\n以下是您將翻譯成繁體中文的部分。\n對於t檢驗，最常用的效應量測量方法是科恩的d (Cohen, 1988)。從原理上看，它是一個非常簡單的方法，但當您深入研究細節時，會有相當多的變化。科恩本人主要在獨立樣本t檢驗的上下文中對其進行了定義，特別是學生檢驗。在該背景下，定義效應量的自然方法是將均值之間的差除以標準差的估計。換句話說，我們要計算的是類似於以下公式的東西：\n\\[d=\\frac{(\\text{平均值1})-(\\text{平均值2})}{\\text{標準差}}\\]\n他在 ?tbl-tab11-3中建議了一個解釋$d$的粗略指南。\n\n\n\n\n\nTable 11.3:  解釋科恩的d的（非常）粗略指南。我的個人建議是不要盲目地使用這些。d統計量本身具有自然的解釋。它將均值之間的差重新描述為將這些均值分開的標準差數。因此，通常最好考慮一下這在實際條件下意味著什麼。在某些情境下，一個「小」的效應可能具有很大的實際重要性。在其他情況下，一個「大」的效應可能並不是那麼有趣。 \n\nd-valuerough interpretation\n\nabout 0.2\"small\" effect\n\nabout 0.5\"moderate\" effect\n\nabout 0.8\"large\" effect\n\n\n\n\n\n以下是一些您將翻譯成繁體中文的單詞。\n您可能會認為這應該是非常明確的，但事實並非如此。這主要是因為科恩對他認為應該用作標準差測量的方法並未過多具體（在他的辯護中，他在書中試圖闡述一個更廣泛的觀點，而不是對微小細節吹毛求疵）。如@McGrath2006所討論，常用的有幾個不同版本，每位作者都傾向於採用略有不同的表示法。為了簡單起見（而非準確性），我將使用d來指代您從樣本中計算出的任何統計量，並使用\\(\\delta\\)來指代理論的群體效應。顯然，這確實意味著有幾個不同的東西都被稱為d。\n我的懷疑是，您需要科恩的d的唯一時刻是當您運行t檢驗時，jamovi提供了一個選項可以計算它提供的所有不同類型t檢驗的效應量。\n\n\n11.8.1 單一樣本的Cohen’s d\n最簡單的情況是與單樣本t檢驗相對應的情況。在這種情況下，這是一個樣本均值\\(\\bar{X}\\)和一個（假設的）群體均值\\(\\mu_0\\)進行比較。不僅如此，真正只有一種合理的方法來估計群體標準差。我們只需使用我們通常的估計\\(\\hat{\\sigma}\\)。因此，我們最終得出以下計算\\(d\\)的唯一方法\n\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]\n當我們回顧@fig-fig11-6中的結果時，效應量值是科恩的\\(d = 0.50\\)。因此，總的來說，Zeppo博士班上的心理學生取得的成績（\\(mean = 72.3\\%\\)）比您預期的水平（\\(67.5\\%\\)）高出約0.5個標準差，如果他們的表現與其他學生相同。根據科恩的粗略指南，這是一個中等效應量。\n\n\n\n11.8.2 獨立樣本的Cohen’s d\n大多數關於科恩的\\(d\\)的討論都集中在與Student獨立樣本t檢驗相似的情況上，正是在這種情境下，故事變得更加混亂，因為在這種情況下，您可能想要使用幾個不同版本的\\(d\\)。為了理解為什麼\\(d\\)有多個版本，抽點時間寫下與真實群體效應大小\\(\\delta\\)相對應的公式是有幫助的。這很簡單，\n\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\]\n其中，和往常一樣，\\(\\mu_1\\)和\\(\\mu_2\\)分別是與第1組和第2組相對應的群體均值，\\(\\sigma\\)是標準差（兩個群體都相同）。顯然，估計\\(\\delta\\)的方法是做我們在t檢驗本身中所做的完全相同的事情，即在分子使用樣本均值，並在分母使用池化標準差估計值\n\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\n其中\\(\\hat{\\sigma}_p\\)是t檢驗中出現的完全相同的池化標準差度量。這是應用於Student t檢驗結果時最常用的科恩的d版本，也是jamovi中提供的版本。有時它被稱為Hedges的\\(g\\)統計量(Hedges, 1981)。\n然而，還有其他可能性，我將簡要描述。首先，您可能有理由只想用兩個組中的一個作為計算標準差的基礎。這種方法（通常稱為Glass的\\(\\triangle\\)，讀作delta）在您有充分理由將兩個組中的一個視為比另一個更純粹地反映「自然變異」時，才最有意義。例如，如果兩個組中的一個是對照組，就可能發生這種情況。其次，回憶一下，在計算池化標準差的過程中，我們通常會除以\\(N - 2\\)以糾正樣本方差的偏差。在科恩的d的一個版本中，省略了這個糾正，而是除以\\(N\\)。當您試圖在樣本中計算效應量而不是估計群體中的效應量時，這個版本主要是有意義的。最後，有一個基於@Hedges1985的版本，叫做Hedge的g，他指出在科恩的d的常規（池化）估計中存在一個小的偏差。20\n\n\n\n11.8.3 相依樣本的Cohen’s d\n最後，對於配對樣本t檢驗，我們應該怎麼做？在這種情況下，答案取決於您試圖做什麼。jamovi假設您希望根據差異分數的分佈來衡量效應量，您計算的d度量為：\n\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\n其中\\(\\hat{\\sigma}_D\\)是差異的標準差估計。在@fig-fig11-16中，科恩的\\(d = 1.45\\)，表示時間2的分數平均比時間1的分數高出\\(1.45\\)個標準差。\n這是jamovi「配對樣本T檢驗」分析報告的科恩\\(d\\)版本。唯一的麻煩是弄清楚這是否是您想要的度量。在您關心研究的實際後果的程度上，您通常希望根據原始變量而不是差異分數（例如，與學生間的成績變異量相比，Chico博士班上隨著時間的1%改進相當小）來衡量效應量，在這種情況下，您使用的是與Student或Welch檢驗相同的科恩d版本。在jamovi中做到這一點並不那麼簡單；基本上您必須在試算表視圖中更改數據結構，所以我在這裡不會深入討論21，但是從這個角度看，科恩的d相當不同：它是\\(0.22\\)，在原始變量的尺度上評估相當小。"
  },
  {
    "objectID": "11-Comparing-two-means.html#非常態資料的檢定",
    "href": "11-Comparing-two-means.html#非常態資料的檢定",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 非常態資料的檢定",
    "text": "11.10 非常態資料的檢定\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 獨立樣本的曼－惠特尼U檢定\n<<<<<<< HEAD I’ll start by describing the 曼－惠特尼U檢定(Mann-Whitney U test), since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group. ======= I’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group. >>>>>>> 49b1e97cffa47ef082c903513735779e8d1e5ed6\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.4).\n\n\n\n\nTable 11.4:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 24 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for the Mann-Whitney \\(U\\) test\n\n\n\n\n\n\n11.10.2 單一樣本的Wilcoxon檢定\n<<<<<<< HEAD What about the one sample Wilcoxon檢定(Wilcoxon test) (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5. ======= What about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.5. >>>>>>> 49b1e97cffa47ef082c903513735779e8d1e5ed6\n\n\n\n\nTable 11.5:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\npositive differences7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests"
  },
  {
    "objectID": "11-Comparing-two-means.html#本章小結",
    "href": "11-Comparing-two-means.html#本章小結",
    "title": "11  比較單一與兩組平均值",
    "section": "11.11 本章小結",
    "text": "11.11 本章小結\n\n單一樣本z檢定 用來比對樣本平均值是否不同於母群平均值(各位可考慮跳過)。\n獨立樣本t檢定用於比較兩組平均值的差異，虛無假設設定兩組平均值相等。實務上會使用適用條件 是兩組變異相同的獨立樣本t檢定，或是兩組變異不同的獨立樣本t檢定(Welch t檢定)。\n相依樣本t檢定用於比較的資料來自同一個樣本的測量，虛無假設設定兩次測量的平均值相等。分析方法與適用條件與單一樣本t檢定相同。\n只有事前規劃差異比較方向，單尾檢定 才有真正的意義。\nt檢定的效果量以Cohen’ d的公式計算。\n我們能使用QQ圖及Shapiro-Wilk檢定檢測樣本常態性\n如果資料真的違反常態性，你可以使用等級資料的平均值檢定像是曼－惠特尼U檢定，以及Wilcoxon檢定。\n\n\n\n\n\n\nBox, J. F. (1987). Guinness, gosset, fisher, and small samples. Statistical Science, 2, 45–52.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Lawrence Erlbaum.\n\n\nHedges, L. V. (1981). Distribution theory for glass’s estimator of effect size and related estimators. Journal of Educational Statistics, 6, 107–128.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52, 591–611.\n\n\nWelch, B. L. (1947). The generalization of “Student’s” problem when several different population variances are involved. Biometrika, 34, 28–35."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html",
    "href": "12-Correlation-and-linear-regression.html",
    "title": "12  相闗與線性迴歸",
    "section": "",
    "text": "The goal in this chapter is to introduce correlation and linear regression. These are the standard tools that statisticians rely on when analysing the relationship between continuous predictors and continuous outcomes."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#correlations",
    "href": "12-Correlation-and-linear-regression.html#correlations",
    "title": "12  Correlation and linear regression",
    "section": "12.1 Correlations",
    "text": "12.1 Correlations\nIn this section we’ll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data (Table 12.1).\n\n12.1.1 The data\n\n\n\n\nTable 12.1:  Data for correlation analysis - descriptive statistics for the parenthood data \n\nvariableminmaxmeanmedianstd. devIQR\n\nDani's grumpiness419163.716210.0514\n\nDani's hours slept4.849.006.977.031.021.45\n\nDani's son's hours slept3.2512.078.057.952.073.21\n\n\n\n\n\nLet’s turn to a topic close to every parent’s heart: sleep. The data set we’ll use is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to \\(100\\) (grumpy as a very, very grumpy old man or woman). And lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for \\(100\\) days. And, being a nerd, I’ve saved the data as a file called parenthood.csv. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.1\nNext, I’ll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, Figure 12.1 plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.2 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s pretty standard.\n\n\n\n\n\nFigure 12.1: Histograms for the three interesting variables in the parenthood data set\n\n\n\n\n\n\n12.1.2 The strength and direction of a relationship\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dani.sleep and dani.grump (Figure 12.1), left) with that between baby.sleep and dani.grump (Figure 12.2), right). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\n\n\n\n\n\nFigure 12.2: Scatterplots showing the relationship between dani.sleep and dani.grump (left) and the relationship between baby.sleep and dani.grump (right)\n\n\n\n\nIn contrast, let’s consider the two scatterplots shown in Figure 12.3. If we compare the scatterplot of “baby.sleep v dani.grump” (left) to the scatterplot of “’baby.sleep v dani.sleep” (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps more then I get less grumpy (negative relationship, left hand side).\n\n\n\n\n\nFigure 12.3: Scatterplots showing the relationship between baby.sleep and dani.grump (left), as compared to the relationship between baby.sleep and dani.sleep (right)\n\n\n\n\n\n\n12.1.3 The correlation coefficient\nWe can make these ideas a bit more explicit by introducing the idea of a correlation coefficient (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\) ), which we’ll define more precisely in the next section, is a measure that varies from -1 to 1. When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure 12.4, you can see several plots showing what different correlations look like.\n[Additional technical detail 3]\n\n\n\n\n\nFigure 12.4: Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are \\(0, .33, .66\\) and \\(1\\). In the right hand column, the correlations are \\(0, -.33, -.66\\) and \\(-1\\)\n\n\n\n\nBy standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in the section on Interpreting a correlation. But before I do, let’s look at how to calculate correlations in jamovi.\n\n\n12.1.4 Calculating correlations in jamovi\nCalculating correlations in jamovi can be done by clicking on the ‘Regression’ - ‘Correlation Matrix’ button. Transfer all four continuous variables across into the box on the right to get the output in Figure 12.5.\n\n\n\n\n\nFigure 12.5: A jamovi screenshot showing correlations between variables in the parenthood.csv file\n\n\n\n\n\n\n12.1.5 Interpreting a correlation\nNaturally, in real life you don’t see many correlations of \\(1\\). So how should you interpret a correlation of, say, r = \\(.4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table 12.2 is pretty typical.\n\n\n\n\nTable 12.2:  A rough guide to interpreting correlations. Note that I say a rough guide. There aren’t hard and fast rules for what counts as strong or weak relationships. It depends on the context. \n\nCorrelationStrengthDirection\n\n-1.0 to -0.9Very strongNegative\n\n-0.9 to -0.7StrongNegative\n\n-0.7 to -0.4ModerateNegative\n\n-0.4 to -0.2WeakNegative\n\n-0.2 to 0NegligibleNegative\n\n0 to 0.2NegligiblePositive\n\n0.2 to 0.4WeakPositive\n\n0.4 to 0.7ModeratePositive\n\n0.7 to 0.9StrongPositive\n\n0.9 to 1.0Very strongPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” (Anscombe, 1973), a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is \\(9\\) and the mean for \\(Y\\) is \\(7.5\\). The standard deviations for all \\(X\\) variables are almost identical, as are those for the Y variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since I happen to have saved it in a file called anscombe.csv.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 12.6, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is “always graph your raw data” (see Chapter 5).\n\n\n\n\n\nFigure 12.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of r = .816, but they are qualitatively different from one another\n\n\n\n\n\n\n12.1.6 Spearman’s rank correlations\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable Y , but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort (\\(X\\)) into learning a subject then you should expect a grade of \\(0\\%\\) (\\(Y\\)). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of \\(90\\%\\) than it takes to get a grade of \\(55\\%\\). What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure 12.7, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of \\(0.91\\). However, this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\n\n\n\n\n\nFigure 12.7: The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(\\rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved\n\n\n\n\nHow should we address this? Actually, it’s really easy. If we’re looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all \\(10\\) of our students in order of hours worked. That is, student \\(1\\) did the least work out of anyone (\\(2\\) hours) so they get the lowest rank (rank = \\(1\\)). Student \\(4\\) was the next laziest, putting in only \\(6\\) hours of work over the whole semester, so they get the next lowest rank (rank = \\(2\\)). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = \\(1\\)” to mean “top rank” rather than “bottom rank”. So be careful, you can rank “from smallest value to largest value” (i.e., small equals rank \\(1\\)) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, but as it’s really easy to forget which way you set things up you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward Table 12.3.\n\n\n\n\nTable 12.3:  Students ranked in terms of effort and reward \n\nrank (hours worked)rank (grade received)\n\nstudent 111\n\nstudent 21010\n\nstudent 366\n\nstudent 422\n\nstudent 533\n\nstudent 655\n\nstudent 744\n\nstudent 888\n\nstudent 977\n\nstudent 1099\n\n\n\n\n\nHmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.\nWhat we’ve just re-invented is Spearman’s rank order correlation, usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation r. We can calculate Spearman’s \\(\\rho\\) using jamovi simply by clicking the ‘Spearman’ check box in the ‘Correlation Matrix’ screen."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#scatterplots",
    "href": "12-Correlation-and-linear-regression.html#scatterplots",
    "title": "12  Correlation and linear regression",
    "section": "12.2 Scatterplots",
    "text": "12.2 Scatterplots\nScatterplots are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on Correlations. It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in jamovi, using the same parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we’re after. The first way is to use the ‘Plot’ option under the ‘Regression’ - ‘Correlation Matrix’ button, giving us the output shown in Figure 12.8. Note that jamovi draws a line through the points, we’ll come onto this a bit later in the section on What is a linear regression model?. Plotting a scatterplot in this way also allow you to specify ‘Densities for variables’ and this option adds a density curve showing how the data in each variable is distributed.\n\n\n\n\n\nFigure 12.8: Scatterplot via the ‘Correlation Matrix’ command in jamovi\n\n\n\n\nThe second way do to it is to use one of the jamovi add-on modules. This module is called ‘scatr’ and you can install it by clicking on the large ‘\\(+\\)’ icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find ‘scatr’ and clicking ‘install’. When you have done this, you will find a new ‘Scatterplot’ command available under the ‘Exploration’ button. This plot is a bit different than the first way, see Figure 12.9, but the important information is the same.\n\n\n\n\n\nFigure 12.9: Scatterplot via the ‘scatr’ add-on module in - jamovi\n\n\n\n\n\n12.2.1 More elaborate options\nOften you will want to look at the relationships between several variables at once, using a scatterplot matrix (in jamovi via the ‘Correlation Matrix’ - ‘Plot’ command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in Figure 12.10.\n\n\n\n\n\nFigure 12.10: A matrix of scatterplots produced using jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#what-is-a-linear-regression-model",
    "href": "12-Correlation-and-linear-regression.html#what-is-a-linear-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.3 What is a linear regression model?",
    "text": "12.3 What is a linear regression model?\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see Correlations), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 12.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 12.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 12.11 (b).\n\n\n\n\n\nFigure 12.11: Panel (a) shows the sleep-grumpiness scatterplot from Figure 12.9 with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel (b) shows the same data, but with a very poor choice of regression line drawn over the top\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 12.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).4 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).5 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#estimating-a-linear-regression-model",
    "href": "12-Correlation-and-linear-regression.html#estimating-a-linear-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.4 Estimating a linear regression model",
    "text": "12.4 Estimating a linear regression model\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 12.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 12.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 12.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n12.4.1 Linear regression in jamovi\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 12.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 12.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 12.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n12.4.2 Interpreting the estimated model\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#multiple-linear-regression",
    "href": "12-Correlation-and-linear-regression.html#multiple-linear-regression",
    "title": "12  Correlation and linear regression",
    "section": "12.5 Multiple linear regression",
    "text": "12.5 Multiple linear regression\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n12.5.1 Doing it in jamovi\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 12.4.\n\n\n\n\nTable 12.4:  Adding multiple variables as predictors in a regression \n\n(Intercept)dani.sleepbaby.sleep\n\n125.97-8.950.01\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 12.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 12.14: A 3D visualisation of a multiple regression model. There are two predictors in the model, dani.sleep and baby.sleep and the outcome variable is dani.grump. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we’re trying to do is find a plane that is as close to all the blue dots as possible\n\n\n\n\n[Additional technical detail7]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#quantifying-the-fit-of-the-regression-model",
    "href": "12-Correlation-and-linear-regression.html#quantifying-the-fit-of-the-regression-model",
    "title": "12  Correlation and linear regression",
    "section": "12.6 Quantifying the fit of the regression model",
    "text": "12.6 Quantifying the fit of the regression model\nSo we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction \\(\\hat{Y}_i\\) about what my mood is like, but my actual mood is \\(Y_i\\) . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n\n12.6.1 The \\(R^2\\) value\nOnce again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\nwhich we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\nWhile we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the \\(\\hat{Y}\\) values, and for the simple model that uses only a single predictor we would do the following:\n\ncreate a new column called ‘Y.pred’ using the formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’\ncalculate the SS(resid) by creating a new column called ‘(Y-Y.pred)^2’ using the formula ’ = (dani.grump - Y.pred)^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ’ sum( ( Y-Y.pred)^2 ) .\nAt the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ‘mean’ in its function).\nThen create a new column, called ’ (Y - mean(Y))^2 )’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ‘sum( (Y - mean(Y))^2 )’.\nCalculate R.squared by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.\n\nThis gives a value for \\(R^2\\) of ‘0.8161018’. The \\(R^2\\) value, sometimes called the coefficient of determination8 has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained \\(R^2 = .816\\) means that the predictor (my.sleep) explains \\(81.6\\%\\) of the variance in the outcome (my.grump).\nNaturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the \\(R^2\\) value for your regression model. As we’ll see later on in the section on Running the hypothesis tests in jamovi, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of \\(R^2\\) that I want to point out.\n\n\n12.6.2 The relationship between regression and correlation\nAt this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol \\(r\\) to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient \\(r\\) and the \\(R^2\\) value from linear regression? Of course there is: the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.\n\n\n12.6.3 The adjusted \\(R^2\\) value\nOne final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted \\(R^2\\)”. The motivation behind calculating the adjusted \\(R^2\\) value is the observation that adding more predictors into the model will always cause the \\(R^2\\) value to increase (or at least not decrease).\n[Additional technical detail9]\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted \\(R^2\\) value is that when you add more predictors to the model, the adjusted \\(R^2\\) value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted \\(R^2\\) value can’t be interpreted in the elegant way that \\(R^2\\) can. \\(R^2\\) has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted \\(R^2\\).\nAn obvious question then is whether you should report \\(R^2\\) or adjusted \\(R^2\\) . This is probably a matter of personal preference. If you care more about interpretability, then \\(R^2\\) is better. If you care more about correcting for bias, then adjusted \\(R^2\\) is probably better. Speaking just for myself, I prefer \\(R^2\\). My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in Hypothesis tests for regression models, if you’re worried that the improvement in \\(R^2\\) that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#hypothesis-tests-for-regression-models",
    "href": "12-Correlation-and-linear-regression.html#hypothesis-tests-for-regression-models",
    "title": "12  Correlation and linear regression",
    "section": "12.7 Hypothesis tests for regression models",
    "text": "12.7 Hypothesis tests for regression models\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n12.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail10]\nWe’ll see much more of the F statistic in Chapter 13, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n12.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the Multiple linear regression model we have already looked at (Table 12.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.11 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n12.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 12.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 12.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 12.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.12\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#regarding-regression-coefficients",
    "href": "12-Correlation-and-linear-regression.html#regarding-regression-coefficients",
    "title": "12  Correlation and linear regression",
    "section": "12.8 Regarding regression coefficients",
    "text": "12.8 Regarding regression coefficients\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n12.8.1 Confidence intervals for the coefficients\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 12.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n12.8.2 Calculating standardised regression coefficients\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 12.16.\n\n\n\n\n\nFigure 12.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#assumptions-of-regression",
    "href": "12-Correlation-and-linear-regression.html#assumptions-of-regression",
    "title": "12  Correlation and linear regression",
    "section": "12.9 Assumptions of regression",
    "text": "12.9 Assumptions of regression\nThe linear regression model that I’ve been discussing relies on several assumptions. In Model checking we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.\n\nLinearity. A pretty fundamental assumption of the linear regression model is that the relationship between \\(X\\) and \\(Y\\) actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.\nIndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.\nNormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors \\(X\\) and the outcome \\(Y\\) are non-normal, so long as the residuals \\(\\epsilon\\) are normal. See the Checking the normality of the residuals section.\nEquality (or ‘homogeneity’) of variance. Strictly speaking, the regression model assumes that each residual \\(\\epsilon_i\\) is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation \\(\\sigma\\) that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of \\(\\hat{Y}\\) , and (if we’re being especially paranoid) all values of every predictor \\(X\\) in the model.\n\nSo, we have four main assumptions for linear regression (that neatly form the acronym ‘LINE’). And there are also a couple of other things we should also check for:\n\nUncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See the Checking for collinearity section.\nNo “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on Three kinds of anomalous data."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-Model-checking",
    "href": "12-Correlation-and-linear-regression.html#sec-Model-checking",
    "title": "10  相闗與線性迴歸",
    "section": "10.10 診斷適用條件",
    "text": "10.10 診斷適用條件\nThe main focus of this section is regression diagnostics, a term that refers to the art of checking that the assumptions of your regression model have been met, figuring out how to fix the model if the assumptions are violated, and generally to check that nothing “funny” is going on. I refer to this as the “art” of model checking with good reason. It’s not easy, and while there are a lot of fairly standardised tools that you can use to diagnose and maybe even cure the problems that ail your model (if there are any, that is!), you really do need to exercise a certain amount of judgement when doing this. It’s easy to get lost in all the details of checking this thing or that thing, and it’s quite exhausting to try to remember what all the different things are. This has the very nasty side effect that a lot of people get frustrated when trying to learn all the tools, so instead they decide not to do any model checking. This is a bit of a worry!\nIn this section I describe several different things you can do to check that your regression model is doing what it’s supposed to. It doesn’t cover the full space of things you could do, but it’s still much more detailed than what I see a lot of people doing in practice, and even I don’t usually cover all of this in my intro stats class either. However, I do think it’s important that you get a sense of what tools are at your disposal, so I’ll try to introduce a bunch of them here. Finally, I should note that this section draws quite heavily from Fox & Weisberg (2011), the book associated with the ‘car’ package that is used to conduct regression analysis in R. The ‘car’ package is notable for providing some excellent tools for regression diagnostics, and the book itself talks about them in an admirably clear fashion. I don’t want to sound too gushy about it, but I do think that Fox & Weisberg (2011) is well worth reading, even if some of the advanced diagnostic techniques are only available in R and not jamovi.\n\n10.10.1 三種殘差\nThe majority of regression diagnostics revolve around looking at the residuals, and by now you’ve probably formed a sufficiently pessimistic theory of statistics to be able to guess that, precisely because of the fact that we care a lot about the residuals, there are several different kinds of residual that we might consider. In particular, the following three kinds of residuals are referred to in this section: “ordinary residuals”, “standardised residuals”, and “Studentised residuals”. There is a fourth kind that you’ll see referred to in some of the Figures, and that’s the “Pearson residual”. However, for the models that we’re talking about in this chapter the Pearson residual is identical to the ordinary residual.\nThe first and simplest kind of residuals that we care about are ordinary residuals. These are the actual raw residuals that I’ve been talking about throughout this chapter so far. The ordinary residual is just the difference between the fitted value \\(\\hat{Y}_i\\) and the observed value \\(Y_i\\). I’ve been using the notation \\(\\epsilon_i\\) to refer to the i-th ordinary residual, and by gum I’m going to stick to it. With this in mind, we have the very simple equation\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\nThis is of course what we saw earlier, and unless I specifically refer to some other kind of residual, this is the one I’m talking about. So there’s nothing new here. I just wanted to repeat myself. One drawback to using ordinary residuals is that they’re always on a different scale, depending on what the outcome variable is and how good the regression model is. That is, unless you’ve decided to run a regression model without an intercept term, the ordinary residuals will have mean 0 but the variance is different for every regression. In a lot of contexts, especially where you’re only interested in the pattern of the residuals and not their actual values, it’s convenient to estimate the standardised residuals, which are normalised in such a way as to have standard deviation of 1.\n[Additional technical detail16]\nThe third kind of residuals are Studentised residuals (also called “jackknifed residuals”) and they’re even fancier than standardised residuals. Again, the idea is to take the ordinary residual and divide it by some quantity in order to estimate some standardised notion of the residual. 17\nBefore moving on, I should point out that you don’t often need to obtain these residuals yourself, even though they are at the heart of almost all regression diagnostics. Most of the time the various options that provide the diagnostics, or assumption checks, will take care of these calculations for you. Even so, it’s always nice to know how to actually get hold of these things yourself in case you ever need to do something non-standard.\n\n\n10.10.2 三種反常資料\nOne danger that you can run into with linear regression models is that your analysis might be disproportionately sensitive to a smallish number of “unusual” or “anomalous” observations. I discussed this idea previously in Section 5.2.3 in the context of discussing the outliers that get automatically identified by the boxplot option under ‘Exploration’ - ‘Descriptives’, but this time we need to be much more precise. In the context of linear regression, there are three conceptually distinct ways in which an observation might be called “anomalous”. All three are interesting, but they have rather different implications for your analysis.\nThe first kind of unusual observation is an outlier. The definition of an outlier (in this context) is an observation that is very different from what the regression model predicts. An example is shown in Figure 10.17. In practice, we operationalise this concept by saying that an outlier is an observation that has a very large Studentised residual, \\(\\epsilon_i^*\\). Outliers are interesting: a big outlier might correspond to junk data, e.g., the variables might have been recorded incorrectly in the data set, or some other defect may be detectable. Note that you shouldn’t throw an observation away just because it’s an outlier. But the fact that it’s an outlier is often a cue to look more closely at that case and try to find out why it’s so different.\n\n\n\n\n\nFigure 10.17: An illustration of outliers. The dotted lines plot the regression line that would have been estimated without the anomalous observation included, and the corresponding residual (i.e., the Studentised residual). The solid line shows the regression line with the anomalous observation included. The outlier has an unusual value on the outcome (y axis location) but not the predictor (x axis location), and lies a long way from the regression line\n\n\n\n\nThe second way in which an observation can be unusual is if it has high 槓桿作用(leverage), which happens when the observation is very different from all the other observations. This doesn’t necessarily have to correspond to a large residual. If the observation happens to be unusual on all variables in precisely the same way, it can actually lie very close to the regression line. An example of this is shown in Figure 10.18. The leverage of an observation is operationalised in terms of its hat value, usually written \\(h_i\\) . The formula for the hat value is rather complicated18 but its interpretation is not: \\(h_i\\) is a measure of the extent to which the i-th observation is “in control” of where the regression line ends up going.\n\n\n\n\n\nFigure 10.18: An illustration of high leverage points. The anomalous observation in this case is unusual both in terms of the predictor (x axis) and the outcome (y axis), but this unusualness is highly consistent with the pattern of correlations that exists among the other observations. The observation falls very close to the regression line and does not distort it\n\n\n\n\nIn general, if an observation lies far away from the other ones in terms of the predictor variables, it will have a large hat value (as a rough guide, high leverage is when the hat value is more than 2-3 times the average; and note that the sum of the hat values is constrained to be equal to \\(K + 1\\)). High leverage points are also worth looking at in more detail, but they’re much less likely to be a cause for concern unless they are also outliers.\nThis brings us to our third measure of unusualness, the 影響力(influence) of an observation. A high influence observation is an outlier that has high leverage. That is, it is an observation that is very different to all the other ones in some respect, and also lies a long way from the regression line. This is illustrated in Figure 10.19. Notice the contrast to the previous two figures. Outliers don’t move the regression line much and neither do high leverage points. But something that is both an outlier and has high leverage, well that has a big effect on the regression line. That’s why we call these points high influence, and it’s why they’re the biggest worry. We operationalise influence in terms of a measure known as Cook’s distance. 19\n\n\n\n\n\nFigure 10.19: An illustration of high influence points. In this case, the anomalous observation is highly unusual on the predictor variable (x axis), and falls a long way from the regression line. As a consequence, the regression line is highly distorted, even though (in this case) the anomalous observation is entirely typical in terms of the outcome variable (y axis)\n\n\n\n\nIn order to have a large Cook’s distance an observation must be a fairly substantial outlier and have high leverage. As a rough guide, Cook’s distance greater than 1 is often considered large (that’s what I typically use as a quick and dirty rule).\nIn jamovi, information about Cook’s distance can be calculated by clicking on the ‘Cook’s Distance’ checkbox in the ‘Assumption Checks’ - ‘Data Summary’ options. When you do this, for the multiple regression model we have been using as an example in this chapter, you get the results as shown in Figure 10.20.\n\n\n\n\n\nFigure 10.20: jamovi output showing the table for the Cook’s distance statistics\n\n\n\n\nYou can see that, in this example, the mean Cook’s distance value is \\(0.01\\), and the range is from \\(0.00\\) to \\(0.11\\), so this is some way off the rule of thumb figure mentioned above that a Cook’s distance greater than 1 is considered large.\nAn obvious question to ask next is, if you do have large values of Cook’s distance what should you do? As always, there’s no hard and fast rule. Probably the first thing to do is to try running the regression with the outlier with the greatest Cook’s distance20 excluded and see what happens to the model performance and to the regression coefficients. If they really are substantially different, it’s time to start digging into your data set and your notes that you no doubt were scribbling as your ran your study. Try to figure out why the point is so different. If you start to become convinced that this one data point is badly distorting your results then you might consider excluding it, but that’s less than ideal unless you have a solid explanation for why this particular case is qualitatively different from the others and therefore deserves to be handled separately.\n\n\n10.10.3 檢測殘差常態性\nLike many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option. The output is shown in Figure 10.21, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.\n\n\n\n\n\nFigure 10.21: Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi\n\n\n\n\nAnother thing we should check is the relationship between the fitted values and the residuals themselves. We can get jamovi to do this using the ‘Residuals Plots’ option, which provides a scatterplot for each predictor variable, the outcome variable, and the fitted values against residuals, see Figure 10.22. In these plots we are looking for a fairly uniform distribution of ‘dots’, with no clear bunching or patterning of the ‘dots’. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.\n\n\n\n\n\nFigure 10.22: Residuals plots produced in jamovi\n\n\n\n\nIf we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in Section 6.3, but I do want to make special note of one additional possibility that I didn’t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it’s very widely used. 21\nYou can calculate it using the BOXCOX function in the ‘Compute’ variables screen in jamovi.\n\n\n10.10.4 檢測共線性\nThe last kind of regression diagnostic that I’m going to discuss in this chapter is the use of variance inflation factors (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor \\(X_k\\) in the model. 22\nThe square root of the VIF is pretty interpretable. It tells you how much wider the confidence interval for the corresponding coefficient bk is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options in jamovi. For both dani.sleep and baby.sleep the VIF is \\(1.65\\). And since the square root of \\(1.65\\) is \\(1.28\\), we see that the correlation between our two predictors isn’t causing much of a problem.\nTo give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables (Table 10.5).\n\n\n\n\nTable 10.5:  Correlation matrix for all four variables \n\ndani.sleepbaby.sleepdani.grumpday\n\ndani.sleep1.000000000.62794934$-0.90338404$$-0.09840768$\n\nbaby.sleep0.627949341.00000000$-0.56596373$$-0.01043394$\n\ndani.grump$-0.90338404$$-0.56596373$1.000000000.07647926\n\nday$-0.09840768$$-0.01043394$0.076479261.00000000\n\n\n\n\n\nWe have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in Figure 10.23 and you can see from the VIF values that, yep, that’s some mighty fine collinearity there.\n\n\n\n\n\nFigure 10.23: Collinearity statistics for multiple regression, produced in jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#model-selection",
    "href": "12-Correlation-and-linear-regression.html#model-selection",
    "title": "12  Correlation and linear regression",
    "section": "12.11 Model selection",
    "text": "12.11 Model selection\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 23\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n12.11.1 Backward elimination\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n12.11.2 Forward selection\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n12.11.3 A caveat\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n12.11.4 Comparing two regression models\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.24\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 25\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 12.24. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 12.24.\n\n\n\n\n\nFigure 12.24: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#summary",
    "href": "12-Correlation-and-linear-regression.html#summary",
    "title": "12  Correlation and linear regression",
    "section": "12.12 Summary",
    "text": "12.12 Summary\n\nWant to know how strong the relationship is between two variables? Calculate Correlations\nDrawing Scatterplots\nBasic ideas about What is a linear regression model? and Estimating a linear regression model\nMultiple linear regression\nQuantifying the fit of the regression model using \\(R^2\\).\nHypothesis tests for regression models\nIn Regarding regression coefficients we talked about calculating Confidence intervals for the coefficients and Calculating standardised regression coefficients\nThe Assumptions of regression and Model checking\nRegression Model selection\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html",
    "href": "13-Comparing-several-means-one-way-ANOVA.html",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "",
    "text": "This chapter introduces one of the most widely used tools in psychological statistics, known as “the analysis of variance”, but usually referred to as ANOVA. The basic technique was developed by Sir Ronald Fisher in the early 20th century and it is to him that we owe the rather unfortunate terminology. The term ANOVA is a little misleading, in two respects. Firstly, although the name of the technique refers to variances, ANOVA is concerned with investigating differences in means. Secondly, there are several different things out there that are all referred to as ANOVAs, some of which have only a very tenuous connection to one another. Later on in the book we’ll encounter a range of different ANOVA methods that apply in quite different situations, but for the purposes of this chapter we’ll only consider the simplest form of ANOVA, in which we have several different groups of observations, and we’re interested in finding out whether those groups differ in terms of some outcome variable of interest. This is the question that is addressed by a one-way ANOVA.\nThe structure of this chapter is as follows: first I’ll introduce a fictitious data set that we’ll use as a running example throughout the chapter. After introducing the data, I’ll describe the mechanics of how a one-way ANOVA actually works How ANOVA works and then focus on how you can run one in jamovi Running an ANOVA in jamovi. These two sections are the core of the chapter.\nThe remainder of the chapter discusses a range of important topics that inevitably arise when running an ANOVA, namely how to calculate effect sizes, post hoc tests and corrections for multiple comparisons and the assumptions that ANOVA relies upon. We’ll also talk about how to check those assumptions and some of the things you can do if the assumptions are violated. Then we’ll cover repeated measures ANOVA."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#an-illustrative-data-set",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#an-illustrative-data-set",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.1 An illustrative data set",
    "text": "13.1 An illustrative data set\nSuppose you’ve become involved in a clinical trial in which you are testing a new antidepressant drug called Joyzepam. In order to construct a fair test of the drug’s effectiveness, the study involves three separate drugs to be administered. One is a placebo, and the other is an existing antidepressant / anti-anxiety drug called Anxifree. A collection of 18 participants with moderate to severe depression are recruited for your initial testing. Because the drugs are sometimes administered in conjunction with psychological therapy, your study includes 9 people undergoing cognitive behavioural therapy (CBT) and 9 who are not. Participants are randomly assigned (doubly blinded, of course) a treatment, such that there are 3 CBT people and 3 no-therapy people assigned to each of the 3 drugs. A psychologist assesses the mood of each person after a 3 month run with each drug, and the overall improvement in each person’s mood is assessed on a scale ranging from \\(-5\\) to \\(+5\\). With that as the study design, let’s now load up the data file in clinicaltrial.csv. We can see that this data set contains the three variables drug, therapy and mood.gain.\nFor the purposes of this chapter, what we’re really interested in is the effect of drug on mood.gain. The first thing to do is calculate some descriptive statistics and draw some graphs. In the Chapter 4 chapter we showed you how to do this, and some of the descriptive statistics we can calculate in jamovi are shown in Figure 13.1\n\n\n\n\n\nFigure 13.1: Descriptives for mood gain, and box plots by drug administered\n\n\n\n\nAs the plot makes clear, there is a larger improvement in mood for participants in the Joyzepam group than for either the Anxifree group or the placebo group. The Anxifree group shows a larger mood gain than the control group, but the difference isn’t as large. The question that we want to answer is are these difference “real”, or are they just due to chance?"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.2 變異數分析的運作原理",
    "text": "13.2 變異數分析的運作原理\n為了回答我們的臨床試驗數據所提出的問題，我們將進行單因素變異數分析（one-way ANOVA）。首先，我將通過自下而上地構建統計工具並向您展示，如果您無法使用jamovi中的任何酷炫內置ANOVA功能，該如何做。我希望您能仔細閱讀，嘗試一兩次用較長的方法來確保您真正了解ANOVA是如何運作的，然後一旦您掌握了概念，就永遠不要再用這種方法了。\n在上一節中我描述的實驗設計強烈表明，我們對比較三種不同藥物的平均心情變化感興趣。在這個意義上，我們討論的分析類似於t檢驗（參見 章节 11 ），但涉及多於兩個組別。如果我們讓\\(\\mu_P\\)表示安慰劑引起的情緒變化的母體平均值，並讓\\(\\mu_A\\)和\\(\\mu_J\\)表示我們的兩種藥物Anxifree和Joyzepam的對應平均值，那麼我們要檢驗的（有些悲觀的）虛無假設是：所有三個母體平均值都相同。也就是說，這兩種藥物都沒有比安慰劑更有效。我們可以將此虛無假設寫為：\n\\[H_0: \\text{ 事實上 } \\mu_P=\\mu_A=\\mu_J\\]\n因此，我們的替代假設是：三種不同治療中至少有一種與其他治療不同。將其用數學表示有點困難，因為（正如我們將要討論的那樣）虛無假設可能以很多不同的方式是錯誤的。所以目前我們只將替代假設寫成這樣：\n\\[H_1: \\text{ 事實 }\\underline{ 不是 }\\text{  } \\mu_P=\\mu_A=\\mu_J\\]\n這個虛無假設比我們之前見過的任何一個都要棘手得多。我們應該如何檢驗它？一個明智的猜測是「進行方差分析」，因為這是本章的標題，但是目前還不太清楚為什麼「方差分析」會幫助我們了解有關均值的有用信息。事實上，這是人們首次接觸方差分析時遇到的最大概念困難之一。要了解其原理，我認為從方差開始談起是最有幫助的，具體來說就是組間變異性和組內變異性（ 图 13.2 ）。\n\n\n\n\n\n\n图 13.2: 圖形說明 ‘組間’ 變異 (面板 (a)) 和 ‘組內’ 變異 (面板 (b))。在左側，箭頭顯示組平均值之間的差異。在右側，箭頭強調每個組內的變異性。\n\n\n\n\n\n13.2.1 計算依變項變異數的兩套公式\n首先，讓我們引入一些符號。我們將使用 G 來表示組的總數。對於我們的數據集，有三種藥物，所以有 \\(G = 3\\) 個組。接下來，我們將使用 \\(N\\) 表示總樣本大小；在我們的數據集中，一共有 \\(N = 18\\) 人。同樣地，讓我們用 \\(N_k\\) 表示第 k 個組中的人數。在我們的虛擬臨床試驗中，所有三組的樣本大小都是 \\(N_k = 6\\)。1 最後，我們將使用 Y 表示結果變項。在我們的案例中，Y 指的是心情變化。具體來說，我們將使用 Yik 指代第 k 個組中第 i 個成員所經歷的心情變化。同樣，我們將使用 \\(\\bar{Y}\\) 作為實驗中所有 18 人的平均心情變化，並使用 \\(\\bar{Y}_k\\) 指代第 k 組中 6 人所經歷的平均心情變化。\n現在我們已經整理好符號，我們可以開始寫下公式。首先，讓我們回想一下在 章节 4.2 中使用的方差公式，在那個做描述性統計的較早時期。Y 的樣本方差被定義為以下公式 \\[Var(Y)=\\frac{1}{N}\\sum_{k=1}^{G}\\sum_{i=1}^{N_k}(Y_{ik}-\\bar{Y})^2\\] 這個公式看起來與 章节 4.2 中的方差公式幾乎相同。唯一的區別是這次我有兩個求和：我對組進行求和（即 \\(k\\) 的值）以及對組內的人進行求和（即 \\(i\\) 的值）。這只是一個純粹的表面細節。如果我使用符號 \\(Y_p\\) 來表示樣本中第 p 個人的結果變項值，那麼我只有一個求和。我們在這裡有兩個求和的唯一原因是我將人分類到組，然後為組內的人分配數字。\n在這裡，具體的例子可能很有用。讓我們考慮 表格 13.1 ，在這個表格中，我們有總共 \\(N = 5\\) 個人分成 \\(G = 2\\) 個組。任意地說，讓我們說「酷」的人是第 1 組，「不酷」的人是第 2 組。結果發現我們有三個酷人（\\(N_1 = 3\\)）和兩個不酷的人（\\(N_2 = 2\\)）。\n\n\n\n\n\n表格 13.1: 在酷和不酷的團體中的脾氣。\n\n\nname\nperson P\ngroup\ngroup num. k\nindex in group\ngrumpiness \\( Y_{ik} \\) or \\( Y_p \\)\n\n\nAnn\n1\ncool\n1\n1\n20\n\n\nBen\n2\ncool\n1\n2\n55\n\n\nCat\n3\ncool\n1\n3\n21\n\n\nTim\n4\nuncool\n2\n1\n91\n\n\nEgg\n5\nuncool\n2\n2\n22\n\n\n\n\n\n\n\n\n注意到這裡我構建了兩個不同的標記方案。我們有一個「人」變項 p，所以說到 Yp 作為樣本中的第 p 人的脾氣是完全合理的。例如，表格顯示 Tim 是第四個，所以我們會說 \\(p = 4\\)。所以，在談到這個「Tim」這個人（無論他是誰）的脾氣 \\(Y\\) 時，我們可以通過說 \\(Y_p = 91\\) 來指稱他的脾氣，即對於人 \\(p = 4\\)。然而，這不是我們唯一可以指稱 Tim 的方法。作為一個替代方法，我們可以注意到 Tim 屬於「不酷」的組（\\(k = 2\\)），實際上是不酷組中列出的第一個人（\\(i = 1\\)）。所以，通過說 \\(Y_{ik} = 91\\)，在 \\(k = 2\\) 和 \\(i = 1\\) 的情況下，同樣有效地指稱 Tim 的脾氣。\n換句話說，每個人 p 都對應一個唯一的 ik 組合，所以我之前給出的公式實際上與我們原始的方差公式是相同的，即 \\[Var(Y)=\\frac{1}{N}\\sum_{p=1}^{N}(Y_p-\\bar{Y})^2\\] 在兩個公式中，我們所做的就是對樣本中的所有觀察值求和。大多數時候，我們只使用更簡單的 Yp 記號；使用 \\(Y_p\\) 的等式顯然是兩者中更簡單的一個。然而，在進行方差分析（ANOVA）時，我們需要跟踪哪些參與者屬於哪個組別，並且我們需要使用 Yik 記號來完成這項工作。\n\n\n\n\n13.2.2 變異數與平方差總和\n好的，既然我們對方差的計算有了很好的了解，讓我們定義一個叫做總平方和（total sum of squares）的東西，記作 SStot。這很簡單。計算方差時，我們是對平方偏差求平均，而計算總平方和時，我們只需將它們加起來。2\n當我們在 ANOVA 的上下文中談論分析變異數時，我們實際上是在處理總平方和，而不是實際的方差。3\n接下來，我們可以定義一個僅捕捉組間差異的變異概念。我們通過查看組平均值 \\(\\bar{Y}_k\\) 和整體平均值 \\(\\bar{Y}\\) 之間的差異來實現這一點。4\n這並不太難以證明，實驗中人們之間的總變異（\\(SS_{tot}\\)）實際上是組間差異（\\(SS_b\\)）和組內變異（\\(SS_w\\)）之和。即，\n\\[SS_w+SS_b=SS_{tot}\\] 好耶。\n好的，那麼我們發現了什麼？我們已經發現了與結果變項相關的總變異（\\(SS_{tot}\\)）可以在數學上被劃分為“由於不同組的樣本均值之間的差異所產生的變異”（\\(SS_b\\)）加上“其他所有變異”（\\(SS_w\\)）之和5。\n那怎麼幫助我找出這些組是否有不同的母體均值呢？嗯。等等。稍等一下。現在想想，這正是我們在尋找的。如果原假設成立，那麼您會期望所有樣本均值彼此非常相似，對吧？這將意味著您會期望 \\(SS_b\\) 非常小，或者至少您會期望它比“與其他所有事物相關的變異”（\\(SS_w\\)）小得多。嗯。我感覺到了一個假設檢驗的來臨。\n\n\n\n13.2.3 平方差總和與F檢定\n正如我們在上一節中看到的，ANOVA 的定性思想是將兩個平方和值 \\(SS_b\\) 和 \\(SS_w\\) 相互比較。如果組間變異 \\(SS_b\\) 相對於組內變異 \\(SS_w\\) 較大，那麼我們有理由懷疑不同組的母體均值彼此並不相同。為了將這一點轉化為可操作的假設檢驗，我們需要進行一些“小小的調整”。首先，我將向您展示我們如何計算檢驗統計量——F 值(F ratio)，然後嘗試讓您了解為什麼我們要這樣做。\n為了將我們的 SS 值轉換為 F 比，我們首先需要計算與 \\(SS_b\\) 和 \\(SS_w\\) 值相關的自由度。通常情況下，自由度對應於對特定計算做出貢獻的唯一“數據點”的數量，減去它們需要滿足的“約束”條件的數量。對於組內變異性，我們計算的是個體觀測值（\\(N\\) 個數據點）與組平均值（\\(G\\) 個約束）之間的變異。相反，對於組間變異性，我們關心的是組平均值（\\(G\\) 個數據點）在整體平均值（1 個約束）周圍的變化。因此，在這裡的自由度為：\n\\[df_b=G-1\\] \\[df_w=N-G\\]\n好吧，這似乎很簡單。接下來，我們將平方和值轉換為“平均平方”值，方法是除以自由度：\n\\[MS_b=\\frac{SS_b}{df_b}\\] \\[MS_w=\\frac{SS_w}{df_w}\\]\n最後，我們通過將組間 MS 除以組內 MS 來計算 F 比：\n\\[F=\\frac{MS_b}{MS_w}\\]\n從非常一般的層面上，F 統計量背後的直覺很簡單。F 值越大，表示組間變異相對於組內變異越大。因此，F 值越大，我們反駁虛無假設的證據就越多。但是 \\(F\\) 必須多大才能實際拒絕 \\(H_0\\)？要理解這一點，您需要更深入地了解 ANOVA 是什麼以及平均平方值實際上是什麼。\n下一節將詳細討論這個問題，但對於不感興趣實際衡量試驗內容的讀者，我將直接進入主題。為了完成我們的假設檢定，我們需要知道在虛無假設為真時 F 的抽樣分佈。不足為奇的是，在虛無假設下 F 統計量的抽樣分佈是一個 \\(F\\) 分佈。如果您回顧我們在 章节 7 中關於 F 分佈的討論，\\(F\\) 分佈有兩個參數，對應於涉及的兩個自由度。第一個 \\(df_1\\) 是組間自由度 \\(df_b\\)，第二個 \\(df_2\\) 是組內自由度 \\(df_w\\)。\n\n\n\n\n\n表格 13.2: ANOVA 中涉及的所有關鍵數量都組織成一個“標準” ANOVA 表。所有數量的公式（除了 p 值，它有一個非常難看的公式，如果沒有計算機，計算起來會非常困難）都有顯示。\n\n\n\nbetween\ngroups\nwithin\ngroups\n\n\ndf\n\\( df_b=G-1 \\)\n\\( df_w=N-G \\)\n\n\nsum of squares\n\\( SS_b=\\sum_{k=1}^{G} N_k (\\bar{Y}_k-\\bar{Y})^2 \\)\n\\( SS_w=\\sum_{k=1}^{G} \\sum_{i=1}^{N_k} (Y_{ik}-\\bar{Y}_k)^2 \\)\n\n\nmean squares\n\\( MS_b=\\frac{SS_b}{df_b} \\)\n\\( MS_w=\\frac{SS_w}{df_w} \\)\n\n\nF-statistic\n\\( F=\\frac{MS_b}{df_b} \\)\n-\n\n\np-value\n[complicated]\n-\n\n\n\n\n\n\n\n\n在 表格 13.2 中顯示了涉及單因素 ANOVA 的所有關鍵數量的概要，包括顯示如何計算它們的公式。\n[額外的技術細節 6]\n\n\n\n13.2.4 實例演練\n先前的討論相當抽象且有點技術性，所以我認為此刻可能需要看一個實際示例。為此，讓我們回到本章開頭介紹的臨床試驗數據。我們在開始時計算的描述性統計數據告訴我們各組的平均值：安慰劑的平均情緒增益為 \\(0.45\\)，Anxifree 為 \\(0.72\\)，Joyzepam 為 \\(1.48\\)。有了這個想法，讓我們像 1899 年一樣開趴7，開始用鉛筆和紙做一些計算。我只會對前 \\(5\\) 個觀察值進行此操作，因為現在不是該死的 \\(1899\\) 年，而且我非常懶。讓我們從計算 \\(SS_w\\) 開始，即組內平方和。首先，讓我們繪製一個漂亮的表格來協助我們的計算（ 表格 13.3 ）\n\n\n\n\n\n\n表格 13.3: 示範演算第一步\n\n\ngroup k\noutcome \\( Y_{ik} \\)\n\n\nplacebo\n0.5\n\n\nplacebo\n0.3\n\n\nplacebo\n0.1\n\n\nanxifree\n0.6\n\n\nanxifree\n0.4\n\n\n\n\n\n\n\n\n在這個階段，我在表格中包含的只是原始數據本身。也就是說，每個人的分組變項（即藥物）和結果變項（即心情增益）。請注意，這裡的結果變項對應於我們先前方程式中的 \\(\\bar{Y}_{ik}\\) 值。接下來的計算步驟是為研究中的每個人寫下相應的組平均值，\\(\\bar{Y}_k\\)。這有點重複，但並不是特別困難，因為我們在進行描述性統計時已經計算了這些組平均值，見 表格 13.4 。\n\n\n\n\n\n\n表格 13.4: 示範演算第二步\n\n\ngroup k\noutcome \\( Y_{ik} \\)\ngroup mean \\( \\bar{Y}_k \\)\n\n\nplacebo\n0.5\n0.45\n\n\nplacebo\n0.3\n0.45\n\n\nplacebo\n0.1\n0.45\n\n\nanxifree\n0.6\n0.72\n\n\nanxifree\n0.4\n0.72\n\n\n\n\n\n\n\n\n既然我們已經寫下了這些，我們需要再次為每個人計算與相應組平均值的偏差。也就是說，我們想要減去 \\(Y_{ik} - \\bar{Y}_k\\)。在我們做完這個之後，我們需要將所有東西平方。當我們這樣做時，這就是我們得到的結果（ 表格 13.5 ）\n\n\n\n\n\n表格 13.5: 示範演算第三步\n\n\ngroup k\noutcome \\( Y_{ik} \\)\ngroup mean \\( \\bar{Y}_k \\)\ndev. from group mean \\( Y_{ik} - \\bar{Y}_k \\)\nsquared deviation \\( (Y_{ik}-\\bar{Y}_k)^2 \\)\n\n\nplacebo\n0.5\n0.45\n0.05\n0.0025\n\n\nplacebo\n0.3\n0.45\n-0.15\n0.0225\n\n\nplacebo\n0.1\n0.45\n-0.35\n0.1225\n\n\nanxifree\n0.6\n0.72\n-0.12\n0.0136\n\n\nanxifree\n0.4\n0.72\n-0.32\n0.1003\n\n\n\n\n\n\n\n\n最後一步同樣簡單。為了計算組內平方和，我們只需將所有觀察值的平方偏差相加：\n\\[\n\\begin{split}\nSS_w & = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 \\\\\n& = 0.2614\n\\end{split}\n\\]\n當然，如果我們真的想得到正確的答案，我們需要對數據集中的所有18個觀察值進行此操作，而不僅僅是前五個。如果我們想要的話，我們可以繼續使用鉛筆和紙進行計算，但這相當繁瑣。或者，使用專用的電子表格程序（如 OpenOffice 或 Excel）也不是很困難。嘗試自己做。我在 Excel 中做的那個文件名為 clinicaltrial_anova.xls。當你做完後，你應該得到一個組內平方和值為 \\(1.39\\)。\n好的。現在我們已經計算了組內變異 \\(SS_w\\)，是時候將我們的注意力轉向組間平方和 \\(SS_b\\) 了。對於這種情況，計算非常相似。主要區別在於，對於所有觀察值，我們不再計算觀察值 Yik 和組平均值 \\(\\bar{Y}_k\\) 之間的差異，而是計算所有組的組平均值 \\(\\bar{Y}_k\\) 和總平均值 \\(\\bar{Y}\\)（在這種情況下為 \\(0.88\\)）之間的差異（表格 13.6）。\n\n\n\n\n\n表格 13.6: 示範演算第4步\n\n\ngroup k\ngroup mean \\( \\bar{Y}_k \\)\ngrand mean \\( \\bar{Y} \\)\ndeviation \\( \\bar{Y}_k - \\bar{Y} \\)\nsquared deviation \\( ( \\bar{Y}_k-\\bar{Y})^2 \\)\n\n\nplacebo\n0.45\n0.88\n-0.43\n0.19\n\n\nanxifree\n0.72\n0.88\n-0.16\n0.03\n\n\njoyzepam\n1.48\n0.88\n0.60\n0.36\n\n\n\n\n\n\n\n\n然而，對於組間計算，我們需要將每個平方偏差乘以 \\(N_k\\)，即組中的觀察值數量。我們這樣做是因為該組中的每個觀察值（所有 \\(N_k\\) 個觀察值）都與組間差異有關。因此，如果安慰劑組有六個人，並且安慰劑組的平均值與總平均值相差 \\(0.19\\)，那麼這六個人與組間變異之間的關聯總和為 \\(6 \\times 0.19 = 1.14\\)。因此，我們必須擴展我們的計算表格（ 表格 13.7 ）。\n\n\n\n\n\n\n表格 13.7: 示範演算第5步\n\n\ngroup k\n...\nsquared deviations \\( (\\bar{Y}_k-\\bar{Y})^2 \\)\nsample size \\( N_k \\)\nweighted squared dev \\( N_k (\\bar{Y}_k-\\bar{Y})^2 \\)\n\n\nplacebo\n...\n0.19\n6\n1.14\n\n\nanxifree\n...\n0.03\n6\n0.18\n\n\njoyzepam\n...\n0.36\n6\n2.16\n\n\n\n\n\n\n\n\n現在，我們的組間平方和是通過將這些“加權平方偏差”在研究中的所有三組中求和而得到的：\n\\[\\begin{aligned} SS_b & = 1.14 + 0.18 + 2.16 \\\\ &= 3.48 \\end{aligned}\\]\n如您所見，組間計算要短得多 8。現在我們已經計算出了平方和值 \\(SS_b\\) 和 \\(SS_w\\)，剩下的 ANOVA 分析就相當簡單了。下一步是計算自由度。由於我們有 \\(G = 3\\) 個組和 \\(N = 18\\) 個觀察值，我們的自由度可以通過簡單的減法來計算：\n\\[\n\\begin{split}\ndf_b & = G-1 = 2 \\\\\ndf_w & = N-G = 15\n\\end{split}\n\\]\n接下來，由於我們已經計算了平方和值和自由度的值，對於組內變異性和組間變異性，我們可以通過將一個除以另一個來獲得平均平方值：\n\\[\n\\begin{split}\nMS_b & = \\frac{SS_b}{df_b} = \\frac{3.48}{2} = 1.74 \\\\\nMS_w & = \\frac{SS_w}{df_w} = \\frac{1.39}{15} = 0.09\n\\end{split}\n\\]\n我們快完成了。平均平方值可用於計算我們感興趣的 F 值，這是我們感興趣的檢驗統計量。我們通過將組間 MS 值除以組內 MS 值來完成此操作。\n\\[\n\\begin{split}\nF & = \\frac{MS_b}{MS_w}  = \\frac{1.74}{0.09} \\\\\n& = 19.3\n\\end{split}\n\\]\n哇！這真的非常令人興奮，對嗎？現在我們有了檢驗統計量，最後一步是找出檢驗本身是否給我們一個顯著結果。如 章节 9 在“過去的日子”中所討論的，我們要做的是打開一本統計教科書或翻到後面的部分，這裡會有一個巨大的查找表，我們會找到對應特定 alpha 值（空假設拒絕區域）的閾值 F 值，例如 \\(0.05\\)，\\(0.01\\) 或 \\(0.001\\)，對於 2 和 15 度的自由度。用這種方法，對於 alpha 為 \\(0.001\\) 的情況，我們會得到一個閾值 F 值為 \\(11.34\\)。由於這小於我們計算出的 F 值，我們說 \\(p &lt; 0.001\\)。但那是過去的日子，現在花哨的統計軟件會為您計算出確切的 p 值。實際上，確切的 p 值為 \\(0.000071\\)。所以，除非我們對 Type I 錯誤率非常保守，否則我們幾乎可以保證拒絕虛無假設。\n此刻，我們基本上完成了。完成計算後，將所有這些數字整理成類似於表 12.1 的 ANOVA 表是傳統做法。對於我們的臨床試驗數據，ANOVA 表將如 表格 13.8 。\n\n\n\n\n\n\n表格 13.8: 完整的變異數分析結果表\n\n\n\ndf\nsum of squares\nmean squares\nF-statistic\np-value\n\n\nbetween groups\n2\n3.48\n1.74\n19.3\n0.000071\n\n\nwithin groups\n15\n1.39\n0.09\n-\n-\n\n\n\n\n\n\n\n\n如今，您可能永遠沒有太多理由想要自己構建這樣的表格，但您會發現幾乎所有的統計軟件（包括 jamovi）都傾向於將 ANOVA 的輸出組織成這樣的表格，所以最好習慣閱讀它們。然而，儘管軟件將輸出完整的 ANOVA 表，但幾乎從來沒有充分理由在您的撰寫中包含整個表格。報告此結果的統計塊的一種非常標準的方法是寫下類似以下的內容：\n\n單因素 ANOVA 顯示藥物對情緒增益有顯著影響（F(2,15) = 19.3，p &lt; .001）。\n\n嘆氣。這麼多工作，只為了一個簡短的句子。"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.3 jamovi的變異數分析模組",
    "text": "13.3 jamovi的變異數分析模組\n我相當確定在讀完上一節之後，您在想什麼，特別是如果您按照我的建議，用鉛筆和紙（即在試算表中）自己完成所有這些工作。自己做 ANOVA 計算很糟糕。沿途我們需要做相當多的計算，如果每次想做 ANOVA 都要一次又一次地做這些計算，會讓人厭煩。\n\n\n13.3.1 使用jamovi完成變異數分析\n為了讓您的生活更輕鬆，jamovi 可以做 ANOVA… 哈拉！ 轉到「ANOVA」-「ANOVA」分析，將 mood.gain 變項移到「依賴變項」框中，然後將 drug 變項移到「固定因子」框中。這樣應該會得到 图 13.3 中所示的結果。9 注意我還勾選了 ’Effect Size’選項下的 \\(\\eta^2\\) 复选框，念作“ eta 平方”，這也顯示在結果表格上。稍後我們將回到效應大小。\n\n\n\n\n\n\n\n图 13.3: jamovi的結果表格，用於根據施用的藥物進行情緒增益的 ANOVA。\n\n\n\n\njamovi 的結果表格顯示了平方和值、自由度以及我們現在並不真正感興趣的其他一些數量。然而，請注意，jamovi 不使用「組間」和「組內」這兩個名稱。 相反，它嘗試分配更有意義的名稱。 在我們的特定示例中，組間方差對應於藥物對結果變項的影響，組內方差對應於“剩餘”的可變性，因此它將其稱為殘差。 如果我們將這些數字與 [A worked example] 中我手工計算的數字進行比較，可以看到它們或多或少是相同的，除了四捨五入誤差。組間平方和為 \\(SS_b = 3.45\\)，組內平方和為 \\(SS_w = 1.39\\)，各自的自由度為 \\(2\\) 和 \\(15\\)。我們還得到了 F 值和 p 值，同樣，這些數字與我們在手工計算時的數字差不多相同，只是四捨五入誤差。"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#effect-size",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#effect-size",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.4 Effect size",
    "text": "13.4 Effect size\nThere’s a few different ways you could measure the effect size in an ANOVA, but the most commonly used measures are \\(\\eta^2\\) (eta squared) and partial \\(\\eta^2\\). For a one way analysis of variance they’re identical to each other, so for the moment I’ll just explain \\(\\eta^2\\) . The definition of \\(\\eta^2\\) is actually really simple\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\nThat’s all it is. So when I look at the ANOVA table in Figure 13.3, I see that \\(SS_b = 3.45\\) and \\(SS_tot = 3.45 + 1.39 = 4.84\\). Thus we get an \\(\\eta^2\\) value of\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\nThe interpretation of \\(\\eta^2\\) is equally straightforward. It refers to the proportion of the variability in the outcome variable (mood.gain) that can be explained in terms of the predictor (drug). A value of \\(\\eta^2=0\\) means that there is no relationship at all between the two, whereas a value of \\(\\eta^2=1\\) means that the relationship is perfect. Better yet, the \\(\\eta^2\\) value is very closely related to \\(R^2\\), as discussed previously in Section 12.6.1, and has an equivalent interpretation. Although many statistics text books suggest \\(\\eta^2\\) as the default effect size measure in ANOVA, there’s an interesting blog post by Daniel Lakens suggesting that eta-squared is perhaps not the best measure of effect size in real world data analysis, because it can be a biased estimator. Usefully, there is also an option in jamovi to specify omega-squared (\\(\\omega^2\\)), which is less biased, alongside eta-squared."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#multiple-comparisons-and-post-hoc-tests",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#multiple-comparisons-and-post-hoc-tests",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.5 Multiple comparisons and post hoc tests",
    "text": "13.5 Multiple comparisons and post hoc tests\nAny time you run an ANOVA with more than two groups and you end up with a significant effect, the first thing you’ll probably want to ask is which groups are actually different from one another. In our drugs example, our null hypothesis was that all three drugs (placebo, Anxifree and Joyzepam) have the exact same effect on mood. But if you think about it, the null hypothesis is actually claiming three different things all at once here. Specifically, it claims that:\n\nYour competitor’s drug (Anxifree) is no better than a placebo (i.e., \\(\\mu_A = \\mu_P\\) )\nYour drug (Joyzepam) is no better than a placebo (i.e., \\(\\mu_J = \\mu_P\\) )\nAnxifree and Joyzepam are equally effective (i.e., \\(\\mu_J = \\mu_A\\))\n\nIf any one of those three claims is false, then the null hypothesis is also false. So, now that we’ve rejected our null hypothesis, we’re thinking that at least one of those things isn’t true. But which ones? All three of these propositions are of interest. Since you certainly want to know if your new drug Joyzepam is better than a placebo, it would be nice to know how well it stacks up against an existing commercial alternative (i.e., Anxifree). It would even be useful to check the performance of Anxifree against the placebo. Even if Anxifree has already been extensively tested against placebos by other researchers, it can still be very useful to check that your study is producing similar results to earlier work.\nWhen we characterise the null hypothesis in terms of these three distinct propositions, it becomes clear that there are eight possible “states of the world” that we need to distinguish between (Table 13.9).\n\n\n\n\nTable 13.9:  The null hypothesis and eight possible ‘states of the world’ \n\npossibility:is \\( \\mu_P = \\mu_A \\)?is \\( \\mu_P = \\mu_J \\)?is \\( \\mu_A = \\mu_J \\)?which hypothesis?\n\n1\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)null\n\n2\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n3\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n4\\( \\checkmark \\)alternative\n\n5\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n6\\( \\checkmark \\)alternative\n\n7\\( \\checkmark \\)alternative\n\n8alternative\n\n\n\n\n\nBy rejecting the null hypothesis, we’ve decided that we don’t believe that #1 is the true state of the world. The next question to ask is, which of the other seven possibilities do we think is right? When faced with this situation, its usually helps to look at the data. For instance, if we look at the plots in Figure 13.1, it’s tempting to conclude that Joyzepam is better than the placebo and better than Anxifree, but there’s no real difference between Anxifree and the placebo. However, if we want to get a clearer answer about this, it might help to run some tests.\n\n13.5.1 Running “pairwise” t-tests\nHow might we go about solving our problem? Given that we’ve got three separate pairs of means (placebo versus Anxifree, placebo versus Joyzepam, and Anxifree versus Joyzepam) to compare, what we could do is run three separate t-tests and see what happens. This is easy to do in jamovi. Go to the ANOVA ‘Post Hoc Tests’ options, move the ‘drug’ variable across into the active box on the right, and then click on the ‘No correction’ checkbox. This will produce a neat table showing all the pairwise t-test comparisons amongst the three levels of the drug variable, as in Figure 13.4\n\n\n\n\n\nFigure 13.4: Uncorrected pairwise t-tests as post hoc comparisons in jamovi\n\n\n\n\n\n\n13.5.2 Corrections for multiple testing\nIn the previous section I hinted that there’s a problem with just running lots and lots of t-tests. The concern is that, when running these analyses, what we’re doing is going on a “fishing expedition”. We’re running lots and lots of tests without much theoretical guidance in the hope that some of them come up significant. This kind of theory-free search for group differences is referred to as post hoc analysis (“post hoc” being Latin for “after this”).10\nIt’s okay to run post hoc analyses, but a lot of care is required. For instance, the analysis that I ran in the previous section should be avoided, as each individual t-test is designed to have a 5% Type I error rate (i.e., \\(\\alpha = .05\\)) and I ran three of these tests. Imagine what would have happened if my ANOVA involved 10 different groups, and I had decided to run 45 “post hoc” t-tests to try to find out which ones were significantly different from each other, you’d expect 2 or 3 of them to come up significant by chance alone. As we saw in Chapter 9, the central organising principle behind null hypothesis testing is that we seek to control our Type I error rate, but now that I’m running lots of t-tests at once in order to determine the source of my ANOVA results, my actual Type I error rate across this whole family of tests has gotten completely out of control.\nThe usual solution to this problem is to introduce an adjustment to the p-value, which aims to control the total error rate across the family of tests (see Shaffer (1995)). An adjustment of this form, which is usually (but not always) applied because one is doing post hoc analysis, is often referred to as a correction for multiple comparisons, though it is sometimes referred to as “simultaneous inference”. In any case, there are quite a few different ways of doing this adjustment. I’ll discuss a few of them in this section and in Section 14.8 in the next chapter, but you should be aware that there are many other methods out there (see, e.g., Hsu (1996)).\n\n\n13.5.3 Bonferroni corrections\nThe simplest of these adjustments is called the Bonferroni correction (Dunn, 1961), and it’s very very simple indeed. Suppose that my post hoc analysis consists of m separate tests, and I want to ensure that the total probability of making any Type I errors at all is at most \\(\\alpha\\).11 If so, then the Bonferroni correction just says “multiply all your raw p-values by m”. If we let \\(p\\) denote the original p-value, and let \\(p_j^{'}\\) be the corrected value, then the Bonferroni correction tells that:\n\\[p_j^{'}=m \\times p\\]\nAnd therefore, if you’re using the Bonferroni correction, you would reject the null hypothesis if \\(p_j^{'} < \\alpha\\). The logic behind this correction is very straightforward. We’re doing m different tests, so if we arrange it so that each test has a Type I error rate of at most \\(\\frac{\\alpha}{m}\\), then the total Type I error rate across these tests cannot be larger than \\(\\alpha\\). That’s pretty simple, so much so that in the original paper, the author writes:\n\nThe method given here is so simple and so general that I am sure it must have been used before this. I do not find it, however, so can only conclude that perhaps its very simplicity has kept statisticians from realizing that it is a very good method in some situations (Dunn (1961), pp 52-53).\n\nTo use the Bonferroni correction in jamovi, just click on the ‘Bonferroni’ checkbox in the ‘Correction’ options, and you will see another column added to the ANOVA results table showing the adjusted p-values for the Bonferroni correction (Table 13.8). If we compare these three p-values to those for the uncorrected, pairwise t-tests, it is clear that the only thing that jamovi has done is multiply them by \\(3\\).\n\n\n13.5.4 Holm corrections\nAlthough the Bonferroni correction is the simplest adjustment out there, it’s not usually the best one to use. One method that is often used instead is the Holm correction (Holm, 1979). The idea behind the Holm correction is to pretend that you’re doing the tests sequentially, starting with the smallest (raw) p-value and moving onto the largest one. For the j-th largest of the p-values, the adjustment is either\n\\[p_j^{'}=j \\times p_j\\]\n(i.e., the biggest p-value remains unchanged, the second biggest p-value is doubled, the third biggest p-value is tripled, and so on), or\n\\[p_j^{'}=p_{j+1}^{'}\\]\nwhichever one is larger. This might sound a little confusing, so let’s go through it a little more slowly. Here’s what the Holm correction does. First, you sort all of your p-values in order, from smallest to largest. For the smallest p-value all you do is multiply it by \\(m\\), and you’re done. However, for all the other ones it’s a two-stage process. For instance, when you move to the second smallest p value, you first multiply it by \\(m - 1\\). If this produces a number that is bigger than the adjusted p-value that you got last time, then you keep it. But if it’s smaller than the last one, then you copy the last p-value. To illustrate how this works, consider Table 13.10 which shows the calculations of a Holm correction for a collection of five p-values.\n\n\n\n\nTable 13.10:  Holm corrected p values \n\nraw prank jp \\( \\times \\) jHolm p\n\n.0015.005.005\n\n.0054.020.020\n\n.0193.057.057\n\n.0222.044.057\n\n.1031.103.103\n\n\n\n\n\nHopefully that makes things clear.\nAlthough it’s a little harder to calculate, the Holm correction has some very nice properties. It’s more powerful than Bonferroni (i.e., it has a lower Type II error rate) but, counter-intuitive as it might seem, it has the same Type I error rate. As a consequence, in practice there’s never any reason to use the simpler Bonferroni correction since it is always outperformed by the slightly more elaborate Holm correction. Because of this, the Holm correction should be your go to multiple comparison correction. Figure 13.4 also shows the Holm corrected p-values and, as you can see, the biggest p-value (corresponding to the comparison between Anxifree and the placebo) is unaltered. At a value of .15, it is exactly the same as the value we got originally when we applied no correction at all. In contrast, the smallest p-value (Joyzepam versus placebo) has been multiplied by three.\n\n\n13.5.5 Writing up the post hoc test\nFinally, having run the post hoc analysis to determine which groups are significantly different to one another, you might write up the result like this:\n\nPost hoc tests (using the Holm correction to adjust p) indicated that Joyzepam produced a significantly larger mood change than both Anxifree (p = .001) and the placebo (\\((p = 9.0 \\times{10^{-5}}\\)). We found no evidence that Anxifree performed better than the placebo (\\(p = .15\\)).\n\nOr, if you don’t like the idea of reporting exact p-values, then you’d change those numbers to \\(p < .01\\), \\(p < .001\\) and \\(p > .05\\) respectively. Either way, the key thing is that you indicate that you used Holm’s correction to adjust the p-values. And of course, I’m assuming that elsewhere in the write up you’ve included the relevant descriptive statistics (i.e., the group means and standard deviations), since these p-values on their own aren’t terribly informative."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#the-assumptions-of-one-way-anova",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#the-assumptions-of-one-way-anova",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.6 The assumptions of one-way ANOVA",
    "text": "13.6 The assumptions of one-way ANOVA\nLike any statistical test, analysis of variance relies on some assumptions about the data, specifically the residuals. There are three key assumptions that you need to be aware of: normality, homogeneity of variance and independence.\n[Additional technical detail 12]\nSo, how do we check whether the assumption about the residuals is accurate? Well, as I indicated above, there are three distinct claims buried in this one statement, and we’ll consider them separately.\n\nHomogeneity of variance. Notice that we’ve only got the one value for the population standard deviation (i.e., \\(\\sigma\\)), rather than allowing each group to have it’s own value (i.e., \\(\\sigma_k\\)). This is referred to as the homogeneity of variance (sometimes called homoscedasticity) assumption. ANOVA assumes that the population standard deviation is the same for all groups. We’ll talk about this extensively in the Checking the homogeneity of variance assumption section.\nNormality. The residuals are assumed to be normally distributed. As we saw in Section 11.9, we can assess this by looking at QQ plots (or running a Shapiro-Wilk test. I’ll talk about this more in an ANOVA context in the Checking the normality assumption section.\nIndependence. The independence assumption is a little trickier. What it basically means is that, knowing one residual tells you nothing about any other residual. All of the \\(\\epsilon_{ik}\\) values are assumed to have been generated without any “regard for” or “relationship to” any of the other ones. There’s not an obvious or simple way to test for this, but there are some situations that are clear violations of this. For instance, if you have a repeated measures design, where each participant in your study appears in more than one condition, then independence doesn’t hold. There’s a special relationship between some observations, namely those that correspond to the same person! When that happens, you need to use something like a Repeated measures one-way ANOVA.\n\n\n13.6.1 Checking the homogeneity of variance assumption\n\nTo make the preliminary test on variances is rather like putting to sea in a rowing boat to find out whether conditions are sufficiently calm for an ocean liner to leave port!\n– George Box (Box, 1953)\n\nThere’s more than one way to skin a cat, as the saying goes, and more than one way to test the homogeneity of variance assumption, too (though for some reason no-one made a saying out of that). The most commonly used test for this that I’ve seen in the literature is the Levene test (Levene, 1960), and the closely related Brown-Forsythe test (Brown & Forsythe, 1974).\nRegardless of whether you’re doing the standard Levene test or the Brown-Forsythe test, the test statistic, which is sometimes denoted \\(F\\) but also sometimes written as \\(W\\), is calculated in exactly the same way that the F-statistic for the regular ANOVA is calculated, just using a \\(Z_{ik}\\) rather than \\(Y_{ik}\\). With that in mind, we can go on to look at how to run the test in jamovi.\n[Additional technical detail 13]\n\n\n13.6.2 Running the Levene test in jamovi\nOkay, so how do we run the Levene test? Simple really - under the ANOVA ‘Assumption Checks’ option, just click on the ‘Homogeneity tests’ checkbox. If we look at the output, shown in Figure 13.5, we see that the test is non-significant (\\(F_{2,15} = 1.45, p = .266\\)), so it looks like the homogeneity of variance assumption is fine. However, looks can be deceptive! If your sample size is pretty big, then the Levene test could show up a significant effect (i.e. p < .05) even when the homogeneity of variance assumption is not violated to an extent which troubles the robustness of ANOVA. This was the point George Box was making in the quote above. Similarly, if your sample size is quite small, then the homogeneity of variance assumption might not be satisfied and yet a Levene test could be non-significant (i.e. p > .05). What this means is that, alongside any statistical test of the assumption being met, you should always plot the standard deviation around the means for each group / category in the analysis…just to see if they look fairly similar (i.e. homogeneity of variance) or not.\n\n\n\n\n\nFigure 13.5: Levene test output for one-way ANOVA in jamovi\n\n\n\n\n\n\n13.6.3 Removing the homogeneity of variance assumption\nIn our example, the homogeneity of variance assumption turned out to be a pretty safe one: the Levene test came back non-significant (notwithstanding that we should also look at the plot of standard deviations), so we probably don’t need to worry. However, in real life we aren’t always that lucky. How do we save our ANOVA when the homogeneity of variance assumption is violated? If you recall from our discussion of t-tests, we’ve seen this problem before. The Student t-test assumes equal variances, so the solution was to use the Welch t-test, which does not. In fact, Welch (1951) also showed how we can solve this problem for ANOVA too (the Welch one-way test). It’s implemented in jamovi using the One-Way ANOVA analysis. This is a specific analysis approach just for one-way ANOVA, and to run the Welch one-way ANOVA for our example, we would re-run the analysis as previously, but this time use the jamovi ANOVA - One Way ANOVA analysis command, and check the option for Welch’s test (see Figure 13.6). To understand what’s happening here, let’s compare these numbers to what we got earlier when Running an ANOVA in jamovi originally. To save you the trouble of flicking back, this is what we got last time: \\(F(2, 15) = 18.611, p = .00009\\), also shown as the Fisher’s test in the One-Way ANOVA shown in Figure 13.6.\n\n\n\n\n\nFigure 13.6: Welch’s test as part of the One Way ANOVA analysis in jamovi\n\n\n\n\nOkay, so originally our ANOVA gave us the result \\(F(2, 15) = 18.6\\), whereas the Welch one way test gave us \\(F(2, 9.49) = 26.32\\). In other words, the Welch test has reduced the within-groups degrees of freedom from 15 to 9.49, and the F-value has increased from 18.6 to 26.32.\n\n\n13.6.4 Checking the normality assumption\nTesting the normality assumption is relatively straightforward. We covered most of what you need to know in Section 11.9. The only thing we really need to do is draw a QQ plot and, in addition if it is available, run the Shapiro-Wilk test. The QQ plot is shown in Figure 13.7 and it looks pretty normal to me. If the Shapiro-Wilk test is not significant (i.e. \\(p > .05\\)) then this indicates that the assumption of normality is not violated. However, as with Levene’s test, if the sample size is large then a significant Shapiro-Wilk test may in fact be a false positive, where the assumption of normality is not violated in any substantive problematic sense for the analysis. And, similarly, a very small sample can produce false negatives. That’s why a visual inspection of the QQ plot is important.\n\n\n\n\n\nFigure 13.7: QQ plot in the One Way ANOVA analysis in jamovi\n\n\n\n\nAlongside inspecting the QQ plot for any deviations from normality, the Shapiro-Wilk test for our data does show a non-significant effect, with p = 0.6053 (see Figure 13.6. This therefore supports the QQ plot assessment; both checks find no indication that normality is violated.\n\n\n13.6.5 Removing the normality assumption\nNow that we’ve seen how to check for normality, we are led naturally to ask what we can do to address violations of normality. In the context of a one-way ANOVA, the easiest solution is probably to switch to a non-parametric test (i.e., one that doesn’t rely on any particular assumption about the kind of distribution involved). We’ve seen non-parametric tests before, in Chapter 11. When you only have two groups, the Mann-Whitney or the Wilcoxon test provides the non-parametric alternative that you need. When you’ve got three or more groups, you can use the Kruskal-Wallis rank sum test (Kruskal & Wallis, 1952). So that’s the test we’ll talk about next.\n\n\n13.6.6 The logic behind the Kruskal-Wallis test\nThe Kruskal-Wallis test is surprisingly similar to ANOVA, in some ways. In ANOVA we started with \\(Y_{ik}\\), the value of the outcome variable for the ith person in the kth group. For the Kruskal Wallis test what we’ll do is rank order all of these \\(Y_{ik}\\) values and conduct our analysis on the ranked data. 14\n\n\n13.6.7 Additional details\nThe description in the previous section illustrates the logic behind the Kruskal-Wallis test. At a conceptual level, this is the right way to think about how the test works.15\nBut wait, there’s more! Dear lord, why is there always more? The story I’ve told so far is only actually true when there are no ties in the raw data. That is, if there are no two observations that have exactly the same value. If there are ties, then we have to introduce a correction factor to these calculations. At this point I’m assuming that even the most diligent reader has stopped caring (or at least formed the opinion that the tie-correction factor is something that doesn’t require their immediate attention). So I’ll very quickly tell you how it’s calculated, and omit the tedious details about why it’s done this way. Suppose we construct a frequency table for the raw data, and let fj be the number of observations that have the j-th unique value. This might sound a bit abstract, so here’s a concrete example from the frequency table of mood.gain from the clinicaltrials.csv data set (Table 13.11)\n\n\n\n\nTable 13.11:  Frequency table of mood gain from the clinicaltrials.csv data \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\nLooking at this table, notice that the third entry in the frequency table has a value of 2. Since this corresponds to a mood.gain of 0.3, this table is telling us that two people’s mood increased by 0.3. 16\nAnd so jamovi uses a tie-correction factor to calculate the tie-corrected Kruskall-Wallis statistic. And at long last, we’re actually finished with the theory of the Kruskal-Wallis test. I’m sure you’re all terribly relieved that I’ve cured you of the existential anxiety that naturally arises when you realise that you don’t know how to calculate the tie-correction factor for the Kruskal-Wallis test. Right?\n\n\n13.6.8 How to run the Kruskal-Wallis test in jamovi\nDespite the horror that we’ve gone through in trying to understand what the Kruskal Wallis test actually does, it turns out that running the test is pretty painless, since jamovi has an analysis as part of the ANOVA analysis set called ‘Non-Parametric’ - ‘One Way ANOVA (Kruskall-Wallis)’ Most of the time you’ll have data like the clinicaltrial.csv data set, in which you have your outcome variable mood.gain and a grouping variable drug. If so, you can just go ahead and run the analysis in jamovi. What this gives us is a Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\), as in Figure 13.8\n\n\n\n\n\nFigure 13.8: Kruskall-Wallis one-way non-parametric ANOVA in jamovi"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#repeated-measures-one-way-anova",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#repeated-measures-one-way-anova",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.7 Repeated measures one-way ANOVA",
    "text": "13.7 Repeated measures one-way ANOVA\nThe one-way repeated measures ANOVA test is a statistical method of testing for significant differences between three or more groups where the same participants are used in each group (or each participant is closely matched with participants in other experimental groups). For this reason, there should always be an equal number of scores (data points) in each experimental group. This type of design and analysis can also be called a ‘related ANOVA’ or a ‘within subjects ANOVA’.\nThe logic behind a repeated measures ANOVA is very similar to that of an independent ANOVA (sometimes called a ‘between-subjects’ ANOVA). You’ll remember that earlier we showed that in a between-subjects ANOVA total variability is partitioned into between-groups variability (\\(SS_b\\)) and within-groups variability (\\(SS_w\\)), and after each is divided by the respective degrees of freedom to give MSb and MSw (see Table 13.1) the F-ratio is calculated as:\n\\[F=\\frac{MS_b}{MS_w}\\]\nIn a repeated measures ANOVA, the F-ratio is calculated in a similar way, but whereas in an independent ANOVA the within-group variability (\\(SS_w\\)) is used as the basis for the \\(MS_w\\) denominator, in a repeated measures ANOVA the \\(SS_w\\) is partioned into two parts. As we are using the same subjects in each group, we can remove the variability due to the individual differences between subjects (referred to as SSsubjects) from the within-groups variability. We won’t go into too much technical detail about how this is done, but essentially each subject becomes a level of a factor called subjects. The variability in this within-subjects factor is then calculated in the same way as any between-subjects factor. And then we can subtract SSsubjects from \\(SS_w\\) to provide a smaller SSerror term:\n\\[\\text{Independent ANOVA: } SS_{error} = SS_w\\] \\[\\text{Repeated Measures ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] This change in \\(SS_{error}\\) term often leads to a more powerful statistical test, but this does depend on whether the reduction in the \\(SS_{error}\\) more than compensates for the reduction in degrees of freedom for the error term (as degrees of freedom go from \\((n - k)\\) 17 to \\((n - 1)(k - 1)\\) (remembering that there are more subjects in the independent ANOVA design).\n\n13.7.1 Repeated measures ANOVA in jamovi\nFirst, we need some data. Geschwind (1972) has suggested that the exact nature of a patient’s language deficit following a stroke can be used to diagnose the specific region of the brain that has been damaged. A researcher is concerned with identifying the specific communication difficulties experienced by six patients suffering from Broca’s Aphasia (a language deficit commonly experienced following a stroke) (Table 13.12).\n\n\n\n\nTable 13.12:  Word recognition task scores in stroke patients \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\nThe patients were required to complete three word recognition tasks. On the first (speech production) task, patients were required to repeat single words read out aloud by the researcher. On the second (conceptual) task, designed to test word comprehension, patients were required to match a series of pictures with their correct name. On the third (syntax) task, designed to test knowledge of correct word order, patients were asked to reorder syntactically incorrect sentences. Each patient completed all three tasks. The order in which patients attempted the tasks was counterbalanced between participants. Each task consisted of a series of 10 attempts. The number of attempts successfully completed by each patient are shown in Table 13.11. Enter these data into jamovi ready for analysis (or take a short-cut and load up the broca.csv file).\nTo perform a one-way related ANOVA in jamovi, open the one-way repeated measures ANOVA dialogue box, as in Figure 13.9, via ANOVA - Repeated Measures ANOVA.\n\n\n\n\n\nFigure 13.9: Repeated measures ANOVA dialogue box in jamovi\n\n\n\n\nThen:\n\nEnter a Repeated Measures Factor Name. This should be a label that you choose to describe the conditions repeated by all participants. For example, to describe the speech, conceptual and syntax tasks completed by all participants a suitable label would be ‘Task’. Note that this new factor name represents the independent variable in the analysis.\nAdd a third level in the Repeated Measures Factors text box, as there are three levels representing the three tasks: speech, conceptual and syntax. Change the labels of the levels accordingly.\nThen move each of the levels variables across to the Repeated Measures Cells text box.\nFinally, under the Assumption Checks option, tick the “Sphericity checks” text box.\n\njamovi output for a one-way repeated measures ANOVA is produced as shown in Figure 13.10 to Figure 13.13. The first output we should look at is Mauchly’s Test of Sphericity, which tests the hypothesis that the variances of the differences between the conditions are equal (meaning that the spread of difference scores between the study conditions is approximately the same). In Figure 13.10 Mauchly’s test significance level is \\(p = .720\\). If Mauchly’s test is non-significant (i.e. p < .05, as is the case in this analysis) then it is reasonable to conclude that the variances of the differences are not significantly different (i.e. they are roughly equal and sphericity can be assumed.).\n\n\n\n\n\nFigure 13.10: One-way repeated measures ANOVA output - Mauchly’s Test of Sphericity\n\n\n\n\nIf, on the other hand, Mauchly’s test had been significant (p < .05) then we would conclude that there are significant differences between the variance of the differences, and the requirement of sphericity has not been met. In this case, we should apply a correction to the F-value obtained in the one-way related ANOVA analysis:\n\nIf the Greenhouse-Geisser value in the “Tests of Sphericity” table is > .75 then you should use the Huynh-Feldt correction\nBut if the Greenhouse-Geisser value is < .75, then you should use the Greenhouse-Geisser correction.\n\nBoth these corrected F-values can be specified in the Sphericity Corrections check boxes under the Assumption Checks options, and the corrected F-values are then shown in the results table, as in Figure 13.11.\n\n\n\n\n\nFigure 13.11: One-way repeated measures ANOVA output - Tests of Within-Subjects Effects\n\n\n\n\nIn our analysis, we saw that the significance of Mauchly’s Test of Sphericity was p = .720 (i.e. p > 0.05). So, this means we can assume that the requirement of sphericity has been met so no correction to the F-value is needed. Therefore, we can use the ‘None’ Sphericity Correction output values for the repeated measure ‘Task’: \\(F = 6.93\\), \\(df = 2\\), \\(p = .013\\), and we can conclude that the number of tests successfully completed on each language task did vary significantly depending on whether the task was speech, comprehension or syntax based (\\(F(2, 10) = 6.93\\), \\(p = .013\\)).\nPost-hoc tests can also be specified in jamovi for repeated measures ANOVA in the same way as for independent ANOVA. The results are shown in Figure 13.12. These indicate that there is a significant difference between Speech and Syntax, but not between other levels.\n\n\n\n\n\nFigure 13.12: Post-hoc tests in repeated measures ANOVA in jamovi\n\n\n\n\nDescriptive statistics (marginal means) can be reviewed to help interpret the results, produced in the jamovi output as in Figure 13.13. Comparison of the mean number of trials successfully completed by participants shows that Broca’s Aphasics perform reasonably well on speech production (mean = 7.17) and language comprehension (mean = 6.17) tasks. However, their performance was considerably worse on the syntax task (mean = 4.33), with a significant difference in post-hoc tests between Speech and Syntax task performance.\n\n\n\n\n\nFigure 13.13: One-way repeated measures ANOVA output - Descriptive Statistics"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#the-friedman-non-parametric-repeated-measures-anova-test",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#the-friedman-non-parametric-repeated-measures-anova-test",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.8 The Friedman non-parametric repeated measures ANOVA test",
    "text": "13.8 The Friedman non-parametric repeated measures ANOVA test\nThe Friedman test is a non-parametric version of a repeated measures ANOVA and can be used instead of the Kruskall-Wallis test when testing for differences between three or more groups where the same participants are in each group, or each participant is closely matched with participants in other conditions. If the dependent variable is ordinal, or if the assumption of normality is not met, then the Friedman test can be used.\nAs with the Kruskall-Wallis test, the underlying mathematics is complicated, and won’t be presented here. For the purpose of this book, it is sufficient to note that jamovi calculates the tie-corrected version of the Friedman test, and in Figure 13.14 there is an example using the Broca’s Aphasia data we have already looked at.\n\n\n\n\n\nFigure 13.14: The ‘Repeated Measures ANOVA (Non-parametric)’ dialogue box and results in jamovi\n\n\n\n\nIt’s pretty straightforward to run a Friedman test in jamovi. Just select Analyses - ANOVA - Repeated Measures ANOVA (Non-parametric), as in Figure 13.14. Then highlight and transfer the names of the repeated measures variables you wish to compare (Speech, Conceptual, Syntax) into the ‘Measures:’ text box. To produce descriptive statistics (means and medians) for the three repeated measures variables, click on the Descriptives button\nThe jamovi results show descriptive statistics, chi-square value, degrees of freedom, and the p-value (Figure 13.14). Since the p-value is less than the level conventionally used to determine significance (p < .05), we can conclude that Broca’s Aphasics perform reasonably well on speech production (median = 7.5) and language comprehension (median = 6.5) tasks. However, their performance was considerably worse on the syntax task (median = 4.5), with a significant difference in post-hoc tests between Speech and Syntax task performance."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.9 變異數分析與t檢定的關聯",
    "text": "13.9 變異數分析與t檢定的關聯\n在結束之前，我想指出的最後一點是，許多人對此感到驚訝，但了解它是很有價值的。具有兩個組別的ANOVA與學生t檢驗相同。不，真的。它們不僅相似，而且在每個有意義的方面實際上都是等效的。我不會試圖證明這總是成立，但我將給你展示一個具體的演示。假設，我們不對mood.gain ~ drug模型進行ANOVA，而是使用療法作為預測指標。如果我們運行此ANOVA，我們將得到一個F統計量 \\(F(1,16) = 1.71\\)，和一個 p值 = \\(0.21\\)。由於我們只有兩組，實際上我不需要求助於ANOVA，我可以選擇運行一個學生t檢驗。那麼，讓我們看看這樣做會發生什麼：我得到一個t統計量 \\(t(16) = -1.3068\\) 和一個 \\(p值 = 0.21\\)。好奇的是，p值是相同的。再一次，我們得到一個值 \\(p = .21\\)。但是，檢驗統計量呢？運行t檢驗而不是ANOVA，我們得到了一個略有不同的答案，即 \\(t(16) = -1.3068\\)。然而，這裡有一個相當直接的關係。如果將t統計量平方，我們就會得到之前的F統計量：\\(-1.3068^{2} = 1.7077\\)"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#summary",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#summary",
    "title": "13  Comparing several means (one-way ANOVA)",
    "section": "13.10 Summary",
    "text": "13.10 Summary\nThere’s a fair bit covered in this chapter, but there’s still a lot missing 18. Most obviously, I haven’t discussed how to run an ANOVA when you are interested in more than one grouping variable, but that will be discussed in a lot of detail in Chapter 14. In terms of what we have discussed, the key topics were:\n\nThe basic logic behind How ANOVA works and Running an ANOVA in jamovi\nHow to compute an Effect size for an ANOVA.\nMultiple comparisons and post hoc tests for multiple testing.\nThe assumptions of one-way ANOVA\nChecking the homogeneity of variance assumption and what to do if it is violated: Removing the homogeneity of variance assumption\nChecking the normality assumption and what to do if it is violated: Removing the normality assumption\nRepeated measures one-way ANOVA and the non-parametric equivalent, The Friedman non-parametric repeated measures ANOVA test\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific American, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et al (Ed.), Contributions to probability and statistics: Essays in honor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance: Fixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An alternative approach. Biometrika, 38, 330–336."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#相關",
    "href": "12-Correlation-and-linear-regression.html#相關",
    "title": "10  相闗與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\nIn this section we’ll talk about how to describe the relationships between variables in the data. To do that, we want to talk mostly about the correlation between variables. But first, we need some data (Table 10.1).\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  Data for correlation analysis - descriptive statistics for the parenthood data \n\nvariableminmaxmeanmedianstd. devIQR\n\nDani's grumpiness419163.716210.0514\n\nDani's hours slept4.849.006.977.031.021.45\n\nDani's son's hours slept3.2512.078.057.952.073.21\n\n\n\n\n\nLet’s turn to a topic close to every parent’s heart: sleep. The data set we’ll use is fictitious, but based on real events. Suppose I’m curious to find out how much my infant son’s sleeping habits affect my mood. Let’s say that I can rate my grumpiness very precisely, on a scale from 0 (not at all grumpy) to \\(100\\) (grumpy as a very, very grumpy old man or woman). And lets also assume that I’ve been measuring my grumpiness, my sleeping patterns and my son’s sleeping patterns for quite some time now. Let’s say, for \\(100\\) days. And, being a nerd, I’ve saved the data as a file called parenthood.csv. If we load the data we can see that the file contains four variables dani.sleep, baby.sleep, dani.grump and day. Note that when you first load this data set jamovi may not have guessed the data type for each variable correctly, in which case you should fix it: dani.sleep, baby.sleep, dani.grump and day can be specified as continuous variables, and ID is a nominal(integer) variable.1\nNext, I’ll take a look at some basic descriptive statistics and, to give a graphical depiction of what each of the three interesting variables looks like, Figure 10.1 plots histograms. One thing to note: just because jamovi can calculate dozens of different statistics doesn’t mean you should report all of them. If I were writing this up for a report, I’d probably pick out those statistics that are of most interest to me (and to my readership), and then put them into a nice, simple table like the one in Table 12.1.2 Notice that when I put it into a table, I gave everything “human readable” names. This is always good practice. Notice also that I’m not getting enough sleep. This isn’t good practice, but other parents tell me that it’s pretty standard.\n\n\n\n\n\nFigure 10.1: Histograms for the three interesting variables in the parenthood data set\n\n\n\n\n\n\n10.1.2 相關的強度與方向\nWe can draw scatterplots to give us a general sense of how closely related two variables are. Ideally though, we might want to say a bit more about it than that. For instance, let’s compare the relationship between dani.sleep and dani.grump (Figure 10.1), left) with that between baby.sleep and dani.grump (Figure 10.2), right). When looking at these two plots side by side, it’s clear that the relationship is qualitatively the same in both cases: more sleep equals less grump! However, it’s also pretty obvious that the relationship between dani.sleep and dani.grump is stronger than the relationship between baby.sleep and dani.grump. The plot on the left is “neater” than the one on the right. What it feels like is that if you want to predict what my mood is, it’d help you a little bit to know how many hours my son slept, but it’d be more helpful to know how many hours I slept.\n\n\n\n\n\nFigure 10.2: Scatterplots showing the relationship between dani.sleep and dani.grump (left) and the relationship between baby.sleep and dani.grump (right)\n\n\n\n\nIn contrast, let’s consider the two scatterplots shown in Figure 10.3. If we compare the scatterplot of “baby.sleep v dani.grump” (left) to the scatterplot of “’baby.sleep v dani.sleep” (right), the overall strength of the relationship is the same, but the direction is different. That is, if my son sleeps more, I get more sleep (positive relationship, right hand side), but if he sleeps more then I get less grumpy (negative relationship, left hand side).\n\n\n\n\n\nFigure 10.3: Scatterplots showing the relationship between baby.sleep and dani.grump (left), as compared to the relationship between baby.sleep and dani.sleep (right)\n\n\n\n\n\n\n10.1.3 相關係數\nWe can make these ideas a bit more explicit by introducing the idea of a 相關係數(correlation coefficient) (or, more specifically, Pearson’s correlation coefficient), which is traditionally denoted as r. The correlation coefficient between two variables \\(X\\) and \\(Y\\) (sometimes denoted \\(r_{XY}\\) ), which we’ll define more precisely in the next section, is a measure that varies from -1 to 1. When \\(r = -1\\) it means that we have a perfect negative relationship, and when \\(r = 1\\) it means we have a perfect positive relationship. When \\(r = 0\\), there’s no relationship at all. If you look at Figure 10.4, you can see several plots showing what different correlations look like.\n[Additional technical detail 3]\n\n\n\n\n\nFigure 10.4: Illustration of the effect of varying the strength and direction of a correlation. In the left hand column, the correlations are \\(0, .33, .66\\) and \\(1\\). In the right hand column, the correlations are \\(0, -.33, -.66\\) and \\(-1\\)\n\n\n\n\nBy standardising the covariance, not only do we keep all of the nice properties of the covariance discussed earlier, but the actual values of r are on a meaningful scale: r = 1 implies a perfect positive relationship and \\(r = -1\\) implies a perfect negative relationship. I’ll expand a little more on this point later, in the section on [Interpreting a correlation]. But before I do, let’s look at how to calculate correlations in jamovi.\n\n\n10.1.4 相關係數計算實務\nCalculating correlations in jamovi can be done by clicking on the ‘Regression’ - ‘Correlation Matrix’ button. Transfer all four continuous variables across into the box on the right to get the output in Figure 10.5.\n\n\n\n\n\nFigure 10.5: A jamovi screenshot showing correlations between variables in the parenthood.csv file\n\n\n\n\n\n\n10.1.5 解讀相關係數\nNaturally, in real life you don’t see many correlations of \\(1\\). So how should you interpret a correlation of, say, r = \\(.4\\)? The honest answer is that it really depends on what you want to use the data for, and on how strong the correlations in your field tend to be. A friend of mine in engineering once argued that any correlation less than \\(.95\\) is completely useless (I think he was exaggerating, even for engineering). On the other hand, there are real cases, even in psychology, where you should really expect correlations that strong. For instance, one of the benchmark data sets used to test theories of how people judge similarities is so clean that any theory that can’t achieve a correlation of at least \\(.9\\) really isn’t deemed to be successful. However, when looking for (say) elementary correlates of intelligence (e.g., inspection time, response time), if you get a correlation above \\(.3\\) you’re doing very very well. In short, the interpretation of a correlation depends a lot on the context. That said, the rough guide in Table 10.2 is pretty typical.\n\n\n\n\nTable 10.2:  A rough guide to interpreting correlations. Note that I say a rough guide. There aren’t hard and fast rules for what counts as strong or weak relationships. It depends on the context. \n\nCorrelationStrengthDirection\n\n-1.0 to -0.9Very strongNegative\n\n-0.9 to -0.7StrongNegative\n\n-0.7 to -0.4ModerateNegative\n\n-0.4 to -0.2WeakNegative\n\n-0.2 to 0NegligibleNegative\n\n0 to 0.2NegligiblePositive\n\n0.2 to 0.4WeakPositive\n\n0.4 to 0.7ModeratePositive\n\n0.7 to 0.9StrongPositive\n\n0.9 to 1.0Very strongPositive\n\n\n\n\n\nHowever, something that can never be stressed enough is that you should always look at the scatterplot before attaching any interpretation to the data. A correlation might not mean what you think it means. The classic illustration of this is “Anscombe’s Quartet” (Anscombe, 1973), a collection of four data sets. Each data set has two variables, an \\(X\\) and a \\(Y\\). For all four data sets the mean value for \\(X\\) is \\(9\\) and the mean for \\(Y\\) is \\(7.5\\). The standard deviations for all \\(X\\) variables are almost identical, as are those for the Y variables. And in each case the correlation between \\(X\\) and \\(Y\\) is \\(r = 0.816\\). You can verify this yourself, since I happen to have saved it in a file called anscombe.csv.\nYou’d think that these four data sets would look pretty similar to one another. They do not. If we draw scatterplots of \\(X\\) against \\(Y\\) for all four variables, as shown in Figure 10.6, we see that all four of these are spectacularly different to each other. The lesson here, which so very many people seem to forget in real life, is “always graph your raw data” (see Chapter 5).\n\n\n\n\n\nFigure 10.6: Anscombe’s quartet. All four of these data sets have a Pearson correlation of r = .816, but they are qualitatively different from one another\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\nThe Pearson correlation coefficient is useful for a lot of things, but it does have shortcomings. One issue in particular stands out: what it actually measures is the strength of the linear relationship between two variables. In other words, what it gives you is a measure of the extent to which the data all tend to fall on a single, perfectly straight line. Often, this is a pretty good approximation to what we mean when we say “relationship”, and so the Pearson correlation is a good thing to calculate. Sometimes though, it isn’t.\nOne very common situation where the Pearson correlation isn’t quite the right thing to use arises when an increase in one variable \\(X\\) really is reflected in an increase in another variable Y , but the nature of the relationship isn’t necessarily linear. An example of this might be the relationship between effort and reward when studying for an exam. If you put zero effort (\\(X\\)) into learning a subject then you should expect a grade of \\(0\\%\\) (\\(Y\\)). However, a little bit of effort will cause a massive improvement. Just turning up to lectures means that you learn a fair bit, and if you just turn up to classes and scribble a few things down your grade might rise to 35%, all without a lot of effort. However, you just don’t get the same effect at the other end of the scale. As everyone knows, it takes a lot more effort to get a grade of \\(90\\%\\) than it takes to get a grade of \\(55\\%\\). What this means is that, if I’ve got data looking at study effort and grades, there’s a pretty good chance that Pearson correlations will be misleading.\nTo illustrate, consider the data plotted in Figure 10.7, showing the relationship between hours worked and grade received for 10 students taking some class. The curious thing about this (highly fictitious) data set is that increasing your effort always increases your grade. It might be by a lot or it might be by a little, but increasing effort will never decrease your grade. If we run a standard Pearson correlation, it shows a strong relationship between hours worked and grade received, with a correlation coefficient of \\(0.91\\). However, this doesn’t actually capture the observation that increasing hours worked always increases the grade. There’s a sense here in which we want to be able to say that the correlation is perfect but for a somewhat different notion of what a “relationship” is. What we’re looking for is something that captures the fact that there is a perfect ordinal relationship here. That is, if student 1 works more hours than student 2, then we can guarantee that student 1 will get the better grade. That’s not what a correlation of \\(r = .91\\) says at all.\n\n\n\n\n\nFigure 10.7: The relationship between hours worked and grade received for a toy data set consisting of only 10 students (each circle corresponds to one student). The dashed line through the middle shows the linear relationship between the two variables. This produces a strong Pearson correlation of \\(r = .91\\). However, the interesting thing to note here is that there’s actually a perfect monotonic relationship between the two variables. In this toy example, increasing the hours worked always increases the grade received, as illustrated by the solid line. This is reflected in a Spearman correlation of \\(\\rho = 1\\). With such a small data set, however, it’s an open question as to which version better describes the actual relationship involved\n\n\n\n\nHow should we address this? Actually, it’s really easy. If we’re looking for ordinal relationships all we have to do is treat the data as if it were ordinal scale! So, instead of measuring effort in terms of “hours worked”, lets rank all \\(10\\) of our students in order of hours worked. That is, student \\(1\\) did the least work out of anyone (\\(2\\) hours) so they get the lowest rank (rank = \\(1\\)). Student \\(4\\) was the next laziest, putting in only \\(6\\) hours of work over the whole semester, so they get the next lowest rank (rank = \\(2\\)). Notice that I’m using “rank =1” to mean “low rank”. Sometimes in everyday language we talk about “rank = \\(1\\)” to mean “top rank” rather than “bottom rank”. So be careful, you can rank “from smallest value to largest value” (i.e., small equals rank \\(1\\)) or you can rank “from largest value to smallest value” (i.e., large equals rank 1). In this case, I’m ranking from smallest to largest, but as it’s really easy to forget which way you set things up you have to put a bit of effort into remembering!\nOkay, so let’s have a look at our students when we rank them from worst to best in terms of effort and reward Table 10.3.\n\n\n\n\nTable 10.3:  Students ranked in terms of effort and reward \n\nrank (hours worked)rank (grade received)\n\nstudent 111\n\nstudent 21010\n\nstudent 366\n\nstudent 422\n\nstudent 533\n\nstudent 655\n\nstudent 744\n\nstudent 888\n\nstudent 977\n\nstudent 1099\n\n\n\n\n\nHmm. These are identical. The student who put in the most effort got the best grade, the student with the least effort got the worst grade, etc. As the table above shows, these two rankings are identical, so if we now correlate them we get a perfect relationship, with a correlation of 1.0.\nWhat we’ve just re-invented is 斯皮爾曼等級相關(Spearman’s rank order correlation), usually denoted \\(\\rho\\) to distinguish it from the Pearson correlation r. We can calculate Spearman’s \\(\\rho\\) using jamovi simply by clicking the ‘Spearman’ check box in the ‘Correlation Matrix’ screen."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#散佈圖",
    "href": "12-Correlation-and-linear-regression.html#散佈圖",
    "title": "10  相闗與線性迴歸",
    "section": "10.2 散佈圖",
    "text": "10.2 散佈圖\nScatterplots are a simple but effective tool for visualising the relationship between two variables, like we saw with the figures in the section on [Correlations]. It’s this latter application that we usually have in mind when we use the term “scatterplot”. In this kind of plot each observation corresponds to one dot. The horizontal location of the dot plots the value of the observation on one variable, and the vertical location displays its value on the other variable. In many situations you don’t really have a clear opinions about what the causal relationship is (e.g., does A cause B, or does B cause A, or does some other variable C control both A and B). If that’s the case, it doesn’t really matter which variable you plot on the x-axis and which one you plot on the y-axis. However, in many situations you do have a pretty strong idea which variable you think is most likely to be causal, or at least you have some suspicions in that direction. If so, then it’s conventional to plot the cause variable on the x-axis, and the effect variable on the y-axis. With that in mind, let’s look at how to draw scatterplots in jamovi, using the same parenthood data set (i.e. parenthood.csv) that I used when introducing correlations.\nSuppose my goal is to draw a scatterplot displaying the relationship between the amount of sleep that I get (dani.sleep) and how grumpy I am the next day (dani.grump). There are two different ways in which we can use jamovi to get the plot that we’re after. The first way is to use the ‘Plot’ option under the ‘Regression’ - ‘Correlation Matrix’ button, giving us the output shown in Figure 10.8. Note that jamovi draws a line through the points, we’ll come onto this a bit later in the section on [What is a linear regression model?]. Plotting a scatterplot in this way also allow you to specify ‘Densities for variables’ and this option adds a density curve showing how the data in each variable is distributed.\n\n\n\n\n\nFigure 10.8: Scatterplot via the ‘Correlation Matrix’ command in jamovi\n\n\n\n\nThe second way do to it is to use one of the jamovi add-on modules. This module is called ‘scatr’ and you can install it by clicking on the large ‘\\(+\\)’ icon in the top right of the jamovi screen, opening the jamovi library, scrolling down until you find ‘scatr’ and clicking ‘install’. When you have done this, you will find a new ‘Scatterplot’ command available under the ‘Exploration’ button. This plot is a bit different than the first way, see Figure 10.9, but the important information is the same.\n\n\n\n\n\nFigure 10.9: Scatterplot via the ‘scatr’ add-on module in - jamovi\n\n\n\n\n\n10.2.1 更多探討散佈圖的方案\nOften you will want to look at the relationships between several variables at once, using a scatterplot matrix (in jamovi via the ‘Correlation Matrix’ - ‘Plot’ command). Just add another variable, for example baby.sleep to the list of variables to be correlated, and jamovi will create a scatterplot matrix for you, just like the one in Figure 10.10.\n\n\n\n\n\nFigure 10.10: A matrix of scatterplots produced using jamovi"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "href": "12-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "title": "10  相闗與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see [Correlations]), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 10.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 10.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 10.11 (b).\n\n\n\n\n\nFigure 10.11: Panel (a) shows the sleep-grumpiness scatterplot from Figure 10.9 with the best fitting regression line drawn over the top. Not surprisingly, the line goes through the middle of the data. In contrast, panel (b) shows the same data, but with a very poor choice of regression line drawn over the top\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 10.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).4 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).5 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#線性迴歸模型參數估計",
    "href": "12-Correlation-and-linear-regression.html#線性迴歸模型參數估計",
    "title": "12  相闗與線性迴歸",
    "section": "12.4 線性迴歸模型參數估計",
    "text": "12.4 線性迴歸模型參數估計\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 12.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 12.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 12.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n12.4.1 實作線性迴歸模型\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 12.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 12.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 12.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n12.4.2 解讀線性迴歸模型參數估計\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#多元線性迴歸",
    "href": "12-Correlation-and-linear-regression.html#多元線性迴歸",
    "title": "10  相闗與線性迴歸",
    "section": "10.5 多元線性迴歸",
    "text": "10.5 多元線性迴歸\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n10.5.1 Doing it in jamovi\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 10.4.\n\n\n\n\nTable 10.4:  Adding multiple variables as predictors in a regression \n\n(Intercept)dani.sleepbaby.sleep\n\n125.97-8.950.01\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 10.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 10.14: A 3D visualisation of a multiple regression model. There are two predictors in the model, dani.sleep and baby.sleep and the outcome variable is dani.grump. Together, these three variables form a 3D space. Each observation (dot) is a point in this space. In much the same way that a simple linear regression model forms a line in 2D space, this multiple regression model forms a plane in 3D space. When we estimate the regression coefficients what we’re trying to do is find a plane that is as close to all the blue dots as possible\n\n\n\n\n[Additional technical detail7]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "href": "12-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "title": "10  相闗與線性迴歸",
    "section": "10.6 量化迴歸模型的適配性",
    "text": "10.6 量化迴歸模型的適配性\nSo we now know how to estimate the coefficients of a linear regression model. The problem is, we don’t yet know if this regression model is any good. For example, the regression.1 model claims that every hour of sleep will improve my mood by quite a lot, but it might just be rubbish. Remember, the regression model only produces a prediction \\(\\hat{Y}_i\\) about what my mood is like, but my actual mood is \\(Y_i\\) . If these two are very close, then the regression model has done a good job. If they are very different, then it has done a bad job.\n\n10.6.1 The \\(R^2\\) value\nOnce again, let’s wrap a little bit of mathematics around this. Firstly, we’ve got the sum of the squared residuals\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\nwhich we would hope to be pretty small. Specifically, what we’d like is for it to be very small in comparison to the total variability in the outcome variable\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\nWhile we’re here, let’s calculate these values ourselves, not by hand though. Let’s use something like Excel or another standard spreadsheet programme. I have done this by opening up the parenthood.csv file in Excel and saving it as parenthood rsquared.xls so that I can work on it. The first thing to do is calculate the \\(\\hat{Y}\\) values, and for the simple model that uses only a single predictor we would do the following:\n\ncreate a new column called ‘Y.pred’ using the formula ‘= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’\ncalculate the SS(resid) by creating a new column called ‘(Y-Y.pred)^2’ using the formula ’ = (dani.grump - Y.pred)^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ’ sum( ( Y-Y.pred)^2 ) .\nAt the bottom of the dani.grump column, calculate the mean value for dani.grump (NB Excel uses the word ’ AVERAGE ’ rather than ‘mean’ in its function).\nThen create a new column, called ’ (Y - mean(Y))^2 )’ using the formula ’ = (dani.grump - AVERAGE(dani.grump))^2 ’.\nThen, at the bottom of this column calculate the sum of these values, i.e. ‘sum( (Y - mean(Y))^2 )’.\nCalculate R.squared by typing into a blank cell the following: ‘= 1 - (SS(resid) / SS(tot) )’.\n\nThis gives a value for \\(R^2\\) of ‘0.8161018’. The \\(R^2\\) value, sometimes called the coefficient of determination8 has a simple interpretation: it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. So, in this case the fact that we have obtained \\(R^2 = .816\\) means that the predictor (my.sleep) explains \\(81.6\\%\\) of the variance in the outcome (my.grump).\nNaturally, you don’t actually need to type all these commands into Excel yourself if you want to obtain the \\(R^2\\) value for your regression model. As we’ll see later on in the section on Running the hypothesis tests in jamovi, all you need to do is specify this as an option in jamovi. However, let’s put that to one side for the moment. There’s another property of \\(R^2\\) that I want to point out.\n\n\n10.6.2 The relationship between regression and correlation\nAt this point we can revisit my earlier claim that regression, in this very simple form that I’ve discussed so far, is basically the same thing as a correlation. Previously, we used the symbol \\(r\\) to denote a Pearson correlation. Might there be some relationship between the value of the correlation coefficient \\(r\\) and the \\(R^2\\) value from linear regression? Of course there is: the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. In other words, running a Pearson correlation is more or less equivalent to running a linear regression model that uses only one predictor variable.\n\n\n10.6.3 The adjusted \\(R^2\\) value\nOne final thing to point out before moving on. It’s quite common for people to report a slightly different measure of model performance, known as “adjusted \\(R^2\\)”. The motivation behind calculating the adjusted \\(R^2\\) value is the observation that adding more predictors into the model will always cause the \\(R^2\\) value to increase (or at least not decrease).\n[Additional technical detail9]\nThis adjustment is an attempt to take the degrees of freedom into account. The big advantage of the adjusted \\(R^2\\) value is that when you add more predictors to the model, the adjusted \\(R^2\\) value will only increase if the new variables improve the model performance more than you’d expect by chance. The big disadvantage is that the adjusted \\(R^2\\) value can’t be interpreted in the elegant way that \\(R^2\\) can. \\(R^2\\) has a simple interpretation as the proportion of variance in the outcome variable that is explained by the regression model. To my knowledge, no equivalent interpretation exists for adjusted \\(R^2\\).\nAn obvious question then is whether you should report \\(R^2\\) or adjusted \\(R^2\\) . This is probably a matter of personal preference. If you care more about interpretability, then \\(R^2\\) is better. If you care more about correcting for bias, then adjusted \\(R^2\\) is probably better. Speaking just for myself, I prefer \\(R^2\\). My feeling is that it’s more important to be able to interpret your measure of model performance. Besides, as we’ll see in [Hypothesis tests for regression models], if you’re worried that the improvement in \\(R^2\\) that you get by adding a predictor is just due to chance and not because it’s a better model, well we’ve got hypothesis tests for that."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "href": "12-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "title": "10  相闗與線性迴歸",
    "section": "10.7 迴歸模型的假設檢定",
    "text": "10.7 迴歸模型的假設檢定\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n10.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail10]\nWe’ll see much more of the F statistic in Chapter 13, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n10.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the [Multiple linear regression] model we have already looked at (Table 10.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.11 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n10.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 10.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 10.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 10.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.12\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#更多迴歸模型透露的資訊",
    "href": "12-Correlation-and-linear-regression.html#更多迴歸模型透露的資訊",
    "title": "12  相闗與線性迴歸",
    "section": "12.8 更多迴歸模型透露的資訊",
    "text": "12.8 更多迴歸模型透露的資訊\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n12.8.1 迴歸係數的信賴區間\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 12.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n12.8.2 標準化迴歸係數的計算方法\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 12.16.\n\n\n\n\n\nFigure 12.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "href": "12-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "title": "10  相闗與線性迴歸",
    "section": "10.9 迴歸模型的適用條件",
    "text": "10.9 迴歸模型的適用條件\nThe linear regression model that I’ve been discussing relies on several assumptions. In [Model checking] we’ll talk a lot more about how to check that these assumptions are being met, but first let’s have a look at each of them.\n\nLinearity. A pretty fundamental assumption of the linear regression model is that the relationship between \\(X\\) and \\(Y\\) actually is linear! Regardless of whether it’s a simple regression or a multiple regression, we assume that the relationships involved are linear.\nIndependence: residuals are independent of each other. This is really just a “catch all” assumption, to the effect that “there’s nothing else funny going on in the residuals”. If there is something weird (e.g., the residuals all depend heavily on some other unmeasured variable) going on, it might screw things up.\nNormality. Like many of the models in statistics, basic simple or multiple linear regression relies on an assumption of normality. Specifically, it assumes that the residuals are normally distributed. It’s actually okay if the predictors \\(X\\) and the outcome \\(Y\\) are non-normal, so long as the residuals \\(\\epsilon\\) are normal. See the [Checking the normality of the residuals] section.\nEquality (or ‘homogeneity’) of variance. Strictly speaking, the regression model assumes that each residual \\(\\epsilon_i\\) is generated from a normal distribution with mean 0, and (more importantly for the current purposes) with a standard deviation \\(\\sigma\\) that is the same for every single residual. In practice, it’s impossible to test the assumption that every residual is identically distributed. Instead, what we care about is that the standard deviation of the residual is the same for all values of \\(\\hat{Y}\\) , and (if we’re being especially paranoid) all values of every predictor \\(X\\) in the model.\n\nSo, we have four main assumptions for linear regression (that neatly form the acronym ‘LINE’). And there are also a couple of other things we should also check for:\n\nUncorrelated predictors. The idea here is that, in a multiple regression model, you don’t want your predictors to be too strongly correlated with each other. This isn’t “technically” an assumption of the regression model, but in practice it’s required. Predictors that are too strongly correlated with each other (referred to as “collinearity”) can cause problems when evaluating the model. See the [Checking for collinearity] section.\nNo “bad” outliers. Again, not actually a technical assumption of the model (or rather, it’s sort of implied by all the others), but there is an implicit assumption that your regression model isn’t being too strongly influenced by one or two anomalous data points because this raises questions about the adequacy of the model and the trustworthiness of the data in some cases. See the section on [Three kinds of anomalous data]."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "href": "12-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "title": "10  相闗與線性迴歸",
    "section": "10.11 決定線性模型的變項組合",
    "text": "10.11 決定線性模型的變項組合\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 23\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n10.11.1 逐步排除法\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n10.11.2 逐步納入法\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n10.11.3 使用警告\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n10.11.4 比較迴歸模型\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.24\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 25\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 10.24. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 10.24.\n\n\n\n\n\nFigure 10.24: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#本章小結",
    "href": "12-Correlation-and-linear-regression.html#本章小結",
    "title": "12  相關與線性迴歸",
    "section": "12.12 本章小結",
    "text": "12.12 本章小結\n\n想了解兩個變項之間的關聯性有多強？就計算相關係數\n散佈圖繪製方法\n前進下一章前必學的課題：什麼是線性迴歸模型 以及使用線性迴歸模型估計參數\n多元線性迴歸\n量化迴歸模型的適配性 要了解 \\(R^2\\) 。\n迴歸模型的假設檢定\n在迴歸係數的更多資訊 這一節，我們學習如何計算迴歸係數的信賴區間以及標準化迴歸係數的計算方法\n迴歸模型的適用條件 以及診斷適用條件\n決定線性模型的變項組合\n\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.1 本章示範資料",
    "text": "13.1 本章示範資料\nSuppose you’ve become involved in a clinical trial in which you are testing a new antidepressant drug called Joyzepam. In order to construct a fair test of the drug’s effectiveness, the study involves three separate drugs to be administered. One is a placebo, and the other is an existing antidepressant / anti-anxiety drug called Anxifree. A collection of 18 participants with moderate to severe depression are recruited for your initial testing. Because the drugs are sometimes administered in conjunction with psychological therapy, your study includes 9 people undergoing cognitive behavioural therapy (CBT) and 9 who are not. Participants are randomly assigned (doubly blinded, of course) a treatment, such that there are 3 CBT people and 3 no-therapy people assigned to each of the 3 drugs. A psychologist assesses the mood of each person after a 3 month run with each drug, and the overall improvement in each person’s mood is assessed on a scale ranging from \\(-5\\) to \\(+5\\). With that as the study design, let’s now load up the data file in clinicaltrial.csv. We can see that this data set contains the three variables drug, therapy and mood.gain.\nFor the purposes of this chapter, what we’re really interested in is the effect of drug on mood.gain. The first thing to do is calculate some descriptive statistics and draw some graphs. In the Chapter 4 chapter we showed you how to do this, and some of the descriptive statistics we can calculate in jamovi are shown in Figure 13.1\n\n\n\n\n\nFigure 13.1: Descriptives for mood gain, and box plots by drug administered\n\n\n\n\nAs the plot makes clear, there is a larger improvement in mood for participants in the Joyzepam group than for either the Anxifree group or the placebo group. The Anxifree group shows a larger mood gain than the control group, but the difference isn’t as large. The question that we want to answer is are these difference “real”, or are they just due to chance?"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.3 jamovi的變異數分析模組",
    "text": "13.3 jamovi的變異數分析模組\nI’m pretty sure I know what you’re thinking after reading the last section, especially if you followed my advice and did all of that by pencil and paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations yourself sucks. There’s quite a lot of calculations that we needed to do along the way, and it would be tedious to have to do this over and over again every time you wanted to do an ANOVA.\n\n13.3.1 使用jamovi完成變異數分析\nTo make life easier for you, jamovi can do ANOVA…hurrah! Go to the ‘ANOVA’ - ‘ANOVA’ analysis, and move the mood.gain variable across so it is in the ‘Dependent Variable’ box, and then move the drug variable across so it is in the ‘Fixed Factors’ box. This should give the results as shown in Figure 13.3. 9 Note I have also checked the \\(\\eta^2\\) checkbox, pronounced “eta” squared, under the ‘Effect Size’ option and this is also shown on the results table. We will come back to effect sizes a bit later.\n\n\n\n\n\nFigure 13.3: jamovi results table for ANOVA of mood gain by drug administered\n\n\n\n\nThe jamovi results table shows you the sums of squares values, the degrees of freedom, and a couple of other quantities that we’re not really interested in right now. Notice, however, that jamovi doesn’t use the names “between-group” and “within-group”. Instead, it tries to assign more meaningful names. In our particular example, the between groups variance corresponds to the effect that the drug has on the outcome variable, and the within groups variance corresponds to the “leftover” variability so it calls that the residuals. If we compare these numbers to the numbers that I calculated by hand in [A worked example], you can see that they’re more or less the same, apart from rounding errors. The between groups sums of squares is \\(SS_b = 3.45\\), the within groups sums of squares is \\(SS_w = 1.39\\), and the degrees of freedom are \\(2\\) and \\(15\\) respectively. We also get the F-value and the p-value and, again, these are more or less the same, give or take rounding errors, to the numbers that we calculated ourselves when doing it the long and tedious way."
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#效果量",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#效果量",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.4 效果量",
    "text": "13.4 效果量\n有幾種不同的方法可以衡量 ANOVA 中的效應大小，但最常用的衡量指標是 \\(\\eta^2\\)（eta 平方）和偏 \\(\\eta^2\\)。對於單因素變異數分析，它們彼此相同，所以目前我只解釋 \\(\\eta^2\\)。\\(\\eta^2\\) 的定義實際上非常簡單：\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\n就是這樣。所以當我查看 图 13.3 中的 ANOVA 表時，我看到 \\(SS_b = 3.45\\) 和 \\(SS_tot = 3.45 + 1.39 = 4.84\\)。因此，我們得到一個 \\(\\eta^2\\) 值：\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\n\\(\\eta^2\\) 的解釋同樣直接。它表示可以根據預測變項（藥物）解釋的結果變項（mood.gain）可變性的比例。\\(\\eta^2=0\\) 表示兩者之間完全沒有關係，而 \\(\\eta^2=1\\) 表示關係是完美的。更好的是，\\(\\eta^2\\) 值與 章节 12.6.1 中討論的 \\(R^2\\) 關係非常密切，並具有等效的解釋。儘管許多統計教科書建議在 ANOVA 中使用 \\(\\eta^2\\) 作為默認的效應大小衡量指標，但 Daniel Lakens 的一篇有趣的博客文章表明，eta 平方在實際數據分析中可能不是最好的效應大小衡量指標，因為它可能是一個有偏估計量。有用的是，jamovi 中還有一個選項可以指定 ω 平方（\\(\\omega^2\\)），它與 η 平方相比偏差較小。"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.5 多重比較與事後檢定",
    "text": "13.5 多重比較與事後檢定\n每當您對多於兩個組進行 ANOVA，並得到顯著效應時，您可能首先想問的是哪些組之間實際上存在差異。在我們的藥物示例中，我們的零假設是所有三種藥物（安慰劑、Anxifree 和 Joyzepam）對情緒的影響完全相同。但是如果你仔細想一想，實際上零假設一次聲稱了三個不同的事情。具體來說，它聲稱：\n\n您的競爭對手的藥物（Anxifree）並不比安慰劑更好（即，\\(\\mu_A = \\mu_P\\) ）\n您的藥物（Joyzepam）並不比安慰劑更好（即，\\(\\mu_J = \\mu_P\\) ）\nAnxifree 和 Joyzepam 同樣有效（即，\\(\\mu_J = \\mu_A\\)）\n\n如果上述三個聲稱中的任何一個是偽的，那麼零假設也是偽的。因此，現在我們已經拒絕了我們的零假設，我們認為至少有一件事是不正確的。但哪些呢？所有三個命題都很有趣。既然您肯定想知道您的新藥 Joyzepam 是否比安慰劑更好，那麼了解它與現有商業替代品（即 Anxifree）的比較如何就變得很重要了。甚至有用的是檢查 Anxifree 與安慰劑的表現。即使 Anxifree 已經被其他研究人員廣泛地與安慰劑進行了對照測試，但檢查您的研究是否產生了與早期工作相似的結果仍然非常有用。\n當我們根據這三個不同的命題來描述零假設時，我們需要區分的八種可能的“世界狀態”變得清晰了（ 表格 13.9 ）。\n\n\n\n\n\n表格 13.9: 虛無假設與八種可能的”現實世界”\n\n\npossibility:\nis \\( \\mu_P = \\mu_A \\)?\nis \\( \\mu_P = \\mu_J \\)?\nis \\( \\mu_A = \\mu_J \\)?\nwhich hypothesis?\n\n\n1\n\\( \\checkmark \\)\n\\( \\checkmark \\)\n\\( \\checkmark \\)\nnull\n\n\n2\n\\( \\checkmark \\)\n\\( \\checkmark \\)\n\nalternative\n\n\n3\n\\( \\checkmark \\)\n\n\\( \\checkmark \\)\nalternative\n\n\n4\n\\( \\checkmark \\)\n\n\nalternative\n\n\n5\n\\( \\checkmark \\)\n\\( \\checkmark \\)\n\\( \\checkmark \\)\nalternative\n\n\n6\n\n\\( \\checkmark \\)\n\nalternative\n\n\n7\n\n\n\\( \\checkmark \\)\nalternative\n\n\n8\n\n\n\nalternative\n\n\n\n\n\n\n\n\n通過拒絕零假設，我們已經決定我們不相信 #1 是真實的世界狀態。下一個問題是，我們認為其他七個可能性中的哪一個*是對的？面對這種情況，通常最好先看看數據。例如，如果我們查看 图 13.1 中的繪圖，我們很容易得出 Joyzepam 優於安慰劑和 Anxifree，但 Anxifree 和安慰劑之間沒有實際差別的結論。然而，如果我們想對此得到更清晰的答案，則可能需要進行一些測試。\n\n\n13.5.1 成對t檢定\n我們如何解決問題？考慮到我們需要比較三對不同的平均值（安慰劑對 Anxifree，安慰劑對 Joyzepam，和 Anxifree 對 Joyzepam），我們可以執行三個單獨的 t 檢驗，看看會發生什麼。在 jamovi 中這很容易做到。轉到 ANOVA 的 ‘Post Hoc Tests’（事後檢驗）選項，將 ‘drug’（藥物）變項移到右側的活動框中，然後單擊 ‘No correction’（無校正）複選框。這將產生一個整齊的表格，顯示藥物變項的三個水平之間的所有成對 t 檢驗比較，如 图 13.4 中所示。\n\n\n\n\n\n\n\n图 13.4: 未經校正的成對 t 檢驗作為 jamovi 中的事後比較。\n\n\n\n\n\n\n13.5.2 多重檢定的校正\n在上一節中，我暗示了執行大量 t 檢驗存在問題。我們擔心的是，在執行這些分析時，我們正在進行一個「捕魚之旅」。我們在沒有太多理論指導的情況下執行了大量測試，希望其中一些測試顯示出顯著性。這種對團體差異的無理論基礎的搜索被稱為事後分析（“post hoc” 是拉丁語，意為 “after this”）。[^13-comparing-several-means-one-way-anova-10]\n[^13-comparing-several-means-one-way-anova-10]：如果您確實有一些理論基礎，希望研究某些比較而不是其他比較，那就是另一回事了。在這種情況下，您實際上並不是在執行「事後分析」，而是在進行「預先計劃的比較」。我確實在本書後面談到了這種情況- 章节 14.9 ，但現在我想保持簡單。\n進行事後分析是可以的，但需要非常小心。例如，在上一節中進行的分析應該避免，因為每個單獨的 t 檢驗都設計為 5% 的第一型錯誤率（即 \\(\\alpha = .05\\)），而我執行了其中的三個檢驗。想象一下，如果我的 ANOVA 涉及 10 個不同的組，我決定執行 45 個「事後」t 檢驗，試圖找出哪些組之間存在顯著差異，那麼僅憑機會就會出現 2 到 3 個顯著結果。正如我們在 章节 9 中看到的那樣，虛無假設檢驗背後的核心組織原則是控制我們的第一型錯誤率，但是現在，由於我同時執行了大量 t 檢驗以確定 ANOVA 結果的來源，整個試驗家族的實際第一型錯誤率已經完全失控。\n解決這個問題的常用方法是對 p 值進行調整，目的是控制整個試驗家族的總誤差率（參見 Shaffer (1995)）。這種調整通常（但不總是）應用於事後分析，通常被稱為多重比較校正，儘管有時也被稱為「同時推斷」。無論如何，進行這種調整的方法有很多。我將在本節和下一章節 章节 14.8 中討論其中的一些方法，但您應該意識到還有很多其他方法（例如，參見 Hsu (1996) ）。\n\n\n\n13.5.3 Bonferroni校正\n這些調整中最簡單的一種被稱為邦弗隆尼校正(Dunn, 1961)，它確實非常簡單。假設我的事後分析包括 m 個單獨的檢驗，我希望確保出現任何第一型錯誤的總概率最多為 \\(\\alpha\\)。[^13-comparing-several-means-one-way-anova-11] 如果是這樣，那麼邦弗隆尼校正只是說「將所有原始 p 值乘以 m」。如果讓 \\(p\\) 表示原始 p 值，讓 \\(p_j^{'}\\) 表示經過校正的值，那麼邦弗隆尼校正告訴我們：\n[^13-comparing-several-means-one-way-anova-11]：順便值得一提的是，並非所有調整方法都試圖這樣做。我在這裡描述的是一種用於控制「家族式第一型錯誤率」的方法。然而，還有其他事後檢驗試圖控制「偽發現率」，這是一個有點不同的概念。\n\\[p_j^{'}=m \\times p\\]\n因此，如果您使用邦弗隆尼校正，則在 \\(p_j^{'} &lt; \\alpha\\) 的情況下拒絕零假設。這種校正背後的邏輯非常簡單。我們正在進行 m 個不同的檢驗，因此，如果我們安排使每個檢驗的第一型錯誤率至多為 \\(\\frac{\\alpha}{m}\\)，那麼這些檢驗的總第一型錯誤率不能大於 \\(\\alpha\\)。這很簡單，簡單到在原始論文中，作者寫道：\n\n在這裡給出的方法如此簡單，而且如此通用，我確信它肯定已經被使用過了。然而，我沒有找到它，所以只能得出一個結論：也許正是它的極簡單讓統計學家意識不到它在某些情況下是一個非常好的方法（Dunn (1961)，第52-53頁）。\n\n要在 jamovi 中使用邦弗隆尼校正，只需單擊「校正」選項中的「邦弗隆尼」復選框，您將在 ANOVA 結果表中看到另一列，顯示邦弗隆尼校正的調整後 p 值（ 表格 13.8 ）。如果我們將這三個 p 值與未校正的成對 t 檢驗的 p 值進行比較，很明顯 jamovi 所做的唯一事情就是將它們乘以 \\(3\\)。\n\n\n\n\n13.5.4 Holm校正\n雖然邦弗隆尼校正是最簡單的調整方法，但它通常不是最好的選擇。經常使用的另一種方法是霍爾姆校正（Holm correction）(Holm, 1979)。霍爾姆校正背後的思路是假設您正在按順序進行測試，從最小（原始）的 p 值開始，然後移動到最大的 p 值。對於第 j 大的 p 值，調整是以下兩者之一\n\\[p_j^{'}=j \\times p_j\\]\n（即最大的 p 值保持不變，第二大的 p 值翻倍，第三大的 p 值翻三倍，依此類推），或者\n\\[p_j^{'}=p_{j+1}^{'}\\]\n其中較大者。這可能聽起來有點困惑，所以讓我們慢慢解釋。霍爾姆校正的工作原理如下。首先，您按順序對所有 p 值進行排序，從最小到最大。對於最小的 p 值，您只需將其乘以 \\(m\\)，然後就完成了。然而，對於其他所有的 p 值，這是一個兩階段的過程。例如，當您移動到第二小的 p 值時，首先將其乘以 \\(m - 1\\)。如果這產生的數字大於您上次得到的調整後的 p 值，那麼保留它。但如果它比上一個小，那麼您將複製上一個 p 值。為了說明這是如何工作的，請考慮 表格 13.10 ，該表顯示了五個 p 值的霍爾姆校正計算。\n\n\n\n\n\n表格 13.10: 經過霍爾姆校正計算的p值\n\n\nraw p\nrank j\np \\( \\times \\) j\nHolm p\n\n\n.001\n5\n.005\n.005\n\n\n.005\n4\n.020\n.020\n\n\n.019\n3\n.057\n.057\n\n\n.022\n2\n.044\n.057\n\n\n.103\n1\n.103\n.103\n\n\n\n\n\n\n\n\n希望這能讓事情變得清晰。\n雖然計算起來稍微困難一些，但霍爾姆校正具有一些非常好的特性。它比邦弗隆尼更具威力（即具有更低的 Type II 錯誤率），但是，儘管可能令人反直覺，它具有相同的 Type I 錯誤率。因此，在實踐中，沒有理由使用更簡單的邦弗隆尼校正，因為它總是被稍微複雜一點的霍爾姆校正所超越。正因為如此，霍爾姆校正應該是您的首選多重比較校正。 图 13.4 還顯示了霍爾姆校正後的 p 值，如您所見，最大的 p 值（對應於 Anxifree 和安慰劑之間的比較）沒有改變。它的值為 .15，與我們最初在完全不做校正時得到的值完全相同。相比之下，最小的 p 值（Joyzepam 與安慰劑）已乘以三。\n\n\n\n13.5.5 事後檢定的報告格式\n最後，在執行事後分析以確定哪些組別之間的差異顯著之後，您可以這樣寫出結果：\n\n事後檢驗（使用霍爾姆校正來調整 p 值）表明，與 Anxifree（p = .001）和安慰劑（\\(（p = 9.0 \\times{10^{-5}}\\)）相比，Joyzepam 產生了顯著更大的心情變化。我們沒有發現 Anxifree 表現優於安慰劑的證據（\\(p = .15\\)）。\n\n或者，如果您不喜歡報告精確的 p 值，那麼分別將這些數字更改為 \\(p &lt; .01\\)、\\(p &lt; .001\\) 和 \\(p &gt; .05\\)。無論哪種方式，關鍵是要表明您使用了霍爾姆的校正來調整 p 值。當然，我假設在撰寫的其他部分，您已經包括了相關的描述性統計資料（即組平均值和標準差），因為這些 p 值本身並不是很有信息量。"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.6 單因子變異數分析的執行條件",
    "text": "13.6 單因子變異數分析的執行條件\nLike any statistical test, analysis of variance relies on some assumptions about the data, specifically the residuals. There are three key assumptions that you need to be aware of: normality, homogeneity of variance and independence.\n[Additional technical detail 12]\nSo, how do we check whether the assumption about the residuals is accurate? Well, as I indicated above, there are three distinct claims buried in this one statement, and we’ll consider them separately.\n\nHomogeneity of variance. Notice that we’ve only got the one value for the population standard deviation (i.e., \\(\\sigma\\)), rather than allowing each group to have it’s own value (i.e., \\(\\sigma_k\\)). This is referred to as the homogeneity of variance (sometimes called homoscedasticity) assumption. ANOVA assumes that the population standard deviation is the same for all groups. We’ll talk about this extensively in the [Checking the homogeneity of variance assumption] section.\nNormality. The residuals are assumed to be normally distributed. As we saw in Section 11.9, we can assess this by looking at QQ plots (or running a Shapiro-Wilk test. I’ll talk about this more in an ANOVA context in the [Checking the normality assumption] section.\nIndependence. The independence assumption is a little trickier. What it basically means is that, knowing one residual tells you nothing about any other residual. All of the \\(\\epsilon_{ik}\\) values are assumed to have been generated without any “regard for” or “relationship to” any of the other ones. There’s not an obvious or simple way to test for this, but there are some situations that are clear violations of this. For instance, if you have a repeated measures design, where each participant in your study appears in more than one condition, then independence doesn’t hold. There’s a special relationship between some observations, namely those that correspond to the same person! When that happens, you need to use something like a [Repeated measures one-way ANOVA].\n\n\n13.6.1 同質性檢核\n\nTo make the preliminary test on variances is rather like putting to sea in a rowing boat to find out whether conditions are sufficiently calm for an ocean liner to leave port!\n– George Box (Box, 1953)\n\nThere’s more than one way to skin a cat, as the saying goes, and more than one way to test the homogeneity of variance assumption, too (though for some reason no-one made a saying out of that). The most commonly used test for this that I’ve seen in the literature is the Levene test (Levene, 1960), and the closely related Brown-Forsythe test (Brown & Forsythe, 1974).\nRegardless of whether you’re doing the standard Levene test or the Brown-Forsythe test, the test statistic, which is sometimes denoted \\(F\\) but also sometimes written as \\(W\\), is calculated in exactly the same way that the F-statistic for the regular ANOVA is calculated, just using a \\(Z_{ik}\\) rather than \\(Y_{ik}\\). With that in mind, we can go on to look at how to run the test in jamovi.\n[Additional technical detail 13]\n\n\n13.6.2 jamovi的Levene檢定\nOkay, so how do we run the Levene test? Simple really - under the ANOVA ‘Assumption Checks’ option, just click on the ‘Homogeneity tests’ checkbox. If we look at the output, shown in Figure 13.5, we see that the test is non-significant (\\(F_{2,15} = 1.45, p = .266\\)), so it looks like the homogeneity of variance assumption is fine. However, looks can be deceptive! If your sample size is pretty big, then the Levene test could show up a significant effect (i.e. p < .05) even when the homogeneity of variance assumption is not violated to an extent which troubles the robustness of ANOVA. This was the point George Box was making in the quote above. Similarly, if your sample size is quite small, then the homogeneity of variance assumption might not be satisfied and yet a Levene test could be non-significant (i.e. p > .05). What this means is that, alongside any statistical test of the assumption being met, you should always plot the standard deviation around the means for each group / category in the analysis…just to see if they look fairly similar (i.e. homogeneity of variance) or not.\n\n\n\n\n\nFigure 13.5: Levene test output for one-way ANOVA in jamovi\n\n\n\n\n\n\n13.6.3 校正異質性的分析結果\nIn our example, the homogeneity of variance assumption turned out to be a pretty safe one: the Levene test came back non-significant (notwithstanding that we should also look at the plot of standard deviations), so we probably don’t need to worry. However, in real life we aren’t always that lucky. How do we save our ANOVA when the homogeneity of variance assumption is violated? If you recall from our discussion of t-tests, we’ve seen this problem before. The Student t-test assumes equal variances, so the solution was to use the Welch t-test, which does not. In fact, Welch (1951) also showed how we can solve this problem for ANOVA too (the Welch one-way test). It’s implemented in jamovi using the One-Way ANOVA analysis. This is a specific analysis approach just for one-way ANOVA, and to run the Welch one-way ANOVA for our example, we would re-run the analysis as previously, but this time use the jamovi ANOVA - One Way ANOVA analysis command, and check the option for Welch’s test (see Figure 13.6). To understand what’s happening here, let’s compare these numbers to what we got earlier when [Running an ANOVA in jamovi] originally. To save you the trouble of flicking back, this is what we got last time: \\(F(2, 15) = 18.611, p = .00009\\), also shown as the Fisher’s test in the One-Way ANOVA shown in Figure 13.6.\n\n\n\n\n\nFigure 13.6: Welch’s test as part of the One Way ANOVA analysis in jamovi\n\n\n\n\nOkay, so originally our ANOVA gave us the result \\(F(2, 15) = 18.6\\), whereas the Welch one way test gave us \\(F(2, 9.49) = 26.32\\). In other words, the Welch test has reduced the within-groups degrees of freedom from 15 to 9.49, and the F-value has increased from 18.6 to 26.32.\n\n\n13.6.4 常態性檢核\nTesting the normality assumption is relatively straightforward. We covered most of what you need to know in Section 11.9. The only thing we really need to do is draw a QQ plot and, in addition if it is available, run the Shapiro-Wilk test. The QQ plot is shown in Figure 13.7 and it looks pretty normal to me. If the Shapiro-Wilk test is not significant (i.e. \\(p > .05\\)) then this indicates that the assumption of normality is not violated. However, as with Levene’s test, if the sample size is large then a significant Shapiro-Wilk test may in fact be a false positive, where the assumption of normality is not violated in any substantive problematic sense for the analysis. And, similarly, a very small sample can produce false negatives. That’s why a visual inspection of the QQ plot is important.\n\n\n\n\n\nFigure 13.7: QQ plot in the One Way ANOVA analysis in jamovi\n\n\n\n\nAlongside inspecting the QQ plot for any deviations from normality, the Shapiro-Wilk test for our data does show a non-significant effect, with p = 0.6053 (see Figure 13.6. This therefore supports the QQ plot assessment; both checks find no indication that normality is violated.\n\n\n13.6.5 排除非常態性的分析結果\nNow that we’ve seen how to check for normality, we are led naturally to ask what we can do to address violations of normality. In the context of a one-way ANOVA, the easiest solution is probably to switch to a non-parametric test (i.e., one that doesn’t rely on any particular assumption about the kind of distribution involved). We’ve seen non-parametric tests before, in Chapter 11. When you only have two groups, the Mann-Whitney or the Wilcoxon test provides the non-parametric alternative that you need. When you’ve got three or more groups, you can use the Kruskal-Wallis rank sum test (Kruskal & Wallis, 1952). So that’s the test we’ll talk about next.\n\n\n13.6.6 Kruskal-Wallis檢定的邏輯\nThe Kruskal-Wallis test is surprisingly similar to ANOVA, in some ways. In ANOVA we started with \\(Y_{ik}\\), the value of the outcome variable for the ith person in the kth group. For the Kruskal Wallis test what we’ll do is rank order all of these \\(Y_{ik}\\) values and conduct our analysis on the ranked data. 14\n\n\n13.6.7 更多分析細節\nThe description in the previous section illustrates the logic behind the Kruskal-Wallis test. At a conceptual level, this is the right way to think about how the test works.15\nBut wait, there’s more! Dear lord, why is there always more? The story I’ve told so far is only actually true when there are no ties in the raw data. That is, if there are no two observations that have exactly the same value. If there are ties, then we have to introduce a correction factor to these calculations. At this point I’m assuming that even the most diligent reader has stopped caring (or at least formed the opinion that the tie-correction factor is something that doesn’t require their immediate attention). So I’ll very quickly tell you how it’s calculated, and omit the tedious details about why it’s done this way. Suppose we construct a frequency table for the raw data, and let fj be the number of observations that have the j-th unique value. This might sound a bit abstract, so here’s a concrete example from the frequency table of mood.gain from the clinicaltrials.csv data set (Table 13.11)\n\n\n\n\nTable 13.11:  Frequency table of mood gain from the clinicaltrials.csv data \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\nLooking at this table, notice that the third entry in the frequency table has a value of 2. Since this corresponds to a mood.gain of 0.3, this table is telling us that two people’s mood increased by 0.3. 16\nAnd so jamovi uses a tie-correction factor to calculate the tie-corrected Kruskall-Wallis statistic. And at long last, we’re actually finished with the theory of the Kruskal-Wallis test. I’m sure you’re all terribly relieved that I’ve cured you of the existential anxiety that naturally arises when you realise that you don’t know how to calculate the tie-correction factor for the Kruskal-Wallis test. Right?\n\n\n13.6.8 使用jamovi完成Kruskal-Wallis檢定\nDespite the horror that we’ve gone through in trying to understand what the Kruskal Wallis test actually does, it turns out that running the test is pretty painless, since jamovi has an analysis as part of the ANOVA analysis set called ‘Non-Parametric’ - ‘One Way ANOVA (Kruskall-Wallis)’ Most of the time you’ll have data like the clinicaltrial.csv data set, in which you have your outcome variable mood.gain and a grouping variable drug. If so, you can just go ahead and run the analysis in jamovi. What this gives us is a Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\), as in Figure 13.8\n\n\n\n\n\nFigure 13.8: Kruskall-Wallis one-way non-parametric ANOVA in jamovi"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.7 單因子重覆量數變異數分析",
    "text": "13.7 單因子重覆量數變異數分析\nThe one-way repeated measures ANOVA test is a statistical method of testing for significant differences between three or more groups where the same participants are used in each group (or each participant is closely matched with participants in other experimental groups). For this reason, there should always be an equal number of scores (data points) in each experimental group. This type of design and analysis can also be called a ‘related ANOVA’ or a ‘within subjects ANOVA’.\nThe logic behind a repeated measures ANOVA is very similar to that of an independent ANOVA (sometimes called a ‘between-subjects’ ANOVA). You’ll remember that earlier we showed that in a between-subjects ANOVA total variability is partitioned into between-groups variability (\\(SS_b\\)) and within-groups variability (\\(SS_w\\)), and after each is divided by the respective degrees of freedom to give MSb and MSw (see Table 13.1) the F-ratio is calculated as:\n\\[F=\\frac{MS_b}{MS_w}\\]\nIn a repeated measures ANOVA, the F-ratio is calculated in a similar way, but whereas in an independent ANOVA the within-group variability (\\(SS_w\\)) is used as the basis for the \\(MS_w\\) denominator, in a repeated measures ANOVA the \\(SS_w\\) is partioned into two parts. As we are using the same subjects in each group, we can remove the variability due to the individual differences between subjects (referred to as SSsubjects) from the within-groups variability. We won’t go into too much technical detail about how this is done, but essentially each subject becomes a level of a factor called subjects. The variability in this within-subjects factor is then calculated in the same way as any between-subjects factor. And then we can subtract SSsubjects from \\(SS_w\\) to provide a smaller SSerror term:\n\\[\\text{Independent ANOVA: } SS_{error} = SS_w\\] \\[\\text{Repeated Measures ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] This change in \\(SS_{error}\\) term often leads to a more powerful statistical test, but this does depend on whether the reduction in the \\(SS_{error}\\) more than compensates for the reduction in degrees of freedom for the error term (as degrees of freedom go from \\((n - k)\\) 17 to \\((n - 1)(k - 1)\\) (remembering that there are more subjects in the independent ANOVA design).\n\n13.7.1 jamovi的重覆量數變異數分析\nFirst, we need some data. Geschwind (1972) has suggested that the exact nature of a patient’s language deficit following a stroke can be used to diagnose the specific region of the brain that has been damaged. A researcher is concerned with identifying the specific communication difficulties experienced by six patients suffering from Broca’s Aphasia (a language deficit commonly experienced following a stroke) (Table 13.12).\n\n\n\n\nTable 13.12:  Word recognition task scores in stroke patients \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\nThe patients were required to complete three word recognition tasks. On the first (speech production) task, patients were required to repeat single words read out aloud by the researcher. On the second (conceptual) task, designed to test word comprehension, patients were required to match a series of pictures with their correct name. On the third (syntax) task, designed to test knowledge of correct word order, patients were asked to reorder syntactically incorrect sentences. Each patient completed all three tasks. The order in which patients attempted the tasks was counterbalanced between participants. Each task consisted of a series of 10 attempts. The number of attempts successfully completed by each patient are shown in Table 13.11. Enter these data into jamovi ready for analysis (or take a short-cut and load up the broca.csv file).\nTo perform a one-way related ANOVA in jamovi, open the one-way repeated measures ANOVA dialogue box, as in Figure 13.9, via ANOVA - Repeated Measures ANOVA.\n\n\n\n\n\nFigure 13.9: Repeated measures ANOVA dialogue box in jamovi\n\n\n\n\nThen:\n\nEnter a Repeated Measures Factor Name. This should be a label that you choose to describe the conditions repeated by all participants. For example, to describe the speech, conceptual and syntax tasks completed by all participants a suitable label would be ‘Task’. Note that this new factor name represents the independent variable in the analysis.\nAdd a third level in the Repeated Measures Factors text box, as there are three levels representing the three tasks: speech, conceptual and syntax. Change the labels of the levels accordingly.\nThen move each of the levels variables across to the Repeated Measures Cells text box.\nFinally, under the Assumption Checks option, tick the “Sphericity checks” text box.\n\njamovi output for a one-way repeated measures ANOVA is produced as shown in Figure 13.10 to Figure 13.13. The first output we should look at is Mauchly’s Test of Sphericity, which tests the hypothesis that the variances of the differences between the conditions are equal (meaning that the spread of difference scores between the study conditions is approximately the same). In Figure 13.10 Mauchly’s test significance level is \\(p = .720\\). If Mauchly’s test is non-significant (i.e. p < .05, as is the case in this analysis) then it is reasonable to conclude that the variances of the differences are not significantly different (i.e. they are roughly equal and sphericity can be assumed.).\n\n\n\n\n\nFigure 13.10: One-way repeated measures ANOVA output - Mauchly’s Test of Sphericity\n\n\n\n\nIf, on the other hand, Mauchly’s test had been significant (p < .05) then we would conclude that there are significant differences between the variance of the differences, and the requirement of sphericity has not been met. In this case, we should apply a correction to the F-value obtained in the one-way related ANOVA analysis:\n\nIf the Greenhouse-Geisser value in the “Tests of Sphericity” table is > .75 then you should use the Huynh-Feldt correction\nBut if the Greenhouse-Geisser value is < .75, then you should use the Greenhouse-Geisser correction.\n\nBoth these corrected F-values can be specified in the Sphericity Corrections check boxes under the Assumption Checks options, and the corrected F-values are then shown in the results table, as in Figure 13.11.\n\n\n\n\n\nFigure 13.11: One-way repeated measures ANOVA output - Tests of Within-Subjects Effects\n\n\n\n\nIn our analysis, we saw that the significance of Mauchly’s Test of Sphericity was p = .720 (i.e. p > 0.05). So, this means we can assume that the requirement of sphericity has been met so no correction to the F-value is needed. Therefore, we can use the ‘None’ Sphericity Correction output values for the repeated measure ‘Task’: \\(F = 6.93\\), \\(df = 2\\), \\(p = .013\\), and we can conclude that the number of tests successfully completed on each language task did vary significantly depending on whether the task was speech, comprehension or syntax based (\\(F(2, 10) = 6.93\\), \\(p = .013\\)).\nPost-hoc tests can also be specified in jamovi for repeated measures ANOVA in the same way as for independent ANOVA. The results are shown in Figure 13.12. These indicate that there is a significant difference between Speech and Syntax, but not between other levels.\n\n\n\n\n\nFigure 13.12: Post-hoc tests in repeated measures ANOVA in jamovi\n\n\n\n\nDescriptive statistics (marginal means) can be reviewed to help interpret the results, produced in the jamovi output as in Figure 13.13. Comparison of the mean number of trials successfully completed by participants shows that Broca’s Aphasics perform reasonably well on speech production (mean = 7.17) and language comprehension (mean = 6.17) tasks. However, their performance was considerably worse on the syntax task (mean = 4.33), with a significant difference in post-hoc tests between Speech and Syntax task performance.\n\n\n\n\n\nFigure 13.13: One-way repeated measures ANOVA output - Descriptive Statistics"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.8 Friedman無母數重覆量數變異數分析",
    "text": "13.8 Friedman無母數重覆量數變異數分析\nFriedman檢驗是一元重覆量數變數分析的非參數版本，可以在測試三個或更多組之間的差異時使用，其中每個組中的參與者相同，或者每個參與者與其他條件中的參與者密切匹配。如果因變項是序數，或者未滿足正態性假設，則可以使用Friedman檢驗。\n與Kruskall-Wallis檢驗一樣，基本數學知識很複雜，這裡不會介紹。對於本書的目的，僅需注意jamovi計算了Friedman檢驗的綁定修正版本，在 图 13.14 中有一個我們已經查看過的布洛卡失語症數據的示例。\n\n\n\n\n\n\n图 13.14: jamovi中的“重覆量數變數分析（非參數）”對話框和結果\n\n\n\n\n在jamovi中運行Friedman檢驗非常簡單。只需選擇分析 - ANOVA - 重覆量數變數分析（非參數），如 图 13.14 所示。然後將要比較的重複測量變項的名稱（語言、概念、語法）突顯並轉移到“測量：”文本框中。要為三個重複測量變項生成描述性統計（平均值和中位數），請單擊描述性按鈕。\njamovi結果顯示描述性統計、卡方值、自由度和p值（ 图 13.14 ）。由於p值小於通常用於確定顯著性的水平（p &lt; .05），我們可以得出結論，布洛卡失語症患者在語言生產（中位數= 7.5）和語言理解（中位數= 6.5）任務上表現相當好。然而，他們在語法任務上的表現明顯較差（中位數= 4.5），在事後檢驗中語言和語法任務表現之間存在顯著差異。"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.10 本章小結",
    "text": "13.10 本章小結\n這一章份量不少，但是有一些細節我並未提到12。最明顯的是在此並未討論處理不只一個分組變項的資料，我們在下一章 章节 14 將學習其中一部分。本章的學習重點有：\n\n理解變異數分析的運作原理 以及使用jamovi完成變異數分析\n學習如何計算變異數分析的效果量\n多重比較與事後檢定\n單因子變異數分析的執行條件\n同質性檢核 以及 校正異質性的分析結果\n常態性檢核以及排除非常態性的分析結果\n單因子重覆量數變異數分析 以及其無母數版本單因子重覆量數變異數分析\n\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific American, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th 本). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple Comparisons: Theory and Methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. 收入 I. O. et al (编), Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling (页 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The Analysis of Variance: Fixed, Random and Mixed Models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An alternative approach. Biometrika, 38, 330–336."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-1-balanced-designs-no-interactions",
    "href": "14-Factorial-ANOVA.html#factorial-anova-1-balanced-designs-no-interactions",
    "title": "14  多因子變異數分析",
    "section": "14.1 Factorial ANOVA 1: balanced designs, no interactions",
    "text": "14.1 Factorial ANOVA 1: balanced designs, no interactions\nWhen we discussed analysis of variance in Chapter 13, we assumed a fairly simple experimental design. Each person is in one of several groups and we want to know whether these groups have different mean scores on some outcome variable. In this section, I’ll discuss a broader class of experimental designs known as factorial designs, in which we have more than one grouping variable. I gave one example of how this kind of design might arise above. Another example appears in Chapter 13 in which we were looking at the effect of different drugs on the mood.gain experienced by each person. In that chapter we did find a significant effect of drug, but at the end of the chapter we also ran an analysis to see if there was an effect of therapy. We didn’t find one, but there’s something a bit worrying about trying to run two separate analyses trying to predict the same outcome. Maybe there actually is an effect of therapy on mood gain, but we couldn’t find it because it was being “hidden” by the effect of drug? In other words, we’re going to want to run a single analysis that includes both drug and therapy as predictors. For this analysis each person is cross-classified by the drug they were given (a factor with 3 levels) and what therapy they received (a factor with 2 levels). We refer to this as a \\(3 \\times 2\\) factorial design.\nIf we cross-tabulate drug by therapy, using the ‘Frequencies’ - ‘Contingency Tables’ analysis in jamovi (see Section 6.1), we get the table shown in Figure 14.1.\n\n\n\n\n\nFigure 14.1: jamovi contingency table of drug by therapy\n\n\n\n\nAs you can see, not only do we have participants corresponding to all possible combinations of the two factors, indicating that our design is completely crossed, it turns out that there are an equal number of people in each group. In other words, we have a balanced design. In this section I’ll talk about how to analyse data from balanced designs, since this is the simplest case. The story for unbalanced designs is quite tedious, so we’ll put it to one side for the moment.\n\n14.1.1 What hypotheses are we testing?\nLike one-way ANOVA, factorial ANOVA is a tool for testing certain types of hypotheses about population means. So a sensible place to start would be to be explicit about what our hypotheses actually are. However, before we can even get to that point, it’s really useful to have some clean and simple notation to describe the population means. Because of the fact that observations are cross-classified in terms of two different factors, there are quite a lot of different means that one might be interested in. To see this, let’s start by thinking about all the different sample means that we can calculate for this kind of design. Firstly, there’s the obvious idea that we might be interested in this list of group means (Table 14.1).\n\n\n\n\nTable 14.1:  Group means for drug and therapy groups in the clinicaltrial.csv data \n\ndrugtherapymood.gain\n\nplacebono.therapy0.300000\n\nanxifreeno.therapy0.400000\n\njoyzepamno.therapy1.466667\n\nplaceboCBT0.600000\n\nanxifreeCBT1.033333\n\njoyzepamCBT1.500000\n\n\n\n\n\nNow, the next Table (Table 14.2) shows a list of the group means for all possible combinations of the two factors (e.g., people who received the placebo and no therapy, people who received the placebo while getting CBT, etc.). It is helpful to organise all these numbers, plus the marginal and grand means, into a single table which looks like this:\n\n\n\n\nTable 14.2:  Group and total means for drug and therapy groups in the clintrial.csv data \n\nno therapyCBTtotal\n\nplacebo0.300.600.45\n\nanxifree0.401.030.72\n\njoyzepam1.471.501.48\n\ntotal0.721.040.88\n\n\n\n\n\nNow, each of these different means is of course a sample statistic. It’s a quantity that pertains to the specific observations that we’ve made during our study. What we want to make inferences about are the corresponding population parameters. That is, the true means as they exist within some broader population. Those population means can also be organised into a similar table, but we’ll need a little mathematical notation to do so (Table 14.3). As usual, I’ll use the symbol \\(\\mu\\) to denote a population mean. However, because there are lots of different means, I’ll need to use subscripts to distinguish between them.\nHere’s how the notation works. Our table is defined in terms of two factors. Each row corresponds to a different level of Factor A (in this case drug), and each column corresponds to a different level of Factor B (in this case therapy). If we let R denote the number of rows in the table, and \\(C\\) denote the number of columns, we can refer to this as an \\(R \\times C\\) factorial ANOVA. In this case \\(R = 3\\) and \\(C = 2\\). We’ll use lowercase letters to refer to specific rows and columns, so \\(\\mu_{rc}\\) refers to the population mean associated with the \\(r\\)-th level of Factor \\(A\\) (i.e. row number \\(r\\)) and the \\(c\\)-th level of Factor B (column number c).1 So the population means are now written like in Table 14.1:\n\n\n\n\nTable 14.3:  Notation for population means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\n\ntotal\n\n\n\n\n\nOkay, what about the remaining entries? For instance, how should we describe the average mood gain across the entire (hypothetical) population of people who might be given Joyzepam in an experiment like this, regardless of whether they were in CBT? We use the “dot” notation to express this. In the case of Joyzepam, notice that we’re talking about the mean associated with the third row in the table. That is, we’re averaging across two cell means (i.e., \\(\\mu_{31}\\) and \\(\\mu_{32}\\)). The result of this averaging is referred to as a marginal mean, and would be denoted \\(\\mu_3.\\) in this case. The marginal mean for CBT corresponds to the population mean associated with the second column in the table, so we use the notation because it is the mean obtained by averaging (marginalising2) over both. So our full table of population means can be written down like in Table 14.4.\n\n\n\n\nTable 14.4:  Notation for population and total means in a factorial table \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\\( \\mu_{1.} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\\( \\mu_{2.} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\\( \\mu_{3.} \\)\n\ntotal\\( \\mu_{.1} \\)\\( \\mu_{.2} \\)\\( \\mu_{..} \\)\n\n\n\n\n\nNow that we have this notation, it is straightforward to formulate and express some hypotheses. Let’s suppose that the goal is to find out two things. First, does the choice of drug have any effect on mood? And second, does CBT have any effect on mood? These aren’t the only hypotheses that we could formulate of course, and we’ll see a really important example of a different kind of hypothesis in the section Factorial ANOVA 2: balanced designs, interactions allowed, but these are the two simplest hypotheses to test, and so we’ll start there. Consider the first test. If the drug has no effect then we would expect all of the row means to be identical, right? So that’s our null hypothesis. On the other hand, if the drug does matter then we should expect these row means to be different. Formally, we write down our null and alternative hypotheses in terms of the equality of marginal means:\n\\[\\text{Null hypothesis, } H_0 \\text{: row means are the same, i.e., } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{Alternative hypothesis, } H_1 \\text{: at least one row mean is different}\\]\nIt’s worth noting that these are exactly the same statistical hypotheses that we formed when we ran a one-way ANOVA on these data in Chapter 13. Back then I used the notation \\(\\mu \\times {P}\\) to refer to the mean mood gain for the placebo group, with \\(\\mu{A}\\) and \\(\\mu \\times {J}\\) corresponding to the group means for the two drugs, and the null hypothesis was \\(\\mu{P} = \\mu{A} = \\mu{J}\\) . So we’re actually talking about the same hypothesis, it’s just that the more complicated ANOVA requires more careful notation due to the presence of multiple grouping variables, so we’re now referring to this hypothesis as \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\) . However, as we’ll see shortly, although the hypothesis is identical the test of that hypothesis is subtly different due to the fact that we’re now acknowledging the existence of the second grouping variable.\nSpeaking of the other grouping variable, you won’t be surprised to discover that our second hypothesis test is formulated the same way. However, since we’re talking about the psychological therapy rather than drugs our null hypothesis now corresponds to the equality of the column means:\n\\[\\text{Null hypothesis, } H_0 \\text{: column means are the same, i.e., } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{Alternative hypothesis, } H_1 \\text{: column means are different, i.e., } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n14.1.2 Running the analysis in jamovi\nThe null and alternative hypotheses that I described in the last section should seem awfully familiar. They’re basically the same as the hypotheses that we were testing in our simpler oneway ANOVAs in Chapter 13. So you’re probably expecting that the hypothesis tests that are used in factorial ANOVA will be essentially the same as the F-test from Chapter 13. You’re expecting to see references to sums of squares (SS), mean squares (MS), degrees of freedom (df), and finally an F-statistic that we can convert into a p-value, right? Well, you’re absolutely and completely right. So much so that I’m going to depart from my usual approach. Throughout this book, I’ve generally taken the approach of describing the logic (and to an extent the mathematics) that underpins a particular analysis first and only then introducing the analysis in jamovi. This time I’m going to do it the other way around and show you how to do it in jamovi first. The reason for doing this is that I want to highlight the similarities between the simple one-way ANOVA tool that we discussed in Chapter 13, and the more complicated approach that we’re going to use in this chapter.\nIf the data you’re trying to analyse correspond to a balanced factorial design then running your analysis of variance is easy. To see how easy it is, let’s start by reproducing the original analysis from Chapter 13. In case you’ve forgotten, for that analysis we were using only a single factor (i.e., drug) to predict our outcome variable (i.e., mood.gain), and we got the results shown in Figure 14.2.\n\n\n\n\n\nFigure 14.2: jamovi one way anova of mood.gain by drug\n\n\n\n\nNow, suppose I’m also curious to find out if therapy has a relationship to mood.gain. In light of what we’ve seen from our discussion of multiple regression in Chapter 12, you probably won’t be surprised that all we have to do is add therapy as a second ‘Fixed Factor’ in the analysis, see Figure 14.3.\n\n\n\n\n\nFigure 14.3: jamovi two way anova of mood.gain by drug and therapy\n\n\n\n\nThis output is pretty simple to read too. The first row of the table reports a between-group sum of squares (SS) value associated with the drug factor, along with a corresponding between-group df value. It also calculates a mean square value (MS), an F-statistic and a p-value. is also a row corresponding to the therapy factor and a row corresponding to the residuals (i.e., the within groups variation).\nNot only are all of the individual quantities pretty familiar, the relationships between these different quantities has remained unchanged, just like we saw with the original one-way ANOVA. Note that the mean square value is calculated by dividing \\(SS\\) by the corresponding \\(df\\). That is, it’s still true that\n\\[MS=\\frac{SS}{df}\\]\nregardless of whether we’re talking about drug, therapy or the residuals. To see this, let’s not worry about how the sums of squares values are calculated. Instead, let’s take it on faith that jamovi has calculated the \\(SS\\) values correctly, and try to verify that all the rest of the numbers make sense. First, note that for the drug factor, we divide \\(3.45\\) by \\(2\\) and end up with a mean square value of \\(1.73\\). For the therapy factor, there’s only 1 degree of freedom, so our calculations are even simpler: dividing \\(0.47\\) (the \\(SS\\) value) by 1 gives us an answer of \\(0.47\\) (the \\(MS\\) value).\nTurning to the F statistics and the p values, notice that we have two of each; one corresponding to the drug factor and the other corresponding to the therapy factor. Regardless of which one we’re talking about, the F statistic is calculated by dividing the mean square value associated with the factor by the mean square value associated with the residuals. If we use “A” as shorthand notation to refer to the first factor (factor A; in this case drug) and “R” as shorthand notation to refer to the residuals, then the F statistic associated with factor A is denoted FA, and is calculated as follows:\n\\[F_A=\\frac{MS_A}{MS_R}\\]\nand an equivalent formula exists for factor B (i.e., therapy). Note that this use of “R” to refer to residuals is a bit awkward, since we also used the letter R to refer to the number of rows in the table, but I’m only going to use “R” to mean residuals in the context of SSR and MSR, so hopefully this shouldn’t be confusing. Anyway, to apply this formula to the drugs factor we take the mean square of 1.73 and divide it by the residual mean square value of \\(0.07\\), which gives us an F-statistic of 26.15. The corresponding calculation for the therapy variable would be to divide \\(0.47\\) by \\(0.07\\) which gives \\(7.08\\) as the F-statistic. Not surprisingly, of course, these are the same values that jamovi has reported in the ANOVA table above.\nAlso in the ANOVA table is the calculation of the p values. Once again, there is nothing new here. For each of our two factors what we’re trying to do is test the null hypothesis that there is no relationship between the factor and the outcome variable (I’ll be a bit more precise about this later on). To that end, we’ve (apparently) followed a similar strategy to what we did in the one way ANOVA and have calculated an F-statistic for each of these hypotheses. To convert these to p values, all we need to do is note that the sampling distribution for the F statistic under the null hypothesis (that the factor in question is irrelevant) is an F distribution. Also note that the two degrees of freedom values are those corresponding to the factor and those corresponding to the residuals. For the drug factor we’re talking about an F distribution with 2 and 14 degrees of freedom (I’ll discuss degrees of freedom in more detail later). In contrast, for the therapy factor the sampling distribution is F with 1 and 14 degrees of freedom.\nAt this point, I hope you can see that the ANOVA table for this more complicated factorial analysis should be read in much the same way as the ANOVA table for the simpler one way analysis. In short, it’s telling us that the factorial ANOVA for our \\(3 \\times 2\\) design found a significant effect of drug (\\(F_{2,14} = 26.15, p < .001\\)) as well as a significant effect of therapy (\\(F_{1,14} = 7.08, p = .02\\)). Or, to use the more technically correct terminology, we would say that there are two main effects of drug and therapy. At the moment, it probably seems a bit redundant to refer to these as “main” effects, but it actually does make sense. Later on, we’re going to want to talk about the possibility of “interactions” between the two factors, and so we generally make a distinction between main effects and interaction effects.\n\n\n14.1.3 How are the sum of squares calculated?\nIn the previous section I had two goals. Firstly, to show you that the jamovi method needed to do factorial ANOVA is pretty much the same as what we used for a one way ANOVA. The only difference is the addition of a second factor. Secondly, I wanted to show you what the ANOVA table looks like in this case, so that you can see from the outset that the basic logic and structure behind factorial ANOVA is the same as that which underpins one way ANOVA. Try to hold onto that feeling. It’s genuinely true, insofar as factorial ANOVA is built in more or less the same way as the simpler one-way ANOVA model. It’s just that this feeling of familiarity starts to evaporate once you start digging into the details. Traditionally, this comforting sensation is replaced by an urge to hurl abuse at the authors of statistics textbooks.\nOkay, let’s start by looking at some of those details. The explanation that I gave in the last section illustrates the fact that the hypothesis tests for the main effects (of drug and therapy in this case) are F-tests, but what it doesn’t do is show you how the sum of squares (SS) values are calculated. Nor does it tell you explicitly how to calculate degrees of freedom (df values) though that’s a simple thing by comparison. Let’s assume for now that we have only two predictor variables, Factor A and Factor B. If we use Y to refer to the outcome variable, then we would use Yrci to refer to the outcome associated with the i-th member of group rc (i.e., level/row r for Factor A and level/column c for Factor B). Thus, if we use \\(\\bar{Y}\\) to refer to a sample mean, we can use the same notation as before to refer to group means, marginal means and grand means. That is, \\(\\bar{Y}_{rc}\\) is the sample mean associated with the rth level of Factor A and the cth level of Factor: \\(\\bar{Y}_{r.}\\) would be the marginal mean for the rth level of Factor A, \\(\\bar{Y}_{.c}\\) would be the marginal mean for the cth level of Factor B, and \\(\\bar{Y}_{..}\\) is the grand mean. In other words, our sample means can be organised into the same table as the population means. For our clinical trial data, that table is shown in Table 14.5.\n\n\n\n\nTable 14.5:  Notation for sample means for the clinical trial data \n\nno therapyCBTtotal\n\nplacebo\\( \\bar{Y}_{11} \\)\\( \\bar{Y}_{12} \\)\\( \\bar{Y}_{1.} \\)\n\nanxifree\\( \\bar{Y}_{21} \\)\\( \\bar{Y}_{22} \\)\\( \\bar{Y}_{2.} \\)\n\njoyzepam\\( \\bar{Y}_{31} \\)\\( \\bar{Y}_{32} \\)\\( \\bar{Y}_{3.} \\)\n\ntotal\\( \\bar{Y}_{.1} \\)\\( \\bar{Y}_{.2} \\)\\( \\bar{Y}_{..} \\)\n\n\n\n\n\nAnd if we look at the sample means that I showed earlier, we have \\(\\bar{Y}_{11} = 0.30\\), \\(\\bar{Y}_{12} = 0.60\\) etc. In our clinical trial example, the drugs factor has 3 levels and the therapy factor has 2 levels, and so what we’re trying to run is a \\(3 \\times 2\\) factorial ANOVA. However, we’ll be a little more general and say that Factor A (the row factor) has R levels and Factor B (the column factor) has C levels, and so what we’re running here is an \\(R \\times C\\) factorial ANOVA.\n[Additional technical detail 3]\n\n\n14.1.4 What are our degrees of freedom?\nThe degrees of freedom are calculated in much the same way as for one-way ANOVA. For any given factor, the degrees of freedom is equal to the number of levels minus 1 (i.e., \\(R - 1\\) for the row variable Factor A, and \\(C - 1\\) for the column variable Factor B). So, for the drugs factor we obtain \\(df = 2\\), and for the therapy factor we obtain \\(df = 1\\). Later on, when we discuss the interpretation of ANOVA as a regression model (see Section 14.6), I’ll give a clearer statement of how we arrive at this number. But for the moment we can use the simple definition of degrees of freedom, namely that the degrees of freedom equals the number of quantities that are observed, minus the number of constraints. So, for the drugs factor, we observe 3 separate group means, but these are constrained by 1 grand mean, and therefore the degrees of freedom is 2. For the residuals, the logic is similar, but not quite the same. The total number of observations in our experiment is 18. The constraints correspond to 1 grand mean, the 2 additional group means that the drug factor introduces, and the 1 additional group mean that the the therapy factor introduces, and so our degrees of freedom is 14. As a formula, this is \\(N - 1 - (R - 1) - (C - 1)\\), which simplifies to \\(N - R - C + 1\\).\n\n\n14.1.5 Factorial ANOVA versus one-way ANOVAs\nNow that we’ve seen how a factorial ANOVA works, it’s worth taking a moment to compare it to the results of the one way analyses, because this will give us a really good sense of why it’s a good idea to run the factorial ANOVA. In Chapter 13 I ran a one-way ANOVA that looked to see if there are any differences between drugs, and a second one-way ANOVA to see if there were any differences between therapies. As we saw in the section Section 14.1.1, the null and alternative hypotheses tested by the one-way ANOVAs are in fact identical to the hypotheses tested by the factorial ANOVA. Looking even more carefully at the ANOVA tables, we can see that the sum of squares associated with the factors are identical in the two different analyses (3.45 for drug and 0.92 for therapy), as are the degrees of freedom (2 for drug, 1 for therapy). But they don’t give the same answers! Most notably, when we ran the one-way ANOVA for therapy in Section 13.9 we didn’t find a significant effect (the p-value was .21). However, when we look at the main effect of therapy within the context of the two-way ANOVA, we do get a significant effect (p = .019). The two analyses are clearly not the same.\nWhy does that happen? The answer lies in understanding how the residuals are calculated. Recall that the whole idea behind an F-test is to compare the variability that can be attributed to a particular factor with the variability that cannot be accounted for (the residuals). If you run a one-way ANOVA for therapy, and therefore ignore the effect of drug, the ANOVA will end up dumping all of the drug-induced variability into the residuals! This has the effect of making the data look more noisy than they really are, and the effect of therapy which is correctly found to be significant in the two-way ANOVA now becomes non-significant. If we ignore something that actually matters (e.g., drug) when trying to assess the contribution of something else (e.g., therapy) then our analysis will be distorted. Of course, it’s perfectly okay to ignore variables that are genuinely irrelevant to the phenomenon of interest. If we had recorded the colour of the walls, and that turned out to be a non-significant factor in a three-way ANOVA, it would be perfectly okay to disregard it and just report the simpler two-way ANOVA that doesn’t include this irrelevant factor. What you shouldn’t do is drop variables that actually make a difference!\n\n\n14.1.6 What kinds of outcomes does this analysis capture?\nThe ANOVA model that we’ve been talking about so far covers a range of different patterns that we might observe in our data. For instance, in a two-way ANOVA design there are four possibilities: (a) only Factor A matters, (b) only Factor B matters, (c) both A and B matter, and (d) neither A nor B matters. An example of each of these four possibilities is plotted in Figure 14.4."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-2-balanced-designs-interactions-allowed",
    "href": "14-Factorial-ANOVA.html#factorial-anova-2-balanced-designs-interactions-allowed",
    "title": "14  多因子變異數分析",
    "section": "14.2 Factorial ANOVA 2: balanced designs, interactions allowed",
    "text": "14.2 Factorial ANOVA 2: balanced designs, interactions allowed\nThe four patterns of data shown in Figure 14.4 are all quite realistic. There are a great many data sets that produce exactly those patterns. However, they are not the whole story and the ANOVA model that we have been talking about up to this point is not sufficient to fully account for a table of group means. Why not? Well, so far we have the ability to talk about the idea that drugs can influence mood, and therapy can influence mood, but no way of talking about the possibility of an interaction between the two. An interaction between \\(A\\) and \\(B\\) is said to occur whenever the effect of Factor \\(A\\) is different, depending on which level of Factor \\(B\\) we’re talking about. Several examples of an interaction effect with the context of a \\(2 \\times 2\\) ANOVA are shown in Figure 14.5. To give a more concrete example, suppose that the operation of Anxifree and Joyzepam is governed by quite different physiological mechanisms. One consequence of this is that while Joyzepam has more or less the same effect on mood regardless of whether one is in therapy, Anxifree is actually much more effective when administered in conjunction with CBT. The ANOVA that we developed in the previous section does not capture this idea. To get some idea of whether an interaction is actually happening here, it helps to plot the various group means. In jamovi this is done via the ANOVA ‘Estimated Marginal Means’ option - just move drug and therapy across into the ‘Marginal Means’ box under ‘Term 1’. This should look something like Figure 14.6. Our main concern relates to the fact that the two lines aren’t parallel. The effect of CBT (difference between solid line and dotted line) when the drug is Joyzepam (right side) appears to be near zero, even smaller than the effect of CBT when a placebo is used (left side). However, when Anxifree is administered, the effect of CBT is larger than the placebo (middle). Is this effect real, or is this just random variation due to chance? Our original ANOVA cannot answer this question, because we make no allowances for the idea that interactions even exist! In this section, we’ll fix this problem.\n\n\n\n\n\nFigure 14.4: The four different outcomes for a \\(2 \\times 2\\) ANOVA when no interactions are present. In panel (a) we see a main effect of Factor A and no effect of Factor B. Panel (b) shows a main effect of Factor B but no effect of Factor A. Panel (c) shows main effects of both Factor A and Factor B. Finally, panel (d) shows no effect of either factor\n\n\n\n\n\n\n\n\n\nFigure 14.5: Qualitatively different interactions for a \\(2 \\times 2\\) ANOVA\n\n\n\n\n\n\n\n\n\nFigure 14.6: jamovi screen showing how to generate a descriptive interaction plot in ANOVA using the clinical trial data\n\n\n\n\n\n14.2.1 What exactly is an interaction effect?\nThe key idea that we’re going to introduce in this section is that of an interaction effect. In the ANOVA model we have looked at so far there are only two factors involved in our model (i.e., drug and therapy). But when we add an interaction we add a new component to the model: the combination of drug and therapy. Intuitively, the idea behind an interaction effect is fairly simple. It just means that the effect of Factor A is different, depending on which level of Factor B we’re talking about. But what does that actually mean in terms of our data? The plot in Figure 14.5 depicts several different patterns that, although quite different to each other, would all count as an interaction effect. So it’s not entirely straightforward to translate this qualitative idea into something mathematical that a statistician can work with.\n[Additional technical detail 4]\n\n\n14.2.2 Degrees of freedom for the interaction\nCalculating the degrees of freedom for the interaction is, once again, slightly trickier than the corresponding calculation for the main effects. To start with, let’s think about the ANOVA model as a whole. Once we include interaction effects in the model we’re allowing every single group to have a unique mean, \\(mu_{rc}\\). For an \\(R \\times C\\) factorial ANOVA, this means that there are \\(R \\times C\\) quantities of interest in the model and only the one constraint: all of the group means need to average out to the grand mean. So the model as a whole needs to have (\\(R \\times C\\)) - 1 degrees of freedom. But the main effect of Factor A has \\(R - 1\\) degrees of freedom, and the main effect of Factor B has \\(C - 1\\) degrees of freedom. This means that the degrees of freedom associated with the interaction is\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\nwhich is just the product of the degrees of freedom associated with the row factor and the column factor.\nWhat about the residual degrees of freedom? Because we’ve added interaction terms which absorb some degrees of freedom, there are fewer residual degrees of freedom left over. Specifically, note that if the model with interaction has a total of \\((R \\times C) - 1\\), and there are \\(N\\) observations in your data set that are constrained to satisfy 1 grand mean, your residual degrees of freedom now become \\(N - (R \\times C) - 1 + 1\\), or just \\(N - (R \\times C)\\).\n\n\n14.2.3 Running the ANOVA in jamovi\nAdding interaction terms to the ANOVA model in jamovi is straightforward. In fact it is more than straightforward because it is the default option for ANOVA. This means that when you specify an ANOVA with two factors, e.g. drug and therapy then the interaction component - drug \\(\\times\\) therapy - is added automatically to the model5. When we run the ANOVA with the interaction term included, then we get the results shown in Figure 14.7.\n\n\n\n\n\nFigure 14.7: Results for the full factorial model, including the interaction component drug \\(\\times\\) therapy\n\n\n\n\nAs it turns out, while we do have a significant main effect of drug (\\(F_{2,12} = 31.7, p < .001\\)) and therapy type (\\(F_{1,12} = 8.6, p = .013\\)), there is no significant interaction between the two (\\(F_{2,12} = 2.5, p = 0.125\\)).\n\n\n14.2.4 Interpreting the results\nThere’s a couple of very important things to consider when interpreting the results of factorial ANOVA. First, there’s the same issue that we had with one-way ANOVA, which is that if you obtain a significant main effect of (say) drug, it doesn’t tell you anything about which drugs are different to one another. To find that out, you need to run additional analyses. We’ll talk about some analyses that you can run in later Sections: Different ways to specify contrasts and Post hoc tests. The same is true for interaction effects. Knowing that there’s a significant interaction doesn’t tell you anything about what kind of interaction exists. Again, you’ll need to run additional analyses.\nSecondly, there’s a very peculiar interpretation issue that arises when you obtain a significant interaction effect but no corresponding main effect. This happens sometimes. For instance, in the crossover interaction shown in Figure 14.5 a, this is exactly what you’d find. In this case, neither of the main effects would be significant, but the interaction effect would be. This is a difficult situation to interpret, and people often get a bit confused about it. The general advice that statisticians like to give in this situation is that you shouldn’t pay much attention to the main effects when an interaction is present. The reason they say this is that, although the tests of the main effects are perfectly valid from a mathematical point of view, when there is a significant interaction effect the main effects rarely test interesting hypotheses. Recall from Section 14.1.1 that the null hypothesis for a main effect is that the marginal means are equal to each other, and that a marginal mean is formed by averaging across several different groups. But if you have a significant interaction effect then you know that the groups that comprise the marginal mean aren’t homogeneous, so it’s not really obvious why you would even care about those marginal means.\nHere’s what I mean. Again, let’s stick with a clinical example. Suppose that we had a \\(2 \\times 2\\) design comparing two different treatments for phobias (e.g., systematic desensitisation vs flooding), and two different anxiety reducing drugs (e.g., Anxifree vs Joyzepam). Now, suppose what we found was that Anxifree had no effect when desensitisation was the treatment, and Joyzepam had no effect when flooding was the treatment. But both were pretty effective for the other treatment. This is a classic crossover interaction, and what we’d find when running the ANOVA is that there is no main effect of drug, but a significant interaction. Now, what does it actually mean to say that there’s no main effect? Well, it means that if we average over the two different psychological treatments, then the average effect of Anxifree and Joyzepam is the same. But why would anyone care about that? When treating someone for phobias it is never the case that a person can be treated using an “average” of flooding and desensitisation. That doesn’t make a lot of sense. You either get one or the other. For one treatment one drug is effective, and for the other treatment the other drug is effective. The interaction is the important thing and the main effect is kind of irrelevant.\nThis sort of thing happens a lot. The main effect are tests of marginal means, and when an interaction is present we often find ourselves not being terribly interested in marginal means because they imply averaging over things that the interaction tells us shouldn’t be averaged! Of course, it’s not always the case that a main effect is meaningless when an interaction is present. Often you can get a big main effect and a very small interaction, in which case you can still say things like “drug A is generally more effective than drug B” (because there was a big effect of drug), but you’d need to modify it a bit by adding that “the difference in effectiveness was different for different psychological treatments”. In any case, the main point here is that whenever you get a significant interaction you should stop and think about what the main effect actually means in this context. Don’t automatically assume that the main effect is interesting."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#effect-size",
    "href": "14-Factorial-ANOVA.html#effect-size",
    "title": "14  多因子變異數分析",
    "section": "14.3 Effect size",
    "text": "14.3 Effect size\nThe effect size calculation for a factorial ANOVA is pretty similar to those used in one way ANOVA (see Effect size section). Specifically, we can use \\(\\eta^2\\) (eta-squared) as a simple way to measure how big the overall effect is for any particular term. As before, \\(\\eta^2\\) is defined by dividing the sum of squares associated with that term by the total sum of squares. For instance, to determine the size of the main effect of Factor A, we would use the following formula:\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\nAs before, this can be interpreted in much the same way as \\(R^2\\) in regression.6 It tells you the proportion of variance in the outcome variable that can be accounted for by the main effect of Factor A. It is therefore a number that ranges from 0 (no effect at all) to 1 (accounts for all of the variability in the outcome). Moreover, the sum of all the \\(\\eta^2\\) values, taken across all the terms in the model, will sum to the the total \\(R^2\\) for the ANOVA model. If, for instance, the ANOVA model fits perfectly (i.e., there is no within-groups variability at all!), the \\(\\eta^2\\) values will sum to 1. Of course, that rarely if ever happens in real life.\nHowever, when doing a factorial ANOVA, there is a second measure of effect size that people like to report, known as partial \\(\\eta^2\\). The idea behind partial \\(\\eta^2\\) (which is sometimes denoted \\(p^{\\eta^2}\\) or \\(\\eta_p^2\\)) is that, when measuring the effect size for a particular term (say, the main effect of Factor A), you want to deliberately ignore the other effects in the model (e.g., the main effect of Factor B). That is, you would pretend that the effect of all these other terms is zero, and then calculate what the \\(\\eta^2\\) value would have been. This is actually pretty easy to calculate. All you have to do is remove the sum of squares associated with the other terms from the denominator. In other words, if you want the partial \\(\\eta^2\\) for the main effect of Factor A, the denominator is just the sum of the SS values for Factor A and the residuals\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\nThis will always give you a larger number than \\(\\eta^2\\), which the cynic in me suspects accounts for the popularity of partial \\(\\eta^2\\). And once again you get a number between 0 and 1, where 0 represents no effect. However, it’s slightly trickier to interpret what a large partial \\(\\eta^2\\) value means. In particular, you can’t actually compare the partial \\(\\eta^2\\) values across terms! Suppose, for instance, there is no within-groups variability at all: if so, \\(SS_R = 0\\). What that means is that every term has a partial \\(\\eta^2\\) value of 1. But that doesn’t mean that all terms in your model are equally important, or indeed that they are equally large. All it mean is that all terms in your model have effect sizes that are large relative to the residual variation. It is not comparable across terms.\nTo see what I mean by this, it’s useful to see a concrete example. First, let’s have a look at the effect sizes for the original ANOVA (Table 14.6) without the interaction term, from Figure 14.3.\n\n\n\n\nTable 14.6:  Effect sizes when the interaction term is not included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.79\n\ntherapy0.100.34\n\n\n\n\n\nLooking at the \\(\\eta^2\\) values first, we see that drug accounts for 71% of the variance (i.e. \\(\\eta^2 = 0.71\\)) in mood.gain, whereas therapy only accounts for 10%. This leaves a total of 19% of the variation unaccounted for (i.e., the residuals constitute 19% of the variation in the outcome). Overall, this implies that we have a very large effect7 of drug and a modest effect of therapy.\nNow let’s look at the partial \\(\\eta^2\\) values, shown in Figure 14.3. Because the effect of therapy isn’t all that large, controlling for it doesn’t make much of a difference, so the partial \\(\\eta^2\\) for drug doesn’t increase very much, and we obtain a value of \\(p^{\\eta^2} = 0.79\\). In contrast, because the effect of drug was very large, controlling for it makes a big difference, and so when we calculate the partial \\(\\eta^2\\) for therapy you can see that it rises to \\(p^{\\eta^2} = 0.34\\). The question that we have to ask ourselves is, what do these partial \\(\\eta^2\\) values actually mean? The way I generally interpret the partial \\(\\eta^2\\) for the main effect of Factor A is to interpret it as a statement about a hypothetical experiment in which only Factor A was being varied. So, even though in this experiment we varied both A and B, we can easily imagine an experiment in which only Factor A was varied, and the partial \\(\\eta^2\\) statistic tells you how much of the variance in the outcome variable you would expect to see accounted for in that experiment. However, it should be noted that this interpretation, like many things associated with main effects, doesn’t make a lot of sense when there is a large and significant interaction effect.\nSpeaking of interaction effects, Table 14.7 shows what we get when we calculate the effect sizes for the model that includes the interaction term, as in Figure 14.7. As you can see, the \\(\\eta^2\\) values for the main effects don’t change, but the partial \\(\\eta^2\\) values do:\n\n\n\n\nTable 14.7:  Effect sizes when the interaction term is included in the ANOVA model \n\neta.sqpartial.eta.sq\n\ndrug0.710.84\n\ntherapy0.100.42\n\ndrug*therapy0.060.29\n\n\n\n\n\n\n14.3.1 Estimated group means\nIn many situations you will find yourself wanting to report estimates of all the group means based on the results of your ANOVA, as well as confidence intervals associated with them. You can use the ‘Estimated Marginal Means’ option in the jamovi ANOVA analysis to do this, as in Figure 14.8. If the ANOVA that you have run is a saturated model (i.e., contains all possible main effects and all possible interaction effects) then the estimates of the group means are actually identical to the sample means, though the confidence intervals will use a pooled estimate of the standard errors rather than use a separate one for each group.\n\n\n\n\n\nFigure 14.8: jamovi screenshot showing the marginal means for the saturated model, i.e. including the interaction component, with the clinical trial data set\n\n\n\n\nIn the output we see that the estimated mean mood gain for the placebo group with no therapy was \\(0.300\\), with a \\(95\\%\\) confidence interval from \\(0.006\\) to \\(0.594\\). Note that these are not the same confidence intervals that you would get if you calculated them separately for each group, because of the fact that the ANOVA model assumes homogeneity of variance and therefore uses a pooled estimate of the standard deviation.\nWhen the model doesn’t contain the interaction term, then the estimated group means will be different from the sample means. Instead of reporting the sample mean, jamovi will calculate the value of the group means that would be expected on the basis of the marginal means (i.e., assuming no interaction). Using the notation we developed earlier, the estimate reported for µrc, the mean for level r on the (row) Factor A and level c on the (column) Factor B would be \\(\\mu_{..} + \\alpha_r + \\beta_c\\). If there are genuinely no interactions between the two factors, this is actually a better estimate of the population mean than the raw sample mean would be. Removing the interaction term from the model, via the ‘Model’ options in the jamovi ANOVA analysis, provides the marginal means for the analysis shown in Figure 14.9.\n\n\n\n\n\nFigure 14.9: jamovi screenshot showing the marginal means for the unsaturated model, i.e. without the interaction component, with the clinical trial data set"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#assumption-checking",
    "href": "14-Factorial-ANOVA.html#assumption-checking",
    "title": "14  多因子變異數分析",
    "section": "14.4 Assumption checking",
    "text": "14.4 Assumption checking\nAs with one-way ANOVA, the key assumptions of factorial ANOVA are homogeneity of variance (all groups have the same standard deviation), normality of the residuals, and independence of the observations. The first two are things we can check for. The third is something that you need to assess yourself by asking if there are any special relationships between different observations, for example repeated measures where the independent variable is time so there is a relationship between the observations at time one and time two: observations at different time points are from the same people. Additionally, if you aren’t using a saturated model (e.g., if you’ve omitted the interaction terms) then you’re also assuming that the omitted terms aren’t important. Of course, you can check this last one by running an ANOVA with the omitted terms included and see if they’re significant, so that’s pretty easy. What about homogeneity of variance and normality of the residuals? As it turns out, these are pretty easy to check. It’s no different to the checks we did for a one-way ANOVA.\n\n14.4.1 Homogeneity of variance\nAs mentioned in Section 13.6.1 in the last chapter, it’s a good idea to visually inspect a plot of the standard deviations compared across different groups / categories, and also see if the Levene test is consistent with the visual inspection. The theory behind the Levene test was discussed in Section 13.6.1, so I won’t discuss it again. This test expects that you have a saturated model (i.e., including all of the relevant terms), because the test is primarily concerned with the within-group variance, and it doesn’t really make a lot of sense to calculate this any way other than with respect to the full model. The Levene test can be specified under the ANOVA ‘Assumption Checks’ - ‘Homogeneity Tests’ option in jamovi, with the result shown as in Figure 14.10. The fact that the Levene test is non-significant means that, providing it is consistent with a visual inspection of the plot of standard deviations, we can safely assume that the homogeneity of variance assumption is not violated.\n\n\n14.4.2 Normality of residuals\nAs with one-way ANOVA we can test for the normality of residuals in a straightforward fashion (see Section 13.6.4). Primarily though, it’s generally a good idea to examine the residuals graphically using a QQ plot. See Figure 14.10.\n\n\n\n\n\nFigure 14.10: Checking assumptions in an ANOVA model"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "href": "14-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "title": "14  多因子變異數分析",
    "section": "14.5 共變數分析 (ANCOVA)",
    "text": "14.5 共變數分析 (ANCOVA)\nANOVA的一種變體是當您擁有一個可能與因變量相關的額外連續變量時。這個額外的變量可以作為協變量添加到分析中，正如協方差分析（ANCOVA）這個貼切的名稱所示。\n在ANCOVA中，因變量的值會根據協變量的影響進行「調整」，然後以通常的方式在各組之間測試「調整後」的均值。這種技術可以提高實驗的精確性，因此提供了對因變量中組均值相等性的更「有效」的檢驗。ANCOVA是如何做到這一點的呢？儘管協變量本身通常不是實驗性的，但對協變量的調整可以降低實驗誤差的估計，從而通過減少誤差變異，提高精確性。這意味著不適當地無法拒絕虛無假設（偽陰性或第二類錯誤）的可能性較小。\n儘管存在這個優勢，ANCOVA仍然存在解除組間實際差異的風險，這應該避免。例如，看看@fig-fig13-11，它顯示了統計焦慮與年齡的關係，並顯示了兩個不同的組別–具有藝術或科學背景或偏好的學生。以年齡為協變量的ANCOVA可能會得出統計焦慮在兩個組別之間沒有差異的結論。這個結論是否合理呢？很可能不合理，因為兩個組別的年齡不重疊，方差分析實質上是「將結果外推到沒有數據的區域」（Everitt (1996)，第68頁）。\n\n\n\n\n\n\n图 14.11: 統計焦慮與年齡的圖示，對於兩個不同的組別\n\n\n\n\n顯然，需要仔細考慮對區別鮮明的組別進行協方差分析。這適用於單因素和因子設計，因為ANCOVA可以用於兩者。\n\n\n14.5.1 使用jamovi完成共變數分析\n一位健康心理學家對例行騎自行車和壓力對幸福程度的影響感興趣，並將年齡作為協變量。您可以在ancova.csv文件中找到數據集。在jamovi中打開此文件，然後選擇分析 - ANOVA - ANCOVA 以打開ANCOVA分析窗口（图 14.12）。突顯因變量「幸福」，將其轉移到「因變量」文本框中。突顯自變量「壓力」和「通勤」，將它們轉移到「固定因素」文本框中。突顯協變量「年齡」，將其轉移到「協變量」文本框中。然後單擊估計邊際均值以顯示圖表和表格選項。\n\n\n\n\n\n\n\n图 14.12: jamovi ANCOVA分析窗口\n\n\n\n\njamovi結果窗口中產生了一個顯示主題效應測試的ANCOVA表格（图 14.13）。協變量「年齡」的F值在 \\(p = .023\\) 上顯著，這表明年齡是影響因變量幸福的重要預測因子。當我們查看估計的邊際平均分數（图 14.14）時，由於在此ANCOVA中包含協變量「年齡」，所以已進行了調整（與未包含協變量的分析相比）。圖表（图 14.15）是一個很好的視覺化和解釋顯著效應的方法。\n\n\n\n\n\n\n\n图 14.13: jamovi ANCOVA輸出，將幸福度作為壓力和通勤方法的函數，年齡作為協變量\n\n\n\n\n\n\n\n\n\n图 14.14: 作為壓力和通勤方式函數的平均幸福水平表（根據協變量年齡進行調整），帶有95％置信區間\n\n\n\n\n\\(F\\) 值主要效果「壓力」（52.61）的相應概率為 \\(p &lt; .001\\)。主要效果「通勤」（42.33）的 \\(F\\) 值的相應概率為 \\(p &lt; .001\\)。由於這兩者都小於通常用於判定統計結果是否顯著的概率（\\(p &lt; .05\\)），我們可以得出壓力的顯著主要效應（\\(F(1, 15) = 52.61, p &lt; .001\\)）和通勤方式的顯著主要效應（\\(F(1, 15) = 42.33, p &lt; .001\\)）。還發現了壓力和通勤方式之間的顯著交互作用（\\(F(1, 15) = 14.15, p = .002\\)）。\n在 图 14.15 中，我們可以看到年齡作為協變量時的調整後、邊際的平均幸福分數。在這個分析中，存在一個顯著的交互效應，即壓力較低的騎自行車上班的人比壓力較低的開車上班的人和壓力較高的人（無論他們是騎自行車還是開車上班）更幸福。還有壓力的顯著主要效應——壓力較低的人比壓力較高的人更幸福。而且通勤行為的顯著主要效應也是如此——平均而言，騎自行車上班的人比開車上班的人更幸福。\n\n\n\n\n\n\n图 14.15: 作為壓力和通勤方式函數的平均幸福水平圖\n\n\n\n\n需要注意的一點是，如果您想在 ANOVA 中包含協變量，那麼還有一個額外的假設：協變量與因變量之間的關係應對所有自變量的水平都是相似的。這可以通過在 jamovi Model - Model terms 選項中為協變量和每個自變量添加交互項來檢查。如果交互效應不顯著，則可以將其移除。如果它顯著，則可能需要使用更高級的統計技術（這超出了本書的範疇，所以您可能需要諮詢一位友好的統計學家）。總之，在進行 ANCOVA 分析時，要仔細考慮協變量與自變量的關係，以確保結果的準確性和有效性。\n\n總的來說，ANCOVA 分析可以幫助我們更好地瞭解不同變量之間的關係，並通過引入協變量來提高實驗的精確性。然而，在實際應用中，我們需要仔細思考和評估協變量的選擇，以確保結果的可靠性。在進行分析時，要注意檢查假設，並在必要時尋求統計專家的幫助。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "href": "14-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "title": "14  多因子變異數分析",
    "section": "14.6 變異數分析就是線性模型",
    "text": "14.6 變異數分析就是線性模型\n一個非常重要的事情是要了解 ANOVA 和回歸實際上是一回事。從表面上看，您可能不會認為這是真的。畢竟，到目前為止我對它們的描述表明 ANOVA 主要關注測試組間差異，而回歸主要關注瞭解變量之間的相關性。在這一點上，這完全沒錯。但是當你深入了解時，所謂的 ANOVA 和回歸的基本機制非常相似。事實上，如果你仔細想想，你已經看到了這一點的證據。 ANOVA 和回歸都依賴於平方和 (SS)，都使用 F 檢驗等。回顧過去，很難逃避這樣的感覺： 章节 12 和 章节 13 有點重複。\n這樣做的原因是 ANOVA 和回歸都是線性模型。在回歸的情況下，這是顯而易見的。我們用來定義預測因素和結果之間關係的回歸方程是一條直線的方程，所以這顯然是一個線性模型，方程式為\n\\[Y_p=b_0+b_1 X_{1p} +b_2 X_{2p} + \\epsilon_p\\]\n其中 \\(Y_p\\) 是第 p 次觀察（例如，第 p 個人）的結果值，\\(X_{1p}\\) 是第 p 次觀察的第一個預測因子的值，\\(X_{2p}\\) 是第 p 次觀察的第二個預測因子的值，\\(b_0\\)，\\(b_1\\) 和 \\(b_2\\) 是我們的回歸係數，\\(\\epsilon_p\\) 是第 p 個殘差。如果我們忽略殘差 \\(\\epsilon_p\\)，僅關注回歸線本身，我們得到以下公式：\n\\[\\hat{Y}_p=b_0+b_1 X_{1p} +b_2 X_{2p} \\]\n其中 \\(\\hat{Y}_p\\) 是回歸線為第 p 個人預測的 Y 值，而不是實際觀察到的值 \\(Y_p\\)。不是立即顯而易見的事情是，我們可以將 ANOVA 也寫成線性模型。然而，實際上這非常簡單。讓我們從一個非常簡單的例子開始，將 \\(2 \\times 2\\) 因子 ANOVA 重寫為線性模型。\n\n\n\n14.6.1 示範資料\n為了具體說明，假設我們的結果變量是學生在我的課堂上獲得的成績，這是一個比例尺度變量，對應於 \\(0％\\) 到 \\(100％\\) 的分數。有兩個感興趣的預測變量：學生是否參加了課程（出席變量）以及學生是否真正閱讀了教科書（閱讀變量）。我們將假設如果學生參加課程，那麼 attend = 1，如果他們沒有參加，那麼 attend = 0。同樣，如果學生閱讀了教科書，我們將說 reading = 1，如果他們沒有，那麼 reading = 0。\n好吧，到目前為止，這還算簡單。接下來我們需要做的是將一些數學概念應用在這裡（抱歉！）。為了舉例，讓 \\(Y_p\\) 表示課堂中第 p 個學生的成績。這與我們在本章前面使用的符號不完全相同。以前，我們用符號 \\(Y_{rci}\\) 來表示第 1 個預測因子的第 r 個組中的第 i 個人（行因子）和第 2 個預測因子的第 c 個組（列因子）。這種擴展符號對於描述如何計算 SS 值非常方便，但在目前的情況下很繁瑣，所以我在這裡更換符號。現在，\\(Y_p\\) 符號比 \\(Y_{rci}\\) 更簡單，但它的缺點是它實際上沒有跟踪組成員資訊！也就是說，如果我告訴你 \\(Y_{0,0,3} = 35\\)，你會立刻知道我們在談論一個沒有上課（即 attend = 0）並且沒有閱讀教科書（即 reading = 0）的學生（事實上是第 3 個這樣的學生），並且最終未通過課程（成績 = 35）。但如果我告訴你 \\(Y_p = 35\\)，你只知道第 p 個學生沒有取得好成績。我們在這裡丟失了一些關鍵資訊。當然，想想如何解決這個問題並不費力。相反，我們將引入兩個新變量 \\(X_{1p}\\) 和 \\(X_{2p}\\) 來追踪這些資訊。在我們假設的學生案例中，我們知道 \\(X_{1p} = 0\\)（即 attend = 0）和 \\(X_{2p} = 0\\)（即 reading = 0）。因此，數據可能如 表格 14.8 所示。\n\n\n\n\n\n表格 14.8: 成績，出勤和閱讀教科書的數據\n\n\nperson, \\(p\\)\ngrade, \\(Y_p\\)\nattendance, \\(X_{1p}\\)\nreading, \\(X_{2p}\\)\n\n\n1\n90\n1\n1\n\n\n2\n87\n1\n1\n\n\n3\n75\n0\n1\n\n\n4\n60\n1\n0\n\n\n5\n35\n0\n0\n\n\n6\n50\n0\n0\n\n\n7\n65\n1\n0\n\n\n8\n70\n0\n1\n\n\n\n\n\n\n\n\n當然，這並沒有什麼特別之處。這正是我們期望看到的數據格式！請參閱數據文件 rtfm.csv。我們可以使用 jamovi 的 ‘Descriptives’ 分析來確認這個數據集對應於一個平衡設計，對於 attend 和 reading 的每個組合，都有 2 個觀測值。同樣，我們還可以為每個組合計算平均成績。這在 图 14.16 中顯示。看著平均分數，人們會強烈感覺到閱讀課本和上課都非常重要。\n\n\n\n\n\n\n图 14.16: rtfm 數據集的 jamovi 描述性統計\n\n\n\n\n\n\n14.6.2 以迴歸模型處理非連續因子\n好吧，讓我們回到數學上的討論。現在，我們的數據已用三個數值變量表示：連續變量 \\(Y\\) 和兩個二元變量 \\(X_1\\) 和 \\(X_2\\)。我希望您能認識到，我們的 \\(2 \\times 2\\) 因子分析變異數完全等同於迴歸模型\n\\[Y_p=b_0+b_1 X_{1p} + b_2 X_{2p} + \\epsilon_p\\]\n當然，這正是我之前用來描述具有兩個預測變量的迴歸模型的完全相同的方程式！唯一的區別是 \\(X_1\\) 和 \\(X_2\\) 現在是二元變量（即，值只能為 0 或 1），而在迴歸分析中，我們期望 \\(X_1\\) 和 \\(X_2\\) 是連續的。有幾種方法可以說服您相信這一點。一個可能性是進行冗長的數學練習，證明這兩者是相同的。然而，我要冒昧地猜測，這本書的大多數讀者會覺得這很煩人而不是有幫助。相反，我將解釋基本思想，然後依賴 jamovi 來說明 ANOVA 分析和迴歸分析不僅相似，而且在所有意圖和目的上是相同的。讓我們首先將其作為 ANOVA 運行。為此，我們將使用 rtfm 數據集，图 14.17 顯示了在 jamovi 中運行分析時我們得到的結果。\n\n\n\n\n\n\n图 14.17: 在 jamovi 中的 rtfm.csv 數據集 ANOVA，不包含交互作用項\n\n\n\n\n好的，通過從 ANOVA 表和我們之前呈現的平均分數中讀取關鍵數字，我們可以看到，如果學生參加課程（\\(F_{1,5} = 21.6, p = .0056\\)），他們的成績會更高，如果他們閱讀教材（\\(F_{1,5} = 52.3, p = .0008\\)）。讓我們記下這些 p 值和這些 \\(F\\) 統計數字。\n現在讓我們從線性迴歸的角度考慮相同的分析。在 rtfm 數據集中，我們將 attend 和 reading 編碼為數值預測變量。在這種情況下，這是完全可以接受的。在某種意義上，參加課程的學生（即 attend = 1）事實上的確比沒有參加的學生（即 attend = 0）做了“更多的出席”。因此，將其作為迴歸模型中的預測變量完全不是不合理的。這有點不尋常，因為預測變量只有兩個可能的值，但這並不違反線性迴歸的任何假設。而且易於解釋。如果 attend 的迴歸係數大於 0，則意味著參加課程的學生會獲得更高的成績。如果小於零，那麼參加課程的學生會獲得較低的成績。對於我們的閱讀變量也是如此。\n等一下。為什麼會這樣？這對於接受過幾堂統計課程並熟悉數學的人來說是直觀明顯的，但對其他人來說一開始並不清楚。要理解為什麼會這樣，有助於仔細觀察幾個特定的學生。讓我們首先考慮我們數據集中的第 6 位和第 7 位學生（即 \\(p = 6\\) 和 \\(p = 7\\)）。兩者都沒有閱讀教科書，因此在這兩種情況下，我們都可以將 reading 設為 0。換句話說，用我們的數學符號表示，我們觀察到 \\(X_{2,6} = 0\\) 和 \\(X_{2,7} = 0\\)。然而，第 7 位學生參加了課程（即 attend = 1，\\(X_{1,7} = 1\\)），而第 6 位學生沒有參加（即 attend = 0，\\(X_{1,6} = 0\\)）。現在讓我們看看當我們將這些數字插入迴歸線的一般公式時會發生什麼。對於第 6 位學生，迴歸預測：\n\\[\n\\begin{split}\n\\hat{Y}_6 & = b_0 + b_1 X_{1,6} + b_2 X_{2,6} \\\\\n& = b_0 + (b_1 \\times 0) + (b_2 \\times 0) \\\\\n& = b_0\n\\end{split}\n\\]\n因此，我們預計這位學生將獲得與截距項 \\(b_0\\) 相對應的成績。那麼第 7 位學生呢？這次當我們將數字插入迴歸線公式時，我們得到以下結果：\n\\[\n\\begin{split}\n\\hat{Y}_7 & = b_0 + b_1 X_{1,7} + b_2 X_{2,7} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 0) \\\\\n& = b_0 + b_1\n\\end{split}\n\\]\n因為這位學生參加了課程，預計成績等於截距項 b0 加上與 attend 變量相關的係數 \\(b_1\\)。所以，如果 \\(b_1\\) 大於零，我們預期參加課程的學生將比那些沒有參加的學生獲得更高的成績。如果這個係數為負，我們則預期相反：上課的學生表現會更差。實際上，我們可以更進一步。第一位學生呢？他參加了課程（\\(X_{1,1} = 1\\)），並且閱讀了教科書（\\(X_{2,1} = 1\\)）？如果我們將這些數字插入迴歸，我們得到：\n\\[\n\\begin{split}\n\\hat{Y}_1 & = b_0 + b_1 X_{1,1} + b_2 X_{2,1} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 1) \\\\\n& = b_0 + b_1 + b_2\n\\end{split}\n\\]\n因此，如果我們假設參加課程有助於獲得好成績（即 \\(b1 \\&gt; 0\\)），並假設閱讀教科書也有助於獲得好成績（即 \\(b2 \\&gt; 0\\)），那麼我們的預期是，第 1 位學生將比第 6 位學生和第 7 位學生獲得更高的成績。\n此時，你可能一點也不會感到驚訝地了解到迴歸模型預測，讀了書但沒有參加課程的第 3 位學生將獲得 \\(b_{2} + b_{0}\\) 的成績。我不會再用另一個迴歸公式來煩悶你。相反，我將向你展示的是帶有預期成績的 表格 14.9。\n\n\n\n\n\n\n表格 14.9: 迴歸模型的預期成績\n\n\n\n\nread textbook\n\n\n\n\n\nno\nyes\n\n\nattended?\nno\n\\( \\beta_0 \\)\n\\( \\beta_0 + \\beta_2 \\)\n\n\n\nyes\n\\( \\beta_0 + \\beta_1 \\)\n\\( \\beta_0 + \\beta_1 + \\beta_2 \\)\n\n\n\n\n\n\n\n\n正如你所看到的，截距項 \\(b_0\\) 作為一種基線成績，用來表示那些沒有花時間參加課程或閱讀教科書的學生所期望的成績。同樣，\\(b_1\\) 表示你預期能從上課中得到的提升，而 \\(b_2\\) 表示閱讀教科書帶來的提升。事實上，如果這是一個 ANOVA，你可能很想將 b1 稱為出席的主要效應，將 \\(b_2\\) 稱為閱讀的主要效應！事實上，對於一個簡單的 \\(2 \\times 2\\) ANOVA，情況確實是這樣。\n好的，既然我們已經開始看到為什麼 ANOVA 和迴歸基本上是同一回事，讓我們實際運用 rtfm 數據和 jamovi 迴歸分析來確信這確實是真的。以通常的方式運行迴歸會得到 图 14.18 中顯示的結果。\n\n\n\n\n\n\n图 14.18: 數據集rtfm.csv 在 jamovi 中的迴歸分析，無交互作用項\n\n\n\n\n這裡有幾個有趣的地方需要注意。首先，注意截距項是 43.5，接近觀察到的那兩個既沒有閱讀文本也沒有參加課程的學生的 “組” 平均值 42.5。其次，注意我們得到了參加變量的迴歸係數 \\(b_1 = 18.0\\)，這表明參加課程的學生比沒有參加課程的學生高出 18%。因此，我們的期望是，那些上課但沒有閱讀教科書的學生將獲得 \\(b_0 + b_1\\) 的成績，即 \\(43.5 + 18.0 = 61.5\\)。當我們觀察閱讀教科書的學生時，你可以自己驗證同樣的事情。\n實際上，我們可以在建立 ANOVA 和迴歸等價性方面進一步推進。看看迴歸輸出中與 attend 變量和 reading 變量相關的 p 值。它們與我們之前在運行 ANOVA 時遇到的完全相同。這可能看起來有點奇怪，因為運行我們的迴歸模型時使用的檢驗計算了一個 t 統計量，而 ANOVA 計算了一個 F 統計量。然而，如果您還記得我們在 章节 7 中提到的內容，t 分布和 F 分布之間存在著某種關係。如果你有一個根據 k 自由度的 t 分布的數量，然後將其平方，那麼這個新的平方數量就遵循一個自由度為 1 和 k 的 F 分布。對於我們迴歸模型中的 t 統計量，我們可以檢查這一點。對於 attend 變量，我們得到一個 t 值為 4.65。如果我們將這個數字平方，我們最終得到的是 21.6，這與我們 ANOVA 中相應的 F 統計量相匹配。\n最後，你還應該知道一件事。因為 jamovi 瞭解到 ANOVA 和迴歸都是線性模型的例子，所以它允許您使用 ‘Linear Regression’ - ‘Model Coefficients’ - ‘Omnibus Test’ - ‘ANOVA Test’ 從迴歸模型中提取經典的 ANOVA 表，這將給你在 图 14.19 中顯示的表格。\n\n\n\n\n\n\n\n图 14.19: jamovi迴歸分析的Omnibus ANOVA Test結果\n\n\n\n\n\n\n14.6.3 比較因子間平均值的編碼\n\n在這一點上，我已經向您展示了如何將 \\(2 \\times 2\\) ANOVA 轉換為線性模型。從而很容易看出這如何擴展到 \\(2 \\times 2 \\times 2\\) ANOVA 或 \\(2 \\times 2 \\times 2 \\times 2\\) ANOVA。事實上，這是同一件事。對於每個因子，你只需添加一個新的二元變量。當我們考慮具有多於兩個級別的因子時，問題變得更加複雜。例如，考慮我們在本章前面使用clinicaltrial.csv 數據運行的 \\(3 \\times 2\\) ANOVA。我們如何將具有三個級別的藥物因子轉換為適合迴歸的數值形式？\n事實上，這個問題的答案相當簡單。我們所要做的就是意識到三級因子可以被重新描述為兩個二元變量。假設，例如，我要創建一個名為 druganxifree 的新二元變量。每當藥物變量等於 “anxifree” 時，我們將 druganxifree 設為 1。否則，將 druganxifree 設為 0。這個變量設立了一個對比，在這種情況下是在 anxifree 和其他兩種藥物之間。當然，僅憑 druganxifree 對比還不足以完全捕捉我們藥物變量中的所有信息。我們需要第二個對比，一個能讓我們區分 joyzepam 和安慰劑的對比。為此，我們可以創建第二個二元對比，名為 drugjoyzepam，如果藥物是 joyzepam，則等於 1，否則等於 0。這兩個對比結合在一起，使我們能夠完美區分所有三種可能的藥物。表格 14.10 說明了這一點。\n\n\n\n\n\n表格 14.10: 二元對比以區分所有三種可能的藥物\n\n\ndrug\ndruganxifree\ndrugjoyzepam\n\n\n\"placebo\"\n0\n0\n\n\n\"anxifree\"\n1\n0\n\n\n\"joyzepam\"\n0\n1\n\n\n\n\n\n\n\n\n如果給病人用的藥物是安慰劑，那麼這兩個對比變量都將等於 0。如果藥物是 Anxifree，那麼 druganxifree 變量將等於 1，而 drugjoyzepam 將為 0。對於 Joyzepam，情況剛好相反：drugjoyzepam 為 1，而 druganxifree 為 0。\n使用 jamovi 計算新變量命令創建對比變量並不困難。例如，要創建 druganxifree 變量，請在計算新變量公式框中編寫此邏輯表達式：IF(drug == ‘anxifree’, 1, 0)‘。同樣，要創建新變量 drugjoyzepam，請使用此邏輯表達式：IF(drug == ’joyzepam’, 1, 0)。對於 CBTtherapy，請使用：IF(therapy == ‘CBT’, 1, 0)。您可以在 jamovi 數據文件 clinicaltrial2.omv 中查看這些新變量和相應的邏輯表達式。\n我們現在已經將三級因子根據兩個二元變量進行了重新編碼，我們已經看到，對於二元變量，ANOVA 和迴歸的行為方式是相同的。然而，在這種情況下，還有一些額外的複雜性，我們將在下一節中討論。\n\n\n\n14.6.4 變異數分析與非二元因子迴歸分析的等價性\n現在，我們有兩個不同版本的相同數據集。我們的原始數據中，clinicaltrial.csv 文件中的藥物變量表示為單個三級因子，而在擴展數據 clinicaltrial2.omv 中，它擴展為兩個二元對比。再次，我們想要證明的是，我們原來的 \\(3 \\times 2\\) 因子 ANOVA 等同於應用於對比變量的迴歸模型。讓我們首先重新執行 ANOVA，結果顯示在 图 14.20。\n\n\n\n\n\n\n图 14.20: jamovi ANOVA 結果，無交互組件\n\n\n\n\n顯然，這裡沒有什麼驚喜。這正是我們之前執行的相同 ANOVA。接下來，讓我們使用 druganxifree、drugjoyzepam 和 CBTtherapy 作為預測因子進行迴歸。結果顯示在 图 14.21。\n\n\n\n\n\n\n图 14.21: jamovi 迴歸結果，包含對比變量 druganxifree 和 drugjoyzepam\n\n\n\n\n嗯。這不是我們上次得到的相同輸出。毫不奇怪，迴歸輸出將每個預測因子的結果分別打印出來，就像我們之前進行迴歸分析的每一次一樣。一方面，我們可以看到 CBTtherapy 變量的 p 值與我們原始 ANOVA 中治療因子的 p 值完全相同，因此我們可以放心，迴歸模型與 ANOVA 做的事情相同。另一方面，這個迴歸模型分別測試 druganxifree 對比和 drugjoyzepam 對比，好像它們是兩個完全無關的變量。當然，這並不奇怪，因為可憐的迴歸分析根本無法知道 drugjoyzepam 和 druganxifree 實際上是我們用來編碼三級藥物因子的兩個不同對比。就它所知，drugjoyzepam 和 druganxifree 與 drugjoyzepam 和 therapyCBT 之間的關係沒有任何區別。然而，您和我都知道得更好。在這個階段，我們根本不感興趣確定這兩個對比是否各自具有顯著性。我們只想知道是否存在藥物的“整體”效果。也就是說，我們希望 jamovi 執行某種“模型比較”檢驗，在此檢驗中，為了檢驗的目的，將兩個“與藥物相關”的對比合併在一起。聽起來熟悉嗎？我們所需要做的就是指定我們的零假設模型，該模型將包括 CBTtherapy 預測因子，並省略所有與藥物相關的變量，如 图 14.22 所示。\n\n\n\n\n\n\n图 14.22: jamovi 迴歸中的模型比較，零模型 1 與對比模型 2\n\n\n\n\n啊，這樣好多了。我們的 F 統計量是 26.15，自由度是 2 和 14，p 值是 0.00002。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。我們再次看到，變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。這一事實的重要性不應被低估。在本章的其餘部分，我們將重點依賴這個想法。\n雖然我們在 jamovi 中計算了新變量 druganxifree 和 drugjoyzepam 進行對比，僅僅為了顯示變異數分析和迴歸本質上是相同的，在 jamovi 線性迴歸分析中實際上有一個巧妙的捷徑來獲得這些對比，見 图 14.23。jamovi 在這裡做的是允許您將因子作為預測變量輸入，等待它…因子！聰明，對吧。您還可以通過 ‘Reference Levels’ 選項指定要用作參考級別的組。我們將其分別更改為 ‘placebo’ 和 ‘no.therapy’，因為這是最有意義的。\n\n\n\n\n\n\n图 14.23: jamovi 中帶有因子和對比的迴歸分析，包括整體變異數分析檢驗結果\n\n\n\n\n如果您還在 ‘Model Coefficients’ - ‘Omnibus Test’ 選項下勾選 ‘ANOVA’ 檢驗復選框，我們會看到 F 統計量為 26.15，自由度為 2 和 14，p 值為 0.00002（图 14.23）。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。再次，我們看到變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。\n\n\n\n14.6.5 自由度就是計算有多少參數\n經過漫長的等待，我終於可以給出一個我滿意的自由度定義。自由度是根據模型中需要估計的參數數量來定義的。對於迴歸模型或變異數分析，參數數量對應於迴歸係數的數量（即 b 值），包括截距。請記住，任何 F 檢驗都始終是兩個模型之間的比較，第一個 df 是參數數量的差。例如，在上面的模型比較中，零模型（mood.gain ~ therapyCBT）有兩個參數：therapyCBT 變量的一個迴歸係數和截距的第二個參數。替代模型（mood.gain ~ druganxifree + drugjoyzepam + therapyCBT）有四個參數：三個對比中的一個迴歸係數和截距的一個參數。因此，這兩個模型之間的差的自由度是 \\(df_1 = 4 - 2 = 2\\)。\n那麼，在似乎沒有零模型的情況下呢？例如，您可能正在考慮在「線性迴歸」-「模型擬合」選項下選擇「F 檢驗」時出現的 F 檢驗。我最初將其描述為對整個迴歸模型的檢驗。然而，這仍然是兩個模型之間的比較。零模型是僅包含 1 個迴歸係數的簡單模型，用於截距項。替代模型包含 \\(K + 1\\) 個迴歸係數，每個 K 個預測變量一個，再加上截距。因此，您在此 F 檢驗中看到的 df 值等於 \\(df_1 = K + 1 - 1 = K\\)。\n那麼，在 F 檢驗中出現的第二個 df 值呢？這總是指與殘差相關的自由度。也可以用參數的方式來思考這個問題，但這有點反直覺。想象一下，假設整個研究的觀察值總數為 N。如果您想完美地描述這些 N 個值，您需要使用… N 個數字。當您建立迴歸模型時，您實際上在指定一些數字需要完美地描述數據。如果您的模型有 \\(K\\) 個預測變量和一個截距，那麼您已經指定了 \\(K + 1\\) 個數字。那麼，無需確定這將如何完成，您認為還需要多少數字才能將 K 个 1 參數迴歸模型轉換為原始數據的完美描述呢？如果您發現自己在想 \\((K + 1) + (N - K - 1) = N\\)，因此答案必須是 \\(N - K - 1\\)，那就做得很好！這正是對的。原則上，您可以想像一個包含每個數據點的參數的極其複雜的迴歸模型，它當然可以完美地描述數據。這個模型總共包含了 \\(N\\) 個參數，但是我們感興趣的是描述這個完整模型（即 \\(N\\)）所需的參數數量與您實際感興趣的更簡單的迴歸模型所使用的參數數量（即 \\(K + 1\\)）之間的差別，因此 F 檢驗中的第二個自由度是 \\(df_2 = N - K - 1\\)，其中 K 是預測變量的數量（在迴歸模型中）或對比的數量（在變異數分析中）。在我上面給出的示例中，數據集中有 \\((N = 18\\) 觀察值，並且與變異數分析模型相關的 \\(K + 1 = 4\\) 個迴歸係數，因此殘差的自由度是 \\(df_2 = 18 - 4 = 14\\)。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#different-ways-to-specify-contrasts",
    "href": "14-Factorial-ANOVA.html#different-ways-to-specify-contrasts",
    "title": "14  多因子變異數分析",
    "section": "14.7 Different ways to specify contrasts",
    "text": "14.7 Different ways to specify contrasts\nIn the previous section, I showed you a method for converting a factor into a collection of contrasts. In the method I showed you we specify a set of binary variables in which we defined a table like Table 14.11.\n\n\n\n\nTable 14.11:  Binary contrasts to discriminate between all three possible drugs \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\nEach row in the table corresponds to one of the factor levels, and each column corresponds to one of the contrasts. This table, which always has one more row than columns, has a special name. It is called a contrast matrix. However, there are lots of different ways to specify a contrast matrix. In this section I discuss a few of the standard contrast matrices that statisticians use and how you can use them in jamovi. If you’re planning to read the section on Factorial ANOVA 3: unbalanced designs later on, it’s worth reading this section carefully. If not, you can get away with skimming it, because the choice of contrasts doesn’t matter much for balanced designs.\n\n14.7.1 Treatment contrasts\nIn the particular kind of contrasts that I’ve described above, one level of the factor is special, and acts as a kind of “baseline” category (i.e., placebo in our example), against which the other two are defined. The name for these kinds of contrasts is treatment contrasts, also known as “dummy coding”. In this contrast each level of the factor is compared to a base reference level, and the base reference level is the value of the intercept.\nThe name reflects the fact that these contrasts are quite natural and sensible when one of the categories in your factor really is special because it actually does represent a baseline. That makes sense in our clinical trial example. The placebo condition corresponds to the situation where you don’t give people any real drugs, and so it’s special. The other two conditions are defined in relation to the placebo. In one case you replace the placebo with Anxifree, and in the other case your replace it with Joyzepam.\nThe table shown above is a matrix of treatment contrasts for a factor that has 3 levels. But suppose I want a matrix of treatment contrasts for a factor with 5 levels? You would set this out like Table 14.12.\n\n\n\n\nTable 14.12:  Matrix of treatment contrasts with 5 levels \n\nLevel2345\n\n10000\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nIn this example, the first contrast is level 2 compared with level 1, the second contrast is level 3 compared with level 1, and so on. Notice that, by default, the first level of the factor is always treated as the baseline category (i.e., it’s the one that has all zeros and doesn’t have an explicit contrast associated with it). In jamovi you can change which category is the first level of the factor by manipulating the order of the levels of the variable shown in the ‘Data Variable’ window (double click on the name of the variable in the spreadsheet column to bring up the ‘Data Variable’ view.\n\n\n14.7.2 Helmert contrasts\nTreatment contrasts are useful for a lot of situations. However, they make most sense in the situation when there really is a baseline category, and you want to assess all the other groups in relation to that one. In other situations, however, no such baseline category exists, and it may make more sense to compare each group to the mean of the other groups. This is where we meet Helmert contrasts, generated by the ‘helmert’ option in the jamovi ‘ANOVA’ - ‘Contrasts’ selection box. The idea behind Helmert contrasts is to compare each group to the mean of the “previous” ones. That is, the first contrast represents the difference between group 2 and group 1, the second contrast represents the difference between group 3 and the mean of groups 1 and 2, and so on. This translates to a contrast matrix that looks like Table 14.13 for a factor with five levels.\n\n\n\n\nTable 14.13:  Matrix of helmert contrasts with 5 levels \n\n1-1-1-1-1\n\n21-1-1-1\n\n302-1-1\n\n4003-1\n\n50004\n\n\n\n\n\nOne useful thing about Helmert contrasts is that every contrast sums to zero (i.e., all the columns sum to zero). This has the consequence that, when we interpret the ANOVA as a regression, the intercept term corresponds to the grand mean \\(\\mu_{..}\\) if we are using Helmert contrasts. Compare this to treatment contrasts, in which the intercept term corresponds to the group mean for the baseline category. This property can be very useful in some situations. It doesn’t matter very much if you have a balanced design, which we’ve been assuming so far, but it will turn out to be important later when we consider unbalanced designs. In fact, the main reason why I’ve even bothered to include this section is that contrasts become important if you want to understand unbalanced ANOVA.\n\n\n14.7.3 Sum to zero contrasts\nThe third option that I should briefly mention are “sum to zero” contrasts, called “Simple” contrasts in jamovi, which are used to construct pairwise comparisons between groups. Specifically, each contrast encodes the difference between one of the groups and a baseline category, which in this case corresponds to the first group (Table 14.14).\n\n\n\n\nTable 14.14:  Matrix of ’sum-to’zero contrasts with 5 levels \n\n1-1-1-1-1\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\nMuch like Helmert contrasts, we see that each column sums to zero, which means that the intercept term corresponds to the grand mean when ANOVA is treated as a regression model. When interpreting these contrasts, the thing to recognise is that each of these contrasts is a pairwise comparison between group 1 and one of the other four groups. Specifically, contrast 1 corresponds to a “group 2 minus group 1” comparison, contrast 2 corresponds to a “group 3 minus group 1” comparison, and so on.8\n\n\n14.7.4 Optional contrasts in jamovi\njamovi also comes with a variety of options that can generate different kinds of contrasts in ANOVA. These can be found in the ‘Contrasts’ option in the main ANOVA analysis window, where the contrast types in Table 14.15 are listed:\n\n\n\n\nTable 14.15:  Contrasts types available in the jamovi ANOVA analysis \n\nContrast type\n\nDeviationCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\nSimpleLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\nDifferenceCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\nHelmertCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\nRepeatedCompares the mean of each level (except the last) to the mean of the subsequent level\n\nPolynomialCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "href": "14-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "title": "14  多因子變異數分析",
    "section": "14.8 事後檢定",
    "text": "14.8 事後檢定\n現在轉換到另一個主題。而不是使用對比測試您已經計劃好的比較，假設您已經完成了 ANOVA，結果發現您獲得了一些顯著的效果。因為 F 檢驗是“全面”檢驗，它們實際上只測試各組之間沒有差異的虛無假設，所以獲得顯著效果並不能告訴你哪些組與其他組有所不同。我們在 章节 13 中討論了這個問題，並且在那章節中，我們的解決方案是對所有可能的組對執行 t 檢驗，對多重比較（例如，Bonferroni、Holm）進行校正，以控制所有比較中的 I 類型誤差率。我們在 章节 13 中使用的方法具有相對簡單的優點，並且可以在您在測試多個假設的多種不同情況下使用，但它們並非在 ANOVA 背景下進行有效的事後檢驗的最佳選擇。統計文獻中有很多用於執行多重比較的方法(Hsu, 1996)，本書將超出範疇，無法詳細討論所有這些方法。\n話雖如此，有一個工具我想引起您的注意，那就是 Tukey 的 “誠實顯著差異”，簡稱Tukey’s HSD。這次，我將不提供公式，只講解質性思路。Tukey’s HSD 的基本思想是檢查所有相關的組之間的成對比較，而且只有在您對成對差異感興趣時，使用 Tukey’s HSD 才合適。8 例如，前面我們使用 clinicaltrial.csv 數據集進行了因子 ANOVA，並指定了藥物的主要作用和治療的主要作用，我們對以下四種比較感興趣：\n\n給予 Anxifree 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Joyzepam 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Anxifree 的人與給予 Joyzepam 的人的情緒提升之間的差異。\n接受 CBT 治療的人與未接受治療的人的情緒提升之間的差異。\n\n對於這些比較中的任何一個，我們都對（群體）組平均值之間的真實差異感興趣。Tukey 的 HSD 會為這四種比較構建同時置信區間。我們所說的 95% “同時”置信區間是指，如果我們重複這個研究很多次，那麼在 95% 的研究結果中，置信區間將包含相關的真實值。此外，我們可以使用這些置信區間計算任何特定比較的校正 p 值。\n在 jamovi 中使用 TukeyHSD 函數非常容易。您只需指定要為其執行事後檢驗的 ANOVA 模型項。例如，如果我們要為主效應進行事後檢驗，但不考慮交互作用，我們將在 ANOVA 分析屏幕中打開“事後檢驗”選項，將藥物和治療變量移到右側的框中，然後在可能的事後校正列表中選中“Tukey”複選框。這與相應的結果表在 图 14.24 中顯示。\n\n\n\n\n\n\n\n图 14.24: 不帶交互作用的 jamovi 因子 ANOVA 中的 Tukey HSD 事後檢驗\n\n\n\n\n在「事後檢驗」結果表中顯示的輸出非常直觀。例如，第一個比較是Anxifree與安慰劑之間的差異，輸出的第一部分顯示組均值之間的觀察差異為0.27。接下來的數字是差異的標準誤，如果我們想要，我們可以根據此計算出95%置信區間，儘管jamovi目前尚不提供此選項。然後有一列是自由度，一列是t值，最後一列是p值。對於第一個比較，調整後的p值為0.21。相比之下，如果您看下一行，我們會發現安慰劑和Joyzepam之間的觀察差異為1.03，並且這個結果顯著（p &lt; .001）。\n到目前為止，一切都很好。那麼，如果您的模型包括交互作用項怎麼辦？例如，jamovi中的默認選項是允許藥物和療法之間存在交互作用。如果是這樣，我們需要考慮的兩兩比較數量開始增加。像以前一樣，我們需要考慮與藥物主效應相關的三個比較以及與療法主效應相關的一個比較。但是，如果我們要考慮顯著交互作用的可能性（並嘗試找到支持這一顯著交互作用的組差異），我們需要包括以下比較：\n\n使用Anxifree並接受CBT治療的人的情緒增益與使用安慰劑並接受CBT治療的人的情緒增益之間的差異\n使用Anxifree並不接受治療的人與使用安慰劑並不接受治療的人的情緒增益之間的差異。\n等等\n\n您需要考慮相當多的這些比較。因此，當我們對此ANOVA模型運行Tukey事後分析時，我們會發現它進行了很多兩兩比較（共19個），如@fig-fig13-25所示。您會發現它看起來與之前非常相似，但進行了更多的比較。\n\n\n\n\n\n\n\n图 14.25: jamovi因子ANOVA中的Tukey HSD事後檢驗，包含交互作用項"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "href": "14-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "title": "14  多因子變異數分析",
    "section": "14.9 事前檢定方法",
    "text": "14.9 事前檢定方法\n延續前面關於ANOVA中對比和事後檢驗的部分，我認為預先設定的比較方法（planned comparisons）非常重要，值得簡要討論。在前面的章節以及@sec-Comparing-several-means-one-way-ANOVA中對多重比較的討論中，我們假設你想運行的檢驗確實是事後檢驗。例如，在上面的藥物示例中，可能你認為這些藥物對情緒的影響各有不同（即你假設藥物有主效應），但你沒有關於它們如何不同的具體假設，也沒有任何真正的想法關於哪些兩兩比較值得觀察。如果是這樣，那麼你確實需要使用Tukey的HSD來進行兩兩比較。\n然而，情況會有所不同，如果你真的有關於哪些比較感興趣的確切、具體的假設，而且你絕對無意觀察除了提前指定的那些比較以外的任何其他比較。當這是真的，並且如果你真誠並嚴格地堅持不進行任何其他比較的高尚意圖（即使數據看起來似乎對你沒有假設的東西顯示出非常顯著的效應），那麼使用像Tukey的HSD這樣的方法並不合理，因為它對一整套你從未關心過，也從未打算觀察的比較進行了矯正。在這種情況下，你可以安全地運行有限數量的假設檢驗，而無需對多重檢驗進行調整。這種情況被稱為預先設定的比較方法，有時用於臨床試驗。然而，進一步的考慮超出了這本入門書的範疇，但至少你知道這種方法是存在的！"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#factorial-anova-3-unbalanced-designs",
    "href": "14-Factorial-ANOVA.html#factorial-anova-3-unbalanced-designs",
    "title": "14  多因子變異數分析",
    "section": "14.10 Factorial ANOVA 3: unbalanced designs",
    "text": "14.10 Factorial ANOVA 3: unbalanced designs\nFactorial ANOVA is a very handy thing to know about. It’s been one of the standard tools used to analyse experimental data for many decades, and you’ll find that you can’t read more than two or three papers in psychology without running into an ANOVA in there somewhere. However, there’s one huge difference between the ANOVAs that you’ll see in a lot of real scientific articles and the ANOVAs that I’ve described so far. In in real life we’re rarely lucky enough to have perfectly balanced designs. For one reason or another, it’s typical to end up with more observations in some cells than in others. Or, to put it another way, we have an unbalanced design.\nUnbalanced designs need to be treated with a lot more care than balanced designs, and the statistical theory that underpins them is a lot messier. It might be a consequence of this messiness, or it might be a shortage of time, but my experience has been that undergraduate research methods classes in psychology have a nasty tendency to ignore this issue completely. A lot of stats textbooks tend to gloss over it too. The net result of this, I think, is that a lot of active researchers in the field don’t actually know that there’s several different “types” of unbalanced ANOVAs, and they produce quite different answers. In fact, reading the psychological literature, I’m kind of amazed at the fact that most people who report the results of an unbalanced factorial ANOVA don’t actually give you enough details to reproduce the analysis. I secretly suspect that most people don’t even realise that their statistical software package is making a whole lot of substantive data analysis decisions on their behalf. It’s actually a little terrifying when you think about it. So, if you want to avoid handing control of your data analysis to stupid software, read on.\n\n14.10.1 The coffee data\nAs usual, it will help us to work with some data. The coffee.csv file contains a hypothetical data set that produces an unbalanced \\(3 \\times 2\\) ANOVA. Suppose we were interested in finding out whether or not the tendency of people to babble when they have too much coffee is purely an effect of the coffee itself, or whether there’s some effect of the milk and sugar that people add to the coffee. Suppose we took 18 people and gave them some coffee to drink. The amount of coffee / caffeine was held constant, and we varied whether or not milk was added, so milk is a binary factor with two levels, “yes” and “no”. We also varied the kind of sugar involved. The coffee might contain “real” sugar or it might contain “fake” sugar (i.e., artificial sweetener) or it might contain “none” at all, so the sugar variable is a three level factor. Our outcome variable is a continuous variable that presumably refers to some psychologically sensible measure of the extent to which someone is “babbling”. The details don’t really matter for our purpose. Take a look at the data in the jamovi spreadsheet view, as in Figure 14.26.\n\n\n\n\n\nFigure 14.26: The coffee.csv data set in jamovi, with descriptive information aggregated by factor levels\n\n\n\n\nLooking at the table of means in Figure 14.26 we get a strong impression that there are differences between the groups. This is especially true when we compare these means to the standard deviations for the babble variable. Across groups, this standard deviation varies from .14 to .71, which is fairly small relative to the differences in group means.10 Whilst this at first may seem like a straightforward factorial ANOVA, a problem arises when we look at how many observations we have in each group. See the different Ns for different groups shown in Figure 14.26. This violates one of our original assumptions, namely that the number of people in each group is the same. We haven’t really discussed how to handle this situation.\n\n\n14.10.2 “Standard ANOVA” does not exist for unbalanced designs\nUnbalanced designs lead us to the somewhat unsettling discovery that there isn’t really any one thing that we might refer to as a standard ANOVA. In fact, it turns out that there are three fundamentally different ways11 in which you might want to run an ANOVA in an unbalanced design. If you have a balanced design all three versions produce identical results, with the sums of squares, F-values, etc., all conforming to the formulas that I gave at the start of the chapter. However, when your design is unbalanced they don’t give the same answers. Furthermore, they are not all equally appropriate to every situation. Some methods will be more appropriate to your situation than others. Given all this, it’s important to understand what the different types of ANOVA are and how they differ from one another.\nThe first kind of ANOVA is conventionally referred to as Type I sum of squares. I’m sure you can guess what the other two are called. The “sum of squares” part of the name was introduced by the SAS statistical software package and has become standard nomenclature, but it’s a bit misleading in some ways. I think the logic for referring to them as different types of sum of squares is that, when you look at the ANOVA tables that they produce, the key difference in the numbers is the SS values. The degrees of freedom don’t change, the MS values are still defined as SS divided by df, etc. However, what the terminology gets wrong is that it hides the reason why the SS values are different from one another. To that end, it’s a lot more helpful to think of the three different kinds of ANOVA as three different hypothesis testing strategies. These different strategies lead to different SS values, to be sure, but it’s the strategy that is the important thing here, not the SS values themselves. Recall from the section ANOVA as a linear model that any particular F-test is best thought of as a comparison between two linear models. So, when you’re looking at an ANOVA table, it helps to remember that each of those F-tests corresponds to a pair of models that are being compared. Of course, this leads naturally to the question of which pair of models is being compared. This is the fundamental difference between ANOVA Types I, II and III: each one corresponds to a different way of choosing the model pairs for the tests.\n\n\n14.10.3 Type I sum of squares\nThe Type I method is sometimes referred to as the “sequential” sum of squares, because it involves a process of adding terms to the model one at a time. Consider the coffee data, for instance. Suppose we want to run the full \\(3 \\times 2\\) factorial ANOVA, including interaction terms. The full model contains the outcome variable babble, the predictor variables sugar and milk, and the interaction term sugar \\(\\times\\) milk. This can be written as \\(babble \\sim sugar + milk + sugar {\\times} milk\\). The Type I strategy builds this model up sequentially, starting from the simplest possible model and gradually adding terms.\nThe simplest possible model for the data would be one in which neither milk nor sugar is assumed to have any effect on babbling. The only term that would be included in such a model is the intercept, written as babble ~ 1. This is our initial null hypothesis. The next simplest model for the data would be one in which only one of the two main effects is included. In the coffee data, there are two different possible choices here, because we could choose to add milk first or to add sugar first. The order actually turns out to matter, as we’ll see later, but for now let’s just make a choice arbitrarily and pick sugar. So, the second model in our sequence of models is babble ~ sugar, and it forms the alternative hypothesis for our first test. We now have our first hypothesis test (Table 14.16).\n\n\n\n\nTable 14.16:  Null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim 1\\)\n\nAlternative model:\\(babble \\sim  sugar\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of sugar. The next step in our model building exercise is to add the other main effect term, so the next model in our sequence is babble ~ sugar + milk. The second hypothesis test is then formed by comparing the following pair of models (Table 14.17).\n\n\n\n\nTable 14.17:  Further null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar\\)\n\nAlternative model:\\(babble \\sim  sugar + milk\\)\n\n\n\n\n\nThis comparison forms our hypothesis test of the main effect of milk. In one sense, this approach is very elegant: the alternative hypothesis from the first test forms the null hypothesis for the second one. It is in this sense that the Type I method is strictly sequential. Every test builds directly on the results of the last one. However, in another sense it’s very inelegant, because there’s a strong asymmetry between the two tests. The test of the main effect of sugar (the first test) completely ignores milk, whereas the test of the main effect of milk (the second test) does take sugar into account. In any case, the fourth model in our sequence is now the full model, babble ~ sugar + milk + sugar \\(\\times\\) milk, and the corresponding hypothesis test is shown in Table 14.18.\n\n\n\n\nTable 14.18:  And more possible null and alternative hypotheses with the outcome variable ‘babble’ \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk + sugar * milk \\)\n\n\n\n\n\nType III sum of squares is the default hypothesis testing method used by jamovi ANOVA, so to run a Type I sum of squares analysis we have to select ‘Type 1’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options. This gives us the ANOVA table shown in Figure 14.27.\n\n\n\n\n\nFigure 14.27: ANOVA results table using Type I sum of squares in jamovi\n\n\n\n\nThe big problem with using Type I sum of squares is the fact that it really does depend on the order in which you enter the variables. Yet, in many situations the researcher has no reason to prefer one ordering over another. This is presumably the case for our milk and sugar problem. Should we add milk first or sugar first? It feels exactly as arbitrary as a data analysis question as it does as a coffee-making question. There may in fact be some people with firm opinions about ordering, but it’s hard to imagine a principled answer to the question. Yet, look what happens when we change the ordering, as in Figure 14.28.\n\n\n\n\n\nFigure 14.28: ANOVA results table using Type I sum of squares in jamovi, but with factors entered in a different order (milk first)\n\n\n\n\nThe p-values for both main effect terms have changed, and fairly dramatically. Among other things, the effect of milk has become significant (though one should avoid drawing any strong conclusions about this, as I’ve mentioned previously). Which of these two ANOVAs should one report? It’s not immediately obvious.\nWhen you look at the hypothesis tests that are used to define the “first” main effect and the “second” one, it’s clear that they’re qualitatively different from one another. In our initial example, we saw that the test for the main effect of sugar completely ignores milk, whereas the test of the main effect of milk does take sugar into account. As such, the Type I testing strategy really does treat the first main effect as if it had a kind of theoretical primacy over the second one. In my experience there is very rarely if ever any theoretically primacy of this kind that would justify treating any two main effects asymmetrically.\nThe consequence of all this is that Type I tests are very rarely of much interest, and so we should move on to discuss Type II tests and Type III tests.\n\n\n14.10.4 Type III sum of squares\nHaving just finished talking about Type I tests, you might think that the natural thing to do next would be to talk about Type II tests. However, I think it’s actually a bit more natural to discuss Type III tests (which are simple and the default in jamovi ANOVA) before talking about Type II tests (which are trickier). The basic idea behind Type III tests is extremely simple. Regardless of which term you’re trying to evaluate, run the F-test in which the alternative hypothesis corresponds to the full ANOVA model as specified by the user, and the null model just deletes that one term that you’re testing. For instance, in the coffee example, in which our full model was babble ~ sugar + milk + sugar \\(\\times\\) milk, the test for a main effect of sugar would correspond to a comparison between the following two models (Table 14.19).\n\n\n\n\nTable 14.19:  Null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  milk + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nSimilarly the main effect of milk is evaluated by testing the full model against a null model that removes the milk term, like in Table 14.20.\n\n\n\n\nTable 14.20:  Further null and alternative hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nFinally, the interaction term sugar \\(\\times\\) milk is evaluated in exactly the same way. Once again, we test the full model against a null model that removes the sugar \\(\\times\\) milk interaction term, like in Table 14.21.\n\n\n\n\nTable 14.21:  Removing the interaction term from hypotheses with the outcome variable ‘babble’, with Type III sum of squares \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\nThe basic idea generalises to higher order ANOVAs. For instance, suppose that we were trying to run an ANOVA with three factors, A, B and C, and we wanted to consider all possible main effects and all possible interactions, including the three way interaction A \\(\\times\\) B \\(\\times\\) C. (Table 14.22) shows you what the Type III tests look like for this situation).\n\n\n\n\nTable 14.22:  Type III tests with three factors and all main effect and interaction term \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB\\(A + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\nC\\(A + B + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B\\(A + B + C + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*C\\(A + B + C + A*B + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB*C\\(A + B + C + A*B + A*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nAs ugly as that table looks, it’s pretty simple. In all cases, the alternative hypothesis corresponds to the full model which contains three main effect terms (e.g. A), three two-way interactions (e.g. A*B) and one three-way interaction (i.e., A*B*C)). The null model always contains 6 of these 7 terms, and the missing one is the one whose significance we’re trying to test.\nAt first pass, Type III tests seem like a nice idea. Firstly, we’ve removed the asymmetry that caused us to have problems when running Type I tests. And because we’re now treating all terms the same way, the results of the hypothesis tests do not depend on the order in which we specify them. This is definitely a good thing. However, there is a big problem when interpreting the results of the tests, especially for main effect terms. Consider the coffee data. Suppose it turns out that the main effect of milk is not significant according to the Type III tests. What this is telling us is that babble ~ sugar + sugar*milk is a better model for the data than the full model. But what does that even mean? If the interaction term sugar*milk was also non significant, we’d be tempted to conclude that the data are telling us that the only thing that matters is sugar. But suppose we have a significant interaction term, but a non-significant main effect of milk. In this case, are we to assume that there really is an “effect of sugar”, an “interaction between milk and sugar”, but no “effect of milk”? That seems crazy. The right answer simply must be that it’s meaningless12 to talk about the main effect if the interaction is significant. In general, this seems to be what most statisticians advise us to do, and I think that’s the right advice. But if it really is meaningless to talk about non-significant main effects in the presence of a significant interaction, then it’s not at all obvious why Type III tests should allow the null hypothesis to rely on a model that includes the interaction but omits one of the main effects that make it up. When characterised in this fashion, the null hypotheses really don’t make much sense at all.\nLater on, we’ll see that Type III tests can be redeemed in some contexts, but first let’s take a look at the ANOVA results table using Type III sum of squares, see Figure 14.29.\n\n\n\n\n\nFigure 14.29: ANOVA results table using Type III sum of squares in jamovi\n\n\n\n\nBut be aware, one of the perverse features of the Type III testing strategy is that typically the results turn out to depend on the contrasts that you use to encode your factors (see the Different ways to specify contrasts section if you’ve forgotten what the different types of contrasts are).13\nOkay, so if the p-values that typically come out of Type III analyses (but not in jamovi) are so sensitive to the choice of contrasts, does that mean that Type III tests are essentially arbitrary and not to be trusted? To some extent that’s true, and when we turn to a discussion of Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\nIn Type II tests we’ll see that Type II analyses avoid this arbitrariness entirely, but I think that’s too strong a conclusion. Firstly, it’s important to recognise that some choices of contrasts will always produce the same answers (ah, so this is what is happening in jamovi). Of particular importance is the fact that if the columns of our contrast matrix are all constrained to sum to zero, then the Type III analysis will always give the same answers.\n\n\n14.10.5 Type II sum of squares\nOkay, so we’ve seen Type I and III tests now, and both are pretty straightforward. Type I tests are performed by gradually adding terms one at a time, whereas Type III tests are performed by taking the full model and looking to see what happens when you remove each term. However, both can have some limitations. Type I tests are dependent on the order in which you enter the terms, and Type III tests are dependent on how you code up your contrasts. Type II tests are a little harder to describe, but they avoid both of these problems, and as a result they are a little easier to interpret.\nType II tests are broadly similar to Type III tests. Start with a “full” model, and test a particular term by deleting it from that model. However, Type II tests are based on the marginality principle which states that you should not omit a lower order term from your model if there are any higher order ones that depend on it. So, for instance, if your model contains the two-way interaction A \\(\\times\\) B (a 2nd order term), then it really ought to contain the main effects A and B (1st order terms). Similarly, if it contains a three-way interaction term A \\(\\times\\) B \\(\\times\\) C, then the model must also include the main effects A, B and C as well as the simpler interactions A \\(\\times\\) B, A \\(\\times\\) C and B \\(\\times\\) C. Type III tests routinely violate the marginality principle. For instance, consider the test of the main effect of A in the context of a three-way ANOVA that includes all possible interaction terms. According to Type III tests, our null and alternative models are in Table 14.23.\n\n\n\n\nTable 14.23:  Type III tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\nNotice that the null hypothesis omits A, but includes A \\(\\times\\) B, A \\(\\times\\) C and A \\(\\times\\) B \\(\\times\\) C as part of the model. This, according to the Type II tests, is not a good choice of null hypothesis. What we should do instead, if we want to test the null hypothesis that A is not relevant to our outcome, is to specify the null hypothesis that is the most complicated model that does not rely on A in any form, even as an interaction. The alternative hypothesis corresponds to this null model plus a main effect term of A. This is a lot closer to what most people would intuitively think of as a “main effect of A”, and it yields the following as our Type II test of the main effect of A (Table 14.24). 14\n\n\n\n\nTable 14.24:  Type II tests for a main effect, A, in a three-way ANOVA with all possible interaction terms \n\nNull model:\\(outcome \\sim B + C + B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\nAnyway, just to give you a sense of how the Type II tests play out, here’s the full table (Table 14.25) of tests that would be applied in a three-way factorial ANOVA:\n\n\n\n\nTable 14.25:  Type II tests for a three-way factorial model \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + B*C \\)\\(A + B + C + B*C \\)\n\nB\\(A + C + A*C \\)\\(A + B + C + A*C\\)\n\nC\\(A + B + A*B \\)\\(A + B + C + A*B\\)\n\nA*B\\(A + B + C + A*C + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*C\\(A + B + C + A*B + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nB*C\\(A + B + C + A*B + A*C \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\nIn the context of the two way ANOVA that we’ve been using in the coffee data, the hypothesis tests are even simpler. The main effect of sugar corresponds to an F-test comparing these two models (Table 14.26).\n\n\n\n\nTable 14.26:  Type II tests for the main effect of sugar in the coffee data \n\nNull model:\\(babble \\sim milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nThe test for the main effect of milk is in Table 14.27.\n\n\n\n\nTable 14.27:  Type II tests for the main effect of milk in the coffee data \n\nNull model:\\(babble \\sim  sugar \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\nFinally, the test for the interaction sugar \\(\\times\\) milk is in Table 14.28.\n\n\n\n\nTable 14.28:  Type II tests for the sugar x milk interaction term \n\nNull model:\\(babble \\sim  sugar + milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk  + sugar*milk \\)\n\n\n\n\n\nRunning the tests are again straightforward. Just select ‘Type 2’ in the ‘Sum of squares’ selection box in the jamovi ‘ANOVA’ - ‘Model’ options, This gives us the ANOVA table shown in Figure 14.30.\n\n\n\n\n\nFigure 14.30: ANOVA results table using Type II sum of squares in jamovi\n\n\n\n\nType II tests have some clear advantages over Type I and Type III tests. They don’t depend on the order in which you specify factors (unlike Type I), and they don’t depend on the contrasts that you use to specify your factors (unlike Type III). And although opinions may differ on this last point, and it will definitely depend on what you’re trying to do with your data, I do think that the hypothesis tests that they specify are more likely to correspond to something that you actually care about. As a consequence, I find that it’s usually easier to interpret the results of a Type II test than the results of a Type I or Type III test. For this reason my tentative advice is that, if you can’t think of any obvious model comparisons that directly map onto your research questions but you still want to run an ANOVA in an unbalanced design, Type II tests are probably a better choice than Type I or Type III.15\n\n\n14.10.6 Effect sizes (and non-additive sums of squares)\njamovi also provides the effect sizes \\(\\eta^2\\) and partial \\(\\eta^2\\) when you select these options, as in Figure 14.30. However, when you’ve got an unbalanced design there’s a bit of extra complexity involved.\nIf you remember back to our very early discussions of ANOVA, one of the key ideas behind the sums of squares calculations is that if we add up all the SS terms associated with the effects in the model, and add that to the residual SS, they’re supposed to add up to the total sum of squares. And, on top of that, the whole idea behind \\(\\eta^2\\) is that, because you’re dividing one of the SS terms by the total SS value, an \\(\\eta^2\\) value can be interpreted as the proportion of variance accounted for by a particular term. But this is not so straightforward in unbalanced designs because some of the variance goes “missing”.\nThis seems a bit odd at first, but here’s why. When you have unbalanced designs your factors become correlated with one another, and it becomes difficult to tell the difference between the effect of Factor A and the effect of Factor B. In the extreme case, suppose that we’d run a \\(2 \\times 2\\) design in which the number of participants in each group had been as in Table 14.29.\n\n\n\n\nTable 14.29:  N participants in a 2 x 2 very (very!) unbalanced factorial design \n\nsugarno sugar\n\nmilk1000\n\nno milk0100\n\n\n\n\n\nHere we have a spectacularly unbalanced design: 100 people have milk and sugar, 100 people have no milk and no sugar, and that’s all. There are 0 people with milk and no sugar, and 0 people with sugar but no milk. Now suppose that, when we collected the data, it turned out there is a large (and statistically significant) difference between the “milk and sugar” group and the “no-milk and no-sugar” group. Is this a main effect of sugar? A main effect of milk? Or an interaction? It’s impossible to tell, because the presence of sugar has a perfect association with the presence of milk. Now suppose the design had been a little more balanced (Table 14.30).\n\n\n\n\nTable 14.30:  N participants in a 2 x 2 still very unbalanced factorial design \n\nsugarno sugar\n\nmilk1005\n\nno milk5100\n\n\n\n\n\nThis time around, it’s technically possible to distinguish between the effect of milk and the effect of sugar, because we have a few people that have one but not the other. However, it will still be pretty difficult to do so, because the association between sugar and milk is still extremely strong, and there are so few observations in two of the groups. Again, we’re very likely to be in the situation where we know that the predictor variables (milk and sugar) are related to the outcome (babbling), but we don’t know if the nature of that relationship is a main effect of one or the other predictor, or the interaction."
  },
  {
    "objectID": "14-Factorial-ANOVA.html#summary",
    "href": "14-Factorial-ANOVA.html#summary",
    "title": "14  多因子變異數分析",
    "section": "14.11 Summary",
    "text": "14.11 Summary\n\nFactorial ANOVA 1: balanced designs, no interactions and with interactions included\nEffect size, estimated means, and confidence intervals in a factorial ANOVA\nAssumption checking in ANOVA\nAnalysis of Covariance (ANCOVA)\nUnderstanding ANOVA as a linear model, including Different ways to specify contrasts\nPost hoc tests using Tukey’s HSD and a brief commentary on The method of planned comparisons\nFactorial ANOVA 3: unbalanced designs\n\n\n\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A second-level course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall."
  },
  {
    "objectID": "15-Factor-Analysis.html#exploratory-factor-analysis",
    "href": "15-Factor-Analysis.html#exploratory-factor-analysis",
    "title": "15  因素分析",
    "section": "15.1 Exploratory Factor Analysis",
    "text": "15.1 Exploratory Factor Analysis\nExploratory Factor Analysis (EFA) is a statistical technique for revealing any hidden latent factors that can be inferred from our observed data. This technique calculates to what extent a set of measured variables, for example \\(V1, V2, V3, V4\\), and \\(V5\\), can be represented as measures of an underlying latent factor. This latent factor cannot be measured through just one observed variable but instead is manifested in the relationships it causes in a set of observed variables.\nIn Figure 15.1 each observed variable \\(V\\) is ‘caused’ to some extent by the underlying latent factor (\\(F\\)), depicted by the coefficients \\(b_1\\) to \\(b_5\\) (also called factor loadings). Each observed variable also has an associated error term, e1 to e5. Each error term is the variance in the associated observed variable, \\(V_i\\) , that is unexplained by the underlying latent factor.\n\n\n\n\n\nFigure 15.1: Latent factor underlying the relationship between several observed variables\n\n\n\n\nIn Psychology, latent factors represent psychological phenomena or constructs that are difficult to directly observe or measure. For example, personality, or intelligence, or thinking style. In the example in Figure 15.1 we may have asked people five specific questions about their behaviour or attitudes, and from that we are able to get a picture about a personality construct called, for example, extraversion. A different set of specific questions may give us a picture about an individual’s introversion, or their conscientiousness.\nHere’s another example: we may not be able to directly measure statistics anxiety, but we can measure whether statistics anxiety is high or low with a set of questions in a questionnaire. For example, “\\(Q1\\): Doing the assignment for a statistics course”, “\\(Q2\\): Trying to understand the statistics described in a journal article”, and “\\(Q3\\): Asking the lecturer for help in understanding something from the course”, etc., each rated from low anxiety to high anxiety. People with high statistics anxiety will tend to give similarly high responses on these observed variables because of their high statistics anxiety. Likewise, people with low statistics anxiety will give similar low responses to these variables because of their low statistics anxiety.\nIn exploratory factor analysis (EFA), we are essentially exploring the correlations between observed variables to uncover any interesting, important underlying (latent) factors that are identified when observed variables co-vary. We can use statistical software to estimate any latent factors and to identify which of our variables have a high loading1 (e.g. loading > 0.5) on each factor, suggesting they are a useful measure, or indicator, of the latent factor. Part of this process includes a step called rotation, which to be honest is a pretty weird idea but luckily we don’t have to worry about understanding it; we just need to know that it is helpful because it makes the pattern of loadings on different factors much clearer. As such, rotation helps with seeing more clearly which variables are linked substantively to each factor. We also need to decide how many factors are reasonable given our data, and helpful in this regard is something called Eigen values. We’ll come back to this in a moment, after we have covered some of the main assumptions of EFA.\n\n15.1.1 Checking assumptions\nThere are a couple of assumptions that need to be checked as part of the analysis. The first assumption is sphericity, which essentially checks that the variables in your dataset are correlated with each other to the extent that they can potentially be summarised with a smaller set of factors. Bartlett’s test for sphericity checks whether the observed correlation matrix diverges significantly from a zero (or null) correlation matrix. So, if Bartlett’s test is significant (\\(p < .05\\)), this indicates that the observed correlation matrix is significantly divergent from the null, and is therefore suitable for EFA.\nThe second assumption is sampling adequacy and is checked using the Kaiser-MeyerOlkin (KMO) Measure of Sampling Adequacy (MSA). The KMO index is a measure of the proportion of variance among observed variables that might be common variance. Using partial correlations, it checks for factors that load just two items. We seldom, if ever, want EFA producing a lot of factors loading just two items each. KMO is about sampling adequacy because partial correlations are typically seen with inadequate samples. If the KMO index is high (\\(\\approx 1\\)), the EFA is efficient whereas if KMO is low (\\(\\approx 0\\)), the EFA is not relevant. KMO values smaller than \\(0.5\\) indicates that EFA is not suitable and a KMO value of \\(0.6\\) should be present before EFA is considered suitable. Values between \\(0.5\\) and \\(0.7\\) are considered adequate, values between \\(0.7\\) and \\(0.9\\) are good and values between \\(0.9\\) and \\(1.0\\) are excellent.\n\n\n15.1.2 What is EFA good for?\nIf the EFA has provided a good solution (i.e. factor model), then we need to decide what to do with our shiny new factors. Researchers often use EFA during psychometric scale development. They will develop a pool of questionnaire items that they think relate to one or more psychological constructs, use EFA to see which items “go together” as latent factors, and then they will assess whether some items should be removed because they don’t usefully or distinctly measure one of the latent factors.\nIn line with this approach, another consequence of EFA is to combine the variables that load onto distinct factors into a factor score, sometimes known as a scale score. There are two options for combining variables into a scale score:\n\nCreate a new variable with a score weighted by the factor loadings for each item that contributes to the factor.\nCreate a new variable based on each item that contributes to the factor, but weighting them equally.\n\nIn the first option each item’s contribution to the combined score depends on how strongly it relates to the factor. In the second option we typically just average across all the items that contribute substantively to a factor to create the combined scale score variable. Which to choose is a matter of preference, though a disadvantage with the first option is that loadings can vary quite a bit from sample to sample, and in behavioural and health sciences we are often interested in developing and using composite questionnaire scale scores across different studies and different samples. In which case it is reasonable to use a composite measure that is based on the substantive items contributing equally rather than weighting by sample specific loadings from a different sample. In any case, understanding a combined variable measure as an average of items is simpler and more intuitive than using a sample specific optimally-weighted combination.\nA more advanced statistical technique, one which is beyond the scope of this book, undertakes regression modelling where latent factors are used in prediction models of other latent factors. This is called “structural equation modelling” and there are specific software programmes and R packages dedicated to this approach. But let’s not get ahead of ourselves; what we should really focus on now is how to do an EFA in jamovi.\n\n\n15.1.3 EFA in jamovi\nFirst, we need some data. Twenty-five personality self-report items (see Figure 15.2) taken from the International Personality Item Pool were included as part of the Synthetic Aperture Personality Assessment (SAPA) web-based personality assessment (SAPA: http://sapa-project.org) project. The 25 items are organized by five putative factors: Agreeableness, Conscientiousness, Extraversion, Neuroticism, and Openness.\nThe item data were collected using a 6-point response scale:\n\nVery Inaccurate\nModerately Inaccurate\nSlightly Inaccurate\nSlightly Accurate\nModerately Accurate\nVery Accurate.\n\nA sample of \\(N=250\\) responses is contained in the dataset bfi_sample.csv. As researchers, we are interested in exploring the data to see whether there are some underlying latent factors that are measured reasonably well by the \\(25\\) observed variables in the bfi_sample.csv data file. Open up the dataset and check that the \\(25\\) variables are coded as continuous variables (technically they are ordinal though for EFA in jamovi it mostly doesn’t matter, except if you decide to calculate weighted factor scores in which case continuous variables are needed). To perform EFA in jamovi:\n\n\n\n\n\nFigure 15.2: Twenty-five observed variable items organised by five putative personality factors in the dataset bfi_sample.csv\n\n\n\n\n\nSelect Factor - Exploratory Factor Analysis from the main jamovi button bar to open the EFA analysis window (Figure 15.3).\nSelect the 25 personality questions and transfer them into the ‘Variables’ box.\nCheck appropriate options, including ‘Assumption Checks’, but also Rotation ‘Method’, ‘Number of Factors’ to extract, and ‘Additional Output’ options. See Figure 15.3 for suggested options for this illustrative EFA, and please note that the Rotation ‘Method’ and ‘Number of Factors’ extracted is typically adjusted by the researcher during the analysis to find the best result, as described below.\n\n\n\n\n\n\nFigure 15.3: The jamovi EFA analysis window\n\n\n\n\n\n\n\n\n\nFigure 15.4: jamovi EFA assumption checks for the personality questionnaire data\n\n\n\n\nFirst, check the assumptions (Figure 15.4). You can see that (1) Bartlett’s test of sphericity is significant, so this assumption is satisfied; and (2) the KMO measure of sampling adequacy (MSA) is \\(0.81\\) overall, suggesting good sampling adequacy. No problems here then!\nThe next thing to check is how many factors to use (or “extract” from the data). Three different approaches are available:\n\nOne convention is to choose all components with Eigen values greater than 12 . This would give us four factors with our data (try it and see).\nExamination of the scree plot, as in Figure 15.5, lets you identify the “point of inflection”. This is the point at which the slope of the scree curve clearly levels off, below the “elbow”. This would give us five factors with our data. Interpreting scree plots is a bit of an art: in Figure 15.5 there is a noticeable step from \\(5\\) to \\(6\\) factors, but in other scree plots you look at it will not be so clear cut.\nUsing a parallel analysis technique, the obtained Eigen values are compared to those that would be obtained from random data. The number of factors extracted is the number with Eigen values greater than what would be found with random data.\n\n\n\n\n\n\nFigure 15.5: Scree plot of the personality data in jamovi EFA, showing a noticeable inflection and levelling off after point 5 (the ‘elbow’)\n\n\n\n\nThe third approach is a good one according to Fabrigar et al. (1999), although in practice researchers tend to look at all three and then make a judgement about the number of factors that are most easily or helpfully interpreted. This can be understood as the “meaningfulness criterion”, and researchers will typically examine, in addition to the solution from one of the approaches above, solutions with one or two more or fewer factors. They then adopt the solution which makes the most sense to them.\nAt the same time, we should also consider the best way to rotate the final solution. There are two main approaches to rotation: orthogonal (e.g. ‘varimax’) rotation forces the selected factors to be uncorrelated, whereas oblique (e.g. ‘oblimin’) rotation allows the selected factors to be correlated. Dimensions of interest to psychologists and behavioural scientists are not often dimensions we would expect to be orthogonal, so oblique solutions are arguably more sensible2\n\n\n\n\n\nFigure 15.6: Factor summary statistics and correlations for a five factor solution in jamovi EFA\n\n\n\n\nPractically, if in an oblique rotation the factors are found to be substantially correlated (positive or negative, and > 0.3), as in Figure 15.6 where a correlation between two of the extracted factors is 0.31, then this would confirm our intuition to prefer oblique rotation. If the factors are, in fact, correlated, then an oblique rotation will produce a better estimate of the true factors and a better simple structure than will an orthogonal rotation. And, if the oblique rotation indicates that the factors have close to zero correlations between one another, then the researcher can go ahead and conduct an orthogonal rotation (which should then give about the same solution as the oblique rotation).\nOn checking the correlation between the extracted factors at least one correlation was greater than 0.3 (Figure 15.6), so an oblique (‘oblimin’) rotation of the five extracted factors is preferred. We can also see in Figure 15.6 that the proportion of overall variance in the data that is accounted for by the five factors is 46%. Factor one accounts for around 10% of the variance, factors two to four around 9% each, and factor five just over 7%. This isn’t great; it would have been better if the overall solution accounted for a more substantive proportion of the variance in our data.\nBe aware that in every EFA you could potentially have the same number of factors as observed variables, but every additional factor you include will add a smaller amount of explained variance. If the first few factors explain a good amount of the variance in the original 25 variables, then those factors are clearly a useful, simpler substitute for the 25 variables. You can drop the rest without losing too much of the original variability. But if it takes 18 factors (for example) to explain most of the variance in those 25 variables, you might as well just use the original 25.\nFigure 15.7 shows the factor loadings. That is, how the 25 different personality items load onto each of the five selected factors. We have hidden loadings less than \\(0.3\\) (set in the options shown in Figure 15.3.\nFor Factors \\(1, 2, 3\\) and \\(4\\) the pattern of factor loadings closely matches the putative factors specified in Figure 15.2. Phew! And factor \\(5\\) is pretty close, with four of the five observed variables that putatively measure “openness” loading pretty well onto the factor. Variable \\(04\\) doesn’t quite seem to fit though, as the factor solution in Figure 15.7 suggests that it loads onto factor \\(4\\) (albeit with a relatively low loading) but not substantively onto factor \\(5\\).\nThe other thing to note is that those variables that were denoted as “R: reverse coding” in Figure 15.2 are those that have negative factor loadings. Take a look at the items A1 (“Am indifferent to the feelings of others”) and A2 (“Inquire about others’ well-being”). We can see that a high score on \\(A1\\) indicates low Agreeableness, whereas a high score on \\(A2\\) (and all the other “A” variables for that matter) indicates high Agreeableness. Therefore A1 will be negatively correlated with the other “A” variables, and this is why it has a negative factor loading, as shown in Figure 15.7.\n\n\n\n\n\nFigure 15.7: Factor loadings for a five factor solution in jamovi EFA\n\n\n\n\nWe can also see in Figure 15.7 the “uniqueness” of each variable. Uniqueness is the proportion of variance that is ‘unique’ to the variable and not explained by the factors3. For example, 72% of the variance in ‘A1’ is not explained by the factors in the five factor solution. In contrast, ‘N1’ has relatively low variance not accounted for by the factor solution (35%). Note that the greater the ‘uniqueness’, the lower the relevance or contribution of the variable in the factor model.\nTo be honest, it’s unusual to get such a neat solution in EFA. It’s typically quite a bit more messy than this, and often interpreting the meaning of the factors is more challenging. It’s not often that you have such a clearly delineated item pool. More often you will have a whole heap of observed variables that you think may be indicators of a few underlying latent factors, but you don’t have such a strong sense of which variables are going to go where!\nSo, we seem to have a pretty good five factor solution, albeit accounting for a relatively low overall proportion of the observed variance. Let’s assume we are happy with this solution and want to use our factors in further analysis. The straightforward option is to calculate an overall (average) score for each factor by adding together the score for each variable that loads substantively onto the factor and then dividing by the number of variables (in other words create a ‘mean score’ for each person across the items for each scale. For each person in our dataset that entails, for example for the Agreeableness factor, adding together \\(A1 + A2 + A3 + A4 + A5\\), and then dividing by 5. 4 In essence, the factor score we have calculated is based on equally weighted scores from each of the included variables/itmes. We can do this in jamovi in two steps:\n\nRecode A1 into “A1R” by reverse scoring the values in the variable (i.e. \\(6 = 1\\); \\(5 = 2\\); \\(4 = 3\\); \\(3 = 4\\); \\(2 = 5\\); \\(1 = 6\\)) using the jamovi transform variable command (see Figure 15.8).\nCompute a new variable, called “Agreeableness’, by calculating the mean of A1R, A2, A3, A4 and A5. Do this using the jamovi compute new variable command (see Figure 15.9).\n\n\n\n\n\n\nFigure 15.8: Recode variable using the jamovi Transform command\n\n\n\n\n\n\n\n\n\nFigure 15.9: Compute new scale score variable using the jamovi Computed variable command\n\n\n\n\nAnother option is to create an optimally-weighted factor score index. To do this, save the factor scores to the data set, using the ‘Save’ - ‘Factor scores’ checkbox. Once you have done this you will see that five new variables (columns) have been added to the data, one for each factor extracted. See Figure 15.10 and Figure 15.11.\n\n\n\n\n\nFigure 15.10: jamovi option for factor scores for the five factor solution, using the ‘Bartlett’ optimal weighting method\n\n\n\n\n\n\n\n\n\nFigure 15.11: Data sheet view showing the five newly created factor score variables\n\n\n\n\nNow you can go ahead and undertake further analyses, using either the mean score based factor scales (e.g. as in Figure 15.9) or using the optimally-weighted factor scores calculated by jamovi. Your choice! For example, one thing you might like to do is see whether there are any gender differences in each of our personality scales. We did this for the Agreeableness score that we calculated using the mean score approach, and although the t test plot (Figure 15.12) showed that males were less agreeable than females, this was not a significant difference (Mann-Whitney \\(U = 5768\\), \\(p = .075\\)).\n\n\n\n\n\nFigure 15.12: Comparing differences in Agreeableness factor-based scores between males and females\n\n\n\n\n\n\n15.1.4 Writing up an EFA\nHopefully, so far we have given you some sense of EFA and how to undertake EFA in jamovi. So, once you have completed your EFA, how do you write it up? There is not a formal standard way to write up an EFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nWhat are the theoretical underpinnings for the area you are studying, and specifically for the constructs that you are interested in uncovering through EFA.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nDescribe how you went about testing the assumptions for EFA. Details regarding sphericity checks and measures of sampling adequacy should be reported.\nExplain what FA extraction method (e.g. ‘Minimum residuals’ or ‘Maximum likelihood’) was used.\nExplain the criteria and process used for deciding how many factors were extracted in the final solution, and which items were selected. Clearly explain the rationale for key decisions during the EFA process.\nExplain what rotation methods were attempted, the reasons why, and the results.\nFinal factor loadings should be reported in the results, in a table. This table should also report the uniqueness (or communality) for each variable (in the final column). Factor loadings should be reported with descriptive labels in addition to item numbers. Correlations between the factors should also be included, either at the bottom of this table, in a separate table.\nMeaningful names for the extracted factors should be provided. You may like to use previously selected factor names, but on examining the actual items and factors you may think a different name is more appropriate"
  },
  {
    "objectID": "15-Factor-Analysis.html#principal-component-analysis",
    "href": "15-Factor-Analysis.html#principal-component-analysis",
    "title": "15  因素分析",
    "section": "15.2 Principal Component Analysis",
    "text": "15.2 Principal Component Analysis\nIn the previous section we saw that EFA works to identify underlying latent factors. And, as we saw, in one scenario the smaller number of latent factors can be used in further statistical analysis using some sort of combined factor scores.\nIn this way EFA is being used as a “data reduction” technique. Another type of data reduction technique, sometimes seen as part of the EFA family, is principal component analysis (PCA) . However, PCA does not identify underlying latent factors. Instead it creates a linear composite score from a larger set of measured variables.\nPCA simply produces a mathematical transformation to the original data with no assumptions about how the variables co-vary. The aim of PCA is to calculate a few linear combinations (components) of the original variables that can be used to summarize the observed data set without losing much information. However, if identification of underlying structure is a goal of the analysis, then EFA is to be preferred. And, as we saw, EFA produces factor scores that can be used for data reduction purposes just like principal component scores (Fabrigar et al., 1999).\nPCA has been popular in Psychology for a number of reasons, and therefore it’s worth mentioning, although nowadays EFA is just as easy to do given the power of desktop computers and can be less susceptible to bias than PCA, especially with a small number of factors and variables. Much of the procedure is similar to EFA, so although there are some conceptual differences, practically the steps are the same, and with large samples and a sufficient number of factors and variables, the results from PCA and EFA should be fairly similar.\nTo undertake PCA in jamovi, all you need to do is select ‘Factor’ - ‘Principal Component Analysis’ from the main jamovi button bar to open the PCA analysis window. Then you can follow the same steps from EFA in jamovi above."
  },
  {
    "objectID": "15-Factor-Analysis.html#confirmatory-factor-analysis",
    "href": "15-Factor-Analysis.html#confirmatory-factor-analysis",
    "title": "15  因素分析",
    "section": "15.3 Confirmatory Factor Analysis",
    "text": "15.3 Confirmatory Factor Analysis\nSo, our attempt to identify underlying latent factors using EFA with carefully selected questions from the personality item pool seemed to be pretty successful. The next step in our quest to develop a useful measure of personality is to check the latent factors we identified in the original EFA with a different sample. We want to see if the factors hold up, if we can confirm their existence with different data. This is a more rigorous check, as we will see. And it’s called Confirmatory Factor Analysis (CFA) as we will, unsurprisingly, be seeking to confirm a pre-specified latent factor structure.5\nIn CFA, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.13, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the personality items would therefore specify five latent factors as shown in Figure 15.13, each measured by five observed variables. Each variable is a measure of an underlying latent factor. For example, A1 is predicted by the underlying latent factor Agreeableness. And because A1 is not a perfect measure of the Agreeableness factor, there is an error term, \\(e\\), associated with it. In other words, \\(e\\) represents the variance in A1 that is not accounted for by the Agreeableness factor. This is sometimes called measurement error.\n\n\n\n\n\nFigure 15.13: Initial pre-specification of latent factor structure for the five factor personality scales, for use in CFA\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that some of our personality factors may be correlated with each other. So, in our model, we should allow these latent factors to co-vary, as shown by the double-headed arrows in Figure 15.13.\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. One reason for this might be that there is a shared methodological feature for particular sub-sets of the observed variables such that the observed variables might be correlated for methodological rather than substantive latent factor reasons. We’ll return to this possibility in a later section but, for now, there are no clear reasons that we can see that would justify correlating some of the error terms with each other\nWithout any correlated error terms, the model we are testing to see how well it fits with our observed data is just as specified in Figure 15.13. Only parameters that are included in the model are expected to be found in the data, so in CFA all other possible parameters (coefficients) are set to zero. So, if these other parameters are not zero (for example there may be a substantial loading from A1 onto the latent factor Extraversion in the observed data, but not in our model) then we may find a poor fit between our model and the observed data.\nRight, let’s take a look at how we set this CFA analysis up in jamovi.\n\n15.3.1 CFA in jamovi\nOpen up the bfi_sample2.csv file, check that the 25 variables are coded as ordinal (or continuous; it won’t make any difference for this analysis). To perform CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.14).\nSelect the 5 A variables and transfer them into the ‘Factors’ box and give then the label “Agreeableness”.\nCreate a new Factor in the ‘Factors’ box and label it “Conscientiousness”. Select the 5 C variables and transfer them into the ‘Factors’ box under the “Conscientiousness” label.\nCreate another new Factor in the ‘Factors’ box and label it “Extraversion”. Select the 5 E variables and transfer them into the ‘Factors’ box under the “Extraversion” label.\nCreate another new Factor in the ‘Factors’ box and label it “Neuroticism”. Select the 5 N variables and transfer them into the ‘Factors’ box under the “Neuroticism” label.\nCreate another new Factor in the ‘Factors’ box and label it “Openness”. Select the 5 O variables and transfer them into the ‘Factors’ box under the “Openness” label.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.13.\n\n\n\n\n\n\nFigure 15.14: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is model fit (Figure 15.15) as this tells us how good a fit our model is to the observed data. NB in our model only the pre-specified covariances are estimated, including the factor correlations by default. Everything else is set to zero.\n\n\n\n\n\nFigure 15.15: The jamovi CFA Model Fit results for our CFA model\n\n\n\n\nThere are several ways of assessing model fit. The first is a chi-square statistic that, if small, indicates that the model is a good fit to the data. However, the chi-squared statistic used for assessing model fit is pretty sensitive to sample size, meaning that with a large sample a good enough fit between the model and the data almost always produces a large and significant (p < .05) chi-square value.\nSo, we need some other ways of assessing model fit. In jamovi several are provided by default. These are the Comparative Fit Index (CFI), the Tucker Lewis Index (TLI) and the Root Mean Square Error of Approximation (RMSEA) together with the 90% confidence interval for the RMSEA. Some useful rules of thumb are that a satisfactory fit is indicated by CFI > 0.9, TLI > 0.9, and RMSEA of about 0.05 to 0.08. A good fit is CFI > 0.95, TLI > 0.95, and RMSEA and upper CI for RMSEA < 0.05.\nSo, looking at Figure 15.15 we can see that the chi-square value is large and highly significant. Our sample size is not too large, so this possibly indicates a poor fit. The CFI is \\(0.762\\) and the TLI is 0.731, indicating poor fit between the model and the data. The RMSEA is \\(0.085\\) with a \\(90\\%\\) confidence interval from \\(0.077\\) to \\(0.092\\), again this does not indicate a good fit.\nPretty disappointing, huh? But perhaps not too surprising given that in the earlier EFA, when we ran with a similar data set (see Exploratory Factor Analysis section), only around half of the variance in the data was accounted for by the five factor model.\nLet’s go on to look at the factor loadings and the factor covariance estimates, shown in Figure 15.16 and Figure 15.17. The Z-statistic and p-value for each of these parameters indicates they make a reasonable contribution to the model (i.e. they are not zero) so there doesn’t appear to be any reason to remove any of the specified variable-factor paths, or factor-factor correlations from the model. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.16: The jamovi CFA Factor Loadings table for our CFA model\n\n\n\n\n\n\n\n\n\nFigure 15.17: The jamovi CFA Factor Covariances table for our CFA model\n\n\n\n\nHow could we improve the model? One option is to go back a few stages and think again about the items / measures we are using and how they might be improved or changed. Another option is to make some post hoc tweaks to the model to improve the fit. One way of doing this is to use “modification indices” (Figure 15.18), specified as an ‘Additional output’ option in jamovi.\n\n\n\n\n\nFigure 15.18: The jamovi CFA Factor Loadings Modification Indices\n\n\n\n\nWhat we are looking for is the highest modification index (MI) value. We would then judge whether it makes sense to add that additional term into the model, using a post hoc rationalisation. For example, we can see in Figure 15.18 that the largest MI for the factor loadings that are not already in the model is a value of 28.786 for the loading of N4 (“Often feel blue”) onto the latent factor Extraversion. This indicates that if we add this path into the model then the chi-square value will reduce by around the same amount.\nBut in our model adding this path arguably doesn’t really make any theoretical or methodological sense, so it’s not a good idea (unless you can come up with a persuasive argument that “Often feel blue” measures both Neuroticism and Extraversion). I can’t think of a good reason. But, for the sake of argument, let’s pretend it does make some sense and add this path into the model. Go back to the CFA analysis window (see Figure 15.14) and add N4 into the Extraversion factor. The results of the CFA will now change (not shown); the chi-square has come down to around 709 (a drop of around 30, roughly similar to the size of the MI) and the other fit indices have also improved, though only a bit. But it’s not enough: it’s still not a good fitting model.\nIf you do find yourself adding new parameters to a model using the MI values then always re-check the MI tables after each new addition, as the MIs are refreshed each time.\nThere is also a Table of Residual Covariance Modification Indices produced by jamovi (Figure 15.19). In other words, a table showing which correlated errors, if added to the model, would improve the model fit the most. It’s a good idea to look across both MI tables at the same time, spot the largest MI, think about whether the addition of the suggested parameter can be reasonably justified and, if it can, add it to the model. And then you can start again looking for the biggest MI in the re-calculated results.\n\n\n\n\n\nFigure 15.19: Residual Covariance Modification Indices produced by jamovi\n\n\n\n\nYou can keep going this way for as long as you like, adding parameters to the model based on the largest MI, and eventually you will achieve a satisfactory fit. But there will also be a strong possibility that in doing this you will have created a monster! A model that is ugly and deformed and doesn’t have any theoretical sense or purity. In other words, be very careful!\nSo far, we have checked out the factor structure obtained in the EFA using a second sample and CFA. Unfortunately, we didn’t find that the factor structure from the EFA was confirmed in the CFA, so it’s back to the drawing board as far as the development of this personality scale goes.\nAlthough we could have tweaked the CFA using modification indexes, there really were not any good reasons (that I could think of) for these suggested additional factor loadings or residual covariances to be included. However, sometimes there is a good reason for residuals to be allowed to co-vary (or correlate), and a good example of this is shown in the next section on Multi-Trait Multi-Method CFA. Before we do that, let’s cover how to report the results of a CFA.\n\n\n15.3.2 Reporting a CFA\nThere is not a formal standard way to write up a CFA, and examples tend to vary by discipline and researcher. That said, there are some fairly standard pieces of information to include in your write-up:\n\nA theoretical and empirical justification for the hypothesized model.\nA complete description of how the model was specified (e.g. the indicator variables for each latent factor, covariances between latent variables, and any correlations between error terms). A path diagram, like the one in Figure 15.13 would be good to include.\nA description of the sample (e.g. demographic information, sample size, sampling method).\nA description of the type of data used (e.g., nominal, continuous) and descriptive statistics.\nTests of assumptions and estimation method used.\nA description of missing data and how the missing data were handled.\nThe software and version used to fit the model.\nMeasures, and the criteria used, to judge model fit.\nAny alterations made to the original model based on model fit or modification indices.\nAll parameter estimates (i.e., loadings, error variances, latent (co)variances) and their standard errors, probably in a table."
  },
  {
    "objectID": "15-Factor-Analysis.html#multi-trait-multi-method-cfa",
    "href": "15-Factor-Analysis.html#multi-trait-multi-method-cfa",
    "title": "15  因素分析",
    "section": "15.4 Multi-Trait Multi-Method CFA",
    "text": "15.4 Multi-Trait Multi-Method CFA\nIn this section we’re going to consider how different measurement techniques or questions can be an important source of data variability, known as method variance. To do this, we’ll use another psychological data set, one that contains data on “attributional style”.\nThe Attributional Style Questionnaire (ASQ) was used (Hewitt et al., 2004) to collect psychological wellbeing data from young people in the United Kingdom and New Zealand. They measured attributional style for negative events, which is how people habitually explain the cause of bad things that happen to them (Peterson & Seligman, 1984). The attributional style questionnaire (ASQ) measures three aspects of attributional style:\n\nInternality is the extent to which a person believes that the cause of a bad event is due to his/her own actions.\nStability refers to the extent to which a person habitually believes the cause of a bad event is stable across time.\nGlobality refers to the extent to which a person habitually believes that the cause of a bad event in one area will affect other areas of their lives.\n\nThere are six hypothetical scenarios and for each scenario respondents answer a question aimed at (a) internality, (b) stability and (c) globality. So there are \\(6 \\times 3 = 18\\) items overall. See Figure 15.20 for more details.\n\n\n\n\n\nFigure 15.20: The Attributional Style Questionnaire (ASQ) for negative events\n\n\n\n\nResearchers are interested in checking their data to see whether there are some underlying latent factors that are measured reasonably well by the 18 observed variables in the ASQ.\nFirst, they try EFA with these 18 variables (not shown), but no matter how they extract or rotate, they can’t find a good factor solution. Their attempt to identify underlying latent factors in the Attributional Style Questionnaire (ASQ) proved fruitless. If you get results like this then either your theory is wrong (there is no underlying latent factor structure for attributional style, which is possible), the sample is not relevant (which is unlikely given the size and characteristics of this sample of young adults from the United Kingdom and New Zealand), or the analysis was not the right tool for the job. We’re going to look at this third possibility.\nRemember that there were three dimensions measured in the ASQ: Internality, Stability and Globality, each measured by six questions as shown in Figure 15.21.\nWhat if, instead of doing an analysis where we see how the data goes together in an exploratory sense, we instead impose a structure, like in Figure 15.21, on the data and see how well the data fits our pre-specified structure. In this sense, we are undertaking a confirmatory analysis, to see how well a pre-specified model is confirmed by the observed data.\nA straightforward confirmatory factor analysis (CFA) of the ASQ would therefore specify three latent factors as shown in the columns of Figure 15.27, each measured by six observed variables.\n\n\n\n\n\nFigure 15.21: Six questions on the ASQ for each of the Internality, Stability and Globality dimensions\n\n\n\n\nWe could depict this as in the diagram in Figure 15.22, which shows that each variable is a measure of an underlying latent factor. For example INT1 is predicted by the underlying latent factor Internality. And because INT1 is not a perfect measure of the Internality factor, there is an error term, e1, associated with it. In other words, e1 represents the variance in INT1 that is not accounted for by the Internality factor. This is sometimes called “measurement error”.\n\n\n\n\n\nFigure 15.22: Initial pre-specification of latent factor structure for the ASQ\n\n\n\n\nThe next step is to consider whether the latent factors should be allowed to correlate in our model. As mentioned earlier, in the psychological and behavioural sciences constructs are often related to each other, and we also think that Internality, Stability, and Globality might be correlated with each other, so in our model we should allow these latent factors to co-vary, as shown in Figure 15.23.\n\n\n\n\n\nFigure 15.23: Final pre-specification of latent factor structure for the ASQ, including latent factor correlations, and shared method error term correlations for the observed variable INT1, STAB1 and GLOB1, in a CFA MTMM model. For clarity, other pre-specified error term correlations are not shown\n\n\n\n\nAt the same time, we should consider whether there is any good, systematic, reason for some of the error terms to be correlated with each other. Thinking back to the ASQ questions, there were three different sub-questions (a, b and c) for each main question (1-6). Q1 was about unsuccessful job hunting and it is plausible that this question has some distinctive artefactual or methodological aspects over and above the other questions (2-5), something to do with job hunting perhaps. Similarly, Q2 was about not helping a friend with a problem, and there may be some distinctive artefactual or methodological aspects to do with not helping a friend that is not present in the other questions (1, and 3-5).\nSo, as well as multiple factors, we also have multiple methodological features in the ASQ, where each of Questions 1-6 has a slightly different “method”, but each “method” is shared across the sub-questions a, b and c. In order to incorporate these different methodological features into the model we can specify that certain error terms are correlated with each other. For example, the errors associated with INT1, STAB1 and GLOB1 should be correlated with each other to reflect the distinct and shared methodological variance of Q1a, Q1b and Q1c. Looking at Figure 15.21, this means that as well as the latent factors represented by the columns, we will have correlated measurement errors for the variables in each row of the Table.\nWhilst a basic CFA model like the one shown in Figure 15.22 could be tested against our observed data, we have in fact come up with a more sophisticated model, as shown in the diagram in Figure 15.23. This more sophisticated CFA model is known as a Multi-Trait Multi-Method (MTMM) model, and it is the one we will test in jamovi.\n\n15.4.1 MTMM CFA in jamovi\nOpen up the ASQ.csv file and check that the 18 variables (six “Internality”, six “Stability” and six “Globality” variables) are specified as continuous variables.\nTo perform MTMM CFA in jamovi:\n\nSelect Factor - Confirmatory Factor Analysis from the main jamovi button bar to open the CFA analysis window (Figure 15.24).\nSelect the 6 INT variables and transfer them into the ‘Factors’ box and give them the label “Internality”.\nCreate a new Factor in the ‘Factors’ box and label it “Stability”. Select the 6 STAB variables and transfer them into the ‘Factors’ box under the “Stability” label.\nCreate another new Factor in the ‘Factors’ box and label it “Globality”. Select the 6 GLOB variables and transfer them into the ‘Factors’ box under the “Globality” label.\nOpen up the Residual Covariances options, and for each of our pre-specified correlations move the associated variables across into the ‘Residual Covariances’ box on the right. For example, highlight both INT1 and STAB1 and then click the arrow to move these across. Now do the same for INT1 and GLOB1, for STAB1 and GLOB1, for INT2 and STAB2, for INT2 and GLOB2, for STAB2 and GLOB2, for INT3 and STAB3, and so on.\nCheck other appropriate options, the defaults are ok for this initial work through, though you might want to check the “Path diagram” option under ‘Plots’ to see jamovi produce a (fairly) similar diagram to our Figure 15.23, and including all the error term correlations that we have added above.\n\n\n\n\n\n\nFigure 15.24: The jamovi CFA analysis window\n\n\n\n\nOnce we have set up the analysis we can turn our attention to the jamovi results window and see what’s what. The first thing to look at is “Model fit” as this tells us how good a fit our model is to the observed data (Figure 15.25). NB in our model only the pre-specified covariances are estimated, everything else is set to zero, so model fit is testing both whether the pre-specified “free” parameters are not zero, and conversely whether the other relationships in the data – the ones we have not specified in the model – can be held at zero.\n\n\n\n\n\nFigure 15.25: The jamovi CFA Model Fit results for our CFA MTMM model\n\n\n\n\nLooking at Figure 15.25 we can see that the chi-square value is highly significant, which is not a surprise given the large sample size (N = 2748). The CFI is 0.98 and the TLI is also 0.98, indicating a very good fit. The RMSEA is 0.02 with a 90% confidence interval from 0.02 to 0.02 – pretty tight!\nOverall, I think we can be satisfied that our pre-specified model is a very good fit to the observed data, lending support to our MTMM model for the ASQ.\nWe can now go on to look at the factor loadings and the factor covariance estimates, as in Figure 15.26. Often the standardized estimates are easier to interpret, and these can be specified under the ‘Estimates’ option. These tables can usefully be incorporated into a written report or scientific article.\n\n\n\n\n\nFigure 15.26: The jamovi CFA Factor Loadings and Covariances tables for our CFA MTMM model\n\n\n\n\nYou can see from Figure 15.26 that all of our pre-specified factor loadings and factor covariances are significantly different from zero. In other words, they all seem to be making a useful contribution to the model.\nWe’ve been pretty lucky with this analysis, getting a very good fit on our first attempt!"
  },
  {
    "objectID": "15-Factor-Analysis.html#sec-Internal-consistency-reliability-analysis",
    "href": "15-Factor-Analysis.html#sec-Internal-consistency-reliability-analysis",
    "title": "15  因素分析",
    "section": "15.5 內部一致性信度分析",
    "text": "15.5 內部一致性信度分析\n\n\n初步了解運用探索性因素分析和確認性因素分析開發量表的過程後,現在對運用因素分析的認識應該到達了新階段:有效的量表利用不同樣本的確認性因素分析會得到一致的結果。到了這個階段,也許會想知道使用反應變項組合的量表所測得的因素分數有多可信。\n心理測量常使用可靠性分析評估心理建構的測量一致性資訊(印象模糊的話，請複習一下 小單元 2.3 )。這裡要討論的是內部一致性,這是指組成一個量表的所有題目之間一致性。比如有 \\(V_1、V_2、V_3、V_4\\) 和 \\(V_5\\) 等反應變項,就可以計算一個統計量,以此評估這些變項在測量潛在因素的內部一致性。\n克隆巴赫\\(\\alpha\\)係數是最常被用來檢驗量表內部一致性的統計量(Chronbach, 1951)。克隆巴赫\\(\\alpha\\)是一種評估測量等價性的指標(不同量表的題目產生相同測量結果的程度)。驗證測量等價性的方法是將量表項目分半,查看各半的分析結果是否具有可比性。同一套題目的題目有多種分半方式,如果比較所有的分半分析結果,就會得到一個反映整體分半等價係數的統計量。克隆巴赫\\(\\alpha\\)就是這樣一種統計量:綜合一個量表的所有項目分半係數的函數。如果一組測量一個構建的項目(例如外向性量表)的 \\(\\alpha\\) 為 \\(0.80\\),那麼該量表的誤差變異比例為 \\(0.20\\)。換句話說,\\(\\alpha\\) 為 \\(0.80\\) 的量表包括大約 20% 的誤差。\n, 但是，克隆巴赫\\(\\alpha\\)不是單維性測量值(指係數呈現單一量表只測量單一因子或構建，而不是測量複數構建)。如果一份量表不只評估一種因素，得到的\\(\\alpha\\)會是被低估的值。也就是說如果一份評估不只一種潛在因素的量表\\(\\alpha\\)為0.80，這個量表的題目呈現的一致性不只反映在單一因素。這是為什麼本書建議先進行探索性因素分析或確認性因素分析的原因。\n克隆巴赫\\(\\alpha\\)的另一個特性是會呈現樣本特異性:不只是量表題目的內部一致性，而是樣本資料的特徵也會影響\\(\\alpha\\)係數。有取樣偏誤、不具代表性或小樣本資料的\\(\\alpha\\)係數，可能與有代表性的大樣本所測出的\\(\\alpha\\)係數非常不同。即使是兩套樣本量相當的大型資料之間,\\(\\alpha\\)也可能不一樣。儘管有這些使用限制,心理學領域一直很愛用克隆巴赫\\(\\alpha\\)估計內部一致性。因為計算簡單、容易理解和解釋。研究者將量表用於不同樣本、不同環境或人群的施測,可以作為評估量表是否有用的初級指標。\n另一種指標是麥當勞\\(\\omega\\)，也可以用jamovi計算。使用\\(\\alpha\\)有這些條件：(a)殘差項目之間沒有相關,(b)所有題目的因素負荷量相等,以及(c)量表只有測量單一因素。不過使用\\(\\omega\\)沒有這些限制，因此是一種更穩健的內部一致性統計量。本書建議如果沒有上述使用條件，\\(\\alpha\\)和\\(\\omega\\)的值會相當接近；如果有違反，應優先採用\\(\\omega\\)。\n有的時候研究人員需要一套閾值，判斷\\(\\alpha\\)或\\(\\omega\\)夠不夠好。像是\\(\\alpha\\)為\\(0.70\\) 或 \\(0.80\\) 分別代表量表內部一致性“可接受”及“良好”。更好的報告方法是呈現\\(\\alpha\\)或\\(\\omega\\)的值都是\\(0.70\\)，且量表測量誤差約佔30%；或者呈現\\(\\alpha\\)或\\(\\omega\\)的值都是\\(0.80\\)，且量表測量誤差約佔20%。\n\\(\\alpha\\)太高是不是好事？若是係數值大於\\(0.95\\)，表示量表題目之間有高相關，不過測量方法可能存在過度冗餘資訊導致的特異性,而且要測量的因素建構可能有過度窄化之風險。\n\n15.5.1 使用jamovi完成內部一致性信度分析\n\n您提醒得非常到位,文本中还存在简体词汇“變項”和“資料”。已经校对替换为“變項”和“資料”。譯文重新生成如下:\n我們有第三個人格資料樣本可以用於進行可靠性分析:在 bfi_sample3.csv 檔中。再次檢查25個人格項目變項是否被編碼為連續變項。要在 jamovi 中執行可靠性分析:\n\n從主界面按鈕欄中選擇“因子” - “可靠性分析”以打開可靠性分析窗口(圖 15.27)。\n選擇5個A變項並將其轉移到“項目”框中。\n在“反向計分項目”選項下,選擇“正常計分項目”框中的A1變項,並將其移動到“反向計分項目”框中。\n檢查其他適當的選項,如@fig-fig15-27所示。\n\n\n\n\n\n\n圖 15.27: jamovi 可靠性分析窗口\n\n\n\n\n\n完成後,查看 jamovi 結果窗口。您應該看到類似 圖 15.28 的內容。這告訴我們親和力量表的克隆巴赫 α 係數為 0.72。這意味著親和力量表分數中的錯誤變異略低於 30%。麥當勞 ω 也給出了,為 0.74,與 α 沒有太大區別。\n\n\n\n\n\n圖 15.28: jamovi 親和力因子的可靠性分析結果\n\n\n\n\n\n我們也可以檢查如果從量表中刪除特定項目,α 或 ω 如何得到改善。例如,如果刪除項目 A1,α 將增加到 0.74,ω 增加到 0.75。這個增幅並不大,所以可能不值得這樣做。\n計算和檢查量表統計量(α 和 ω)的過程對於所有其他量表也是相同的,除了開放性,它們都有類似的可靠性估計。 對於開放性,量表分數中的錯誤變異量約為 40%,這很高,並表明與其他人格特徵量表相比,開放性作為人格特徵可靠測量的一致性要差得多。"
  },
  {
    "objectID": "15-Factor-Analysis.html#summary",
    "href": "15-Factor-Analysis.html#summary",
    "title": "15  因素分析",
    "section": "15.6 Summary",
    "text": "15.6 Summary\nIn this chapter on factor analysis and related techniques we have introduced and demonstrated statistical analyses that assess the pattern of relationships in a data set. Specifically, we have covered:\n\nExploratory Factor Analysis (EFA). EFA is a statistical technique for identifying underlying latent factors in a data set. Each observed variable is conceptualised as representing the latent factor to some extent, indicated by a factor loading. Researchers also use EFA as a way of data reduction, i.e. identifying observed variables than can be combined into new factor variables for subsequent analysis.\nPrincipal Component Analysis (PCA) is a data reduction technique which, strictly speaking, does not identify underlying latent factors. Instead, PCA simply produces a linear combination of observed variables.\nConfirmatory Factor Analysis (CFA). Unlike EFA, with CFA you start with an idea - a model - of how the variables in your data are related to each other. You then test your model against the observed data and assess how good a fit the model is to the data.\nIn Multi-Trait Multi-Method CFA (MTMM CFA), both latent factor and method variance are included in the model in an approach that is useful when there are different methodological approaches used and therefore method variance is an important consideration.\nInternal consistency reliability analysis. This form of reliability analysis tests how consistently a scale measures a measurement (psychological) construct.\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "Prelude-Part-IV.html#邏輯推理的侷限",
    "href": "Prelude-Part-IV.html#邏輯推理的侷限",
    "title": "中場故事",
    "section": "邏輯推理的侷限",
    "text": "邏輯推理的侷限\n\n這是正式翻譯這一節原始文檔的繁体中文译稿,包括引言和对话部分:\n\n整個戰爭藝術在於瞭解山的另一邊是什麼,或者換句話說,從我們所知道的東西中學習我們不知道的東西。\n\n關於這句名言,據說是在一次鄉村馬車旅行中產生的。威靈頓公爵和他的同伴 J. W. Croker 玩着一個猜謎遊戲,各自試圖預測每座山的另一邊會是什麼。結果證明,在每一案例中威靈頓都是對的,而 Croker 都是錯的。多年后當有人詢問威靈頓這場遊戲時,他解釋說“整個戰爭的藝術在於瞭解山的另一邊是什麼”。的確,戰爭在這方面並不特殊。生活的所有方面都是某種形式的猜謎遊戲,而想要度過一天又一天,我們需要做出好的猜測。那麼,讓我們玩一場自己的猜謎遊戲。1\n\n假設你和我正在觀察Wellesley-Croker競賽,在每三座山之后,你我需要預測誰將贏得下一座山,Wellesley還是Croker。讓我們說W指的是Wellesley的勝利,C指的是Croker的勝利。在三座山之后,我們的數據集如下:\n\\(WWW\\)\n\n我們的對話如下:\n\n你:三連勝並沒有太大意義。我猜Wellesley在這個遊戲上可能比Croker更擅長,但這也可能只是巧合。儘管如此,我還是有點賭徒精神。我會押註Wellesley。\n\n\n我:我同意三連勝並沒有提供足夠信息,我也找不到任何理由更青睞Wellesley而不是Croker。我在這個階段不能證明下注是正確的。很抱歉,我不會下注。\n\n\n你的賭注成功了:又過去三座山,Wellesley贏得了所有三座。進入我們遊戲的下一輪,比分是1比0,你獲勝,我們的資料如下:\\(WWW\\) \\(WWW\\) 我已將數據組織成三個數據塊,以便您可以看到哪個數據塊對應於我們在遊戲每一步可用的觀測。看到這個新數據塊后,我們的對話繼續:\n\nyou: Okay, this is pretty obvious. Wellesley is way better at this game. We both agree he’s going to win the next hill, right?\n\n\nme: Is there really any logical evidence for that? Before we started this game, there were lots of possibilities for the first 10 outcomes, and I had no idea which one to expect. \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\) was one possibility, but so was \\(WCC\\) \\(CWC\\) \\(WWC\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or even \\(CCC\\) \\(CCC\\) \\(CCC\\) \\(C\\). Because I had no idea what would happen so I’d have said they were all equally likely. I assume you would have too, right? I mean, that’s what it means to say you have “no idea”, isn’t it?\n\n\nyou: I suppose so.\n\n\nme: Well then, the observations we’ve made logically rule out all possibilities except two: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) or \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). Both of these are perfectly consistent with the evidence we’ve encountered so far, aren’t they?\n\n\nyou: Yes, of course they are. Where are you going with this?\n\n\nme: So what’s changed then? At the start of our game, you’d have agreed with me that these are equally plausible and none of the evidence that we’ve encountered has discriminated between these two possibilities. Therefore, both of these possibilities remain equally plausible and I see no logical reason to prefer one over the other. So yes, while I agree with you that Wellesley’s run of 9 wins in a row is remarkable, I can’t think of a good reason to think he’ll win the 10th hill. No bet.\n\n\nyou: I see your point, but I’m still willing to chance it. I’m betting on Wellesley.\n\nWellesley’s winning streak continues for the next three hills. The score in the Wellesley-Croker game is now 12-0, and the score in our game is now 3-0. As we approach the fourth round of our game, our data set is this: \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) and the conversation continues:\n\nyou: Oh yeah! Three more wins for Wellesley and another victory for me. Admit it, I was right about him! I guess we’re both betting on Wellesley this time around, right?\n\n\nme: I don’t know what to think. I feel like we’re in the same situation we were in last round, and nothing much has changed. There are only two legitimate possibilities for a sequence of 13 hills that haven’t already been ruled out, \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) and \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\). It’s just like I said last time. If all possible outcomes were equally sensible before the game started, shouldn’t these two be equally sensible now given that our observations don’t rule out either one? I agree that it feels like Wellesley is on an amazing winning streak, but where’s the logical evidence that the streak will continue?\n\n\nyou: I think you’re being unreasonable. Why not take a look at our scorecard, if you need evidence? You’re the expert on statistics and you’ve been using this fancy logical analysis, but the fact is you’re losing. I’m just relying on common sense and I’m winning. Maybe you should switch strategies.\n\n\nme: Hmm, that is a good point and I don’t want to lose the game, but I’m afraid I don’t see any logical evidence that your strategy is better than mine. It seems to me that if there were someone else watching our game, what they’d have observed is a run of three wins to you. Their data would look like this: \\(YYY\\). Logically, I don’t see that this is any different to our first round of watching Wellesley and Croker. Three wins to you doesn’t seem like a lot of evidence, and I see no reason to think that your strategy is working out any better than mine. If I didn’t think that \\(WWW\\) was good evidence then for Wellesley being better than Croker at their game, surely I have no reason now to think that YYY is good evidence that you’re better at ours?\n\n\nyou: Okay, now I think you’re being a jerk.\n\n\nme: I don’t see the logical evidence for that.\n\n—&gt;\n\n你:很明顯,Wellesley 在這個遊戲中表現更好。我們都同意他會贏得下一場比賽,對吧?\n\n\n我:這個結論真的有什麼邏輯依據嗎?在我們開始這場遊戲之前,前10次結果有很多可能的組合,我也不知道會是哪一種。\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\) 是一種可能,但 \\(WCC\\) \\(CWC\\) \\(WWC\\) \\(C\\) 和 \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) 或者甚至 \\(CCC\\) \\(CCC\\) \\(CCC\\) \\(C\\) 也都是可能的。因為我根本不知道會發生什麼,所以我會說這些結果的可能性是一樣的。我假設你也是這樣想的,對吧?我的意思是,這就是“完全不知道”的意思,不是嗎?\n\n\n你:我想是的。\n\n\n我:那麼,我們的觀察從邏輯上排除了所有可能性,只剩下兩種:\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) 或者 \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\)。這兩種情況都和我們目前觀察到的證據完全吻合,不是嗎?\n\n\n你:是的,當然是的。你想說什麼?\n\n\n我:那麼現在情況有什麼改變呢?在遊戲開始時,你會同意這兩種可能性同樣合理,而我們觀察到的證據也没有區分兩者的優劣。所以,這兩種可能性仍然同樣合理,我看不出有任何邏輯理由偏好其中一種。所以,是的,雖然我同意你的看法Wellesley的9連勝很令人印象深刻,但是我想不出任何理由認為他會贏得第10場。不押注。\n\n\n你:我理解你的看法,但是我仍然願意嘗試下注。我押Wellesley。\n\n在接下來的三場比賽中,Wellesley的連勝仍在繼續。Wellesley-Croker比賽的比分現在是12比0,我們遊戲的比分是3比0。當我們進入遊戲的第四輪時,我們的資料集如下:\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) 對話繼續:\n\n你:太好了!Wellesley再次取得三連勝,我也再次獲勝了。你得承認,我對他的判斷是對的!我想這一次我們都會押Wellesley,對吧?\n\n\n我:我不知道該怎麼想。我覺得我們和上一輪的情況差不多,沒有太大變化。對於一系列13次未被排除的結果,只有兩種合法的可能:\\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(C\\) 和 \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(WWW\\) \\(W\\)。就像我上次說的那樣。如果在遊戲開始前所有結果都同樣合理的話,那么鑑於我們的觀察並没有排除其中任何一種,這兩種結果現在應該也同樣合理,不是嗎?我同意你的感覺,Wellesley正處在令人驚嘆的連勝中,但是哪裡有邏輯證據可以證明他會繼續保持下去呢?\n\n\n你:我認為你很不合理。如果你需要證據,為什麼不看看我們的記分牌呢?你是統計學專家,一直在使用這種花哨的邏輯分析,但事實是你正在輸。我只是依靠常識,而我正在獲勝。或許你應該改變策略。\n\n\n我:嗯,你說的有道理,我也不想輸,但是恕我直言,我看不出有任何邏輯證據證明你的策略比我的更好。在我看來,如果有其他人在觀看我們的比賽,他們會看到你取得了三連勝。他們的資料會像這樣:\\(YYY\\)。 在邏輯上,我認為這和我們第一次觀察 Wellesley 和 Croker 沒有兩樣。你三次獲勝並不算很有力的證據,我也不認為你的策略有任何優於我的地方。如果我不認為 \\(WWW\\) 是 Wellesley 比 Croker 更優秀的有力證據的話,那么我現在當然也沒有理由相信 YYY 是你比我更擅長這場比賽的有力證據。\n\n\n你:好吧,現在我認為你在裝傻。\n\n\n我:我看不出有什麼邏輯證據可以證明這一點。"
  },
  {
    "objectID": "Prelude-Part-IV.html#學習統計不需要預設條件嗎",
    "href": "Prelude-Part-IV.html#學習統計不需要預設條件嗎",
    "title": "中場故事",
    "section": "學習統計不需要預設條件嗎",
    "text": "學習統計不需要預設條件嗎\n\n我們可以從多個角度分析上面的對話,但是由于這本書的目標讀者是心理學家而不是哲學或推理心理學專業人士,我會簡單帶過。上面描述的有時被稱為歸納謎題。認為 Wellesley 12連勝是他贏得第13場比賽的非常有力證據,這種想法看起來非常合理,但是很難對這種信念提供恰當的邏輯證明。相反,儘管答案很明顯,但如果不依賴某些你沒有任何邏輯依據的預設條件,實際上不可能證明押注 Wellesley。\n歸納作為哲學難題最常與大衛·休謨(David Hume)和納爾遜·古德曼(Nelson Goodman)的哲學工作聯繫在一起,但你可以在各個領域找到這個問題的例子,如文學作家(像是寫愛麗絲夢遊境的路易斯·卡羅 )和機器學習(“免費午餐”定理)。嘗試“從我們所知道的中學習我們不知道的東西”確實有些怪異。關鍵點是,如果你想了解世界上的任何事物,則預設條件2和偏見是不可避免的。這是無法逃避的,統計推論和人類推理一樣也是如此。在對話中,我試圖採取合理的人類推理,但是你所依賴的常識推理跟統計學家所做的没有區別。你在對話中“常識”那一半所依賴的隱含預設條件是 Wellesley 和 Croker 之間在技能上存在某些區別,你要做的就是設法找出他們之間的技能差異大小。而我的“邏輯分析”完全拒絕這種預設條件。我所願意接受的只是存在勝利和失敗的序列,并且我不知道會觀察到哪些序列。在整個對話中,我堅持認為在 Wellesley-Croker 比賽開始時所有在邏輯上可能的數據集都同樣合理,我修改信念的唯一方式就是消除與觀察事實不一致的可能性。\n\n就其自身而言,這聽起來非常合理。事實上,它甚至聽起來像是良好的演繹推理的標誌。像福爾摩斯一樣,我的方法是排除不可能發生的事情,以期獲得真相。然而,正如我們所見,排除不可能的事情從未使我能夠做出預測。就其自身而言,我在對話中說的每一件事情都是完全正確的。無法作出任何預測是“不做任何預設條件”的邏輯結果。最終我輸掉了比賽,因為你做出了一些預設條件,而這些觀察結果是正確的。技能是真實存在的,你相信技能的存在,所以你能夠學習到 Wellesley 的技能勝過 Croker。如果你的學習依賴於一個不太合理的預設條件,你可能就不會贏得比賽了。\n最終,有兩件事情是你應該了解的。首先,正如我所說,如果你想從數據中學習任何東西,你就必須設定預設條件。但是其次,一旦你意識到預設條件是必要的,那麼確保你做出正確的預設條件就變得非常重要!依賴較少條件的資料分析不一定比依賴更多條件的分析好,這完全取决于這些預設條件對你的數據是否合適。在我們學習這本書的後半部時,我經常會指出某種統計技術所依賴的預設條件,以及如何檢查這些條件是否合理。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "href": "14-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.1 平衡且無交互作用的因子設計分析",
    "text": "14.1 平衡且無交互作用的因子設計分析\n當我們在 章节 13 中討論變異數分析時，我們假設了一個相對簡單的實驗設計。每個人都在幾個小組中，我們想知道這些小組在某些結果變量上的平均分數是否有所不同。在本節中，我將討論一個更廣泛的實驗設計類別，稱為因子設計，其中我們有多個分組變量。在上面給出了這種設計可能產生的一個例子。另一個例子出現在 章节 13 中，我們在其中研究了不同藥物對每個人的情緒增益的影響。在那一章中，我們確實發現了藥物的顯著影響，但在章節的最後，我們還進行了一個分析，以查看治療是否有影響。我們沒有找到，但是在試圖預測相同結果的兩個單獨分析中有點令人擔憂。也許治療對情緒增益確實有影響，但我們找不到它，因為它被藥物的影響“隱藏”了？換句話說，我們將要進行一個包括藥物和治療作為預測因子的單一分析。對於這種分析，每個人都按照他們給定的藥物（具有3個水平的因子）和接受的治療（具有2個水平的因子）進行交叉分類。我們將此稱為\\(3 \\times 2\\)因子設計。\n如果我們用jamovi（見 章节 6.1 ）中的“頻率” - “應急表”分析交叉制表藥物和治療，我們將獲得在 图 14.1 中顯示的表格。\n\n\n\n\n\n\n\n图 14.1: jamovi藥物與治療的應急表\n\n\n\n\n如您所見，我們不僅有與兩個因子的所有可能組合相對應的參與者，表明我們的設計是完全交叉，而且事實上每個組中都有相等數量的人。換句話說，我們擁有一個平衡設計。在本節中，我將談論如何分析來自平衡設計的數據，因為這是最簡單的情況。對於不平衡設計的情況相當繁瑣，所以我們暫時將其擱置。\n\n\n14.1.1 多因子設計是因應什麼樣的假設？\n就像單因子變異數分析一樣，因子變異數分析是一個用於測試關於母體均值的某些類型假設的工具。因此，一個明智的開始方式是明確我們的假設實際上是什麼。然而，在我們甚至到達這一點之前，有一個簡單清晰的表示法來描述母體均值是非常有用的。由於觀察是根據兩個不同因子進行交叉分類的事實，可能有很多不同的均值會引起我們的興趣。為了理解這一點，讓我們首先考慮在這種設計中可以計算出所有不同樣本均值。首先，很明顯，我們可能對此類組均值感興趣（ 表格 14.1 ）。\n\n\n\n\n\n表格 14.1: 藥物和治療組的組均值在* clinicaltrial.csv *數據中\n\n\ndrug\ntherapy\nmood.gain\n\n\nplacebo\nno.therapy\n0.30\n\n\nanxifree\nno.therapy\n0.40\n\n\njoyzepam\nno.therapy\n1.47\n\n\nplacebo\nCBT\n0.60\n\n\nanxifree\nCBT\n1.03\n\n\njoyzepam\nCBT\n1.50\n\n\n\n\n\n\n\n\n接下來，下表（ 表格 14.2 ）顯示了兩個因子所有可能組合的組均值列表（例如，接受安慰劑且未接受治療的人、接受安慰劑並接受CBT的人等）。將所有這些數字，以及邊際和總體均值，整合到一個單一的表格中是非常有幫助的，這個表格看起來是這樣的：\n\n\n\n\n\n\n表格 14.2: 藥物和治療組的組和總體均值在clintrial.csv數據中\n\n\n\nno therapy\nCBT\ntotal\n\n\nplacebo\n0.30\n0.60\n0.45\n\n\nanxifree\n0.40\n1.03\n0.72\n\n\njoyzepam\n1.47\n1.50\n1.48\n\n\ntotal\n0.72\n1.04\n0.88\n\n\n\n\n\n\n\n\n現在，這些不同的均值當然是樣本統計量。它是一個與我們在研究過程中所做的具體觀察相關的數量。我們想要對應的母體參數進行推斷。也就是說，真實的均值是在某個更廣泛的母體內存在的。這些母體均值也可以整理成一個類似的表格，但是我們需要一些數學符號來表示（表格 14.3）。像往常一樣，我將使用符號\\(\\mu\\)來表示母體均值。然而，由於有很多不同的均值，我需要使用下標來區分它們。\n這裡是符號如何運作的。我們的表格是根據兩個因子定義的。每行對應於因子A（在本例中為藥物）的不同水平，每列對應於因子B（在本例中為治療）的不同水平。如果我們讓R表示表格中的行數，並讓\\(C\\)表示列數，我們可以將其稱為\\(R \\times C\\)因子變異數分析。在這種情況下\\(R = 3\\)和\\(C = 2\\)。我們將使用小寫字母來表示特定的行和列，因此\\(\\mu_{rc}\\)表示與因子\\(A\\)的第\\(r\\)級（即第\\(r\\)行）和因子B的第\\(c\\)級（第c列）相關的母體均值。1 所以現在母體的均值是寫成@tbl-tab13-1的形式：\n\n\n\n\n\n\n表格 14.3: 因子表中母體均值的符號表示法\n\n\n\nno therapy\nCBT\ntotal\n\n\nplacebo\n\\( \\mu_{11} \\)\n\\( \\mu_{12} \\)\n\n\n\nanxifree\n\\( \\mu_{21} \\)\n\\( \\mu_{22} \\)\n\n\n\njoyzepam\n\\( \\mu_{31} \\)\n\\( \\mu_{32} \\)\n\n\n\ntotal\n\n\n\n\n\n\n\n\n\n\n\n好的，那剩下的項目呢？例如，我們應該如何描述在這樣一個實驗中可能被給予Joyzepam的整個（假設的）人群的平均情緒提升，而不管他們是否接受了CBT治療？我們使用“點”符號來表示這一點。在Joyzepam的例子中，注意到我們正在討論表中第三行相關的均值。也就是說，我們將兩個單元格的均值（即\\(\\mu_{31}\\)和\\(\\mu_{32}\\)）求平均。這個求平均的結果被稱為邊際均值，並在這種情況下表示為\\(\\mu_3.\\)。CBT的邊際均值對應於表中第二列相關的母體均值，因此我們使用表示法，因為它是通過平均（邊際化2）兩者而得到的均值。因此，我們的整個母體均值表格可以寫成@tbl-tab13-4。\n\n\n\n\n\n\n表格 14.4: 因子表中母體和總體均值的符號表示法\n\n\n\nno therapy\nCBT\ntotal\n\n\nplacebo\n\\( \\mu_{11} \\)\n\\( \\mu_{12} \\)\n\\( \\mu_{1.} \\)\n\n\nanxifree\n\\( \\mu_{21} \\)\n\\( \\mu_{22} \\)\n\\( \\mu_{2.} \\)\n\n\njoyzepam\n\\( \\mu_{31} \\)\n\\( \\mu_{32} \\)\n\\( \\mu_{3.} \\)\n\n\ntotal\n\\( \\mu_{.1} \\)\n\\( \\mu_{.2} \\)\n\\( \\mu_{..} \\)\n\n\n\n\n\n\n\n\n現在我們有了這個表示法，很容易就可以形成和表達一些假設。假設目標是找出兩件事。首先，藥物的選擇是否對情緒有影響？其次，CBT 是否對情緒有影響？當然，這些不是我們可以制定的唯一假設，並且我們將在[因子 ANOVA 2：平衡設計，允許交互作用]一節中看到一個不同類型假設的非常重要示例，但這兩個假設是最簡單的檢驗，所以我們從這裡開始。考慮第一個檢驗。如果藥物沒有影響，那麼我們應該期望所有行均值相同，對吧？所以那就是我們的虛無假設。另一方面，如果藥物確實有關，那麼我們應該期望這些行均值不同。形式上，我們將虛無假設和替代假設表示為邊際均值的相等性：\n\\[\\text{虛無假設, } H_0 \\text{: 行均值相同，即 } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{替代假設, } H_1 \\text{: 至少有一個行均值不同}\\]\n值得注意的是，這些與我們在 章节 13 中對這些數據進行單因素 ANOVA 時形成的統計假設完全相同。當時我使用符號 \\(\\mu_{P}\\) 來表示安慰劑組的平均情緒增益，\\(\\mu_{A}\\) 和 \\(\\mu_{J}\\) 分別對應兩種藥物的組均值，並且虛無假設是 \\(\\mu_{P} = \\mu_{A} = \\mu_{J}\\)。所以我們實際上在談論相同的假設，只不過由於存在多個分組變量，更複雜的 ANOVA 需要更仔細的表示法，因此我們現在將此假設表示為 \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\)。然而，正如我們將很快看到的那樣，儘管假設相同，但由於我們現在承認了第二個分組變量的存在，對該假設的檢驗存在微妙的不同。\n談到其他分組變量，你可能不會感到驚訝地發現，我們的第二個假設檢驗也以相同的方式制定。然而，由於我們談論的是心理治療而不是藥物，我們的虛無假設現在對應於列均值的相等：\n\\[\\text{虛無假設, } H_0 \\text{: 列均值相同，即， } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{替代假設, } H_1 \\text{: 列均值不同，即， } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n\n14.1.2 使用jamovi完成多因子變異數分析\n我在上一節中描述的虛無假設和替代假設應該讓你感到非常熟悉。它們基本上與我們在 章节 13 中執行的更簡單的單因素ANOVA檢驗相同。所以你可能期望因子ANOVA中使用的假設檢驗本質上與 章节 13 中的F檢驗相同。您期望看到對平方和（SS）、平均平方（MS）、自由度（df）以及最終可以將之轉換為p值的F統計量的引用，對吧？好吧，你絕對是對的。 這麼多，以至於我要改變我的常規方法。在本書中，我通常採用先描述特定分析的基礎邏輯（在某程度上還有數學），然後再介紹jamovi中的分析的方法。這次我要相反地做，先告訴你如何在jamovi中執行它。這樣做的原因是我想強調 章节 13 中討論的簡單的單因素ANOVA工具，以及我們將在本章中使用的更複雜的方法之間的相似之處。\n如果您試圖分析的數據對應於平衡的因子設計，那麼執行方差分析就很容易。要了解它有多容易，讓我們先重現 章节 13 中的原始分析。以防你忘了，對於那個分析，我們只使用一個因素（即藥物）來預測我們的結果變量（即mood.gain），並且得到了 图 14.2 中顯示的結果。\n\n\n\n\n\n\n图 14.2: 藥物對 mood.gain 的單因子變異分析\n\n\n\n\n現在，假設我也好奇心理治療是否與mood.gain有關。根據我們在 章节 12 中對多元回歸的討論，你可能不會感到意外，我們所要做的就是在分析中將治療作為第二個”固定因素”，如 图 14.3 所示。 \n\n\n\n\n\n图 14.3: 藥物和心理治療對 mood.gain 的雙因子變異數分析\n\n\n\n\n這個輸出也很容易閱讀。表格的第一行報告了與藥物因素相關的組間平方和（SS）值，以及相應的組間 df 值。它還計算了平均平方（MS）、F統計量和p值。還有一行對應於心理治療因素和一行對應於殘差（即組內變異）。\n所有的單個量都非常熟悉，這些不同量之間的關係也保持不變，就像我們在原始單因素ANOVA中看到的一樣。注意，平均平方值是通過將\\(SS\\)除以相應的\\(df\\)來計算的。也就是說，無論我們談論的是藥物、治療還是殘差，都還是\n\\[MS=\\frac{SS}{df}\\]\n為了看到這一點，讓我們不要擔心平方和值是如何計算的。相反，讓我們相信 jamovi 已經正確計算了 \\(SS\\) 值，並嘗試驗證所有其他數字是否合理。首先，注意對於藥物因素，我們將 \\(3.45\\) 除以 \\(2\\)，得到平均平方值為 \\(1.73\\)。對於心理治療因素，只有1個自由度，所以我們的計算更簡單：將 \\(0.47\\)（\\(SS\\) 值）除以1，得到答案為 \\(0.47\\)（\\(MS\\) 值）。\n轉向 F 統計量和 p 值，注意我們有兩個；一個對應藥物因素，另一個對應心理治療因素。無論我們談論的是哪一個，F 統計量都是將與因素相關的平均平方值除以與殘差相關的平均平方值。如果我們用 “A” 作為簡寫符號來指代第一個因素（因素 A；在本例中是藥物），用 “R” 作為簡寫符號來指代殘差，那麼與因素 A 相關的 F 統計量表示為 \\(F_A\\)，並按如下方式計算：\n\\[F_A=\\frac{MS_A}{MS_R}\\]\n因素 B（即心理治療）也有等效公式。注意，這裡使用 “R” 來指代殘差有點尷尬，因為我們也用字母 R 來指代表格中的行數，但我只會在 SSR 和 MSR 的上下文中用 “R” 表示殘差，所以希望這不會令人困惑。無論如何，將這個公式應用於藥物因素，我們將平均平方值 1.73 除以殘差平均平方值 \\(0.07\\)，得到 F 統計量為 26.15。對於心理治療變量的相應計算將是將 \\(0.47\\) 除以 \\(0.07\\)，得到 \\(7.08\\) 作為 F 統計量。當然，這些值與 jamovi 在上面的 ANOVA 表中報告的值相同。\n同樣在 ANOVA 表中的是 p 值的計算。再次，這裡沒有什麼新鮮事物。對於我們的兩個因素，我們試圖做的是測試關於因素與結果變量之間沒有關係的虛無假設（稍後我將更加明確）。為此，我們（顯然）遵循了類似於單因素 ANOVA 的策略，為每個假設計算了一個 F 統計量。要將這些轉換為 p 值，我們只需要注意，在虛無假設下（即所謂因素無關）的 F 統計量的抽樣分佈是一個 F 分佈。還要注意，兩個自由度值分別對應於因素和殘差。對於藥物因素，我們談論的是具有 2 和 14 自由度的 F 分佈（稍後我將更詳細地討論自由度）。相反，對於心理治療因素，抽樣分佈是具有 1 和 14 自由度的 F 分佈。\n在這一點上，我希望您能看到，這個更複雜的因子分析的 ANOVA 表應該以與較簡單的單因素分析的 ANOVA 表相同的方式進行閱讀。簡而言之，它告訴我們，我們的 \\(3 \\times 2\\) 設計的因子 ANOVA 發現藥物的顯著效應（\\(F_{2,14} = 26.15, p &lt; .001\\)）以及心理治療的顯著效應（\\(F_{1,14} = 7.08, p = .02\\)）。或者，使用更技術正確的術語，我們會說藥物和心理治療有兩個主要效果。此刻，將這些稱為“主要”效果似乎有點多餘，但實際上是有道理的。稍後，我們將討論兩個因素之間可能存在的“交互作用”，因此我們通常區分主要效果和交互作用。\n\n\n\n\n14.1.3 計算多因子變異數分析的平方差總和\n在上一節中，我有兩個目標。首先，向您顯示執行因子 ANOVA 所需的 jamovi 方法與我們用於單因素 ANOVA 的方法幾乎相同。唯一的區別是添加了第二個因素。其次，我想向您展示在這種情況下 ANOVA 表的樣子，以便您從一開始就可以看到因子 ANOVA 背後的基本邏輯和結構與支撐單因素 ANOVA 的那些相同。試著抱著這種感覺。這是真實的，因為因子 ANOVA 的建立方式與更簡單的單因素 ANOVA 模型大致相同。只是一旦您開始挖掘細節，這種熟悉的感覺就會消失。傳統上，這種令人欣慰的感覺會被向統計教科書作者傾訴怒氣的衝動所替代。\n好的，讓我們先看看其中一些細節。上一節中的解釋表明了主效應（本例中為藥物和心理治療）的假設檢驗是 F 檢驗，但它沒有顯示如何計算平方和（SS）值。也沒有明確告訴您如何計算自由度（df 值），儘管相比之下這是一個簡單的事情。現在讓我們假設我們只有兩個預測變量，因子 A 和因子 B。如果我們用 Y 來表示結果變量，那麼我們可以用 Yrci 來表示與 rc 組的第 i 位成員相關的結果（即因子 A 的第 r 級/行和因子 B 的第 c 級/列）。因此，如果我們用 \\(\\bar{Y}\\) 表示樣本均值，我們可以使用與之前相同的表示法來表示組均值、邊際均值和總均值。也就是說，\\(\\bar{Y}_{rc}\\) 是與因子 A 的第 r 級和因子 B 的第 c 級相關的樣本均值：\\(\\bar{Y}_{r.}\\) 將是因子 A 的第 r 級的邊際均值，\\(\\bar{Y}_{.c}\\) 將是因子 B 的第 c 級的邊際均值，\\(\\bar{Y}_{..}\\) 是總體均值。換句話說，我們的樣本均值可以按照與母體均值相同的表格進行組織。對於我們的臨床試驗數據，該表格如@tbl-tab13-5所示。\n\n\n\n\n\n\n表格 14.5: 臨床試驗數據的樣本均值表示法\n\n\n\nno therapy\nCBT\ntotal\n\n\nplacebo\n\\( \\bar{Y}_{11} \\)\n\\( \\bar{Y}_{12} \\)\n\\( \\bar{Y}_{1.} \\)\n\n\nanxifree\n\\( \\bar{Y}_{21} \\)\n\\( \\bar{Y}_{22} \\)\n\\( \\bar{Y}_{2.} \\)\n\n\njoyzepam\n\\( \\bar{Y}_{31} \\)\n\\( \\bar{Y}_{32} \\)\n\\( \\bar{Y}_{3.} \\)\n\n\ntotal\n\\( \\bar{Y}_{.1} \\)\n\\( \\bar{Y}_{.2} \\)\n\\( \\bar{Y}_{..} \\)\n\n\n\n\n\n\n\n\n如果我們查看之前顯示的樣本均值，我們有 \\(\\bar{Y}_{11} = 0.30\\)，\\(\\bar{Y}_{12} = 0.60\\) 等。在我們的臨床試驗示例中，藥物因子有 3 個水平，治療因子有 2 個水平，因此我們試圖執行的是一個 \\(3 \\times 2\\) 因子方差分析。然而，我們將更一般地說，因子 A（行因子）有 R 個水平，因子 B（列因子）有 C 個水平，因此我們在這裡執行的是一個 \\(R \\times C\\) 因子方差分析。\n[額外的技術細節3]\n\n\n\n\n14.1.4 計算自由度的規則\n自由度的計算方式與單因素變異數分析非常相似。對於任何給定因子，自由度等於級別數減 1（即，行變量因子 A 的 \\(R - 1\\)，列變量因子 B 的 \\(C - 1\\)）。因此，對於藥物因子，我們得到 \\(df = 2\\)，對於療法因子，我們得到 \\(df = 1\\)。稍後，在我們討論將 ANOVA 解釋為回歸模型時（參見 章节 14.6），我將更清楚地說明我們如何得出此數字。但就目前而言，我們可以使用自由度的簡單定義，即自由度等於觀察到的數量數目減去約束數目。因此，對於藥物因子，我們觀察到 3 個單獨的組平均值，但這些受到 1 個總平均值的約束，因此自由度為 2。對於殘差，邏輯相似但不完全相同。我們實驗中的總觀察次數是 18。約束對應於 1 個總平均值，藥物因子引入的 2 個額外組平均值，以及療法因子引入的 1 個額外組平均值，因此我們的自由度為 14。作為公式，這是 \\(N - 1 - (R - 1) - (C - 1)\\)，簡化後是 \\(N - R - C + 1\\)。\n\n\n\n14.1.5 多因子與單因子變異數分析\n既然我們已經了解了因子變異數分析（factorial ANOVA）的運作方式，那麼花一點時間將其與單因素分析的結果進行比較是值得的，因為這將讓我們真正理解為什麼進行因子變異數分析是個好主意。在 章节 13 中，我進行了一個單因素變異數分析，以查看藥物之間是否存在差異，並進行了第二個單因素變異數分析，以查看療法之間是否存在差異。正如我們在 章节 14.1.1 節中看到的，單因素變異數分析所檢驗的零假設和對立假設實際上與因子變異數分析所檢驗的假設相同。更仔細地查看變異數分析表，我們可以看到，在兩種不同的分析中，與因子相關的平方和是相同的（藥物為 3.45，療法為 0.92），自由度也是相同的（藥物為 2，療法為 1）。但它們的答案並不相同！最值得注意的是，當我們在 章节 13.9 中對療法進行單因素變異數分析時，我們沒有發現顯著效應（p 值為 .21）。然而，當我們在兩因素變異數分析的背景下查看療法的主效應時，我們確實得到了顯著效應（p = .019）。這兩種分析顯然不同。\n為什麼會發生這種情況？答案在於理解殘差是如何計算的。回想一下 F 檢驗背後的整個概念是將可以歸因於特定因子的變異性與無法解釋的變異性（殘差）進行比較。如果您對療法進行單因素變異數分析，因此忽略了藥物的影響，那麼變異數分析將把所有藥物引起的變異性放入殘差中！這會使數據看起來比實際情況更加嘈雜，而在兩因素變異數分析中被正確認為顯著的療法效果現在變得不顯著。如果我們在試圖評估其他事物（例如，療法）的貢獻時忽略了實際上有意義的事物（例如，藥物），那麼我們的分析將受到扭曲。當然，如果我們記錄了墻壁的顏色，並且在三因素變異數分析中發現這是一個無關緊要的因素，那麼完全可以忽略它，僅報告不包括這個無關因素的更簡單的兩因素變異數分析。您不應該放棄那些確實有所作為的變數！\n\n總之，比較因子變異數分析與單因素變異數分析的結果可以使我們更好地理解為什麼進行因子變異數分析是一個好主意。當我們忽略了真正重要的變數時，分析將受到扭曲。因此，為了確保我們的分析更加準確，我們應該在分析中包括所有具有重要影響的變數。\n\n\n\n\n\n14.1.6 解讀多因子變異數分析的結果\n迄今為止我們討論的變異數分析模型涵蓋了我們可能在數據中觀察到的各種不同模式。例如，在兩因素變異數分析設計中有四種可能性：（a）只有因素A有意義，（b）只有因素B有意義，（c）A和B都有意義，（d）A和B都無意義。 ?fig-fig13-4中繪製了這四種可能性的示例。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "href": "14-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.2 平衡且有交互作用的因子設計分析",
    "text": "14.2 平衡且有交互作用的因子設計分析\n?fig-fig13-4中顯示的四種數據模式都非常現實。有很多數據集正好產生這些模式。然而，它們並非全部故事，到目前為止我們一直在談論的變異數分析模型並不足以充分解釋一個組均值表格。為什麼呢？嗯，到目前為止，我們可以討論藥物如何影響心情，以及治療如何影響心情，但無法談論兩者之間可能存在的交互作用。只有當因素\\(A\\)的效果因為我們討論的因素\\(B\\)的水平而有差異時，我們才說\\(A\\)和\\(B\\)之間存在交互作用。?fig-fig13-5中顯示了在$2 $ ANOVA環境下的幾個交互作用效果實例。舉一個更具體的例子，假設Anxifree和Joyzepam的運作受到完全不同的生理機制控制。這導致了一個結果，即儘管在接受治療的情況下，Joyzepam對心情的影響基本相同，但與CBT一起使用時，Anxifree實際上更有效。我們在上一節中開發的ANOVA無法捕捉到這個想法。要判斷此處是否確實存在交互作用，最好繪製各個組的均值。在jamovi中，這是通過ANOVA的“估計邊際均值”選項完成的——只需將藥物和治療移至“條款1”下的“邊際均值”框中。這應該看起來像@fig-fig13-6。我們的主要關注點與這兩條線不平行的事實有關。當藥物為Joyzepam（右側）時，CBT的效果（實線與虛線之間的差異）似乎接近零，甚至比使用安慰劑時（左側）的CBT效果還要小。然而，當給予Anxifree時，CBT的效果大於安慰劑（中間）。這種效果是真實的，還是僅僅由於機會引起的隨機變化？我們最初的ANOVA無法回答這個問題，因為我們根本不允許交互作用的存在！在本節中，我們將解決這個問題。\n\n\n\n\n\n\n\n图 14.4: 在沒有交互作用的情況下，\\(2 \\times 2\\) ANOVA的四種不同結果。在面板（a）中，我們看到因子A的主要效應以及因子B的無效應。面板（b）顯示因子B的主要效應，但因子A沒有影響。面板（c）顯示因子A和因子B的主要影響。最後，面板（d）顯示兩個因子都沒有影響。\n\n\n\n\n\n\n\n\n\n图 14.5: \\(2 \\times 2\\) ANOVA中質量上不同的交互作用\n\n\n\n\n\n\n\n\n\n图 14.6: 使用臨床試驗數據的ANOVA中，jamovi屏幕顯示如何生成描述性交互作用圖\n\n\n\n\n\n14.2.1 交互作用代表什麼？\n本節我們要介紹的關鍵概念是交互作用效應。在我們迄今為止討論的ANOVA模型中，我們的模型中只有兩個因素（即藥物和治療）。但是，當我們添加交互作用時，我們在模型中添加了一個新的組件：藥物和治療的組合。直觀地說，交互作用效應的概念相當簡單。這只是意味著因子A的效應會因為我們談論的因子B的不同水平而有所不同。但是，在我們的數據方面，這實際上意味著什麼呢？ ?fig-fig13-5中的圖表描述了幾種不同的模式，儘管它們彼此相當不同，但它們都被視為交互作用效應。因此，將這個質的概念轉化為統計學家可以使用的數學概念並不完全簡單。\n[附加技術細節4]\n\n\n\n\n14.2.2 交互作用的自由度\n將交互作用納入計算後，計算自由度變得稍微複雜一些。首先，讓我們考慮整個ANOVA模型。一旦我們在模型中包括交互效應，我們允許每個單獨組具有唯一的平均值，\\(mu_{rc}\\)。對於一個\\(R \\times C\\)的因子ANOVA，這意味著模型中有\\(R \\times C\\)個感興趣的數量，並且只有一個約束：所有組均值需要平均為總體均值。因此，整個模型需要有(\\(R \\times C\\)) - 1個自由度。但是因子A的主效應具有\\(R-1\\)個自由度，因子B的主效應具有\\(C-1\\)個自由度。這意味著與交互作用相關的自由度為\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\n這只是與行因子和列因子相關的自由度之積。\n那剩餘自由度呢？由於我們添加了吸收一些自由度的交互項，剩餘的自由度變得更少。具體來說，請注意，如果具有交互作用的模型總共有\\((R \\times C) - 1\\)，並且在數據集中有\\(N\\)個受1個總體均值約束的觀察值，那麼您的剩餘自由度現在變為\\(N-(R \\times C)-1+1\\)，或者只是\\(N-(R \\times C)\\)。\n\n\n\n14.2.3 使用jamovi完成多因子變異數分析\n在jamovi中將交互項添加到ANOVA模型非常簡單。實際上，這不僅簡單，而且是ANOVA的默認選項。這意味著當您為ANOVA指定兩個因子時，例如藥物和療法，則交互組件 - 藥物\\(\\times\\)療法 - 會自動添加到模型中5。當我們將交互項納入ANOVA運行後，我們將得到@fig-fig13-7中顯示的結果。\n\n\n\n\n\n\n图 14.7: 包括交互組件藥物\\(\\times\\)療法的完整因子模型結果\n\n\n\n\n結果顯示，儘管我們確實對藥物有顯著的主效應（\\(F_{2,12} = 31.7, p &lt; .001\\)）和療法類型（\\(F_{1,12} = 8.6, p = .013\\)），但兩者之間沒有顯著的交互作用（\\(F_{2,12} = 2.5, p = 0.125\\)）。\n\n\n\n14.2.4 解讀分析結果\n在解釋因子ANOVA結果時，有幾個非常重要的事情需要考慮。首先，我們在單因子ANOVA中遇到的相同問題，即如果您獲得（例如）藥物的顯著主效應，它並不能告訴您哪些藥物彼此有差異。要找出這個答案，您需要進行額外的分析。稍後我們將討論可以在[指定對比方式的不同方法]和[事後檢驗]中運行的一些分析。對於交互作用效果也是如此。知道有顯著的交互作用並不能告訴您存在哪種類型的交互作用。同樣，您需要進行額外的分析。\n其次，在獲得顯著的交互作用效果但沒有相應主效應的情況下，會出現非常奇特的解釋問題。有時候會發生這種情況。例如，在@fig-fig13-5 a中顯示的交叉互動中，這正是您會發現的情況。在這種情況下，主效應都不顯著，但交互作用效果顯著。這是一個難以解釋的情況，人們通常對此感到困惑。統計學家在這種情況下喜歡給出的一般建議是，當存在交互作用時，您不應該過多地關注主效應。他們這樣說的原因是，雖然從數學的角度看，主效應的檢驗完全有效，但是當存在顯著的交互作用效果時，主效應很少檢驗有趣的假設。回想一下@sec-What-hypotheses-are-we-testing，主效應的虛無假設是邊際均值彼此相等，邊際均值是由幾個不同組的平均值形成的。但是，如果您有一個顯著的交互作用效果，那麼您就知道組成邊際均值的組並不是同質的，所以真的不明顯為什麼您會關心那些邊際均值。\n以下是我的意思。再次以臨床實例為例。假設我們有一個\\(2 \\times 2\\)設計，比較了兩種不同的恐懼症治療方法（例如，系統性緩解法和淹沒療法），以及兩種不同的減輕焦慮藥物（例如，Anxifree和Joyzepam）。現在，假設我們發現當緩解法是治療方法時，Anxifree無效；當淹沒療法是治療方法時，Joyzepam無效。但對於另一種治療方法，兩者都相當有效。這是一個典型的交叉互動，當我們運行ANOVA時，我們會發現沒有藥物的主要效果，但有顯著的互動。那麼，說沒有主效應究竟意味著什麼呢？那意味著，如果我們平均兩種不同的心理治療方法，那麼Anxifree和Joyzepam的平均效果是相同的。但是，有誰會在意這個呢？在治療恐懼症時，從來沒有一個人可以使用“平均”的淹沒療法和緩解法進行治療。這並不合理。您要么得到一個，要么得到另一個。對於一種治療方法，一種藥物是有效的，對於另一種治療方法，另一種藥物是有效的。交互作用是重要的，而主效應在某種程度上是無關緊要的。\n這樣的事情經常發生。主效應是邊際均值的檢驗，當交互作用存在時，我們經常會發現自己對邊際均值不感興趣，因為它們意味著在交互作用告訴我們不應該取平均值的事物上取平均值！當然，並非總是在存在交互作用的情況下，主效應就毫無意義。經常出現的情況是，主效應很大，交互作用很小，這種情況下，您仍然可以說類似於“藥物A通常比藥物B更有效”（因為藥物效果很大），但您需要對其進行一些修改，添加“對於不同的心理治療，有效性差異有所不同。”無論如何，這裡的主要觀點是，每當您獲得顯著的交互作用時，您應該停下來思考主效應在這個語境中的真正意義。不要自動假設主效應是有趣的。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#變異數分析的效果量",
    "href": "14-Factorial-ANOVA.html#變異數分析的效果量",
    "title": "14  多因子變異數分析",
    "section": "14.3 變異數分析的效果量",
    "text": "14.3 變異數分析的效果量\n對於因子分析變異數分析（factorial ANOVA），效應量計算與單因素變異數分析中使用的非常相似（參見[效應量]部分）。具體來說，我們可以使用 \\(\\eta^2\\)（eta 平方）作為簡單衡量任何特定條款的整體效應大小的方法。與以前一樣，\\(\\eta^2\\)是通過將與該條款相關的平方和除以總平方和來定義的。例如，要確定因子A主效應的大小，我們將使用以下公式：\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\n與以前一樣，這可以以與回歸中的 \\(R^2\\) 類似的方式進行解釋。6 它告訴您由因子A的主效應解釋的結果變量變異的比例。因此，這是一個從0（完全沒有效果）到1（解釋結果變異的全部）的範圍內的數字。此外，所有\\(\\eta^2\\)值的總和，跨越模型中的所有條款，將總和為ANOVA模型的總\\(R^2\\)。例如，如果ANOVA模型完美適合（即，根本沒有組內變異！），則\\(\\eta^2\\)值將總和為1。當然，這在現實生活中很少（如果有的話）發生。\n然而，在進行因子分析變異數分析時，人們喜歡報告的效應量有第二種衡量方法，稱為部分 \\(\\eta^2\\)。部分 \\(\\eta^2\\)（有時表示為\\(p^{\\eta^2}\\) 或\\(\\eta_p^2\\)）背後的想法是，當衡量特定條款的效應量時（例如，因子A的主效應），您希望刻意忽略模型中的其他效應（例如，因子B的主效應）。也就是說，您想假設所有這些其他條款的效應都為零，然後計算\\(\\eta^2\\)值本來是什麼。這實際上很容易計算。您所要做的就是從分母中移除與其他條款相關的平方和。換句話說，如果您想要因子A主效應的部分\\(\\eta^2\\)，分母就是因子A和殘差的SS值之和。\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\n這將始終給您一個比\\(\\eta^2\\)更大的數字，這我猜想是部分\\(\\eta^2\\)受歡迎的原因。再次，您得到一個介於0和1之間的數字，其中0表示沒有影響。然而，解釋較大的部分\\(\\eta^2\\)值意味著什麼，這有點棘手。尤其是，您實際上無法比較不同條款的部分\\(\\eta^2\\)值！例如，假設根本沒有組內變異：如果是這樣，\\(SS_R = 0\\)。這意味著每個條款的部分\\(\\eta^2\\)值都是1。但這並不意味著模型中的所有條款都同樣重要，或者它們的大小相同。這只是意味著模型中所有條款的效應量相對於殘差變化都很大。它無法跨條款進行比較。\n要了解我的意思，查看具體示例非常有用。首先，讓我們看一下原始ANOVA（表格 14.6）中的效應量，無交互作用條款，來自@fig-fig13-3。\n\n\n\n\n\n\n表格 14.6: ANOVA模型未包括交互作用項目的效應量\n\n\n\neta.sq\npartial.eta.sq\n\n\ndrug\n0.71\n0.79\n\n\ntherapy\n0.10\n0.34\n\n\n\n\n\n\n\n\n首先觀察\\(\\eta^2\\)值，我們可以看到藥物解釋了心情改善的71%變異（即\\(\\eta^2 = 0.71\\)），而治療僅解釋了10%。這使得總共有19%的變異未被解釋（即，殘差佔結果變異的19%）。整體來說，這意味著我們有非常大的藥物效應[^factorial-anova-7]和適中的治療效應。\n[^factorial-anova-7]：我認為這個數值大得令人難以置信。這個數據集的人工特徵現在真的開始顯露出來了！\n現在讓我們看看部分\\(\\eta^2\\)值，如@fig-fig13-3所示。由於治療的效果並不是很大，因此對其進行控制並不會產生很大的差異，所以藥物的部分\\(\\eta^2\\)不會增加很多，我們得到一個值\\(p^{\\eta^2} = 0.79\\)。相反，因為藥物的效果非常大，對其進行控制會產生很大的差異，因此當我們計算治療的部分\\(\\eta^2\\)時，可以看到它上升到\\(p^{\\eta^2} = 0.34\\)。我們必須問自己的問題是，這些部分\\(\\eta^2\\)值實際上意味著什麼？我通常將因子A主效應的部分\\(\\eta^2\\)解釋為關於僅因子A變化的假設實驗的說明。所以，即使在這個實驗中我們改變了A和B，我們可以很容易地想像一個僅因子A變化的實驗，部分\\(\\eta^2\\)統計數據告訴您在該實驗中預期結果變量的多少變異會被解釋。然而，應該注意的是，這種解釋，就像許多與主效應相關的事物一樣，在存在大的和顯著的交互作用效應時沒有太多意義。\n說到交互作用效應，?tbl-tab13-7顯示了我們在包含交互作用條款的模型中計算效應大小時會得到什麼結果，如@fig-fig13-7所示。如您所見，主效應的\\(\\eta^2\\)值沒有改變，但部分\\(\\eta^2\\)值發生了變化：\n\n\n\n\n\n表格 14.7: ANOVA模型包含交互作用項目的效應大小\n\n\n\neta.sq\npartial.eta.sq\n\n\ndrug\n0.71\n0.84\n\n\ntherapy\n0.10\n0.42\n\n\ndrug*therapy\n0.06\n0.29\n\n\n\n\n\n\n\n\n\n14.3.1 估計組間平均\n在許多情況下，您可能會想要根據ANOVA結果報告所有組均值的估計以及與之相關的置信區間。您可以使用jamovi ANOVA分析中的“估計邊際均值”選項來執行此操作，如@fig-fig13-8所示。如果您運行的ANOVA是一個飽和模型（即，包含所有可能的主效應和所有可能的交互作用效應），那麼組均值的估計實際上與樣本均值相同，儘管置信區間將使用標準誤差的合併估計而不是為每個組使用單獨的估計。\n\n\n\n\n\n\n图 14.8: jamovi屏幕截圖顯示了飽和模型的邊際均值，即包括臨床試驗數據集中的交互作用分量\n\n\n\n\n在輸出中，我們看到安慰劑組無治療時的估計平均情緒增益為\\(0.300\\)，\\(95\\%\\)置信區間從\\(0.006\\)到\\(0.594\\)。請注意，由於ANOVA模型假定方差同質性並因此使用方差的合併估計，這些置信區間與您單獨為每個組計算的置信區間不同。\n當模型不包含交互作用項時，估計的組均值將與樣本均值不同。jamovi將根據邊際均值（即假定無交互作用）計算預期的組均值，而不是報告樣本均值。使用我們之前開發的符號，報告了μrc，對於（行）因子A上的第r級和（列）因子B上的第c級的均值，將是\\(\\mu_{..} + \\alpha_r + \\beta_c\\)。如果兩個因素之間確實沒有交互作用，那麼這實際上比原始樣本均值更好的估計了母體均值。通過jamovi ANOVA分析中的“Model”選項從模型中刪除交互作用項，為@fig-fig13-9所示的分析提供邊際均值。\n\n\n\n\n\n\n图 14.9: jamovi屏幕截圖顯示了未飽和模型的邊際均值，即未包括臨床試驗數據集中的交互作用分量"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "href": "14-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "title": "14  多因子變異數分析",
    "section": "14.4 檢核變異數分析的執行條件",
    "text": "14.4 檢核變異數分析的執行條件\n與單因素ANOVA一樣，因子ANOVA的關鍵假設是方差同質性（所有組具有相同的標準差）、殘差正態性和觀察值獨立性。前兩者是我們可以檢查的內容。對於第三點，您需要自己評估不同觀察值之間是否存在特殊關係，例如獨立變量是時間的重複測量，因此觀察值在時間1和時間2之間存在關係：不同時間點的觀察值來自同一個人。此外，如果您沒有使用飽和模型（例如，如果您省略了交互作用項），那麼您還假定省略的項不重要。當然，您可以通過運行包含省略項的ANOVA來檢查這最後一點，看看它們是否顯著，所以這非常簡單。那麼方差同質性和殘差正態性呢？事實證明，這些非常容易檢查。這與我們為單因素ANOVA所做的檢查沒有區別。\n\n\n14.4.1 變異數的同質性\n正如上一章 章节 13.6.1 中提到的，最好是通過視覺檢查標準差在不同組/類別之間的比較圖，並查看Levene檢驗是否與視覺檢查一致。Levene檢驗的理論在 ?sec-Checking-the-homogeneity-of-variance-assumption中 討論過，所以我不再討論。該檢驗要求您使用一個飽和模型（即，包含所有相關條款），因為該檢驗主要關注的是組內方差，而不是使用與完整模型相關的其他方法計算。在jamovi中，可以在ANOVA ‘Assumption Checks’ - ’Homogeneity Tests’選項下指定Levene檢驗，結果如 图 14.10 所示。Levene檢驗的非顯著性意味著，只要與標準差圖的視覺檢查一致，我們就可以放心地假定方差同質性假設沒有被違反。\n\n\n\n14.4.2 殘差的常態性\n與單因素ANOVA一樣，我們可以用直接的方式測試殘差的正態性（參見 章节 13.6.4 )。然而，通常最好是使用QQ圖以圖形方式檢查殘差。請參見 图 14.10 。\n\n\n\n\n\n\n图 14.10: 檢查ANOVA模型的假設"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#各種多重比較方案",
    "href": "14-Factorial-ANOVA.html#各種多重比較方案",
    "title": "14  多因子變異數分析",
    "section": "14.7 各種多重比較方案",
    "text": "14.7 各種多重比較方案\n在上一節中，我向您展示了一種將因子轉換為對比組合的方法。在我向您展示的方法中，我們指定了一組二進制變量，其中我們定義了一個類似於 表格 14.11 的表格。\n\n\n\n\n\n\n表格 14.11: 二進制對比以區分所有三種可能的藥物\n\n\ndrug\ndruganxifree\ndrugjoyzepam\n\n\n\"placebo\"\n0\n0\n\n\n\"anxifree\"\n1\n0\n\n\n\"joyzepam\"\n0\n1\n\n\n\n\n\n\n\n\n表格中的每一行對應於因子水平之一，每一列對應於對比之一。這個表格，始終比列多一行，有一個特殊的名稱。它被稱為對比矩陣。然而，有很多不同的方法可以指定對比矩陣。在本節中，我討論了統計學家使用的一些標準對比矩陣以及如何在 jamovi 中使用它們。如果您打算稍後閱讀[因子變異數分析 3：不平衡設計]一節，那麼仔細閱讀本節是值得的。如果沒有，您可以瀏覽一下它，因為對於平衡設計來說，對比的選擇並不重要。\n\n\n\n14.7.1 比較操作效果\n在我上面描述的這種對比中，因子的一個水平是特殊的，並作為一種 “基線” 類別（例如，在我們的示例中是安慰劑），其他兩個則是根據這個基線來定義的。這種對比的名稱是治療對比，也稱為 “哑变量编码”。在此對比中，因子的每個水平都與基本參考水平進行比較，基本參考水平是截距的值。\n這個名字反映了這樣一個事實，當你的因子中的一個類別確實很特殊，因為它確實代表了基線時，這些對比是非常自然和合理的。這在我們的臨床試驗示例中是有道理的。安慰劑條件對應於不給人們使用任何真正的藥物的情況，因此它是特殊的。其他兩個條件是相對於安慰劑定義的。在一種情況下，您用 Anxifree 替換安慰劑，而在另一種情況下，您用 Joyzepam 替換安慰劑。\n上面顯示的表格是具有3個水平的因子的治療對比矩陣。但是，如果我想要一個具有5個水平的因子的治療對比矩陣呢？您可以像 表格 14.12 那樣排列它。\n\n\n\n\n\n\n表格 14.12: 具有5個水平的治療對比矩陣\n\n\nLevel\n2\n3\n4\n5\n\n\n1\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n\n\n5\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n在這個例子中，第一個對比是第2級與第1級比較，第二個對比是第3級與第1級比較，依此類推。請注意，默認情況下，因子的第一水平始終被視為基線類別（即，它是所有零的那個，並且沒有與之相關的顯式對比）。在 jamovi 中，您可以通過操作 “數據變量” 窗口中顯示的變量的水平來更改哪個類別是因子的第一個水平（雙擊電子表格列中的變量名稱以彈出 “數據變量” 視圖。\n\n\n\n14.7.2 Helmert 比較法\n治療對比在很多情況下都很有用。然而，它們在真正存在基線類別的情況下最有意義，並且您希望根據該類別評估所有其他組。然而，在其他情況下，可能不存在這樣的基線類別，與其將每個組與其他組的平均值進行比較可能更有意義。這就是我們遇到 Helmert 對比的地方，由 jamovi ‘ANOVA’ - ‘Contrasts’ 選擇框中的 ‘helmert’ 選項生成。Helmert 對比背後的想法是將每個組與 “前一個” 組的平均值進行比較。也就是說，第一個對比表示第2組和第1組之間的差異，第二個對比表示第3組與第1組和第2組的平均值之間的差異，依此類推。對於具有五個水平的因子，這轉換為看起來像 表格 14.13 的對比矩陣。\n\n\n\n\n\n表格 14.13: 具有5個水平的 helmert 對比矩陣\n\n\n1\n-1\n-1\n-1\n-1\n\n\n2\n1\n-1\n-1\n-1\n\n\n3\n0\n2\n-1\n-1\n\n\n4\n0\n0\n3\n-1\n\n\n5\n0\n0\n0\n4\n\n\n\n\n\n\n\n\nHelmert 對比的一個有用之處是每個對比都加起來為零（即，所有列加起來為零）。這導致了當我們將 ANOVA 解釋為回歸時，如果我們使用 Helmert 對比，截距項對應於大平均數 \\(\\mu_{..}\\)。將其與治療對比進行比較，在治療對比中，截距項對應於基線類別的組平均值。這個性質在某些情況下可能非常有用。如果您有一個平衡設計，這並不太重要，到目前為止，我們一直在這樣假設，但是當我們考慮不平衡設計時，它將變得重要。事實上，我甚至麻煩包括這一部分的主要原因是，如果您想了解不平衡 ANOVA，對比變得很重要。\n\n\n\n14.7.3 簡單比較\n第三個選項是我應該簡要提及的 “總和至零” 對比，在 jamovi 中稱為 “簡單” 對比，它們用於構建組間的兩兩比較。具體來說，每個對比都編碼了某個組與基線類別之間的差異，這種情況下基線類別對應於第一組 (表格 14.14)。\n\n\n\n\n\n表格 14.14: 具有5個水平的 ‘總和至’ 零對比矩陣\n\n\n1\n-1\n-1\n-1\n-1\n\n\n2\n1\n0\n0\n0\n\n\n3\n0\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n\n\n5\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n與 Helmert 對比非常相似，我們看到每一列的和都是零，這意味著當 ANOVA 被視為回歸模型時，截距項對應於整體平均值。在解釋這些對比時，需要認識到的是，每個對比都是第1組與其他四個組之間的兩兩比較。具體來說，對比1對應於 “第2組減去第1組” 的比較，對比2對應於 “第3組減去第1組” 的比較，依此類推。7\n\n\n\n14.7.4 jamovi的各種比較選項\njamovi 還提供了多種可以在 ANOVA 中生成不同類型對比的選項。這些可以在主要的 ANOVA 分析窗口的 ‘對比’ 選項中找到，其中 表格 14.15 列出了對比類型：\n\n\n\n\n\n表格 14.15: 在 jamovi ANOVA 分析中可用的對比類型\n\n\nContrast type\n\n\n\nDeviation\nCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\n\nSimple\nLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\n\nDifference\nCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\n\nHelmert\nCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\n\nRepeated\nCompares the mean of each level (except the last) to the mean of the subsequent level\n\n\nPolynomial\nCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#不平衡的因子設計分析",
    "href": "14-Factorial-ANOVA.html#不平衡的因子設計分析",
    "title": "14  多因子變異數分析",
    "section": "14.10 不平衡的因子設計分析",
    "text": "14.10 不平衡的因子設計分析\n因子ANOVA是一個非常方便的工具。它已經成為分析實驗數據的標準工具之一，已有數十年的歷史，你會發現在心理學上，你無法閱讀超過兩三篇論文而不在其中某個地方遇到ANOVA。然而，在很多真實的科學文章中，你將會看到的ANOVA和我迄今為止描述的ANOVA之間有一個巨大的差異。在現實生活中，我們很少有幸擁有完美平衡的設計。出於某種原因，通常會在某些單元中得到比其他單元更多的觀察結果。換句話說，我們有一個不平衡的設計。\n不平衡設計需要比平衡設計更加謹慎地處理，支撐它們的統計理論也更為混亂。這種混亂可能是結果，也可能是時間短缺，但我的經驗是心理學本科研究方法課程有一個令人討厭的傾向，那就是完全忽略這個問題。很多統計教科書也容易忽略它。我認為，這導致了很多領域內的在職研究人員實際上並不知道存在幾種不同類型的不平衡ANOVA，而它們產生的答案相差很大。事實上，閱讀心理學文獻時，我對大多數報告不平衡因子ANOVA結果的人實際上無法提供足夠的詳細信息來重現分析感到驚訝。我暗自懷疑，大多數人甚至沒有意識到他們的統計軟件包正在代替他們做出大量實質性的數據分析決策。當你想到這一點時，它實際上是有點恐怖的。因此，如果你想避免將數據分析的控制權交給愚蠢的軟件，請繼續閱讀。\n\n\n14.10.1 咖啡飲用資料\n跟往常一樣，使用一些數據將對我們有所幫助。coffee.csv 文件包含了一個產生不平衡 \\(3 \\times 2\\) ANOVA 的假設數據集。假設我們有興趣了解人們在喝太多咖啡時胡言亂語的趨勢是否純粹是咖啡本身的影響，還是人們在咖啡中加入牛奶和糖所產生的影響。假設我們找了18個人，給他們喝了一些咖啡。咖啡/咖啡因的含量保持恆定，我們改變是否加入牛奶，所以牛奶是一個有兩個水平的二元因子，分別是“是”和“否”。我們還改變了涉及的糖的種類。咖啡中可能含有“真正”的糖，或者可能含有“假”的糖（即人工甜味劑），或者可能根本不含糖，所以糖變量是一個有三個水平的因子。我們的結果變量是一個連續變量，這可能意味著某個心理上有意義的衡量某人“胡言亂語”程度的指標。對於我們的目的，細節並不真正重要。查看 jamovi 試算表視圖中的數據，如 图 14.26。\n\n\n\n\n\n\n图 14.26: coffee.csv數據集在 jamovi 中，描述性信息按因子水平匯總\n\n\n\n\n查看 图 14.26 中的平均值表，我們可以強烈感受到各組之間的差異。與 babble 變量的標準偏差相比，這一點尤其明顯。在各組中，這個標準偏差從 .14 到 .71 不等，這相對於組間平均值的差異來說相當小。9 雖然這一開始看起來像是一個簡單的因子ANOVA，但當我們查看每個組中有多少個觀察值時，問題就出現了。參見 图 14.26 中顯示的不同組別的不同 N 值。這違反了我們最初的假設，即每個組中的人數是相同的。我們還沒有真正討論如何處理這種情況。\n\n\n\n\n14.10.2 不平衡設計不適用「標準變異數分析」\n不平衡設計讓我們發現，實際上並不存在我們可能稱之為標準 ANOVA 的任何一種事物。事實上，你可能希望在不平衡設計中以三種根本不同的方式 10來運行 ANOVA。如果您有一個平衡設計，這三個版本都會產生相同的結果，與我在本章開始時給出的公式一致的平方和、F 值等。然而，當您的設計不平衡時，它們的答案並不相同。此外，它們對於每種情況的適用程度也不盡相同。有些方法對您的情況可能更適用。鑒於此，了解不同類型的 ANOVA 及其相互之間的差異非常重要。\n第一種 ANOVA 通常被稱為第一類平方和。我敢肯定你能猜到其他兩個叫什麼。名稱中的“平方和”部分是由 SAS 統計軟件包引入的，並已成為標準術語，但在某些方面有點誤導。我認為把它們稱為不同類型的平方和的邏輯是，當你看到它們產生的 ANOVA 表時，數字之間的關鍵區別是 SS 值。自由度沒有變化，MS 值仍然被定義為 SS 除以 df 等。然而，這種術語的錯誤之處在於它掩蓋了 SS 值之間為何會有差異的原因。為此，了解三種不同類型的 ANOVA 作為三種不同的假設檢驗策略要有幫助得多。這些不同的策略確實導致了不同的 SS 值，但這裡重要的是策略，而不是 SS 值本身。回想一下[ANOVA 作為線性模型]一節，任何特定的 F 檢驗最好被認為是兩個線性模型之間的比較。因此，當您查看 ANOVA 表時，請記住每個 F 檢驗對應於要比較的模型對。當然，這自然引出了要比較哪對模型的問題。這就是 ANOVA 類型 I、II 和 III 之間的根本區別：每一種都對應於為檢驗選擇模型對的不同方式。\n\n\n\n\n14.10.3 第一型平方差總和\n類型 I 方法有時被稱為”序列”平方和，因為它涉及一次添加一個術語到模型的過程。以 coffee 數據為例。假設我們要運行完整的 \\(3 \\times 2\\) 因子 ANOVA，包括交互作用術語。完整的模型包含了結果變量 babble，預測變量 sugar 和 milk，以及交互術語 sugar \\(\\times\\) milk。這可以寫成 \\(babble \\sim sugar + milk + sugar {\\times} milk\\)。類型 I 策略會按順序構建這個模型，從最簡單的模型開始，逐步添加術語。\n數據的最簡單模型將是一個假設牛奶和糖都不影響胡言亂語的模型。這樣的模型只包括截距，寫成 babble ~ 1。這是我們最初的零假設。數據的下一個最簡單模型將是其中只包含兩個主效應之一的模型。在 coffee 數據中，這裡有兩個不同的可能選擇，因為我們可以選擇先添加 milk 或者先添加 sugar。實際上，順序是很重要的，我們稍後會看到，但現在讓我們只是隨意選擇一個，選擇 sugar。所以，我們模型序列中的第二個模型是 babble ~ sugar，它形成了我們第一次檢驗的替代假設。我們現在有了我們的第一個假設檢驗（表格 14.16）。\n\n\n\n\n\n表格 14.16: 使用結果變量 ‘babble’ 的零假設和替代假設。\n\n\nNull model:\n\\(babble \\sim 1\\)\n\n\nAlternative model:\n\\(babble \\sim sugar\\)\n\n\n\n\n\n\n\n\n這個比較形成了我們對糖的主效應的假設檢驗。我們模型建立練習的下一步是添加另一個主效應術語，因此我們序列中的下一個模型是 babble ~ sugar + milk。然後，通過比較以下模型對（表格 14.17）形成第二個假設檢驗。\n\n\n\n\n\n表格 14.17: 使用結果變量 ‘babble’ 的進一步零假設和替代假設\n\n\nNull model:\n\\(babble \\sim sugar\\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n\n\n\n這個比較形成了我們對牛奶主效應的假設檢驗。在某種意義上，這種方法非常優雅：第一次測試的替代假設形成了第二次測試的零假設。正是在這個意義上，類型 I 方法是嚴格有序的。每個測試都直接基於上一個測試的結果。然而，在另一個意義上，它非常不優雅，因為這兩個測試之間有很強的不對稱性。糖的主效應測試（第一次測試）完全忽略了牛奶，而牛奶的主效應測試（第二次測試）確實考慮了糖。無論如何，我們序列中的第四個模型現在是完整的模型，babble ~ sugar + milk + sugar \\(\\times\\) milk，相應的假設檢驗顯示在 表格 14.18 中。\n\n\n\n\n\n表格 14.18: 使用結果變量 ‘babble’ 的更多可能的零假設和替代假設\n\n\nNull model:\n\\(babble \\sim sugar + milk\\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk + sugar * milk \\)\n\n\n\n\n\n\n\n\n類型 III 平方和是 jamovi ANOVA 使用的默認假設檢驗方法，因此要運行類型 I 平方和分析，我們必須在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘平方和’ 選擇框中選擇 ‘Type 1’。這給我們提供了 图 14.27 中顯示的 ANOVA 表。\n\n\n\n\n\n\n\n图 14.27: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表\n\n\n\n\n使用類型 I 平方和的最大問題是它確實取決於您輸入變量的順序。然而，在許多情況下，研究人員無需優先考慮一種排序而不考慮另一種排序。這可能是我們牛奶和糖問題的情況。我們應該先添加牛奶還是糖？在數據分析問題中，這與製作咖啡問題一樣隨意。事實上，可能有一些人對排序有堅定的看法，但很難想像這個問題有一個原則性的答案。然而，當我們改變順序時，看看會發生什麼，如 图 14.28 中所示。\n\n\n\n\n\n\n图 14.28: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表，但因子以不同的順序（首先是牛奶）輸入\n\n\n\n\n兩個主效應術語的 p 值都發生了變化，而且變化相當大。在其他方面，牛奶的效果已經顯著（儘管我之前提到過，人們應該避免對此得出任何強烈的結論）。應該報告這兩個 ANOVA 中的哪一個？這並不是立即明顯的。\n當您查看用於定義“第一”主效應和“第二”主效應的假設檢驗時，很明顯它們之間有質的不同。在我們最初的示例中，我們看到了糖的主效應檢驗完全忽略了牛奶，而牛奶的主效應檢驗確實考慮了糖。因此，類型 I 檢驗策略確實將第一個主效應視為在第二個主效應之上具有某種理論優越性。根據我的經驗，很少（甚至從未）有這種理論優越性，可以證明將任何兩個主效應非對稱地對待。\n所有這些的結果是類型 I 測試很少有趣，所以我們應該繼續討論類型 II 測試和類型 III 測試。\n\n\n\n14.10.4 第三型平方差總和\n剛剛談完類型 I 測試後，您可能會認為接下來自然要談論類型 II 測試。然而，我認為在談論類型 II 測試（比較棘手）之前，先討論類型 III 測試（簡單且是 jamovi ANOVA 的默認設置）實際上更自然。類型 III 測試背後的基本思想非常簡單。無論您要評估哪個術語，都要運行 F 檢驗，其中替代假設對應用戶指定的完整 ANOVA 模型，而零模型僅刪除您正在測試的那一個術語。例如，在咖啡示例中，我們的完整模型是 babble ~ sugar + milk + sugar × milk，糖的主效應檢驗將對應於以下兩個模型之間的比較（表格 14.19）。\n\n\n\n\n\n表格 14.19: 以’babble’作為結果變量的零假設和替代假設，使用類型 III 平方和\n\n\nNull model:\n\\(babble \\sim milk + sugar * milk\\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk +sugar * milk \\)\n\n\n\n\n\n\n\n\n同樣，通過將完整模型與刪除牛奶術語的零模型進行檢驗，可以評估牛奶的主效應，如 表格 14.20。\n\n\n\n\n\n表格 14.20: 以’babble’作為結果變量的更多零假設和替代假設，使用類型 III 平方和\n\n\nNull model:\n\\(babble \\sim sugar + sugar * milk\\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk +sugar * milk \\)\n\n\n\n\n\n\n\n\n最後，用完全相同的方式評估糖 × 牛奶的交互項。再次，我們將完整模型與刪除糖 × 牛奶交互項的零模型進行比較，如 表格 14.21。\n\n\n\n\n\n表格 14.21: 從以“babble”為結果變量的假設中刪除交互項，使用類型 III 平方和\n\n\nNull model:\n\\(babble \\sim sugar + milk\\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk +sugar * milk \\)\n\n\n\n\n\n\n\n\n基本思想可以推廣到更高階 ANOVA。例如，假設我們嘗試運行一個具有三個因素 A、B 和 C 的 ANOVA，並且我們希望考慮所有可能的主效應和所有可能的交互作用，包括三者之間的交互作用 A × B × C。(表格 14.22)為您展示了這種情況下類型 III 測試的外觀。\n\n\n\n\n\n表格 14.22: 具有三個因素和所有主效應和交互項的類型 III 測試\n\n\nTerm being tested is\nNull model is outcome ~ ...\nAlternative model is outcome ~ ...\n\n\nA\n\\(B + C + A*B + A*C + B*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\nB\n\\(A + C + A*B + A*C + B*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\nC\n\\(A + B + A*B + A*C + B*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\nA*B\n\\(A + B + C + A*C + B*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\nA*C\n\\(A + B + C + A*B + B*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\nB*C\n\\(A + B + C + A*B + A*C + A*B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\nA*B*C\n\\(A + B + C + A*B + A*C + B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n\n\n\n儘管該表看起來很難看，但它相當簡單。在所有情況下，替代假設對應於包含三個主效應術語（例如 A）、三個雙向交互作用（例如 A * B）和一個三向交互作用（即 A * B * C）的完整模型。零模型總是包含其中 7 項中的 6 項，而缺少的一項就是我們正在嘗試測試其顯著性的那一項。\n初步看來，類型 III 測試似乎是一個好主意。首先，我們消除了在運行類型 I 測試時遇到問題的不對稱性。而且，由於我們現在以相同的方式對待所有術語，假設測試的結果不依賴於我們指定它們的順序。這絕對是一件好事。然而，在解釋測試結果，特別是主效應術語時存在一個大問題。考慮咖啡數據。假設根據類型 III 測試，牛奶的主要效果並不顯著。這告訴我們的是，相比於完整模型，babble ~ sugar + sugar * milk 是數據的更好模型。但這意味著什麼呢？如果糖 * 牛奶的交互項也不顯著，我們會想得出結論說數據告訴我們唯一重要的事情是糖。但是，假設我們有一個顯著的交互項，但是牛奶的主效應不顯著。在這種情況下，我們是否應該認為真的有一個“糖的效果”，一個“牛奶和糖之間的交互作用”，但沒有“牛奶的效果”？那看起來很瘋狂。正確的答案簡直一定是，在交互作用顯著的情況下，談論主效應是毫無意義的11。一般來說，這似乎是大多數統計學家給我們的建議，而我認為這是正確的建議。但是，如果談論存在顯著交互作用的非顯著主效應確實是毫無意義的，那麼類型 III 測試應該允許零假設依賴於一個包括交互作用但省略了其中一個主效應的模型就不是很明顯了。以這種方式表述的零假設實際上根本沒有什麼意義。\n稍後，我們將看到類型 III 測試在某些情況下可以挽救，但首先讓我們看一下使用類型 III 和平方的 ANOVA 結果表，見 图 14.29。\n\n\n\n\n\n\n\n图 14.29: 在 jamovi 中使用類型 III 和平方的 ANOVA 結果表\n\n\n\n\n但要注意，類型 III 測試策略的一個怪異特徵是，通常結果依賴於用於編碼因子的對比（如果您忘記了不同類型對比是什麼，請參見 [指定對比的不同方式]一節）。12\n那麼，如果類型 III 分析（但不是在 jamovi 中）通常產生的 p 值對對比的選擇非常敏感，那是否意味著類型 III 測試本質上是任意的，不值得信任？在某種程度上，這是真的，當我們轉向討論類型 II 測試時，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n在類型 II 測試中，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n\n\n\n14.10.5 第二型平方差總和\n好的，到目前為止，我們已經看過了類型 I 和 III 測試，兩者都非常簡單。類型 I 測試是通過逐一添加條款進行的，而類型 III 測試是通過使用完整模型並檢查在刪除每個條款時會發生什麼來執行的。然而，這兩者都可能有一些局限性。類型 I 測試取決於您輸入條款的順序，而類型 III 測試則取決於您如何編碼對比。類型 II 測試描述起來稍微困難一些，但它們避免了這兩個問題，因此解釋起來稍微容易一些。\n類型 II 測試與類型 III 測試大致相似。從一個“完整”模型開始，通過從該模型中刪除特定條款來進行測試。然而，類型 II 測試是基於邊際性原則的，該原則規定如果您的模型中有任何依賴於較低階條款的較高階條款，則不應從模型中省略較低階條款。所以，例如，如果您的模型包含兩個因素的交互作用 A × B（二階條款），那麼它確實應該包含主效應 A 和 B（一階條款）。同樣，如果它包含三個因素的交互作用項 A × B × C，那麼模型還必須包括主效應 A、B 和 C 以及簡單的交互作用 A × B、A × C 和 B × C。類型 III 測試經常違反邊際性原則。例如，考慮在包含所有可能交互作用條款的三因子 ANOVA 中測試 A 的主效應。根據類型 III 測試，我們的零假設和對立假設在 表格 14.23 中。\n\n\n\n\n\n表格 14.23: 類型 III 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試\n\n\nNull model:\n\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\n\nAlternative model:\n\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\n\n\n\n注意，零假設省略了 A，但將 A × B、A × C 和 A × B × C 作為模型的一部分。根據類型 II 測試，這並不是一個好的零假設選擇。相反，如果我們希望檢驗 A 對結果無關的零假設，我們應該指定一個不依賴於 A 的任何形式（即使是交互作用）的最復雜的模型。對立假設對應於該零模型加上 A 的主效應項。這個概念更接近大多數人對 “A 的主效應” 的直觀理解，並且得出了 A 的主效應的類型 II 測試（表格 14.24）。13\n\n\n\n\n\n表格 14.24: 類型 II 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試\n\n\nNull model:\n\\(outcome \\sim B + C + B*C\\)\n\n\nAlternative model:\n\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\n\n\n\n無論如何，僅為了讓您了解類型 II 測試是如何進行的，這是在三因子因素分析中應用的完整測試表格（表格 14.25）：\n\n\n\n\n\n\n表格 14.25: 三因子因素模型的類型 II 測試\n\n\nTerm being tested is\nNull model is outcome ~ ...\nAlternative model is outcome ~ ...\n\n\nA\n\\(B + C + B*C \\)\n\\(A + B + C + B*C \\)\n\n\nB\n\\(A + C + A*C \\)\n\\(A + B + C + A*C\\)\n\n\nC\n\\(A + B + A*B \\)\n\\(A + B + C + A*B\\)\n\n\nA*B\n\\(A + B + C + A*C + B*C \\)\n\\(A + B + C + A*B + A*C + B*C \\)\n\n\nA*C\n\\(A + B + C + A*B + B*C \\)\n\\(A + B + C + A*B + A*C + B*C \\)\n\n\nB*C\n\\(A + B + C + A*B + A*C \\)\n\\(A + B + C + A*B + A*C + B*C \\)\n\n\nA*B*C\n\\(A + B + C + A*B + A*C + B*C \\)\n\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n\n\n\n在我們一直在咖啡數據中使用的雙因子 ANOVA 的背景下，假設檢驗更為簡單。糖的主效應對應於比較這兩個模型的 F 檢驗（表格 14.26）。\n\n\n\n\n\n表格 14.26: 咖啡數據中糖主效應的類型 II 測試\n\n\nNull model:\n\\(babble \\sim milk \\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n\n\n\n對牛奶主效應的檢驗位於 表格 14.27。\n\n\n\n\n\n表格 14.27: 咖啡數據中牛奶主效應的類型 II 測試\n\n\nNull model:\n\\(babble \\sim sugar \\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n\n\n\n最後，糖與牛奶交互作用的檢驗位於 表格 14.28。\n\n\n\n\n\n表格 14.28: 以類型 II方法分析糖與牛奶交互作用\n\n\nNull model:\n\\(babble \\sim sugar + milk \\)\n\n\nAlternative model:\n\\(babble \\sim sugar + milk + sugar*milk \\)\n\n\n\n\n\n\n\n\n運行測試再次很簡單。只需在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘Sum of squares’ 選擇框中選擇 ‘Type 2’，這將給我們提供 ANOVA 表格，如 图 14.30 所示。\n\n\n\n\n\n\n\n图 14.30: 在 jamovi 中使用類型 II 平方和的 ANOVA 結果表\n\n\n\n\n類型 II 測試比類型 I 和類型 III 測試具有一些明顯的優勢。它們不依賴於指定因子的順序（與類型 I 不同），也不依賴於用於指定因子的對比（與類型 III 不同）。雖然對於最後一點意見可能會有所不同，而且這肯定取決於你想用你的數據做什麼，但我認為它們指定的假設檢驗更有可能對應於你真正關心的事物。因此，我發現解釋類型 II 測試的結果通常比解釋類型 I 或類型 III 測試的結果更容易。基於這個原因，我的初步建議是，如果您無法想出任何直接映射到研究問題的明顯模型比較，但仍想在不平衡設計中運行 ANOVA，類型 II 測試可能是比類型 I 或類型 III 更好的選擇。14\n\n\n\n14.10.6 效果量(還有非加成性平方差總和)\njamovi 在您選擇這些選項時還會提供效應大小 \\(\\eta^2\\) 和部分 \\(\\eta^2\\)，如 图 14.30。然而，在不平衡設計中，涉及的額外複雜性有所增加。\n如果您回顧我們對 ANOVA 的早期討論，其中一個關鍵想法是在平方和計算的背後，如果我們把所有與模型中的效應相關的 SS 項加起來，再加上殘差 SS，它們應該加起來等於總平方和。此外，\\(\\eta^2\\) 背後的整個想法是，因為您將某個 SS 項除以總 SS 值，\\(\\eta^2\\) 值可以解釋為由特定項解釋的變異比例。但在不平衡設計中，這並不那麼簡單，因為有些變異會”丟失”。\n起初這似乎有點奇怪，但原因是這樣的。當您擁有不平衡設計時，您的因子會相互關聯，因此很難分辨因子 A 的效應和因子 B 的效應。在極端情況下，假設我們進行了一個 \\(2 \\times 2\\) 設計，每個組中的參與者人數如 表格 14.29。\n\n\n\n\n\n表格 14.29: 2 x 2 非常（非常！）不平衡的因子設計中的 N 參與者\n\n\n\nsugar\nno sugar\n\n\nmilk\n100\n0\n\n\nno milk\n0\n100\n\n\n\n\n\n\n\n\n在這裡，我們有一個非常不平衡的設計：100人有牛奶和糖，100人沒有牛奶和糖，就是這樣。有0人有牛奶沒有糖，也有0人有糖沒有牛奶。現在假設，當我們收集數據時，發現”牛奶和糖”組與”無牛奶無糖”組之間存在很大（並且具有統計顯著性）的差異。這是糖的主要效應嗎？牛奶的主要效應？還是交互作用？這是不可能知道的，因為糖的存在與牛奶的存在具有完美的關聯。現在假設設計稍微平衡一些（表格 14.30）。\n\n\n\n\n\n表格 14.30: 2 x 2 仍然非常不平衡的因子設計中的 N 參與者\n\n\n\nsugar\nno sugar\n\n\nmilk\n100\n5\n\n\nno milk\n5\n100\n\n\n\n\n\n\n\n\n這次，從技術上講，可以區分牛奶和糖的效應，因為我們有一些人只擁有其中之一。然而，由於糖和牛奶之間的關聯仍然非常強烈，兩個小組中的觀察值非常少，因此進行區分仍然相當困難。再次，我們很可能處於這樣一種情況：我們知道預測變量（牛奶和糖）與結果（發聲）相關，但我們不知道這種關係的本質是一個或另一個預測因子的主要效應，還是交互作用。\n在不平衡設計的背景下，當糖和牛奶的效應相互關聯時，應該選擇 Type II ANOVA 進行分析，因為它更易於解釋並可以避免因變量順序或對比方式帶來的問題。無論您是使用什麼軟件，都應該在報告結果時說明您使用的是哪種類型的 ANOVA 測試，以確保分析結果的準確性。"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#本章小結",
    "href": "14-Factorial-ANOVA.html#本章小結",
    "title": "14  多因子變異數分析",
    "section": "14.11 本章小結",
    "text": "14.11 本章小結\n\n平衡且無交互作用的因子設計分析以及有交互作用因子設計分析\n因子設計變異數分析的效果量估計平均值以及信賴區間。\n檢核變異數分析的執行條件\n共變數分析 (ANCOVA)\n變異數分析就是線性模型還有各種多重比較方案\n事後檢定談到杜凱氏HSD，還有提到規劃使用事前檢定方法要思考的條件。\n不平衡的因子設計分析\n\n\n\n\n\nEveritt, B. S. (1996). Making Sense of Statistics in Psychology. A Second-Level Course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple Comparisons: Theory and Methods. Chapman; Hall."
  },
  {
    "objectID": "15-Factor-Analysis.html#探索性因素分析",
    "href": "15-Factor-Analysis.html#探索性因素分析",
    "title": "15  因素分析",
    "section": "15.1 探索性因素分析",
    "text": "15.1 探索性因素分析\n\n探索性因素分析(EFA)是一種可以從觀察資料推斷出任何隱藏的潛在因素的統計技術，這種方法的原理是計算一組測量變項(例如\\(V_1、V_2、V_3、V_4\\) 和 \\(V_5\\))在多大程度上可以構成潛在因素的測量尺度。這種潛在因素不能只用一個觀察變項來測量,而是體現於受影響的一組觀察變項構成的關係網路。\n以 圖 15.1 來說，每個觀察變項 \\(V_i\\) 都在一定程度上受到潛在因素(\\(F\\))的影響，影響力用係數 \\(b_1\\) 到 \\(b_5\\)(也稱為因子負荷)表示。每個觀察變項\\(V_i\\)都有一個誤差項,也就是\\(e_1\\) 到 \\(e_5\\)，代表與潛在因素無關的資料料誤差。\n\n\n\n\n\n圖 15.1: 隱藏於數個觀察變項之間的潛在因素\n\n\n\n\n\n心理學的潛在因素通常是難以直接觀察或測量的心理現象或構造，像是個性,智力或思考模式。以 圖 15.1 來說，觀察變項可以是五個請參與者回答的行為或態度之具體問題,影響五個問題的潛在因素可能是一種被學者稱為外向性的性格，另一組問題則可能受到內向性或責任感等潛在因素的影響。\n這裡還有另一個範例:有一個測量統計焦慮的問卷，雖然統計焦慮無法直接測量，不過問卷中的每個問題能測量因統計課作業引發的一部分焦慮感。例如,“\\(Q1\\):完成統計課程中的作業”,“\\(Q2\\):試圖理解期刊文章中描述的統計”,和“\\(Q3\\):向講師請教課程中不理解之處的幫助”等等，填答者從低焦慮到高焦慮給每題一個評分。若填答者有明顯的統計焦慮，每一題會傾向給較高的評分；反之，如果填答者沒有太明顯的統計焦慮，每一題會傾向給較低的評分。\n探索性因素分析的本質是探索變項之間的相關性，從變項之間的共變關係識別任何有趣和重要的潛在因素。使用統計軟體可以評估任何潛在因素,以及與受潛在因素影響的每個變項的因素負荷量1。例如因素負荷量 &gt; 0.5的變項，很可能是測量潛在因素的有效指標。計算因素負荷量的過程之一稱為轉軸法(rotation)，這是一個目前不好懂的名詞，好在目前讀者不需要理解這個過程，只要知道轉軸能清理不同因素對同一組變項的因素負荷量。經過轉軸法的處理，能清楚地看到哪些變項與每個潛在因素有實質的關聯性。此外，還要決定需要多少因素才能合理解釋資料，這就需要計算特徵值(Eigen value)。介紹探索性因素分析的執行條件之後，我們再回來討論這些名詞。\n\n15.1.1 探索性因素分析的執行條件\n\n\n需要檢查的第一個執行條件是**球形性*,這是檢查資料變項之間的相關性,相關的變項越多就可以用越少潛在因素解釋。巴特利球形檢驗是用來檢驗變項的相關矩陣，是否顯著不同於零相關矩陣。如果巴特利檢驗結果有達到顯著(\\(p &lt; .05\\)),表示反應變項之間有關聯性，可使用探索性因素分析。\n第二個要檢查的執行條件是樣本適足性,這是使用Kaiser-Meyer-Olkin樣本適足量數(簡稱KMO)做為評估指標。KMO樣本適足量數代表觀察變項之間的共變佔總變異量的比例。 因為通常報告呈現的潛在因素，很少納入只能解釋兩個變項的因素，因為這代表樣本不足。如果存在只影響兩個變項的因素，可計算部分相關係數。如果 KMO 量數偏高($ \\(),可認定因素分析結果有效;如果 KMO 量數偏低(\\) $),則因素分析結果找到的潛在因素與變項不相關。 KMO 量數小於 \\(0.5\\) 表示因素分析結果不適用,KMO 值為 \\(0.6\\) 時才會認定因素分析結果是低度適用。 \\(0.5\\) 和 \\(0.7\\) 之間的值被認為是中度適用,\\(0.7\\) 和 \\(0.9\\) 之間的值是高度適用,\\(0.9\\) 和 \\(1.0\\) 之間的值是非常適用。\n\n\n15.1.2 探索性因素分析的用途\n\n如果因素分析結果確立了適用的因素模型,接著需要決定如何處理結果顯現的新因素。EFA通常用來開發心理測量量表，研究人員會先建立與單一與多種心理特質有關的問卷題庫，收集次料後 EFA 檢視哪些題目是”聚斂於”一種潛在因素,然後評估是否應刪除沒有清楚測出任何一個潛在因素的某些題目。\nEFA的另一種用途是將因素負荷量高的變項合併為因素分數,有時可稱為量表分數。有兩種方法能將合併的變項資料轉為量表分數 :\n\n創建新變項代表要計分的因素，合併為一份量表的變項分數按照因素負荷量加權調整。\n創建新變項代表要計分的因素，合併為一份量表的變項分數均權調整。\n\n第一種方法的每個變項對合併分數的貢獻取決於與因素的關係強度。第二種方法就是拿所有受到同一因素實質影響的變項，取變項平均值做為合併的量表分數。何種方法較好沒有參考標準，只是第一種方法的缺點是因素負荷量會隨樣本檢源改變。行為和健康科學的研究人員經常會開發和使用於不同研究場景，或不同對象的綜合問卷。這種問卷的量表分數適合用第二種方法較合理，因為是基於等量貢獻的實質變項，而不是使用來自不同來源的特定樣本的負荷量加權。其實在任何研究問題，用變項平均值當成合併變項的測量較簡單，而且比較直觀。\n還有一種使用迴歸式建立因素模型的進階方法，這是使用潛在因素預測其他潛在因素。這種方法就是“結構方式模型”(structural equation modelling)，已經有專門的軟體和R套件能執行這種方法。不過讀者還不必急著去學，現在先學好如何用 jamovi 執行 EFA。\n\n\n15.1.3 使用jamovi完成探索性因素分析\n\n在此使用取自 國際性格量表題庫的25項人格自我報告題目(請見 圖 15.2 )， 組合的網路性格問卷–合成孔徑性格量表(SAPA:http://sapa-project.org)其中一部分資料做為範例。25個題目假設受五個因素影響:親和性(Agreeableness)、盡責性(Conscientiousness)、外向性(Extraversion)、神經質(Neuroticism)和開放性(Openness)。\n每個題目都是6點量表紀錄參與者的反應資料:\n1.非常不準確\n2.中度不準確\n3.輕度不準確\n4.輕度準確\n5.中度準確\n6.非常準確。\n資料檔案bfi_sample.csv包括\\(N=250\\)份參與者樣本(lsj檔案庫Personality Questionnaire)。研究人員有興趣的，是看看檔案中的\\(25\\)個觀察變項，有那些能合理測量的潛在因素。請先開啟這份檔案檢查這\\(25\\)個變項是否以連續變項編碼？(至少要用次序變項編碼，雖然jamovi的EFA模組可以處理。若是要用因素負荷加權計算因素分數，就需要先轉換為連續變項)。接著執行jamovi的EFA模組：\n\n\n\n\n\n圖 15.2: 資料檔案bfi_sample.csv的25個假設可測量五大性格因素的題目\n\n\n\n\n\n\n從主要jamovi按鈕欄中選擇“因素”- “探索性因素分析”以打開EFA分析窗口(圖 15.3)。\n選擇25個人格問題并將其轉移到“變項”框中。\n檢查適當的選項,包括“預設檢查”,但也選擇旋轉“方法”、要提取的“因子數量”和“其他輸出”選項。有關此示例EFA的建議選項,請參閱@fig-fig15-3,并請注意,如下所述,通常研究人員會在分析過程中調整旋轉“方法”和提取的“因子數量”以找到最佳結果。\n\n\n\n\n\n\n圖 15.3: jamovi EFA分析窗口\n\n\n\n\n\n\n\n\n\n圖 15.4: jamovi人格問卷資料的EFA預設檢查\n\n\n\n\n\n首先,檢查預設條件(圖 15.4)。您可以看到(1)巴特利球形性檢驗顯著,所以滿足這個預設條件;和(2)抽樣適度性測度(MSA)的總體KMO測度為\\(0.81\\),這表明了很好的抽樣適度性。這裡沒有問題!\n下一件要檢查的事情是使用多少因素(或從資料中“提取”多少因素)。有三種不同的方法:\n\n一個公約是選擇所有特徵值大於1的組件2。這會給我們四個因素(嘗試一下)。\n如@fig-fig15-5所示,檢查斜率圖可以幫助您識別“拐點”。這是斜率曲線明顯趨於平緩的點,在“肘部”以下。这會给我们的数据五个因素。解釋斜率圖有點藝術性:在@fig-fig15-5中,從\\(5\\)個到\\(6\\)個因素有一個明顯的跨度,但是在您查看的其他斜率圖中,情況並不會那么清晰。\n使用平行分析技術,獲得的特徵值與從隨機資料獲得的特徵值進行比較。 提取的因素數量是特徵值大於隨機資料中找到的值的數量。\n\n\n\n\n\n\n圖 15.5: 在jamovi EFA 中顯示人格資料的斜率圖,顯示在點5(“肘部”)后明顯拐點和趨於平緩\n\n\n\n\n\n根據 Fabrigar et al. (1999) 的說法,第三種方法很好,儘管在實踐中,研究人員傾向於查看所有三個方法,然后根據最容易或最有幫助解釋的因素數量來進行判斷。這可以理解為“意義標准”,研究人員通常除了檢查上述方法之一的解决方案外,还會檢查具有一個或兩個更多或更少因素的解决方案。 然后他們采用對他們最有意义的解决方案。\n與此同時,我們還應考慮最佳旋轉最終解决方案的最佳方法。旋轉有兩種主要方法:正交(例如“varimax”)旋轉強制所選因素不相關,Whereas斜交(例如“oblimin”)旋轉允許所選因素相關。心理學家和行為科學家感興趣的維度通常並非我們希望正交的維度,因此斜交解決方案可以說更加合理2\n\n\n\n\n\n圖 15.6: jamovi EFA 中五因素解的因素摘要統計量和相關量\n\n\n\n\n\n從實踐的角度來看,如果在斜交旋轉中發現因素之間存在顯著相關(正相關或負相關,並且大於0.3),如@fig-fig15-6所示,其中提取的兩個因素之間的相關為0.31,那麼這將證實我們更喜歡斜交旋轉的直覺。如果這些因素確實相關,那麼斜交旋轉將產生比正交旋轉更好的真實因素估計和更好的簡單結構。而且,如果斜交旋轉表明各因素之間的相關接近零,那麼研究人員可以繼續進行正交旋轉(這時應給出與斜交旋轉大致相同的解決方案)。\n在檢查提取因素之間的相關時,至少有一個相關大於0.3(圖 15.6),因此更喜歡提取的五個因素的斜交(“oblimin”)旋轉。我們還可以在@fig-fig15-6中看到,五個因素解釋的資料總變異數的比例為46%。第一因素解釋了大約10%的變異,第二至第四因素解釋了大約9%的變異,第五因素解釋了略多於7%的變異。這並不是很好;如果整體解釋我們資料中的更多變異那會更好。\n請注意,在每個EFA中,您可以潛在地具有與觀察變項數量相同的因素,但是您包含的每個附加因素都會增加更小量的解釋變異數量。如果前几个因素解释了原始25 个變項中的大量方差,那麼這些因素顯然是25 个變項的一個有用、更简单的替代品。您可以放心刪除其餘部分,而不會失去太多原始變異性。但是如果需要18個因素(例如)來解釋這25個變項中的大部分變異數,那麼您不如直接使用原始的25個變項。\n?fig-fig15-7顯示了因子負荷。也就是說,25個不同的人格項目如何在五個選定的因素上加載。我們隱藏了小於0.3的負荷(設置在@fig-fig15-3中顯示的選項中)。\n對於因素\\(1、2、3\\)和\\(4\\),因子負荷的模式與@fig-fig15-2中指定的假定因素緊密匹配。太好了!第5個因素也相當接近,五個被認為測量“開放性”的觀察變項中有四個在該因素上載入得相當好。但是,變項\\(04\\)似乎並不太適合,因為@fig-fig15-7中的因子解决方案表明它載入到因素\\(4\\)上(儘管載荷相對較低),但沒有實質上載入到因素\\(5\\)上。\n需要注意的另一件事情是,在@fig-fig15-2中標記為“R:反向編碼”的那些變項是那些具有負因子負荷的變項。查看項目A1(“漠不關心他人的感受”)和A2(“詢問他人的幸福”)。我們可以看到,\\(A1\\)的高分表示親和力較低,Whereas \\(A2\\)(和所有其他“A”變項)的高分表示親和力較高。因此,A1將與其他“A”變項負相關,這就是為什麼它在@fig-fig15-7中具有負因子負荷。\n\n\n\n\n\n圖 15.7: jamovi EFA中的五因素解的因子負荷量\n\n\n\n\n\n我們也可以在 圖 15.7 中看到每個變項的“獨特性”。獨特性是指變項中“獨特”的變異比例,而不是被因子所描述的 3。例如,“A1”中有 72% 的變異沒有被五因子解中的因子描述。相反,“N1”的無法被因子解描述的變異相對較低(35%)。請注意,“獨特性”越高,該變項在因子模式中的相關性或貢獻越低。\n說實話,在探索性因子分析中得到這樣整潔的解決方案並不尋常。結果通常會混亂得多,而解析因子的意義也更具挑戰性。很少會有這樣清晰劃分的變項池。您會有大量的反應變項,您認為它們可能是少數潛在因子的指標,但是您並不如此強烈地知道哪些變項將會到哪裡!\n所以,我們似乎有一個相當好的五因子解,儘管它描述了相對較低的整體變異比例。假設我們對這個解決方案滿意,並希望在進一步分析中使用因子。直接的選擇是計算每個因子的整體平均分數。例如,對於資料集中的每個人的親和力因子,這涉及添加 \\(A1 + A2 + A3 + A4 + A5\\),然後除以 5 4。我們計算的因子得分是基於每個包含變項的等權得分。我們可以通過兩個步驟在 jamovi 中執行:\n\n使用 jamovi 將 A1 重新編碼為 “A1R”,方法是反向評分變項中的值。\n使用 jamovi 計算新變項 “親和力”,方法是計算 A1R、A2、A3、A4 和 A5 的平均值。\n\n\n\n\n\n\n圖 15.8: 使用 jamovi 的變項轉換命令重編碼變項\n\n\n\n\n\n\n\n\n\n圖 15.9: 使用 jamovi 的計算新變項命令計算新的量表分數變項\n\n\n\n\n\n另一個選擇是創建一個最佳加權的因子分數指數。要做到這一點,使用“保存” - “因子分數”複選框將因子分數保存到資料集。一旦您這樣做,您將看到已經向資料添加了5個新變項(列),每個提取的因子一列。請參見@fig-fig15-10和圖 15.11。\n\n\n\n\n\n圖 15.10: 使用“Bartlett”最佳加權方法的五因子解的因子分數選項的jamovi\n\n\n\n\n\n\n\n\n\n圖 15.11: 顯示五個新創建的因子分數變項的資料表視圖\n\n\n\n\n\n現在,您可以繼續開展進一步的分析,使用基於平均分數的因子量表(例如@fig-fig15-9中的量表)或使用jamovi計算的最佳加權因子分數。由您選擇!例如,您可能想做的一件事是查看這些人格特徵中的每一項在性別上的差異。我們對計算的親和力分數使用平均分法進行了t檢驗圖(圖 15.12),結果顯示男性的親和力低於女性,但這個差異並不顯著(Mann-Whitney U = 5768,p = .075)。\n\n\n\n\n\n圖 15.12: 比較男性和女性之間基於親和力因子的分數差異\n\n\n\n\n\n\n15.1.4 探索性因素分析的報告須知\n\n\n希望讀者現在已經了解如何使用jamovi 執行探索性因素分析。 接下來,完成探索性因素分析後,報告要如何撰寫呢?其實並沒有標準的探索性因素分析報告格式，不同領域或學術社群各有習慣的範例。在此只介紹因素分析報告應包含的資訊：\n\n你目前的研究所遵循的理論基礎,特別是透過探索性因素分析而發現的理論建構。\n描述樣本資訊(例如人口統計訊息、樣本量、取樣方法)。\n描述資料類型(例如名義變項、連續變項)以及描述統計資訊。\n描述執行探索性因素分析的條件。 尤其是球形假設檢定以及樣本適足性檢定的詳細資訊。\n描述提取潛在因素的方法(例如“最小殘差法”或“最大似然法”)。\n描述決定因素數目以及題目的標準和過程。建議清楚說明探索性因素分析過程每個決定的理由。\n描述所使用的轉軸方法、原因以及結果。\n整理最後算出的因素負荷量於報告的表格中。 表格內容的最後一列應報告每個變項的獨特性(或公因性)。 除了項目編號,因素負荷量還要附上描述性標籤。 因素之間的相關係數也應呈現於表格內,無論是在總表格的底部或單獨製表。\n為決定提取的潛在因素取個有意義的名稱。 可以是預先選定的名稱,檢視有效的題目與最後提取的因素後,可以更換為更合適的名稱。"
  },
  {
    "objectID": "15-Factor-Analysis.html#主成分分析",
    "href": "15-Factor-Analysis.html#主成分分析",
    "title": "15  因素分析",
    "section": "15.2 主成分分析",
    "text": "15.2 主成分分析\n\n探索性因素分析一節示範如何決定影響題目變項的潛在因素。範例也說明在某些情況，有較少的因素的變項結構，有利使用綜合的因素分數做進一步的統計分析中。\n這種方法被稱為“資料降維”，除了探索性因素分析，研究者也可以用主成分分析(principal component analysis, PCA)做“資料降維”。不過，PCA並是不用來辨識潛在因素,而是從更大的測量變項集合建立線性綜合分數。\nPCA只是產生原始資料的數學轉換,並不限制變項之間必須存在共變。目的是計算原始變項的幾種可能線性組合,這些組件可以用來總結收集的資料,且幾乎不丟失任何資訊。然而,如果分析的目的是辨識潛在的變項結構,那麼應該使用探索性因素分析。正如探索性因素分析的範例,此方法產生的因素分數與主成分分析一樣可以達到資料降維 (Fabrigar et al., 1999)。\n因為過去使用計算機並不容易，上個世紀的心理學研究很流行使用PCA，所以在此介紹一下。如今各型電腦的計算能力越來越強大，使得研究者完成探索性因素分析時間成本大幅降低,而且與PCA相比,特別是在因素和變項數量較少的情況,探索性因素分析更能避免誤差。PCA的大部分程序與探索性因素分析相似,所以儘管執行條件有一些差異,但是實際操作步驟是相同的,並且對於足夠大的樣本以及充分數量的因素和變項,PCA和EFA的結果應該差不多。\n要使用jamovi執行PCA,只要從主界面按鈕欄選擇“因子” - “主成分分析”以打開PCA分析視窗。 然後,您可以遵循使用jamovi完成探索性因素分析的示範步驟，就能完成PCA。"
  },
  {
    "objectID": "15-Factor-Analysis.html#驗證性因素分析",
    "href": "15-Factor-Analysis.html#驗證性因素分析",
    "title": "15  因素分析",
    "section": "15.3 驗證性因素分析",
    "text": "15.3 驗證性因素分析\n\n使用精選自國際性格量表題庫的網路問卷資料來確定人類性格的潛在因素似乎相當有效。開發可用的性格測量工具的下一步，是招募不同樣本檢驗從探索性因素分析確定的潛在因素。研究人員想知道能否用另一批資料驗證這些因素,為了進行驗證,研究人員需要一個更嚴格的方法，也就是這一節介紹的驗證性因素分析(CFA)。這種因素分析是開發出來確認預先指定的潛在因素結構。 5\n執行CFA不會用探索性的方式查看資料的整合方式,而是像 圖 15.13 所示先建立一套因素結構，查看資料符合我們預先指定的結構的程度。研究人員使用CFA時的主要目的，是想要查看反應資料對預先指定的模型的確認程度。\n直接對這些性格題目資料進行確證性因素分析將如 圖 15.13 所示指定五個潛在因素,每個因素各以五個反應變項測量，每個變項被認定是潛在因素的測量尺度之一。例如,題目A1的反應可被潛在因素「親和力」預測，然而因為A1不能完美測量親和力,所以變項的變異還包括一個誤差項\\(e\\)。換句話說,\\(e\\)代表了A1中無法被親和力因子預測的變異，這個部分又被稱為測量誤差。\n\n\n\n\n\n圖 15.13: 根據預先指定的潛在因素結構，使用確認性因素分析檢驗五大性格特徵量表題目與影響因素。\n\n\n\n\n\n接著要考慮模型裡的潛在因素是否應該有相關性。如同探索性因素分析的範例，心理和行為科學的模型經常有彼此相關的因素，心理學家也同意各種性格特徵可能互有關聯。因此五大人格的模型裡，也包括潛在因素之間的共變，如同 圖 15.13 中串連各潛在因素的雙向箭頭。\n研究人員同時要考慮某些測量誤差因為任何合理的、有系統性的理由而互有相關。如此設定的一個原因可能是有幾種反應變項之間存在共變的測量誤差,以致反應變項之間的相關可能是出於研究方法造成的系統誤差，而非與潛在變項的關聯性。下一節多種特質多項相關驗證性因素分析介紹的進階方法將討論如何處理這種可能性,目前因為沒有任何明確的理由確認某些誤差項彼此相關，因此先不討論。\n如果沒有必要檢查誤差項之間的相關,研究人員只要檢測 圖 15.13 所指定的因素結構，與反應資料的匹配程度。由於預期只有模型裡的因素才能由資料變項提取，所以與模型內因素及變項無關的係數都被設置為零，例如,A1的原始資料與潛在因素「外向性」有高負荷量,但在是要驗證的模型假設此係數為零。如果與模型無關的係數不為零，確認性因素分析結果可能會顯示模型與資料之間的匹配程度不高。\n接著來看看如何使用jamovi設定確認性因素分析的報行參數。\n\n15.3.1 使用jamovi完成驗證性因素分析\n\n打開 bfi_sample2.csv 文件,檢查25個變項是否被編碼為順序變項(或連續變項;對於這個分析不會產生任何區別)。要在jamovi中執行確認性因素分析:\n\n從主界面按鈕欄中選擇“因子” - “確認性因素分析”以打開確認性因素分析窗口(圖 15.14)。\n在“因子”框中選擇5個A變項並將其轉移到“因子”框中,並給予“親和力”的標籤。\n在“因子”框中創建一個新的因子並給它貼上“盡責性”的標籤。選擇5個C變項並將其轉移到“盡責性”標籤下的“因子”框中。\n在“因子”框中再創建一個新的因子並給它貼上“外向性”的標籤。選擇5個E變項並將其轉移到“外向性”標籤下的“因子”框中。\n在“因子”框中再創建一個新的因子並給它貼上“神經質”的標籤。選擇5個N變項並將其轉移到“神經質”標籤下的“因子”框中。\n在“因子”框中再創建一個新的因子並給它貼上“開放性”的標籤。選擇5個O變項並將其轉移到“開放性”標籤下的“因子”框中。\n檢查其他適當的選項,默認值對於這第一個嘗試來說是可以的,儘管您可能想要在“圖形”下檢查“路徑圖”選項,以查看jamovi生成的圖(相當)類似於我們的@fig-fig15-13。\n\n\n\n\n\n\n圖 15.14: jamovi確認性因素分析窗口\n\n\n\n\n\n設定好分析後,我們可以將注意力轉到jamovi的結果窗口,看看情況如何。首先要看的是模型適配度(圖 15.15),因為這告訴了我們模型與反應資料的匹配程度。請注意,在我們的模型中,只估計了預先指定的協方差,默認包括因子相關。其餘都設置為零。\n\n\n\n\n\n圖 15.15: jamovi CFA模型適配結果\n\n\n\n\n\n\n有幾種方法可以評估模型的適配度。第一個是卡方統計量,如果很小,則表示模型與資料的匹配很好。 然而,用於評估模型適配的卡方統計對樣本大小相當敏感,這意味著對於大樣本而言,模型與資料之間足夠好的匹配幾乎總會產生很大的顯著(\\(p\\) &lt; .05)卡方值。\n因此,我們需要其他的模型適配度評估方法。在 jamovi 中預設提供了幾種。這些是比較適配指數(CFI)、塔克-劉易斯指數(TLI)和近似誤差均方根(RMSEA)以及 RMSEA 的 90%置信區間。一些實用的經驗法則是,CFI &gt; 0.9、TLI &gt; 0.9和 RMSEA 約為 0.05 到 0.08 表示滿意的適配。CFI &gt; 0.95、TLI &gt; 0.95和 RMSEA 及 RMSEA 上界 CI &lt; 0.05 表示很好的適配。\n所以,看@fig-fig15-15我們可以看到,卡方值很大並且顯著性很高。我們的樣本量不大,所以這可能表示模型適配較差。CFI為\\(0.762\\),TLI為0.731,表示模型與資料之間的匹配較差。RMSEA為\\(0.085\\),90%置信區間從\\(0.077\\)到\\(0.092\\),同樣也沒有顯示很好的適配。\n這個結果相當令人失望,不是嗎?但考慮到在前一節的 EFA 分析中,當我們使用類似的資料集(見 探索性因素分析)運行時,五因子模型只描述了資料中的大約一半變異,這結果可能也不太令人驚訝。\n讓我們繼續查看@fig-fig15-16和?fig-fig15-17中顯示的因子加載量和因子協方差估計。 每個參數的 Z 統計量和 P 值表明他們對模型做出了合理的貢獻(即它們不為零),所以沒有任何理由從模型中刪除任何指定的變項-因子路徑或因子-因子相關。通常標準化估計值更容易解釋,這些可以在“估計”選項下指定。 這些表可以用fully併入書面報告或科學文章中。\n\n\n\n\n\n圖 15.16: jamovi CFA因子加載表\n\n\n\n\n\n\n\n\n\n圖 15.17: jamovi CFA因子協方差表\n\n\n\n\n\n我們如何改進模型呢?一個選擇是返回幾個階段並重新考慮我們正在使用的項目/測量以及如何改進或更改它們。另一種選擇是對模型進行一些事後調整以改進適配度。實現這一目的的一種方法是使用“修正指數”(圖 15.18),在 jamovi 中它被指定為“其他輸出”選項。\n\n\n\n\n\n圖 15.18: jamovi CFA 因子加載修正指數\n\n\n\n\n\n\n我們要尋找的是最大修正指數(MI)值。 然後,我們將判斷是否有理由將該附加項添加到模型中,使用事後合理化。 例如,我們可以在@fig-fig15-18中看到,在模型中還沒有的因子加載量中,最大的 MI 值是 N4(“經常感到沮喪”)加載到潛在因子外向性上的值 28.786。 這表明如果我們將此路徑添加到模型中,卡方值將減少差不多相同的量。\n但是在我們的模型中增加這條路徑在理論上或方法論上並不合理,所以這並不是一個好主意(除非您能提出有說服力的論據認為“經常感到沮喪”同時測量神經質和外向性)。 我想不出好的理由。 但是,為了論證的目的,讓我們假裝確實有一定的意義並將此路徑添加到模型中。 返回確認性因素分析窗口(圖 15.14)並將 N4 添加到外向性因子中。 CFA 的結果現在會改變(未顯示); 卡方下降到約 709 左右(下降了約 30,大致與 MI 的大小相當),其他適配指標也有所改善,儘管只是一點點。 但這還不夠:這仍然不是一個很好的匹配模型。\n如果您發現自己正在使用 MI 值向模型中添加新的參數,則每次新增後都應重新檢查 MI 表,因為 MI 會在每次重新計算。\njamovi 還產生了殘差協方差修正指數表(圖 15.19)。 換句話說,如果將這些相關誤差添加到模型中,那麼這是一個顯示哪些相關誤差可以最大程度改進模型適配的表格。同時查看這兩個 MI 表是一個好主意,找出最大的 MI,考慮是否可以合理證明建議參數的增加,如果可以的話,將其添加到模型中。 然後,您可以在重新計算的結果中再次開始尋找最大的 MI。\n\n\n\n\n\n圖 15.19: jamovi 產生的殘差協方差修正指數\n\n\n\n\n\n您可以盡可能長時間以這種方式繼續操作——根據最大的 MI 將參數添加到模型中,最終您將實現令人滿意的適配效果。 但這樣做的強大可能性是您將創建一個怪物!一個醜陋、畸形的模型,在理論上毫無意義或純粹性。 換句話說,要非常小心!\n到目前為止,我們已經使用第二個樣本和確認性因素分析檢查了在探索性因素分析中獲得的因子結構。 不幸的是,我們發現探索性因素分析中的因子結構在確認性因素分析中沒有被確認,所以就這個人格特徵量表的開發而言,我們又回到了起點。\n儘管我們本可以使用修正指數調整確認性因素分析,但我真的想不出任何很好的理由(至少我想不出)證明模型中建議的這些額外的因子加載量或殘差協方差應該被包括在內。 然而,在某些情況下,允許殘差共變(或相關)是有很好理由的,下一節 多種特質多項相關驗證性因素分析 就給出了一個很好的例子。 在進入下一部分之前,讓我們先了解如何報告確認性因素分析的結果。\n\n\n15.3.2 驗證性因素分析的報告須知\n\n\n確認性因素分析報告沒有正式的標準格試,不同領域或學術社群的報告範例也有所不同。儘管如此,建議報告裡應包括以下幾項:\n\n描述假設模型的理論和實證依據。\n完整描述模型內的因素關聯性(例如,每個潛在因素的影響變項,潛在變項之間的共變異數，以及任何誤差項之間的相關係數)。以 圖 15.13 這樣的路徑圖呈現在報告是種清楚的報告方法。\n描述樣本資訊(例如人口統計資訊、樣本量、取樣方法)。\n描述資料類型(例如名義變項、連續變項)以及完整的描述統計。\n描述使用條件的檢驗方法，以及關鍵係數的估計方法。\n描述缺失資料的來源以及處理方式。\n測試模型適配度的軟體和版本。\n判斷模型適配度的測量尺度和標準。\n基於模型適配度或修正指數所進行的模型變更。\n以表格整理所有參數估計值(因素負荷量、誤差變異數、潛在變異數或共變數)及其標準誤。"
  },
  {
    "objectID": "15-Factor-Analysis.html#多種特質多項相關驗證性因素分析",
    "href": "15-Factor-Analysis.html#多種特質多項相關驗證性因素分析",
    "title": "15  因素分析",
    "section": "15.4 多種特質多項相關驗證性因素分析",
    "text": "15.4 多種特質多項相關驗證性因素分析\n\n\n這一節介紹的因素分析方法將考慮方法變異，也就是來自不同的測量尺度或題目所形成的資料變異。所以這一節的範例採用另一個包含“歸因風格”的資料檔案。\nHewitt et al. (2004) 歸因風格問卷(ASQ)從英國和紐西蘭的年輕人收集心理健康資料，他們測量了負面事件的歸因風格,這是指每個人有自行解釋壞事如何發生的歸因類型(Peterson & Seligman, 1984)。言份問卷測量三種歸因風格:\n\n內隱性(Internality)是他/她相信壞事發生是個人行為造成的程度。\n穩定性(Stability)是指他/她習慣地認為隔一段時間就會發生壞事的程度。\n全局性(Globality)是指他/她習慣地認為導致一件壞事的原因會影響個人生活各層面的程度。\n\n問卷設定六種假設情境,對於每種情境,受測者要根據情境設定，回答一組問題,回答是用於測量(a)內隱性,(b)穩定性和(c)全局性。所以要填的問題總共有\\(6 \\times 3 = 18\\)個。英文版問卷內容請參考 圖 15.20 。\n\n\n\n\n\n圖 15.20: 負面事件的歸因風格問卷(ASQ)部分題目\n\n\n\n\n\n研究人員想要分析他們收集的資料,評估ASQ的18個反應變項是否確實測量到有興趣的潛在因素。\n首先,他們使用探索性因素分析,但嘗試各種因素選取或轉軸方法,都找不到一個好的因素結構，因此沒有得到可靠的結論。像這樣的狀況，要麼研究人員的理論是錯誤的(歸因風格可能不是潛在因素),要麼選取到不相關的樣本(樣本規模和特徵可能不是英國和紐西蘭的年輕人),要麼分析方法不是解決這項研究問題的合適工具。在此我們探研第三種可能性。\n我們先將ASQ測量的三種歸因：內隱性、穩定性和全局性，還有各情境的六項題目測量資料的關聯性，以 圖 15.21 的路徑圖呈現。\n如果研究人員不用探索性的分析方法找出資料變項的組合，而是預先設定如同 圖 15.21 的因素及變項結構,那要如何檢驗資料符合這套結構的程度？這樣的分析角度是一種確認性分析,重點是檢視原始資料適配預先指定模型的程度。實際使用ASQ資料執行確證性因素分析，要先確認每位受測者有如同 圖 15.27 所列的18項反應資料，三個因素各有六個測量變項。\n\n\n\n\n\n圖 15.21: ASQ包括的三個潛在因素~內隱性、穩定性和全局性及對應的六道題目編碼\n\n\n\n\n\n每個變項與潛在因素可以描繪成 圖 15.22 的路徑圖。例如,INT1受到潛在因素內隱性影響，並且因為INT1無法完美測量內隱性,所以還有誤差項\\(e_1\\)。換句話說,\\(e_1\\)代表INT1無法被內隱性解釋的變異，這種誤差又被稱為“測量誤差”。\n\n\n\n\n\n圖 15.22: ASQ潛在因子結構的最初預先指定\n\n\n\n\n\n接著要考慮模型裡的潛在因素是否應該有相關性。如同探索性因素分析的範例，心理和行為科學的模型經常有彼此相關的因素，使用ASQ的研究人員也同意內隱性、穩定性和全局性可能互有關聯。因此 圖 15.23 展示的模型還包括潛在因素之間的共變，以串連各潛在因素的雙向箭頭表示。\n圖 15.23 還有呈現共用方法誤差之間的相關性，也就是INT1, STAB1 與 GLOB1 三個項目誤差項之間的雙向箭頭。為了保持視覺簡潔，其他誤差異的相關性就予以省略。\n\n\n\n\n\n圖 15.23: 進行CFA MTMM分析前，預先假設的ASQ 潛在因素結構最終版,包括潛在因素之間相關、以及反應變項INT1、STAB1和GLOB1的共享方法誤差之間相關。為清楚起見,未呈現其他預先指定的誤差項相關。\n\n\n\n\n\n\n研究人員同時要考慮某些測量誤差因為任何合理的、有系統性的理由而互有相關。仔細看一下ASQ的題目,每個主要題目(1-6)都有三個子題(a、b和c)。Q1與找不到工作有關,這個問題與其他問題(2-5)相比,在找工作方面可能具有某些獨特的人為或方法論特性。同樣地,Q2與不幫助朋友解決問題有關,在不幫助朋友這一點上,與其他問題(1和3-5)相比可能存在某些獨特的人為或方法特性。\n因此,除了不只一個因素,ASQ還有多種方法特性,其中每個主要題目都有特定的“測量方法”,但是每種“測量方法”貫穿a、b和c子題。為了將不同方法特性納入模型,研究人員會指定某些誤差項彼此相關。例如,INT1、STAB1和GLOB1的誤差項應該彼此相關,代表Q1a、Q1b和Q1c各自的方法誤差有共變。\n儘管可以用如同 圖 15.22 的基本CFA模型檢驗資料,但這裡設計一個更複雜的模型,如同 圖 15.23 的模型所示。這種更複雜的CFA模型被稱為多種特質多項相關(MTMM)模型,以下示範如何使用jamovi執行。\n\n15.4.1 使用jamovi完成多種特質多項相關驗證性因素分析\n\n打開 ASQ.csv 文件並檢查18個變項(6個“內隱性”、6個“穩定性”和6個“全局性”變項)是否被指定為連續變項。\n要在jamovi中執行多特徵多方法確認性因素分析:\n\n從主界面按鈕欄中選擇“因子” - “確認性因素分析”以打開確認性因素分析窗口(圖 15.24)。\n在“因子”框中選擇6個INT變項並將其轉移到“因子”框中,並給予“內隱性”的標籤。\n在“因子”框中創建一個新的因子並給它貼上“穩定性”的標籤。選擇6個STAB變項並將其轉移到“穩定性”標籤下的“因子”框中。\n在“因子”框中再創建一個新的因子並給它貼上“全局性”的標籤。選擇6個GLOB變項並將其轉移到“全局性”標籤下的“因子”框中。\n打開殘差協方差選項,並對於每個預先指定的相關,將相關的變項移動到右側的“殘差協方差”框中。例如,同時突出顯示INT1和STAB1,然後單擊箭頭將其移動。現在對INT1和GLOB1、STAB1和GLOB1、INT2和STAB2、INT2和GLOB2、STAB2和GLOB2、INT3和STAB3等執行相同操作。\n檢查其他適當的選項,默認值對於這第一個嘗試來說是可以的,儘管您可能想要在“圖形”下檢查“路徑圖”選項,以查看jamovi生成的圖(相當)類似於我們的@fig-fig15-23,並包括我們上面添加的所有誤差項相關。\n\n\n\n\n\n\n圖 15.24: jamovi確認性因素分析窗口\n\n\n\n\n\n一旦設定好分析,我們就可以把注意力轉到jamovi的結果窗口,看看狀況如何。首先要看的是“模型適配度”,因為這告訴了我們模型與反應資料的匹配程度(圖 15.25)。請注意,在我們的模型中,只估計了預先指定的協方差,其他都是設定為零,所以模型適配度測試的是預先指定的“自由”參數是否不為零,反過來是否資料中的其他關係——那些我們沒有在模型中指定的關係——可以保持為零。\n\n\n\n\n\n圖 15.25: jamovi MTMM確認性因素分析模型適配結果\n\n\n\n\n\n\n看@fig-fig15-25我們可以看到,卡方值非常顯著,考慮到樣本量很大(N = 2748)這一點就不足為奇了。 CFI為0.98,TLI也為0.98,表示模型適配非常好。 RMSEA為0.02,90%置信區間從0.02到0.02,非常緊密!\n總的來說,我認為我們可以滿意地認為我們預先指定的模型與反應資料的匹配非常好,這支持了我們對ASQ的MTMM模型。\n現在我們可以繼續查看因子加載量和因子協方差估計,如@fig-fig15-26所示。通常標準化估計值更容易解釋,這些可以在“估計”選項下指定。這些表可以很好地併入書面報告或科學文章中。\n\n\n\n\n\n圖 15.26: jamovi MTMM確認性因素分析的因子加載量和協方差表\n\n\n\n\n\n從@fig-fig15-26您可以看到,我們預先指定的所有因子加載量和因子協方差明顯不同於零。換句話說,它們似乎都在為模型做出有用的貢獻。\n在這次分析中,我們相當幸運,在第一次嘗試就得到了非常好的適配效果!"
  },
  {
    "objectID": "15-Factor-Analysis.html#section",
    "href": "15-Factor-Analysis.html#section",
    "title": "15  因素分析",
    "section": "15.6 ",
    "text": "15.6 \nIn this chapter on factor analysis and related techniques we have introduced and demonstrated statistical analyses that assess the pattern of relationships in a data set. Specifically, we have covered:\n\n[Exploratory Factor Analysis] (EFA). EFA is a statistical technique for identifying underlying latent factors in a data set. Each observed variable is conceptualised as representing the latent factor to some extent, indicated by a factor loading. Researchers also use EFA as a way of data reduction, i.e. identifying observed variables than can be combined into new factor variables for subsequent analysis.\n[Principal Component Analysis] (PCA) is a data reduction technique which, strictly speaking, does not identify underlying latent factors. Instead, PCA simply produces a linear combination of observed variables.\n[Confirmatory Factor Analysis] (CFA). Unlike EFA, with CFA you start with an idea - a model - of how the variables in your data are related to each other. You then test your model against the observed data and assess how good a fit the model is to the data.\nIn [Multi-Trait Multi-Method CFA] (MTMM CFA), both latent factor and method variance are included in the model in an approach that is useful when there are different methodological approaches used and therefore method variance is an important consideration.\n[Internal consistency reliability analysis]. This form of reliability analysis tests how consistently a scale measures a measurement (psychological) construct.\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "15-Factor-Analysis.html#本章小結",
    "href": "15-Factor-Analysis.html#本章小結",
    "title": "15  因素分析",
    "section": "15.6 本章小結",
    "text": "15.6 本章小結\n這一章我們學習因素分析的相關技術，特別是評估資料內各種相關性的方法。本章的學習重點包括：\n\n探索性因素分析 (EFA)用於辨識資料內的潛在因素。根據因素負荷量，每個觀察變項都有可能代表某個潛在因素。研究者也會使用EFA簡化資料項目，像是使用序列分析整合數個觀察變項為一個因素。\n主成分分析 (PCA)是一種簡化資料項目的技術，但是並非用於辨識潛在變項。PCA只是生成觀察變項的線性組合。\n驗證性因素分析 (CFA)不同於EFA，執行前已經有一個理想的模型～也就是觀察變項之間的關聯模型。CFA的用途是檢測理想模性與資料模型的擬合度。\n多種特質多項相關驗證性因素分析 (MTMM CFA) 用於分析潛在因素模型的方法不只一種，需要評估各種分析方法所所估計的變異合理程度。\n內部一致性信度分析 用於評估量表所測對象，與假設的心理建構之間的一致性程度。\n\n\n\n\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of tests. Psychometrika, 16(3), 297–334.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J. (1999). Evaluating the use of exploratory factor analysis in psychological research. Psychological Methods, 4, 272–299.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004). Multitrait-multimethod confirmatory factor analysis of the attributional style questionnaire. Personality and Individual Differences, 37(7), 1483–1491.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk factor for depression: Theory and evidence. Psychological Review, 91, 347–374."
  },
  {
    "objectID": "16-Bayesian-statistics.html#理性人類的機率推論",
    "href": "16-Bayesian-statistics.html#理性人類的機率推論",
    "title": "16  貝氏統計",
    "section": "16.1 理性人類的機率推論",
    "text": "16.1 理性人類的機率推論\nFrom a Bayesian perspective statistical inference is all about belief revision. I start out with a set of candidate hypotheses h about the world. I don’t know which of these hypotheses is true, but do I have some beliefs about which hypotheses are plausible and which are not. When I observe the data, d, I have to revise those beliefs. If the data are consistent with a hypothesis, my belief in that hypothesis is strengthened. If the data are inconsistent with the hypothesis, my belief in that hypothesis is weakened. That’s it! At the end of this section I’ll give a precise description of how Bayesian reasoning works, but first I want to work through a simple example in order to introduce the key ideas. Consider the following reasoning problem.\n\nI’m carrying an umbrella. Do you think it will rain?\n\nIn this problem I have presented you with a single piece of data (d = I’m carrying the umbrella), and I’m asking you to tell me your belief or hypothesis about whether it’s raining. You have two alternatives, h: either it will rain today or it will not. How should you solve this problem?\n\n16.1.1 事前機率：你一開始的信念\nThe first thing you need to do is ignore what I told you about the umbrella, and write down your pre-existing beliefs about rain. This is important. If you want to be honest about how your beliefs have been revised in the light of new evidence (data) then you must say something about what you believed before those data appeared! So, what might you believe about whether it will rain today? You probably know that I live in Australia and that much of Australia is hot and dry. The city of Adelaide where I live has a Mediterranean climate, very similar to southern California, southern Europe or northern Africa. I’m writing this in January and so you can assume it’s the middle of summer. In fact, you might have decided to take a quick look on Wikipedia2 and discovered that Adelaide gets an average of 4.4 days of rain across the 31 days of January. Without knowing anything else, you might conclude that the probability of January rain in Adelaide is about 15%, and the probability of a dry day is 85% (see Table 16.1). If this is really what you believe about Adelaide rainfall (and now that I’ve told it to you I’m betting that this really is what you believe) then what I have written here is your prior distribution, written \\(P(h)\\).\n\n\n\n\nTable 16.1:  How likely is it to rain in Adelaide - pre-existing beliefs based on knowledge average January rainfall \n\nHypothesisDegree of Belief\n\nRainy day0.15\n\nDry day0.85\n\n\n\n\n\n\n\n16.1.2 似然值: 對手上資料的理論\nTo solve the reasoning problem you need a theory about my behaviour. When does Dan carry an umbrella? You might guess that I’m not a complete idiot,3 and I try to carry umbrellas only on rainy days. On the other hand, you also know that I have young kids, and you wouldn’t be all that surprised to know that I’m pretty forgetful about this sort of thing. Let’s suppose that on rainy days I remember my umbrella about 30% of the time (I really am awful at this). But let’s say that on dry days I’m only about 5% likely to be carrying an umbrella. So you might write this out as in Table 16.2.\n\n\n\n\nTable 16.2:  How likely am I to be carrying an umbrella on rainy and dry days \n\nDataData\n\nHypothesisUmbrellaNo umbrella\n\nRainy day0.300.70\n\nDry day0.050.95\n\n\n\n\n\nIt’s important to remember that each cell in this table describes your beliefs about what data d will be observed, given the truth of a particular hypothesis \\(h\\). This “conditional probability” is written \\(P(d|h)\\), which you can read as “the probability of \\(d\\) given \\(h\\)”. In Bayesian statistics, this is referred to as the likelihood of the data \\(d\\) given the hypothesis \\(h\\).4\n\n\n16.1.3 資料與理論的聯合機率\nAt this point all the elements are in place. Having written down the priors and the likelihood, you have all the information you need to do Bayesian reasoning. The question now becomes how do we use this information? As it turns out, there’s a very simple equation that we can use here, but it’s important that you understand why we use it so I’m going to try to build it up from more basic ideas.\nLet’s start out with one of the rules of probability theory. I listed it way back in Table 7.1, but I didn’t make a big deal out of it at the time and you probably ignored it. The rule in question is the one that talks about the probability that two things are true. In our example, you might want to calculate the probability that today is rainy (i.e., hypothesis h is true) and I’m carrying an umbrella (i.e., data \\(d\\) is observed). The joint probability of the hypothesis and the data is written \\(P(d,h)\\), and you can calculate it by multiplying the prior \\(P(h)\\) by the likelihood \\(P(d|h)\\). Mathematically, we say that\n\\[P(d,h)=P(d|h)P(h)\\]\nSo, what is the probability that today is a rainy day and I remember to carry an umbrella? As we discussed earlier, the prior tells us that the probability of a rainy day is 15%, and the likelihood tells us that the probability of me remembering my umbrella on a rainy day is \\(30\\%\\). So the probability that both of these things are true is calculated by multiplying the two\n\\[\n\\begin{split}\nP(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\n& = 0.30 \\times 0.15 \\\\\n& = 0.045\n\\end{split}\n\\]\nIn other words, before being told anything about what actually happened, you think that there is a 4.5% probability that today will be a rainy day and that I will remember an umbrella. However, there are of course four possible things that could happen, right? So let’s repeat the exercise for all four. If we do that, we end up with Table 16.3.\n\n\n\n\nTable 16.3:  Four possibilities combining rain (or not) and umbrella carrying (or not) \n\nUmbrellaNo-umbrella\n\nRainy0.0450.105\n\nDry0.04250.807\n\n\n\n\n\nThis table captures all the information about which of the four possibilities are likely. To really get the full picture, though, it helps to add the row totals and column totals. That gives us Table 16.4.\n\n\n\n\nTable 16.4:  Four possibilities combining rain (or not) and umbrella carrying (or not), with row and column totals \n\nUmbrellaNo-umbrellaTotal\n\nRainy0.0450.1050.15\n\nDry0.04250.8070.85\n\nTotal0.08750.9121\n\n\n\n\n\nThis is a very useful table, so it’s worth taking a moment to think about what all these numbers are telling us. First, notice that the row sums aren’t telling us anything new at all. For example, the first row tells us that if we ignore all this umbrella business, the chance that today will be a rainy day is 15%. That’s not surprising, of course, as that’s our prior.5 The important thing isn’t the number itself. Rather, the important thing is that it gives us some confidence that our calculations are sensible! Now take a look at the column sums and notice that they tell us something that we haven’t explicitly stated yet. In the same way that the row sums tell us the probability of rain, the column sums tell us the probability of me carrying an umbrella. Specifically, the first column tells us that on average (i.e., ignoring whether it’s a rainy day or not) the probability of me carrying an umbrella is 8.75%. Finally, notice that when we sum across all four logically-possible events, everything adds up to 1. In other words, what we have written down is a proper probability distribution defined over all possible combinations of data and hypothesis.\nNow, because this table is so useful, I want to make sure you understand what all the elements correspond to and how they written (Table 16.5):\n\n\n\n\nTable 16.5:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities \n\nUmbrellaNo-umbrella\n\nRainyP(Umbrella, Rainy)P(No-umbrella, Rainy)P(Rainy)\n\nDryP(Umbrella, Dry)P(No-umbrella, Dry)P(Dry)\n\nP(Umbrella)P(No-umbrella)\n\n\n\n\n\nFinally, let’s use “proper” statistical notation. In the rainy day problem, the data corresponds to the observation that I do or do not have an umbrella. So we’ll let \\(d_1\\) refer to the possibility that you observe me carrying an umbrella, and \\(d_2\\) refers to you observing me not carrying one. Similarly, \\(h_1\\) is your hypothesis that today is rainy, and \\(h_2\\) is the hypothesis that it is not. Using this notation, the table looks like Table 16.6.\n\n\n\n\nTable 16.6:  Four possibilities combining rain (or not) and umbrella carrying (or not), expressed in hypothjetical terms as conditional probabilities \n\n\\( d_1 \\)\\( d_2 \\)\n\n\\( h_1 \\)\\(P(h_1, d_1)\\)\\(P(h_1, d_2)\\)\\( P(h_1) \\)\n\n\\( h_2 \\)\\(P(h_2, d_1)\\)\\(P(h_2, d_2)\\)\\( P(h_2) \\)\n\n\\( P(d_1) \\)\\( P(d_2) \\)\n\n\n\n\n\n\n\n16.1.4 透過貝氏法則更新信念\nThe table we laid out in the last section is a very powerful tool for solving the rainy day problem, because it considers all four logical possibilities and states exactly how confident you are in each of them before being given any data. It’s now time to consider what happens to our beliefs when we are actually given the data. In the rainy day problem, you are told that I really am carrying an umbrella. This is something of a surprising event. According to our table, the probability of me carrying an umbrella is only 8.75%. But that makes sense, right? A woman carrying an umbrella on a summer day in a hot dry city is pretty unusual, and so you really weren’t expecting that. Nevertheless, the data tells you that it is true. No matter how unlikely you thought it was, you must now adjust your beliefs to accommodate the fact that you now know that I have an umbrella.6 To reflect this new knowledge, our revised table must have the following numbers. (see Table 16.7).\n\n\n\n\nTable 16.7:  Revising beliefs given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0\n\nDry0\n\nTotal10\n\n\n\n\n\nIn other words, the facts have eliminated any possibility of “no umbrella”, so we have to put zeros into any cell in the table that implies that I’m not carrying an umbrella. Also, you know for a fact that I am carrying an umbrella, so the column sum on the left must be 1 to correctly describe the fact that \\(P(umbrella) = 1\\).\nWhat two numbers should we put in the empty cells? Again, let’s not worry about the maths, and instead think about our intuitions. When we wrote out our table the first time, it turned out that those two cells had almost identical numbers, right? We worked out that the joint probability of “rain and umbrella” was 4.5%, and the joint probability of “dry and umbrella” was 4.25%. In other words, before I told you that I am in fact carrying an umbrella, you’d have said that these two events were almost identical in probability, yes? But notice that both of these possibilities are consistent with the fact that I actually am carrying an umbrella. From the perspective of these two possibilities, very little has changed. I hope you’d agree that it’s still true that these two possibilities are equally plausible. So what we expect to see in our final table is some numbers that preserve the fact that “rain and umbrella” is slightly more plausible than “dry and umbrella”, while still ensuring that numbers in the table add up. Something like Table 16.8, perhaps?\n\n\n\n\nTable 16.8:  Revising probabilities given new data about umbrella carrying \n\nUmbrellaNo-umbrella\n\nRainy0.5140\n\nDry0.4860\n\nTotal10\n\n\n\n\n\nWhat this table is telling you is that, after being told that I’m carrying an umbrella, you believe that there’s a 51.4%) chance that today will be a rainy day, and a 48.6% chance that it won’t. That’s the answer to our problem! The posterior probability of rain \\(P(h\\|d)\\) given that I am carrying an umbrella is 51.4%\nHow did I calculate these numbers? You can probably guess. To work out that there was a \\(0.514\\) probability of “rain”, all I did was take the \\(0.045\\) probability of “rain and umbrella” and divide it by the \\(0.0875\\) chance of “umbrella”. This produces a table that satisfies our need to have everything sum to 1, and our need not to interfere with the relative plausibility of the two events that are actually consistent with the data. To say the same thing using fancy statistical jargon, what I’ve done here is divide the joint probability of the hypothesis and the data \\(P(d, h)\\) by the marginal probability of the data \\(P(d)\\), and this is what gives us the posterior probability of the hypothesis given the data that have been observed. To write this as an equation: 7\n\\[P(h|d)=\\frac{P(h|d)}{P(d)}\\]\nHowever, remember what I said at the start of the last section, namely that the joint probability \\(P(d, h)\\) is calculated by multiplying the prior Pphq by the likelihood \\(P(d|h)\\). In real life, the things we actually know how to write down are the priors and the likelihood, so let’s substitute those back into the equation. This gives us the following formula for the posterior probability:\n\\[P(h|d)=\\frac{P(d|h)P(h)}{P(d)}\\]\nAnd this formula, folks, is known as Bayes’ rule. It describes how a learner starts out with prior beliefs about the plausibility of different hypotheses, and tells you how those beliefs should be revised in the face of data. In the Bayesian paradigm, all statistical inference flows from this one simple rule."
  },
  {
    "objectID": "16-Bayesian-statistics.html#貝氏假設檢定",
    "href": "16-Bayesian-statistics.html#貝氏假設檢定",
    "title": "16  貝氏統計",
    "section": "16.2 貝氏假設檢定",
    "text": "16.2 貝氏假設檢定\n\n在 單元 9 中,我描述了正統的假設檢定方法。 這需要整整一章來描述,因為虛無假設檢定是一個非常精密的設備,人們覺得很難理解。 相反,貝氏假設檢定的方法簡直簡單得令人難以置信。 讓我們選擇一種與正統場景緊密類似的設置。 有兩個要比較的假設,虛無假設 \\(h_0\\) 和替代假設 \\(h_1\\)。 在運行實驗之前,我們對哪些假設是真實的有一些信念 \\(P(h)\\)。我們運行一項實驗並獲得數據 d。 與次數主義統計不同,貝氏統計允許我們討論虛無假設為真的機率。 更妙的是,它允許我們使用貝葉斯規則計算虛無假設的後驗機率:\n\\[P(h_0|d)=\\frac{P(d|h_0)P(h_0)}{P(d)}\\]\n該公式準確地告訴我們在觀察到數據 d 後,我們對虛無假設應該有多大的信念。 類似地,我們可以使用基本相同的方程式確定對替代假設的信任程度。 我們所要做的只是更改下標:\n\\[P(h_1|d)=\\frac{P(d|h_1)P(h_1)}{P(d)}\\]\n這非常簡單,以至於我覺得我連把這些方程寫下來都很傻,因為我所做的只是從前一節複製了貝氏規則。8\n在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。\n\n16.2.1 貝氏因子\n\n在實踐中,大多數貝氏數據分析師傾向於不以原始後驗機率 \\(P(h_0|d)\\) 和 \\(P(h_1|d)\\) 的形式談論。相反,我們傾向於以後驗賠率比(Posterior odds)的形式談論。可以想像成下注。例如,假設虛無假設的後驗機率為25%,替代假設的後驗機率為75%。替代假設的機率是虛無假設的三倍,所以我們說賠率比為3:1,有利于替代假設。數學上,我們計算後驗賠率比所要做的就是將一個假設的後驗機率除以另一個假設的後驗機率\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{0.75}{0.25}=3\\]\n或者,用這一節開始提到的公式改寫成\n\\[\\frac{P(h_1|d)}{P(h_0|d)}=\\frac{d|h_1}{d|h_0} \\times \\frac{h_1}{h_0}\\]\n實際上,這個等式值得擴展。 這裡有三個不同的術語您應該知道。 在左側,我們有後驗賠率比,它告訴您在看到數據后對虛無假設和替代假設的相對合理性的信念。 在右側,我們有先驗賠率比(Prior odds),它指示您在看到數據之前的想法。在中間,我們有貝氏因子(Bayes factor),它描述了數據提供的證據量。 (表 16.9)。\n\n\n\n\n表 16.9: 基於貝氏法則(Bayesian Rule)，可運用貝氏因子(Bayes factor)和先驗賠率比(Prior Odds)，估算後驗賠率比(Posterior odds)\n\n\n\\(\\frac{P(h_1|d)}{h_0|d}\\)\n\\(=\\)\n\\(\\frac{P(d|h_1)}{d|h_0}\\)\n\\(\\times \\)\n\\(\\frac{P(h_1)}{h_0}\\)\n\n\n\\(\\Uparrow\\)\n\n\\(\\Uparrow\\)\n\n\\(\\Uparrow\\)\n\n\nPosterior odds\n\nBayes factor\n\nPrior odds\n\n\n\n\n\n\n\n\n\n貝氏因子(有時縮寫為 BF)在貝氏假設檢定中佔有特殊地位,因為它在正統假設檢定中的 p 值起著類似作用。 貝氏因子量化了數據提供的證據力度,因此報告貝氏因子是人們在運行貝氏假設檢定時傾向於報告的內容。 報告貝氏因子而不是後驗賠率比的原因是不同的研究人員會有不同的先驗分布。有些人可能傾向於相信虛無假設為真,而其他人可能傾向於相信它為假。因此,禮貌的應用研究者應做的是報告貝氏因子。這樣,閱讀論文的任何人都可以將貝氏因子與自己的個人先驗賠率比相乘,並且他們可以自己算出後驗賠率比。無論如何,按照慣例,我們喜歡假裝我們給予虛無假設和替代假設同等考慮,在這種情況下,先驗賠率比等於1,後驗賠率比就等於貝氏因子。\n\n\n16.2.2 解讀貝氏因子\n\n\n貝氏因子的一個真正好處是這些數字本身就很有意義。如果您運行一項實驗並計算出貝氏因子為4,這意味著您的數據提供的證據對應於有利於替代假設的4:1的賭注比例。 然而,已經有一些嘗試量化在科學環境下被認為有意義的證據標准。最廣泛使用的兩個是來自 Jeffreys (1961) 和 Kass & Raftery (1995) 。在這兩者中,我傾向於更喜歡 Kass & Raftery (1995) 表,因為它更加保守。 所以在這裡它是(表 16.10)。\n\n\n\n\n表 16.10: 貝氏因子和證據力度\n\n\nBayes factor\nInterpretation\n\n\n1 - 3\nNegligible evidence\n\n\n3 - 20\nPositive evidence\n\n\n20 - 150\nStrong evidence\n\n\n&gt; 150\nVery strong evidence\n\n\n\n\n\n\n\n\n\n\n而且要老實說,我認為即使是 Kass & Raftery (1995) 的標準也有點太仁慈了。如果由我決定,我會將“積極證據”類別稱為“弱證據”。 對我來說,3:1 到 20:1 的任何範圍都只是“弱”或“溫和”的證據而已。 但這裡沒有硬性規定。什麼算作強或弱證據完全取決於您的保守程度以及您的社區在願意將一項發現標籤為“真”之前堅持的標準。\n無論如何,請注意,如果貝氏因子大於1(即證據有利於替代假設),那麼上面列出的所有數字都是有意義的。 然而,與正統方法相比,貝氏方法的一個大的實際優勢是它還允許您量化虛無假設的證據。 當這種情況發生時,貝氏因子將小於1。 您可以選擇報告小於1的貝氏因子,但說實話,我覺得這很容易造成困惑。 例如,假設在虛無假設 \\(h_0\\) 下數據的似然性 \\(P(d|h_0)\\) 等於0.2,而在替代假設 \\(h_1\\) 下對應的似然性 \\(P(d|h_1)\\) 為 0.1。 使用上面給出的方程式,這裡的貝氏因子為\n\\[BF=\\\\frac{P(d|h_1)}{P(d|h_0)}=\\\\frac{0.1}{0.2}=0.5\\]\n如果逐字理解,這個結果告訴我們,支持替代假設的證據為 0.5:1。 我覺得這很難理解。 對我來說,把等式“上下翻轉”更有意義,並報告支持虛無假設的證據量。 換句話說,我們計算的是\n\\[BF' = \\\\frac{P(d|h_0)}{P(d|h_1)}=\\\\frac{0.2}{0.1}=2\\]\n而我們要報告的是 2:1 的貝氏因子支持虛無假設。更容易理解,並且可以使用上面的表來解釋。"
  },
  {
    "objectID": "16-Bayesian-statistics.html#為何需要貝氏統計",
    "href": "16-Bayesian-statistics.html#為何需要貝氏統計",
    "title": "16  貝氏統計",
    "section": "16.3 為何需要貝氏統計",
    "text": "16.3 為何需要貝氏統計\n\n以下是翻譯初稿:\n到目前為止,我一直專注於支撐貝氏統計的邏輯基礎。我們討論了“機率作為信念程度”的觀念以及它隱含的關於理性代理人應如何推理世界的內容。您必須自行回答的問題是:您希望如何進行統計學? 您想作為正統統計學家依靠抽樣分布和 p 值來指導您的決策? 還是您想作為一個貝氏主義者,依靠先驗信念、貝氏因子和有理性信念修正規則等? 老實說,我不能為您回答這個問題。 最終,這取決於您認為什麼是正確的。 這是您的決定,僅您一人的決定。 雖然如此,我可以談談我為什麼更喜歡貝氏方法。\n\n16.3.1 統計學呈現你所相信的世界面貌\n\n\n\n“您一直在使用那個詞。我不認為它意味著您認為的意思”\n– 伊尼戈·蒙托亞,《公主新娘》9\n\n對我來說,貝氏方法的最大優勢之一是它能夠回答正確的問題。 在貝氏框架內,提到“假設為真的機率”是完全合理和允許的。 您甚至可以嘗試計算這個機率。 最終,這難道不是您希望統計檢驗告訴您的嗎? 對於真正的人類而言,這似乎是做統計的全部重點,即確定什麼是真的什麼不是真的。 每當您不完全確定真相時,都應使用機率論的語言來說些“理論 A 為真的機率為 80%,但理論 B 為真的機率為 20%”之類的話。\n對人類而言,這似乎非常明顯,但在正統框架中這明確被禁止了。 對次數主義者來說,這種陳述是無意義的,因為“理論是真實的”不是一個可重複的事件。一個理論要麼是真的,要麼是假的,不管您多麼希望做出這樣的機率陳述,都是不允許的。 這就是為什麼在 小單元 9.5 中我一再警告您不要將 p 值解釋為虛無假設為真的機率。這就是為什麼幾乎每本統計學教科書都不得不重複這一警告。 那是因為人們迫切希望這是正確的解釋。 儘管有次數主義教條,但終生教授本科生和每天進行數據分析的經驗告訴我,大多數實際的人類認為“假設為真的機率”不僅有意義,而且是我們最關心的事情。 這是一個非常吸引人的想法,甚至受過訓練的統計學家也會誤解試圖以這種方式解釋 p 值。例如,以下是 2013 年 Newspoll 官方報告中的一段話,解釋如何解釋他們的(次數主義)數據分析:10\n\n在整個報告中,在相關的地方,已注意到統計上顯著的變化。所有顯著性測試都是基於 95% 的置信水平進行的。這意味著如果注意到一項變化在統計上顯著,那麼實際變化發生的機率為 95%,並且僅僅是由於機會變異引起的。(強調添加)\n\n不!這不是 p &lt; .05 的意思。對次數主義統計學家來說,95% 的置信水平也不是這個意思。加粗部分就是完全錯誤的。正統方法不能告訴您“實際變化發生的機率為 95%”,因為這不是次數主義機率可以分配給的事件類型。對於意識形態次數主義者來說,這句話應該是沒有意義的。即使您是更務實的次數主義者,它仍然是 p 值的錯誤定義。如果您想依賴正統統計工具,那麼這句話就根本不被允許或正確。\n另一方面,假設您是貝氏主義者。儘管加粗通過是 p 值的錯誤定義,但當貝氏主義者說替代假設的後驗機率大於 95% 時,這幾乎正是他們的意思。 而這裡有一個問題。如果您實際上想報告的是貝氏後驗,那麼您為什麼還要嘗試使用正統方法呢? 如果您想做出貝氏主義者的認定,您所要做的就是成為貝氏主義者並使用貝氏工具。\n就我個人而言,我發現切換到貝氏觀點最自由的就是這一點。 一旦您轉換了思維方式,您就不再需要圍繞 p 值的反直覺定義困惑您的大腦。 您不必費心記住為什麼您不能說您 95% 確定真值落在某個區間內。 您所要做的只是誠實地說出您在運行該研究之前的信念,然後報告您從中學到了什麼。 聽起來不錯,不是嗎? 對我來說,這就是貝氏方法的大前提。 您可以真正地做出您想要的分析,並表達您真正相信數據正在告訴您的內容。\n\n\n16.3.2 你能相信的證據標準\n\n\n如果 \\(p\\)低於.02,則強烈表明 \\(null\\) 假設無法解釋全部事實。如果我們在 .05 畫一條常規線,並認為更小的 \\(p\\)值表示真實的差異,我們通常不會犯太大的錯誤。\n\n\n– 羅納德·費雪爵士 (Fisher, 1925)\n\n考慮上述羅納德·費雪爵士的引文,他是當今正統統計方法的奠基人之一。 如果有人有資格表達對 p 值預期功能的意見,那就是費雪了。 在摘自他的經典指南《研究工作者的統計方法》的這段話中,他非常清楚地說明了什麼是在 p &lt; .05 時拒絕虛無假設。 在他看來,如果我們認為 p &lt; .05 意味著“真實效應”,那麼“我們通常不會犯太大的錯誤”。 這種觀點並不罕見。 根據我的經驗,大多數從業人員的觀點與費雪的觀點非常相似。 從本質上講,假定 p &lt; .05 建議代表了相當嚴格的證據標准。\n這是真的嗎?解決這個問題的一種方法是嘗試將 p 值轉換為貝氏因子,並查看兩者的比較情況。 這並不容易,因為 p 值與貝氏因子是一種基本不同的計算,它們不測量同樣的事情。 然而,已經有一些嘗試找出兩者之間的關係,而且有些令人吃驚。 例如,Johnson (2013) 提出了一個相當有說服力的論點,即(至少對於 t 檢驗)p &lt; .05 的閾值大致對應於大約 3:1 到 5:1 之間的貝氏因子支持替代假設。 如果這是正確的,那麼費雪的說法有些牽強。 假設虛無假設一半時間為真(即 \\(H_0\\) 的先驗機率為 0.5),並使用這些數字計算在 p &lt; .05 拒絕虛無假設時虛無假設的後驗機率。 使用 Johnson (2013) 的數據,我們看到如果您在 p ă .05 時拒絕虛無假設,那麼大約 80% 的時候您是正確的。 我不知道您怎麼看,但在我看來,一個確保您在 20% 的決策中會錯誤的證據標准還不夠好。 事實仍然是,與費雪的說法完全相反,如果您在 p &lt; .05 時拒絕虛無假設,您確實會經常犯錯。 這根本不是一個非常嚴格的證據閾值。\n\n\n16.3.3 p值只是幻象\n\n\n – 出自電玩遊戲「傳送門」的迷因梗11\n\n\n在這一點上,您可能會認為真正的問題不在於正統統計學,只在於 p &lt; .05 的標準。在某種意義上,這是正確的。 Johnson (2013) 的建議並不是“現在每個人都必須成為貝氏主義者”。相反,建議是更明智的做法是將常規標準轉移到類似 p &lt; .01 的水平。這不是一個不合理的觀點,但在我看來,問題比這更嚴重一些。在我看來,大多數(但不是全部)正統假設檢定所構建的方式中存在一個相當大的問題。它們在研究人員如何進行研究方面極為天真,因此大多數 p 值都是錯誤的。\n聽起來像一個荒謬的說法,對嗎?好吧,思考一下以下情景。您提出了一個非常激動人心的研究假設,並設計了一項研究來測試它。您非常勤奮,所以您運行了功效分析以確定樣本量應該是多少,並進行了研究。您運行了您的假設檢定,跳出了一個 p 值為 0.072。真他媽的令人惱火,不是嗎?\n您應該怎麼做?以下是一些可能性:\n\n您得出結論認為沒有效應並嘗試將其作為無效果結果發表\n您猜測可能存在效應並嘗試將其作為“邊緣顯著”結果發表\n您放棄並嘗試新的研究\n您收集更多數據以查看 p 值是否上升或(更好的是!)下降至“魔力”標准 p &lt; .05 以下\n\n你會選哪一個? 在繼續閱讀之前,我強烈建議您花些時間思考一下。 跟自己誠實。 但不要為此費太多心思,因為無論您選擇什麼,您都是搞砸了。 基於我作為作者、評審員和編輯的親身經歷,以及我從其他人那裡聽到的故事,以下是每種情況下會發生的事情:\n\n讓我們從選項 1 開始。如果您嘗試將其作為無效果結果發表,論文的發表將會困難重重。一些評審員會認為 p = .072 並不是真的無效果。他們會認為這是邊緣顯著。其他評審員會同意這是無效果,但會聲稱儘管某些無效果結果是可以發表的,但您的研究不是。一兩個評審員甚至可能站在您這邊,但您在讓論文獲得接受方面將面臨一場苦戰。\n好的,讓我們考慮選項 2。假設您嘗試將其作為邊緣顯著結果發表。一些評審員會聲稱這是無效果並不應該發表。其他人會聲稱證據不明確,並且您應該收集更多數據直到獲得明確的顯著結果。同樣,發表過程並不青睞您。\n鑑於發表像 p = .072 這樣的“模糊”結果的困難,選項 3 似乎很誘人:放棄並做點其他的事情。但這是職業自殺的計劃。如果您每次面對模糊時都放棄並嘗試新的項目,那麼您的工作永遠不會被發表。如果您在學術領域沒有發表記錄,您可能會失去工作。因此該選項被排除。\n看起來您只剩下選項 4 了。您沒有決定性的結果,所以您決定收集更多數據並重新運行分析。看起來很合理,但不幸的是,如果您這樣做,那麼您所有的 p 值現在都是不正確的。全部都是。不僅僅是您為這項研究計算的 p 值。全部都是。您過去計算的所有 p 值以及您將來計算的所有 p 值。幸運的是,沒有人會注意到。您將得到發表,而且您撒謊了。\n\n等等,什麼?最后一部分怎麼可能是真的?我的意思是,這聽起來像一個非常合理的策略,不是嗎?您收集了一些數據,結果並不確定,所以現在您想要做的就是收集更多數據直到結果確定。這有什麼問題嗎?\n誠實地說,這沒有任何問題。這是一個合理的、明智的和理性的做法。在現實生活中,這正是每個研究人員所做的。不幸的是,如我在 單元 9 中描述的虛無假設檢定理論禁止您這樣做。12 原因是理論假設實驗已經結束並且所有的數據已經收集完畢。並且因為它假設實驗已經結束,它只考慮兩種可能的決策。如果您使用常規的 p &lt; .05 標準,這些決策如 表 16.11 所示。\n\n\n\n\n表 16.11: 常規虛無假設顯著性檢定 (NHST),p &lt; .05)。\n\n\nOutcome\nAction\n\n\np less than .05\nReject the null\n\n\np greater than .05\nRetain the null\n\n\n\n\n\n\n\n\n\n您正在做的是向決策問題添加第三種可能的行動。具體來說,您正在使用 p 值本身作為繼續該實驗的理由。因此,您將決策過程轉變為更類似于 表 16.12 的過程。\n基于初步測試中獲得的 p 值繼續收集數據。\n\n\n\n\n表 16.12: Carrying on data collecting based on p-values obtained in preliminary testing\n\n\nOutcome\nAction\n\n\np less than .05\nStop the experiment and reject the null\n\n\np between .05 and .1\nContinue the experiment\n\n\np greater than .1\nStop the experiment and retain the null\n\n\n\n\n\n\n\n\n\n\n我在 單元 9 中描述的虛無假設檢定的“基本”理論並沒有建立來處理這類事情。如果您是在現實生活中會選擇“收集更多數據”的那類人,這意味著您并沒有按照虛無假設檢定的規則進行決策。即使您碰巧與假設檢定得出相同的決定,您也沒有遵循其所暗示的決策過程,這种過程違規正是導致問題的原因。13 您的 p 值只是幻覺。\n更糟糕的是,它們以一種危險的方式撒謊,因為它們都_太小_了。為了讓您感受一下這有多嚴重,請考慮以下(最壞)情況。假設您是一位預算十分緊張的超級熱心的研究員,完全沒有注意我上面的警告。您設計了一項比較兩組的研究。您迫切希望在 \\(p &lt; .05\\) 水平上看到顯著結果,但您真的不想收集比需要的更多數據(因為這很昂貴)。為了節省成本,您開始收集數據,但每次有新的觀察值到來時,您都會對數據運行 t 檢驗。如果 t 檢驗說 \\(p &lt; .05\\),那麼您停止實驗並報告顯著結果。如果不是,您會繼續收集數據。您會一直這樣做,直到達到實驗的預定花費限額。假設該限制在 \\(N = 1000\\) 的觀察上觸發。事實證明,真相是沒有真正的效應:虛無假設為真。所以,您達到實驗結束並(正確地)得出沒有影響的結論的機率是多少?在理想的世界中,答案在此應該是 95%。畢竟,\\(p &lt; .05\\) 標准的全部要點是將型I錯誤率控制在 5% 左右,所以我們希望的就是在這種情況下錯誤拒絕虛無假設的機率只有 5%。然而,沒有保證這將是真的。您是在破壞規則。因為您反复運行測試,“偷看”數據以查看是否獲得了顯著結果,所以所有賭注都打消了。\n那麼情況究竟有多糟糕呢?答案如 圖 16.1 中實線所示,這是驚人的糟糕。如果您在每個單獨的觀察后查看數據,您將有 49% 的可能性會產生型I錯誤。也就是說,嗯,這比應該的 5% 大得多。相比之下,設想您使用了以下策略。開始收集數據。每次觀察到達時,運行 貝氏t檢定 並查看貝氏因子。我假設 Johnson (2013) 是正確的,並且我會將 3:1 的貝氏因子大致視為等同於 p 值為 .05。14 這次,我們的扣發狂研究人員使用以下過程。如果貝氏因子大於或等於 3:1 支持虛無假設,則停止實驗並保留虛無假設。如果它大於或等於 3:1 支持替代假設,則停止實驗並拒絕虛無假設。否則繼續測試。現在,就像上次一樣,假設虛無假設為真。會發生什麼?實際上,我也為此情況運行了模擬,結果如圖 圖 16.1 中的虛線所示。結果報表明,I 型錯誤率遠遠低於使用正統 t 檢驗獲得的 49%。\n\n\n\n\n\n圖 16.1: 如果每次有新數據到達時您重新運行測試,事情會變得多糟?如果您是次數主義者,答案是非常糟。\n\n\n\n\n\n在某種意義上,這很令人驚訝。正統虛無假設檢定的全部重點是控制型I錯誤率。貝氏方法實際上根本沒有設計來做這件事。然而,事實證明,當面對一個不斷運行假設檢定的“扳機快”研究人員時,貝氏方法更加有效。即使是大多數貝氏主義者認為可以接受的寬鬆 3:1 標準,也比 p &lt; .05 規則更安全。\n\n\n16.3.4 顯著性檢定真的有那麼糟嗎？\n\n在上一節中給出的例子是一種相當極端的情況。在現實生活中,人們不會在每次有新觀察值到達時運行假設檢定。所以說 p &lt; .05 標準“實際上”對應 49% 的型I錯誤率(即 \\(p = .49\\))並不公平。但事實仍然是,如果您想要您的 p 值是誠實的,那麼您要麼需要切換到一種完全不同的假設檢定方法,要麼執行嚴格的不偷看規則。您不允許使用數據來決定何時終止實驗。您不允許查看“邊緣” p 值並決定收集更多數據。您甚至不允許在查看數據后更改數據分析策略。您嚴格要求遵循這些規則,否則您計算的 p 值將是無意義的。\n而且是的,這些規則出人意料地嚴格。幾年前的一次課堂練習中,我要求學生考慮這種情況。假設您開始運行研究,意圖收集 \\(N = 80\\) 人。 當研究開始時,您遵循規則,拒絕查看數據或運行任何測試。 但是當您達到 \\(N = 50\\) 時,您的意志力屈服了……您偷看了一眼。 猜猜怎麼著? 您得到了顯著結果! 現在,當然,您知道您說過您會繼續運行研究,樣本量為 \\(N = 80\\),但現在這似乎有點毫無意義,不是嗎? 當樣本量為 \\(N = 50\\) 時結果就是顯著的,所以繼續收集數據豈不是浪費和低效嗎? 您不會感到誘惑停下來嗎?就是一點點? 請記住,如果您這樣做了,您在 \\(p &lt; .05\\) 水平上的型I錯誤率剛剛膨脹到了 8%。 當您在論文中報告 \\(p &lt; .05\\) 時,您真正在說的是 \\(p &lt; .08\\)。 這就是“只偷看一眼”的後果可能會有多糟。\n現在考慮一下。 科學文獻中充滿了 t 檢驗、方差分析、回歸和卡方檢驗。 當我寫這本書的時候,我並沒有隨意選擇這些測試。 出現在大多數入門統計教科書中的這四種工具的原因是它們是科學的主要工具。 這些工具中沒有一個包括處理“查看數據”的校正:它們都假定您不這樣做。 但這個假設有多現實呢? 在現實生活中,您認為在實驗結束前有多少人“查看”過數據並在看到數據的樣子后調整了后續行為呢? 除非採樣過程受外在約束的限制,否則我猜答案是“大多數人都這樣做過”。如果發生這種情況,您可以推斷報告的 p 值是錯誤的。 更糟糕的是,因為我們不知道他們實際遵循的決策過程,我們無從知道 p 值本該是麼。 如果不知道研究人員使用的決策制定過程,則無法計算 p 值。 所以報告的 p 值仍然在撒謊。\n鑑於上述所有內容,結論是麼呢? 並不是貝氏方法天衣無縫。如果一個研究者決心欺騙,他們總是可以這樣做的。 貝葉斯規則無法阻止人們撒謊,也無法阻止他們操縱實驗。 這不是我在此要論述的要點。 這個論點與本書一開始就強調的一樣:我們運行統計檢驗的原因是為了防止我們遭受自身的傷害。(見 小單元 1.1 ) 而“查看數據”之所以這麼令人擔憂的是因為它非常誘人,即使對誠實的研究人員也是如此。 推論理論必須承認這一點。 是的,您可能會通過說研究人員沒有正確使用它們來為 p 值辯護,但在我看來,這遺漏了重點。 一種統計推論理論如果對人類的了解是如此天真,甚至不考慮研究人員可能會查看自己的數據的可能性,那麼這種理論就不值得擁有。 從本質上講,我的論點是:\n\n好的法律起源於不良道德。\n– 安布羅西烏斯·馬克羅比烏斯 15\n\n良好的統計檢驗規則必須承認人性軟弱。 我們所有人都有罪。我們所有人都面臨誘惑。 一個良好的統計推論系統應該仍然有效,即使它是由真實的人使用的。 正統虛無假設檢定並未做到這一點。 16"
  },
  {
    "objectID": "16-Bayesian-statistics.html#貝氏t檢定",
    "href": "16-Bayesian-statistics.html#貝氏t檢定",
    "title": "16  貝氏統計",
    "section": "16.4 貝氏t檢定",
    "text": "16.4 貝氏t檢定\n\n本書中討論的一種重要的統計推論問題是比較兩個均值,在 單元 11 t檢定部分有詳細討論。如果您還記得那麼遠的內容,您會回憶起有幾種t檢定的版本。在本節中,我將稍微談論獨立樣本t檢定和配對樣本t檢定的貝氏版本。\n\n16.4.1 獨立樣本t檢定\n\n最常見的t檢定是獨立樣本t檢定,當您擁有類似於我們在 單元 11 t檢定部分使用的harpo.csv數據集的數據時就會出現。在這個數據集中,我們有兩組學生,那些接受Anastasia授課的學生和那些跟Bernadette上課的學生。我們要回答的問題是這兩組學生獲得的成績是否有任何區別。 在 單元 11 ,我建議您可以使用jamovi中的獨立樣本t檢定分析這種數據,它給我們 圖 16.2 的結果。由於我們獲得了小於0.05的p值,我們拒絕了虛無假設。\n\n\n\n\n\n圖 16.2: jamovi的獨立樣本t檢定結果報表\n\n\n\n\n\n\n那麼貝氏t檢定看起來是什麼樣子呢?我們可以通過在“測試”選項下選擇“貝氏因子”複選框並接受對“先驗”的建議默認值來獲得貝氏因子分析。這給出了 圖 16.3 表格中顯示的結果。我們在這個表中得到的貝氏因子統計量為1.75,這意味著這些數據提供的證據大約以1.8:1的比率支持替代假設。\n在繼續之前,有必要強調正統檢驗結果和貝氏檢驗結果之間的區別。根據正統檢驗,我們獲得了顯著結果,儘管只是勉强顯著。儘管如此,很多人會很高興地接受p = .043作為效應的合理有力的證據。相比之下,請注意,貝氏檢驗甚至沒有達到2:1的比率支持效應,最多被認為是非常弱的證據。根據我的經驗,這是一個非常典型的結果。在拒絕虛無假設之前,貝氏方法通常需要更多的證據。\n\n\n\n\n\n圖 16.3: 在獨立樣本t檢定報表並列貝氏因子\n\n\n\n\n\n\n16.4.2 相依樣本t檢定\n\n\n在 小單元 11.5 ,我討論了chico.csv數據集,其中測量了學生在兩個測試中的成績,我們有興趣了解從測試1到測試2的成績是否有所提高。因為每個學生都參加了兩次測試,所以我們使用的數據分析工具是相依樣本t檢定。 圖 16.4 顯示了jamovi結果報表,傳統的成對t檢定結果與貝氏因子分析並列。在這一點上,我希望您可以毫不費力地讀取這個輸出。數據提供了大約6000:1的證據支持替代假設。我們可能可以非常自信地拒絕虛無假設!\n\n\n\n\n\n圖 16.4: 在相依樣本t檢定報表並列貝氏因子"
  },
  {
    "objectID": "16-Bayesian-statistics.html#本章小結",
    "href": "16-Bayesian-statistics.html#本章小結",
    "title": "16  貝氏統計",
    "section": "16.5 本章小結",
    "text": "16.5 本章小結\n本章前半部主要討論貝氏統計學的理論基礎。理性者的機率推論這一節介紹貝氏推論的數學推導，接著概述貝氏假設檢定。然後我在為何需要貝氏統計說明希望同學學習貝氏統計的理由。\n後半部是實例說明及演練貝氏t檢定。如果同學想知道更多貝氏統計，市面上已經有許多好書問世，等著你去研讀學習。 Kruschke (2011) 撰寫的教科書實作貝氏資料分析(Doing Bayesian Data Analysis)結合理與實作範例，是不錯的第一選擇17。這本書並非以本章介紹的”貝氏因子”介紹具氏統計，所以你不必充分了解本章內容也能閱讀學習。如果是對認知心理學有興趣的同學，應該要找 Lee & Wagenmakers (2014) 的教科書做為進階學習。原作者推薦以上兩本是因為作者的專長領域相近，還有許多不同領域學者所撰寫的貝氏統計教科書或教程，有心學習的同學請好好留意。\n\n\n\n\n\nFisher, R. A. (1925). Statistical methods for research workers. Oliver & Boyd.\n\n\nJeffreys, H. (1961). The theory of probability (3rd ed.). Oxford.\n\n\nJohnson, V. E. (2013). Revised standards for statistical evidence. Proceedings of the National Academy of Sciences, 48, 19313–19317.\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of the American Statistical Association, 90, 773–795.\n\n\nKruschke, J. K. (2011). Doing Bayesian data analysis: A tutorial with R and BUGS. Academic Press.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive modeling: A practical course. Cambridge University Press."
  },
  {
    "objectID": "index.html#作者序",
    "href": "index.html#作者序",
    "title": "用jamovi上手統計學",
    "section": "作者序",
    "text": "作者序\n本書內容涵蓋各大學心理學、公共衛生或社會科學大學部基礎統計的學習項目。本書同時提供以jamovi做為處理資料的工具操作指引。如同一般的統計教科書，本書從描述統計及統計圖開始，接著討論機率理論，取樣及估計，還有虛無假設檢定。理解理論概念之後，接著學習統計方法：包括列聯表分析、相關、t檢定、迴歸、變異數分析以及因素分析。最後一章將介紹貝氏統計。"
  },
  {
    "objectID": "index.html#譯者序",
    "href": "index.html#譯者序",
    "title": "用jamovi上手統計學",
    "section": "譯者序",
    "text": "譯者序\n\n\n原書作者Danielle Navarro與David Foxcroft是任教於澳洲阿得雷德大學心理學系，教授統計學的講師，本書內容是根據Danielle Navarro自行開發的教學講義Learning Statistics with R改編，書中有許多例子是採用澳洲政府或民間機構收集的資料，中文化為保留原範例說明，於特定地理或風俗名稱增加譯註說明。除了解釋統計原理，原作者的大部分用詞接近日常對話，因此除了討論重要觀念的部分，中文翻譯儘可能使用日常用語。如果有部分內容需要進一步解釋或增加說明，可參考譯者自行增加的腳註。\n本書使用的專有名詞譯名，主要參考來源為國家教育研究院營運的樂詞網以及中文維基百科。未有正式翻譯的專有名詞則保留英文名稱；不同領域有不一樣的翻譯名詞，譯者儘可能優先考慮心理學或統計學的釋譯，未來學術界對於專有名詞有統一翻譯，本書將配合更新。本書的中文化內容採用使用OpenAI開發的自然語言聊天機器人ChatGPT及Anthropic開發的大型語言模型Claude 2.1進行初步翻譯，再由譯者手動編修。。正式公開之初已完成第一到第九單元的中文編修，第十到第十六單元只有完成部分原理說明的中文編修。前言表列編修進度，歡迎讀者使用在各單元網頁右側的編輯本頁面或問報問題常駐連結，協助編修及提出指正。有關中文化素材的使用授權，請讀者先參閱前言的授權說明，依指定授權方式使用。\n原作者有提供本書範例之電子檔，jamovi開發團隊已設定為主要學習資料庫。雖然jamovi操作介面正在進行繁體中文詞庫開發，為保持電子書與教學影的一致性，操作示範部分保留英文詞彙，不影響讀者以熟悉的介面語言學習及操作jamovi。使用這本電子書修習任何實體或線上課程的同學，建議先觀看導覽影片，了解如何使用這本書及jamovi開發團隊的資源學習。\n\n\n\n引用建議(英): Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#談一談辛普森悖論",
    "href": "01-Why-do-we-learn-statistics.html#談一談辛普森悖論",
    "title": "1  為什麼要學習統計",
    "section": "1.2 談一談辛普森悖論",
    "text": "1.2 談一談辛普森悖論\n接著來談一則真實故事(我想應該是真的)。1973年，美國加州大學柏克萊分校高層擔憂該年研究所的入學申請人數及錄取的狀況。更明白的說，他們覺得錄取的學生呈現性別不平等的狀況(見表1.4)。\n\n表1.4 柏克萊男女新生報考人數及錄取比例\n\n\n\n申請者人數\n通過比例\n\n\n男性\n8442\n44%\n\n\n女性\n4321\n35%\n\n\n\n\n當年的柏克萊校方擔心被申請入學的學生告上法院！由於將近有13,000名申請者，男女之間的錄取率相差9％，這樣的差距實在太大了，不可能是巧合。而且人數如此龐大，可說是鐵一般的事實。但是如果我對你說，這些數據實際上反映了對女性申請者些微的偏袒，你可能會認為我搞錯了或者有性別歧視。\n\n\n然而，實際情況卻有點出人意料。仔細檢視錄取資料後，有人發現了另一個版本的故事(Bickel et al., 1975)。具體地說，當校方按照學系逐一計算錄取率時，會看到多數學系的女性錄取率實際上略高於男性。表1.5 顯示了六個最多申請者的學系錄取情形（為了保障隱私，以下省略學系名稱）：\n\n表1.5 1973年伯克萊大學六個學系錄取學生的性別分佈\n\n\n\n男性\n\n女性\n\n\n\n學系\n申請者人數\n錄取比例\n申請者人數\n錄取比例\n\n\nA\n825\n62%\n108\n82%\n\n\nB\n560\n63%\n25\n68%\n\n\nC\n325\n37%\n593\n34%\n\n\nD\n417\n33%\n375\n35%\n\n\nE\n191\n28%\n393\n24%\n\n\nF\n272\n6%\n341\n7%\n\n\n\n\n令人費解的是，大多數系所的女性錄取率都比男性高！但是整個大學的女性錄取率卻低於男性。這怎麼可能？這兩種說法怎麼可能同時成立？\n\n其中究竟發生了什麼事。首先，請留意各系的錄取率並不相同：某些學系，像是A系和B系，傾向錄取最多合格申請者，而其他學系則寧缺勿濫，像是F系，即使申請者資質不差，也傾向不錄取多數申請者。表1.5顯示的六個學系，A系是最好上榜的，其他五系錄取率依遞減。其次請注意，男生和女生申請的學系並不相同。以男性申請者人數排序，會看到六個系的錄取率排序是A&gt;B&gt;D&gt;C&gt;F&gt;E(粗體字是“最好上“的學系)。整體而言，男生偏好申請錄取率高的學系。接著比較一下各系女性申請者的分佈情況。以女生申請者人數排序，就會發現六個系錄取率的排序是C&gt;E&gt;D&gt;F&gt;A&gt;B。也就是說，申請人數似乎顯示多數女生申請”很硬”的學系。事實上，如果我們看一下圖1.1，會發現這樣的趨勢是系統性的，並且非常顯著。這種效應被稱為辛普森悖論。這種效應並不常見，但確實在現實世界曾經發生，大多數人第一次遇到這種現象都會非常驚訝，甚至許多人拒絕相信這是真實的現象。但是這種現象再真實不過。雖然其中有很多很微妙的統計教訓，但我想用這個例子指出一個更重要的教訓：每個研究都是困難的，往往隱藏很多很微妙，違反人類直覺的陷阱等著不謹慎的人掉進去。這也是科學家喜歡統計的第二個原因，也是為什麼這門課要教研究方法的原因。因為科學很難，而且真相有時會巧妙地隱藏在複雜數據的縫隙之間。\n\n在結束這個主題之前，我想指出一些研究方法課程常常忽略的事情。那就是統計只解決了問題的一部分。記得我們一開始關注是伯克利大學的招生程序可能對女性申請者不公平。當我們檢視”聚合”的資料時，整體似乎指向柏克萊明顯歧視女性，但是當我們”分解”各個學系的資料並深入檢視男女生個人行為，其實資料顯示各學系招生狀況的差異。如果真的有偏見，其實是各系輕微偏好錄取女生。總錄取率的性別偏差其實是因為女生傾向選擇較難上榜的系所。從法律角度來看，大學高層並沒有任何責仼。要錄取誰當研究生，權責是在個別系所，並且每個系所都有充分的理由決定要怎麼做。在系所的層次，錄取決策幾乎是公平的（各系所偏好錄取女性偏好很微弱，並且不一致）。由於大學高層無法決定學生想申請哪些科系，並且決策權限在各系所，所以校方幾乎不能干預招生程序，也無需對任何偏見負起責任。\n\n\n\n\n\n\n\n圖 1.1: 1973年柏克萊大學招生數據，取自 Bickel et al. (1975) 的圖1。圖中85個點分別代表至少接受一位女性申請入學的學系，依不分性別錄取率與女性申請入學比例繪圖。圓圈代表申請者超過40人的學系；圓圈面積代表該系申請人數佔全校申請人數的比例。十字代表申請者未達40人的學系。\n\n\n\n\n這是一開始我用輕鬆閒話介紹這個案例的真正原由，但故事沒這麼簡單，對吧？畢竟，如果從社會學和心理學的角度來看這個問題，我們更想知道為什麼各系申請者有這麼大的性別差異。為什麼工程科學系的男性申請者比女性多，而英語系則是相反呢？為什麼那些女性申請者較多的科系，錄取率比男性申請者較多的系別低呢？為什麽女性申請人數較多的科系錄取率偏低，而男性申請人數較多科系錄取率卻偏高？即使每個科系取才程序都是公平的，但這是否也是一種性別偏見？也許是吧。我們可以探討為何多數男生想唸“硬科學”領域的系所，而女生大都偏好“人文”領域。我們還能探討為何人文領域系所錄取率低？是因為政府部門給的補助不夠多嗎？例如博士級職缺與政府能給的專案補助經費額度有關。這些都是造成性別偏誤的條件嗎？還是人文領域的價值並未被重視？如果政府官員都覺得人文領域只是“沒什麼用處的小玩意”，動輒刪減相關經費。這樣是不是一種公然的性別歧視？至此討論的各種問題，都超出統計能解決的範圍，不過都能形成有意義的研究專案。若是你想了解造成性別歧視的整體結構因果關係，可能要”聚合”與“分解”的資料都要檢視。若是只想探討柏克萊校內負責招生的各級部門如何決策，只要檢討“分解”的資料就行。\n\n簡而言之，有很多重要問題是無法只靠統計數據回答的，但是分析和解釋數據對於回答這些問題有相當巨大的作用。這就是為什麼你應該把統計學當成解析數據的工具，因為正好符合心理學等領域的需要。即使統計是一個非常好用的工具，人類在獲得謹慎思考的道路上並沒有捷徑。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#匯入非示範格式的資料檔",
    "href": "03-Getting-started-with-jamovi.html#匯入非示範格式的資料檔",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.5 匯入非示範格式的資料檔",
    "text": "3.5 匯入非示範格式的資料檔\n本書的示範資料都是以jamovi專案檔(.omv)或csv文字檔提供讀者操作7。不過在你的實際統計實務場景，非常有可能遇到各式各樣的資料檔。這個小節我分享一些很多人常遇到的其他格式的資料，以及不合乎格式的資料檔處理技巧。\n\n\n3.5.1 格式純出錯的文字資料檔\n首先來談談遇到純文字資料檔案不符合csv格式，要如何修正才能讓jamovi開啟。出錯的地方大致來自以下幾個細節，有時需要邊修邊試，才能讓純文字資料檔案符合csv格式：\n\n變項名稱。通常第一列應該是變項名稱而非資料，如果不是的話jamovi可以開啟，但是無法做正確的資料分析。這種狀況可以用試算表軟體重新開啟，手動編輯應該有的變項名稱。\n欄位間隔。合格的csv檔以半形逗號(,)區隔變項欄位，不過有些歐洲國家習慣用半形逗號代表小數點，因此csv檔的欄位間隔是以半形分號(;)代表。台灣各單位資料的csv檔會混合使用半形逗號與製表鍵(Tab鍵)代表欄位間隔。如果遇到請先置換並存檔後再用jamovi開啟。\n引號。如同 booksales.csv 的示範，csv資料檔案內的文字資料都是包 在兩個半形雙引號(“)之間。有的csv資料檔案會用半形單引號(’)。\n跳列資訊。有的csv檔案內會在開頭幾行插入非資料的說明資訊，因為不符合檔案格式，無法被正確讀取。\n遺漏值。統計實務最常要處理的狀況就是遺漏值。遺漏值的來源千奇百怪，用心的資料收集者會用特別的記號代表遺漏值。不論是數字還是文字資料，jamovi預設以NA代表遺漏值8。請注意用jamovi開啟資料檔之前，確保資料內的遺漏值都已轉換為預設記號。開啟資料庫之後，試算表介面可看到的遺漏值會以空白或灰階顯示。如果某個變項裡有不符合預設記號的遺漏值，可以開啟Data面板的Setup對話視窗，自行加入遺漏值記號。\n\n\n\n\n\n3.5.2 套裝軟體專用格式(如SPSS)\n如果你習慣了使用jamovi進行統計分析，在非本書示範的真實場景，你有可能要處理的資料檔是套裝軟體專用格式。最常見的是SPSS資料檔(.sav)，最新版本的jamovi都可以匯入，操作如同開啟csv資料檔。但是要注意資料中的遺漏值除非標記是”system missing”或jamovi的預設記號，其他遺漏值記號都會被當成是有效數值。建議第一次匯入SPSS資料庫，馬上匯出為csv資料檔，在試算表軟體裡處理好遺漏值再用jamovi重新開啟9。\n如果遺漏值問題能解決，jamovi能做和SPSS一樣的分析。其他套裝統計軟體的資料檔，像是SAS與STATA，也可以用同樣的操作處理。\n\n\n\n3.5.3 微軟Excel資料格式\n本書原作者很不喜歡資料以Excel格式存檔，因為用jamovi開啟Excel檔案問題更多。如果你處理的作業遇到的話，最好先用試算表軟體開啟，另存為csv文字檔，再用jamovi開啟。"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#本章小結",
    "href": "03-Getting-started-with-jamovi.html#本章小結",
    "title": "3  與jamovi的第一次接觸",
    "section": "3.9 本章小結",
    "text": "3.9 本章小結\n每本統計軟體教科書都會有這些大同小異的起始課程。本書也不例外，幫助讀者通過最少障礙上手jamovi。這一章的各節主題重點如下：\n\n安裝jamovi 下載、安裝、啟動jamovi\n分析模組 簡介主要介面各部分功能，像是如何開始分析，結果輸出位置。\n資料試算表 了解資料試算表的特色，各種變項設定，建立計算變項。\n載入資料檔案 示範開啟要分析的資料。\n匯入非示範格式的資料檔 開啟各種格式資料庫的注意事項。\n更動資料變項性質 如何改變變項尺度。\n安裝jamovi擴充模組 找到及安裝社區使用者開發的模組。\n關閉jamovi 關閉之前如何存檔的建議。\n\n到這裡還沒有真的開始處理資料，下一章才是真正的開始。"
  },
  {
    "objectID": "06-Pragmatic-matters.html#邏輯運算",
    "href": "06-Pragmatic-matters.html#邏輯運算",
    "title": "6  實務課題",
    "section": "6.2 邏輯運算",
    "text": "6.2 邏輯運算\n在jamovi進行資料轉換，多數狀況都要使用邏輯值設定轉換條件。邏輯值是指根據某種條件評估，評估結果為真或為假。用jamovi執行邏輯評估相當直覺，執行結果只有TRUE或FALSE兩種數值。儘管簡單，邏輯值在許多統計實務場景相當有用。以下介紹常見的運作範例。\n\n\n6.2.1 判斷算式真假值\n喬治．歐威爾的著名反烏托邦小說「1984」虛構的極權政府經常向不順從的人民宣傳「2 + 2 = 5」。這個設定表達了當政治極權壓過了人類的自由思想，有可能會顛覆人們對現實世界的基本認知。小說中驚悚的高潮，是當男主角史密斯．溫斯頓終於無法忍受刑求，接受了極權宣傳，同意「2 + 2 = 5」。小說有個主張「人的可塑性是無限的」，原作者相當不同意這樣的主張，同意的話就無法使用jamovi了。至少在處理基本邏輯運算的問題，jamovi不可能給出「刻意塑造」的答案。同學們可以試著在jamovi計算變項設定選單，輸入 \\(2 + 2\\) 看看會不會得到極權政府要人民相信的52！\n到目前所學的，都是使用jamovi進行計算。還沒有做過判斷 \\(2 + 2 = 4\\) 為真的例子。同學只要在計算變項設定選單的公式視窗裡輸入 \\(2 + 2 == 4\\) ，就會在試算表介面看到真值(true)。\n這個例子用到了一種邏輯運算子：相等運算子 \\(==\\) 3。我們現在可以用這個運算子，看看jamovi會不會服從極權統治，請在公式視窗輸入小說裡的判斷式：\n\\[2 + 2 == 5\\]\n這個例子是讓同學們牛刀小試，了解如何在jamovi裡設定邏輯判斷式。如果同學有機會使用其他程式語言測試，像是R，會得到錯誤訊息。最新版的jamovi也會偵測使用者輸入的相等運算子是否正確，如果輸入錯誤，就會得到相當醒目的錯誤訊息4。\n\n\n\n6.2.2 邏輯運算子\n透過上一節的簡單示範，我們了解邏輯運算子如何運作，結合其他運算子與函式會有更多用途。像是這兩個例子： \\(3 \\times 3 + 4 \\times 4 == 5 \\times 5\\) 還有 \\(SQRT(25) == 5\\)\n除此之外，還有其他的基本邏輯運算子，讓我們能在適合條件下運用，如同 表 6.2 的示範。希望同學能從示範中自行認識各種運算子的功能。像是運算子 \\(&lt;\\) 用來計算左邊的數值是不是小於右邊的數值。如果運算結果為真，jamovi會傳回TRUE。如果數值相等，或者右邊的數值實際小於左邊的數值，jamovi則會傳回FALSE。\n另一方面，小於或等於運算子 \\(&lt;=\\) 的功能正如字面的意思，只要左邊的數值小於或等於右邊的數值就會傳回TRUE。相對地，大於運算子 \\(&gt;\\) 與大於或等於運算子 \\(&gt;=\\) 則是會傳回相反的真假值。\n現在要理解不等於運算子 \\(!=\\) 的功能就比較簡單了。只有兩邊的數值都不一樣，運算結果才會是TRUE。如以下的例子，我們只要心算就知道 \\(2 + 2\\) 不會等於 \\(5\\)。同學可以開啟你的jamovi試試看。\n\n\\[2 + 2 \\text{ != } 5\\]\n\n\n\n\n表 6.2: 比較數值的邏輯運算子\n\n\n運算功能\n運算子\n輸入範例\n輸出\n\n\n\n\n小於\n&lt;\n2 &lt; 3\nTRUE\n\n\n小於或等於\n&lt;=\n2 &lt;= 2\nTRUE\n\n\n大於\n&gt;\n2 &gt; 3\nFALSE\n\n\n大於或等於\n&gt;=\n2 &gt;= 2\nTRUE\n\n\n等於\n==\n2 == 3\nFALSE\n\n\n不等於\n!=\n2 != 3\nTRUE\n\n\n\n\n\n\n\n\n在 表 6.3 還有三個邏輯運算子是我們必須學會的。他們是差集運算子\\(NOT\\)，交集運算子\\(and\\)，以及聯集運算子\\(or\\)。每個運算子的功能也如同名字的字面意思。像是我現在問同學”\\(2 + 2 = 4\\) 或 \\(2 + 2 = 5\\)“其中一個算式是真的嗎？同學一定要答”是“。因為用”或”連結的兩個命題，我們只要確定其中一個算式是真的，整個命題就為真。5\n\\[(2+2 == 4) \\text{ or } (2+2 == 5)\\]\n\n換個問法，如果現在是問你”\\(2 + 2 = 4\\) 且 \\(2 + 2 = 5\\)“都是真的嗎？你應該要答“否”。因為這個命題是交集，兩個算式都要為真，整個命題才為真。在jamovi計算變項設定，我們可以這樣編輯：\n\n\\[(2+2 == 4) \\text{ and } (2+2 == 5)\\]\n最後來看拗口的差集運算子。假如我問你“\\(2 + 2 = 5\\)這個算式非真嗎?”，你應該會答“是”。因為正確的命題應該是“\\(2 + 2 = 5\\)為假”。在jamovi計算變項設定，我們可以這樣編輯：\n\n\\[NOT(2+2 == 5)\\]\n也就是 \\(2+2 == 5\\) 的輸出是FALSE，\\(NOT(2+2 == 5)\\) 的輸出就是TRUE。這個例子讓我們曉得“非假”就是“真”。雖然現實世界許多事情真假難辨，至少在jamovi的世界一切非黑即白。資料放到jamovi不是真就是假，沒有灰色地帶。\n其實我們不需要用差集運算子，得到 \\(2+2 == 5\\) 為假的答案，使用“等於”運算子\\(==\\)就是FALSE的輸出，如果要得到TRUE的輸出，使用”不等於”運算子\\(!=\\)就行了：\n\n\\[2+2 \\text{ != } 5\\]\n\n\n\n\n表 6.3: 差集，交集以及聯集運算子\n\n\n運算功能\n運算子\n輸入範例\n輸出\n\n\n\n\n差集\nNOT\nNOT(1==1)\nFALSE\n\n\n聯集\nor\n(1==1) or (2==3)\nTRUE\n\n\n交集\nand\n(1==1) and (2==3)\nFALSE\n\n\n\n\n\n\n\n\n\n\n6.2.3 在報告中表達邏輯運算子\n(以下為AI初翻，尚待校稿)\n\n這是6.2.3節“在報告中表達邏輯運算子”的原始檔,其中使用’$’符號包圍的部分是latex code,要保持原樣不做翻譯。最後還有表格說明。我生成了以下繁體中文翻譯初稿:\n我也想簡要指出,您可以將這些邏輯運算符應用於文本以及邏輯數據。只是在理解jamovi如何解讀不同運算時,我們需要更加小心。在本節中,我將談論等於運算符\\(==\\)如何應用於文本,因為這是最重要的一個。顯然,不等於運算符\\(!=\\)會給出與\\(==\\)完全相反的答案,所以我也隱含地談到了這個運算符,但我不會給出顯示使用\\(!=\\)的具體命令。\n好的,讓我們看看它的工作原理。在某種意義上,這非常簡單。例如,我可以詢問jamovi“cat”一詞是否與“dog”一詞相同,像这样:\n“cat”\\(==\\)“dog”\n這很明顯,很高興知道即使是jamovi也能明白這一點。同樣,jamovi也確實認識到“cat”就是“cat”:\n“cat”\\(==\\)“cat”\n同樣,這正是我們的預期。然而,您需要记住的是,在語法和空格方面,jamovi完全不寬容。如果兩個字符串在任何方面有任何區別,jamovi會說它們不等於彼此,如下所示:\n” cat” \\(==\\) “cat”\n“cat” \\(==\\) “CAT”\n“cat” \\(==\\) “c a t”\n您也可以使用其他邏輯運算符。例如,jamovi還允許您使用\\&gt;和\\&lt;運算符來確定兩個文本“字符串”在字母順序上的先後順序。某種意義上。其實,這比較複雜,但讓我們從一個簡單的示例開始:\n“cat”\\(&lt;\\)“dog”\n在jamovi中,這個示例被評估為“真”。這是因為“cat”確實在字母順序上位於“dog”之前,所以jamovi判斷該語句為真。但是,如果我們要求jamovi告訴我們“cat”是否位於“anteater”之前,它會將該運算式評估為假。到目前為止都很好。但文本數據比字典所示的要複雜一些。“cat”和“CAT”怎麼樣?哪個在前?試試看:\n“CAT”\\(&lt;\\)“cat”\n這實際上被評估為“真”。換句話說,jamovi假定大寫字母在小寫字母之前。這很公平。不太可能有人會為此感到驚訝。您可能會感到驚訝的是,jamovi假設所有的大寫字母都在所有小寫字母之前。也就是說,“anteater”\\(&lt;\\)“zebra”是一個真語句,而大寫對應的“ANTEATER”\\(&lt;\\)“ZEBRA”也為真,但是“anteater”\\(&lt;\\)“ZEBRA”不是真的,如下所示。嘗試一下:\n“anteater”\\(&lt;\\)“ZEBRA”\n這被評估為“假”,這看起來有點反直覺。考慮到這一點,快速查看 表 6.4 可能會有幫助,其中列出了 jamovi 處理的各種文本字符的順序。\n\n\n\n\n表 6.4: 表內文本字符按照 jamovi 處理順序排列。\n\n\n\\( \\text{!} \\)\n\\( \\text{\"} \\)\n\\( \\# \\)\n\\( \\text{\\$} \\)\n\\( \\% \\)\n\\( \\& \\)\n\\( \\text{'} \\)\n\\( \\text{(} \\)\n\n\n\\( \\text{)} \\)\n\\( \\text{*} \\)\n\\( \\text{+} \\)\n\\( \\text{,} \\)\n\\( \\text{-} \\)\n\\( \\text{.} \\)\n\\( \\text{/} \\)\n0\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n9\n\\( \\text{:} \\)\n\\( \\text{;} \\)\n&lt;\n\\( \\text{=} \\)\n&gt;\n\\( \\text{?} \\)\n\\( \\text{@} \\)\n\n\nA\nB\nC\nD\nE\nF\nG\nH\n\n\nI\nJ\nK\nL\nM\nN\nO\nP\n\n\nQ\nR\nS\nT\nU\nV\nW\nX\n\n\nY\nZ\n\\( \\text{[} \\)\n\\( \\backslash \\)\n\\( \\text{]} \\)\n\\( \\hat{} \\)\n\\( \\_ \\)\n\\( \\text{`} \\)\n\n\na\nb\nc\nd\ne\ng\nh\ni\n\n\nj\nk\nl\nm\nn\no\np\nq\n\n\nr\ns\nt\nu\nv\nw\nx\ny\n\n\nz\n\\(\\text{\\{}\\)\n\\(\\text{|}\\)\n\\(\\text{\\}}\\)"
  },
  {
    "objectID": "06-Pragmatic-matters.html#數學函式及運算子",
    "href": "06-Pragmatic-matters.html#數學函式及運算子",
    "title": "6  實務課題",
    "section": "6.4 數學函式及運算子",
    "text": "6.4 數學函式及運算子\n\n這是下一節”數學函式及運算子“的原始文檔。我生成了以下繁体中文翻译初稿:\n在[變量轉換和編碼]一節中,我討論了變量轉換背后的思想,并展示了您可能想對數據應用的大多數轉換都基於相當簡單的數學函數和運算。在本節中,我想返回該討論,并提到幾個其他實際上對大多數現實世界數據分析很有用的數學函數和算術運算。表 6.5 簡要概述了我在這裡或之后要談論的各種數學函數。9 顯然,這甚至無法接近目錄化可用的可能性範圍,但它确实涵蓋了在數據分析中經常使用並在 jamovi 中可用的一系列函數。\n\n6.4.1 對數與指數\n\n正如我之前提到的,jamovi 中內置了很有用的數學函數範圍,并且試圖描述甚至列出它們所有內容真的没有太大意義。在大多數情况下,我只專注於本書所需的那些嚴格必要的函數。然而,我確實想對對數和指數函數做一個例外。儘管它們在本書的其他部分都不是必需的,但是 在更廣泛的統計學領域中它們無所不在。不僅如此,還有很多 情況下很方便分析變量的對數(即對變量采取“對數轉換”)。我認為這本書的許多(也許大多數)讀者以前都遇到過對數和指數,但根據過去的經驗,我知道有相當比例的社會科學統計課學生從高中以来就没有碰過對数,并會很感謝一些複習。\n為了理解對數和指數,最簡單的方法是实際計算它們并看它們如何與其他簡單計算相關。我特别想談論 jamovi 中的三个函數,即 LN()、LOG10() 和 EXP()。首先,讓我們考慮 LOG10(),它被稱為“以 10 為底的對數”。理解對數的技巧是理解它基本上是求冪的“相反”。具體而言,10 為底的對數與 10 的冪密切相關。那麼,讓我們從指出 10 的三次方是 1000 開始。數學上,我們將其寫為:\n\\[10^3=1000\\]\n理解對數的技巧是認識到“10 的 3 次方等于 1000”的語句相當於“1000 的對數(基數為 10)等於 3”的語句。數學上,我們將其写为:\n\\[log_{10}(1000)=3\\]\n好的,由於 LOG10() 函數與 10 的冪相關,您可能會期望還有與其他底數的冪相關的其他對數。當然,這是真的:在數學上,數字 10 没有真正特殊之處。您和我发现它很有用是因為十進制數字是圍繞數字 10 構建的,但是大壞數學世界嘲笑我們的十進制數字。遺憾的是,實際上宇宙并不在乎我們如何寫下數字。無論如何,這種宇宙漠不關心的後果是,計算以 10 為底的對數没有什麼特別之處。例如,您可以計算以 2 為底的對數。或者,我們在統計中看到的第三種對数類型,比 10 進制或 2 進制都多得多,稱為自然對數,並對應于以 e 為底的對數。既然您有朝一日可能會遇到它,我最好解釋一下 e 是什麼。e 數,稱為歐拉數,是那些令人討厭的“無理”數之一,其小數擴展是無限長的,並被認為是數學中最重要的數字之一。 e 的前几位數字是:\n\\[e = 2.718282\\]\n統計中需要我們計算 \\(e\\) 的幂的情况還是頗多的,儘管它們没有在本書出現。數字 \\(e\\) 的 \\(x\\) 次冪被稱為 \\(x\\) 的指數,所以通常可以看到 \\(e^x\\) 用 exp(x) 来写。 所以 jamovi 有一個計算指數的函數並不令人驚訝,該函數稱為 EXP()。因為數字 e 在統計中經常出現,所以自然對數(即以 e 為底的對數)也往往會出現。數學家經常把它寫成 \\(log_e(x)\\) 或 \\(ln(x)\\)。实际上,jamovi 的工作方式也是相同的:LN() 函數對應于自然對數。\n至此,我認為對於本書來說,我們已經有足够多的指數和對數了!"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群抽樣",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#樣本母群抽樣",
    "title": "8  運用樣本估計未知量數",
    "section": "8.1 樣本、母群、抽樣",
    "text": "8.1 樣本、母群、抽樣\nIn the Prelude to part IV I discussed the riddle of induction and highlighted the fact that all learning requires you to make assumptions. Accepting that this is true, our first task to come up with some fairly general assumptions about data that make sense. This is where sampling theory comes in. If probability theory is the foundations upon which all statistical theory builds, sampling theory is the frame around which you can build the rest of the house. Sampling theory plays a huge role in specifying the assumptions upon which your statistical inferences rely. And in order to talk about “making inferences” the way statisticians think about it we need to be a bit more explicit about what it is that we’re drawing inferences from (the sample) and what it is that we’re drawing inferences about (the population).\nIn almost every situation of interest what we have available to us as researchers is a sample of data. We might have run experiment with some number of participants, a polling company might have phoned some number of people to ask questions about voting intentions, and so on. In this way the data set available to us is finite and incomplete. We can’t possibly get every person in the world to do our experiment, for example a polling company doesn’t have the time or the money to ring up every voter in the country. In our earlier discussion of descriptive statistics in Chapter 4 this sample was the only thing we were interested in. Our only goal was to find ways of describing, summarising and graphing that sample. This is about to change.\n\n8.1.1 何謂母群\nA sample is a concrete thing. You can open up a data file and there’s the data from your sample. A population, on the other hand, is a more abstract idea. It refers to the set of all possible people, or all possible observations, that you want to draw conclusions about and is generally much bigger than the sample. In an ideal world the researcher would begin the study with a clear idea of what the population of interest is, since the process of designing a study and testing hypotheses with the data does depend on the population about which you want to make statements.\nSometimes it’s easy to state the population of interest. For instance, in the “polling company” example that opened the chapter the population consisted of all voters enrolled at the time of the study, millions of people. The sample was a set of 1000 people who all belong to that population. In most studies the situation is much less straightforward. In a typical psychological experiment determining the population of interest is a bit more complicated. Suppose I run an experiment using 100 undergraduate students as my participants. My goal, as a cognitive scientist, is to try to learn something about how the mind works. So, which of the following would count as “the population”:\n\nAll of the undergraduate psychology students at the University of Adelaide?\nUndergraduate psychology students in general, anywhere in the world?\nAustralians currently living?\nAustralians of similar ages to my sample?\nAnyone currently alive?\nAny human being, past, present or future?\nAny biological organism with a sufficient degree of intelligence operating in a terrestrial environment?\nAny intelligent being?\n\nEach of these defines a real group of mind-possessing entities, all of which might be of interest to me as a cognitive scientist, and it’s not at all clear which one ought to be the true population of interest. As another example, consider the Wellesley-Croker game that we discussed in the Prelude to part IV. The sample here is a specific sequence of 12 wins and 0 losses for Wellesley. What is the population? Again, it’s not obvious what the population is.\n\nAll outcomes until Wellesley and Croker arrived at their destination?\nAll outcomes if Wellesley and Croker had played the game for the rest of their lives?\nAll outcomes if Wellseley and Croker lived forever and played the game until the world ran out of hills?\nAll outcomes if we created an infinite set of parallel universes and the Wellesely/Croker pair made guesses about the same 12 hills in each universe?\n\n\n\n8.1.2 簡單隨機樣本\nIrrespective of how I define the population, the critical point is that the sample is a subset of the population and our goal is to use our knowledge of the sample to draw inferences about the properties of the population. The relationship between the two depends on the procedure by which the sample was selected. This procedure is referred to as a sampling method and it is important to understand why it matters.\nTo keep things simple, let’s imagine that we have a bag containing 10 chips. Each chip has a unique letter printed on it so we can distinguish between the 10 chips. The chips come in two colours, black and white. This set of chips is the population of interest and it is depicted graphically on the left of Figure 8.1. As you can see from looking at the picture there are 4 black chips and 6 white chips, but of course in real life we wouldn’t know that unless we looked in the bag. Now imagine you run the following “experiment”: you shake up the bag, close your eyes, and pull out 4 chips without putting any of them back into the bag. First out comes the a chip (black), then the c chip (white), then j (white) and then finally b (black). If you wanted you could then put all the chips back in the bag and repeat the experiment, as depicted on the right hand side of Figure 8.1. Each time you get different results but the procedure is identical in each case. The fact that the same procedure can lead to different results each time we refer to as a random process.1 However, because we shook the bag before pulling any chips out, it seems reasonable to think that every chip has the same chance of being selected. A procedure in which every member of the population has the same chance of being selected is called a simple random sample. The fact that we did not put the chips back in the bag after pulling them out means that you can’t observe the same thing twice, and in such cases the observations are said to have been sampled without replacement.\n\n\n\n\n\nFigure 8.1: Simple random sampling without replacement from a finite population\n\n\n\n\nTo help make sure you understand the importance of the sampling procedure, consider an alternative way in which the experiment could have been run. Suppose that my 5-year old son had opened the bag and decided to pull out four black chips without putting any of them back in the bag. This biased sampling scheme is depicted in Figure 8.2. Now consider the evidential value of seeing 4 black chips and 0 white chips. Clearly it depends on the sampling scheme, does it not? If you know that the sampling scheme is biased to select only black chips then a sample that consists of only black chips doesn’t tell you very much about the population! For this reason statisticians really like it when a data set can be considered a simple random sample, because it makes the data analysis much easier.\n\n\n\n\n\nFigure 8.2: Biased sampling without replacement from a finite population\n\n\n\n\nA third procedure is worth mentioning. This time around we close our eyes, shake the bag, and pull out a chip. This time, however, we record the observation and then put the chip back in the bag. Again we close our eyes, shake the bag, and pull out a chip. We then repeat this procedure until we have 4 chips. Data sets generated in this way are still simple random samples, but because we put the chips back in the bag immediately after drawing them it is referred to as a sample with replacement. The difference between this situation and the first one is that it is possible to observe the same population member multiple times, as illustrated in Figure 8.3.\n\n\n\n\n\nFigure 8.3: Simple random sampling with replacement from a finite population\n\n\n\n\nIn my experience, most psychology experiments tend to be sampling without replacement, because the same person is not allowed to participate in the experiment twice. However, most statistical theory is based on the assumption that the data arise from a simple random sample with replacement. In real life this very rarely matters. If the population of interest is large (e.g., has more than 10 entities!) the difference between sampling with- and without- replacement is too small to be concerned with. The difference between simple random samples and biased samples, on the other hand, is not such an easy thing to dismiss.\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\nAs you can see from looking at the list of possible populations that I showed above, it is almost impossible to obtain a simple random sample from most populations of interest. When I run experiments I’d consider it a minor miracle if my participants turned out to be a random sampling of the undergraduate psychology students at Adelaide university, even though this is by far the narrowest population that I might want to generalise to. A thorough discussion of other types of sampling schemes is beyond the scope of this book, but to give you a sense of what’s out there I’ll list a few of the more important ones.\n\nStratified sampling. Suppose your population is (or can be) divided into several different sub-populations, or strata. Perhaps you’re running a study at several different sites, for example. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. Stratified sampling is sometimes easier to do than simple random sampling, especially when the population is already divided into the distinct strata. It can also be more efficient than simple random sampling, especially when some of the sub-populations are rare. For instance, when studying schizophrenia it would be much better to divide the population into two 2 strata (schizophrenic and not-schizophrenic) and then sample an equal number of people from each group. If you selected people randomly you would get so few schizophrenic people in the sample that your study would be useless. This specific kind of of stratified sampling is referred to as oversampling because it makes a deliberate attempt to over-represent rare groups\nSnowball sampling is a technique that is especially useful when sampling from a “hidden” or hard to access population and is especially common in social sciences. For instance, suppose the researchers want to conduct an opinion poll among transgender people. The research team might only have contact details for a few trans folks, so the survey starts by asking them to participate (stage 1). At the end of the survey the participants are asked to provide contact details for other people who might want to participate. In stage 2 those new contacts are surveyed. The process continues until the researchers have sufficient data. The big advantage to snowball sampling is that it gets you data in situations that might otherwise be impossible to get any. On the statistical side, the main disadvantage is that the sample is highly non-random, and non-random in ways that are difficult to address. On the real life side, the disadvantage is that the procedure can be unethical if not handled well, because hidden populations are often hidden for a reason. I chose transgender people as an example here to highlight this issue. If you weren’t careful you might end up outing people who don’t want to be outed (very, very bad form), and even if you don’t make that mistake it can still be intrusive to use people’s social networks to study them. It’s certainly very hard to get people’s informed consent before contacting them, yet in many cases the simple act of contacting them and saying “hey we want to study you” can be hurtful. Social networks are complex things, and just because you can use them to get data doesn’t always mean you should.\nConvenience sampling is more or less what it sounds like. The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Snowball sampling is one type of convenience sampling, but there are many others. A common example in psychology are studies that rely on undergraduate psychology students. These samples are generally non-random in two respects. First, reliance on undergraduate psychology students automatically means that your data are restricted to a single sub-population. Second, the students usually get to pick which studies they participate in, so the sample is a self selected subset of psychology students and not a randomly selected subset. In real life most studies are convenience samples of one form or another. This is sometimes a severe limitation, but not always.\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\nOkay, so real world data collection tends not to involve nice simple random samples. Does that matter? A little thought should make it clear to you that it can matter if your data are not a simple random sample. Just think about the difference between Figure 8.1 and Figure 8.2. However, it’s not quite as bad as it sounds. Some types of biased samples are entirely unproblematic. For instance, when using a stratified sampling technique you actually know what the bias is because you created it deliberately, often to increase the effectiveness of your study, and there are statistical techniques that you can use to adjust for the biases you’ve introduced (not covered in this book!). So in those situations it’s not a problem.\nMore generally though, it’s important to remember that random sampling is a means to an end, and not the end in itself. Let’s assume you’ve relied on a convenience sample, and as such you can assume it’s biased. A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. When viewed from that perspective, I’d argue that we don’t need the sample to be randomly generated in every respect, we only need it to be random with respect to the psychologically-relevant phenomenon of interest. Suppose I’m doing a study looking at working memory capacity. In study 1, I actually have the ability to sample randomly from all human beings currently alive, with one exception: I can only sample people born on a Monday. In study 2, I am able to sample randomly from the Australian population. I want to generalise my results to the population of all living humans. Which study is better? The answer, obviously, is study 1. Why? Because we have no reason to think that being “born on a Monday” has any interesting relationship to working memory capacity. In contrast, I can think of several reasons why “being Australian” might matter. Australia is a wealthy, industrialised country with a very well-developed education system. People growing up in that system will have had life experiences much more similar to the experiences of the people who designed the tests for working memory capacity. This shared experience might easily translate into similar beliefs about how to “take a test”, a shared assumption about how psychological experimentation works, and so on. These things might actually matter. For instance, “test taking” style might have taught the Australian participants how to direct their attention exclusively on fairly abstract test materials much more than people who haven’t grown up in a similar environment. This could therefore lead to a misleading picture of what working memory capacity is.\nThere are two points hidden in this discussion. First, when designing your own studies, it’s important to think about what population you care about and try hard to sample in a way that is appropriate to that population. In practice, you’re usually forced to put up with a “sample of convenience” (e.g., psychology lecturers sample psychology students because that’s the least expensive way to collect data, and our coffers aren’t exactly overflowing with gold), but if so you should at least spend some time thinking about what the dangers of this practice might be. Second, if you’re going to criticise someone else’s study because they’ve used a sample of convenience rather than laboriously sampling randomly from the entire human population, at least have the courtesy to offer a specific theory as to how this might have distorted the results.\n\n\n8.1.5 母群參數與樣本統計\nOkay. Setting aside the thorny methodological issues associated with obtaining a random sample, let’s consider a slightly different issue. Up to this point we have been talking about populations the way a scientist might. To a psychologist a population might be a group of people. To an ecologist a population might be a group of bears. In most cases the populations that scientists care about are concrete things that actually exist in the real world. Statisticians, however, are a funny lot. On the one hand, they are interested in real world data and real science in the same way that scientists are. On the other hand, they also operate in the realm of pure abstraction in the way that mathematicians do. As a consequence, statistical theory tends to be a bit abstract in how a population is defined. In much the same way that psychological researchers operationalise our abstract theoretical ideas in terms of concrete measurements (Section 2.1), statisticians operationalise the concept of a “population” in terms of mathematical objects that they know how to work with. You’ve already come across these objects in Chapter 7. They’re called probability distributions.\nThe idea is quite simple. Let’s say we’re talking about IQ scores. To a psychologist the population of interest is a group of actual humans who have IQ scores. A statistician “simplifies” this by operationally defining the population as the probability distribution depicted in Figure 8.4 (a). IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. That is, we say that the population mean µ is 100 and the population standard deviation σ is 15.\n\n\n\n\n\nFigure 8.4: The population distribution of IQ scores (panel (a)) and two samples drawn randomly from it. In panel (b) we have a sample of 100 observations, and panel (c) we have a sample of 10,000 observations\n\n\n\n\nNow suppose I run an experiment. I select 100 people at random and administer an IQ test, giving me a simple random sample from the population. My sample would consist of a collection of numbers like this:\n106 101 98 80 74 … 107 72 100\nEach of these IQ scores is sampled from a normal distribution with mean 100 and standard deviation 15. So if I plot a histogram of the sample I get something like the one shown in Figure 8.4 (b). As you can see, the histogram is roughly the right shape but it’s a very crude approximation to the true population distribution shown in Figure 8.4 (a). When I calculate the mean of my sample, I get a number that is fairly close to the population mean 100 but not identical. In this case, it turns out that the people in my sample have a mean IQ of 98.5, and the standard deviation of their IQ scores is 15.9. These sample statistics are properties of my data set, and although they are fairly similar to the true population values they are not the same. In general, sample statistics are the things you can calculate from your data set and the population parameters are the things you want to learn about. Later on in this chapter I’ll talk about [Estimating population parameters] using your sample statistics and also [Estimating a confidence interval] but before we get to that there’s a few more ideas in sampling theory that you need to know about"
  },
  {
    "objectID": "09-Hypothesis-testing.html#值得繼續學習的主題",
    "href": "09-Hypothesis-testing.html#值得繼續學習的主題",
    "title": "9  假設檢定",
    "section": "9.9 值得繼續學習的主題",
    "text": "9.9 值得繼續學習的主題\n\n譯者註 20230418初步以ChatGPT-4完成翻譯，內容待編修。\n\n在本章中，我向您描述的是虛無假設顯著性檢驗（NHST）的正統框架。理解NHST如何運作是絕對必要的，因為自20世紀初成為主流以來，它一直是主要的推論統計方法。這是絕大多數在職科學家依賴的數據分析方法，因此即使您討厭它，也需要了解它。然而，這種方法並非沒有問題。框架中存在一些怪異之處，其產生的歷史奇觀，關於框架是否正確的理論爭議，以及對不慎的人來說有很多實際陷阱。我不打算詳細討論這個主題，但我認為簡要討論一下這些問題是值得的。\n\n\n9.9.1 尼曼與費雪\n您首先應該知道的是，正統虛無假設檢定實際上是將羅納德·費雪爵士和傑茲·尼曼提出的兩種相當不同的假設檢定方法混合在一起（有關歷史概要，請參見 Lehmann (2011) ）。歷史很混亂，因為費雪和尼曼是現實生活中的人，他們的觀點會隨著時間而改變，而且他們都沒有提供“如何解釋幾十年後他們的工作的明確說法”。也就是說，這是我對這兩種方法的快速總結。\n首先，讓我們談論費雪的方法。據我所知，費雪假設您只有一個假設（虛無假設），您想做的是找出虛無假設是否與數據不一致。從他的角度來看，您應該做的是檢查數據是否根據虛無假設是“足夠不可能”。實際上，如果您回想一下我們之前的討論，這就是費雪如何定義p值的。根據費雪的說法，如果虛無假設對數據的解釋非常差，那麼您可以放心地拒絕它。但是，由於您無法將其與其他假設進行比較，因此無法“接受對立假設”，因為您不一定有一個明確陳述的對立假設。這就是所有的內容。\n相比之下，尼曼認為假設檢定的目的是作為行動指南，他的方法比費雪的更正式。他認為，您可以做很多事情（接受虛無假設或接受對立假設），檢驗的目的是告訴您數據支持哪一個。從這個角度來看，確定對立假設至關重要。如果您不知道對立假設是什麼，那麼您就不知道測試的有效性，甚至不知道哪個動作是有意義的。他的框架確實需要不同假設之間的競爭。對於尼曼來說，\\(p\\)值並未直接衡量在虛無假設下的數據（或更極端的數據）的機率，而更像是一個抽象描述，關於哪些“可能的測試”告訴您接受虛無假設，以及哪些“可能的測試”告訴您接受對立假設。\n如您所見，今天的虛無假設檢定是兩者之間的奇怪混合物。我們談論擁有虛無假設和對立假設（尼曼），但通常[^09-hypothesis-testing-13]根據極端數據來定義\\(p\\)值（費雪），但我們仍然有α值（尼曼）。部分統計檢驗具有明確指定的對立假設（尼曼），但其他則相對含糊（費雪）。至少根據某些人的看法，我們不允許談論接受對立假設（費雪）。假設檢定是至今還是一團混亂的統計學歷史遺產，但我希望這至少能解釋為什麼會這樣。\n[^09-hypothesis-testing-13]：儘管這本書描述了尼曼和費雪對\\(p\\)值的定義，但大多數不會。大多數入門教材只會給你費雪版本。\n\n\n\n9.9.2 貝氏統計與次數主義統計\n在統計檢定的p值這一節，我強調不能將p值解釋為虛無假設為真的機率。NHST基本上是一個頻率主義工具（參見@sec-Introduction-to-probability），因此它不允許您為假設分配機率。虛無假設要么是對的，要么是錯的。貝氏統計方法將機率解釋為信念程度，因此完全可以說虛無假設為真的機率是\\(10\\%\\)。這只是反映了您對這個假設的信心程度。在頻率主義方法中，您不能這樣做。請記住，如果您是頻率主義者，機率只能根據大量獨立重複實驗（即長期頻率）來定義。如果這是您對機率的解釋，那麼談論“虛無假設的機率”就是完全無意義的：虛無假設要么是對的，要么是錯的。您無法將這個陳述的長期頻率表達出來。談論“虛無假設的機率”就像談論“自由的顏色”一樣無意義。它沒有顏色！\n最重要的是，這不僅僅是一個純粹的意識形態問題。如果您決定成為貝氏學派，並且您可以對假設做出機率陳述，那麼您必須遵循貝氏規則來計算這些機率。在 單元 16 中我將進一步談論這個問題，但目前我想指出的是，p值對於\\(H_0\\)為真的機率是一個糟糕的近似。如果您想知道虛無假設的機率，那麼p值並非您要尋找的東西！\n\n\n\n9.9.3 決策陷阱\n如您所見，假設檢定背後的理論是一團糟，甚至到現在統計學家們還在爭論它“應該”如何運作。然而，統計學家之間的分歧不是我們真正關心的問題。我們真正關心的是實用數據分析。儘管“正統”的虛無假設顯著性檢驗存在許多缺陷，但即使是像我這樣毫不猶豫的貝氏學者也會同意，如果使用得當，它們是有用的。大多數時候，它們會給出合理的答案，並且您可以用它們來學習有趣的事物。撇開我們已經討論過的各種意識形態和歷史混亂，事實仍然是統計中最大的危險在於不加思考。我不是指愚蠢，而是字面意思上的不加思考。在沒有花時間考慮每個檢驗實際上對數據說了什麼，以及檢查這是否與您的解釋一致的情況下，匆忙地解釋結果。這就是最大的陷阱所在。\n舉個例子，考慮以下例子（參見 Gelman & Stern (2006) ）。假設我正在進行ESP研究，並且我已經決定分別為男性參與者和女性參與者分析數據。在男性參與者中，有\\(33\\)人中的\\(50\\)人猜對了卡片的顏色。這是一個顯著效應（\\(p=.03\\)）。在女性參與者中，有\\(29\\)人中的\\(50\\)人猜對了。這不是一個顯著效應（\\(p=.32\\)）。看到這一點，人們極其想知道為什麼男性和女性在通靈能力方面存在差異。然而，這是錯誤的。如果你仔細想想，我們實際上並沒有運行一個明確比較男性和女性的測試。我們所做的一切就是將男性與機會進行比較（二項檢驗顯著）和將女性與機會進行比較（二項檢驗不顯著）。如果我們想要認為男性和女性之間存在實際差異，我們可能應該運行一個虛無假設沒有差異的檢驗！我們可以使用另一種假設檢定14，但是當我們這樣做時，事實證明，我們並沒有證據表明男性和女性之間存在顯著差異（\\(p=.54\\)）。現在，您是否認為兩組之間存在根本差異？當然不是。這裡發生的情況是，兩組數據（男性和女性）都非常接近邊界。純粹憑機運，其中一組恰好落在\\(p=.05\\)這條神奇的線上，而另一組則沒有。這並不意味著男性和女性之間存在差異。這個錯誤非常普遍，您應該時刻警惕。顯著與不顯著之間的差異並不能證明存在實際差異。如果您想說兩組之間存在差異，那麼您必須測試該差異！\n上面的例子僅僅是一個例子。我將其單獨拿出來，因為它非常常見，但更大的畫面是數據分析可能很難做對。思考您要測試什麼，為什麼要測試它，以及您的測試結果是否可能在現實世界中有意義。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#兩種決策失誤",
    "href": "09-Hypothesis-testing.html#兩種決策失誤",
    "title": "9  假設檢定",
    "section": "9.2 兩種決策失誤",
    "text": "9.2 兩種決策失誤\n在深入了解如何設定統計檢定程序之前，理解其哲學基礎是很有幫助的。我(原作者)曾經提過，虛無假設檢定和法庭審判之間的相似之處，不過現在要說得更清楚。最理想情況，我們希望每個檢定的結果都不會出錯。不幸的是，由於現實世界太複雜了，這是不可能達成的理想。有時候只是運氣不好，例如像是你測試擲一枚硬幣是否符合隨機，但是連續擲了10次，每次都是正面朝上。這似乎是表明硬幣不夠隨機的有力證據。然而，即使硬幣是完全公平的，出現這種結果的機率是1/1024。換句話說，在現實生活中，我們必須接受假設檢定出錯的可能性。因此，統計假設檢定的目標不是消除錯誤，而是儘可能減低錯誤的機率。\n說到這裡，我們需要更精確地定義什麼是「錯誤」。首先要知道虛無假設最後會被判定為真或為假，會發生什麼狀況。也就是根據檢定，我們會保留或拒絕虛無假設5。如同 表 9.2 所示，在我們進行檢定並做出選擇後，可能會發生的四種狀況：\n\n\n\n\n\n表 9.2: 圖解虛無假設檢定(NHST)的四種結果\n\n\n\nretain \\( H_0 \\)\nreject \\( H_0 \\)\n\n\n\\( H_0 \\) is true\ncorrect decision\nerror (type I)\n\n\n\\( H_0 \\) is false\nerror (type II)\ncorrect decision\n\n\n\n\n\n\n\n\n我們看到實際上有兩種不同的錯誤類型。如果我們拒絕了實際為真的虛無假設，那麼我們就是犯了型一錯誤。另一方面，當虛無假設實際上是錯的，我們仍然保留它，那麼我們就犯了型二錯誤。\n還記得我之前說過統計檢定有點像法庭審判嗎？我這樣說是認真的。要認定被告有罪，審判長會要求你要提出「超越合理的懷疑」。所有關於證據的法律規定（至少在理論上），都是確保幾乎沒有人能錯誤地判定一個無辜的被告有罪。審判標準如此嚴格的目的是保護被告的權利，英國法官威廉·布萊克斯通曾說過「與其冤枉一個無辜的人，不如放過十個有罪的人。」換句話說，法庭裡審判長不會等價看待兩種類型的錯誤。因為冤枉無辜者的代價遠高於讓罪犯逍遙法外的代價。統計檢定基本上也是這樣。可靠的假設檢定最重要設計原則是控制「型一錯誤」的發生機率，講求控制在某個固定機率以下。這個機率通常以 \\(\\alpha\\) 表示，稱為檢定的顯著水準。我再強調一次，因為這是整個設計的核心，如果「型一錯誤率」不大於 \\(\\alpha\\)，那麼假設檢定的顯著水準就是 \\(\\alpha\\)。\n那麼要怎麼處理型二錯誤率呢？我們也希望能控制在一定範圍內，通常用 \\(\\beta\\) 來表示型二錯誤的發生機率。不過更常見做法的是估計檢定力，也就是虛無假設為假時，拒絕虛無假設的機率，這個機率是 \\(1 - \\beta\\)。為了更好地理解，我們將 表 9.2 改寫一下，並加入相關數字（見 表 9.3）：\n\n\n\n\n\n\n表 9.3: 進一步解析虛無假設檢定(NHST)的四種結果\n\n\n\nretain \\( H_0 \\)\nreject \\( H_0 \\)\n\n\n\\( H_0 \\) is true\n1-\\( \\alpha \\) (probability of correct retention)\n\\(\\alpha\\) (type I error rate)\n\n\n\\( H_0 \\) is false\n\\(\\beta\\) (type II error rate)\n\\(1 - \\beta\\) (power of the test)\n\n\n\n\n\n\n\n\n一個「有力」的假設檢定是指保持最小 \\(\\beta\\) 值，同時保持 \\(\\alpha\\) 在預期的水準。依照領域慣例，科學家通常會使用三種 \\(\\alpha\\) 水準：\\(0.05\\)、\\(0.01\\) 和 \\(0.001\\)。值得注意的是，兩者的控制強度有一種不對稱性：檢定的目的是確保 \\(\\alpha\\) 值如同預期的小，但是對於 \\(\\beta\\) 則不預其能控制到相應的水準。當然，我們也希望型二錯誤率能夠保持最小，並設計一些測試來實現，但這方面的控制需求通常不如型一錯誤率的迫切。倘若把布萊克斯通 的話換成統計學家的說法，那就是說“保留十個結論錯誤的虛無假設，總比拒絕一個真實的假設要好”。老實說，我不完全同意這種哲學觀點。在某些情況下，我認為這種觀點是有道理的，其他情況則不然。但是這不是重點，重點是這就是假設檢定的設計原則。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "href": "12-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "title": "10  相闗與線性迴歸",
    "section": "10.4 線性迴歸模型的參數估計",
    "text": "10.4 線性迴歸模型的參數估計\nOkay, now let’s redraw our pictures but this time I’ll add some lines to show the size of the residual for all observations. When the regression line is good, our residuals (the lengths of the solid black lines) all look pretty small, as shown in Figure 10.12 (a), but when the regression line is a bad one the residuals are a lot larger, as you can see from looking at Figure 10.12 (b). Hmm. Maybe what we “want” in a regression model is small residuals. Yes, that does seem to make sense. In fact, I think I’ll go so far as to say that the “best fitting” regression line is the one that has the smallest residuals. Or, better yet, since statisticians seem to like to take squares of everything why not say that:\n\nThe estimated regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\), are those that minimise the sum of the squared residuals, which we could either write as \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) or as \\(\\sum_i \\epsilon_i^2\\).\n\n\n\n\n\n\nFigure 10.12: A depiction of the residuals associated with the best fitting regression line (panel a), and the residuals associated with a poor regression line (panel b). The residuals are much smaller for the good regression line. Again, this is no surprise given that the good line is the one that goes right through the middle of the data\n\n\n\n\nYes, yes that sounds even better. And since I’ve indented it like that, it probably means that this is the right answer. And since this is the right answer, it’s probably worth making a note of the fact that our regression coefficients are estimates (we’re trying to guess the parameters that describe a population!), which is why I’ve added the little hats, so that we get \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\) rather than \\(b_0\\) and \\(b_1\\). Finally, I should also note that, since there’s actually more than one way to estimate a regression model, the more technical name for this estimation process is ordinary least squares (OLS) regression.\nAt this point, we now have a concrete definition for what counts as our “best” choice of regression coefficients, \\(\\hat{b}_0\\) and \\(\\hat{b}_1\\). The natural question to ask next is, if our optimal regression coefficients are those that minimise the sum squared residuals, how do we find these wonderful numbers? The actual answer to this question is complicated and doesn’t help you understand the logic of regression.6 This time I’m going to let you off the hook. Instead of showing you the long and tedious way first and then “revealing” the wonderful shortcut that jamovi provides, let’s cut straight to the chase and just use jamovi to do all the heavy lifting.\n\n10.4.1 實作線性迴歸模型\nTo run my linear regression, open up the ‘Regression’ - ‘Linear Regression’ analysis in jamovi, using the parenthood.csv data file. Then specify dani.grump as the ‘Dependent Variable’ and dani.sleep as the variable entered in the ‘Covariates’ box. This gives the results shown in Figure 10.13, showing an intercept \\(\\hat{b}_0 = 125.96\\) and the slope \\(\\hat{b}_1 = -8.94\\). In other words, the best fitting regression line that I plotted in Figure 10.11 has this formula:\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 10.13: A jamovi screenshot showing a simple linear regression analysis\n\n\n\n\n\n\n10.4.2 解讀線性迴歸模型參數估計\nThe most important thing to be able to understand is how to interpret these coefficients. Let’s start with \\(\\hat{b}_1\\), the slope. If we remember the definition of the slope, a regression coefficient of \\(\\hat{b}_1 = -8.94\\) means that if I increase Xi by 1, then I’m decreasing Yi by 8.94. That is, each additional hour of sleep that I gain will improve my mood, reducing my grumpiness by 8.94 grumpiness points. What about the intercept? Well, since \\(\\hat{b}_0\\) corresponds to “the expected value of \\(Y_i\\) when \\(X_i\\) equals 0”, it’s pretty straightforward. It implies that if I get zero hours of sleep (\\(X_i = 0\\)) then my grumpiness will go off the scale, to an insane value of (\\(Y_i = 125.96\\)). Best to be avoided, I think."
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "href": "12-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "title": "10  相闗與線性迴歸",
    "section": "10.8 迴歸係數的更多資訊",
    "text": "10.8 迴歸係數的更多資訊\nBefore moving on to discuss the assumptions underlying linear regression and what you can do to check if they’re being met, there’s two more topics I want to briefly discuss, both of which relate to the regression coefficients. The first thing to talk about is calculating confidence intervals for the coefficients. After that, I’ll discuss the somewhat murky question of how to determine which predictor is most important.\n\n10.8.1 迴歸係數的信賴區間\nLike any population parameter, the regression coefficients b cannot be estimated with complete precision from a sample of data; that’s part of why we need hypothesis tests. Given this, it’s quite useful to be able to report confidence intervals that capture our uncertainty about the true value of \\(b\\). This is especially useful when the research question focuses heavily on an attempt to find out how strongly variable \\(X\\) is related to variable \\(Y\\) , since in those situations the interest is primarily in the regression weight \\(b\\).\n[Additional technical detail13]\nIn jamovi we had already specified the ‘95% Confidence interval’ as shown in Figure 10.15, although we could easily have chosen another value, say a ‘99% Confidence interval’ if that is what we decided on.\n\n\n10.8.2 標準化迴歸係數的計算方法\nOne more thing that you might want to do is to calculate “standardised” regression coefficients, often denoted \\(\\beta\\). The rationale behind standardised coefficients goes like this. In a lot of situations, your variables are on fundamentally different scales. Suppose, for example, my regression model aims to predict people’s \\(IQ\\) scores using their educational attainment (number of years of education) and their income as predictors. Obviously, educational attainment and income are not on the same scales. The number of years of schooling might only vary by 10s of years, whereas income can vary by \\(10,000s\\) of dollars (or more). The units of measurement have a big influence on the regression coefficients. The b coefficients only make sense when interpreted in light of the units, both of the predictor variables and the outcome variable. This makes it very difficult to compare the coefficients of different predictors. Yet there are situations where you really do want to make comparisons between different coefficients. Specifically, you might want some kind of standard measure of which predictors have the strongest relationship to the outcome. This is what standardised coefficients aim to do.\nThe basic idea is quite simple; the standardised coefficients are the coefficients that you would have obtained if you’d converted all the variables to z-scores before running the regression.14 The idea here is that, by converting all the predictors to z-scores, they all go into the regression on the same scale, thereby removing the problem of having variables on different scales. Regardless of what the original variables were, a \\(\\beta\\) value of 1 means that an increase in the predictor of 1 standard deviation will produce a corresponding 1 standard deviation increase in the outcome variable. Therefore, if variable A has a larger absolute value of \\(\\beta\\) than variable B, it is deemed to have a stronger relationship with the outcome. Or at least that’s the idea. It’s worth being a little cautious here, since this does rely very heavily on the assumption that “a 1 standard deviation change” is fundamentally the same kind of thing for all variables. It’s not always obvious that this is true.\n[Additional technical detail15]\nTo make things even simpler, jamovi has an option that computes the \\(\\beta\\) coefficients for you using the ‘Standardized estimate’ checkbox in the ‘Model Coefficients’ options, see results in Figure 10.16.\n\n\n\n\n\nFigure 10.16: Standardised coefficients, with 95% confidence intervals, for multiple linear regression\n\n\n\n\nThese results clearly show that the dani.sleep variable has a much stronger effect than the baby.sleep variable. However, this is a perfect example of a situation where it would probably make sense to use the original coefficients b rather than the standardised coefficients \\(\\beta\\). After all, my sleep and the baby’s sleep are already on the same scale: number of hours slept. Why complicate matters by converting these to z-scores?"
  },
  {
    "objectID": "16-Bayesian-statistics.html#理性者的機率推論",
    "href": "16-Bayesian-statistics.html#理性者的機率推論",
    "title": "16  貝氏統計",
    "section": "16.1 理性者的機率推論",
    "text": "16.1 理性者的機率推論\n\n從貝氏的角度來看,統計推論都是關於信念修正. 我從一組有關世界的候選假設 h 出發。我不知道這些假設中哪一個是真的,但我確實對哪些假設是合理的和哪些不是有一些信念。當我觀察到數據 d 時,我必須修正那些信念。如果數據與某個假設一致,那麼我對該假設的信心就會增強。如果數據與假設不一致,那麼我對該假設的信心就會減弱。就這樣! 在本節的末尾,我將給出貝葉斯推理如何運作的精確描述,但首先我想通過一個簡單的示例來介紹關鍵思想。考慮以下推理問題。\n\n我帶著雨傘。你認為會下雨嗎?\n\n在這個問題中,我向您提供了一個數據(d = 我帶著雨傘),並要求您告訴我您對是否下雨的信念或假設。您有兩個選擇,h:今天要麼下雨,要麼不下雨。你應該如何解決這個問題?\n\n16.1.1 事前機率：你一開始的信念\n\n以下是翻譯初稿:\n你需要做的第一件事是忽略我告訴你的有關雨傘的信息,並記下你預先存在的關於雨水的信念。這很重要。如果您想誠實地說明您的信念在新的證據(數據)出現後是如何修正的,那麼您必須說明您在那些數據出現之前相信什麼! 那麼,您可能相信今天是否會下雨呢?您可能知道我住在澳大利亞,澳大利亞的大部分地區都很熱,也很乾燥。我所居住的阿德萊德市擁有地中海氣候,非常類似於南加利福尼亞州、南歐或北非。我是在1月寫這篇文章的,所以您可以假設這是夏季中期。事實上,您可能決定在維基百科2上快速查找,並發現阿德萊德在1月的31天裡平均有4.4天降雨。在不知道任何其他信息的情況下,您可能得出的結論是,阿德萊德1月降雨的機率約為15%,乾燥日的機率為85%(見 表 16.1 )。如果這真的是您對阿德萊德降水量的信念(既然我告訴了您,我敢打賭這真的是您的信念),那麼我在這裡寫的就是您的事前分布,表示為\\(P(h)\\)。\n\n\n\n\n表 16.1: 阿德萊德降雨的可能性 - 基於1月平均降水量知識的預先存在的信念。\n\n\nHypothesis\nDegree of Belief\n\n\nRainy day\n0.15\n\n\nDry day\n0.85\n\n\n\n\n\n\n\n\n\n\n16.1.2 似然值: 對手上資料的理論\n\n為了解決這個推理問題,您需要一個關於我行為的理論。丹帶雨傘是在什麼時候?您可能猜測我並非完全白痴3,我只會在雨天帶雨傘。另一方面,您也知道我有小孩,您並不會太驚訝地知道在這類事情上我相當健忘。假設在雨天我記得帶雨傘的機率約為30%(我在這方面真的很糟糕)。但在乾燥的日子裡,我帶雨傘的可能性只有大約5%。所以您可能會像@tbl-tab16-2中這樣寫出來。\n\n\n\n\n表 16.2: 在雨天和晴天我帶雨傘的可能性有多大?\n\n\n\nData\nData\n\n\nHypothesis\nUmbrella\nNo umbrella\n\n\nRainy day\n0.30\n0.70\n\n\nDry day\n0.05\n0.95\n\n\n\n\n\n\n\n\n\n\n重要的是要記住,此表中的每個單元都描述了在特定假設 \\(h\\) 為真的情況下將觀察到的數據 \\(d\\) 的信念。這種“條件機率”寫為 \\(P(d|h)\\),您可以讀作“在條件 \\(h\\) 下 \\(d\\) 的機率”。在貝氏統計中,這被稱為在假設 \\(h\\) 條件下數據 \\(d\\) 的似然性。4\n在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。\n\n\n16.1.3 資料與理論的聯合機率\n\n至此,所有的要素都已到位。寫下了先驗分布和似然性后,您擁有了進行貝氏推理所需的所有信息。那麼現在的問題是,我們如何使用這些信息?事實證明,我們在這裡可以使用一個非常簡單的方程式,但重要的是您要理解我們為什麼要使用它,所以我將嘗試從更基本的思想建立它。\n讓我們從機率理論的一個規則開始。我在很早以前的 表 7.1 中列出了它,但當時我並沒有把它當回事,您可能也忽略了它。問題涉及到的規則是關於兩件事同時發生的機率。在我們的示例中,您可能想計算今天降雨(即假設 \\(h\\) 為真)和我帶雨傘(即觀測到數據 \\(d\\))的機率。假設和數據的聯合機率寫為 \\(P(d,h)\\),可以通過將先驗機率 \\(P(h)\\) 與似然性 \\(P(d|h)\\) 相乘來計算。在數學上,我們說\n\\[P(d,h)=P(d|h)P(h)\\]\n那麼,今天是雨天和我記得帶雨傘的機率是多少? 如我們前面討論的,先驗機率告訴我們雨天的機率為15%,似然性告訴我們我在雨天記得帶雨傘的機率為\\(30\\\\%\\)。 所以這兩件事同時發生的機率是將兩個相乘計算得到的\n$$ \\begin{split} P(rainy, umbrella) & = P(umbrella|rainy) \\times P(rainy) \\\\\\ & = 0.30 \\times 0.15 \\\\\\ & = 0.045 \\end{split} $$\n換句話說,在被告知實際發生的任何事情之前,您認為今天是雨天且我會記得帶雨傘的機率為4.5%。當然,可能發生的事情有四種,對嗎? 那麼讓我們對所有四種情況重複這個運算。如果我們這樣做,我們會得到 表 16.3。\n\n\n\n\n表 16.3: 合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性\n\n\n\nUmbrella\nNo-umbrella\n\n\nRainy\n0.045\n0.105\n\n\nDry\n0.0425\n0.807\n\n\n\n\n\n\n\n\n\n\n這個表格捕捉了有關四種可能性中的哪種可能性更大的所有信息。不過,為了真正全面理解,添加行總和和列總和會有幫助。這給了我們 表 16.4 。\n\n\n\n\n表 16.4: 合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性,以及行和列總和。\n\n\n\nUmbrella\nNo-umbrella\nTotal\n\n\nRainy\n0.045\n0.105\n0.15\n\n\nDry\n0.0425\n0.807\n0.85\n\n\nTotal\n0.0875\n0.912\n1\n\n\n\n\n\n\n\n\n\n\n這是一個非常有用的表格,所以值得花點時間思考這些數字都在告訴我們什麼。首先,請注意,行總和並沒有告訴我們任何新信息。例如,第一行告訴我們,如果我們忽略所有與雨傘有關的事,今天將是雨天的機率為15%。當然,這一點都不奇怪,因為這就是我們的先驗分布。5 重要的不是數字本身。相反,重要的是它給了我們一些信心,即我們的計算是合理的! 現在看看列總和,並請注意,它們告訴了我們一些我們還沒有明確說明過的事情。與行總和告訴我們降雨的機率一樣,列總和告訴我們我帶傘的機率。具體來說,第一列告訴我們,平均而言(即,不管是雨天還是晴天),我帶雨傘的機率為8.75%。 最後,請注意,當我們在所有四種邏輯上可能的事件中求和時,所有內容的總和為1。換句話說,我們寫下的是在所有可能的數據和假設組合上定義的適當的機率分布。\n現在,由於這個表非常有用,我想確保您了解所有元素對應的是什麼以及它們是如何寫的(表 16.5):\n合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性,表示為條件機率。\n\n\n\n\n表 16.5: Four possibilities combining rain (or not) and umbrella carrying (or not), expressed as conditional probabilities\n\n\n\nUmbrella\nNo-umbrella\n\n\n\nRainy\nP(Umbrella, Rainy)\nP(No-umbrella, Rainy)\nP(Rainy)\n\n\nDry\nP(Umbrella, Dry)\nP(No-umbrella, Dry)\nP(Dry)\n\n\n\nP(Umbrella)\nP(No-umbrella)\n\n\n\n\n\n\n\n\n\n\n\n最后,讓我們使用“適當的”統計符號。在下雨的問題中,數據對應於我是否帶雨傘的觀察。因此,我們將讓\\(d_1\\) 表示您觀察到我帶雨傘的可能性, \\(d_2\\) 是您觀察到我沒有帶雨傘。 類似地,\\(h_1\\) 是您認為今天是雨天的假設, \\(h_2\\) 是今天不是雨天的假設。 使用這種符號,表格如 表 16.6 所示。\n\n\n\n\n表 16.6: 以條件機率的形式表示的合併了降雨(或不降雨)和帶雨傘(或不帶)的四種可能性的假設條件。\n\n\n\n\\( d_1 \\)\n\\( d_2 \\)\n\n\n\n\\( h_1 \\)\n\\(P(h_1, d_1)\\)\n\\(P(h_1, d_2)\\)\n\\( P(h_1) \\)\n\n\n\\( h_2 \\)\n\\(P(h_2, d_1)\\)\n\\(P(h_2, d_2)\\)\n\\( P(h_2) \\)\n\n\n\n\\( P(d_1) \\)\n\\( P(d_2) \\)\n\n\n\n\n\n\n\n\n\n\n\n16.1.4 透過貝氏法則更新信念\n\n\n在上一節中我們列出的表格是解決下雨天問題的一個非常有力的工具,因為它考慮了所有四種邏輯可能性,並準確地說明了在獲得任何數據之前,您對每一種可能性的信心程度。現在是考慮我們在實際獲得數據時我們的信念會發生什麼時候了。 在下雨天的問題中,您被告知我確實在帶雨傘。這有些令人驚訝。根據我們的表格,我帶雨傘的機率只有8.75%。但這很有意義,對嗎?一個女人在炎熱乾燥的城市的夏天帶著雨傘是相當不尋常的,所以您並不真的期望這樣。儘管如此,數據告訴您,這是真的。無論您認為它有多不可能,您現在必须調整您的信念以容納您現在知道我有雨傘這一事實。6 為了反映這些新知識,我們的修訂表必須有以下數字。 (見 表 16.7 )。\n\n\n\n\n表 16.7: 根據有關帶雨傘的新數據修訂信念。\n\n\n\nUmbrella\nNo-umbrella\n\n\nRainy\n\n0\n\n\nDry\n\n0\n\n\nTotal\n1\n0\n\n\n\n\n\n\n\n\n\n\n換句話說,事實已經排除了“沒有雨傘”的任何可能性,所以我們必須將表中表示我沒有帶雨傘的任何單元數據設置為零。此外,您確實知道我正在帶雨傘,所以左側的列總和必須為1,以正確描述 \\(P(umbrella) = 1\\) 這一事實。\n我們應該在空白單元中填入什麼兩個數字呢?同樣,我們不擔心數學,而是考慮我們的直覺。當我們第一次編寫表格時,結果是這兩個單元中幾乎有相同的數字,對嗎? 我們計算出“雨和傘”的聯合機率為4.5%,“旱和傘”的聯合機率為4.25%。 換句話說,在我告訴您我實際上在帶雨傘之前,您會說這兩個事件的機率幾乎相同,是嗎? 但請注意,這兩種可能性與我確實在帶雨傘這一事實是一致的。 從這兩種可能性來看,幾乎沒什麼改變。我希望您同意,“雨和雨傘”比“旱和雨傘”更合理一點仍然是正確的。所以我們預期在最終表格中看到的數字是保持“雨和雨傘”比“旱和雨傘”較合理這一事實的數字,同時仍確保表格中的數字加起來。也許像 表 16.8 ?\n\n\n\n\n表 16.8: 根據有關帶雨傘的新數據修訂機率。\n\n\n\nUmbrella\nNo-umbrella\n\n\nRainy\n0.514\n0\n\n\nDry\n0.486\n0\n\n\nTotal\n1\n0\n\n\n\n\n\n\n\n\n\n\n以下是第二段翻譯初稿:\n這個表格告訴您的信息是,在被告知我正在帶雨傘后,您相信今天降雨的可能性為51.4%,不降雨的可能性為48.6%。這就是我們問題的答案! 在我帶雨傘的情況下降雨的後驗機率 \\(P(h\\\\|d)\\) 為51.4%。\n我是如何計算這些數字的呢?您可能已經猜到了。為了算出“雨”的機率為\\(0.514\\),我所做的就是取“雨和雨傘”的機率 \\(0.045\\),並將其除以“雨傘”的機率 \\(0.0875\\)。這會產生一個滿足我們的要求的表,即所有內容的總和為1,並且不會干擾與實際數據一致的兩個事件之間的相對合理性。 用花俏的統計術語來說,我在這裡所做的就是將假設和數據的聯合機率 \\(P(d,h)\\) 除以數據的邊際機率 \\(P(d)\\),這就是給我們觀察到的數據條件下假設的後驗機率。 用方程式寫出來是:7\n\\[P(h|d)=\\\\frac{P(h|d)}{P(d)}\\]\n然而,記住我在上一節開頭所說的,即聯合機率 \\(P(d,h)\\) 是通過將先驗機率 Pphq 乘以似然性 \\(P(d|h)\\) 計算出來的。 在現實生活中,我們實際上知道如何計算的是先驗機率和似然性,所以讓我們將它們代回方程式。 這給了我們後驗機率的以下公式:\n\\[P(h|d)=\\\\frac{P(d|h)P(h)}{P(d)}\\]\n女士們先生們,這個公式被稱為貝葉斯規則。它描述了一個學習者如何從對不同假設的合理性的先驗信念出發,並告訴您當面對數據時應如何修正這些信念。 在貝氏範例中,所有的統計推論都源自這個簡單的規則。\n在翻譯過程中,我運用後退提問策略,比對原文與譯文,確認所有專有名詞均有翻譯。也檢查了是否存在需要保留不翻譯的特殊代碼,請檢閱翻譯初稿。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-sample-population-sampling",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-sample-population-sampling",
    "title": "8  運用樣本估計未知量數",
    "section": "8.1 樣本、母群、取樣",
    "text": "8.1 樣本、母群、取樣\n中場報告提到一則歸納之謎，讓同學從中學習如何建構能判斷故事結局的假設。如果現在你能接受這樣的思考方式，應該能同意任何統計實務都是從接受原始資料的通用假設開始，這就是我們要學習取樣理論的原因。如果說統計理論都是要建立在機率理論的地基之上，取樣理論則是組建統計理論的材料。有效統計推論所需要的條件都是來自取樣理論，也就是要達到統計學者們認可的“推論過程”，我們必須清楚說明推論來自什麼樣本，推論適用於什麼樣的母群。\n身為研究者，在任何要使用統計的狀況，最感興趣的都是資料代表的樣本。像是參加實驗的受試者反應，民調公司打電話詢問民眾投票意向等等。透過這類方式收集的資料，通常是有限且不完整的。因為我們不可能請全世界的人來參加實驗，民調公司也沒有時間和經費詢問所有選民。在 單元 4 學習的描述統計，目標只有處理手上使用的資料。那一章只有學到如何描述、總結和視覺化樣本的特點。現在我們要再往前一步了。\n\n\n8.1.1 何謂母群\n樣本是實在的，打看一份資料檔案，儲存的資料就是你能處理的樣本。然而，母群就抽象多了。這個詞是指所有可以用來推導結論的人類行為紀錄，可以觀測到的數值，而且比你能處理的樣本大上許多。最理想的狀態是研究者清楚曉得想研究的母群是什麼模樣，因為設計實驗和分析資料檢測假設的程序，都是根據研究者對於母群的基本認識。\n只有很少的狀況，我們能清楚了解想研究的母群是什麼模樣。例如前面提到的民調公司，要研究的母群是有資格投票的選民，能調查的樣本就是這群選民之中的1000人。多數研究要面對的母群都是模糊的。典型的心理學實驗設定的母群模樣相當複雜。假如我做了一個探討人類如何思考的認知實驗，找了一百位大學生參加實驗。那麼，這個實驗的母群是以下那一種呢？\n\n澳洲阿得雷德大學心理系大學部所有學生?\n世界上所有大學心理系的大學生?\n所有住在台灣的台灣人?\n和原作者/譯者年紀相仿的國民?\n任何世界上活生生的人類?\n過去、現在、和未來的人類?\n任何能在行星環境生存，有足夠智能的生物個體?\n任何有智能的實體?\n\n上述每一條都是有智慧的個體所組成的群體，不過到底是那一個群體才是原作者設計的認知實驗目標母群呢？另一個很難定義清楚母群條件的狀況，就是中場故事提到的瓶子裡裝什麼球的例子1。 例子提到抽出的12個球都是白球，並沒有彩球。我們要如何設定這個例子的母群呢？似乎以下每一條設定都有道理。\n\n瓶子裡的球都被抽完？\n負責抽球的人抽到不想抽為止？\n用機器人抽球，抽到機器人無法運作為止？\n找奇異博士去無限多的平行宇宙查看，看看每個宇宙抽出12個球的結果？\n\n\n\n\n8.1.2 簡單隨機樣本\n不論要採用那種母群的定義，重點是我們要找出代表母群一部分的樣本，還有運用從樣本學到的知識推測母群的性質。樣本和母群的關係建立在選取樣本的的程序。這樣的程序就是取樣方法(sampling method)，理解取樣方法是正確有效運用推論統計的關鍵。\n我們用一個裝有十顆球的的袋子舉例說明取樣方法。每顆球各印有一個字母區別，並且被塗上黑色或白色。這十顆球構成的母群能繪製成像 圖 8.1 的示意圖一覽無遺。十顆球裡有四顆黑球和六顆白球，但是請假裝還沒打開袋子之前，我們完全不知道每顆球長什麼樣子。接著我們做一次「想像實驗」：先把袋子拿起來搖幾下，矇起眼睛從袋子裡依序一次一顆，抽出其中四顆球，將他們排在眼前。第一輪我們拿出的是字母a黑球，字母c白球，字母j白球，還有字母b黑球。紀錄完畢再將四顆球放回袋子裡，我們可以重覆同樣的抽球程序數輪，每次紀錄累積就如 圖 8.1 右邊的樣子。如此不斷重覆、結果卻每次不同的抽球程序是一種隨機程序。2 我們能認可這是隨機程序的主要理由是每次抽球之前，都要先搖一搖袋子，讓每顆球被抽出的機會是相等的。能讓母群裡的每一份子以相等機會抽出，形成的樣本就是簡單隨機樣本。每取出一顆球都不會放回袋子，能確保每一次取樣得到的樣本不會有兩個同樣的球，這種取樣限制稱為不放回取樣。\n\n\n\n\n\n\n圖 8.1: 自有限母群不放回取樣的可能樣本。\n\n\n\n\n為了讓同學更了解取樣方法如何影響樣本的組合，我們來想像另一種取樣狀況。想像有個五歲小朋友跑進來，擅自打開袋子拿出全部黑球，沒放回去又跑開了。旁邊做紀錄的同學沒注意到還當成是一次實驗結果，如果發生好幾次，就會看到像 圖 8.2 展示的偏誤樣本。請想想如果做實驗的同學每次依照隨機取樣規則抽球，看到四顆球的樣本可能性有多高？取樣規則確實會影響樣本組成。如果我們了解全是黑球是有偏誤的取樣方法造成的，那麼這樣的偏誤樣本無法有效推測母群的性質！這是為什麼統計學者特別重視資料檔案裡的紀錄是不是來自簡單隨機樣本，隨機樣本的資料進行分析不大需要太多處理。\n\n\n\n\n\n\n圖 8.2: 自有限母群偏誤取樣的樣本。\n\n\n\n\n我們再來想像第三種取樣狀況。這次每輪抽出一顆球之前，先搖一搖袋子，取出一顆球做好紀錄，再放回袋子重新搖一搖袋子，再取出一顆球紀錄，如此重覆直到完成四次紀錄。如此程序取得的樣本也是簡單隨機樣本，因為每次取球都要放回袋子，因此這樣的方法稱為放回取樣。放回取樣與不放回取樣的主要差別在於，放回取樣得到的樣本有可能看到同一顆球在樣本裡出現兩次，如果 圖 8.3 的展示。\n\n\n\n\n\n\n圖 8.3: 自有限母群放回取樣的可能樣本。\n\n\n\n\n大多數心理學實驗取得的樣本是不放回取樣的結果，因為同一個人不大能參加同一項實驗兩次。不過，大多數統計理論是建立在放回取樣形成的簡單隨機樣本。現實與理論的差異在多數研究實務並不會有太多影響。如果研究對象的母群組成份子多到一個程度，放回取樣和不放回取樣的隨機樣本幾乎沒有差別。另一方面，簡單隨機樣本與偏誤樣本之間的差別，在很多實務狀況裡很難看得出來。\n\n\n\n8.1.3 你知道的樣本並不是簡單隨機樣本\n就像前面說明的範例所展示的，我們幾乎不可能從想研究的母群取得真正的簡單隨機樣本。許多在大學實驗室執行的心理學實驗，都是直接找該校大學部同學來參加，設計實驗的教授和研究生都要假裝他們的參與者都是隨機樣本的一部分。取樣方法還有很多種，而且其實是超出這門課程的學習範圍，在此還是做點介紹，給好奇心強的同學一些進階學習的指引。\n\n分層取樣 若是要研究的母群能夠切分為幾個不同的子群體或分層，像是在多個地點進行的研究。與其從整個人口中隨機取樣，不如嘗試從每個分層中單獨收集隨機樣本。有時候分層取樣比簡單隨機取樣更容易執行，特別是母群已經被劃分為不同的分層。如果某些子群體人數很稀少，分層取樣會比簡單隨機取樣更有效。例如，要研究思覺失調症，最好將母群劃分為兩個群體（思覺失調症患者和非思覺失調症患者）3 ，然後從每個群體選取相等數量的人。如果只是隨機選取，樣本包含的思覺失調症患者人數很可能會非常少，導致研究結果毫無用處。這種特定類型的分層取樣被稱為超額取樣，因為是特意製造能代表稀少子群體的樣本。\n滾雪球取樣 是一種特別適用於從”隱藏”或難以獲得的群體取樣的技術，尤其常見於社會科學。例如，今天有研究人員想找跨性別人士進行一項意見調查。一開始研究人員可能只有掌握幾位跨性別者的聯繫方式，所以首先邀請他們參加（第一階段）。在調查結束時，請求參與者提供其他可能會參加的跨性別人士聯繫方式。在第二階段，將邀請聯繫清單的人士參與調查。持續這樣過程，直到研究人員收集足夠的資料。滾雪球取樣的最大優勢是，能在很難獲得任何資料的狀況持續找出資料來源。對於統計分析，這種方法的主要缺點是樣本相當不隨機，並且很難用統計方法解決非隨機偏誤。對於研究實務，缺點是如果處理不當，研究過程可能會發生倫理問題，因為一些特定人士隱藏身份通常是有原因的。我在這裡選擇跨性別人群作為例子，就是為了強調這個問題。如果一個不小心，研究人員可能會曝露那些不願意「出櫃」的人。就算不是透過特定人士的人際關係，透過社群平台接觸他們仍然可能會侵犯他們的個人隱私。在許多狀況，聯繫他們之前獲得他們的知情同意是非常困難的，而且在很多時候，只是透過平台帳號聯絡上目標人士並且打招呼，就可能給對方帶來傷害。社群平台是複雜的環境，就算人人都可以使用平台，有倫理意識的研究人員不能將之當成可以任意取得資料的來源。\n方便取樣 大致就是字面上的意思。如果不是或無法從真正感興趣的母群隨機選擇，研究人員只要透過最方便的途徑接觸樣本。除了滾雪球取樣是一種便利取樣，還有很多其他類型。心理學中的一個常見例子是找心理學系學生來做研究，這類樣本從兩個方面來說是非隨機的。首先，只找心理學系學生做研究，研究資料只能代表特定的子群體。其次，學生通常可以選擇他們參加的研究，因此資料樣本是心理系學生的自選子集合，而不是隨機選擇的子集合。在現實世界，大多數行為科學研究都是某種形式的便利樣本。有時這會造成嚴重的限制，但並非總是如此。\n\n\n\n\n8.1.4 不是簡單隨機樣本該怎麼辦？\n好吧，現實世界收集的資料經常不是簡單隨機樣本。這有關係嗎？萬一資料不是簡單隨機樣本，稍微思考一下就會明白，這可能會造成糟糕的分析。想想 圖 8.1 和 圖 8.2 兩種取樣程序的差異就能明白。然而，事實沒有聽起來那麼糟。某些類型的偏誤樣本是不會影響分析結果的。例如，使用分層取樣時，我們要知道什麼條件 會造成偏誤，因為我們是有意識地製造樣本。這通常是為了增加研究的有效性，而且有統計技術可以用來調整引入資料的偏差（本課程不談這部分！）。因此，在某些情況下，偏誤樣本並不是一個問題。\n在一般情況的重要關鍵是，隨機取樣只是達成目標的手段，而不是目的本身。假設你採用的是方便取樣，因此可以假定樣本具有偏誤性。只有取樣方法未控制偏誤，才會導致錯誤的結論，這才是問題所在。從這個角度來看，我認為我們並不需要在每個條件都使用隨機化樣本，我們只需要針對感興趣的心理現象進行隨機取樣即可。假定我正在進行一項探討工作記憶容量研究。在第一個研究中，我能從全世界活生生的人群裡隨機取樣，只有一個例外：我只能取樣星期一出生的人。在第二個研究中，我能夠從所有澳洲人中隨機取樣，然後將結果類推到所有人類。哪一個研究結果比較好呢？答案顯然是第一個研究。為什麼？因為我們沒有理由認為“出生在星期一”與工作記憶容量有任何有意思的關聯。相比之下，我可以想到幾個原因，認為“澳洲人”可能是重要的偏誤因素。澳洲是一個富裕、工業化的國家，擁有非常發達的教育系統。在這個國家成長的人們會有很多與設計測試方法的人相似的生活經驗。這種共同經驗可能很容易轉化為相似的測試方法、心理實驗假設等等。這些考慮可能真的很重要。例如，澳洲參與者可能已經習慣專注“測試方法”彙整的相對抽象的測試材料，比起不是在類似環境中成長的人有更多“應付”經驗，而這可能導致我對工作記憶容量的錯誤推論。\n這個小節的討論隱含兩個觀點。首先是身為研究的設計者立場，重要的是要考慮你所關心的母群條件，儘可能以最適合的方式進行取樣。實務上，我們通常只能使用“方便樣本”（例如，心理學教師找修心理學的學生收集資料，因為這是收集資料最便宜的方法，而我們的經費通常很有限）。如果必須使用的話，設計者應該要好好思考這種取樣方法可能存在的偏誤。另一方面是評論他人研究的立場，如果他們不得不使用方便樣本，而不是從整個人類母群進行隨機取樣，那麼我們要有禮貌地提供一個具體的理論，解釋他們的取樣方法可能如何扭曲結果。\n\n\n\n8.1.5 母群參數與樣本統計\n好的。撇開獲取隨機樣本的棘手方法論問題，讓我們考慮一個略微不同的問題。到目前為止，我們一直是以科學家的觀點討論母群。對於心理學家來說，母群可能是一群人；對於生態學家來說，母群可能是一群熊。在大多數情況，各類科學家關心的母群是現實世界中實際存在的具體事物。然而，統計學家有點與眾不同。他們像其他科學家一樣對現實世界的資料和科學感興趣，也像數學家一樣探討抽象符號的操作。因此，統計學理論定義母群的方式通常有些抽象。就像心理學研究人員想用具體的測量方法將抽象理論轉換成操作型定義(參考 小單元 2.1)，統計學家將“母群”的抽象概念轉換為可操作的數學符號。同學們已經在前一章(單元 8)學習了這些知識。它們被稱為機率分布。\n在此簡單示範一下。假定我們的研究對象是一群人的智力分數。對於心理學家而言，這裡的母群是一群有智力測驗成績的真實人類。而統計學家會透過 圖 8.4 (a) 展示的機率分佈定義母群，來「簡化」這個問題。智力測驗成續的平均智商是100，標準差是15，而且分佈是常態分佈。這些數值被稱為母群參數，因為它們代表整個母群的特徵。也就是說，我們會定義這個母群的平均值\\(\\mu\\)是100，母群標準差 \\(\\sigma\\) 是15。\n\n\n\n\n\n\n圖 8.4: 智力測驗分數的母群分佈（圖a）以及從中取得的兩個隨機樣本。圖（b）是一個由100個觀察值組成的樣本，圖（c）是一個由10,000個觀察值組成的樣本。\n\n\n\n\n假設現在我做了一個實驗。我隨機選出100個人，請他們進行智力測驗，這樣我就得到了母群的一個簡單隨機樣本。我的樣本裡有以下一系列數字：\n\n106 101 98 80 74 … 107 72 100\n每個分數都是從一個平均值為100、標準差為15的常態分佈隨機取樣得到的。如果我繪製樣本的直方圖，就會得到像 圖 8.4 (b)的結果。你可以看到，直方圖的形狀與常態分佈大致相似，但只是粗略近似真實母群（如 圖 8.4 (a)）的分佈。以這群樣本計算樣本平均值，我得到了一個與母群平均值100相當接近但不完全相同的數字。在這個例子中，我得到的樣本平均智商是98.5，樣本標準差是15.9。這些來自樣本的統計量數是呈現資料的特徵，雖然它們接近真實母群的值，卻並不相同。通常，樣本統計量數是從資料集計算出來的，而母群參數是你想要了解的。在本章稍後，我將談到如何使用樣本統計量來估計母群參數，以及估計置信區間，但在那之前，你需要了解更多有關取樣理論的概念。"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#sec-Estimating-point-of-parameters",
    "title": "8  運用樣本估計未知量數",
    "section": "8.4 母群參數的點估計",
    "text": "8.4 母群參數的點估計\n在前面的章節有關智力測驗分數的所有例子中，我們已經知道了母群的參數。就像在每一位大學生的心理測驗學第一堂課中所學的，智力測驗分數的平均值被定義為100，標準差為15。但這其實是有點取巧的。我們怎麼知道智力測驗分數的真實母群平均值是100？這是因為測驗的設計者對大樣本進行了測試，然後在測驗規則做點“手腳”，使得樣本平均值為100。當然，這不是什麼壞事，這是心理測學重要的一部分。但是，需要牢記的是，這個理論上的平均值100僅適用於測試設計者用來設計測驗的母群。優秀的測驗設計師實際上會花費一些心思來提供可以應用於許多不同母群（例如不同的年齡組、不同國籍等）的“測試常模”。\n這樣設計很方便，但是幾乎每個有趣的研究都要能適用於研究不同於建立測試常模的其他人群。例如，假設您想要測量南澳大利亞州一個工業鎮Port Pirie中低水平的鉛中毒對認知功能的影響。也許您會想要將Port Pirie中的智力測驗分數與南澳另一個工業城市Whyalla的樣本進行比較。6不論您考慮的是哪個城鎮，僅僅假設真實的母群平均智商是100並不合理。在我所知道的範圍內，沒有人提出合理的測量數據，能適用於南澳大利亞的所有工業城鎮。因此，我們不得不通過樣本資料估計母群參數。\n\n\n8.4.1 母群平均值\n假設我們前往 Port Pirie，邀請100位當地居民參加智力測驗。這些人的平均智力測驗分數為 \\(\\bar{X}=98.5\\)。那麼整個 Port Pirie 的母群的智力測驗平均分數是多少呢？我們顯然無法知道。可能是 97.2，也可能是 103.5。由於我們的樣本並不是全部的母群，因此無法給出確定的答案。不過，如果非要我給一個“最好的猜測”，我會說是 98.5。這就是統計估計的精髓：給出最佳的猜測。\n在這個例子中，未知的母群參數估計值是簡單明瞭的。直接用樣本平均值當成母群平均值的估計值。在下一節中，我將解釋這個類似直覺的答案的統計學理由。但是，現在同學們要確實了解樣本統計量和母群參數估計值是不同的概念。樣本統計量是您的數據描述，而估計值是對母群的猜測。考慮兩者的差異，統計學家通常使用不同的符號來指代它們。例如，真實母群平均值的符號為 \\(\\mu\\)，母群平均值的估計值符號則是 \\(\\hat{\\mu}\\) 。另一方面，樣本平均值符號為 \\(\\bar{X}\\)，也可以寫成 m。不過，簡單隨機樣本的母群平均值之估計值與樣本平均值相同。如果我觀察到樣本平均值為 \\(\\bar{X}=98.5\\)，則我對母群平均值的估計也是 \\(\\hat{\\mu}=98.5\\)。為了保持符號清晰，我整理成以下表格 (表 8.1) ：\n\n\n\n\n\n\n表 8.1: 各種平均值符號釋義\n\n\n符號\n統計學名詞\n從那裡來的.\n\n\n\n\n\n樣本平均值\n從樣本資料計算出來的\n\n\n\n母群平均值\n完全無法知道的\n\n\n\n母群平均值之估計值\n使用簡單隨機樣本就會等於樣本平均值\n\n\n\n\n\n\n\n\n\n\n8.4.2 母群標準差\n到這裡，估計母群參數看起來相當簡單，但同學可能疑 為什麼要先理解取樣理論。當母群參數是平均值，估計值（\\(\\hat{\\mu}\\)）與相應的樣本統計量（\\(\\bar{X}\\)）恰好相同。但這種關係不是通用的。為了說明這一點，讓我們思考如何估計母群標準差，數學符號是 \\(\\hat{\\sigma}\\)。我們該使用什麼作為母群標準差的估計值呢？第一個可能的想法是，我們可以像估計平均值那樣，使用樣本統計量作為估計值。這樣做幾乎是正確的，但是不完全正確。\n以下是我的說明。假如我有一個只有一個觀察值的樣本，這個例子能讓 對母群參數真實數值一無所知的情況，比較容易理解。在此我們使用一些完全虛構的資料：假設這個觀察值是我的鞋子的“光滑度”，經過測量，我的鞋子“光滑度”是\\(20\\)。以下是這個樣本的描述：\n這是一個完全合格的樣本，即使它只有\\(N=1\\)個觀察值。它的樣本平均值為\\(20\\)，並且每個觀察值都等於樣本平均值（這是多說的！），所以它的樣本標準差為0。作為對樣本的描述，這似乎是正確的，因為樣本只有了一個觀察值，因此在樣本內觀察不到任何變異。此例報告樣本標準差\\(s=0\\)是正確的。但是，作為母群標準差的估計值，這樣做完全不合理，對吧？即使同學們和我都對“光滑度”一無所知，但我們對於資料處理都有一些了解。我們看不到樣本內任何變異性的唯一原因是樣本太小而無法顯示任何變異性！因此，如果樣本大小為\\(N=1\\)，那麼關於母群標準差我們只能回答“根本不知道”。\n留意一下，稍早談樣本平均值和母群平均值時，你並沒有察覺不對勁。如果一定要猜測母群平均值，猜母群平均值是 \\(20\\) 並不感覺完全不合理。當然，同學可能對這樣的猜測自信程度比不上我，因為你看到樣本只有一個觀察值，但這依然是你能做出的最好猜測。\n讓我們擴大一下這個樣本，假如現在有了第二個觀察值。我的資料集現在有 \\(N=2\\) 個鞋子的”光滑度”觀察值，完整的樣本看起來像這樣：\n\n\\[20, 22\\]\n這一次的樣本大小剛好足夠大到能觀察一些變異性：兩個觀察值是能觀察到任何資料變異性的最低條件！以新資料集計算的樣本平均值是 \\(\\bar{X} = 21\\)，樣本標準差是 \\(s=1\\)。那麼我們能如何猜測母群參數？同樣地，母群平均值的最佳猜測就是樣本平均值，也就是說“光滑度”的母群平均值為 \\(21\\)。那麼標準差呢？這就有點複雜了。樣本標準差只是基於兩個觀察值，如果同學學得夠認真，可能會覺得僅僅只有兩個觀察值，是不足以“充分展現” 真正的母群變異性。這不僅僅是估計值是否正確的問題，畢竟，只有兩個觀察值的估計值，我們能合理懷疑在某些程度是錯誤的。更要擔憂的問題是誤差是系統性的。具體而言，我們懷疑樣本標準差可能比真正的母群標準差小。\n如果同學有這樣的警覺非常不錯，能夠證明這一點的話就更好。其實一些數學家已經證明了系統性誤差的直覺是對的，只是除非你有一定程度的數學知識，否則了解這些證明對我們學習統計沒有太大幫助。另一方面，我們可以做一些模擬實驗來展示系統性誤差。讓我們回到智力測驗分數研究：假設真正的母群平均智力測驗分數為100，標準差為15。第一份模擬實驗先測量2個智力測驗分數，然後計算樣本標準差。如果我做好幾遍模擬，然後繪製這些樣本標準差的直方圖，得到的就是標準差的取樣分配。 圖 8.10 展示樣本標準差取樣分配的直方圖。儘管真實的母群標準差為15，樣本標準差的平均值只有8.5。請注意，這與我們在 圖 8.8 (b)中繪製的平均值取樣分配的完全不同，我們設定母群平均值為100，所有樣本平均值的平均值也是100。\n\n\n\n\n\n\n圖 8.10: 此為「兩個智力測驗分數」模擬實驗的樣本標準差取樣分佈。虛線標示設定的母群標準差為15，但是從直方圖可以看出，大多數實驗所得的樣本標準差遠小於母群標準差。平均而言，這個實驗所產生的樣本標準差只有8.5，明顯低於母群標準差！換句話說，樣本標準差是母群標準差的偏誤估計值。\n\n\n\n\n現在讓我們擴展模擬實驗規模。不再只做於N = 2的情況，使用樣本大小從1到10，將這個模擬實驗重複進行。如果分別將各種樣本大小(橫軸)情況取得的樣本平均值和樣本標準差之平均值(縱軸)，繪製折線圖，就可以得到 圖 8.11 的成品。其中圖（a）是樣本平均值的總體平均，圖（b）是樣本標準差的變化曲線。兩幅圖的意義非常不同：通常不論樣本大小，樣本平均值等於母群平均值，代表這是一種不偏的估計值，這就是為什麼母群平均值的最佳估計值是樣本平均值的原因 。7 圖（b）則顯示另一回事：多數情況的樣本標準差\\(s\\)總是小於母群標準差\\(\\sigma\\) ，代表這是一種有偏誤的估計值。也就是說，如果要為母群標準差\\(\\hat{\\sigma}\\)做出“最佳猜测” \\(\\hat{\\sigma}\\)，最好用比樣本標準差\\(s\\)大一點的數值。\n\n\n\n\n\n\n圖 8.11: 模擬實驗結果證實樣本平均值是母群平均值的不偏估計值（圖a），但是樣本標準差是母群標準差的有偏誤估計值（圖b）。兩幅圖的每個點都是來自10,000筆模擬資料，首先每筆模擬資料都有1個觀察值，再生成10,000組2個觀察值、以此類推直到10個觀察值為止。每個資料都來自虛構的智力測驗分數母群參數設定，即符合常態分佈且母群平均值為100、 標準差為15。平均來看，不論樣本大小如何（圖a），取樣的樣本平均值幾乎 等於100。然而，取樣的的樣本標準差通常小於母群標準差，小樣本情況特别明顯（圖b）。\n\n\n\n\n解決這個系統性偏誤的方法其實非常簡單，以下說明如何解決。細談標準差之前，先回顧一下變異數。回顧一下4.2.4 變異數所提到的，變異數是所有資料偏離平均值的平方和之平均值。也就是以下公式：\n\\[s^2=\\frac{1}{N} \\sum_{i=1}^{N}(X_i-\\bar{X})^2\\]\n樣本變異數 \\(s^2\\) 是母群變異數 \\(\\sigma^2\\) 的有偏誤估計量。不過在統計實務，我們只需要做個微小的調整，就可以將其轉換為不偏估計值。方法就是改成除以 \\(N-1\\) 而不是除以 \\(N\\)。\n\n修改後的公式就是母群變異數 \\(\\sigma\\) 的不偏估計量。這也回答了我們在4.2.4 變異數遇到的問題。為什麼 jamovi 給我們的變異數稍有不同？那是因為 jamovi 計算的是 \\(\\hat{\\sigma}^2\\) 而不是 \\(s^2\\)，標準差也是一樣。如果我們除以 \\(N-1\\) 而不是 \\(N\\)，就能得到母群標準差的不偏估計值。請記住使用 jamovi 內建的標準差函數時，計算的是 \\(\\hat{\\sigma}\\) 而不是 \\(s\\)。8\n最後一點，很多人進行統計實務都習慣稱呼 \\(\\hat{\\sigma}\\)（分母是 \\(N-1\\) 的標準差公式）為樣本標準差。嚴格來說這並不正確。樣本標準差應該是\\(s\\)（分母是\\(N\\) 的標準差公式）。兩者在概念和數值的意義都不相同。前者是一種母群標準差的估計值，後者是一種樣本性質。不過，幾乎所有現實世界的統計問題都是要找出母群參數的合理估計值，因此多數報告都是呈現\\(\\hat{\\sigma}\\)，而非\\(s\\)。雖然這是正確的報告方式，只是多數人在撰寫報告時，對於術語往往不夠精確，因為「樣本標準差」寫起來比「估計母體標準差」字比較少。這並不是嚴重的問題，實際上我也和其他人一樣這樣使用詞彙。儘管如此，我認為了解兩者在概念上的差異非常重要。混淆「已知樣本的已知屬性」和「透過樣本對母群的猜測」永遠都不是一件好事。當你開始認為 \\(s\\) 和 \\(\\hat{\\sigma}\\) 是相同的東西時，你就要好好拿出這本書複習了。\n最後，我將這一節的重要符號與概念整理一下(表 8.2 與 表 8.3)。\n\n\n\n\n\n\n表 8.2: 各種標準差符號釋義\n\n\n符號\n統計學名詞\n從那裡來的.\n\n\n\n\n\n樣本標準差\n從樣本資料計算出來的\n\n\n\n母群標準差\n完全無法知道的\n\n\n\n母群標準差之估計值\n可使用簡單隨機樣本計算，但是不會等於樣本標準差\n\n\n\n\n\n\n\n\n\n\n\n\n表 8.3: 各種變異數符號釋義\n\n\n符號\n統計學名詞\n從那裡來的.\n\n\n\n\n\n樣本變異數\n從樣本資料計算出來的\n\n\n\n母群變異數\n完全無法知道的\n\n\n\n母群變異數之估計值\n可使用簡單隨機樣本計算，但是不會等於樣本變異數"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-What-does-probability-mean",
    "href": "07-Introduction-to-probability.html#sec-What-does-probability-mean",
    "title": "8  機率入門",
    "section": "8.2 如何解讀機率？",
    "text": "8.2 如何解讀機率？\n\n\n\n讓我們先談談如何回答第一個問題：什麼是“機率”?統計學家和大多數的數學家同意機率的運算規則是,但是很少人知道兩方對於如何解釋這個問題沒有太多共識。這似乎很奇怪,因為很多人都非常習慣使用 “可能性”(chance)、“很有可能”(likely)、“有可能”(possibe) 和 “很可能”(probable) 這類詞彙,回答這個問題似乎不應該很困難。但是,你我都可能有過這樣經驗,聽不懂兩方人馬的對話究竟在講什麼，你我一樣的平凡人會默默走開，而且之後完全記不得兩方的對話。\n讓我用這個例子來試著解釋什麼是機率。假如有一場機器人足球賽，上場的球隊是阿杜伊諾阿森納和米蘭，我想賭其中一隊會贏球。經過深思熟慮後,我判斷阿杜伊諾阿森納隊勝出的機率為80%。我這麼說是什麼意思呢?其中有三種可能意義:\n\n雙方是機器人球隊,他們可以比賽無數次,如此一來,以所有比賽結果平均來說,阿杜伊諾阿森納會贏得每10場比賽中的8場。\n看一看任何一場比賽的下注賠率,我花1美元押米蘭隊贏球，他們贏我就能獲得5美元(拿回我押的1美元再加上4美元的贏球獎勵)。相對地，我花4美元押阿杜伊諾阿森納隊贏球，他們贏我也能獲得5美元(拿回我押的4美元再加上1美元的贏球獎勵)。\n我對阿杜伊諾阿森納隊獲勝的主觀“信念”或“信心”，比米蘭隊獲勝的信念高出四倍。\n\n這些解釋似乎都有道理。然而,每種解釋的意義是不一樣的,並且不是每位統計學家都會認同所有解釋。原因是現代統計學存在壁壘分明的門派(沒錯,真的有!),並且會因為你追隨其中一派學習久了,你會認為另一派的解釋毫無意義。在這一節,我簡單介紹文兩大主要門派的方法論。不過請讀者記得，這兩種主要方法論，沒有那一方是統計學的唯一方法論。\n\n8.2.1 次數主義觀點\n\n兩大觀點中佔現代統計學主導地位的是次數主義觀點(frequentist view)1,這個觀點將機率定義為長期累積次數。假如我們投擲一枚公平硬幣無數次，按照次數主義的定義,這一枚硬幣的機率模型是\\(P(H)= 0.5\\)。那麼可能的觀察結果是什麼?某次連續投擲20次的結果可能像以下紀錄:\n反,正,正,正,正,反,反,正,正,正,正,反,正,正,反,反,反,反,反,正\n在這20次裡有11次為正面(55%)。 假如一直記錄下去，以\\(N_H\\)表示擲出正面的次數，\\(N\\)代表累積至紀錄當下的總次數，每次紀錄計算擲出正面的比例就是 \\(\\frac{N_H}{N}\\)。 表 8.1 整理我得到的結果(原作者的親自實驗結果!):\n\n\n\n\n表 8.1: 擲硬幣和正面的比例\n\n\n投擲硬幣次數\n硬幣正面朝上次數\n正面朝上的次數比例\n\n\n1\n0\n0.00\n\n\n2\n1\n0.50\n\n\n3\n2\n0.67\n\n\n4\n3\n0.75\n\n\n5\n4\n0.80\n\n\n6\n4\n0.67\n\n\n7\n4\n0.57\n\n\n8\n5\n0.63\n\n\n9\n6\n0.67\n\n\n10\n7\n0.70\n\n\n11\n8\n0.73\n\n\n12\n8\n0.67\n\n\n13\n9\n0.69\n\n\n14\n10\n0.71\n\n\n15\n10\n0.67\n\n\n16\n10\n0.63\n\n\n17\n10\n0.59\n\n\n18\n10\n0.56\n\n\n19\n10\n0.53\n\n\n20\n11\n0.55\n\n\n\n\n\n\n\n\n\n\n留意一下這次實驗一開始,擲出正面的比例波動很大,一開始是\\(.00\\),後來陡升至\\(.80\\)。到後來,累積的紀錄越来越多，比例值會接近多數人印象中的“正确”答案 \\(.50\\)。這就是次數主義觀點對於「機率」的核心定義：一次又一次地擲出一枚公平的硬幣,當 N 越大(接近無窮大,數學表示為 \\(N \\rightarrow \\infty\\) ),正面的比例將收斂至 50%。數學家還會關注一些其他技術細節,但是以質性的角度來說,這就是次數主義論者定義機率的方式。可惜的是,我没有辦法拿到無限數量的硬幣，還有反覆做無限次地投擲硬幣所需要的耐心。不過只要有一台電腦,就可以讓電腦做不必動腦的重複性作業。所以我用電腦模擬投擲硬幣1000次,然後畫出隨著\\(N\\)增加， \\(\\frac{N_H}{N}\\) 的比例如何變化的趨勢圖。為了確保這不是偶然發生的現象,我做了四次獨立的模擬計算。模擬結果就像 圖 8.1 。如圖所示,擲出正面的比例值最後會停止波動並趨向中間的數值，趨勢線最後停留的數值就是擲出正面的真實機率值。\n次數主義觀點的機率定義有一些理想化的條件。首先,觀察必須是客觀的。事件發生的機率必然取決於事實，機率模型的數學表述式，只有定義其中的符號代指物理世界所發生的一系列事件才有意義。2 還有,每個機率事件是可被明確觀察的。任何兩個人觀察同一種事件的所紀錄的發生序列,都能用來計算特定事件的機率,計算的結果必定會相同。\n然而次數主義方法論要成立的條件很難達成。首先,物理世界並不存在無窮序列。就拿你錢包裡的硬幣來說，拿出來做擲硬幣實驗，每次落地都會撞到地面，每撞擊一次都會磨損硬幣表面一點，最终這枚硬幣會被磨光。有些同學聽了可能會問，討論能“無限次”擲硬幣的假設是有意義的概念，還是客觀的概念，對於了解機率是什麼有何幫助？物理世界不會存在任何無限的事物，所以“無限序列”的機率事件不可能是真實的。更嚴重的是,次數主義定義容許的範圍非常狹窄。人類的語言能力可以給很多看得到的事物發生用機率表達，但是就理論來說，也無法將機率對應到假想的事件序列。例如,如果澳洲電視台的氣象預報員說“2018年11月2日阿德雷德降雨的機率是60%”,所有人都很樂意接受這種說法，但是很難用次數主義的條件來定義報導中所提的機率。 全世界只有一個地方叫阿德雷德,2018年11月2日也只是一個時間點。所以不存在無限序列，只是一時一地的單一事件。然而，次數主義的機率觀點不允許我們對單一事件以機率描述。從次數主義的角度來想,明天要麼下雨,要麼不下雨，不可重複事件不可能有“機率”。 其實次數主義的追隨者可以使用一些非常聰明的技巧來解決這道難題，一種方案就像有的氣象預報員會用的說辭：「以我所使用的預測方法，只要出現某些氣象條件就能預測60%的降雨機率。只要明日的氣象具備這些條件，預測有60%的降雨機率是合理的。」這樣的說法好像繞了好幾圈又違反直覺,不過之後的單元裡，我們會看到這套次數主義方法論如何形塑現代統計方法(例如 小單元 8.5)。\n\n\n\n\n\n圖 8.1: 次數主義機率論的模擬實驗範例。 如果您投擲一枚公平的硬幣無數次,最後會看到正面的比例值會趨近真實機率 0.5。每個趨勢圖呈現四個不同的模擬實驗， 每次實驗都是模擬投擲硬幣 \\(1000\\) 次,並且逐次紀錄擲出正面的比例。 儘管没有一個實驗結果的最後紀錄停在 \\(.5\\) ,如是能將模擬次數增加到無限次則必定會達到。\n\n\n\n\n\n\n8.2.2 貝氏觀點\n\n\n貝氏的機率觀通常也稱為主觀觀點,雖然這個觀點在統計學領域中一直是少數派,但是影響力在過去幾十年持續穩定增長。 貝氏一門還有很多分支,以至於難用三言兩語說明什麼是“貝氏”觀點。解釋主觀機率的最常見說法是將事件的機率定義為有智慧與理性能力的現代人對於事件發生的信念程度(degree of belief)。從這個角度進一步說,機率並不存在於現實世界,而是存在於人類及其他智能生物的思維和想像。3\n然而,為了實際應用主觀機率發展出來的方法，我們需要先把“信念程度”轉換為可操作的形式。今天已經發展出許多方法，在此介紹的形式化方法是“理性博弈”(rational gamling)： 假如我相信明天下雨的機率為 60%。若有人讓我下注,賭明天下雨的話,我會贏5美元,如果沒下雨,我就輸5美元。從我的角度來看,這顯然是一個相當不錯的賭注。 換句話說,如果我看到明天降雨機率只有 40%, 那麼接受這個賭注就是不明智的。 因此,我們可以通過我願意接受的賭注大小操作“主觀機率”。\n貝氏的方法論有什麼優缺點? 主要優點是你能指定想要評估的任何事件是否發生的機率，而且不必考慮那些事件會不會重複。對許多人來說，最大的缺點是你不能完全客觀看待機率事件。指定一個事件機率需要一個對應信念程度的實體，這個實體可能是一個人類、外星人、機器人或者甚至一個統計學家，無論如何必須設定存在一位持有某種信念的智能代理者。對許多計較客觀的學者來說,這是令人不舒服的想法,似乎會讓機率運算變得武斷。儘管貝氏方法論強調信念的代理者是理性的(遵守機率規則),但是每個人都可以有屬於自己，不一定和他人相同的信念。以前面的擲硬幣問題來說，觀察者A可以相信硬幣是公平的,而觀察者B不需要,不過兩者的看法都是理性的。次數主義觀點則不允許任兩位觀察者認為同一事件的機率有不同看法。如果有這種情況,至少其中一種看法是錯的。 然而貝氏方法論觀點容許歧異看法並存的情形， 不同背景知識的兩造觀察者，(在貝氏的機率擂台)可以合法地對同一事件持有不同信念。簡而言之,次數主義觀點有時被認為範圍太狹窄(禁止觀察者對許多事件指定機率),貝氏觀點有時則被認為範圍太廣泛(允許觀察者保留彼此看法的差異)。\n\n\n8.2.3 觀點之間的差異是什麼？何者正確？\n\n既然你已經看懂了這兩種觀點的核心想法,比較兩方觀點的差異有助你回答什麼是機率。回頭來談那場機器人足球比賽結果的三種解釋，你認為次數主義和貝氏兩種觀點會如何看待那種是最正確的機率定義?次數主義的追隨者會說那個解釋最符合他們心目中的機率觀?貝氏的追隨者會贊成那一個?那個解釋對於兩方來說毫無意義?如果您有充分理解兩派觀點,應該多少能回答以上的問題。\n好啦,假如你真的理解觀點之間的區別,你應該想知道那一種觀點是正確的?老實說,我不知道有沒有正確的答案。據我所知,次數主義追隨者思考事件序列的方式，以數學來看沒有什麼不正確的,貝氏觀點設定一個持有信念的理性代理者，這一點也有數學基礎。其實，如果讀者能研究得更深入,會發現兩個觀點在很多事情的意見是一致的。許多基於次數主義方法所做出的決定，也是貝氏觀點追隨者認為一個理性代理者也有相同結論；許多貝氏主義方法具有相當明顯的次數主義特性。\n我(原作者)自認是務實主義者,所以我會使用任何可相信的統計方法。工作多年的經驗,使我更喜歡貝氏方法,理由我將在本書的最後單元解釋。但是我並不會不再使用次數主義方法，而且不是每個人都能在兩種觀點之間自在轉換。像是 羅納德·費雪爵士(Sir Ronald Fisher),20世紀統計學巨擘之一,就是徹底反對貝氏觀點，立場最極端的反對者。他在討論統計學數學基礎的著作裡提到主觀機率觀點是 “不可突破的叢林，(放任發展)會阻止精確統計學概念的進步” (Fisher, 1922, 第 311 頁)。反方代表如心理學家保羅·米爾(Paul Meehl),他認為依靠次數主義方法會讓科學家變成 “能生產知識但無法提出真正洞見的書呆子，,與次數主義相伴的快樂學術路上留下了一整列受害者,其中沒有任何人輸出的科學知識能有效地開枝散葉”(Meehl, 1967, 第 114 頁)。讀者有心挖掘爭論觀點的文獻會發現,現代統計學的歷史和暢銷小說一樣精采。\n無論如何,雖然我(原作者)比較喜歡貝氏主義的觀點,但大多數統計分析都是建立在次數主義方法論，有用的推論方法都有實用價值。本書介紹的統計方法與多數心理學系統計課內容大致相同,如果讀者想了解大多數心理學家使用的統計工具,需要好好掌握次數主義方法。我能保證,學習次數主義的統計方法不會浪費時間。就算之後有一天想要跳到貝氏的陣營,讀者也應該至少要讀通一本談論 “正統” 次數主義觀點的書。此外,我不會完全忽略貝氏觀點。之後每個章節,我會在適當的段落添加一些貝氏觀點, 最後的 單元 16 會詳細介紹貝氏觀點的統計方法。"
  },
  {
    "objectID": "07-Introduction-to-probability.html#sec-Basic-probability-theory",
    "href": "07-Introduction-to-probability.html#sec-Basic-probability-theory",
    "title": "8  機率入門",
    "section": "8.3 基本機率論",
    "text": "8.3 基本機率論\n\n\n8.3.1 機率分佈入門\n\n\n我(原作者)個人最難以啟齒的一件事情是,我的衣櫃裡只有5條褲子。三條牛仔褲,一件西裝褲和一條運動褲。更讓我的朋友感到無聊的是,每條褲子都有個代號:\\(X_1\\)、\\(X_2\\)、\\(X_3\\)、\\(X_4\\) 和 \\(X_5\\)。這是真的，這是為什麼我的朋友都叫我「內心小劇場男子」。每個要外出的日子，我都會想好要穿那一條。即使我充滿想像力，也不會想要穿兩條褲子出門，而且都活了這麼久，我不會不穿褲子就出門。如果要用機率論的語言來描述我今天穿那條褲子出門,每條褲子的代號( \\(X\\) )合稱為基本事件。基本事件的關鍵條件是每紀錄一次觀察，紀錄結果可用其中一個代號代表，像是每天我穿了其中一條褲子就紀下這條的代號。因為現在我出門只會穿一條褲子，每一天穿那條褲子出門的紀錄剛好滿足基本事件的條件。同樣的,所有可能發生的基本事件集合稱為樣本空間。當然,我的朋友或有些學生會說“衣櫃”,那是因為他們不想用機率的詞彙來想像我的褲子。哭哭。\n好的,現在有一個樣本空間(衣櫃),包括許多可能的基本事件(今天穿出門的褲子),現在要做的就是為每一個基本事件分派一個機率。對於其中一個事件\\(X\\),該事件的機率\\(P(X)\\)是介於0和1之間的數字。\\(P(X)\\)的值越大,事件發生的可能性就越大。例如\\(P(X)=0\\) 表示事件\\(X\\)是不可能發生的(我從不穿那一條褲子出門)。另一方面,\\(P(X)=1\\) 表示事件\\(X\\)必定會發生(我總是穿那一條褲子出門)。至於介於兩者之間的機率值,代表我有時候會穿那條褲子。例如 \\(P(X)=0.5\\)代表我會出門的某幾天，有一半都會穿這條褲子。\n到這裡,我們快要建好一套機率分佈模型了。最後一件要知道的事懂是“某個事件總是會發生”。每一天我穿上褲子,我一定會穿好穿滿(你會想這是廢話對吧?)。這幾句陳腔濫調在機率的語彙裡，代表所有基本事件的機率加起來必定等於1。儘管多數人都不太在意，數學家給這個事實命名為總機率律(law of total probability)。更重要的是,只要符合總機率律的條件，就自然會得到了一個機率分佈(probability distribution)。表 8.2 就是有關原作者本人會穿那條褲子出門的機率分佈。\n\n\n\n\n表 8.2: 今天穿那條褲子出門的機率分佈\n\n\n褲子樣式\n事件代號\n事件機率\n\n\n\n\n藍色牛仔褲\nX_1\nP(X_1)=.5\n\n\n灰色牛仔褲\nX_2\nP(X_2) = .3\n\n\n黑色牛仔褲\nX_3\nP(X_3) = .1\n\n\n黑色西裝褲\nX_4\nP(X_4) = .0\n\n\n藍色運動褲\nX_5\nP(X_5) = .1\n\n\n\n\n\n\n\n\n\n每個基本事件都有一個介於 0 和 1 之間的機率值,只要加總所有事件的機率,會發現總和為 1。太棒了！我們還可以畫出一張漂亮的長條圖將這套機率分佈視覺化(複習一下 小單元 5.3),成果就像 圖 8.2 。到這裡的學習如果沒有意外，代表讀者已經熟悉之後的單元討論機率分佈的方式了，我(原作者)也能用一張圖解釋為何我總是穿牛仔褲出門了! 還需要強調的事情是,機率論容許非基本事件以及基本事件並存。舉個例子來說明，同樣也是用原作者會穿那條褲子出門的機率分佈，要如何用樣本空間裡的基本事件，描述穿任何一條牛仔褲出門的機率？也就是今天出門穿的是“藍色牛仔褲”、“黑色牛仔褲”或“灰色牛仔褲”，都是符合這個狀況的事件。以數學的詞彙來說，我們可以將事件 \\(E\\) 定義為對應符合條件的基本事件集合 \\((X1,X2,X3)\\)。只要其中任何一個基本事件發生,就是代表事件 \\(E\\) 發生。\\(E\\) 的定義確立後, \\(P(E)\\) 的機率是多少很容易了，就是全部加起來：因為藍色、灰色和黑色牛仔褲的機率分別是 \\(.5\\)、\\(.3\\) 和 \\(.1\\) ，所以我穿牛仔褲出門的機率等於 \\(.9\\)。用數學式子表達的話，就是 \\[P(E)=P(X_1)+P(X_2)+P(X_3)\\]\n也許您會認為,這些都太好懂，計算也沒什麼困難,這樣的感覺很多學生都會有。 現在討論的內容只是將日常生活的直觀用一些基本數學算則包裝而已。 不過，從簡單的案例開始,我們能構建一些非常強大的數學工具。 雖然這本書不會討論更深的細節, 表 8.3 列出滿足總機率律的樣本空間，可用的其他運算規則。 這些規則可以從前面描述的基本條件推導,由於本書不會用到這些規則,所以不會展示如何推導。\n\n\n\n\n\n圖 8.2: “褲子”機率分佈的視覺化呈現。 五個“基本事件”各自對應五條褲子。 每個事件都有發生的機率 - 這個值是 0 到 1 之間的數字。 所有事件的機率總和為 1。\n\n\n\n\n\n\n\n\n\n表 8.3: 滿足總機率律的一些運算規則\n\n\nEnglish\nNotation\nFormula\n\n\nnot A\n\\(P (\\neg A) \\)\n\\(1-P(A) \\)\n\n\nA or B\n\\(P(A \\cup B) \\)\n\\(P(A) + P(B) - P(A \\cap B) \\)\n\n\nA and B\n\\(P(A \\cap B) \\)\n\\(P(A|B) P(B) \\)"
  },
  {
    "objectID": "09-Hypothesis-testing.html#運用取樣分佈檢測統計值",
    "href": "09-Hypothesis-testing.html#運用取樣分佈檢測統計值",
    "title": "9  假設檢定",
    "section": "9.3 運用取樣分佈檢測統計值",
    "text": "9.3 運用取樣分佈檢測統計值\n現在我們可以開始討論如何建立一個假設檢定的具體步驟，讓我們回到一開始提的靈異感知（ESP）研究題目。暫時不管我們實際獲得的資料，先專注於實驗設計。無論資料裡的數值是什麼，都是表示 \\(N\\) 個人當中有 \\(X\\) 個人正確地辨認出隱藏卡牌的顏色。此外，若是虛無假設（null hypothesis）確實是真的，也就是說靈異感知不存在，每個人正確辨認出卡牌顏色的真實機率 \\(\\theta\\) 等於 \\(0.5\\)。在這種狀況，我們預期的資料是什麼樣子呢？很明顯，我們期望作出正確反應的人數比例接近 \\(50%\\)。換言之，我們可以用更數學式的語言表達 \\(\\frac{X}{N}\\) 大約等於 \\(0.5\\)。當然，我們不會預期真實的比例完全等於 \\(0.5\\)。例如，如果我們測試了 \\(N=100\\) 個人，其中有 \\(X=53\\) 人回答正確，我們也許不得不承認這筆資料與虛無假設是相當一致的。另一方面，如果有 \\(X=99\\) 個參與者回答正確，我們會非常有信心地認為虛無假設是錯的。同樣地，如果只有 \\(X=3\\) 人回答正確，我們也會非常有信心地認為虛無假設是錯的。現在讓我們用更專業的方式描述這些推論：我們有一個數值 \\(X\\) ，可以通過觀察資料計算出來。評估 \\(X\\) 之後，我們就必須決定是相信虛無假設，還是拒絕虛無假設並接受另一種假設。用來幫助我們作出決策的數值就稱為統計檢定值。\n選定了一個統計檢定值後，下一步是正式宣告那些統計值將導致我們拒絕虛無假設，那些統計值將導致我們接受虛無假設。為了做出決策，我們需要確定當實際結果符合虛無假設時，統計值的取樣分佈是什麼（不大記得的話，可以回到 小單元 8.3.1 復習）。為什麼我們需要設定取樣分佈？因為這能告訴我們，如果虛無假設是正確的，我們可以預期會得到那些 \\(X\\) 的數值。因此，我們可以使用這個分佈作為評估虛無假設與我們的資料是否一致的工具。\n如何確定統計值的取樣分佈呢？對於大多數假設檢定程序來說，這個步驟通常相當複雜，甚至有些假設檢定原作者和譯者自己都不是很懂，稍後在本書中同學們會看到某些檢定程序的介紹會有些含糊其辭。不過在某些檢定程序，設定取樣分佈是非常簡單的。ESP研究案例所使用的檢定程序，剛好是最簡單的一種。這個案例的母群參數 \\(\\theta\\) 就是參與者們回答問題時的總機率，而統計值 \\(X\\) 就等於所有參與者人數 \\(N\\) 裡正確回答的人數。我們之前在 小單元 8.4 這一節裡已經見過「二項分佈」，ESP案例的取樣分佈性質正好符合二項分佈！因此，為了使用二項分佈的符號和術語，我們會說虛無假設預測 \\(X\\) 的分佈是二項分佈，數學式就寫作\n\n\\[X \\sim Binomial(\\theta,N)\\]\n既然虛無假設主張 \\(\\theta = 0.5\\)，而我們的實驗有 \\(N=100\\) 位參與者 ，所以我們已經擁有所需要的取樣分佈。這個取樣分佈的視覺化如同 圖 9.1 。沒有什麼特別的，由視覺化繪圖可知，既然虛無假設說 \\(X=50\\) 是最有可能的結果，那麼我們有很大的機會看到 \\(40\\) 到 \\(60\\) 個正確的回答。\n\n\n\n\n\n\n圖 9.1: 這是當虛無假設為真時，我們測試統計值 \\(X\\) 的取樣分佈。對於 ESP 的案例，取樣分佈會符合二項式分佈。因為虛無假設主張正確回答的機率是 \\(\\theta = 0.5\\)，所以取樣分佈顯示，100次測試裡有50次 正確回答，是最有可能發生的結果。大多數的機率質量(probability mass)分散在40次到60次之間。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#sec-Hypothesis-testing-decision",
    "href": "09-Hypothesis-testing.html#sec-Hypothesis-testing-decision",
    "title": "9  假設檢定",
    "section": "9.4 統計推論的決策要素",
    "text": "9.4 統計推論的決策要素\n我們已經非常接近最後一步了。前一步設定了一個統計檢定值 \\((X)\\)，並且選擇了我們相當有信心的檢定值數值。如果 \\(X\\) 接近 \\(\\frac{N}{2}\\)，我們應該保留虛無假設，否則我們就應該拒絕虛無假設。剩下的問題是什麼呢？確切地說，我們應該設定那些統計檢定值是對應虛無假設，那些統計檢定值對應對立假設？以ESP研究案為例，假如我觀察到一個值 \\(X=62\\)。我應該做出什麼決策呢？我應該相信虛無假設還是對立假設？\n\n\n9.4.1 棄卻域與臨界值\n要回答這個問題，我需要向各位介紹統計檢定值 \\(X\\) 的棄卻域（critical region）。檢定的棄卻域對應那些會讓我們拒絕虛無假設的 \\(X\\) 數值集合（這就是為什麼棄卻域有時也被稱為拒絕域）。我們如何找到這個棄卻域呢？嗯，讓我們想一想已知的條件：\n\n為了拒絕虛無假設，\\(X\\) 應該非常大或非常小\n如果虛無假設為真，\\(X\\) 的取樣分佈會是 \\(Binomial(0.5, N)\\)\n如果 \\(\\alpha = .05\\)，則棄卻域必須包含這個取樣分佈的 5%。\n\n最後一點非常重要。棄卻域的範圍是指那些會導致我們拒絕虛無假設的 \\(X\\) 數值範圍，而這個範圍是經由取樣分佈代換後的機率質量所決定的。如果我們選擇了一個其涵蓋 \\(20%\\) 機率質量的棄卻域，且虛無假設是符合事實的，那麼拒絕虛無假設的錯誤機率就是 \\(20%\\)。換言之，我們完成了一個顯著水準為 \\(0.2\\) 的檢驗。如果我們要求顯著水準是 \\(\\alpha = .05\\)，那麼棄卻域只能涵蓋統計檢定量取樣分佈的 \\(5%\\) 機率質量。\n\n我們在此總結解決完成假設檢定程序的三個要點。我們的棄卻域包括了機率分佈的最極端數值，也就是機率分佈的尾部。 圖 9.2 展示了這個概念的視覺化。如果我們希望 \\(\\alpha = .05\\)，那麼對應的棄卻域是 \\(X \\leq 40\\) 和 \\(X \\geq 60\\)。6也就是說，如果回答正確的人數在 41 到 59 之間，那麼我們應該保留虛無假設。如果回答正確的人數在 0 到 40 或 60 到 100 之間，那麼我們就應該拒絕虛無假設。數字 40 和 60 通常被稱為臨界值(critical values)，因為這些數值定義了棄卻域的邊界。\n\n\n\n\n\n\n圖 9.2: 這張圖與 圖 9.1 的虛無假設 \\(X\\) 取樣分佈一樣，進一步展示ESP研究的假設檢定的棄卻域，假設檢定的顯著水準為\\(\\alpha = .05\\) 。灰色的柱子表示我們會保留虛無假設的 \\(X\\) 數值集合。深藍色的柱子表示棄卻域，也就是我們會拒絕虛無假設的 \\(X\\) 值。由於對立假設的主張是雙側的（即允許 \\(\\theta &lt; .5\\) 和 \\(\\theta &gt; .5\\)），因此棄卻域涵蓋分佈的兩個尾部。為確保 \\(\\alpha\\) 水準為 \\(.05\\)，我們需要確保左右區域各涵蓋了取樣分佈的 \\(2.5%\\)。\n\n\n\n\n最後，總結一下完成假設檢定的主要步驟：\n\n選擇一個顯著水準 (例如，\\(\\alpha = .05\\))；\n選擇一個適當的統計檢定值 (例如，\\(X\\))，並且設定有比較意義的\\(H_0\\)和\\(H_1\\);\n假設虛無假設是符合事實的，找出該統計檢定值的取樣分佈（在ESP案例為二項分佈）；\n計算會產生符合 \\(\\alpha\\) 的棄卻域（0-40和60-100）。\n\n現在我們所要做的就是用實際資料計算統計檢定值（例如 \\(X=62\\)），然後比較檢定值與臨界值做出決策。由於 \\(62\\) 大於臨界值 \\(60\\)，我們可以拒絕虛無假設。也可以說，我們根據檢定結果得到一個在統計顯著的結論。\n\n\n\n9.4.2 小心使用統計“顯著”\n\n統計學和其他占卜術一樣，擁有一套專門術語，故意設計成讓非專業人員無法從字面理解術語的意思。 – G. O. Ashley 7\n\n在此需要講個關於 “significant”(常見中文說法“顯著”) 這個詞怎麼來的題外話。“significant” 在統計學中的概念其實很簡單，但這樣的命名並不夠好。如果實際資料能讓我們拒絕虛無假設，我們會說 “the result is statistically significant”(常見中文說法”結果有統計顯著性”)，通常簡單寫成 “the result is significant”(常見中文說法”有顯著結果”)。這個英文詞彙其實由來已久，來源可以追溯到 “significant” 的意思只是表達 “indicated”(已確認)的時代，並沒有現代英語的 “重要” 之類的含義。因此，今天許多讀者在開始學習統計學時會感到非常困惑，因為他們認為 “significant result” 必定是一個重要的結果。實際上，這並不是最早統計學家開始使用這個詞的意思。所有用”statistically significant”表達的主張， 只是表示資料允許我們拒絕一個虛無假設。至於結果是不是真的重要，則是另一個完全不同的問題，並且有其他各種因素的影響。\n\n\n\n9.4.3 單側與雙側檢定的不同\n還有一件事情要提醒各位同學，就是虛無假設與對立假設的設定方式。假如前面我所使用的統計假設是：\\[H_0: \\theta=0.5\\] \\[H_1:\\theta \\neq 0.5\\] 我們會發現對立假設涵蓋了 \\(\\theta &lt; .5\\) 和 \\(\\theta &gt; .5\\) 這兩種可能的數值集合。這代表我認為超感官知覺可能造成優於純粹猜測的表現，也可能產生比純粹猜測還差的表現（有些人就是會這麼認為），那麼這樣的設定是有意義的。在統計學的語彙庫，這稱為雙側檢定(two-sided test)。這是因為對立假設涵蓋了 無假設兩側的數值集合，因此檢定的棄卻域覆蓋取樣分佈的兩側尾部（如果 \\(\\alpha = .05\\)，則每側尾部佔取樣分佈的2.5％），如同 圖 9.2 的展示。不過，這不是唯一的可能結果。如果我只在乎超感官知覺能夠產生優於純粹猜測的表現時，才願意相信這是事實，那麼對立假設就只會涵蓋 \\(\\theta &gt; .5\\) 的數值集合。因此，虛無假設和對立假設就會變成\\[H_0: \\theta \\leq 0.5\\] \\[H_1: \\theta &gt; 0.5\\] 這種檢定條件 就是所謂的單側檢定(one-sided test)，此時檢定的棄卻域只有覆蓋取樣分佈的右側尾部，如同 圖 9.3 的展示。\n\n\n\n\n\n\n圖 9.3: 單側檢定的臨界區間。此狀況的對立假設是\\(\\theta \\geq .5\\)，當\\(X\\)的值很大，我們才能拒絕虛無假設。因此，臨界區間僅覆蓋取樣分佈的較大數值，具體來說是分佈的右側的 \\(5%\\) 。比較一下 圖 9.2 的雙側檢定狀況。"
  },
  {
    "objectID": "09-Hypothesis-testing.html#假設的層次",
    "href": "09-Hypothesis-testing.html#假設的層次",
    "title": "9  假設檢定",
    "section": "9.1 假設的層次",
    "text": "9.1 假設的層次\n我們從一個原作者虛構的狂想開始談吧：我認為每個人活得夠老，都會向瘋狂的想法屈服。我這輩子真正想做的研究，將在我升任正教授的那一天開始，因為我在象牙塔中會受到終身職的保障，那天我總算能夠拋棄理智，完全投入那個最沒有成效的心理研究領域：證實人類有超感官知覺（ESP）。3\n假如這一天終於來臨了，我要做的第一項研究是一項簡單的透視力測試實驗。每個參與者坐在桌子前，由實驗者向他展示一張卡片。這張卡片的一面是黑色的，另一面是白色的。實驗者把卡片放在桌子上，接著帶領參與者到隔壁房間。在實驗者與參與者離開後，第二位實驗者會隨機地把卡片翻到黑面或白面朝上，然後第二位實驗者到隔壁房間，詢問參與者現在卡片的那一面朝上。這個實驗的參與者只會進行一次測試，每個人只會看到一張卡片，只回答一個問題，而參與者在回答問題前都不會與知道正確答案的第二位實驗者接觸。因此，我的資料紀錄非常簡單：我問了N個人的問題，其中有 \\(X\\) 個人給了正確的答案。為了具體說明，若是這次實驗我測試了100個人，其中有62個人回答正確。這會是一個令人驚訝的大數字，但是這個數字是否足夠讓我聲稱發現了ESP的證據呢？這就是檢驗假設有無效用的情況。然而，在我們談論如何檢驗假設之前，我們需要明白要如何設定這項實驗的假設。\n\n\n\n9.1.1 研究假設還是統計假設\n首先需要清楚區別的是研究假設和統計假設之間的差別。在我設想的ESP研究中，我的整體科學目標是證實人類有透視能力。在這樣的場景，我有一個清晰的研究目標：我希望發現ESP的證據。在其他場景，我的想法可能會比較中立，也就是我可能會說我的研究目標是確定人類是否有透視能力。無論如何描述我的目標，我想傳達給各位同學的基本觀點是，制定研究假設是提出一個實際的、可測試的科學主張。如果你是一名心理學家，那麼你的研究假設基本上是關於心理學構念的。以下任何一種案例都可說是研究假設：\n\n聆聽音樂會降低你對其他事物的注意力能力。這是一種關於兩個有心理學意義的構念之間有因果關係的主張（聆聽音樂和注意力），因此這是一個非常合理的研究假設。\n智力與個性有關。和前一個一樣，這個假設主張兩個有心理學意義的構念之間存在關係性（智力和個性），但這個主張立埸比較弱：探討相關性而不是因果關係。\n智力是訊息處理速度。這個假設與前面兩個很不一樣。實際上這並不是一個因果關係或關聯性的假設，而是關於智力基本特性的本體論主張（我相當確定是這樣的）。通常來說，設計實驗測試像是“\\(X\\)是否影響\\(Y\\)？”，比回答“\\(X\\)是什麼？”這樣的問題要容易得多。在實際情況通常是你會找到方法，測試基本特性所形成的關聯性假設。例如，如果我相信智力的本質是大腦中訊息處理速度，我就會設計實驗探討智力和訊息處理之間的關係。因此，大多數日常生活中想到的研究問題雖然都與本質有關，但是通常是基於好奇關於自然界本體論問題的更深層動機。\n\n請注意在真實的實驗室，我會設定幾個互相重疊的研究假設。儘管我設計ESP實驗的終極目標是測試“人類有ESP”這樣的本體論主張，但是實際操作會限制自己只測試目標更狹窄的假設，像是“某些人可以用透視‘看見’物體”。話雖如此，有一些看似目標明確的主張，在任何意義上都不算是合適的研究假設：\n\n愛情就像戰場。這個假設過於模糊，無法進行測試。雖然研究假設可以有一定程度的模糊性，但是必須能夠將理論觀念具體化。也許我不夠有創造力，想不到能將這個假設轉化為具體研究設計的方式。如果真的有辦法，那麼應該不是科學的研究假設，而是一首流行歌曲。這並不是說這樣的假設不有趣，而是要指出許多人能想到的深刻問題都是屬於這種類別。也許有一天科學家能夠構建關於愛情的可測試理論，或者測試上帝是否存在等等。但是現在的我們還做不到，我不會指望看到一個令人滿意的科學方法來解決這些問題。\n套套邏輯俱樂部的第一條規則就是套套邏輯俱樂部的第一條規則。這是不具備任何實質意義的主張，儘管形式上是符合邏輯的。因為在任何自然狀態都不能提出與此主張相反的看法，我們會說這是一個不可證偽的假設，因此這樣的主張不屬於科學研究的領域。在科學研究，無論你想研究的問題是什麼，你提出的主張都必須有可能是錯誤的。\n在我的實驗裡，較多的參與者會說‘是’，而不是‘否’。這並不是一個有意義的研究假設，因為重點的是資料本身而非心理學問題（當然，除非真正要研究的問題是關於多數人是不是有回答“是”的偏好！）。實際上，這個假設看起來更像是一個統計假設而非研究假設。\n\n正如同學所見，有的研究假設的主張可能會有些混亂，不過都是各樣科學主張的一種。統計假設則不是一種主張。統計假設必須具有數學精確性，並且必須對資料的生成機制（也就是“母群”）的特徵提出具體的條件。即便如此，統計假設的內在意圖必須有與真正的研究假設有一個明確的關係！例如，在我的ESP研究中，我的研究假設是有些人能夠透視牆壁看到隔壁的物體。我要做的是將這樣的研究假設對應到產生資料的方法陳述。因此，現在我們考慮一下要如何表達這樣的陳述。我感興趣的實驗數值是 \\(P(correct)\\)，也就是實驗參與者正確回答問題的理論上為真但未知之機率。讓我們使用希臘字母\\(\\theta\\)（theta）來表示這項機率。以下是四種不同的統計假設：\n\n如果ESP不存在，而我的實驗設計沒有偏誤，那麼參與者的回答只是猜測。因此，我應該期望有一半參與者的回答是正確的，所以我的統計假設是，回答正確的理論機率是 \\(\\theta=0.5\\)。\n或者，假設ESP存在並且參與者真的能夠看到卡片。如果實驗結果真的是這樣，參與者回答的正確率會高於只是猜測，所以統計假設是 \\(\\theta &gt; 0.5\\)。\n第三種可能是ESP確實存在，但是參與者並沒有意識到透視看到的物體顏色是相反的（好吧，這有些荒謬，但我們永遠無法知道）。如果是這樣的實驗結果，我會期望參與者回答的正確率會低於只是猜測。所以統計假設是 \\(\\theta &lt; 0.5\\)。\n最後，如果人類確實有ESP，但是我不知道參與者是否看到了正確的顏色。在這種情況下，我只能期望參與者回答的正確率不等於0.5。所以統計假設是 \\(\\theta \\neq 0.5\\)。\n\n以上例子都是合乎科學研究目標的統計假設，因為每條陳述都有定義母群參數，並且緊密扣連我的實驗目的。\n我希望這些例子可以讓同學清楚了解，當研究者要構建一個統計假設檢定程序時，實際上要考慮兩種不同層次的假設。首先，研究者要有一個研究假設（關於心理學的主張），能對應到一個統計假設（關於數據生成母群的主張）。以我的ESP實驗來說，會像 表 9.1 的表達。\n\n\n\n\n\n\n表 9.1: 原作者狂想的研究假設與統計假設\n\n\n研究假設\n人類有超感官知覺\n\n\n\n\n統計假設\n\n\n\n\n\n\n\n\n\n小結一下兩種假設的差別。統計假設檢定的測試對象是統計假設，而非研究假設。假如你的研究設計不良，會造成研究假設和統計假設之間的斷裂。舉個有點荒謬的狀況，要是我的ESP研究是在參與者可以從窗戶反光看到卡片的環境裡進行，那麼我肯定能得到非常強的證據證明 \\(\\theta \\neq 0.5\\)，但是這並不能告訴我們”人類真的有ESP”。\n\n\n\n9.1.2 虛無假設與對立假設\n到目前為止還算順利。我有一個研究假設，對應我想相信的世界，還有映射到一個對應於資料生成方式的統計假設。接下來我要創造一個新的統計假設（“虛無假設”，\\(H_0\\)），這對很多人來說有些違反直覺。因為”虛無假設”對應與我想相信的事情完全相反，然後專注於驗證這條統計假設，並且忽略實際關心的事情（現在被稱為”對立假設”，\\(H_1\\)）。在我的ESP研究裡，虛無假設是 \\(\\theta = 0.5\\)，因為如果人類沒有ESP，我會期望看到這個結果。當然，我期望ESP是真的，所以這個虛無假設對立的假設就是 \\(\\theta \\neq 0.5\\)。實際上，我們是在將 \\(\\theta\\) 涵括的可能數值分成兩類：我真心期望不是真的那些數值（虛無假設），以及如果ESP被證實是存在的，我會很高興的那些數值（對立假設）。完成這些設定之後，需要意識到的關鍵是，假設檢定的真正目標不是證實對立假設（可能）是真的，而是證實虛無假設（可能）是假的。很多初學的同學會覺得這樣的邏輯很奇怪。\n就我的學習經驗，最好的比喻是把假設檢定當作刑事法庭審判4。虛無假設就是被告，研究者就像檢察官，統計檢定程序是法官。像真正的刑事審判程序一樣，一開始我們要以無罪推定原則看待虛無假設，也就是說它的主張應被認為是真實的，直到位研究者能夠明確證實它的主張是錯的。研究者可以憑自由意志設計實驗（當然也要合理），目的就是要用可能性最大的資料證實虛無假設是錯的。然而，統計檢定(法官)設定了審判的規則，而這些規則是為了保護虛無假設而設計的，這些規則是要確保如果虛無假設的主張確實是真的，讓法官誤判的機會保持在很低的水準。這非常重要，畢竟虛無假設沒有律師幫忙辨護，而研究者卻在拼命地嘗試證實它的主張是錯的，所以必須有一方提供虛無假設一些保護。"
  },
  {
    "objectID": "Prelude-Part-V.html",
    "href": "Prelude-Part-V.html",
    "title": "線性模型學習進程",
    "section": "",
    "text": "教程原文1\n簡体中文翻譯2\n各章節的線性模型版示範，集合於此。\n\n\n\n\n\n\nhttps://lindeloev.github.io/tests-as-linear/↩︎\nhttps://cosx.org/2019/09/common-tests-as-linear-models/↩︎"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#相關",
    "href": "10-Correlation-and-linear-regression.html#相關",
    "title": "10  相闗與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\n這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的相關。首先，讓我們看一下列在 Table 14.1 的本章示範資料描述統計。\n\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。 \n \n  \n    變項 \n    最小值 \n    最大值 \n    平均值 \n    中位數 \n    標準差 \n    四分位數間距 \n  \n \n\n  \n    老爸的沮喪程度 \n    41.00 \n    91.00 \n    63.71 \n    62.00 \n    10.05 \n    14.00 \n  \n  \n    老爸睡眠小時數 \n    4.84 \n    9.00 \n    6.97 \n    7.03 \n    1.02 \n    1.45 \n  \n  \n    小嬰兒睡眠小時數 \n    3.25 \n    12.07 \n    8.05 \n    7.95 \n    2.07 \n    3.21 \n  \n\n\n\n\n\n\n讓我們從一個與每個新生兒父母都息息相關的主題談起：睡眠。這裡使用的資料集是虛構的，但是來自本人(原作者)的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有測量我的沮喪分數、我的睡眠習慣和我兒子的睡眠習慣持續是100天。身為一位數位時代的書呆子，我把資料保存在一個名為parenthood.csv的檔案。匯入jamovi，我們可以看到四個變項：dani.sleep，baby.sleep，dani.grump和day。請注意，當您首次打開這份檔案，jamovi可能無法正確猜測每個變項的資料類型，同學可以自行修正：dani.sleep，baby.sleep，dani.grump和day都可以被指定為連續變項，而ID是一個名義且為整數的變項。2\n接著我會看一下一些基本的描述性統計數據，並且三個我有興趣的變項視覺化，也就是 Figure 14.1 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計數據，你就要報告所有數據。如果我要以此結果撰寫報告，我會挑出那些我自己以及我的讀者最感興趣的統計數據，然後將它們放入像 Table 14.1 這樣的簡潔的表格裡。3 需要注意的是，當我將數據放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。\n\n\n\n\n\n\n\nFigure 10.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。\n\n\n\n\n\n\n10.1.2 相關的強度與方向\n我們可以繪製散佈圖，讓我們能俯瞰兩個變項之間的相關性。雖 然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較dani.sleep和dani.grump之間的關係（ fig-fig10-2 ，左）與baby.sleep和dani.grump之間的關係（ fig-fig10-2 ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我或者我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，dani.sleep和dani.grump之間的關係比baby.sleep和dani.grump之間的關係更強：左圖比右圖更加整齊。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。\n\n\n\n\n\n\nFigure 10.2: 左圖是dani.sleep(老爸睡眠小時數)與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖。\n\n\n\n\n相反地， Figure 14.3 的另外兩個散佈圖告訴我們另一個角度的故事。比較“baby.sleep 與 dani.grump”的散佈圖（左）和“baby.sleep 與 dani.sleep”的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。\n\n\n\n\n\n\nFigure 10.3: 左圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.sleep(老爸睡眠小時數)的散佈圖。\n\n\n\n\n\n\n10.1.3 相關係數\n現在我們要進一步延伸上述的概念，也就是正式認識 相關係數(correlation coefficient)。更具體地說，本節主要介紹皮爾森相關係數（Pearson’s correlation），慣例書寫符號是 \\(r\\)。在下一節，我們會用更精確符號 \\(r_{XY}\\) ，表示兩個變項 \\(X\\) 和 \\(Y\\) 之間的相關係數，值域涵蓋-1到1。當\\(r = -1\\)時，表示變項之間是完全的負相關；當\\(r = 1\\)時，表示變項之間是完全的正相關；當\\(r = 0\\)時，表示變項之間是完全沒有關係。 Figure 14.4 展示幾種不同相關性的散佈圖。\n[其他技術細節 4]\n\n\n\n\n\n\nFigure 10.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為\\(0, .33, .66, 1\\)。右欄的相關係數由上而下為\\(0, -.33, -.66, -1\\)。\n\n\n\n\n標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: \\(r = 1\\)代表著完美的正相關，\\(r = -1\\)代表著完美的負相關。稍後解讀相關係數這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。\n\n\n\n10.1.4 相關係數計算實務\n只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 Figure 14.5 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。\n\n\n\n\n\n\nFigure 10.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。\n\n\n\n\n\n\n10.1.5 解讀相關係數\n在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀\\(r = 0.4\\)的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於\\(0.95\\)的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到\\(0.9\\)，任何使用這個測驗預測的理論就會失效5。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過\\(0.3\\)，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 Table 14.2 的概略式解讀原則。\n\n\n\n\n\nTable 10.2:  解讀相關係數的粗略指南。強調粗略是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。 \n \n  \n    相關係數 \n    強度 \n    方向 \n  \n \n\n  \n    -1.0 ~ -0.9 \n    非常強 \n    負相關 \n  \n  \n    -0.9 ~ -0.7 \n    強 \n    負相關 \n  \n  \n    -0.7 to -0.4 \n    中等 \n    負相關 \n  \n  \n    -0.4 ~ -0.2 \n    弱 \n    負相關 \n  \n  \n    -0.2 ~ 0 \n    微弱 \n    負相關 \n  \n  \n    0 ~ 0.2 \n    微弱 \n    正相關 \n  \n  \n    0.2 ~ 0.4 \n    弱 \n    正相關 \n  \n  \n    0.4 ~ 0.7 \n    中等 \n    正相關 \n  \n  \n    0.7 ~ 0.9 \n    強 \n    正相關 \n  \n  \n    0.9 ~ 1.0 \n    非常強 \n    正相關 \n  \n\n\n\n\n\n\n然而，有一件點任何一位統計學教師都會不厭其煩地提醒同學，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)(Anscombe, 1973)，其中有四個資料集。每個資料集都有兩個變項， \\(X\\) 與 \\(Y\\)。四個資料集的 \\(X\\) 平均值都是 \\(9\\)， \\(Y\\) 的平均值都是 \\(7.5\\)。所有 \\(X\\) 變項的標準差幾乎相同，\\(Y\\) 變項的標準差也是一致。每種資料集的\\(X\\) 和 \\(Y\\) 相關係數均為 \\(r = 0.816\\)。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。\n也許你認為這四個資料集看起來很相似，其實上並非如此。從 Figure 14.6 的散佈圖可以發現，所有四個資料集的\\(X\\) 和 \\(Y\\) 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始數據」（見 Chapter 5 ）。\n\n\n\n\n\n\nFigure 10.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\n皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。\n線性關係是當一個變項\\(X\\)的數值增加，也能反映另一個變項\\(Y\\)的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（\\(X\\)）準備一個科目，那麼他排名的成績應該只有0％（\\(Y\\)）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。\n我們用 Figure 14.7 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的次序關係(ordinal relationship)。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數\\(r=0.91\\)能表達的。\n\n\n\n\n\n\nFigure 10.7: 這個圖解展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數\\(r = .91\\)。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數\\(\\rho = 1\\)。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。\n\n\n\n\n那麼我們要如何解決這個問題呢？其實很容易。如果我們要評估變項之間的次序關係，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”來衡量學生的努力，而是按照他們的讀書時間長短，將這\\(10\\)名學生排序。也就是說，學生\\(2\\)花在讀書的時間最少（\\(2\\)個小時），所以他獲得了最低的排名（排名=\\(1\\)）。接下來最懶惰的是學生\\(4\\)，整個學期只讀了\\(6\\)個小時的書，所以他獲得了次低的排名（排名=\\(2\\)）。請注意，在此用“排名=\\(1\\)”來表示“低排名”。在日常言談裡，多數人使用“排名=\\(1\\)”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！\n好的，讓我們從最努力且最成功的學生開始排名。 Table 14.3 顯示從最努力且最成功的學生排名的次序值。6\n\n\n\n\n\nTable 10.3:  十位學生的工作時間與得分數值次序 \n \n  \n    學生編號 \n    讀書時間序列 \n    成績序列 \n  \n \n\n  \n    學生 1 \n    10 \n    10 \n  \n  \n    學生 2 \n    1 \n    1 \n  \n  \n    學生 3 \n    5 \n    5 \n  \n  \n    學生 4 \n    8 \n    8 \n  \n  \n    學生 5 \n    9 \n    9 \n  \n  \n    學生 6 \n    6 \n    6 \n  \n  \n    學生 7 \n    7 \n    7 \n  \n  \n    學生 8 \n    3 \n    3 \n  \n  \n    學生 9 \n    4 \n    4 \n  \n  \n    學生 10 \n    2 \n    2 \n  \n\n\n\n\n\n\n有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。\n至此我們等於重新發現 斯皮爾曼等級相關(Spearman’s rank order correlation)，通常用符號 \\(\\rho\\) 表示，以區分皮爾森相關係數\\(r\\)。我們可以在“相關矩陣”選單選擇“Spearman”，使用jamovi計算斯皮爾曼等級相關係數。7"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#散佈圖",
    "href": "10-Correlation-and-linear-regression.html#散佈圖",
    "title": "10  相關與線性迴歸",
    "section": "10.2 散佈圖",
    "text": "10.2 散佈圖\n散佈圖是一種簡單但有效的視覺化工具，用於具現兩個變項之間的關係，就像相關這一節所展示的圖表。通常提到“散佈圖”這個術語時，指的是具體的視覺化結果。在散佈圖中，每個觀察值都是對應一個資料點。一個點的水平位置表示一個變項的觀察值，垂直位置表示觀察值在另一個變項的數值。在許多使用情境，我們對於變項間的因果關係並沒有清晰的看法（例如，A是否引起B，還是B引起A，還是其他變項C控制A和B）。若是這樣，x軸和y軸上代表那個變項並不重要。然而在許多情境，研究者對於那個變項最有可能是原因或結果，會有一個相當明確的想法，或者對於何者為因至少有一些懷疑。若是這樣，用x軸代表原因的自變項，用y軸代表效應的應變項是一種傳統的繪圖規範。了解這樣的規範，讓我們來看一下如何合理運用jamovi繪製散佈圖，同樣使用在相關這一節做為示範的資料集（parenthood.csv）。\n假定我的目標是繪製一個顯示我睡眠時間（dani.sleep）與隔天沮喪程度（dani.grump）兩個變項關係的散佈圖，我們有兩種不同的方法使用jamovi得到我們想要的圖。第一種方法是設定’Regression’ - ‘Correlation Matrix’選單下方的’Plot’選項，這樣可以得到如圖 Figure 14.8 的結果。請注意，jamovi會繪製一條通過資料點的直線，稍後在認識線性迴歸模型這一節進一步說明。以這種方法繪製散佈圖也能繪製’變項密度’，這個選項會添加一條密度曲線，顯示每個變項的資料分佈狀況。\n\n\n\n\n\n\nFigure 10.8: 使用jamovi相關分析模組的’Correlation Matrix’所繪製的散佈圖。\n\n\n\n\n第二種方法是使用jamovi的附加模組之一scatr，只要點擊jamovi介面右上角的那個大「\\(+\\)」，在jamovi模組庫裡找到scatr，然後點擊「install」進行安裝。安裝成功後，在「Exploration」的選單下方會多出新的「Scatterplot」選項。這種方法繪製的散佈圖和第一種方法不大一樣，如同 Figure 14.9 所顯示，但是透露的訊息是一樣的。\n\n\n\n\n\n\nFigure 10.9: 使用jamovi擴充模組’scatr’繪製的散佈圖\n\n\n\n\n\n10.2.1 更多解讀散佈圖的方法\n通常我們會需要查看多個變項之間的關係，可以在 jamovi 的 ‘Correlation Matrix’選單下方的’Plot’ 選項，勾選繪制散佈圖矩陣。只要加入另一個變項到要變項列表，例如 baby.sleep，jamovi 就會生成一個散佈圖矩陣，如同 Figure 10.10 的示範。\n\n\n\n\n\n\nFigure 10.10: jamovi繪製的散佈圖矩陣"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "href": "10-Correlation-and-linear-regression.html#認識線性迴歸模型",
    "title": "10  相闗與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\nStripped to its bare essentials, linear regression models are basically a slightly fancier version of the Pearson correlation (see [Correlations]), though as we’ll see regression models are much more powerful tools.\nSince the basic ideas in regression are closely tied to correlation, we’ll return to the parenthood.csv file that we were using to illustrate how correlations work. Recall that, in this data set we were trying to find out why Dani is so very grumpy all the time and our working hypothesis was that I’m not getting enough sleep. We drew some scatterplots to help us examine the relationship between the amount of sleep I get and my grumpiness the following day, as in Figure 14.9, and as we saw previously this corresponds to a correlation of \\(r = -.90\\), but what we find ourselves secretly imagining is something that looks closer to Figure 10.11 (a). That is, we mentally draw a straight line through the middle of the data. In statistics, this line that we’re drawing is called a regression line. Notice that, since we’re not idiots, the regression line goes through the middle of the data. We don’t find ourselves imagining anything like the rather silly plot shown in Figure 10.11 (b).\n\n\n\n\n\nFigure 10.11: 圖a展示同 Figure 14.9 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。\n\n\n\n\nThis is not highly surprising. The line that I’ve drawn in Figure 10.11 (b) doesn’t “fit” the data very well, so it doesn’t make a lot of sense to propose it as a way of summarising the data, right? This is a very simple observation to make, but it turns out to be very powerful when we start trying to wrap just a little bit of maths around it. To do so, let’s start with a refresher of some high school maths. The formula for a straight line is usually written like this\n\\[y=a+bx\\]\nOr, at least, that’s what it was when I went to high school all those years ago. The two variables are \\(x\\) and \\(y\\), and we have two coefficients, \\(a\\) and \\(b\\).8 The coefficient a represents the y-intercept of the line, and coefficient b represents the slope of the line. Digging further back into our decaying memories of high school (sorry, for some of us high school was a long time ago), we remember that the intercept is interpreted as “the value of y that you get when \\(x = 0\\)”. Similarly, a slope of b means that if you increase the x-value by 1 unit, then the y-value goes up by b units, and a negative slope means that the y-value would go down rather than up. Ah yes, it’s all coming back to me now. Now that we’ve remembered that it should come as no surprise to discover that we use the exact same formula for a regression line. If \\(Y\\) is the outcome variable (the DV) and X is the predictor variable (the \\(IV\\)), then the formula that describes our regression is written like this\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\nHmm. Looks like the same formula, but there’s some extra frilly bits in this version. Let’s make sure we understand them. Firstly, notice that I’ve written \\(X_i\\) and \\(Y_i\\) rather than just plain old \\(X\\) and \\(Y\\) . This is because we want to remember that we’re dealing with actual data. In this equation, \\(X_i\\) is the value of predictor variable for the ith observation (i.e., the number of hours of sleep that I got on day i of my little study), and \\(Y_i\\) is the corresponding value of the outcome variable (i.e., my grumpiness on that day). And although I haven’t said so explicitly in the equation, what we’re assuming is that this formula works for all observations in the data set (i.e., for all i). Secondly, notice that I wrote \\(\\hat{Y}_i\\) and not \\(Y_i\\) . This is because we want to make the distinction between the actual data \\(Y_i\\), and the estimate \\(\\hat{Y}_i\\) (i.e., the prediction that our regression line is making). Thirdly, I changed the letters used to describe the coefficients from a and \\(b\\) to \\(b_0\\) and \\(b_1\\). That’s just the way that statisticians like to refer to the coefficients in a regression model. I’ve no idea why they chose b, but that’s what they did. In any case \\(b_0\\) always refers to the intercept term, and \\(b_1\\) refers to the slope.\nExcellent, excellent. Next, I can’t help but notice that, regardless of whether we’re talking about the good regression line or the bad one, the data don’t fall perfectly on the line. Or, to say it another way, the data \\(Y_i\\) are not identical to the predictions of the regression model \\(\\hat{Y}_i\\). Since statisticians love to attach letters, names and numbers to everything, let’s refer to the difference between the model prediction and that actual data point as a residual, and we’ll refer to it as \\(\\epsilon_i\\).9 Written using mathematics, the residuals are defined as\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\nwhich in turn means that we can write down the complete linear regression model as\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "href": "10-Correlation-and-linear-regression.html#線性迴歸模型的參數估計",
    "title": "10  相關與線性迴歸",
    "section": "10.4 線性迴歸模型的參數估計",
    "text": "10.4 線性迴歸模型的參數估計\n\n好的，現在讓我們重新繪製散佈圖，這次會添加一些線條顯示所有觀察值的殘差。當迴歸線的適合度(fittedness)最佳時，每個殘差數值（實心黑線的長度）看起來都非常小且接近，如同 Figure 10.12 (a) ，但是當迴歸線的適合度不夠好，每個殘差之間的差異就會非常大，可以從 Figure 10.12 (b)看到這樣的差別。嗯，也許在尋找一條最好的迴歸模型時，我們會希望得到儘可能小的殘差。是的，這確實有道理。在統計實務，我們可以說「最適合」的迴歸線是具有所有殘差最小的線。或者更好的說法是，因為統計學家似乎喜歡將所有數值都用平方(sqaured)處理，也就是說：\n\n以資料估計的迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 是殘差平方和最小得到時得到的估計值，我們可以兩者的公式展開寫成 \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) 與 \\(\\sum_i \\epsilon_i^2\\) 。\n\n\n\n\n\n\n\nFigure 10.12: 圖(a)展示各觀察值資料點與穿越資料點中心地帶的最佳迴歸線之殘差，圖(b)展示觀察值資料點與最差迴歸線的殘差。前者的殘差總和明顯小於後者。\n\n\n\n\n是的沒錯，這樣說明起來更有學問一些。而且我將這段話縮排，表示這樣說可能是正確的解答。既然這是正確解答，那麼要值得注意的是，迴歸線的係數都是估計值（請復習 Chapter 8 ，使用點估計方法猜測一個母群的參數！），這也是為什麼我要加個小帽子 \\(\\hat\\) ，區別會放在報告的是\\(\\hat{b}_0\\)和\\(\\hat{b}_1\\)，而不是 \\(b_0\\) 和 \\(b_1\\)。最後，我還要指出，由於實際上有許多方法來估計迴歸模型，這一節說明的估計方法正式名稱是普通最小平方法（Ordinary Least Squares，OLS）。\n至此，我們已經得到「最佳」迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 的具體定義。下一個問題自然是：如果最佳迴歸係數是那些符合最小化殘差平方和的係數，我們要如何算出這些數值呢？實際上，這個問題的答案比較複雜，並且無法幫助你理解迴歸的邏輯。9這一次，我放過各位同學，直接介紹 jamovi 操作方法，瑣碎的讓jamovi來處理。\n\n\n10.4.1 實作線性迴歸模型\n以下是用parenthood.csv 資料檔案執行線性迴歸分析的步驟，請打開 jamovi 的 ‘Regression’ - ‘Linear Regression’ 選單 。接著，將 dani.grump 指定為 ‘Dependent Variable’，dani.sleep 輸入到 ‘Covariates’ 對話框。報表介面將出現如 Figure 10.13 的結果，結果顯示截距 \\(\\hat{b}_0 = 125.96\\) 和斜率 \\(\\hat{b}_1 = -8.94\\)。換言之， Figure 10.11 的最適合迴歸線的公式為：\n\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 10.13: jamovi的線性迴歸分析示範畫面。\n\n\n\n\n\n\n10.4.2 解讀線性迴歸模型參數估計\n最後要知道的是如何解釋這些係數。讓我們從 \\(\\hat{b}_1\\) 開始，也就是斜率。回想一下斜率的定義，\\(\\hat{b}_1=-8.94\\) 代表將 \\(X_i\\) 增加 1， \\(Y_i\\) 就會減少 8.94。換言之，多睡一個小時的話，我的心情就會改善，我的沮喪程度就會降低 8.94 。那麼截距呢？由於 \\(\\hat{b}_0\\) 代表「當 \\(X_i\\) 為 0 時 \\(Y_i\\) 的期望值」，這就是說如果我一夜都沒睡 (\\(X_i = 0\\))，我的沮喪程度就會瘋狂升高到不敢想像的數值 (\\(Y_i = 125.96\\))。我想我最好避免這種狀況。\n\n\n\n還有關於等級資料(Rank data)的迴歸分析，請參考線性模型的學習取向的相關與線性迴歸這一節。\n\n\n\n\n以下 Section 10.5 、 Section 10.10 以及 Section 10.11 等三個小單元，是屬於傳統高等統計課程的範圍，其他單元在多數教科書被劃分為基礎統計的範圍。不過接下來的單元裡原作者都是混合一元迴歸與多元迴歸的示範案例，譯者將在屬於多元迴歸的小單元開頭明示譯註，提供使用這本電子書學習的學生與教學的老師，根據自身的學習目標決定如何運用該節內容。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#多元線性迴歸",
    "href": "10-Correlation-and-linear-regression.html#多元線性迴歸",
    "title": "10  相闗與線性迴歸",
    "section": "10.5 多元線性迴歸",
    "text": "10.5 多元線性迴歸\nThe simple linear regression model that we’ve discussed up to this point assumes that there’s a single predictor variable that you’re interested in, in this case dani.sleep. In fact, up to this point every statistical tool that we’ve talked about has assumed that your analysis uses one predictor variable and one outcome variable. However, in many (perhaps most) research projects you actually have multiple predictors that you want to examine. If so, it would be nice to be able to extend the linear regression framework to be able to include multiple predictors. Perhaps some kind of multiple regression model would be in order?\nMultiple regression is conceptually very simple. All we do is add more terms to our regression equation. Let’s suppose that we’ve got two variables that we’re interested in; perhaps we want to use both dani.sleep and baby.sleep to predict the dani.grump variable. As before, we let \\(Y_{i}\\) refer to my grumpiness on the i-th day. But now we have two $ X $ variables: the first corresponding to the amount of sleep I got and the second corresponding to the amount of sleep my son got. So we’ll let \\(X_{i1}\\) refer to the hours I slept on the i-th day and \\(X_{i2}\\) refers to the hours that the baby slept on that day. If so, then we can write our regression model like this:\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\nAs before, \\(\\epsilon_i\\) is the residual associated with the i-th observation, \\(\\epsilon_i = Y_i - \\hat{Y}_i\\). In this model, we now have three coefficients that need to be estimated: b0 is the intercept, b1 is the coefficient associated with my sleep, and b2 is the coefficient associated with my son’s sleep. However, although the number of coefficients that need to be estimated has changed, the basic idea of how the estimation works is unchanged: our estimated coefficients \\(\\hat{b}_0\\), \\(\\hat{b}_1\\) and \\(\\hat{b}_2\\) are those that minimise the sum squared residuals.\n\n10.5.1 jamovi實務示範\nMultiple regression in jamovi is no different to simple regression. All we have to do is add additional variables to the ‘Covariates’ box in jamovi. For example, if we want to use both dani.sleep and baby.sleep as predictors in our attempt to explain why I’m so grumpy, then move baby.sleep across into the ‘Covariates’ box alongside dani.sleep. By default, jamovi assumes that the model should include an intercept. The coefficients we get this time are shown in Table 14.4.\n\n\n\n\nTable 10.4:  增加預測變項迴歸係數的示 \n \n  \n    截距 \n    老爸睡眠小時數 \n    小嬰兒睡眠小時數 \n  \n \n\n  \n    125.97 \n    -8.95 \n    0.01 \n  \n\n\n\n\n\n\nThe coefficient associated with dani.sleep is quite large, suggesting that every hour of sleep I lose makes me a lot grumpier. However, the coefficient for baby.sleep is very small, suggesting that it doesn’t really matter how much sleep my son gets. What matters as far as my grumpiness goes is how much sleep I get. To get a sense of what this multiple regression model looks like, Figure 10.14 shows a 3D plot that plots all three variables, along with the regression model itself.\n\n\n\n\n\nFigure 10.14: 這張圖展示多元迴歸模型的三維立體視覺化。模型中有兩個預測變項，分別是 “dani.sleep” 和 “baby.sleep”，而目標變項是 “dani.grump”，這三個變項構成圖中的三維空間。每筆資料都是這個空間中的一個點。就像簡單線性迴歸模型在二維空間形成一條線一樣，此多元迴歸模型在三維空間形成一個平面。當我們估計迴歸係數時，我們能做的就是找到一個盡可能靠近所有資料點的平面。\n\n\n\n\n[Additional technical detail11]"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "href": "10-Correlation-and-linear-regression.html#量化迴歸模型的適配性",
    "title": "10  相關與線性迴歸",
    "section": "10.6 量化迴歸模型的適配性",
    "text": "10.6 量化迴歸模型的適配性\n所以我們現在知道如何估計線性迴歸模型的係數。問題是，我們還不知道這個迴歸模型是否有效。例如，regression.1 模型聲稱每小時的睡眠將大大改善我的情緒，但它可能只是廢話。請記住，迴歸模型僅生成關於我的心情的預測 \\(\\hat{Y}_i\\)，但我的實際心情是 \\(Y_i\\)。如果這兩者非常接近，那麼迴歸模型做得很好。如果它們差異很大，那麼它就做得不好。\n\n\n10.6.1 \\(R^2\\)\n再次讓我們用一些數學知識來解釋這個問題。首先，我們有殘差平方和\n\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\n我們希望這個值相當小。具體來說，我們希望它與結果變量的總變異性相比非常小\n\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\n既然我們已經談到這裡，讓我們自己計算這些值，不過不是用手算。讓我們使用類似於Excel或其他標準電子表格程序。我通過在Excel中打開parenthood.csv文件並將其另存為parenthood rsquared.xls來完成這個工作。首先要做的是計算 \\(\\hat{Y}\\) 值，對於僅使用單個預測變量的簡單模型，我們可以執行以下操作：\n\n使用公式’= 125.97 + (-8.94 \\(\\times\\) dani.sleep)’創建一個名為’Y.pred’的新列。\n通過創建一個名為’(Y-Y.pred)^2’的新列，使用公式’ = (dani.grump - Y.pred)^2 ’來計算SS(resid)。\n然後，在此列底部計算這些值的總和，即’ sum( ( Y-Y.pred)^2 ) ’。\n在dani.grump列的底部，計算dani.grump的平均值（注意Excel在其函數中使用’ AVERAGE ‘而不是’mean’）。\n然後創建一個名為’ (Y - mean(Y))^2 )‘的新列，使用公式’ = (dani.grump - AVERAGE(dani.grump))^2 ’。\n然後，在此列底部計算這些值的總和，即’sum( (Y - mean(Y))^2 )’。\n通過在一個空白單元格中輸入以下內容來計算R.squared：‘= 1 - (SS(resid) / SS(tot) )’。\n\n這給出了一個 \\(R^2\\) 值 ‘0.8161018’。\\(R^2\\) 值，有時被稱為決定係數12，有一個簡單的解釋：它是預測變量解釋結果變量方差的比例。因此，在這種情況下，我們得到的 \\(R^2 = .816\\) 意味著預測變量（my.sleep）解釋了結果變量（my.grump）的81.6％的變異。\n很自然，如果您想為迴歸模型獲得 \\(R^2\\) 值，實際上不需要自己在 Excel 中鍵入所有這些命令。稍後在 [在 jamovi 中執行假設檢驗] 部分中，我們將看到，您只需在 jamovi 中將其指定為選項即可。然而，讓我們暫時擱置這個問題。我想指出 \\(R^2\\) 的另一個性質。\n\n\n\n10.6.2 迴歸與相關的關聯\n此時，我們可以重新審視我之前的主張，即迴歸，在迄今為止我討論過的這種非常簡單的形式，基本上與相關是一回事。之前，我們用符號 \\(r\\) 表示皮爾森相關。皮爾森相關係數 \\(r\\) 和線性迴歸的 \\(R^2\\) 值之間可能存在某種關係嗎？當然有：平方相關 \\(r^2\\) 與具有單個預測變量的線性迴歸的 \\(R^2\\) 值相同。換句話說，執行皮爾森相關與執行僅使用一個預測變量的線性迴歸模型基本相同。\n\n\n\n10.6.3 校正後 \\(R^2\\)\n在繼續之前，我要指出的最後一件事是，人們通常會報告一個稱為“校正後 \\(R^2\\) ”的模型性能稍有不同的度量。計算校正後 \\(R^2\\) 值的動機是觀察到將更多預測變量添加到模型中總是會使 \\(R^2\\) 值增加（或至少不降低）。\n[額外技術細節13]\n這種調整是為了考慮自由度。調整後 \\(R^2\\) 值的最大優點是，當您向模型中添加更多預測變量時，調整後的 \\(R^2\\) 值只會在新變量提高模型性能超過您所預期的概率時增加。最大的缺點是調整後的 \\(R^2\\) 值無法像 \\(R^2\\) 那樣解釋。\\(R^2\\) 可以簡單地解釋為迴歸模型解釋結果變量變異的比例。據我所知，調整後的 \\(R^2\\) 沒有等效的解釋。\n那麼一個明顯的問題是您應該報告 \\(R^2\\) 還是調整後的 \\(R^2\\)。這可能是個人喜好的問題。如果您更關心可解釋性，那麼 \\(R^2\\) 較好。如果您更關心糾正偏差，那麼調整後的 \\(R^2\\) 可能更好。僅僅為我自己而言，我更喜歡 \\(R^2\\)。我覺得更重要的是能夠解釋模型性能的度量。此外，正如我們將在 Section 10.8 迴歸模型的假設檢驗 中看到的那樣，如果您擔心通過添加預測變量所獲得的 \\(R^2\\) 改進僅僅是由於機遇而非因為更好的模型，那麼我們有假設檢驗來解決這個問題。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "href": "10-Correlation-and-linear-regression.html#迴歸模型的假設檢定",
    "title": "10  相闗與線性迴歸",
    "section": "10.7 迴歸模型的假設檢定",
    "text": "10.7 迴歸模型的假設檢定\nSo far we’ve talked about what a regression model is, how the coefficients of a regression model are estimated, and how we quantify the performance of the model (the last of these, incidentally, is basically our measure of effect size). The next thing we need to talk about is hypothesis tests. There are two different (but related) kinds of hypothesis tests that we need to talk about: those in which we test whether the regression model as a whole is performing significantly better than a null model, and those in which we test whether a particular regression coefficient is significantly different from zero.\n\n10.7.1 Testing the model as a whole\nOkay, suppose you’ve estimated your regression model. The first hypothesis test you might try is the null hypothesis that there is no relationship between the predictors and the outcome, and the alternative hypothesis that the data are distributed in exactly the way that the regression model predicts.\n[Additional technical detail14]\nWe’ll see much more of the F statistic in Chapter 12, but for now just know that we can interpret large F values as indicating that the null hypothesis is performing poorly in comparison to the alternative hypothesis. In a moment I’ll show you how to do the test in jamovi the easy way, but first let’s have a look at the tests for the individual regression coefficients.\n\n\n10.7.2 Tests for individual coefficients\nThe F-test that we’ve just introduced is useful for checking that the model as a whole is performing better than chance. If your regression model doesn’t produce a significant result for the F-test then you probably don’t have a very good regression model (or, quite possibly, you don’t have very good data). However, while failing this test is a pretty strong indicator that the model has problems, passing the test (i.e., rejecting the null) doesn’t imply that the model is good! Why is that, you might be wondering? The answer to that can be found by looking at the coefficients for the [Multiple linear regression] model we have already looked at (Table 14.4)\nI can’t help but notice that the estimated regression coefficient for the baby.sleep variable is tiny (\\(0.01\\)), relative to the value that we get for dani.sleep (\\(-8.95\\)). Given that these two variables are absolutely on the same scale (they’re both measured in “hours slept”), I find this illuminating. In fact, I’m beginning to suspect that it’s really only the amount of sleep that I get that matters in order to predict my grumpiness. We can re-use a hypothesis test that we discussed earlier, the t-test. The test that we’re interested in has a null hypothesis that the true regression coefficient is zero (\\(b = 0\\)), which is to be tested against the alternative hypothesis that it isn’t (\\(b \\neq 0\\)). That is:\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\nHow can we test this? Well, if the central limit theorem is kind to us we might be able to guess that the sampling distribution of \\(\\hat{b}\\), the estimated regression coefficient, is a normal distribution with mean centred on \\(b\\). What that would mean is that if the null hypothesis were true, then the sampling distribution of \\(\\hat{b}\\) has mean zero and unknown standard deviation. Assuming that we can come up with a good estimate for the standard error of the regression coefficient, \\(se(\\hat{b})\\), then we’re in luck. That’s exactly the situation for which we introduced the one-sample t-test back in Chapter 11. So let’s define a t-statistic like this\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\nI’ll skip over the reasons why, but our degrees of freedom in this case are \\(df = N - K - 1\\). Irritatingly, the estimate of the standard error of the regression coefficient, \\(se(\\hat{b})\\), is not as easy to calculate as the standard error of the mean that we used for the simpler t-tests in Chapter 11. In fact, the formula is somewhat ugly, and not terribly helpful to look at.15 For our purposes it’s sufficient to point out that the standard error of the estimated regression coefficient depends on both the predictor and outcome variables, and it is somewhat sensitive to violations of the homogeneity of variance assumption (discussed shortly).\nIn any case, this t-statistic can be interpreted in the same way as the t-statistics that we discussed in Chapter 11. Assuming that you have a two-sided alternative (i.e., you don’t really care if b \\(>\\) 0 or b \\(<\\) 0), then it’s the extreme values of t (i.e., a lot less than zero or a lot greater than zero) that suggest that you should reject the null hypothesis.\n\n\n10.7.3 Running the hypothesis tests in jamovi\nTo compute all of the statistics that we have talked about so far, all you need to do is make sure the relevant options are checked in jamovi and then run the regression. If we do that, as in Figure 10.15, we get a whole bunch of useful output.\n\n\n\n\n\nFigure 10.15: A jamovi screenshot showing a multiple linear regression analysis, with some useful options checked\n\n\n\n\nThe ‘Model Coefficients’ at the bottom of the jamovi analysis results shown in Figure 10.15 provides the coefficients of the regression model. Each row in this table refers to one of the coefficients in the regression model. The first row is the intercept term, and the later ones look at each of the predictors. The columns give you all of the relevant information. The first column is the actual estimate of \\(b\\) (e.g., \\(125.97\\) for the intercept, and -8.95 for the dani.sleep predictor). The second column is the standard error estimate \\(\\hat{\\sigma}_b\\). The third and fourth columns provide the lower and upper values for the 95% confidence interval around the b estimate (more on this later). The fifth column gives you the t-statistic, and it’s worth noticing that in this table \\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\) every time. Finally, the last column gives you the actual p-value for each of these tests.16\nThe only thing that the coefficients table itself doesn’t list is the degrees of freedom used in the t-test, which is always \\(N - K - 1\\) and is listed in the table at the top of the output, labelled ‘Model Fit Measures’. We can see from this table that the model performs significantly better than you’d expect by chance (\\(F(2,97) = 215.24, p< .001\\)), which isn’t all that surprising: the \\(R^2 = .81\\) value indicate that the regression model accounts for \\(81\\%\\) of the variability in the outcome measure (and \\(82\\%\\) for the adjusted \\(R^2\\) ). However, when we look back up at the t-tests for each of the individual coefficients, we have pretty strong evidence that the baby.sleep variable has no significant effect. All the work in this model is being done by the dani.sleep variable. Taken together, these results suggest that this regression model is actually the wrong model for the data. You’d probably be better off dropping the baby.sleep predictor entirely. In other words, the simple regression model that we started with is the better model."
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "href": "10-Correlation-and-linear-regression.html#迴歸係數的更多資訊",
    "title": "10  相關與線性迴歸",
    "section": "10.8 迴歸係數的更多資訊",
    "text": "10.8 迴歸係數的更多資訊\n在討論線性迴歸背後的假設以及如何檢查它們是否得到滿足之前，我想簡要討論兩個主題，它們都與迴歸係數有關。首先要談論的是如何計算係數的信賴區間。然後，我將討論如何確定哪個預測因子最重要的問題。\n\n10.8.1 迴歸係數的信賴區間\n像任何人口參數一樣，迴歸係數b無法從樣本數據中完全精確地估算出來；這就是為什麼我們需要假設檢定的一部分原因。有鑑於此，能夠報告能捕捉我們對\\(b\\)真實值不確定性的信賴區間是非常有用的。這在研究問題主要集中在嘗試找出變項\\(X\\)與變項\\(Y\\)之間的關係強度時尤其有用，因為在這些情況下，主要關注的是迴歸權重\\(b\\)。\n[額外技術細節17]\n在jamovi中，我們已經指定了“95％信賴區間”，如@fig-fig10-15所示，儘管我們本可以輕鬆選擇另一個值，例如“99％信賴區間”，如果我們決定這樣做的話。\n\n\n10.8.2 標準化迴歸係數的計算方法\n您可能還想做的另一件事是計算“標準化”迴歸係數，通常用\\(\\beta\\)表示。標準化係數背後的基本原理是：在很多情況下，您的變項基本上具有不同的尺度。假設，例如，我的迴歸模型旨在使用受教育程度（受教育年數）和收入作為預測因子來預測人們的智商得分。顯然，受教育程度和收入不在相同的尺度上。教育年限可能只有10多年，而收入可能差距高達10,000美元（或更多）。度量單位對迴歸係數有很大影響。只有在考慮到預測變項和結果變項的單位時，b係數才具有意義。這使得比較不同預測因子的係數變得非常困難。然而，有時您確實希望在不同係數之間進行比較。具體來說，您可能希望找到哪些預測因子與結果之間的關係最強烈的標準衡量。這就是標準化係數要做的事。\n基本思路很簡單；如果您在運行迴歸之前將所有變項轉換為z分數，標準化係數就是您會得到的係數。18這裡的想法是，通過將所有預測因子轉換為z分數，它們在迴歸中具有相同的比例，從而消除了變項在不同尺度上的問題。無論原始變項是什麼，\\(\\beta\\)值為1都意味著預測因子的1個標準差增加將導致結果變項相應的1個標準差增加。因此，如果變項A的\\(\\beta\\)絕對值大於變項B，則被認為與結果的關係更強烈。至少這是這個想法。值得小心的是，這確實非常依賴於“1個標準差變化”對於所有變項來說都是基本相同的這一假設。這並不總是顯而易見的。\n[額外技術細節19]\n為了使事情變得更簡單，jamovi具有使用”Model Coefficients”選項中的”Standardized estimate”復選框為您計算\\(\\beta\\)係數的選項，如@fig-fig10-16中的結果所示。\n\n\n\n\n\nFigure 10.16: 多元線性迴歸的標準化係數及其95％置信區間\n\n\n\n\n這些結果顯然表明，dani.sleep變項比baby.sleep變項具有更強的影響力。然而，這正是一個可能更適合使用原始係數b而不是標準化係數\\(\\beta\\)的完美例子。畢竟，我的睡眠和寶寶的睡眠已經在同一個比例上：睡眠時間。為什麼要通過將它們轉換為z分數來使事情變得更複雜呢？"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "href": "10-Correlation-and-linear-regression.html#迴歸模型的適用條件",
    "title": "10  相關與線性迴歸",
    "section": "10.9 迴歸模型的適用條件",
    "text": "10.9 迴歸模型的適用條件\n我一直在討論的線性迴歸模型依賴於幾個假設。在 Section 10.10 診斷迴歸模型的適用條件這個單元，我們將會學習如何檢查這些假設是否得到滿足，首先簡單說明每個假設的涵意。20\n\n線性。線性迴歸模型的一個相當基本的假設是\\(X\\)和\\(Y\\)之間的關係確實是線性的！無論是簡單迴歸還是多元迴歸，我們都假設涉及的關係是線性的。\n獨立性：殘差彼此獨立。這實際上只是一個“總括一切”的假設，即“在殘差中沒有其他有趣的東西”。如果有一些奇怪的事情發生（例如，所有殘差都嚴重依賴於某些其他未測量變項），可能會破壞事物。\n常態性。像統計中的許多模型一樣，基本的簡單或多元線性迴歸依賴於常態性假設。具體來說，它假設殘差呈常態分佈。實際上，如果預測因子\\(X\\)和結果\\(Y\\)是非常態的，只要殘差\\(\\epsilon\\)是常態的就可以了。參見[檢查殘差的常態性]部分。\n變異相等（或稱’同質性’）。嚴格來說，迴歸模型假設每個殘差\\(\\epsilon_i\\)都是從一個平均值為0的常態分佈中生成的，並且（對於當前目的更重要的是）具有標準差\\(\\sigma\\)，這對於每個殘差都是相同的。實際上，檢驗每個殘差都是相同分佈的假設是不可能的。相反，我們關心的是殘差的標準差對於\\(\\hat{Y}\\)的所有值以及（如果我們特別懷疑的話）模型中每個預測因子\\(X\\)的所有值都是相同的。\n\n因此，我們有四個主要的線性迴歸假設（剛好可以縮寫成LINE）。此外，還有一些其他事項需要檢查：\n\n預測因子無相關。這裡的想法是，在多元迴歸模型中，您不希望預測因子彼此間的相關性過強。這並非迴歸模型的“技術性”假設，但在實際操作中是必需的。預測因子之間相關性過強（稱為“共線性”）可能會在評估模型時導致問題。參見 Section 10.10.4 檢查共線性。\n沒有“不良”離群值。同樣，這實際上並非模型的技術假設（或者說，這是由所有其他假設暗示的），但是有一個隱含的假設，即您的迴歸模型不會因為一兩個異常數據點而受到過於強烈的影響，因為這在某些情況下會引起對模型的適當性和數據可靠性的質疑。參見 Section 10.10.2 三種異常資料。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-Model-checking",
    "href": "10-Correlation-and-linear-regression.html#sec-Model-checking",
    "title": "10  相關與線性迴歸",
    "section": "10.11 診斷迴歸模型的適用條件",
    "text": "10.11 診斷迴歸模型的適用條件\n本節的主要焦點是迴歸診斷，這個術語是指檢查迴歸模型假設是否得到滿足、在假設被違反時如何修正模型以及一般情況下檢查是否存在不尋常情況的技術。我將這稱為模型檢查的“藝術”，理由很充分。這並不容易，儘管有許多相當標準化的工具可以用來診斷甚至可能治愈困擾模型的問題（如果存在的話！），但在這方面真的需要運用一定程度的判斷力。在檢查這件事情或那件事情的所有細節中容易迷失，試圖記住所有不同的事物是相當耗費精力的。這會產生一個非常令人討厭的副作用，很多人在試圖學習所有工具時會感到沮喪，所以他們決定不做任何模型檢查。這有點令人擔憂！\n在本節中，我描述了一些方法，用於檢查迴歸模型是否按照預期工作。它並沒有涵蓋所有您可能做的事情，但仍然比我在實踐中看到的大多數人所做的事情要詳細得多，即使在我的初級統計課程中，我通常也不會涵蓋所有這些內容。但是，我確實認為您應該了解可供您使用的工具，所以我將在這裡嘗試介紹一部分。最後，我應該指出，本節很大程度上借鑒了 Fox & Weisberg (2011) ，即與在 R 中進行迴歸分析的“car”包相關的書籍。 “car”包以提供一些出色的迴歸診斷工具而著稱，而該書本身以極為清晰的方式談論了這些工具。我不想聽起來太過於誇大，但我確實認為即使在 R 和不是 jamovi 的情況下， Fox & Weisberg (2011) 都值得一讀。\n\n10.11.1 三種殘差\n大多數迴歸診斷都圍繞著觀察殘差，到目前為止，你可能已經對統計學形成了足夠悲觀的理論，能夠猜到，正因為我們非常關心殘差，我們可能會考慮幾種不同類型的殘差。特別地，在本節中，我們將提到以下三種殘差：“普通殘差”、“標準化殘差”和“學生化殘差”。還有第四種你會在一些圖中看到的，稱為“皮爾森殘差”。然而，對於我們在本章中討論的模型，皮爾森殘差與普通殘差相同。\n首先，我們關心的最簡單類型的殘差是普通殘差。這些就是我在本章前面一直提到的實際原始殘差。普通殘差僅僅是擬合值 \\(\\hat{Y}_i\\) 和觀察值 \\(Y_i\\) 之間的差。我一直用符號 \\(\\epsilon_i\\) 表示第 i 個普通殘差，並且我將繼續堅持使用它。考慮到這一點，我們有非常簡單的方程式\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\n這當然是我們之前看到的，除非我特別提到其他類型的殘差，否則我就是在談論這個。所以這裡沒有新的東西。我只是想重申一下。使用普通殘差的一個缺點是，它們總是在不同的尺度上，取決於結果變量是什麼以及迴歸模型有多好。也就是說，除非你決定在沒有截距項的情況下運行迴歸模型，否則普通殘差的均值將為 0，但每個迴歸的方差都不同。在很多情境下，特別是當你只對殘差的模式感興趣，而不是它們的實際值時，估計標準化殘差很方便，這些殘差經過規範化後標準差為 1。\n[額外技術細節20]\n第三種殘差是學生化殘差（也稱為 “剃刀切割殘差”），它們比標準化殘差更高級。同樣，目的是將普通殘差除以某個量，以估計殘差的某種標準化概念。21\n在繼續之前，我應該指出，即使這些殘差是幾乎所有迴歸診斷的核心，你通常也不需要自己獲得這些殘差。大多數時候，提供診斷或假設檢查的各種選項將為您處理這些計算。即使如此，知道如何實際自己獲得這些東西，以防你需要進行一些非標準操作，總是很好的。\n\n\n\n\n\n10.11.2 檢測殘差常態性\nLike many of the statistical tools we’ve discussed in this book, regression models rely on a normality assumption. In this case, we assume that the residuals are normally distributed. The first thing we can do is draw a QQ-plot via the ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ option. The output is shown in Figure 10.17, showing the standardised residuals plotted as a function of their theoretical quantiles according to the regression model.\n\n\n\n\n\nFigure 10.17: Plot of the theoretical quantiles according to the model, against the quantiles of the standardised residuals, produced in jamovi\n\n\n\n\nAnother thing we should check is the relationship between the fitted values and the residuals themselves. We can get jamovi to do this using the ‘Residuals Plots’ option, which provides a scatterplot for each predictor variable, the outcome variable, and the fitted values against residuals, see Figure 10.18. In these plots we are looking for a fairly uniform distribution of ‘dots’, with no clear bunching or patterning of the ‘dots’. Looking at these plots, there is nothing particularly worrying as the dots are fairly evenly spread across the whole plot. There may be a little bit of non-uniformity in plot (b), but it is not a strong deviation and probably not worth worrying about.\n\n\n\n\n\nFigure 10.18: Residuals plots produced in jamovi\n\n\n\n\nIf we were worried, then in a lot of cases the solution to this problem (and many others) is to transform one or more of the variables. We discussed the basics of variable transformation in Section 6.3, but I do want to make special note of one additional possibility that I didn’t explain fully earlier: the Box-Cox transform. The Box-Cox function is a fairly simple one and it’s very widely used. 22\nYou can calculate it using the BOXCOX function in the ‘Compute’ variables screen in jamovi.\n\n\n10.11.3 檢測共線性\nThe last kind of regression diagnostic that I’m going to discuss in this chapter is the use of variance inflation factors (VIFs), which are useful for determining whether or not the predictors in your regression model are too highly correlated with each other. There is a variance inflation factor associated with each predictor \\(X_k\\) in the model. 23\nThe square root of the VIF is pretty interpretable. It tells you how much wider the confidence interval for the corresponding coefficient bk is, relative to what you would have expected if the predictors are all nice and uncorrelated with one another. If you’ve only got two predictors, the VIF values are always going to be the same, as we can see if we click on the ‘Collinearity’ checkbox in the ‘Regression’ - ‘Assumptions’ options in jamovi. For both dani.sleep and baby.sleep the VIF is \\(1.65\\). And since the square root of \\(1.65\\) is \\(1.28\\), we see that the correlation between our two predictors isn’t causing much of a problem.\nTo give a sense of how we could end up with a model that has bigger collinearity problems, suppose I were to run a much less interesting regression model, in which I tried to predict the day on which the data were collected, as a function of all the other variables in the data set. To see why this would be a bit of a problem, let’s have a look at the correlation matrix for all four variables (Figure 10.19).\n\n\n\n\n\nFigure 10.19: 四個資料變項之間的相關係數矩陣\n\n\n\n\nWe have some fairly large correlations between some of our predictor variables! When we run the regression model and look at the VIF values, we see that the collinearity is causing a lot of uncertainty about the coefficients. First, run the regression, as in Figure 10.20 and you can see from the VIF values that, yep, that’s some mighty fine collinearity there.\n\n\n\n\n\nFigure 10.20: Collinearity statistics for multiple regression, produced in jamovi"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "href": "10-Correlation-and-linear-regression.html#決定線性模型的變項組合",
    "title": "10  相闗與線性迴歸",
    "section": "10.11 決定線性模型的變項組合",
    "text": "10.11 決定線性模型的變項組合\nOne fairly major problem that remains is the problem of “model selection”. That is, if we have a data set that contains several variables, which ones should we include as predictors, and which ones should we not include? In other words, we have a problem of variable selection. In general, model selection is a complex business but it’s made somewhat simpler if we restrict ourselves to the problem of choosing a subset of the variables that ought to be included in the model. Nevertheless, I’m not going to try covering even this reduced topic in a lot of detail. Instead, I’ll talk about two broad principles that you need to think about, and then discuss one concrete tool that jamovi provides to help you select a subset of variables to include in your model. First, the two principles:\n\nIt’s nice to have an actual substantive basis for your choices. That is, in a lot of situations you the researcher have good reasons to pick out a smallish number of possible regression models that are of theoretical interest. These models will have a sensible interpretation in the context of your field. Never discount the importance of this. Statistics serves the scientific process, not the other way around.\nTo the extent that your choices rely on statistical inference, there is a trade off between simplicity and goodness of fit. As you add more predictors to the model you make it more complex. Each predictor adds a new free parameter (i.e., a new regression coefficient), and each new parameter increases the model’s capacity to “absorb” random variations. So the goodness of fit (e.g., \\(R^2\\) ) continues to rise, sometimes trivially or by chance, as you add more predictors no matter what. If you want your model to be able to generalise well to new observations you need to avoid throwing in too many variables.\n\nThis latter principle is often referred to as Occam’s razor and is often summarised in terms of the following pithy saying: do not multiply entities beyond necessity. In this context, it means don’t chuck in a bunch of largely irrelevant predictors just to boost your R2 . Hmm. Yeah, the original was better.\nIn any case, what we need is an actual mathematical criterion that will implement the qualitative principle behind Occam’s razor in the context of selecting a regression model. As it turns out there are several possibilities. The one that I’ll talk about is the Akaike information criterion (Akaike, 1974) simply because it’s available as an option in jamovi. 27\nThe smaller the AIC value, the better the model performance. If we ignore the low level details it’s fairly obvious what the AIC does. On the left we have a term that increases as the model predictions get worse; on the right we have a term that increases as the model complexity increases. The best model is the one that fits the data well (low residuals, left hand side) using as few predictors as possible (low K, right hand side). In short, this is a simple implementation of Ockham’s razor.\nAIC can be added to the ‘Model Fit Measures’ output Table when the ‘AIC’ checkbox is clicked, and a rather clunky way of assessing different models is seeing if the ‘AIC’ value is lower if you remove one or more of the predictors in the regression model. This is the only way currently implemented in jamovi, but there are alternatives in other more powerful programmes, such as R. These alternative methods can automate the process of selectively removing (or adding) predictor variables to find the best AIC. Although these methods are not implemented in jamovi, I will mention them briefly below just so you know about them.\n\n10.11.1 逐步排除法\nIn backward elimination you start with the complete regression model, including all possible predictors. Then, at each “step” we try all possible ways of removing one of the variables, and whichever of these is best (in terms of lowest AIC value) is accepted. This becomes our new regression model, and we then try all possible deletions from the new model, again choosing the option with lowest AIC. This process continues until we end up with a model that has a lower AIC value than any of the other possible models that you could produce by deleting one of its predictors.\n\n\n10.11.2 逐步納入法\nAs an alternative, you can also try forward selection. This time around we start with the smallest possible model as our start point, and only consider the possible additions to the model. However, there’s one complication. You also need to specify what the largest possible model you’re willing to entertain is.\nAlthough backward and forward selection can lead to the same conclusion, they don’t always.\n\n\n10.11.3 使用警告\nAutomated variable selection methods are seductive things, especially when they’re bundled up in (fairly) simple functions in powerful statistical programmes. They provide an element of objectivity to your model selection, and that’s kind of nice. Unfortunately, they’re sometimes used as an excuse for thoughtlessness. No longer do you have to think carefully about which predictors to add to the model and what the theoretical basis for their inclusion might be. Everything is solved by the magic of AIC. And if we start throwing around phrases like Ockham’s razor, well it sounds like everything is wrapped up in a nice neat little package that no-one can argue with.\nOr, perhaps not. Firstly, there’s very little agreement on what counts as an appropriate model selection criterion. When I was taught backward elimination as an undergraduate, we used F-tests to do it, because that was the default method used by the software. I’ve described using AIC, and since this is an introductory text that’s the only method I’ve described, but the AIC is hardly the Word of the Gods of Statistics. It’s an approximation, derived under certain assumptions, and it’s guaranteed to work only for large samples when those assumptions are met. Alter those assumptions and you get a different criterion, like the BIC for instance (also available in jamovi). Take a different approach again and you get the NML criterion. Decide that you’re a Bayesian and you get model selection based on posterior odds ratios. Then there are a bunch of regression specific tools that I haven’t mentioned. And so on. All of these different methods have strengths and weaknesses, and some are easier to calculate than others (AIC is probably the easiest of the lot, which might account for its popularity). Almost all of them produce the same answers when the answer is “obvious” but there’s a fair amount of disagreement when the model selection problem becomes hard.\nWhat does this mean in practice? Well, you could go and spend several years teaching yourself the theory of model selection, learning all the ins and outs of it so that you could finally decide on what you personally think the right thing to do is. Speaking as someone who actually did that, I wouldn’t recommend it. You’ll probably come out the other side even more confused than when you started. A better strategy is to show a bit of common sense. If you’re staring at the results of an automated backwards or forwards selection procedure, and the model that makes sense is close to having the smallest AIC but is narrowly defeated by a model that doesn’t make any sense, then trust your instincts. Statistical model selection is an inexact tool, and as I said at the beginning, interpretability matters.\n\n\n10.11.4 比較迴歸模型\nAn alternative to using automated model selection procedures is for the researcher to explicitly select two or more regression models to compare to each other. You can do this in a few different ways, depending on what research question you’re trying to answer. Suppose we want to know whether or not the amount of sleep that my son got has any relationship to my grumpiness, over and above what we might expect from the amount of sleep that I got. We also want to make sure that the day on which we took the measurement has no influence on the relationship. That is, we’re interested in the relationship between baby.sleep and dani.grump, and from that perspective dani.sleep and day are nuisance variable or covariates that we want to control for. In this situation, what we would like to know is whether dani.grump ~ dani.sleep + day + baby .sleep (which I’ll call Model 2, or M2) is a better regression model for these data than dani.grump ~ dani.sleep + day (which I’ll call Model 1, or M1). There are two different ways we can compare these two models, one based on a model selection criterion like AIC, and the other based on an explicit hypothesis test. I’ll show you the AIC based approach first because it’s simpler, and follows naturally from discussion in the last section. The first thing I need to do is actually run the two regressions, note the AIC for each one, and then select the model with the smaller AIC value as it is judged to be the better model for these data. Actually, don’t do this just yet. Read on because there is an easy way in jamovi to get the AIC values for different models included in one table.28\nA somewhat different approach to the problem comes out of the hypothesis testing framework. Suppose you have two regression models, where one of them (Model 1) contains a subset of the predictors from the other one (Model 2). That is, Model 2 contains all of the predictors included in Model 1, plus one or more additional predictors. When this happens we say that Model 1 is nested within Model 2, or possibly that Model 1 is a submodel of Model 2. Regardless of the terminology, what this means is that we can think of Model 1 as a null hypothesis and Model 2 as an alternative hypothesis. And in fact we can construct an F test for this in a fairly straightforward fashion. 29\nOkay, so that’s the hypothesis test that we use to compare two regression models to one another. Now, how do we do it in jamovi? The answer is to use the ‘Model Builder’ option and specify the Model 1 predictors dani.sleep and day in ‘Block 1’ and then add the additional predictor from Model 2 (baby.sleep) in ‘Block 2’, as in Figure 10.25. This shows, in the ‘Model Comparisons’ Table, that for the comparisons between Model 1 and Model 2, \\(F(1,96) = 0.00\\), \\(p = 0.954\\). Since we have p > .05 we retain the null hypothesis (M1). This approach to regression, in which we add all of our covariates into a null model, then add the variables of interest into an alternative model, and then compare the two models in a hypothesis testing framework, is often referred to as hierarchical regression.\nWe can also use this ‘Model Comparison’ option to display a table that shows the AIC and BIC for each model, making it easy to compare and identify which model has the lowest value, as in Figure 10.25.\n\n\n\n\n\nFigure 10.25: Model comparison in jamovi using the ‘Model Builder’ option"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#本章小結",
    "href": "10-Correlation-and-linear-regression.html#本章小結",
    "title": "10  相關與線性迴歸",
    "section": "10.12 本章小結",
    "text": "10.12 本章小結\n\n想了解兩個變項之間的關聯性有多強？就計算相關係數\n散佈圖繪製方法\n前進下一章前必學的課題：什麼是線性迴歸模型 以及使用線性迴歸模型估計參數\n多元線性迴歸\n量化迴歸模型的適配性 要了解 \\(R^2\\) 。\n迴歸模型的假設檢定\n在迴歸係數的更多資訊 這一節，我們學習如何計算迴歸係數的信賴區間以及標準化迴歸係數的計算方法\n迴歸模型的適用條件 以及診斷適用條件\n決定線性模型的變項組合\n\n\n\n\n\n\nAkaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American Statistician, 27, 17–21.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to applied regression (2nd ed.). Sage."
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#本章示範資料",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.1 本章示範資料",
    "text": "12.1 本章示範資料\n假設您已參與一個臨床試驗，該試驗正在測試一種名為Joyzepam的新抗抑鬱藥。為了公平地測試藥物的有效性，該研究涉及三種獨立的藥物。其中一種是安慰劑，另一種是現有的抗抑鬱/抗焦慮藥物，名為Anxifree。您的初步測試招募了18名患有中度至重度抑鬱症的參與者。由於藥物有時與心理治療結合使用，因此您的研究包括9個正在進行認知行為治療（CBT）的人和9個未進行治療的人。參與者被隨機分配（當然是雙盲的）治療，使得3個接受CBT的人和3個無治療的人被分配到3種藥物中的每一種。心理學家在每個人使用每種藥物3個月後評估每個人的情緒，並在從\\(-5\\)到\\(+5\\)的範圍內評估每個人情緒的整體改善。在這種研究設計下，讓我們現在加載數據文件 clinicaltrial.csv。我們可以看到這個數據集包含了三個變量，分別是藥物、治療和情緒提升。\n在本章中，我們真正感興趣的是藥物對情緒提升的影響。首先要做的是計算一些描述性統計數據並繪製一些圖表。在@sec-Descriptive-statistics章節中，我們向您展示了如何做到這一點，jamovi中可以計算的一些描述性統計數據顯示在 Figure 12.1 中。\n\n\n\n\n\n\nFigure 12.1: 情緒提升的描述性統計數據，以及按給予的藥物繪製的盒形圖。\n\n\n\n\n如圖所示，對於Joyzepam組的參與者，情緒的改善比Anxifree組或安慰劑組要大。Anxifree組的情緒提升比對照組要大，但差距不是那麼大。我們想要回答的問題是，這些差異是否“真實”，或者僅僅是因為偶然？"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#sec-How-ANOVA-works",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.2 變異數分析的運作原理",
    "text": "12.2 變異數分析的運作原理\n為了回答我們的臨床試驗數據所提出的問題，我們將進行單因素變異數分析（one-way ANOVA）。首先，我將通過自下而上地構建統計工具並向您展示，如果您無法使用jamovi中的任何酷炫內置ANOVA功能，該如何做。我希望您能仔細閱讀，嘗試一兩次用較長的方法來確保您真正了解ANOVA是如何運作的，然後一旦您掌握了概念，就永遠不要再用這種方法了。\n在上一節中我描述的實驗設計強烈表明，我們對比較三種不同藥物的平均心情變化感興趣。在這個意義上，我們討論的分析類似於t檢驗（參見 Chapter 11 ），但涉及多於兩個組別。如果我們讓\\(\\mu_P\\)表示安慰劑引起的情緒變化的母體平均值，並讓\\(\\mu_A\\)和\\(\\mu_J\\)表示我們的兩種藥物Anxifree和Joyzepam的對應平均值，那麼我們要檢驗的（有些悲觀的）虛無假設是：所有三個母體平均值都相同。也就是說，這兩種藥物都沒有比安慰劑更有效。我們可以將此虛無假設寫為：\n\\[H_0: \\text{ 事實上 } \\mu_P=\\mu_A=\\mu_J\\]\n因此，我們的替代假設是：三種不同治療中至少有一種與其他治療不同。將其用數學表示有點困難，因為（正如我們將要討論的那樣）虛無假設可能以很多不同的方式是錯誤的。所以目前我們只將替代假設寫成這樣：\n\\[H_1: \\text{ 事實 }\\underline{ 不是 }\\text{  } \\mu_P=\\mu_A=\\mu_J\\]\n這個虛無假設比我們之前見過的任何一個都要棘手得多。我們應該如何檢驗它？一個明智的猜測是「進行方差分析」，因為這是本章的標題，但是目前還不太清楚為什麼「方差分析」會幫助我們了解有關均值的有用信息。事實上，這是人們首次接觸方差分析時遇到的最大概念困難之一。要了解其原理，我認為從方差開始談起是最有幫助的，具體來說就是組間變異性和組內變異性（ Figure 12.2 ）。\n\n\n\n\n\n\nFigure 12.2: 圖形說明 ‘組間’ 變異 (面板 (a)) 和 ‘組內’ 變異 (面板 (b))。在左側，箭頭顯示組平均值之間的差異。在右側，箭頭強調每個組內的變異性。\n\n\n\n\n\n12.2.1 計算依變項變異數的兩套公式\n首先，讓我們引入一些符號。我們將使用 G 來表示組的總數。對於我們的數據集，有三種藥物，所以有 \\(G = 3\\) 個組。接下來，我們將使用 \\(N\\) 表示總樣本大小；在我們的數據集中，一共有 \\(N = 18\\) 人。同樣地，讓我們用 \\(N_k\\) 表示第 k 個組中的人數。在我們的虛擬臨床試驗中，所有三組的樣本大小都是 \\(N_k = 6\\)。1 最後，我們將使用 Y 表示結果變項。在我們的案例中，Y 指的是心情變化。具體來說，我們將使用 Yik 指代第 k 個組中第 i 個成員所經歷的心情變化。同樣，我們將使用 \\(\\bar{Y}\\) 作為實驗中所有 18 人的平均心情變化，並使用 \\(\\bar{Y}_k\\) 指代第 k 組中 6 人所經歷的平均心情變化。\n現在我們已經整理好符號，我們可以開始寫下公式。首先，讓我們回想一下在 Section 4.2 中使用的方差公式，在那個做描述性統計的較早時期。Y 的樣本方差被定義為以下公式 \\[Var(Y)=\\frac{1}{N}\\sum_{k=1}^{G}\\sum_{i=1}^{N_k}(Y_{ik}-\\bar{Y})^2\\] 這個公式看起來與 Section 4.2 中的方差公式幾乎相同。唯一的區別是這次我有兩個求和：我對組進行求和（即 \\(k\\) 的值）以及對組內的人進行求和（即 \\(i\\) 的值）。這只是一個純粹的表面細節。如果我使用符號 \\(Y_p\\) 來表示樣本中第 p 個人的結果變項值，那麼我只有一個求和。我們在這裡有兩個求和的唯一原因是我將人分類到組，然後為組內的人分配數字。\n在這裡，具體的例子可能很有用。讓我們考慮 Table 12.1 ，在這個表格中，我們有總共 \\(N = 5\\) 個人分成 \\(G = 2\\) 個組。任意地說，讓我們說「酷」的人是第 1 組，「不酷」的人是第 2 組。結果發現我們有三個酷人（\\(N_1 = 3\\)）和兩個不酷的人（\\(N_2 = 2\\)）。\n\n\n\n\n\nTable 12.1:  在酷和不酷的團體中的脾氣。 \n\nnameperson Pgroupgroup num. kindex in groupgrumpiness \\( Y_{ik} \\) or \\( Y_p \\)\n\nAnn1cool1120\n\nBen2cool1255\n\nCat3cool1321\n\nTim4uncool2191\n\nEgg5uncool2222\n\n\n\n\n\n注意到這裡我構建了兩個不同的標記方案。我們有一個「人」變項 p，所以說到 Yp 作為樣本中的第 p 人的脾氣是完全合理的。例如，表格顯示 Tim 是第四個，所以我們會說 \\(p = 4\\)。所以，在談到這個「Tim」這個人（無論他是誰）的脾氣 \\(Y\\) 時，我們可以通過說 \\(Y_p = 91\\) 來指稱他的脾氣，即對於人 \\(p = 4\\)。然而，這不是我們唯一可以指稱 Tim 的方法。作為一個替代方法，我們可以注意到 Tim 屬於「不酷」的組（\\(k = 2\\)），實際上是不酷組中列出的第一個人（\\(i = 1\\)）。所以，通過說 \\(Y_{ik} = 91\\)，在 \\(k = 2\\) 和 \\(i = 1\\) 的情況下，同樣有效地指稱 Tim 的脾氣。\n換句話說，每個人 p 都對應一個唯一的 ik 組合，所以我之前給出的公式實際上與我們原始的方差公式是相同的，即 \\[Var(Y)=\\frac{1}{N}\\sum_{p=1}^{N}(Y_p-\\bar{Y})^2\\] 在兩個公式中，我們所做的就是對樣本中的所有觀察值求和。大多數時候，我們只使用更簡單的 Yp 記號；使用 \\(Y_p\\) 的等式顯然是兩者中更簡單的一個。然而，在進行方差分析（ANOVA）時，我們需要跟踪哪些參與者屬於哪個組別，並且我們需要使用 Yik 記號來完成這項工作。\n\n\n\n\n12.2.2 變異數與平方差總和\n好的，既然我們對方差的計算有了很好的了解，讓我們定義一個叫做總平方和（total sum of squares）的東西，記作 SStot。這很簡單。計算方差時，我們是對平方偏差求平均，而計算總平方和時，我們只需將它們加起來。2\n當我們在 ANOVA 的上下文中談論分析變異數時，我們實際上是在處理總平方和，而不是實際的方差。3\n接下來，我們可以定義一個僅捕捉組間差異的變異概念。我們通過查看組平均值 \\(\\bar{Y}_k\\) 和整體平均值 \\(\\bar{Y}\\) 之間的差異來實現這一點。4\n這並不太難以證明，實驗中人們之間的總變異（\\(SS_{tot}\\)）實際上是組間差異（\\(SS_b\\)）和組內變異（\\(SS_w\\)）之和。即，\n\\[SS_w+SS_b=SS_{tot}\\] 好耶。\n好的，那麼我們發現了什麼？我們已經發現了與結果變項相關的總變異（\\(SS_{tot}\\)）可以在數學上被劃分為“由於不同組的樣本均值之間的差異所產生的變異”（\\(SS_b\\)）加上“其他所有變異”（\\(SS_w\\)）之和5。\n那怎麼幫助我找出這些組是否有不同的母體均值呢？嗯。等等。稍等一下。現在想想，這正是我們在尋找的。如果原假設成立，那麼您會期望所有樣本均值彼此非常相似，對吧？這將意味著您會期望 \\(SS_b\\) 非常小，或者至少您會期望它比“與其他所有事物相關的變異”（\\(SS_w\\)）小得多。嗯。我感覺到了一個假設檢驗的來臨。\n\n\n\n12.2.3 平方差總和與F檢定\n正如我們在上一節中看到的，ANOVA 的定性思想是將兩個平方和值 \\(SS_b\\) 和 \\(SS_w\\) 相互比較。如果組間變異 \\(SS_b\\) 相對於組內變異 \\(SS_w\\) 較大，那麼我們有理由懷疑不同組的母體均值彼此並不相同。為了將這一點轉化為可操作的假設檢驗，我們需要進行一些“小小的調整”。首先，我將向您展示我們如何計算檢驗統計量——F 值(F ratio)，然後嘗試讓您了解為什麼我們要這樣做。\n為了將我們的 SS 值轉換為 F 比，我們首先需要計算與 \\(SS_b\\) 和 \\(SS_w\\) 值相關的自由度。通常情況下，自由度對應於對特定計算做出貢獻的唯一“數據點”的數量，減去它們需要滿足的“約束”條件的數量。對於組內變異性，我們計算的是個體觀測值（\\(N\\) 個數據點）與組平均值（\\(G\\) 個約束）之間的變異。相反，對於組間變異性，我們關心的是組平均值（\\(G\\) 個數據點）在整體平均值（1 個約束）周圍的變化。因此，在這裡的自由度為：\n\\[df_b=G-1\\] \\[df_w=N-G\\]\n好吧，這似乎很簡單。接下來，我們將平方和值轉換為“平均平方”值，方法是除以自由度：\n\\[MS_b=\\frac{SS_b}{df_b}\\] \\[MS_w=\\frac{SS_w}{df_w}\\]\n最後，我們通過將組間 MS 除以組內 MS 來計算 F 比：\n\\[F=\\frac{MS_b}{MS_w}\\]\n從非常一般的層面上，F 統計量背後的直覺很簡單。F 值越大，表示組間變異相對於組內變異越大。因此，F 值越大，我們反駁虛無假設的證據就越多。但是 \\(F\\) 必須多大才能實際拒絕 \\(H_0\\)？要理解這一點，您需要更深入地了解 ANOVA 是什麼以及平均平方值實際上是什麼。\n下一節將詳細討論這個問題，但對於不感興趣實際衡量試驗內容的讀者，我將直接進入主題。為了完成我們的假設檢定，我們需要知道在虛無假設為真時 F 的抽樣分佈。不足為奇的是，在虛無假設下 F 統計量的抽樣分佈是一個 \\(F\\) 分佈。如果您回顧我們在 Chapter 7 中關於 F 分佈的討論，\\(F\\) 分佈有兩個參數，對應於涉及的兩個自由度。第一個 \\(df_1\\) 是組間自由度 \\(df_b\\)，第二個 \\(df_2\\) 是組內自由度 \\(df_w\\)。\n\n\n\n\n\nTable 12.2:  ANOVA 中涉及的所有關鍵數量都組織成一個“標準” ANOVA 表。所有數量的公式（除了 p 值，它有一個非常難看的公式，如果沒有計算機，計算起來會非常困難）都有顯示。 \n\nbetween\ngroupswithin\ngroups\n\ndf\\(  df_b=G-1  \\)\\(  df_w=N-G  \\)\n\nsum of squares\\(  SS_b=\\sum_{k=1}^{G} N_k  (\\bar{Y}_k-\\bar{Y})^2  \\)\\(  SS_w=\\sum_{k=1}^{G} \\sum_{i=1}^{N_k}   (Y_{ik}-\\bar{Y}_k)^2  \\)\n\nmean squares\\(  MS_b=\\frac{SS_b}{df_b}  \\)\\(  MS_w=\\frac{SS_w}{df_w}  \\)\n\nF-statistic\\(  F=\\frac{MS_b}{df_b}  \\)-\n\np-value[complicated]-\n\n\n\n\n\n在 Table 12.2 中顯示了涉及單因素 ANOVA 的所有關鍵數量的概要，包括顯示如何計算它們的公式。\n[額外的技術細節 6]\n\n\n\n12.2.4 實例演練\n先前的討論相當抽象且有點技術性，所以我認為此刻可能需要看一個實際示例。為此，讓我們回到本章開頭介紹的臨床試驗數據。我們在開始時計算的描述性統計數據告訴我們各組的平均值：安慰劑的平均情緒增益為 \\(0.45\\)，Anxifree 為 \\(0.72\\)，Joyzepam 為 \\(1.48\\)。有了這個想法，讓我們像 1899 年一樣開趴7，開始用鉛筆和紙做一些計算。我只會對前 \\(5\\) 個觀察值進行此操作，因為現在不是該死的 \\(1899\\) 年，而且我非常懶。讓我們從計算 \\(SS_w\\) 開始，即組內平方和。首先，讓我們繪製一個漂亮的表格來協助我們的計算（ Table 12.3 ）\n\n\n\n\n\n\nTable 12.3:  示範演算第一步 \n\ngroup koutcome \\( Y_{ik} \\)\n\nplacebo0.5\n\nplacebo0.3\n\nplacebo0.1\n\nanxifree0.6\n\nanxifree0.4\n\n\n\n\n\n在這個階段，我在表格中包含的只是原始數據本身。也就是說，每個人的分組變項（即藥物）和結果變項（即心情增益）。請注意，這裡的結果變項對應於我們先前方程式中的 \\(\\bar{Y}_{ik}\\) 值。接下來的計算步驟是為研究中的每個人寫下相應的組平均值，\\(\\bar{Y}_k\\)。這有點重複，但並不是特別困難，因為我們在進行描述性統計時已經計算了這些組平均值，見 Table 12.4 。\n\n\n\n\n\n\nTable 12.4:  示範演算第二步 \n\ngroup koutcome \\( Y_{ik} \\)group mean \\( \\bar{Y}_k \\)\n\nplacebo0.50.45\n\nplacebo0.30.45\n\nplacebo0.10.45\n\nanxifree0.60.72\n\nanxifree0.40.72\n\n\n\n\n\n既然我們已經寫下了這些，我們需要再次為每個人計算與相應組平均值的偏差。也就是說，我們想要減去 \\(Y_{ik} - \\bar{Y}_k\\)。在我們做完這個之後，我們需要將所有東西平方。當我們這樣做時，這就是我們得到的結果（ Table 12.5 ）\n\n\n\n\n\nTable 12.5:  示範演算第三步 \n\ngroup koutcome \\( Y_{ik} \\)group mean  \\( \\bar{Y}_k \\)dev. from group mean  \\( Y_{ik} - \\bar{Y}_k \\)squared deviation \\(  (Y_{ik}-\\bar{Y}_k)^2 \\)\n\nplacebo0.50.450.050.0025\n\nplacebo0.30.45-0.150.0225\n\nplacebo0.10.45-0.350.1225\n\nanxifree0.60.72-0.120.0136\n\nanxifree0.40.72-0.320.1003\n\n\n\n\n\n最後一步同樣簡單。為了計算組內平方和，我們只需將所有觀察值的平方偏差相加：\n\\[\n\\begin{split}\nSS_w & = 0.0025 + 0.0225 + 0.1225 + 0.0136 + 0.1003 \\\\\n& = 0.2614\n\\end{split}\n\\]\n當然，如果我們真的想得到正確的答案，我們需要對數據集中的所有18個觀察值進行此操作，而不僅僅是前五個。如果我們想要的話，我們可以繼續使用鉛筆和紙進行計算，但這相當繁瑣。或者，使用專用的電子表格程序（如 OpenOffice 或 Excel）也不是很困難。嘗試自己做。我在 Excel 中做的那個文件名為 clinicaltrial_anova.xls。當你做完後，你應該得到一個組內平方和值為 \\(1.39\\)。\n好的。現在我們已經計算了組內變異 \\(SS_w\\)，是時候將我們的注意力轉向組間平方和 \\(SS_b\\) 了。對於這種情況，計算非常相似。主要區別在於，對於所有觀察值，我們不再計算觀察值 Yik 和組平均值 \\(\\bar{Y}_k\\) 之間的差異，而是計算所有組的組平均值 \\(\\bar{Y}_k\\) 和總平均值 \\(\\bar{Y}\\)（在這種情況下為 \\(0.88\\)）之間的差異（Table 12.6）。\n\n\n\n\n\nTable 12.6:  示範演算第4步 \n\ngroup kgroup mean \\( \\bar{Y}_k \\)grand mean  \\( \\bar{Y} \\)deviation  \\( \\bar{Y}_k - \\bar{Y} \\)squared deviation \\(  ( \\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo0.450.88-0.430.19\n\nanxifree0.720.88-0.160.03\n\njoyzepam1.480.880.600.36\n\n\n\n\n\n然而，對於組間計算，我們需要將每個平方偏差乘以 \\(N_k\\)，即組中的觀察值數量。我們這樣做是因為該組中的每個觀察值（所有 \\(N_k\\) 個觀察值）都與組間差異有關。因此，如果安慰劑組有六個人，並且安慰劑組的平均值與總平均值相差 \\(0.19\\)，那麼這六個人與組間變異之間的關聯總和為 \\(6 \\times 0.19 = 1.14\\)。因此，我們必須擴展我們的計算表格（ Table 12.7 ）。\n\n\n\n\n\n\nTable 12.7:  示範演算第5步 \n\ngroup k...squared deviations  \\( (\\bar{Y}_k-\\bar{Y})^2 \\)sample size  \\( N_k \\)weighted squared dev   \\(  N_k (\\bar{Y}_k-\\bar{Y})^2 \\)\n\nplacebo...0.1961.14\n\nanxifree...0.0360.18\n\njoyzepam...0.3662.16\n\n\n\n\n\n現在，我們的組間平方和是通過將這些“加權平方偏差”在研究中的所有三組中求和而得到的：\n\\[\\begin{aligned} SS_b & = 1.14 + 0.18 + 2.16 \\\\ &= 3.48 \\end{aligned}\\]\n如您所見，組間計算要短得多 8。現在我們已經計算出了平方和值 \\(SS_b\\) 和 \\(SS_w\\)，剩下的 ANOVA 分析就相當簡單了。下一步是計算自由度。由於我們有 \\(G = 3\\) 個組和 \\(N = 18\\) 個觀察值，我們的自由度可以通過簡單的減法來計算：\n\\[\n\\begin{split}\ndf_b & = G-1 = 2 \\\\\ndf_w & = N-G = 15\n\\end{split}\n\\]\n接下來，由於我們已經計算了平方和值和自由度的值，對於組內變異性和組間變異性，我們可以通過將一個除以另一個來獲得平均平方值：\n\\[\n\\begin{split}\nMS_b & = \\frac{SS_b}{df_b} = \\frac{3.48}{2} = 1.74 \\\\\nMS_w & = \\frac{SS_w}{df_w} = \\frac{1.39}{15} = 0.09\n\\end{split}\n\\]\n我們快完成了。平均平方值可用於計算我們感興趣的 F 值，這是我們感興趣的檢驗統計量。我們通過將組間 MS 值除以組內 MS 值來完成此操作。\n\\[\n\\begin{split}\nF & = \\frac{MS_b}{MS_w}  = \\frac{1.74}{0.09} \\\\\n& = 19.3\n\\end{split}\n\\]\n哇！這真的非常令人興奮，對嗎？現在我們有了檢驗統計量，最後一步是找出檢驗本身是否給我們一個顯著結果。如 Chapter 9 在“過去的日子”中所討論的，我們要做的是打開一本統計教科書或翻到後面的部分，這裡會有一個巨大的查找表，我們會找到對應特定 alpha 值（空假設拒絕區域）的閾值 F 值，例如 \\(0.05\\)，\\(0.01\\) 或 \\(0.001\\)，對於 2 和 15 度的自由度。用這種方法，對於 alpha 為 \\(0.001\\) 的情況，我們會得到一個閾值 F 值為 \\(11.34\\)。由於這小於我們計算出的 F 值，我們說 \\(p < 0.001\\)。但那是過去的日子，現在花哨的統計軟件會為您計算出確切的 p 值。實際上，確切的 p 值為 \\(0.000071\\)。所以，除非我們對 Type I 錯誤率非常保守，否則我們幾乎可以保證拒絕虛無假設。\n此刻，我們基本上完成了。完成計算後，將所有這些數字整理成類似於表 12.1 的 ANOVA 表是傳統做法。對於我們的臨床試驗數據，ANOVA 表將如 Table 12.8 。\n\n\n\n\n\n\nTable 12.8:  完整的變異數分析結果表 \n\ndfsum of squaresmean squaresF-statisticp-value\n\nbetween groups23.481.7419.30.000071\n\nwithin groups151.390.09--\n\n\n\n\n\n如今，您可能永遠沒有太多理由想要自己構建這樣的表格，但您會發現幾乎所有的統計軟件（包括 jamovi）都傾向於將 ANOVA 的輸出組織成這樣的表格，所以最好習慣閱讀它們。然而，儘管軟件將輸出完整的 ANOVA 表，但幾乎從來沒有充分理由在您的撰寫中包含整個表格。報告此結果的統計塊的一種非常標準的方法是寫下類似以下的內容：\n\n單因素 ANOVA 顯示藥物對情緒增益有顯著影響（F(2,15) = 19.3，p < .001）。\n\n嘆氣。這麼多工作，只為了一個簡短的句子。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#jamovi的變異數分析模組",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.3 jamovi的變異數分析模組",
    "text": "12.3 jamovi的變異數分析模組\nI’m pretty sure I know what you’re thinking after reading the last section, especially if you followed my advice and did all of that by pencil and paper (i.e., in a spreadsheet) yourself. Doing the ANOVA calculations yourself sucks. There’s quite a lot of calculations that we needed to do along the way, and it would be tedious to have to do this over and over again every time you wanted to do an ANOVA.\n\n12.3.1 使用jamovi完成變異數分析\nTo make life easier for you, jamovi can do ANOVA…hurrah! Go to the ‘ANOVA’ - ‘ANOVA’ analysis, and move the mood.gain variable across so it is in the ‘Dependent Variable’ box, and then move the drug variable across so it is in the ‘Fixed Factors’ box. This should give the results as shown in Figure 12.3. 9 Note I have also checked the \\(\\eta^2\\) checkbox, pronounced “eta” squared, under the ‘Effect Size’ option and this is also shown on the results table. We will come back to effect sizes a bit later.\n\n\n\n\n\nFigure 12.3: jamovi results table for ANOVA of mood gain by drug administered\n\n\n\n\nThe jamovi results table shows you the sums of squares values, the degrees of freedom, and a couple of other quantities that we’re not really interested in right now. Notice, however, that jamovi doesn’t use the names “between-group” and “within-group”. Instead, it tries to assign more meaningful names. In our particular example, the between groups variance corresponds to the effect that the drug has on the outcome variable, and the within groups variance corresponds to the “leftover” variability so it calls that the residuals. If we compare these numbers to the numbers that I calculated by hand in [A worked example], you can see that they’re more or less the same, apart from rounding errors. The between groups sums of squares is \\(SS_b = 3.45\\), the within groups sums of squares is \\(SS_w = 1.39\\), and the degrees of freedom are \\(2\\) and \\(15\\) respectively. We also get the F-value and the p-value and, again, these are more or less the same, give or take rounding errors, to the numbers that we calculated ourselves when doing it the long and tedious way."
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#效果量",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#效果量",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.4 效果量",
    "text": "12.4 效果量\n有幾種不同的方法可以衡量 ANOVA 中的效應大小，但最常用的衡量指標是 \\(\\eta^2\\)（eta 平方）和偏 \\(\\eta^2\\)。對於單因素變異數分析，它們彼此相同，所以目前我只解釋 \\(\\eta^2\\)。\\(\\eta^2\\) 的定義實際上非常簡單：\n\\[\\eta^2=\\frac{SS_b}{SS_{tot}}\\]\n就是這樣。所以當我查看 Figure 12.3 中的 ANOVA 表時，我看到 \\(SS_b = 3.45\\) 和 \\(SS_tot = 3.45 + 1.39 = 4.84\\)。因此，我們得到一個 \\(\\eta^2\\) 值：\n\\[\\eta^2=\\frac{3.45}{4.84}=0.71\\]\n\\(\\eta^2\\) 的解釋同樣直接。它表示可以根據預測變項（藥物）解釋的結果變項（mood.gain）可變性的比例。\\(\\eta^2=0\\) 表示兩者之間完全沒有關係，而 \\(\\eta^2=1\\) 表示關係是完美的。更好的是，\\(\\eta^2\\) 值與 Section 10.6.1 中討論的 \\(R^2\\) 關係非常密切，並具有等效的解釋。儘管許多統計教科書建議在 ANOVA 中使用 \\(\\eta^2\\) 作為默認的效應大小衡量指標，但 Daniel Lakens 的一篇有趣的博客文章表明，eta 平方在實際數據分析中可能不是最好的效應大小衡量指標，因為它可能是一個有偏估計量。有用的是，jamovi 中還有一個選項可以指定 ω 平方（\\(\\omega^2\\)），它與 η 平方相比偏差較小。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#多重比較與事後檢定",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.5 多重比較與事後檢定",
    "text": "12.5 多重比較與事後檢定\n每當您對多於兩個組進行 ANOVA，並得到顯著效應時，您可能首先想問的是哪些組之間實際上存在差異。在我們的藥物示例中，我們的零假設是所有三種藥物（安慰劑、Anxifree 和 Joyzepam）對情緒的影響完全相同。但是如果你仔細想一想，實際上零假設一次聲稱了三個不同的事情。具體來說，它聲稱：\n\n您的競爭對手的藥物（Anxifree）並不比安慰劑更好（即，\\(\\mu_A = \\mu_P\\) ）\n您的藥物（Joyzepam）並不比安慰劑更好（即，\\(\\mu_J = \\mu_P\\) ）\nAnxifree 和 Joyzepam 同樣有效（即，\\(\\mu_J = \\mu_A\\)）\n\n如果上述三個聲稱中的任何一個是偽的，那麼零假設也是偽的。因此，現在我們已經拒絕了我們的零假設，我們認為至少有一件事是不正確的。但哪些呢？所有三個命題都很有趣。既然您肯定想知道您的新藥 Joyzepam 是否比安慰劑更好，那麼了解它與現有商業替代品（即 Anxifree）的比較如何就變得很重要了。甚至有用的是檢查 Anxifree 與安慰劑的表現。即使 Anxifree 已經被其他研究人員廣泛地與安慰劑進行了對照測試，但檢查您的研究是否產生了與早期工作相似的結果仍然非常有用。\n當我們根據這三個不同的命題來描述零假設時，我們需要區分的八種可能的“世界狀態”變得清晰了（ Table 12.9 ）。\n\n\n\n\n\nTable 12.9:  虛無假設與八種可能的”現實世界” \n\npossibility:is \\( \\mu_P = \\mu_A \\)?is \\( \\mu_P = \\mu_J \\)?is \\( \\mu_A = \\mu_J \\)?which hypothesis?\n\n1\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)null\n\n2\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n3\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n4\\( \\checkmark \\)alternative\n\n5\\( \\checkmark \\)\\( \\checkmark \\)\\( \\checkmark \\)alternative\n\n6\\( \\checkmark \\)alternative\n\n7\\( \\checkmark \\)alternative\n\n8alternative\n\n\n\n\n\n通過拒絕零假設，我們已經決定我們不相信 #1 是真實的世界狀態。下一個問題是，我們認為其他七個可能性中的哪一個*是對的？面對這種情況，通常最好先看看數據。例如，如果我們查看 Figure 12.1 中的繪圖，我們很容易得出 Joyzepam 優於安慰劑和 Anxifree，但 Anxifree 和安慰劑之間沒有實際差別的結論。然而，如果我們想對此得到更清晰的答案，則可能需要進行一些測試。\n\n\n12.5.1 成對t檢定\n我們如何解決問題？考慮到我們需要比較三對不同的平均值（安慰劑對 Anxifree，安慰劑對 Joyzepam，和 Anxifree 對 Joyzepam），我們可以執行三個單獨的 t 檢驗，看看會發生什麼。在 jamovi 中這很容易做到。轉到 ANOVA 的 ‘Post Hoc Tests’（事後檢驗）選項，將 ‘drug’（藥物）變項移到右側的活動框中，然後單擊 ‘No correction’（無校正）複選框。這將產生一個整齊的表格，顯示藥物變項的三個水平之間的所有成對 t 檢驗比較，如 Figure 12.4 中所示。\n\n\n\n\n\n\n\nFigure 12.4: 未經校正的成對 t 檢驗作為 jamovi 中的事後比較。\n\n\n\n\n\n\n12.5.2 多重檢定的校正\n在上一節中，我暗示了執行大量 t 檢驗存在問題。我們擔心的是，在執行這些分析時，我們正在進行一個「捕魚之旅」。我們在沒有太多理論指導的情況下執行了大量測試，希望其中一些測試顯示出顯著性。這種對團體差異的無理論基礎的搜索被稱為事後分析（“post hoc” 是拉丁語，意為 “after this”）。[^13-comparing-several-means-one-way-anova-10]\n[^13-comparing-several-means-one-way-anova-10]：如果您確實有一些理論基礎，希望研究某些比較而不是其他比較，那就是另一回事了。在這種情況下，您實際上並不是在執行「事後分析」，而是在進行「預先計劃的比較」。我確實在本書後面談到了這種情況- Section 13.9 ，但現在我想保持簡單。\n進行事後分析是可以的，但需要非常小心。例如，在上一節中進行的分析應該避免，因為每個單獨的 t 檢驗都設計為 5% 的第一型錯誤率（即 \\(\\alpha = .05\\)），而我執行了其中的三個檢驗。想象一下，如果我的 ANOVA 涉及 10 個不同的組，我決定執行 45 個「事後」t 檢驗，試圖找出哪些組之間存在顯著差異，那麼僅憑機會就會出現 2 到 3 個顯著結果。正如我們在 Chapter 9 中看到的那樣，虛無假設檢驗背後的核心組織原則是控制我們的第一型錯誤率，但是現在，由於我同時執行了大量 t 檢驗以確定 ANOVA 結果的來源，整個試驗家族的實際第一型錯誤率已經完全失控。\n解決這個問題的常用方法是對 p 值進行調整，目的是控制整個試驗家族的總誤差率（參見 Shaffer (1995)）。這種調整通常（但不總是）應用於事後分析，通常被稱為多重比較校正，儘管有時也被稱為「同時推斷」。無論如何，進行這種調整的方法有很多。我將在本節和下一章節 Section 13.8 中討論其中的一些方法，但您應該意識到還有很多其他方法（例如，參見 Hsu (1996) ）。\n\n\n\n12.5.3 Bonferroni校正\n這些調整中最簡單的一種被稱為邦弗隆尼校正(Dunn, 1961)，它確實非常簡單。假設我的事後分析包括 m 個單獨的檢驗，我希望確保出現任何第一型錯誤的總概率最多為 \\(\\alpha\\)。[^13-comparing-several-means-one-way-anova-11] 如果是這樣，那麼邦弗隆尼校正只是說「將所有原始 p 值乘以 m」。如果讓 \\(p\\) 表示原始 p 值，讓 \\(p_j^{'}\\) 表示經過校正的值，那麼邦弗隆尼校正告訴我們：\n[^13-comparing-several-means-one-way-anova-11]：順便值得一提的是，並非所有調整方法都試圖這樣做。我在這裡描述的是一種用於控制「家族式第一型錯誤率」的方法。然而，還有其他事後檢驗試圖控制「偽發現率」，這是一個有點不同的概念。\n\\[p_j^{'}=m \\times p\\]\n因此，如果您使用邦弗隆尼校正，則在 \\(p_j^{'} < \\alpha\\) 的情況下拒絕零假設。這種校正背後的邏輯非常簡單。我們正在進行 m 個不同的檢驗，因此，如果我們安排使每個檢驗的第一型錯誤率至多為 \\(\\frac{\\alpha}{m}\\)，那麼這些檢驗的總第一型錯誤率不能大於 \\(\\alpha\\)。這很簡單，簡單到在原始論文中，作者寫道：\n\n在這裡給出的方法如此簡單，而且如此通用，我確信它肯定已經被使用過了。然而，我沒有找到它，所以只能得出一個結論：也許正是它的極簡單讓統計學家意識不到它在某些情況下是一個非常好的方法（Dunn (1961)，第52-53頁）。\n\n要在 jamovi 中使用邦弗隆尼校正，只需單擊「校正」選項中的「邦弗隆尼」復選框，您將在 ANOVA 結果表中看到另一列，顯示邦弗隆尼校正的調整後 p 值（ Table 12.8 ）。如果我們將這三個 p 值與未校正的成對 t 檢驗的 p 值進行比較，很明顯 jamovi 所做的唯一事情就是將它們乘以 \\(3\\)。\n\n\n\n\n12.5.4 Holm校正\n雖然邦弗隆尼校正是最簡單的調整方法，但它通常不是最好的選擇。經常使用的另一種方法是霍爾姆校正（Holm correction）(Holm, 1979)。霍爾姆校正背後的思路是假設您正在按順序進行測試，從最小（原始）的 p 值開始，然後移動到最大的 p 值。對於第 j 大的 p 值，調整是以下兩者之一\n\\[p_j^{'}=j \\times p_j\\]\n（即最大的 p 值保持不變，第二大的 p 值翻倍，第三大的 p 值翻三倍，依此類推），或者\n\\[p_j^{'}=p_{j+1}^{'}\\]\n其中較大者。這可能聽起來有點困惑，所以讓我們慢慢解釋。霍爾姆校正的工作原理如下。首先，您按順序對所有 p 值進行排序，從最小到最大。對於最小的 p 值，您只需將其乘以 \\(m\\)，然後就完成了。然而，對於其他所有的 p 值，這是一個兩階段的過程。例如，當您移動到第二小的 p 值時，首先將其乘以 \\(m - 1\\)。如果這產生的數字大於您上次得到的調整後的 p 值，那麼保留它。但如果它比上一個小，那麼您將複製上一個 p 值。為了說明這是如何工作的，請考慮 Table 12.10 ，該表顯示了五個 p 值的霍爾姆校正計算。\n\n\n\n\n\nTable 12.10:  經過霍爾姆校正計算的p值 \n\nraw prank jp \\( \\times \\) jHolm p\n\n.0015.005.005\n\n.0054.020.020\n\n.0193.057.057\n\n.0222.044.057\n\n.1031.103.103\n\n\n\n\n\n希望這能讓事情變得清晰。\n雖然計算起來稍微困難一些，但霍爾姆校正具有一些非常好的特性。它比邦弗隆尼更具威力（即具有更低的 Type II 錯誤率），但是，儘管可能令人反直覺，它具有相同的 Type I 錯誤率。因此，在實踐中，沒有理由使用更簡單的邦弗隆尼校正，因為它總是被稍微複雜一點的霍爾姆校正所超越。正因為如此，霍爾姆校正應該是您的首選多重比較校正。 Figure 12.4 還顯示了霍爾姆校正後的 p 值，如您所見，最大的 p 值（對應於 Anxifree 和安慰劑之間的比較）沒有改變。它的值為 .15，與我們最初在完全不做校正時得到的值完全相同。相比之下，最小的 p 值（Joyzepam 與安慰劑）已乘以三。\n\n\n\n12.5.5 事後檢定的報告格式\n最後，在執行事後分析以確定哪些組別之間的差異顯著之後，您可以這樣寫出結果：\n\n事後檢驗（使用霍爾姆校正來調整 p 值）表明，與 Anxifree（p = .001）和安慰劑（\\(（p = 9.0 \\times{10^{-5}}\\)）相比，Joyzepam 產生了顯著更大的心情變化。我們沒有發現 Anxifree 表現優於安慰劑的證據（\\(p = .15\\)）。\n\n或者，如果您不喜歡報告精確的 p 值，那麼分別將這些數字更改為 \\(p < .01\\)、\\(p < .001\\) 和 \\(p > .05\\)。無論哪種方式，關鍵是要表明您使用了霍爾姆的校正來調整 p 值。當然，我假設在撰寫的其他部分，您已經包括了相關的描述性統計資料（即組平均值和標準差），因為這些 p 值本身並不是很有信息量。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的執行條件",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.6 單因子變異數分析的執行條件",
    "text": "12.6 單因子變異數分析的執行條件\n像任何統計檢驗一樣，變異數分析依賴於關於數據（特別是殘差）的一些假設。您需要了解三個關鍵假設：正態性、方差同質性和獨立性。\n[額外的技術細節 [^13-comparing-several-means-one-way-anova-12]]\n[^13-comparing-several-means-one-way-anova-12]：如果您記得回到[一個實例]，我希望您至少瀏覽了一遍，即使您沒有讀完整篇文章，我以這種方式描述了支撐ANOVA的統計模型：\\[H_0:Y_{ik}=\\mu + \\epsilon_{ik}\\] \\[H_1:Y_{ik}=\\mu_k + \\epsilon_{ik}\\]在這些等式中，\\(\\mu\\)指的是對所有組別都相同的單個總群體均值，µk是第k個組的群體均值。到目前為止，我們主要關心的是我們的數據是最好用單個總均值（零假設）來描述，還是用不同的特定組均值（替代假設）來描述。當然，這是有道理的，因為這實際上是重要的研究問題！然而，我們所有的檢驗過程都是在一個關於殘差 \\(\\epsilon_{ik}\\) 的具體假設下進行的，即：\\[\\epsilon_{ik} \\sim Normal(0,\\sigma^2)\\]如果沒有這部分，所有的數學都不能正常工作。或者，確切地說，您仍然可以進行所有計算，最終得到一個F統計量，但是您無法保證這個F統計量實際上衡量了您認為它衡量的內容，因此您可能基於F檢驗得出的任何結論都可能是錯誤的。\n那麼，我們如何檢查對殘差的假設是否準確呢？嗯，正如我上面所指出的，這個陳述中隱含了三個不同的主張，我們將分別考慮它們。\n\n方差同質性。注意到我們只有一個群體標準差的值（即，\\(\\sigma\\)），而不是讓每個組都有它自己的值（即，\\(\\sigma_k\\)）。這被稱為方差同質性（有時稱為等方差性）假設。ANOVA假定所有組的群體標準差相同。我們將在[檢查方差同質性假設]部分詳細論述這一點。\n正態性。假定殘差呈正態分布。正如我們在@sec-Checking-the-normality-of-a-sample中看到的，我們可以通過查看QQ圖（或運行Shapiro-Wilk檢驗）來評估這一點。我將在[檢查正態性假設]部分中更多地討論這個問題。\n獨立性。獨立性假設有點棘手。它基本上的意思是，了解一個殘差對於了解任何其他殘差都沒有幫助。所有的 \\(\\epsilon_{ik}\\) 值都被假定是在不考慮或與其他任何值無關的情況下生成的。對於這一點，沒有顯而易見或簡單的檢驗方法，但有些情況是明顯違反這一假設的。例如，如果您有一個重複測量設計，每個參與者在研究中出現在多個條件下，那麼獨立性就不成立了。在這種情況下，某些觀察之間存在特殊關係，即對應於同一個人的觀察！當這種情況發生時，您需要使用類似[重複測量單因子ANOVA]的方法。\n\n\n\n12.6.1 同質性檢核\n\n要進行方差的初步檢驗，就像乘坐划艇出海，看看海面條件是否足夠平靜，讓一艘大型遊輪離港！\n– 喬治·博克斯 (Box, 1953)\n\n俗話說，殺貓有很多方法，檢驗方差同質性假設也有很多方法（不過出於某種原因，沒有人把它變成一句俗話）。在文獻中，我見過的最常用的檢驗方法是Levene檢驗(Levene, 1960)，以及與之密切相關的Brown-Forsythe檢驗(Brown & Forsythe, 1974)。\n無論您是進行標準Levene檢驗還是Brown-Forsythe檢驗，檢驗統計量（有時表示為\\(F\\)，但也有時表示為\\(W\\)），都是按照計算常規ANOVA中的F-統計量的方式，只是使用\\(Z_{ik}\\)而不是\\(Y_{ik}\\)。有了這個思路，我們可以繼續看看如何在jamovi中運行檢驗。\n[額外的技術細節[^13-comparing-several-means-one-way-anova-13]]\n[^13-comparing-several-means-one-way-anova-13]：Levene檢驗非常簡單。假設我們有結果變量\\(Y_{ik}\\)。我們所要做的就是定義一個新變量，我將其稱為\\(Z_{ik}\\)，表示與組均值的絕對偏差：\\[Z_{ik}=Y_{ik}-\\bar{Y}_{k}\\]好吧，這對我們有什麼好處呢？那麼，讓我們花一點時間來思考一下\\(Z_{ik}\\)到底是什麼以及我們要檢驗什麼。\\(Z_{ik}\\)的值是度量第\\(i\\)次觀測在第\\(k\\)個組中與其組平均值的偏差程度。我們的零假設是所有組的方差都相同，即所有組平均值的總偏差相同！因此，Levene檢驗中的零假設是所有組的\\(Z\\)的母體平均值相同。嗯。那麼我們現在需要的是一個統計檢驗來檢驗所有組均值相同的零假設。我們在哪裡見過這個檢驗？哦對了，這就是ANOVA，所以Levene檢驗所做的就是對新變量\\(Z_{ik}\\)進行ANOVA。Brown-Forsythe檢驗呢？它有做什麼特別不同的事情嗎？不，與Levene檢驗唯一的不同是它以稍微不同的方式構建轉換變量Z，使用組中位數的偏差而不是組平均值的偏差。也就是說，對於Brown-Forsythe檢驗：\\[Z_{ik}=Y_{ik}-median_k(Y)\\]其中，\\(median_k(Y)\\)是第k組的中位數。\n\n\n\n12.6.2 jamovi的Levene檢定\n好的，那麼我們該如何進行Levene檢驗呢？其實很簡單 - 在ANOVA的”假設檢查”選項下，只需點擊”變異數同質性檢驗”複選框。如果我們查看@fig-fig12-5中的輸出，我們可以看到檢驗結果並無顯著差異（\\(F_{2,15} = 1.45, p = .266\\)），所以變異數同質性假設看起來沒有問題。然而，外表可能會讓人受騙！如果您的樣本量相當大，那麼即使變異數同質性假設沒有被違反到影響ANOVA的穩健性，Levene檢驗也可能顯示出顯著效應（即p < .05）。這正是George Box在上面引述中所指出的觀點。同樣地，如果您的樣本量相當小，那麼變異數同質性假設可能不被滿足，而Levene檢驗可能不顯著（即p > .05）。這意味著，在對假設是否被滿足進行任何統計檢驗的同時，您應該總是繪製每個分組/類別的均值周圍的標準差……只是為了看看它們是否看起來相當相似（即變異數同質性）或不相似。\n\n\n\n\n\n\nFigure 12.5: jamovi中單因素ANOVA的Levene檢驗輸出\n\n\n\n\n\n\n12.6.3 校正異質性的分析結果\n在我們的示例中，變異數同質性假設被證明是相當可靠的：Levene檢驗結果並無顯著差異（儘管我們還應該查看標準差的圖形），因此我們可能不需要擔心。然而，在現實生活中，我們並非總是如此幸運。當變異數同質性假設被違反時，我們該如何拯救我們的ANOVA呢？如果您回想一下我們對t檢驗的討論，我們之前遇到過這個問題。Student t檢驗假設等方差，所以解決方法是使用不需要等方差假設的Welch t檢驗。實際上，(Welch1951還展示了我們如何解決ANOVA的這個問題?)（Welch單因素檢驗）。它在jamovi中使用One-Way ANOVA分析實現。這是一種專為單因素ANOVA設計的分析方法，要在我們的示例中執行Welch單因素ANOVA，我們將按照之前的方式重新運行分析，但這次使用jamovi的ANOVA - One Way ANOVA分析命令，並選擇Welch檢驗的選項（參見@fig-fig12-6）。為了理解這裡發生了什麼，讓我們將這些數字與我們在[最初在jamovi中運行ANOVA]時得到的數字進行比較。為了省去您回顧的麻煩，上次我們得到的是：\\(F(2, 15) = 18.611, p = .00009\\)，這也顯示為@fig-fig12-6中One-Way ANOVA的Fisher檢驗。\n\n\n\n\n\n\nFigure 12.6: Welch檢驗作為jamovi中One Way ANOVA分析的一部分\n\n\n\n\n好的，最初我們的ANOVA結果是\\(F(2, 15) = 18.6\\)，而Welch單因素檢驗給出的是\\(F(2, 9.49) = 26.32\\)。換句話說，Welch檢驗將組內自由度從15降低到了9.49，而F值從18.6上升到了26.32。\n\n\n\n12.6.4 常態性檢核\n檢驗正態性假設相對簡單。我們在@sec-Checking-the-normality-of-a-sample中介紹了大部分你需要了解的內容。我們真正需要做的只是繪製一個QQ圖，並在可行的情況下，運行Shapiro-Wilk檢驗。QQ圖顯示在@fig-fig12-7，對我來說看起來相當正常。如果Shapiro-Wilk檢驗不顯著（即\\(p > .05\\)），那麼這表明正態性假設沒有被違反。然而，與Levene檢驗一樣，如果樣本量很大，那麼顯著的Shapiro-Wilk檢驗實際上可能是偽陽性，也就是說，正態性假設在實質上沒有對分析造成任何問題。同樣地，非常小的樣本量可能會產生偽陰性。這就是為什麼視覺檢查QQ圖很重要。\n\n\n\n\n\n\nFigure 12.7: jamovi中One Way ANOVA分析的QQ圖\n\n\n\n\n除了檢查QQ圖中是否有偏離正態性的情況外，我們的數據的Shapiro-Wilk檢驗確實顯示出非顯著效應，p = 0.6053（見@fig-fig12-6）。因此，這支持了QQ圖的評估；兩個檢查都沒有發現正態性被違反的跡象。\n\n\n\n12.6.5 排除非常態性的分析結果\n現在我們已經了解了如何檢查正態性，我們自然會問可以採取哪些措施來解決正態性的違反。在單因素ANOVA的背景下，最簡單的解決方案可能是轉向非參數檢驗（即不依賴於任何特定的分佈假設的檢驗）。在@sec-Comparing-two-means中，我們之前已經介紹過非參數檢驗。當你只有兩個組別時，Mann-Whitney或Wilcoxon檢驗可以提供你所需的非參數替代方法。當你有三個或更多組別時，你可以使用Kruskal-Wallis秩和檢驗(Kruskal & Wallis, 1952)。接下來我們將講解這個檢驗。\n\n\n\n12.6.6 Kruskal-Wallis檢定的邏輯\nKruskal-Wallis檢驗在某些方面與ANOVA驚人地相似。在ANOVA中，我們從\\(Y_{ik}\\)開始，對於第k個組中的第i個人，這是結果變量的值。對於Kruskal-Wallis檢驗，我們要做的是對所有的\\(Y_{ik}\\)值進行排序，並對排名數據進行分析。9\n\n\n\n12.6.7 更多分析細節\n上一節的描述說明了Kruskal-Wallis檢驗背後的邏輯。從概念上講，這是考慮測試如何工作的正確方法。[^13-comparing-several-means-one-way-anova-15]\n[^13-comparing-several-means-one-way-anova-15]：然而，從純粹的數學角度來看，這是不必要的複雜。我不會向您展示推導，但您可以使用一些代數技巧\\(^b\\)來顯示K的方程式可以是\\[K=\\frac{12}{N(N-1)}\\sum_k N_k \\bar{R}_k^2 -3(N+1)\\] 最後一個方程式有時給出了K的值。這比我在上一節中描述的版本要容易得多，但問題是對實際人類完全沒有意義。將K視為基於排名的ANOVA類比可能是最好的方式。但請記住，計算出來的檢驗統計量與我們最初用於ANOVA的統計量有很大不同。— \\(b\\)就是一些數學運算術語。\n但等等，還有更多！天啊，為什麼總是有更多呢？到目前為止，我講的故事實際上只在原始數據中沒有相同數值的情況下才成立。也就是說，如果沒有兩個觀測值具有完全相同的值。如果有相同的值，那麼我們必須引入一個校正因子來進行這些計算。在這一點上，我假設即使是最勤奮的讀者也已經不再關心（或者至少形成了繫結校正因子不需要他們立即關注的看法）。因此，我將非常快速地告訴您如何計算它，並省略為什麼以這種方式進行的繁瑣細節。假設我們為原始數據構建一個頻率表，讓fj表示具有第j個唯一值的觀測值的數量。這聽起來可能有點抽象，因此我們將從clinicaltrials.csv數據集中的mood.gain頻率表（Table 12.11）給出一個具體的例子。\n\n\n\n\n\n\nTable 12.11:  數據中clinicaltrials.csv心情增益的次數表 \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\n觀察此表，請注意頻率表中的第三個條目值為2。由於這對應於心情增益為0.3，因此此表告訴我們有兩個人的心情增加了0.3。[^13-comparing-several-means-one-way-anova-16]\n[^13-comparing-several-means-one-way-anova-16]：更重要的是，在我上面介紹的數學表示法中，這告訴我們\\(f_3 = 2\\)。耶。那麼，現在我們知道了這一點，繫結校正因子（TCF）是：\\[TCF=1-\\frac{\\sum_j f_j^3 - f_j}{N^3 - N}\\]通過將K值除以這個數量，可以得到Kruskal-Wallis統計量的繫結校正值。這是jamovi計算的繫結校正版本。\n因此，jamovi使用繫結校正因子來計算繫結校正的Kruskall-Wallis統計量。最後，我們實際上已經完成了Kruskal-Wallis檢驗的理論。我確信你們都對我治愈了你們在意識到你們不知道如何計算Kruskal-Wallis檢驗的繫結校正因子時自然產生的存在焦慮感到非常寬慰。對吧？\n\n\n\n\n12.6.8 使用jamovi完成Kruskal-Wallis檢定\n儘管我們在努力理解Kruskal Wallis檢驗實際上做了什麼方面經歷了恐懼，但事實證明，進行該檢驗相當無痛，因為jamovi在ANOVA分析集中有一個名為「非參數」-「單因子ANOVA（Kruskall-Wallis）」的分析。大多數時候，你將擁有像clinicaltrial.csv這樣的數據集，其中包含你的結果變量mood.gain和一個分組變量drug。如果是這樣，你可以直接在jamovi中運行分析。這給我們提供了一個Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\)，如@fig-fig12-8所示。\n\n\n\n\n\n\nFigure 12.8: jamovi中的Kruskall-Wallis單因子非參數ANOVA"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#單因子重覆量數變異數分析",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.7 單因子重覆量數變異數分析",
    "text": "12.7 單因子重覆量數變異數分析\n單因子重複測量ANOVA檢驗是一種統計方法，用於檢驗三個或更多組之間的顯著差異，其中每個組都使用相同的參與者（或者每個參與者與其他實驗組的參與者密切匹配）。因此，每個實驗組中應該始終具有相等數量的分數（數據點）。這種類型的設計和分析也可以稱為「相關ANOVA」或「內部主題ANOVA」。\n重複測量ANOVA的邏輯與獨立ANOVA（有時稱為「間題」ANOVA）非常相似。您可能還記得，我們之前顯示在一個間題ANOVA總變異性可以分為組間變異性（\\(SS_b\\)）和組內變異性（\\(SS_w\\)），在將每個變異性除以相應的自由度後得到MSb和MSw（見表13.1），F比值計算為：\n\\[F=\\frac{MS_b}{MS_w}\\]\n在重複測量ANOVA中，F比值的計算方式類似，但是在獨立ANOVA中，組內變異性（\\(SS_w\\)）被用作\\(MS_w\\)的分母，而在重複測量ANOVA中，\\(SS_w\\)被劃分為兩部分。由於我們在每個組中都使用相同的受試者，因此可以從組內變異性中移除受試者間個別差異（稱為SSsubjects）的變異性。我們不會深入討論這是如何實現的，但本質上，每個受試者都成為名為受試者的因子的一個水平。然後以與任何間題因子相同的方式計算此內部受試者因子中的變異性。然後我們可以將SSsubjects從\\(SS_w\\)中減去，以提供一個較小的SSerror項：\n\\[\\text{獨立ANOVA: } SS_{error} = SS_w\\] \\[\\text{重複測量ANOVA: } SS_{error} = SS_w - SS_{subjects}\\] 這個\\(SS_{error}\\)項的變化通常會導致統計檢驗更加強大，但這確實取決於\\(SS_{error}\\)的減少是否超過了誤差項自由度的減少（因為自由度從\\((n - k)\\)10變為\\((n - 1)(k - 1)\\)（請記住，獨立ANOVA設計中的受試者更多）。\n\n\n\n12.7.1 jamovi的重覆量數變異數分析\n首先，我們需要一些數據。(Geschwind1972表示?)，患者在中風後語言缺陷的確切性質可以用來診斷已受損的大腦特定區域。一位研究人員關心的是確定六位患有Broca失語症（中風後常見的語言缺陷）的患者所經歷的具體交流困難（Table 12.12）。\n\n\n\n\n\nTable 12.12:  中風患者單詞識別作業分數 \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\n患者需要完成三個單詞識別任務。在第一個（言語生成）任務中，患者需要重複研究者大聲朗讀的單詞。在第二個（概念性）任務中，旨在測試單詞理解能力，患者需要將一系列圖片與其正確名稱匹配。在第三個（語法）任務中，旨在測試正確單詞順序的知識，要求患者對語法不正確的句子進行重新排序。每位患者都完成了所有三個任務。患者嘗試任務的順序在參與者之間進行了平衡。每個任務包括一系列10次嘗試。每位患者成功完成的嘗試次數如 Table 12.11 所示。將這些數據輸入jamovi以進行分析（或者使用捷徑加載broca.csv文件）。\n要在jamovi中執行一個單因素相關ANOVA，打開一個單因素重複測量ANOVA對話框，如 Figure 12.9 中所示，通過ANOVA - Repeated Measures ANOVA進行。\n\n\n\n\n\n\nFigure 12.9: jamovi中的重複測量ANOVA對話框\n\n\n\n\n然後：\n\n輸入一個重複測量因子名稱。這應該是您選擇的標籤，用於描述所有參與者重複的條件。例如，要描述所有參與者完成的語音、概念和語法任務，一個合適的標籤是“任務”。請注意，這個新的因子名稱代表了分析中的自變量。\n在重複測量因子文本框中添加第三個級別，因為有三個級別代表三個任務：語音、概念和語法。相應地更改級別的標籤。\n然後將每個級別變量移動到重複測量單元文本框中。\n最後，在“假設檢查”選項下，選中“球形性檢查”文本框。\n\njamovi輸出一個單因素重複測量ANOVA，如 Figure 12.10 至 Figure 12.13 所示。我們應該首先查看的是Mauchly球形性檢驗，該檢驗測試各條件之間的差異方差是否相等（意味著研究條件之間的差異得分的分佈大致相同）。在@fig-fig12-10中，Mauchly檢驗的顯著性水平為\\(p = .720\\)。如果Mauchly檢驗的結果不顯著（即p > .05，正如此分析中的情況），那麼我們有理由得出差異的方差並無顯著差異（即它們大致相等，可以假定球形性。）。\n\n\n\n\n\n\n\nFigure 12.10: 單因子重複測量ANOVA輸出 - Mauchly球形性檢驗\n\n\n\n\n如果另一方面，Mauchly檢驗顯著（p < .05），那麼我們將得出差異方差之間存在顯著差異，並且未滿足球形性要求。在這種情況下，我們應該對單因素相關ANOVA分析中獲得的F值進行修正：\n\n如果”球形性檢驗”表中的Greenhouse-Geisser值> .75，那麼您應該使用Huynh-Feldt修正\n但如果Greenhouse-Geisser值< .75，那麼您應該使用Greenhouse-Geisser修正。\n\n這兩個修正過的F值都可以在“假設檢查”選項下的球形性修正復選框中指定，修正過的F值將顯示在結果表中，如 Figure 12.11 所示。\n\n\n\n\n\n\n\nFigure 12.11: 單因素重複測量ANOVA輸出 - 內部受試者效應檢驗\n\n\n\n\n在我們的分析中，我們發現Mauchly的球形性檢驗的顯著性為p = .720（即p > 0.05）。因此，這意味著我們可以假設已滿足球形性要求，因此無需對F值進行修正。因此，我們可以使用’無’球形性修正輸出值用於重複測量”任務”：\\(F = 6.93\\)，\\(df = 2\\)，\\(p = .013\\)，我們可以得出結論，語言任務中成功完成的測試次數確實會根據任務是語音、理解還是語法為基礎而顯著不同（\\(F(2, 10) = 6.93\\)，\\(p = .013\\)）。\n在jamovi中，與獨立ANOVA相同，也可以為重複測量ANOVA指定事後檢驗。結果顯示在 Figure 12.12 。這些表明語音和語法之間存在顯著差異，但其他級別之間沒有差異。\n\n\n\n\n\n\nFigure 12.12: 重複測量ANOVA中jamovi的事後檢驗\n\n\n\n\n描述性統計（邊際均值）可以用於幫助解釋結果，在jamovi輸出中生成，如 Figure 12.13 。通過比較參與者成功完成試驗的平均次數，可以看出布洛卡失語症患者在語音產生（平均= 7.17）和語言理解（平均= 6.17）任務上表現相對較好。然而，他們在語法任務上的表現明顯較差（平均= 4.33），事後檢驗中語音和語法任務表現之間存在顯著差異。\n\n\n\n\n\n\nFigure 12.13: 單因子重複測量ANOVA輸出-描述性統計"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#friedman無母數重覆量數變異數分析",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.8 Friedman無母數重覆量數變異數分析",
    "text": "12.8 Friedman無母數重覆量數變異數分析\nFriedman檢驗是一元重覆量數變數分析的非參數版本，可以在測試三個或更多組之間的差異時使用，其中每個組中的參與者相同，或者每個參與者與其他條件中的參與者密切匹配。如果因變項是序數，或者未滿足正態性假設，則可以使用Friedman檢驗。\n與Kruskall-Wallis檢驗一樣，基本數學知識很複雜，這裡不會介紹。對於本書的目的，僅需注意jamovi計算了Friedman檢驗的綁定修正版本，在 Figure 12.14 中有一個我們已經查看過的布洛卡失語症數據的示例。\n\n\n\n\n\n\nFigure 12.14: jamovi中的“重覆量數變數分析（非參數）”對話框和結果\n\n\n\n\n在jamovi中運行Friedman檢驗非常簡單。只需選擇分析 - ANOVA - 重覆量數變數分析（非參數），如 Figure 12.14 所示。然後將要比較的重複測量變項的名稱（語言、概念、語法）突顯並轉移到“測量：”文本框中。要為三個重複測量變項生成描述性統計（平均值和中位數），請單擊描述性按鈕。\njamovi結果顯示描述性統計、卡方值、自由度和p值（ Figure 12.14 ）。由於p值小於通常用於確定顯著性的水平（p < .05），我們可以得出結論，布洛卡失語症患者在語言生產（中位數= 7.5）和語言理解（中位數= 6.5）任務上表現相當好。然而，他們在語法任務上的表現明顯較差（中位數= 4.5），在事後檢驗中語言和語法任務表現之間存在顯著差異。"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#sec-On-the-relationship-between-ANOVA-and-the-Student-t-test",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.9 變異數分析與t檢定的關聯",
    "text": "12.9 變異數分析與t檢定的關聯\n在結束之前，我想指出的最後一點是，許多人對此感到驚訝，但了解它是很有價值的。具有兩個組別的ANOVA與學生t檢驗相同。不，真的。它們不僅相似，而且在每個有意義的方面實際上都是等效的。我不會試圖證明這總是成立，但我將給你展示一個具體的演示。假設，我們不對mood.gain ~ drug模型進行ANOVA，而是使用療法作為預測指標。如果我們運行此ANOVA，我們將得到一個F統計量 \\(F(1,16) = 1.71\\)，和一個 p值 = \\(0.21\\)。由於我們只有兩組，實際上我不需要求助於ANOVA，我可以選擇運行一個學生t檢驗。那麼，讓我們看看這樣做會發生什麼：我得到一個t統計量 \\(t(16) = -1.3068\\) 和一個 \\(p值 = 0.21\\)。好奇的是，p值是相同的。再一次，我們得到一個值 \\(p = .21\\)。但是，檢驗統計量呢？運行t檢驗而不是ANOVA，我們得到了一個略有不同的答案，即 \\(t(16) = -1.3068\\)。然而，這裡有一個相當直接的關係。如果將t統計量平方，我們就會得到之前的F統計量：\\(-1.3068^{2} = 1.7077\\)"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#本章小結",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.10 本章小結",
    "text": "12.10 本章小結\n這一章份量不少，但是有一些細節我並未提到12。最明顯的是在此並未討論處理不只一個分組變項的資料，我們在下一章 Chapter 13 將學習其中一部分。本章的學習重點有：\n\n理解變異數分析的運作原理 以及使用jamovi完成變異數分析\n學習如何計算變異數分析的效果量\n多重比較與事後檢定\n單因子變異數分析的執行條件\n同質性檢核 以及 校正異質性的分析結果\n常態性檢核以及排除非常態性的分析結果\n單因子重覆量數變異數分析 以及其無母數版本單因子重覆量數變異數分析\n\n\n\n\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances. Biometrika, 40, 318–335.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of variances. Journal of the American Statistical Association, 69, 364–367.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the American Statistical Association, 56, 52–64.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific American, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 6, 65–70.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47, 583–621.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et al (Ed.), Contributions to probability and statistics: Essays in honor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance: Fixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of Psychology, 46, 561–584.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An alternative approach. Biometrika, 38, 330–336."
  },
  {
    "objectID": "13-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "href": "13-Factorial-ANOVA.html#平衡且無交互作用的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.1 平衡且無交互作用的因子設計分析",
    "text": "13.1 平衡且無交互作用的因子設計分析\n當我們在 Chapter 12 中討論變異數分析時，我們假設了一個相對簡單的實驗設計。每個人都在幾個小組中，我們想知道這些小組在某些結果變量上的平均分數是否有所不同。在本節中，我將討論一個更廣泛的實驗設計類別，稱為因子設計，其中我們有多個分組變量。在上面給出了這種設計可能產生的一個例子。另一個例子出現在 Chapter 12 中，我們在其中研究了不同藥物對每個人的情緒增益的影響。在那一章中，我們確實發現了藥物的顯著影響，但在章節的最後，我們還進行了一個分析，以查看治療是否有影響。我們沒有找到，但是在試圖預測相同結果的兩個單獨分析中有點令人擔憂。也許治療對情緒增益確實有影響，但我們找不到它，因為它被藥物的影響“隱藏”了？換句話說，我們將要進行一個包括藥物和治療作為預測因子的單一分析。對於這種分析，每個人都按照他們給定的藥物（具有3個水平的因子）和接受的治療（具有2個水平的因子）進行交叉分類。我們將此稱為\\(3 \\times 2\\)因子設計。\n如果我們用jamovi（見 Section 6.1 ）中的“頻率” - “應急表”分析交叉制表藥物和治療，我們將獲得在 Figure 13.1 中顯示的表格。\n\n\n\n\n\n\n\nFigure 13.1: jamovi藥物與治療的應急表\n\n\n\n\n如您所見，我們不僅有與兩個因子的所有可能組合相對應的參與者，表明我們的設計是完全交叉，而且事實上每個組中都有相等數量的人。換句話說，我們擁有一個平衡設計。在本節中，我將談論如何分析來自平衡設計的數據，因為這是最簡單的情況。對於不平衡設計的情況相當繁瑣，所以我們暫時將其擱置。\n\n\n13.1.1 多因子設計是因應什麼樣的假設？\n就像單因子變異數分析一樣，因子變異數分析是一個用於測試關於母體均值的某些類型假設的工具。因此，一個明智的開始方式是明確我們的假設實際上是什麼。然而，在我們甚至到達這一點之前，有一個簡單清晰的表示法來描述母體均值是非常有用的。由於觀察是根據兩個不同因子進行交叉分類的事實，可能有很多不同的均值會引起我們的興趣。為了理解這一點，讓我們首先考慮在這種設計中可以計算出所有不同樣本均值。首先，很明顯，我們可能對此類組均值感興趣（ Table 13.1 ）。\n\n\n\n\n\nTable 13.1:  藥物和治療組的組均值在* clinicaltrial.csv *數據中 \n\ndrugtherapymood.gain\n\nplacebono.therapy0.30\n\nanxifreeno.therapy0.40\n\njoyzepamno.therapy1.47\n\nplaceboCBT0.60\n\nanxifreeCBT1.03\n\njoyzepamCBT1.50\n\n\n\n\n\n接下來，下表（ Table 13.2 ）顯示了兩個因子所有可能組合的組均值列表（例如，接受安慰劑且未接受治療的人、接受安慰劑並接受CBT的人等）。將所有這些數字，以及邊際和總體均值，整合到一個單一的表格中是非常有幫助的，這個表格看起來是這樣的：\n\n\n\n\n\n\nTable 13.2:  藥物和治療組的組和總體均值在clintrial.csv數據中 \n\nno therapyCBTtotal\n\nplacebo0.300.600.45\n\nanxifree0.401.030.72\n\njoyzepam1.471.501.48\n\ntotal0.721.040.88\n\n\n\n\n\n現在，這些不同的均值當然是樣本統計量。它是一個與我們在研究過程中所做的具體觀察相關的數量。我們想要對應的母體參數進行推斷。也就是說，真實的均值是在某個更廣泛的母體內存在的。這些母體均值也可以整理成一個類似的表格，但是我們需要一些數學符號來表示（Table 13.3）。像往常一樣，我將使用符號\\(\\mu\\)來表示母體均值。然而，由於有很多不同的均值，我需要使用下標來區分它們。\n這裡是符號如何運作的。我們的表格是根據兩個因子定義的。每行對應於因子A（在本例中為藥物）的不同水平，每列對應於因子B（在本例中為治療）的不同水平。如果我們讓R表示表格中的行數，並讓\\(C\\)表示列數，我們可以將其稱為\\(R \\times C\\)因子變異數分析。在這種情況下\\(R = 3\\)和\\(C = 2\\)。我們將使用小寫字母來表示特定的行和列，因此\\(\\mu_{rc}\\)表示與因子\\(A\\)的第\\(r\\)級（即第\\(r\\)行）和因子B的第\\(c\\)級（第c列）相關的母體均值。1 所以現在母體的均值是寫成@tbl-tab13-1的形式：\n\n\n\n\n\n\nTable 13.3:  因子表中母體均值的符號表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\n\ntotal\n\n\n\n\n\n好的，那剩下的項目呢？例如，我們應該如何描述在這樣一個實驗中可能被給予Joyzepam的整個（假設的）人群的平均情緒提升，而不管他們是否接受了CBT治療？我們使用“點”符號來表示這一點。在Joyzepam的例子中，注意到我們正在討論表中第三行相關的均值。也就是說，我們將兩個單元格的均值（即\\(\\mu_{31}\\)和\\(\\mu_{32}\\)）求平均。這個求平均的結果被稱為邊際均值，並在這種情況下表示為\\(\\mu_3.\\)。CBT的邊際均值對應於表中第二列相關的母體均值，因此我們使用表示法，因為它是通過平均（邊際化2）兩者而得到的均值。因此，我們的整個母體均值表格可以寫成@tbl-tab13-4。\n\n\n\n\n\n\nTable 13.4:  因子表中母體和總體均值的符號表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\mu_{11} \\)\\( \\mu_{12} \\)\\( \\mu_{1.} \\)\n\nanxifree\\( \\mu_{21} \\)\\( \\mu_{22} \\)\\( \\mu_{2.} \\)\n\njoyzepam\\( \\mu_{31} \\)\\( \\mu_{32} \\)\\( \\mu_{3.} \\)\n\ntotal\\( \\mu_{.1} \\)\\( \\mu_{.2} \\)\\( \\mu_{..} \\)\n\n\n\n\n\n現在我們有了這個表示法，很容易就可以形成和表達一些假設。假設目標是找出兩件事。首先，藥物的選擇是否對情緒有影響？其次，CBT 是否對情緒有影響？當然，這些不是我們可以制定的唯一假設，並且我們將在[因子 ANOVA 2：平衡設計，允許交互作用]一節中看到一個不同類型假設的非常重要示例，但這兩個假設是最簡單的檢驗，所以我們從這裡開始。考慮第一個檢驗。如果藥物沒有影響，那麼我們應該期望所有行均值相同，對吧？所以那就是我們的虛無假設。另一方面，如果藥物確實有關，那麼我們應該期望這些行均值不同。形式上，我們將虛無假設和替代假設表示為邊際均值的相等性：\n\\[\\text{虛無假設, } H_0 \\text{: 行均值相同，即 } \\mu_{1. } = \\mu_{2. } = \\mu_{3. }\\]\n\\[\\text{替代假設, } H_1 \\text{: 至少有一個行均值不同}\\]\n值得注意的是，這些與我們在 Chapter 12 中對這些數據進行單因素 ANOVA 時形成的統計假設完全相同。當時我使用符號 \\(\\mu_{P}\\) 來表示安慰劑組的平均情緒增益，\\(\\mu_{A}\\) 和 \\(\\mu_{J}\\) 分別對應兩種藥物的組均值，並且虛無假設是 \\(\\mu_{P} = \\mu_{A} = \\mu_{J}\\)。所以我們實際上在談論相同的假設，只不過由於存在多個分組變量，更複雜的 ANOVA 需要更仔細的表示法，因此我們現在將此假設表示為 \\(\\mu_{ 1.} = \\mu_{ 2.} = \\mu_{ 3.}\\)。然而，正如我們將很快看到的那樣，儘管假設相同，但由於我們現在承認了第二個分組變量的存在，對該假設的檢驗存在微妙的不同。\n談到其他分組變量，你可能不會感到驚訝地發現，我們的第二個假設檢驗也以相同的方式制定。然而，由於我們談論的是心理治療而不是藥物，我們的虛無假設現在對應於列均值的相等：\n\\[\\text{虛無假設, } H_0 \\text{: 列均值相同，即， } \\mu_{ .1} = \\mu_{ .2} \\] \\[\\text{替代假設, } H_1 \\text{: 列均值不同，即， } \\mu_{ .1} \\neq \\mu_{ .2}\\]\n\n\n\n13.1.2 使用jamovi完成多因子變異數分析\n我在上一節中描述的虛無假設和替代假設應該讓你感到非常熟悉。它們基本上與我們在 Chapter 12 中執行的更簡單的單因素ANOVA檢驗相同。所以你可能期望因子ANOVA中使用的假設檢驗本質上與 Chapter 12 中的F檢驗相同。您期望看到對平方和（SS）、平均平方（MS）、自由度（df）以及最終可以將之轉換為p值的F統計量的引用，對吧？好吧，你絕對是對的。 這麼多，以至於我要改變我的常規方法。在本書中，我通常採用先描述特定分析的基礎邏輯（在某程度上還有數學），然後再介紹jamovi中的分析的方法。這次我要相反地做，先告訴你如何在jamovi中執行它。這樣做的原因是我想強調 Chapter 12 中討論的簡單的單因素ANOVA工具，以及我們將在本章中使用的更複雜的方法之間的相似之處。\n如果您試圖分析的數據對應於平衡的因子設計，那麼執行方差分析就很容易。要了解它有多容易，讓我們先重現 Chapter 12 中的原始分析。以防你忘了，對於那個分析，我們只使用一個因素（即藥物）來預測我們的結果變量（即mood.gain），並且得到了 Figure 13.2 中顯示的結果。\n\n\n\n\n\n\nFigure 13.2: 藥物對 mood.gain 的單因子變異分析\n\n\n\n\n現在，假設我也好奇心理治療是否與mood.gain有關。根據我們在 Chapter 10 中對多元回歸的討論，你可能不會感到意外，我們所要做的就是在分析中將治療作為第二個”固定因素”，如 Figure 13.3 所示。 \n\n\n\n\n\nFigure 13.3: 藥物和心理治療對 mood.gain 的雙因子變異數分析\n\n\n\n\n這個輸出也很容易閱讀。表格的第一行報告了與藥物因素相關的組間平方和（SS）值，以及相應的組間 df 值。它還計算了平均平方（MS）、F統計量和p值。還有一行對應於心理治療因素和一行對應於殘差（即組內變異）。\n所有的單個量都非常熟悉，這些不同量之間的關係也保持不變，就像我們在原始單因素ANOVA中看到的一樣。注意，平均平方值是通過將\\(SS\\)除以相應的\\(df\\)來計算的。也就是說，無論我們談論的是藥物、治療還是殘差，都還是\n\\[MS=\\frac{SS}{df}\\]\n為了看到這一點，讓我們不要擔心平方和值是如何計算的。相反，讓我們相信 jamovi 已經正確計算了 \\(SS\\) 值，並嘗試驗證所有其他數字是否合理。首先，注意對於藥物因素，我們將 \\(3.45\\) 除以 \\(2\\)，得到平均平方值為 \\(1.73\\)。對於心理治療因素，只有1個自由度，所以我們的計算更簡單：將 \\(0.47\\)（\\(SS\\) 值）除以1，得到答案為 \\(0.47\\)（\\(MS\\) 值）。\n轉向 F 統計量和 p 值，注意我們有兩個；一個對應藥物因素，另一個對應心理治療因素。無論我們談論的是哪一個，F 統計量都是將與因素相關的平均平方值除以與殘差相關的平均平方值。如果我們用 “A” 作為簡寫符號來指代第一個因素（因素 A；在本例中是藥物），用 “R” 作為簡寫符號來指代殘差，那麼與因素 A 相關的 F 統計量表示為 \\(F_A\\)，並按如下方式計算：\n\\[F_A=\\frac{MS_A}{MS_R}\\]\n因素 B（即心理治療）也有等效公式。注意，這裡使用 “R” 來指代殘差有點尷尬，因為我們也用字母 R 來指代表格中的行數，但我只會在 SSR 和 MSR 的上下文中用 “R” 表示殘差，所以希望這不會令人困惑。無論如何，將這個公式應用於藥物因素，我們將平均平方值 1.73 除以殘差平均平方值 \\(0.07\\)，得到 F 統計量為 26.15。對於心理治療變量的相應計算將是將 \\(0.47\\) 除以 \\(0.07\\)，得到 \\(7.08\\) 作為 F 統計量。當然，這些值與 jamovi 在上面的 ANOVA 表中報告的值相同。\n同樣在 ANOVA 表中的是 p 值的計算。再次，這裡沒有什麼新鮮事物。對於我們的兩個因素，我們試圖做的是測試關於因素與結果變量之間沒有關係的虛無假設（稍後我將更加明確）。為此，我們（顯然）遵循了類似於單因素 ANOVA 的策略，為每個假設計算了一個 F 統計量。要將這些轉換為 p 值，我們只需要注意，在虛無假設下（即所謂因素無關）的 F 統計量的抽樣分佈是一個 F 分佈。還要注意，兩個自由度值分別對應於因素和殘差。對於藥物因素，我們談論的是具有 2 和 14 自由度的 F 分佈（稍後我將更詳細地討論自由度）。相反，對於心理治療因素，抽樣分佈是具有 1 和 14 自由度的 F 分佈。\n在這一點上，我希望您能看到，這個更複雜的因子分析的 ANOVA 表應該以與較簡單的單因素分析的 ANOVA 表相同的方式進行閱讀。簡而言之，它告訴我們，我們的 \\(3 \\times 2\\) 設計的因子 ANOVA 發現藥物的顯著效應（\\(F_{2,14} = 26.15, p < .001\\)）以及心理治療的顯著效應（\\(F_{1,14} = 7.08, p = .02\\)）。或者，使用更技術正確的術語，我們會說藥物和心理治療有兩個主要效果。此刻，將這些稱為“主要”效果似乎有點多餘，但實際上是有道理的。稍後，我們將討論兩個因素之間可能存在的“交互作用”，因此我們通常區分主要效果和交互作用。\n\n\n\n\n13.1.3 計算多因子變異數分析的平方差總和\n在上一節中，我有兩個目標。首先，向您顯示執行因子 ANOVA 所需的 jamovi 方法與我們用於單因素 ANOVA 的方法幾乎相同。唯一的區別是添加了第二個因素。其次，我想向您展示在這種情況下 ANOVA 表的樣子，以便您從一開始就可以看到因子 ANOVA 背後的基本邏輯和結構與支撐單因素 ANOVA 的那些相同。試著抱著這種感覺。這是真實的，因為因子 ANOVA 的建立方式與更簡單的單因素 ANOVA 模型大致相同。只是一旦您開始挖掘細節，這種熟悉的感覺就會消失。傳統上，這種令人欣慰的感覺會被向統計教科書作者傾訴怒氣的衝動所替代。\n好的，讓我們先看看其中一些細節。上一節中的解釋表明了主效應（本例中為藥物和心理治療）的假設檢驗是 F 檢驗，但它沒有顯示如何計算平方和（SS）值。也沒有明確告訴您如何計算自由度（df 值），儘管相比之下這是一個簡單的事情。現在讓我們假設我們只有兩個預測變量，因子 A 和因子 B。如果我們用 Y 來表示結果變量，那麼我們可以用 Yrci 來表示與 rc 組的第 i 位成員相關的結果（即因子 A 的第 r 級/行和因子 B 的第 c 級/列）。因此，如果我們用 \\(\\bar{Y}\\) 表示樣本均值，我們可以使用與之前相同的表示法來表示組均值、邊際均值和總均值。也就是說，\\(\\bar{Y}_{rc}\\) 是與因子 A 的第 r 級和因子 B 的第 c 級相關的樣本均值：\\(\\bar{Y}_{r.}\\) 將是因子 A 的第 r 級的邊際均值，\\(\\bar{Y}_{.c}\\) 將是因子 B 的第 c 級的邊際均值，\\(\\bar{Y}_{..}\\) 是總體均值。換句話說，我們的樣本均值可以按照與母體均值相同的表格進行組織。對於我們的臨床試驗數據，該表格如@tbl-tab13-5所示。\n\n\n\n\n\n\nTable 13.5:  臨床試驗數據的樣本均值表示法 \n\nno therapyCBTtotal\n\nplacebo\\( \\bar{Y}_{11} \\)\\( \\bar{Y}_{12} \\)\\( \\bar{Y}_{1.} \\)\n\nanxifree\\( \\bar{Y}_{21} \\)\\( \\bar{Y}_{22} \\)\\( \\bar{Y}_{2.} \\)\n\njoyzepam\\( \\bar{Y}_{31} \\)\\( \\bar{Y}_{32} \\)\\( \\bar{Y}_{3.} \\)\n\ntotal\\( \\bar{Y}_{.1} \\)\\( \\bar{Y}_{.2} \\)\\( \\bar{Y}_{..} \\)\n\n\n\n\n\n如果我們查看之前顯示的樣本均值，我們有 \\(\\bar{Y}_{11} = 0.30\\)，\\(\\bar{Y}_{12} = 0.60\\) 等。在我們的臨床試驗示例中，藥物因子有 3 個水平，治療因子有 2 個水平，因此我們試圖執行的是一個 \\(3 \\times 2\\) 因子方差分析。然而，我們將更一般地說，因子 A（行因子）有 R 個水平，因子 B（列因子）有 C 個水平，因此我們在這裡執行的是一個 \\(R \\times C\\) 因子方差分析。\n[額外的技術細節3]\n\n\n\n\n13.1.4 計算自由度的規則\n自由度的計算方式與單因素變異數分析非常相似。對於任何給定因子，自由度等於級別數減 1（即，行變量因子 A 的 \\(R - 1\\)，列變量因子 B 的 \\(C - 1\\)）。因此，對於藥物因子，我們得到 \\(df = 2\\)，對於療法因子，我們得到 \\(df = 1\\)。稍後，在我們討論將 ANOVA 解釋為回歸模型時（參見 Section 13.6），我將更清楚地說明我們如何得出此數字。但就目前而言，我們可以使用自由度的簡單定義，即自由度等於觀察到的數量數目減去約束數目。因此，對於藥物因子，我們觀察到 3 個單獨的組平均值，但這些受到 1 個總平均值的約束，因此自由度為 2。對於殘差，邏輯相似但不完全相同。我們實驗中的總觀察次數是 18。約束對應於 1 個總平均值，藥物因子引入的 2 個額外組平均值，以及療法因子引入的 1 個額外組平均值，因此我們的自由度為 14。作為公式，這是 \\(N - 1 - (R - 1) - (C - 1)\\)，簡化後是 \\(N - R - C + 1\\)。\n\n\n\n13.1.5 多因子與單因子變異數分析\n既然我們已經了解了因子變異數分析（factorial ANOVA）的運作方式，那麼花一點時間將其與單因素分析的結果進行比較是值得的，因為這將讓我們真正理解為什麼進行因子變異數分析是個好主意。在 Chapter 12 中，我進行了一個單因素變異數分析，以查看藥物之間是否存在差異，並進行了第二個單因素變異數分析，以查看療法之間是否存在差異。正如我們在 Section 13.1.1 節中看到的，單因素變異數分析所檢驗的零假設和對立假設實際上與因子變異數分析所檢驗的假設相同。更仔細地查看變異數分析表，我們可以看到，在兩種不同的分析中，與因子相關的平方和是相同的（藥物為 3.45，療法為 0.92），自由度也是相同的（藥物為 2，療法為 1）。但它們的答案並不相同！最值得注意的是，當我們在 Section 12.9 中對療法進行單因素變異數分析時，我們沒有發現顯著效應（p 值為 .21）。然而，當我們在兩因素變異數分析的背景下查看療法的主效應時，我們確實得到了顯著效應（p = .019）。這兩種分析顯然不同。\n為什麼會發生這種情況？答案在於理解殘差是如何計算的。回想一下 F 檢驗背後的整個概念是將可以歸因於特定因子的變異性與無法解釋的變異性（殘差）進行比較。如果您對療法進行單因素變異數分析，因此忽略了藥物的影響，那麼變異數分析將把所有藥物引起的變異性放入殘差中！這會使數據看起來比實際情況更加嘈雜，而在兩因素變異數分析中被正確認為顯著的療法效果現在變得不顯著。如果我們在試圖評估其他事物（例如，療法）的貢獻時忽略了實際上有意義的事物（例如，藥物），那麼我們的分析將受到扭曲。當然，如果我們記錄了墻壁的顏色，並且在三因素變異數分析中發現這是一個無關緊要的因素，那麼完全可以忽略它，僅報告不包括這個無關因素的更簡單的兩因素變異數分析。您不應該放棄那些確實有所作為的變數！\n\n總之，比較因子變異數分析與單因素變異數分析的結果可以使我們更好地理解為什麼進行因子變異數分析是一個好主意。當我們忽略了真正重要的變數時，分析將受到扭曲。因此，為了確保我們的分析更加準確，我們應該在分析中包括所有具有重要影響的變數。\n\n\n\n\n\n13.1.6 解讀多因子變異數分析的結果\n迄今為止我們討論的變異數分析模型涵蓋了我們可能在數據中觀察到的各種不同模式。例如，在兩因素變異數分析設計中有四種可能性：（a）只有因素A有意義，（b）只有因素B有意義，（c）A和B都有意義，（d）A和B都無意義。 ?fig-fig13-4中繪製了這四種可能性的示例。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "href": "13-Factorial-ANOVA.html#平衡且有交互作用的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.2 平衡且有交互作用的因子設計分析",
    "text": "13.2 平衡且有交互作用的因子設計分析\n?fig-fig13-4中顯示的四種數據模式都非常現實。有很多數據集正好產生這些模式。然而，它們並非全部故事，到目前為止我們一直在談論的變異數分析模型並不足以充分解釋一個組均值表格。為什麼呢？嗯，到目前為止，我們可以討論藥物如何影響心情，以及治療如何影響心情，但無法談論兩者之間可能存在的交互作用。只有當因素\\(A\\)的效果因為我們討論的因素\\(B\\)的水平而有差異時，我們才說\\(A\\)和\\(B\\)之間存在交互作用。?fig-fig13-5中顯示了在$2 $ ANOVA環境下的幾個交互作用效果實例。舉一個更具體的例子，假設Anxifree和Joyzepam的運作受到完全不同的生理機制控制。這導致了一個結果，即儘管在接受治療的情況下，Joyzepam對心情的影響基本相同，但與CBT一起使用時，Anxifree實際上更有效。我們在上一節中開發的ANOVA無法捕捉到這個想法。要判斷此處是否確實存在交互作用，最好繪製各個組的均值。在jamovi中，這是通過ANOVA的“估計邊際均值”選項完成的——只需將藥物和治療移至“條款1”下的“邊際均值”框中。這應該看起來像@fig-fig13-6。我們的主要關注點與這兩條線不平行的事實有關。當藥物為Joyzepam（右側）時，CBT的效果（實線與虛線之間的差異）似乎接近零，甚至比使用安慰劑時（左側）的CBT效果還要小。然而，當給予Anxifree時，CBT的效果大於安慰劑（中間）。這種效果是真實的，還是僅僅由於機會引起的隨機變化？我們最初的ANOVA無法回答這個問題，因為我們根本不允許交互作用的存在！在本節中，我們將解決這個問題。\n\n\n\n\n\n\n\nFigure 13.4: 在沒有交互作用的情況下，\\(2 \\times 2\\) ANOVA的四種不同結果。在面板（a）中，我們看到因子A的主要效應以及因子B的無效應。面板（b）顯示因子B的主要效應，但因子A沒有影響。面板（c）顯示因子A和因子B的主要影響。最後，面板（d）顯示兩個因子都沒有影響。\n\n\n\n\n\n\n\n\n\nFigure 13.5: \\(2 \\times 2\\) ANOVA中質量上不同的交互作用\n\n\n\n\n\n\n\n\n\nFigure 13.6: 使用臨床試驗數據的ANOVA中，jamovi屏幕顯示如何生成描述性交互作用圖\n\n\n\n\n\n13.2.1 交互作用代表什麼？\n本節我們要介紹的關鍵概念是交互作用效應。在我們迄今為止討論的ANOVA模型中，我們的模型中只有兩個因素（即藥物和治療）。但是，當我們添加交互作用時，我們在模型中添加了一個新的組件：藥物和治療的組合。直觀地說，交互作用效應的概念相當簡單。這只是意味著因子A的效應會因為我們談論的因子B的不同水平而有所不同。但是，在我們的數據方面，這實際上意味著什麼呢？ ?fig-fig13-5中的圖表描述了幾種不同的模式，儘管它們彼此相當不同，但它們都被視為交互作用效應。因此，將這個質的概念轉化為統計學家可以使用的數學概念並不完全簡單。\n[附加技術細節4]\n\n\n\n\n13.2.2 交互作用的自由度\n將交互作用納入計算後，計算自由度變得稍微複雜一些。首先，讓我們考慮整個ANOVA模型。一旦我們在模型中包括交互效應，我們允許每個單獨組具有唯一的平均值，\\(mu_{rc}\\)。對於一個\\(R \\times C\\)的因子ANOVA，這意味著模型中有\\(R \\times C\\)個感興趣的數量，並且只有一個約束：所有組均值需要平均為總體均值。因此，整個模型需要有(\\(R \\times C\\)) - 1個自由度。但是因子A的主效應具有\\(R-1\\)個自由度，因子B的主效應具有\\(C-1\\)個自由度。這意味著與交互作用相關的自由度為\n\\[\n\\begin{aligned}\ndf_{A:B} & = (R \\times C - 1) - (R - 1) - (C - 1) \\\\\n& = RC - R - C + 1 \\\\\n& = (R-1)(C-1)\n\\end{aligned}\n\\]\n這只是與行因子和列因子相關的自由度之積。\n那剩餘自由度呢？由於我們添加了吸收一些自由度的交互項，剩餘的自由度變得更少。具體來說，請注意，如果具有交互作用的模型總共有\\((R \\times C) - 1\\)，並且在數據集中有\\(N\\)個受1個總體均值約束的觀察值，那麼您的剩餘自由度現在變為\\(N-(R \\times C)-1+1\\)，或者只是\\(N-(R \\times C)\\)。\n\n\n\n13.2.3 使用jamovi完成多因子變異數分析\n在jamovi中將交互項添加到ANOVA模型非常簡單。實際上，這不僅簡單，而且是ANOVA的默認選項。這意味著當您為ANOVA指定兩個因子時，例如藥物和療法，則交互組件 - 藥物\\(\\times\\)療法 - 會自動添加到模型中5。當我們將交互項納入ANOVA運行後，我們將得到@fig-fig13-7中顯示的結果。\n\n\n\n\n\n\nFigure 13.7: 包括交互組件藥物\\(\\times\\)療法的完整因子模型結果\n\n\n\n\n結果顯示，儘管我們確實對藥物有顯著的主效應（\\(F_{2,12} = 31.7, p < .001\\)）和療法類型（\\(F_{1,12} = 8.6, p = .013\\)），但兩者之間沒有顯著的交互作用（\\(F_{2,12} = 2.5, p = 0.125\\)）。\n\n\n\n13.2.4 解讀分析結果\n在解釋因子ANOVA結果時，有幾個非常重要的事情需要考慮。首先，我們在單因子ANOVA中遇到的相同問題，即如果您獲得（例如）藥物的顯著主效應，它並不能告訴您哪些藥物彼此有差異。要找出這個答案，您需要進行額外的分析。稍後我們將討論可以在[指定對比方式的不同方法]和[事後檢驗]中運行的一些分析。對於交互作用效果也是如此。知道有顯著的交互作用並不能告訴您存在哪種類型的交互作用。同樣，您需要進行額外的分析。\n其次，在獲得顯著的交互作用效果但沒有相應主效應的情況下，會出現非常奇特的解釋問題。有時候會發生這種情況。例如，在@fig-fig13-5 a中顯示的交叉互動中，這正是您會發現的情況。在這種情況下，主效應都不顯著，但交互作用效果顯著。這是一個難以解釋的情況，人們通常對此感到困惑。統計學家在這種情況下喜歡給出的一般建議是，當存在交互作用時，您不應該過多地關注主效應。他們這樣說的原因是，雖然從數學的角度看，主效應的檢驗完全有效，但是當存在顯著的交互作用效果時，主效應很少檢驗有趣的假設。回想一下@sec-What-hypotheses-are-we-testing，主效應的虛無假設是邊際均值彼此相等，邊際均值是由幾個不同組的平均值形成的。但是，如果您有一個顯著的交互作用效果，那麼您就知道組成邊際均值的組並不是同質的，所以真的不明顯為什麼您會關心那些邊際均值。\n以下是我的意思。再次以臨床實例為例。假設我們有一個\\(2 \\times 2\\)設計，比較了兩種不同的恐懼症治療方法（例如，系統性緩解法和淹沒療法），以及兩種不同的減輕焦慮藥物（例如，Anxifree和Joyzepam）。現在，假設我們發現當緩解法是治療方法時，Anxifree無效；當淹沒療法是治療方法時，Joyzepam無效。但對於另一種治療方法，兩者都相當有效。這是一個典型的交叉互動，當我們運行ANOVA時，我們會發現沒有藥物的主要效果，但有顯著的互動。那麼，說沒有主效應究竟意味著什麼呢？那意味著，如果我們平均兩種不同的心理治療方法，那麼Anxifree和Joyzepam的平均效果是相同的。但是，有誰會在意這個呢？在治療恐懼症時，從來沒有一個人可以使用“平均”的淹沒療法和緩解法進行治療。這並不合理。您要么得到一個，要么得到另一個。對於一種治療方法，一種藥物是有效的，對於另一種治療方法，另一種藥物是有效的。交互作用是重要的，而主效應在某種程度上是無關緊要的。\n這樣的事情經常發生。主效應是邊際均值的檢驗，當交互作用存在時，我們經常會發現自己對邊際均值不感興趣，因為它們意味著在交互作用告訴我們不應該取平均值的事物上取平均值！當然，並非總是在存在交互作用的情況下，主效應就毫無意義。經常出現的情況是，主效應很大，交互作用很小，這種情況下，您仍然可以說類似於“藥物A通常比藥物B更有效”（因為藥物效果很大），但您需要對其進行一些修改，添加“對於不同的心理治療，有效性差異有所不同。”無論如何，這裡的主要觀點是，每當您獲得顯著的交互作用時，您應該停下來思考主效應在這個語境中的真正意義。不要自動假設主效應是有趣的。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#變異數分析的效果量",
    "href": "13-Factorial-ANOVA.html#變異數分析的效果量",
    "title": "13  多因子變異數分析",
    "section": "13.3 變異數分析的效果量",
    "text": "13.3 變異數分析的效果量\n對於因子分析變異數分析（factorial ANOVA），效應量計算與單因素變異數分析中使用的非常相似（參見[效應量]部分）。具體來說，我們可以使用 \\(\\eta^2\\)（eta 平方）作為簡單衡量任何特定條款的整體效應大小的方法。與以前一樣，\\(\\eta^2\\)是通過將與該條款相關的平方和除以總平方和來定義的。例如，要確定因子A主效應的大小，我們將使用以下公式：\n\\[\\eta_A^2=\\frac{SS_A}{SS_T}\\]\n與以前一樣，這可以以與回歸中的 \\(R^2\\) 類似的方式進行解釋。6 它告訴您由因子A的主效應解釋的結果變量變異的比例。因此，這是一個從0（完全沒有效果）到1（解釋結果變異的全部）的範圍內的數字。此外，所有\\(\\eta^2\\)值的總和，跨越模型中的所有條款，將總和為ANOVA模型的總\\(R^2\\)。例如，如果ANOVA模型完美適合（即，根本沒有組內變異！），則\\(\\eta^2\\)值將總和為1。當然，這在現實生活中很少（如果有的話）發生。\n然而，在進行因子分析變異數分析時，人們喜歡報告的效應量有第二種衡量方法，稱為部分 \\(\\eta^2\\)。部分 \\(\\eta^2\\)（有時表示為\\(p^{\\eta^2}\\) 或\\(\\eta_p^2\\)）背後的想法是，當衡量特定條款的效應量時（例如，因子A的主效應），您希望刻意忽略模型中的其他效應（例如，因子B的主效應）。也就是說，您想假設所有這些其他條款的效應都為零，然後計算\\(\\eta^2\\)值本來是什麼。這實際上很容易計算。您所要做的就是從分母中移除與其他條款相關的平方和。換句話說，如果您想要因子A主效應的部分\\(\\eta^2\\)，分母就是因子A和殘差的SS值之和。\n\\[\\text{partial }\\eta_A^2= \\frac{SS_A}{SS_A+SS_R}\\]\n這將始終給您一個比\\(\\eta^2\\)更大的數字，這我猜想是部分\\(\\eta^2\\)受歡迎的原因。再次，您得到一個介於0和1之間的數字，其中0表示沒有影響。然而，解釋較大的部分\\(\\eta^2\\)值意味著什麼，這有點棘手。尤其是，您實際上無法比較不同條款的部分\\(\\eta^2\\)值！例如，假設根本沒有組內變異：如果是這樣，\\(SS_R = 0\\)。這意味著每個條款的部分\\(\\eta^2\\)值都是1。但這並不意味著模型中的所有條款都同樣重要，或者它們的大小相同。這只是意味著模型中所有條款的效應量相對於殘差變化都很大。它無法跨條款進行比較。\n要了解我的意思，查看具體示例非常有用。首先，讓我們看一下原始ANOVA（Table 13.6）中的效應量，無交互作用條款，來自@fig-fig13-3。\n\n\n\n\n\n\nTable 13.6:  ANOVA模型未包括交互作用項目的效應量 \n\neta.sqpartial.eta.sq\n\ndrug0.710.79\n\ntherapy0.100.34\n\n\n\n\n\n首先觀察\\(\\eta^2\\)值，我們可以看到藥物解釋了心情改善的71%變異（即\\(\\eta^2 = 0.71\\)），而治療僅解釋了10%。這使得總共有19%的變異未被解釋（即，殘差佔結果變異的19%）。整體來說，這意味著我們有非常大的藥物效應[^factorial-anova-7]和適中的治療效應。\n[^factorial-anova-7]：我認為這個數值大得令人難以置信。這個數據集的人工特徵現在真的開始顯露出來了！\n現在讓我們看看部分\\(\\eta^2\\)值，如@fig-fig13-3所示。由於治療的效果並不是很大，因此對其進行控制並不會產生很大的差異，所以藥物的部分\\(\\eta^2\\)不會增加很多，我們得到一個值\\(p^{\\eta^2} = 0.79\\)。相反，因為藥物的效果非常大，對其進行控制會產生很大的差異，因此當我們計算治療的部分\\(\\eta^2\\)時，可以看到它上升到\\(p^{\\eta^2} = 0.34\\)。我們必須問自己的問題是，這些部分\\(\\eta^2\\)值實際上意味著什麼？我通常將因子A主效應的部分\\(\\eta^2\\)解釋為關於僅因子A變化的假設實驗的說明。所以，即使在這個實驗中我們改變了A和B，我們可以很容易地想像一個僅因子A變化的實驗，部分\\(\\eta^2\\)統計數據告訴您在該實驗中預期結果變量的多少變異會被解釋。然而，應該注意的是，這種解釋，就像許多與主效應相關的事物一樣，在存在大的和顯著的交互作用效應時沒有太多意義。\n說到交互作用效應，?tbl-tab13-7顯示了我們在包含交互作用條款的模型中計算效應大小時會得到什麼結果，如@fig-fig13-7所示。如您所見，主效應的\\(\\eta^2\\)值沒有改變，但部分\\(\\eta^2\\)值發生了變化：\n\n\n\n\n\nTable 13.7:  ANOVA模型包含交互作用項目的效應大小 \n\neta.sqpartial.eta.sq\n\ndrug0.710.84\n\ntherapy0.100.42\n\ndrug*therapy0.060.29\n\n\n\n\n\n\n13.3.1 估計組間平均\n在許多情況下，您可能會想要根據ANOVA結果報告所有組均值的估計以及與之相關的置信區間。您可以使用jamovi ANOVA分析中的“估計邊際均值”選項來執行此操作，如@fig-fig13-8所示。如果您運行的ANOVA是一個飽和模型（即，包含所有可能的主效應和所有可能的交互作用效應），那麼組均值的估計實際上與樣本均值相同，儘管置信區間將使用標準誤差的合併估計而不是為每個組使用單獨的估計。\n\n\n\n\n\n\nFigure 13.8: jamovi屏幕截圖顯示了飽和模型的邊際均值，即包括臨床試驗數據集中的交互作用分量\n\n\n\n\n在輸出中，我們看到安慰劑組無治療時的估計平均情緒增益為\\(0.300\\)，\\(95\\%\\)置信區間從\\(0.006\\)到\\(0.594\\)。請注意，由於ANOVA模型假定方差同質性並因此使用方差的合併估計，這些置信區間與您單獨為每個組計算的置信區間不同。\n當模型不包含交互作用項時，估計的組均值將與樣本均值不同。jamovi將根據邊際均值（即假定無交互作用）計算預期的組均值，而不是報告樣本均值。使用我們之前開發的符號，報告了μrc，對於（行）因子A上的第r級和（列）因子B上的第c級的均值，將是\\(\\mu_{..} + \\alpha_r + \\beta_c\\)。如果兩個因素之間確實沒有交互作用，那麼這實際上比原始樣本均值更好的估計了母體均值。通過jamovi ANOVA分析中的“Model”選項從模型中刪除交互作用項，為@fig-fig13-9所示的分析提供邊際均值。\n\n\n\n\n\n\nFigure 13.9: jamovi屏幕截圖顯示了未飽和模型的邊際均值，即未包括臨床試驗數據集中的交互作用分量"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "href": "13-Factorial-ANOVA.html#檢核變異數分析的執行條件",
    "title": "13  多因子變異數分析",
    "section": "13.4 檢核變異數分析的執行條件",
    "text": "13.4 檢核變異數分析的執行條件\n與單因素ANOVA一樣，因子ANOVA的關鍵假設是方差同質性（所有組具有相同的標準差）、殘差正態性和觀察值獨立性。前兩者是我們可以檢查的內容。對於第三點，您需要自己評估不同觀察值之間是否存在特殊關係，例如獨立變量是時間的重複測量，因此觀察值在時間1和時間2之間存在關係：不同時間點的觀察值來自同一個人。此外，如果您沒有使用飽和模型（例如，如果您省略了交互作用項），那麼您還假定省略的項不重要。當然，您可以通過運行包含省略項的ANOVA來檢查這最後一點，看看它們是否顯著，所以這非常簡單。那麼方差同質性和殘差正態性呢？事實證明，這些非常容易檢查。這與我們為單因素ANOVA所做的檢查沒有區別。\n\n\n13.4.1 變異數的同質性\n正如上一章 Section 12.6.1 中提到的，最好是通過視覺檢查標準差在不同組/類別之間的比較圖，並查看Levene檢驗是否與視覺檢查一致。Levene檢驗的理論在 ?sec-Checking-the-homogeneity-of-variance-assumption中 討論過，所以我不再討論。該檢驗要求您使用一個飽和模型（即，包含所有相關條款），因為該檢驗主要關注的是組內方差，而不是使用與完整模型相關的其他方法計算。在jamovi中，可以在ANOVA ‘Assumption Checks’ - ’Homogeneity Tests’選項下指定Levene檢驗，結果如 Figure 13.10 所示。Levene檢驗的非顯著性意味著，只要與標準差圖的視覺檢查一致，我們就可以放心地假定方差同質性假設沒有被違反。\n\n\n\n13.4.2 殘差的常態性\n與單因素ANOVA一樣，我們可以用直接的方式測試殘差的正態性（參見 Section 12.6.4 )。然而，通常最好是使用QQ圖以圖形方式檢查殘差。請參見 Figure 13.10 。\n\n\n\n\n\n\nFigure 13.10: 檢查ANOVA模型的假設"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "href": "13-Factorial-ANOVA.html#sec-analysis-of-covariance-ancova",
    "title": "13  多因子變異數分析",
    "section": "13.5 共變數分析 (ANCOVA)",
    "text": "13.5 共變數分析 (ANCOVA)\nANOVA的一種變體是當您擁有一個可能與因變量相關的額外連續變量時。這個額外的變量可以作為協變量添加到分析中，正如協方差分析（ANCOVA）這個貼切的名稱所示。\n在ANCOVA中，因變量的值會根據協變量的影響進行「調整」，然後以通常的方式在各組之間測試「調整後」的均值。這種技術可以提高實驗的精確性，因此提供了對因變量中組均值相等性的更「有效」的檢驗。ANCOVA是如何做到這一點的呢？儘管協變量本身通常不是實驗性的，但對協變量的調整可以降低實驗誤差的估計，從而通過減少誤差變異，提高精確性。這意味著不適當地無法拒絕虛無假設（偽陰性或第二類錯誤）的可能性較小。\n儘管存在這個優勢，ANCOVA仍然存在解除組間實際差異的風險，這應該避免。例如，看看@fig-fig13-11，它顯示了統計焦慮與年齡的關係，並顯示了兩個不同的組別–具有藝術或科學背景或偏好的學生。以年齡為協變量的ANCOVA可能會得出統計焦慮在兩個組別之間沒有差異的結論。這個結論是否合理呢？很可能不合理，因為兩個組別的年齡不重疊，方差分析實質上是「將結果外推到沒有數據的區域」（Everitt (1996)，第68頁）。\n\n\n\n\n\n\nFigure 13.11: 統計焦慮與年齡的圖示，對於兩個不同的組別\n\n\n\n\n顯然，需要仔細考慮對區別鮮明的組別進行協方差分析。這適用於單因素和因子設計，因為ANCOVA可以用於兩者。\n\n\n13.5.1 使用jamovi完成共變數分析\n一位健康心理學家對例行騎自行車和壓力對幸福程度的影響感興趣，並將年齡作為協變量。您可以在ancova.csv文件中找到數據集。在jamovi中打開此文件，然後選擇分析 - ANOVA - ANCOVA 以打開ANCOVA分析窗口（Figure 13.12）。突顯因變量「幸福」，將其轉移到「因變量」文本框中。突顯自變量「壓力」和「通勤」，將它們轉移到「固定因素」文本框中。突顯協變量「年齡」，將其轉移到「協變量」文本框中。然後單擊估計邊際均值以顯示圖表和表格選項。\n\n\n\n\n\n\n\nFigure 13.12: jamovi ANCOVA分析窗口\n\n\n\n\njamovi結果窗口中產生了一個顯示主題效應測試的ANCOVA表格（Figure 13.13）。協變量「年齡」的F值在 \\(p = .023\\) 上顯著，這表明年齡是影響因變量幸福的重要預測因子。當我們查看估計的邊際平均分數（Figure 13.14）時，由於在此ANCOVA中包含協變量「年齡」，所以已進行了調整（與未包含協變量的分析相比）。圖表（Figure 13.15）是一個很好的視覺化和解釋顯著效應的方法。\n\n\n\n\n\n\n\nFigure 13.13: jamovi ANCOVA輸出，將幸福度作為壓力和通勤方法的函數，年齡作為協變量\n\n\n\n\n\n\n\n\n\nFigure 13.14: 作為壓力和通勤方式函數的平均幸福水平表（根據協變量年齡進行調整），帶有95％置信區間\n\n\n\n\n\\(F\\) 值主要效果「壓力」（52.61）的相應概率為 \\(p < .001\\)。主要效果「通勤」（42.33）的 \\(F\\) 值的相應概率為 \\(p < .001\\)。由於這兩者都小於通常用於判定統計結果是否顯著的概率（\\(p < .05\\)），我們可以得出壓力的顯著主要效應（\\(F(1, 15) = 52.61, p < .001\\)）和通勤方式的顯著主要效應（\\(F(1, 15) = 42.33, p < .001\\)）。還發現了壓力和通勤方式之間的顯著交互作用（\\(F(1, 15) = 14.15, p = .002\\)）。\n在 Figure 13.15 中，我們可以看到年齡作為協變量時的調整後、邊際的平均幸福分數。在這個分析中，存在一個顯著的交互效應，即壓力較低的騎自行車上班的人比壓力較低的開車上班的人和壓力較高的人（無論他們是騎自行車還是開車上班）更幸福。還有壓力的顯著主要效應——壓力較低的人比壓力較高的人更幸福。而且通勤行為的顯著主要效應也是如此——平均而言，騎自行車上班的人比開車上班的人更幸福。\n\n\n\n\n\n\nFigure 13.15: 作為壓力和通勤方式函數的平均幸福水平圖\n\n\n\n\n需要注意的一點是，如果您想在 ANOVA 中包含協變量，那麼還有一個額外的假設：協變量與因變量之間的關係應對所有自變量的水平都是相似的。這可以通過在 jamovi Model - Model terms 選項中為協變量和每個自變量添加交互項來檢查。如果交互效應不顯著，則可以將其移除。如果它顯著，則可能需要使用更高級的統計技術（這超出了本書的範疇，所以您可能需要諮詢一位友好的統計學家）。總之，在進行 ANCOVA 分析時，要仔細考慮協變量與自變量的關係，以確保結果的準確性和有效性。\n\n總的來說，ANCOVA 分析可以幫助我們更好地瞭解不同變量之間的關係，並通過引入協變量來提高實驗的精確性。然而，在實際應用中，我們需要仔細思考和評估協變量的選擇，以確保結果的可靠性。在進行分析時，要注意檢查假設，並在必要時尋求統計專家的幫助。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "href": "13-Factorial-ANOVA.html#sec-ANOVA-as-a-linear-model",
    "title": "13  多因子變異數分析",
    "section": "13.6 變異數分析就是線性模型",
    "text": "13.6 變異數分析就是線性模型\n一個非常重要的事情是要了解 ANOVA 和回歸實際上是一回事。從表面上看，您可能不會認為這是真的。畢竟，到目前為止我對它們的描述表明 ANOVA 主要關注測試組間差異，而回歸主要關注瞭解變量之間的相關性。在這一點上，這完全沒錯。但是當你深入了解時，所謂的 ANOVA 和回歸的基本機制非常相似。事實上，如果你仔細想想，你已經看到了這一點的證據。 ANOVA 和回歸都依賴於平方和 (SS)，都使用 F 檢驗等。回顧過去，很難逃避這樣的感覺： Chapter 10 和 Chapter 12 有點重複。\n這樣做的原因是 ANOVA 和回歸都是線性模型。在回歸的情況下，這是顯而易見的。我們用來定義預測因素和結果之間關係的回歸方程是一條直線的方程，所以這顯然是一個線性模型，方程式為\n\\[Y_p=b_0+b_1 X_{1p} +b_2 X_{2p} + \\epsilon_p\\]\n其中 \\(Y_p\\) 是第 p 次觀察（例如，第 p 個人）的結果值，\\(X_{1p}\\) 是第 p 次觀察的第一個預測因子的值，\\(X_{2p}\\) 是第 p 次觀察的第二個預測因子的值，\\(b_0\\)，\\(b_1\\) 和 \\(b_2\\) 是我們的回歸係數，\\(\\epsilon_p\\) 是第 p 個殘差。如果我們忽略殘差 \\(\\epsilon_p\\)，僅關注回歸線本身，我們得到以下公式：\n\\[\\hat{Y}_p=b_0+b_1 X_{1p} +b_2 X_{2p} \\]\n其中 \\(\\hat{Y}_p\\) 是回歸線為第 p 個人預測的 Y 值，而不是實際觀察到的值 \\(Y_p\\)。不是立即顯而易見的事情是，我們可以將 ANOVA 也寫成線性模型。然而，實際上這非常簡單。讓我們從一個非常簡單的例子開始，將 \\(2 \\times 2\\) 因子 ANOVA 重寫為線性模型。\n\n\n\n13.6.1 示範資料\n為了具體說明，假設我們的結果變量是學生在我的課堂上獲得的成績，這是一個比例尺度變量，對應於 \\(0％\\) 到 \\(100％\\) 的分數。有兩個感興趣的預測變量：學生是否參加了課程（出席變量）以及學生是否真正閱讀了教科書（閱讀變量）。我們將假設如果學生參加課程，那麼 attend = 1，如果他們沒有參加，那麼 attend = 0。同樣，如果學生閱讀了教科書，我們將說 reading = 1，如果他們沒有，那麼 reading = 0。\n好吧，到目前為止，這還算簡單。接下來我們需要做的是將一些數學概念應用在這裡（抱歉！）。為了舉例，讓 \\(Y_p\\) 表示課堂中第 p 個學生的成績。這與我們在本章前面使用的符號不完全相同。以前，我們用符號 \\(Y_{rci}\\) 來表示第 1 個預測因子的第 r 個組中的第 i 個人（行因子）和第 2 個預測因子的第 c 個組（列因子）。這種擴展符號對於描述如何計算 SS 值非常方便，但在目前的情況下很繁瑣，所以我在這裡更換符號。現在，\\(Y_p\\) 符號比 \\(Y_{rci}\\) 更簡單，但它的缺點是它實際上沒有跟踪組成員資訊！也就是說，如果我告訴你 \\(Y_{0,0,3} = 35\\)，你會立刻知道我們在談論一個沒有上課（即 attend = 0）並且沒有閱讀教科書（即 reading = 0）的學生（事實上是第 3 個這樣的學生），並且最終未通過課程（成績 = 35）。但如果我告訴你 \\(Y_p = 35\\)，你只知道第 p 個學生沒有取得好成績。我們在這裡丟失了一些關鍵資訊。當然，想想如何解決這個問題並不費力。相反，我們將引入兩個新變量 \\(X_{1p}\\) 和 \\(X_{2p}\\) 來追踪這些資訊。在我們假設的學生案例中，我們知道 \\(X_{1p} = 0\\)（即 attend = 0）和 \\(X_{2p} = 0\\)（即 reading = 0）。因此，數據可能如 Table 13.8 所示。\n\n\n\n\n\nTable 13.8:  成績，出勤和閱讀教科書的數據 \n\nperson, \\(p\\)grade, \\(Y_p\\)attendance, \\(X_{1p}\\)reading, \\(X_{2p}\\)\n\n19011\n\n28711\n\n37501\n\n46010\n\n53500\n\n65000\n\n76510\n\n87001\n\n\n\n\n\n當然，這並沒有什麼特別之處。這正是我們期望看到的數據格式！請參閱數據文件 rtfm.csv。我們可以使用 jamovi 的 ‘Descriptives’ 分析來確認這個數據集對應於一個平衡設計，對於 attend 和 reading 的每個組合，都有 2 個觀測值。同樣，我們還可以為每個組合計算平均成績。這在 Figure 13.16 中顯示。看著平均分數，人們會強烈感覺到閱讀課本和上課都非常重要。\n\n\n\n\n\n\nFigure 13.16: rtfm 數據集的 jamovi 描述性統計\n\n\n\n\n\n\n13.6.2 以迴歸模型處理非連續因子\n好吧，讓我們回到數學上的討論。現在，我們的數據已用三個數值變量表示：連續變量 \\(Y\\) 和兩個二元變量 \\(X_1\\) 和 \\(X_2\\)。我希望您能認識到，我們的 \\(2 \\times 2\\) 因子分析變異數完全等同於迴歸模型\n\\[Y_p=b_0+b_1 X_{1p} + b_2 X_{2p} + \\epsilon_p\\]\n當然，這正是我之前用來描述具有兩個預測變量的迴歸模型的完全相同的方程式！唯一的區別是 \\(X_1\\) 和 \\(X_2\\) 現在是二元變量（即，值只能為 0 或 1），而在迴歸分析中，我們期望 \\(X_1\\) 和 \\(X_2\\) 是連續的。有幾種方法可以說服您相信這一點。一個可能性是進行冗長的數學練習，證明這兩者是相同的。然而，我要冒昧地猜測，這本書的大多數讀者會覺得這很煩人而不是有幫助。相反，我將解釋基本思想，然後依賴 jamovi 來說明 ANOVA 分析和迴歸分析不僅相似，而且在所有意圖和目的上是相同的。讓我們首先將其作為 ANOVA 運行。為此，我們將使用 rtfm 數據集，Figure 13.17 顯示了在 jamovi 中運行分析時我們得到的結果。\n\n\n\n\n\n\nFigure 13.17: 在 jamovi 中的 rtfm.csv 數據集 ANOVA，不包含交互作用項\n\n\n\n\n好的，通過從 ANOVA 表和我們之前呈現的平均分數中讀取關鍵數字，我們可以看到，如果學生參加課程（\\(F_{1,5} = 21.6, p = .0056\\)），他們的成績會更高，如果他們閱讀教材（\\(F_{1,5} = 52.3, p = .0008\\)）。讓我們記下這些 p 值和這些 \\(F\\) 統計數字。\n現在讓我們從線性迴歸的角度考慮相同的分析。在 rtfm 數據集中，我們將 attend 和 reading 編碼為數值預測變量。在這種情況下，這是完全可以接受的。在某種意義上，參加課程的學生（即 attend = 1）事實上的確比沒有參加的學生（即 attend = 0）做了“更多的出席”。因此，將其作為迴歸模型中的預測變量完全不是不合理的。這有點不尋常，因為預測變量只有兩個可能的值，但這並不違反線性迴歸的任何假設。而且易於解釋。如果 attend 的迴歸係數大於 0，則意味著參加課程的學生會獲得更高的成績。如果小於零，那麼參加課程的學生會獲得較低的成績。對於我們的閱讀變量也是如此。\n等一下。為什麼會這樣？這對於接受過幾堂統計課程並熟悉數學的人來說是直觀明顯的，但對其他人來說一開始並不清楚。要理解為什麼會這樣，有助於仔細觀察幾個特定的學生。讓我們首先考慮我們數據集中的第 6 位和第 7 位學生（即 \\(p = 6\\) 和 \\(p = 7\\)）。兩者都沒有閱讀教科書，因此在這兩種情況下，我們都可以將 reading 設為 0。換句話說，用我們的數學符號表示，我們觀察到 \\(X_{2,6} = 0\\) 和 \\(X_{2,7} = 0\\)。然而，第 7 位學生參加了課程（即 attend = 1，\\(X_{1,7} = 1\\)），而第 6 位學生沒有參加（即 attend = 0，\\(X_{1,6} = 0\\)）。現在讓我們看看當我們將這些數字插入迴歸線的一般公式時會發生什麼。對於第 6 位學生，迴歸預測：\n\\[\n\\begin{split}\n\\hat{Y}_6 & = b_0 + b_1 X_{1,6} + b_2 X_{2,6} \\\\\n& = b_0 + (b_1 \\times 0) + (b_2 \\times 0) \\\\\n& = b_0\n\\end{split}\n\\]\n因此，我們預計這位學生將獲得與截距項 \\(b_0\\) 相對應的成績。那麼第 7 位學生呢？這次當我們將數字插入迴歸線公式時，我們得到以下結果：\n\\[\n\\begin{split}\n\\hat{Y}_7 & = b_0 + b_1 X_{1,7} + b_2 X_{2,7} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 0) \\\\\n& = b_0 + b_1\n\\end{split}\n\\]\n因為這位學生參加了課程，預計成績等於截距項 b0 加上與 attend 變量相關的係數 \\(b_1\\)。所以，如果 \\(b_1\\) 大於零，我們預期參加課程的學生將比那些沒有參加的學生獲得更高的成績。如果這個係數為負，我們則預期相反：上課的學生表現會更差。實際上，我們可以更進一步。第一位學生呢？他參加了課程（\\(X_{1,1} = 1\\)），並且閱讀了教科書（\\(X_{2,1} = 1\\)）？如果我們將這些數字插入迴歸，我們得到：\n\\[\n\\begin{split}\n\\hat{Y}_1 & = b_0 + b_1 X_{1,1} + b_2 X_{2,1} \\\\\n& = b_0 + (b_1 \\times 1) + (b_2 \\times 1) \\\\\n& = b_0 + b_1 + b_2\n\\end{split}\n\\]\n因此，如果我們假設參加課程有助於獲得好成績（即 \\(b1 \\> 0\\)），並假設閱讀教科書也有助於獲得好成績（即 \\(b2 \\> 0\\)），那麼我們的預期是，第 1 位學生將比第 6 位學生和第 7 位學生獲得更高的成績。\n此時，你可能一點也不會感到驚訝地了解到迴歸模型預測，讀了書但沒有參加課程的第 3 位學生將獲得 \\(b_{2} + b_{0}\\) 的成績。我不會再用另一個迴歸公式來煩悶你。相反，我將向你展示的是帶有預期成績的 Table 13.9。\n\n\n\n\n\n\nTable 13.9:  迴歸模型的預期成績 \n\nread textbook\n\nnoyes\n\nattended?no\\( \\beta_0 \\)\\( \\beta_0 + \\beta_2 \\)\n\nyes\\( \\beta_0 + \\beta_1 \\)\\( \\beta_0 + \\beta_1 + \\beta_2 \\)\n\n\n\n\n\n正如你所看到的，截距項 \\(b_0\\) 作為一種基線成績，用來表示那些沒有花時間參加課程或閱讀教科書的學生所期望的成績。同樣，\\(b_1\\) 表示你預期能從上課中得到的提升，而 \\(b_2\\) 表示閱讀教科書帶來的提升。事實上，如果這是一個 ANOVA，你可能很想將 b1 稱為出席的主要效應，將 \\(b_2\\) 稱為閱讀的主要效應！事實上，對於一個簡單的 \\(2 \\times 2\\) ANOVA，情況確實是這樣。\n好的，既然我們已經開始看到為什麼 ANOVA 和迴歸基本上是同一回事，讓我們實際運用 rtfm 數據和 jamovi 迴歸分析來確信這確實是真的。以通常的方式運行迴歸會得到 Figure 13.18 中顯示的結果。\n\n\n\n\n\n\nFigure 13.18: 數據集rtfm.csv 在 jamovi 中的迴歸分析，無交互作用項\n\n\n\n\n這裡有幾個有趣的地方需要注意。首先，注意截距項是 43.5，接近觀察到的那兩個既沒有閱讀文本也沒有參加課程的學生的 “組” 平均值 42.5。其次，注意我們得到了參加變量的迴歸係數 \\(b_1 = 18.0\\)，這表明參加課程的學生比沒有參加課程的學生高出 18%。因此，我們的期望是，那些上課但沒有閱讀教科書的學生將獲得 \\(b_0 + b_1\\) 的成績，即 \\(43.5 + 18.0 = 61.5\\)。當我們觀察閱讀教科書的學生時，你可以自己驗證同樣的事情。\n實際上，我們可以在建立 ANOVA 和迴歸等價性方面進一步推進。看看迴歸輸出中與 attend 變量和 reading 變量相關的 p 值。它們與我們之前在運行 ANOVA 時遇到的完全相同。這可能看起來有點奇怪，因為運行我們的迴歸模型時使用的檢驗計算了一個 t 統計量，而 ANOVA 計算了一個 F 統計量。然而，如果您還記得我們在 Chapter 7 中提到的內容，t 分布和 F 分布之間存在著某種關係。如果你有一個根據 k 自由度的 t 分布的數量，然後將其平方，那麼這個新的平方數量就遵循一個自由度為 1 和 k 的 F 分布。對於我們迴歸模型中的 t 統計量，我們可以檢查這一點。對於 attend 變量，我們得到一個 t 值為 4.65。如果我們將這個數字平方，我們最終得到的是 21.6，這與我們 ANOVA 中相應的 F 統計量相匹配。\n最後，你還應該知道一件事。因為 jamovi 瞭解到 ANOVA 和迴歸都是線性模型的例子，所以它允許您使用 ‘Linear Regression’ - ‘Model Coefficients’ - ‘Omnibus Test’ - ‘ANOVA Test’ 從迴歸模型中提取經典的 ANOVA 表，這將給你在 Figure 13.19 中顯示的表格。\n\n\n\n\n\n\n\nFigure 13.19: jamovi迴歸分析的Omnibus ANOVA Test結果\n\n\n\n\n\n\n13.6.3 比較因子間平均值的編碼\n\n在這一點上，我已經向您展示了如何將 \\(2 \\times 2\\) ANOVA 轉換為線性模型。從而很容易看出這如何擴展到 \\(2 \\times 2 \\times 2\\) ANOVA 或 \\(2 \\times 2 \\times 2 \\times 2\\) ANOVA。事實上，這是同一件事。對於每個因子，你只需添加一個新的二元變量。當我們考慮具有多於兩個級別的因子時，問題變得更加複雜。例如，考慮我們在本章前面使用clinicaltrial.csv 數據運行的 \\(3 \\times 2\\) ANOVA。我們如何將具有三個級別的藥物因子轉換為適合迴歸的數值形式？\n事實上，這個問題的答案相當簡單。我們所要做的就是意識到三級因子可以被重新描述為兩個二元變量。假設，例如，我要創建一個名為 druganxifree 的新二元變量。每當藥物變量等於 “anxifree” 時，我們將 druganxifree 設為 1。否則，將 druganxifree 設為 0。這個變量設立了一個對比，在這種情況下是在 anxifree 和其他兩種藥物之間。當然，僅憑 druganxifree 對比還不足以完全捕捉我們藥物變量中的所有信息。我們需要第二個對比，一個能讓我們區分 joyzepam 和安慰劑的對比。為此，我們可以創建第二個二元對比，名為 drugjoyzepam，如果藥物是 joyzepam，則等於 1，否則等於 0。這兩個對比結合在一起，使我們能夠完美區分所有三種可能的藥物。Table 13.10 說明了這一點。\n\n\n\n\n\nTable 13.10:  二元對比以區分所有三種可能的藥物 \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\n如果給病人用的藥物是安慰劑，那麼這兩個對比變量都將等於 0。如果藥物是 Anxifree，那麼 druganxifree 變量將等於 1，而 drugjoyzepam 將為 0。對於 Joyzepam，情況剛好相反：drugjoyzepam 為 1，而 druganxifree 為 0。\n使用 jamovi 計算新變量命令創建對比變量並不困難。例如，要創建 druganxifree 變量，請在計算新變量公式框中編寫此邏輯表達式：IF(drug == ‘anxifree’, 1, 0)‘。同樣，要創建新變量 drugjoyzepam，請使用此邏輯表達式：IF(drug == ’joyzepam’, 1, 0)。對於 CBTtherapy，請使用：IF(therapy == ‘CBT’, 1, 0)。您可以在 jamovi 數據文件 clinicaltrial2.omv 中查看這些新變量和相應的邏輯表達式。\n我們現在已經將三級因子根據兩個二元變量進行了重新編碼，我們已經看到，對於二元變量，ANOVA 和迴歸的行為方式是相同的。然而，在這種情況下，還有一些額外的複雜性，我們將在下一節中討論。\n\n\n\n13.6.4 變異數分析與非二元因子迴歸分析的等價性\n現在，我們有兩個不同版本的相同數據集。我們的原始數據中，clinicaltrial.csv 文件中的藥物變量表示為單個三級因子，而在擴展數據 clinicaltrial2.omv 中，它擴展為兩個二元對比。再次，我們想要證明的是，我們原來的 \\(3 \\times 2\\) 因子 ANOVA 等同於應用於對比變量的迴歸模型。讓我們首先重新執行 ANOVA，結果顯示在 Figure 13.20。\n\n\n\n\n\n\nFigure 13.20: jamovi ANOVA 結果，無交互組件\n\n\n\n\n顯然，這裡沒有什麼驚喜。這正是我們之前執行的相同 ANOVA。接下來，讓我們使用 druganxifree、drugjoyzepam 和 CBTtherapy 作為預測因子進行迴歸。結果顯示在 Figure 13.21。\n\n\n\n\n\n\nFigure 13.21: jamovi 迴歸結果，包含對比變量 druganxifree 和 drugjoyzepam\n\n\n\n\n嗯。這不是我們上次得到的相同輸出。毫不奇怪，迴歸輸出將每個預測因子的結果分別打印出來，就像我們之前進行迴歸分析的每一次一樣。一方面，我們可以看到 CBTtherapy 變量的 p 值與我們原始 ANOVA 中治療因子的 p 值完全相同，因此我們可以放心，迴歸模型與 ANOVA 做的事情相同。另一方面，這個迴歸模型分別測試 druganxifree 對比和 drugjoyzepam 對比，好像它們是兩個完全無關的變量。當然，這並不奇怪，因為可憐的迴歸分析根本無法知道 drugjoyzepam 和 druganxifree 實際上是我們用來編碼三級藥物因子的兩個不同對比。就它所知，drugjoyzepam 和 druganxifree 與 drugjoyzepam 和 therapyCBT 之間的關係沒有任何區別。然而，您和我都知道得更好。在這個階段，我們根本不感興趣確定這兩個對比是否各自具有顯著性。我們只想知道是否存在藥物的“整體”效果。也就是說，我們希望 jamovi 執行某種“模型比較”檢驗，在此檢驗中，為了檢驗的目的，將兩個“與藥物相關”的對比合併在一起。聽起來熟悉嗎？我們所需要做的就是指定我們的零假設模型，該模型將包括 CBTtherapy 預測因子，並省略所有與藥物相關的變量，如 Figure 13.22 所示。\n\n\n\n\n\n\nFigure 13.22: jamovi 迴歸中的模型比較，零模型 1 與對比模型 2\n\n\n\n\n啊，這樣好多了。我們的 F 統計量是 26.15，自由度是 2 和 14，p 值是 0.00002。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。我們再次看到，變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。這一事實的重要性不應被低估。在本章的其餘部分，我們將重點依賴這個想法。\n雖然我們在 jamovi 中計算了新變量 druganxifree 和 drugjoyzepam 進行對比，僅僅為了顯示變異數分析和迴歸本質上是相同的，在 jamovi 線性迴歸分析中實際上有一個巧妙的捷徑來獲得這些對比，見 Figure 13.23。jamovi 在這裡做的是允許您將因子作為預測變量輸入，等待它…因子！聰明，對吧。您還可以通過 ‘Reference Levels’ 選項指定要用作參考級別的組。我們將其分別更改為 ‘placebo’ 和 ‘no.therapy’，因為這是最有意義的。\n\n\n\n\n\n\nFigure 13.23: jamovi 中帶有因子和對比的迴歸分析，包括整體變異數分析檢驗結果\n\n\n\n\n如果您還在 ‘Model Coefficients’ - ‘Omnibus Test’ 選項下勾選 ‘ANOVA’ 檢驗復選框，我們會看到 F 統計量為 26.15，自由度為 2 和 14，p 值為 0.00002（Figure 13.23）。這些數字與我們在原始變異數分析中得到的藥物主效應的數字相同。再次，我們看到變異數分析和迴歸本質上是相同的。它們都是線性模型，變異數分析的底層統計機制與迴歸中使用的機制相同。\n\n\n\n13.6.5 自由度就是計算有多少參數\n經過漫長的等待，我終於可以給出一個我滿意的自由度定義。自由度是根據模型中需要估計的參數數量來定義的。對於迴歸模型或變異數分析，參數數量對應於迴歸係數的數量（即 b 值），包括截距。請記住，任何 F 檢驗都始終是兩個模型之間的比較，第一個 df 是參數數量的差。例如，在上面的模型比較中，零模型（mood.gain ~ therapyCBT）有兩個參數：therapyCBT 變量的一個迴歸係數和截距的第二個參數。替代模型（mood.gain ~ druganxifree + drugjoyzepam + therapyCBT）有四個參數：三個對比中的一個迴歸係數和截距的一個參數。因此，這兩個模型之間的差的自由度是 \\(df_1 = 4 - 2 = 2\\)。\n那麼，在似乎沒有零模型的情況下呢？例如，您可能正在考慮在「線性迴歸」-「模型擬合」選項下選擇「F 檢驗」時出現的 F 檢驗。我最初將其描述為對整個迴歸模型的檢驗。然而，這仍然是兩個模型之間的比較。零模型是僅包含 1 個迴歸係數的簡單模型，用於截距項。替代模型包含 \\(K + 1\\) 個迴歸係數，每個 K 個預測變量一個，再加上截距。因此，您在此 F 檢驗中看到的 df 值等於 \\(df_1 = K + 1 - 1 = K\\)。\n那麼，在 F 檢驗中出現的第二個 df 值呢？這總是指與殘差相關的自由度。也可以用參數的方式來思考這個問題，但這有點反直覺。想象一下，假設整個研究的觀察值總數為 N。如果您想完美地描述這些 N 個值，您需要使用… N 個數字。當您建立迴歸模型時，您實際上在指定一些數字需要完美地描述數據。如果您的模型有 \\(K\\) 個預測變量和一個截距，那麼您已經指定了 \\(K + 1\\) 個數字。那麼，無需確定這將如何完成，您認為還需要多少數字才能將 K 个 1 參數迴歸模型轉換為原始數據的完美描述呢？如果您發現自己在想 \\((K + 1) + (N - K - 1) = N\\)，因此答案必須是 \\(N - K - 1\\)，那就做得很好！這正是對的。原則上，您可以想像一個包含每個數據點的參數的極其複雜的迴歸模型，它當然可以完美地描述數據。這個模型總共包含了 \\(N\\) 個參數，但是我們感興趣的是描述這個完整模型（即 \\(N\\)）所需的參數數量與您實際感興趣的更簡單的迴歸模型所使用的參數數量（即 \\(K + 1\\)）之間的差別，因此 F 檢驗中的第二個自由度是 \\(df_2 = N - K - 1\\)，其中 K 是預測變量的數量（在迴歸模型中）或對比的數量（在變異數分析中）。在我上面給出的示例中，數據集中有 \\((N = 18\\) 觀察值，並且與變異數分析模型相關的 \\(K + 1 = 4\\) 個迴歸係數，因此殘差的自由度是 \\(df_2 = 18 - 4 = 14\\)。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#各種多重比較方案",
    "href": "13-Factorial-ANOVA.html#各種多重比較方案",
    "title": "13  多因子變異數分析",
    "section": "13.7 各種多重比較方案",
    "text": "13.7 各種多重比較方案\n在上一節中，我向您展示了一種將因子轉換為對比組合的方法。在我向您展示的方法中，我們指定了一組二進制變量，其中我們定義了一個類似於 Table 13.11 的表格。\n\n\n\n\n\n\nTable 13.11:  二進制對比以區分所有三種可能的藥物 \n\ndrugdruganxifreedrugjoyzepam\n\n\"placebo\"00\n\n\"anxifree\"10\n\n\"joyzepam\"01\n\n\n\n\n\n表格中的每一行對應於因子水平之一，每一列對應於對比之一。這個表格，始終比列多一行，有一個特殊的名稱。它被稱為對比矩陣。然而，有很多不同的方法可以指定對比矩陣。在本節中，我討論了統計學家使用的一些標準對比矩陣以及如何在 jamovi 中使用它們。如果您打算稍後閱讀[因子變異數分析 3：不平衡設計]一節，那麼仔細閱讀本節是值得的。如果沒有，您可以瀏覽一下它，因為對於平衡設計來說，對比的選擇並不重要。\n\n\n\n13.7.1 比較操作效果\n在我上面描述的這種對比中，因子的一個水平是特殊的，並作為一種 “基線” 類別（例如，在我們的示例中是安慰劑），其他兩個則是根據這個基線來定義的。這種對比的名稱是治療對比，也稱為 “哑变量编码”。在此對比中，因子的每個水平都與基本參考水平進行比較，基本參考水平是截距的值。\n這個名字反映了這樣一個事實，當你的因子中的一個類別確實很特殊，因為它確實代表了基線時，這些對比是非常自然和合理的。這在我們的臨床試驗示例中是有道理的。安慰劑條件對應於不給人們使用任何真正的藥物的情況，因此它是特殊的。其他兩個條件是相對於安慰劑定義的。在一種情況下，您用 Anxifree 替換安慰劑，而在另一種情況下，您用 Joyzepam 替換安慰劑。\n上面顯示的表格是具有3個水平的因子的治療對比矩陣。但是，如果我想要一個具有5個水平的因子的治療對比矩陣呢？您可以像 Table 13.12 那樣排列它。\n\n\n\n\n\n\nTable 13.12:  具有5個水平的治療對比矩陣 \n\nLevel2345\n\n10000\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\n在這個例子中，第一個對比是第2級與第1級比較，第二個對比是第3級與第1級比較，依此類推。請注意，默認情況下，因子的第一水平始終被視為基線類別（即，它是所有零的那個，並且沒有與之相關的顯式對比）。在 jamovi 中，您可以通過操作 “數據變量” 窗口中顯示的變量的水平來更改哪個類別是因子的第一個水平（雙擊電子表格列中的變量名稱以彈出 “數據變量” 視圖。\n\n\n\n13.7.2 Helmert 比較法\n治療對比在很多情況下都很有用。然而，它們在真正存在基線類別的情況下最有意義，並且您希望根據該類別評估所有其他組。然而，在其他情況下，可能不存在這樣的基線類別，與其將每個組與其他組的平均值進行比較可能更有意義。這就是我們遇到 Helmert 對比的地方，由 jamovi ‘ANOVA’ - ‘Contrasts’ 選擇框中的 ‘helmert’ 選項生成。Helmert 對比背後的想法是將每個組與 “前一個” 組的平均值進行比較。也就是說，第一個對比表示第2組和第1組之間的差異，第二個對比表示第3組與第1組和第2組的平均值之間的差異，依此類推。對於具有五個水平的因子，這轉換為看起來像 Table 13.13 的對比矩陣。\n\n\n\n\n\nTable 13.13:  具有5個水平的 helmert 對比矩陣 \n\n1-1-1-1-1\n\n21-1-1-1\n\n302-1-1\n\n4003-1\n\n50004\n\n\n\n\n\nHelmert 對比的一個有用之處是每個對比都加起來為零（即，所有列加起來為零）。這導致了當我們將 ANOVA 解釋為回歸時，如果我們使用 Helmert 對比，截距項對應於大平均數 \\(\\mu_{..}\\)。將其與治療對比進行比較，在治療對比中，截距項對應於基線類別的組平均值。這個性質在某些情況下可能非常有用。如果您有一個平衡設計，這並不太重要，到目前為止，我們一直在這樣假設，但是當我們考慮不平衡設計時，它將變得重要。事實上，我甚至麻煩包括這一部分的主要原因是，如果您想了解不平衡 ANOVA，對比變得很重要。\n\n\n\n13.7.3 簡單比較\n第三個選項是我應該簡要提及的 “總和至零” 對比，在 jamovi 中稱為 “簡單” 對比，它們用於構建組間的兩兩比較。具體來說，每個對比都編碼了某個組與基線類別之間的差異，這種情況下基線類別對應於第一組 (Table 13.14)。\n\n\n\n\n\nTable 13.14:  具有5個水平的 ‘總和至’ 零對比矩陣 \n\n1-1-1-1-1\n\n21000\n\n30100\n\n40010\n\n50001\n\n\n\n\n\n與 Helmert 對比非常相似，我們看到每一列的和都是零，這意味著當 ANOVA 被視為回歸模型時，截距項對應於整體平均值。在解釋這些對比時，需要認識到的是，每個對比都是第1組與其他四個組之間的兩兩比較。具體來說，對比1對應於 “第2組減去第1組” 的比較，對比2對應於 “第3組減去第1組” 的比較，依此類推。7\n\n\n\n13.7.4 jamovi的各種比較選項\njamovi 還提供了多種可以在 ANOVA 中生成不同類型對比的選項。這些可以在主要的 ANOVA 分析窗口的 ‘對比’ 選項中找到，其中 Table 13.15 列出了對比類型：\n\n\n\n\n\nTable 13.15:  在 jamovi ANOVA 分析中可用的對比類型 \n\nContrast type\n\nDeviationCompares the mean of each level (except a reference category) to the mean of all of the levels (grand mean)\n\nSimpleLike the treatment contrasts, the simple contrast compares the mean of each level to the mean of a specified level. This type of contrast is useful when there is a control group. By default the first category is the reference. However, with a simple contrast the intercept is the grand mean of all the levels of the factors.\n\nDifferenceCompares the mean of each level (except the first) to the mean of previous levels. (Sometimes called reverse Helmert contrasts)\n\nHelmertCompares the mean of each level of the factor (except the last) to the mean of subsequent levels\n\nRepeatedCompares the mean of each level (except the last) to the mean of the subsequent level\n\nPolynomialCompares the linear effect and quadratic effect. The first degree of freedom contains the linear effect across all categories; the second degree of freedom, the quadratic effect. These contrasts are often used to estimate polynomial trends"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "href": "13-Factorial-ANOVA.html#sec-Post-hoc-tests",
    "title": "13  多因子變異數分析",
    "section": "13.8 事後檢定",
    "text": "13.8 事後檢定\n現在轉換到另一個主題。而不是使用對比測試您已經計劃好的比較，假設您已經完成了 ANOVA，結果發現您獲得了一些顯著的效果。因為 F 檢驗是“全面”檢驗，它們實際上只測試各組之間沒有差異的虛無假設，所以獲得顯著效果並不能告訴你哪些組與其他組有所不同。我們在 Chapter 12 中討論了這個問題，並且在那章節中，我們的解決方案是對所有可能的組對執行 t 檢驗，對多重比較（例如，Bonferroni、Holm）進行校正，以控制所有比較中的 I 類型誤差率。我們在 Chapter 12 中使用的方法具有相對簡單的優點，並且可以在您在測試多個假設的多種不同情況下使用，但它們並非在 ANOVA 背景下進行有效的事後檢驗的最佳選擇。統計文獻中有很多用於執行多重比較的方法(Hsu, 1996)，本書將超出範疇，無法詳細討論所有這些方法。\n話雖如此，有一個工具我想引起您的注意，那就是 Tukey 的 “誠實顯著差異”，簡稱Tukey’s HSD。這次，我將不提供公式，只講解質性思路。Tukey’s HSD 的基本思想是檢查所有相關的組之間的成對比較，而且只有在您對成對差異感興趣時，使用 Tukey’s HSD 才合適。8 例如，前面我們使用 clinicaltrial.csv 數據集進行了因子 ANOVA，並指定了藥物的主要作用和治療的主要作用，我們對以下四種比較感興趣：\n\n給予 Anxifree 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Joyzepam 的人與給予安慰劑的人的情緒提升之間的差異。\n給予 Anxifree 的人與給予 Joyzepam 的人的情緒提升之間的差異。\n接受 CBT 治療的人與未接受治療的人的情緒提升之間的差異。\n\n對於這些比較中的任何一個，我們都對（群體）組平均值之間的真實差異感興趣。Tukey 的 HSD 會為這四種比較構建同時置信區間。我們所說的 95% “同時”置信區間是指，如果我們重複這個研究很多次，那麼在 95% 的研究結果中，置信區間將包含相關的真實值。此外，我們可以使用這些置信區間計算任何特定比較的校正 p 值。\n在 jamovi 中使用 TukeyHSD 函數非常容易。您只需指定要為其執行事後檢驗的 ANOVA 模型項。例如，如果我們要為主效應進行事後檢驗，但不考慮交互作用，我們將在 ANOVA 分析屏幕中打開“事後檢驗”選項，將藥物和治療變量移到右側的框中，然後在可能的事後校正列表中選中“Tukey”複選框。這與相應的結果表在 Figure 13.24 中顯示。\n\n\n\n\n\n\n\nFigure 13.24: 不帶交互作用的 jamovi 因子 ANOVA 中的 Tukey HSD 事後檢驗\n\n\n\n\n在「事後檢驗」結果表中顯示的輸出非常直觀。例如，第一個比較是Anxifree與安慰劑之間的差異，輸出的第一部分顯示組均值之間的觀察差異為0.27。接下來的數字是差異的標準誤，如果我們想要，我們可以根據此計算出95%置信區間，儘管jamovi目前尚不提供此選項。然後有一列是自由度，一列是t值，最後一列是p值。對於第一個比較，調整後的p值為0.21。相比之下，如果您看下一行，我們會發現安慰劑和Joyzepam之間的觀察差異為1.03，並且這個結果顯著（p < .001）。\n到目前為止，一切都很好。那麼，如果您的模型包括交互作用項怎麼辦？例如，jamovi中的默認選項是允許藥物和療法之間存在交互作用。如果是這樣，我們需要考慮的兩兩比較數量開始增加。像以前一樣，我們需要考慮與藥物主效應相關的三個比較以及與療法主效應相關的一個比較。但是，如果我們要考慮顯著交互作用的可能性（並嘗試找到支持這一顯著交互作用的組差異），我們需要包括以下比較：\n\n使用Anxifree並接受CBT治療的人的情緒增益與使用安慰劑並接受CBT治療的人的情緒增益之間的差異\n使用Anxifree並不接受治療的人與使用安慰劑並不接受治療的人的情緒增益之間的差異。\n等等\n\n您需要考慮相當多的這些比較。因此，當我們對此ANOVA模型運行Tukey事後分析時，我們會發現它進行了很多兩兩比較（共19個），如@fig-fig13-25所示。您會發現它看起來與之前非常相似，但進行了更多的比較。\n\n\n\n\n\n\n\nFigure 13.25: jamovi因子ANOVA中的Tukey HSD事後檢驗，包含交互作用項"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "href": "13-Factorial-ANOVA.html#sec-The-method-of-planned-comparisons",
    "title": "13  多因子變異數分析",
    "section": "13.9 事前檢定方法",
    "text": "13.9 事前檢定方法\n延續前面關於ANOVA中對比和事後檢驗的部分，我認為預先設定的比較方法（planned comparisons）非常重要，值得簡要討論。在前面的章節以及@sec-Comparing-several-means-one-way-ANOVA中對多重比較的討論中，我們假設你想運行的檢驗確實是事後檢驗。例如，在上面的藥物示例中，可能你認為這些藥物對情緒的影響各有不同（即你假設藥物有主效應），但你沒有關於它們如何不同的具體假設，也沒有任何真正的想法關於哪些兩兩比較值得觀察。如果是這樣，那麼你確實需要使用Tukey的HSD來進行兩兩比較。\n然而，情況會有所不同，如果你真的有關於哪些比較感興趣的確切、具體的假設，而且你絕對無意觀察除了提前指定的那些比較以外的任何其他比較。當這是真的，並且如果你真誠並嚴格地堅持不進行任何其他比較的高尚意圖（即使數據看起來似乎對你沒有假設的東西顯示出非常顯著的效應），那麼使用像Tukey的HSD這樣的方法並不合理，因為它對一整套你從未關心過，也從未打算觀察的比較進行了矯正。在這種情況下，你可以安全地運行有限數量的假設檢驗，而無需對多重檢驗進行調整。這種情況被稱為預先設定的比較方法，有時用於臨床試驗。然而，進一步的考慮超出了這本入門書的範疇，但至少你知道這種方法是存在的！"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#不平衡的因子設計分析",
    "href": "13-Factorial-ANOVA.html#不平衡的因子設計分析",
    "title": "13  多因子變異數分析",
    "section": "13.10 不平衡的因子設計分析",
    "text": "13.10 不平衡的因子設計分析\n因子ANOVA是一個非常方便的工具。它已經成為分析實驗數據的標準工具之一，已有數十年的歷史，你會發現在心理學上，你無法閱讀超過兩三篇論文而不在其中某個地方遇到ANOVA。然而，在很多真實的科學文章中，你將會看到的ANOVA和我迄今為止描述的ANOVA之間有一個巨大的差異。在現實生活中，我們很少有幸擁有完美平衡的設計。出於某種原因，通常會在某些單元中得到比其他單元更多的觀察結果。換句話說，我們有一個不平衡的設計。\n不平衡設計需要比平衡設計更加謹慎地處理，支撐它們的統計理論也更為混亂。這種混亂可能是結果，也可能是時間短缺，但我的經驗是心理學本科研究方法課程有一個令人討厭的傾向，那就是完全忽略這個問題。很多統計教科書也容易忽略它。我認為，這導致了很多領域內的在職研究人員實際上並不知道存在幾種不同類型的不平衡ANOVA，而它們產生的答案相差很大。事實上，閱讀心理學文獻時，我對大多數報告不平衡因子ANOVA結果的人實際上無法提供足夠的詳細信息來重現分析感到驚訝。我暗自懷疑，大多數人甚至沒有意識到他們的統計軟件包正在代替他們做出大量實質性的數據分析決策。當你想到這一點時，它實際上是有點恐怖的。因此，如果你想避免將數據分析的控制權交給愚蠢的軟件，請繼續閱讀。\n\n\n13.10.1 咖啡飲用資料\n跟往常一樣，使用一些數據將對我們有所幫助。coffee.csv 文件包含了一個產生不平衡 \\(3 \\times 2\\) ANOVA 的假設數據集。假設我們有興趣了解人們在喝太多咖啡時胡言亂語的趨勢是否純粹是咖啡本身的影響，還是人們在咖啡中加入牛奶和糖所產生的影響。假設我們找了18個人，給他們喝了一些咖啡。咖啡/咖啡因的含量保持恆定，我們改變是否加入牛奶，所以牛奶是一個有兩個水平的二元因子，分別是“是”和“否”。我們還改變了涉及的糖的種類。咖啡中可能含有“真正”的糖，或者可能含有“假”的糖（即人工甜味劑），或者可能根本不含糖，所以糖變量是一個有三個水平的因子。我們的結果變量是一個連續變量，這可能意味著某個心理上有意義的衡量某人“胡言亂語”程度的指標。對於我們的目的，細節並不真正重要。查看 jamovi 試算表視圖中的數據，如 Figure 13.26。\n\n\n\n\n\n\nFigure 13.26: coffee.csv數據集在 jamovi 中，描述性信息按因子水平匯總\n\n\n\n\n查看 Figure 13.26 中的平均值表，我們可以強烈感受到各組之間的差異。與 babble 變量的標準偏差相比，這一點尤其明顯。在各組中，這個標準偏差從 .14 到 .71 不等，這相對於組間平均值的差異來說相當小。9 雖然這一開始看起來像是一個簡單的因子ANOVA，但當我們查看每個組中有多少個觀察值時，問題就出現了。參見 Figure 13.26 中顯示的不同組別的不同 N 值。這違反了我們最初的假設，即每個組中的人數是相同的。我們還沒有真正討論如何處理這種情況。\n\n\n\n\n13.10.2 不平衡設計不適用「標準變異數分析」\n不平衡設計讓我們發現，實際上並不存在我們可能稱之為標準 ANOVA 的任何一種事物。事實上，你可能希望在不平衡設計中以三種根本不同的方式 10來運行 ANOVA。如果您有一個平衡設計，這三個版本都會產生相同的結果，與我在本章開始時給出的公式一致的平方和、F 值等。然而，當您的設計不平衡時，它們的答案並不相同。此外，它們對於每種情況的適用程度也不盡相同。有些方法對您的情況可能更適用。鑒於此，了解不同類型的 ANOVA 及其相互之間的差異非常重要。\n第一種 ANOVA 通常被稱為第一類平方和。我敢肯定你能猜到其他兩個叫什麼。名稱中的“平方和”部分是由 SAS 統計軟件包引入的，並已成為標準術語，但在某些方面有點誤導。我認為把它們稱為不同類型的平方和的邏輯是，當你看到它們產生的 ANOVA 表時，數字之間的關鍵區別是 SS 值。自由度沒有變化，MS 值仍然被定義為 SS 除以 df 等。然而，這種術語的錯誤之處在於它掩蓋了 SS 值之間為何會有差異的原因。為此，了解三種不同類型的 ANOVA 作為三種不同的假設檢驗策略要有幫助得多。這些不同的策略確實導致了不同的 SS 值，但這裡重要的是策略，而不是 SS 值本身。回想一下[ANOVA 作為線性模型]一節，任何特定的 F 檢驗最好被認為是兩個線性模型之間的比較。因此，當您查看 ANOVA 表時，請記住每個 F 檢驗對應於要比較的模型對。當然，這自然引出了要比較哪對模型的問題。這就是 ANOVA 類型 I、II 和 III 之間的根本區別：每一種都對應於為檢驗選擇模型對的不同方式。\n\n\n\n\n13.10.3 第一型平方差總和\n類型 I 方法有時被稱為”序列”平方和，因為它涉及一次添加一個術語到模型的過程。以 coffee 數據為例。假設我們要運行完整的 \\(3 \\times 2\\) 因子 ANOVA，包括交互作用術語。完整的模型包含了結果變量 babble，預測變量 sugar 和 milk，以及交互術語 sugar \\(\\times\\) milk。這可以寫成 \\(babble \\sim sugar + milk + sugar {\\times} milk\\)。類型 I 策略會按順序構建這個模型，從最簡單的模型開始，逐步添加術語。\n數據的最簡單模型將是一個假設牛奶和糖都不影響胡言亂語的模型。這樣的模型只包括截距，寫成 babble ~ 1。這是我們最初的零假設。數據的下一個最簡單模型將是其中只包含兩個主效應之一的模型。在 coffee 數據中，這裡有兩個不同的可能選擇，因為我們可以選擇先添加 milk 或者先添加 sugar。實際上，順序是很重要的，我們稍後會看到，但現在讓我們只是隨意選擇一個，選擇 sugar。所以，我們模型序列中的第二個模型是 babble ~ sugar，它形成了我們第一次檢驗的替代假設。我們現在有了我們的第一個假設檢驗（Table 13.16）。\n\n\n\n\n\nTable 13.16:  使用結果變量 ‘babble’ 的零假設和替代假設。 \n\nNull model:\\(babble \\sim 1\\)\n\nAlternative model:\\(babble \\sim  sugar\\)\n\n\n\n\n\n這個比較形成了我們對糖的主效應的假設檢驗。我們模型建立練習的下一步是添加另一個主效應術語，因此我們序列中的下一個模型是 babble ~ sugar + milk。然後，通過比較以下模型對（Table 13.17）形成第二個假設檢驗。\n\n\n\n\n\nTable 13.17:  使用結果變量 ‘babble’ 的進一步零假設和替代假設 \n\nNull model:\\(babble \\sim  sugar\\)\n\nAlternative model:\\(babble \\sim  sugar + milk\\)\n\n\n\n\n\n這個比較形成了我們對牛奶主效應的假設檢驗。在某種意義上，這種方法非常優雅：第一次測試的替代假設形成了第二次測試的零假設。正是在這個意義上，類型 I 方法是嚴格有序的。每個測試都直接基於上一個測試的結果。然而，在另一個意義上，它非常不優雅，因為這兩個測試之間有很強的不對稱性。糖的主效應測試（第一次測試）完全忽略了牛奶，而牛奶的主效應測試（第二次測試）確實考慮了糖。無論如何，我們序列中的第四個模型現在是完整的模型，babble ~ sugar + milk + sugar \\(\\times\\) milk，相應的假設檢驗顯示在 Table 13.18 中。\n\n\n\n\n\nTable 13.18:  使用結果變量 ‘babble’ 的更多可能的零假設和替代假設 \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk + sugar * milk \\)\n\n\n\n\n\n類型 III 平方和是 jamovi ANOVA 使用的默認假設檢驗方法，因此要運行類型 I 平方和分析，我們必須在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘平方和’ 選擇框中選擇 ‘Type 1’。這給我們提供了 Figure 13.27 中顯示的 ANOVA 表。\n\n\n\n\n\n\n\nFigure 13.27: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表\n\n\n\n\n使用類型 I 平方和的最大問題是它確實取決於您輸入變量的順序。然而，在許多情況下，研究人員無需優先考慮一種排序而不考慮另一種排序。這可能是我們牛奶和糖問題的情況。我們應該先添加牛奶還是糖？在數據分析問題中，這與製作咖啡問題一樣隨意。事實上，可能有一些人對排序有堅定的看法，但很難想像這個問題有一個原則性的答案。然而，當我們改變順序時，看看會發生什麼，如 Figure 13.28 中所示。\n\n\n\n\n\n\nFigure 13.28: 使用 jamovi 中類型 I 平方和的 ANOVA 結果表，但因子以不同的順序（首先是牛奶）輸入\n\n\n\n\n兩個主效應術語的 p 值都發生了變化，而且變化相當大。在其他方面，牛奶的效果已經顯著（儘管我之前提到過，人們應該避免對此得出任何強烈的結論）。應該報告這兩個 ANOVA 中的哪一個？這並不是立即明顯的。\n當您查看用於定義“第一”主效應和“第二”主效應的假設檢驗時，很明顯它們之間有質的不同。在我們最初的示例中，我們看到了糖的主效應檢驗完全忽略了牛奶，而牛奶的主效應檢驗確實考慮了糖。因此，類型 I 檢驗策略確實將第一個主效應視為在第二個主效應之上具有某種理論優越性。根據我的經驗，很少（甚至從未）有這種理論優越性，可以證明將任何兩個主效應非對稱地對待。\n所有這些的結果是類型 I 測試很少有趣，所以我們應該繼續討論類型 II 測試和類型 III 測試。\n\n\n\n13.10.4 第三型平方差總和\n剛剛談完類型 I 測試後，您可能會認為接下來自然要談論類型 II 測試。然而，我認為在談論類型 II 測試（比較棘手）之前，先討論類型 III 測試（簡單且是 jamovi ANOVA 的默認設置）實際上更自然。類型 III 測試背後的基本思想非常簡單。無論您要評估哪個術語，都要運行 F 檢驗，其中替代假設對應用戶指定的完整 ANOVA 模型，而零模型僅刪除您正在測試的那一個術語。例如，在咖啡示例中，我們的完整模型是 babble ~ sugar + milk + sugar × milk，糖的主效應檢驗將對應於以下兩個模型之間的比較（Table 13.19）。\n\n\n\n\n\nTable 13.19:  以’babble’作為結果變量的零假設和替代假設，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  milk + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n同樣，通過將完整模型與刪除牛奶術語的零模型進行檢驗，可以評估牛奶的主效應，如 Table 13.20。\n\n\n\n\n\nTable 13.20:  以’babble’作為結果變量的更多零假設和替代假設，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  sugar + sugar * milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n最後，用完全相同的方式評估糖 × 牛奶的交互項。再次，我們將完整模型與刪除糖 × 牛奶交互項的零模型進行比較，如 Table 13.21。\n\n\n\n\n\nTable 13.21:  從以“babble”為結果變量的假設中刪除交互項，使用類型 III 平方和 \n\nNull model:\\(babble \\sim  sugar + milk\\)\n\nAlternative model:\\(babble \\sim  sugar + milk +sugar * milk \\)\n\n\n\n\n\n基本思想可以推廣到更高階 ANOVA。例如，假設我們嘗試運行一個具有三個因素 A、B 和 C 的 ANOVA，並且我們希望考慮所有可能的主效應和所有可能的交互作用，包括三者之間的交互作用 A × B × C。(Table 13.22)為您展示了這種情況下類型 III 測試的外觀。\n\n\n\n\n\nTable 13.22:  具有三個因素和所有主效應和交互項的類型 III 測試 \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB\\(A + C + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C\\)\n\nC\\(A + B + A*B + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B\\(A + B + C + A*C + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*C\\(A + B + C + A*B + B*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nB*C\\(A + B + C + A*B + A*C + A*B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n儘管該表看起來很難看，但它相當簡單。在所有情況下，替代假設對應於包含三個主效應術語（例如 A）、三個雙向交互作用（例如 A * B）和一個三向交互作用（即 A * B * C）的完整模型。零模型總是包含其中 7 項中的 6 項，而缺少的一項就是我們正在嘗試測試其顯著性的那一項。\n初步看來，類型 III 測試似乎是一個好主意。首先，我們消除了在運行類型 I 測試時遇到問題的不對稱性。而且，由於我們現在以相同的方式對待所有術語，假設測試的結果不依賴於我們指定它們的順序。這絕對是一件好事。然而，在解釋測試結果，特別是主效應術語時存在一個大問題。考慮咖啡數據。假設根據類型 III 測試，牛奶的主要效果並不顯著。這告訴我們的是，相比於完整模型，babble ~ sugar + sugar * milk 是數據的更好模型。但這意味著什麼呢？如果糖 * 牛奶的交互項也不顯著，我們會想得出結論說數據告訴我們唯一重要的事情是糖。但是，假設我們有一個顯著的交互項，但是牛奶的主效應不顯著。在這種情況下，我們是否應該認為真的有一個“糖的效果”，一個“牛奶和糖之間的交互作用”，但沒有“牛奶的效果”？那看起來很瘋狂。正確的答案簡直一定是，在交互作用顯著的情況下，談論主效應是毫無意義的11。一般來說，這似乎是大多數統計學家給我們的建議，而我認為這是正確的建議。但是，如果談論存在顯著交互作用的非顯著主效應確實是毫無意義的，那麼類型 III 測試應該允許零假設依賴於一個包括交互作用但省略了其中一個主效應的模型就不是很明顯了。以這種方式表述的零假設實際上根本沒有什麼意義。\n稍後，我們將看到類型 III 測試在某些情況下可以挽救，但首先讓我們看一下使用類型 III 和平方的 ANOVA 結果表，見 Figure 13.29。\n\n\n\n\n\n\n\nFigure 13.29: 在 jamovi 中使用類型 III 和平方的 ANOVA 結果表\n\n\n\n\n但要注意，類型 III 測試策略的一個怪異特徵是，通常結果依賴於用於編碼因子的對比（如果您忘記了不同類型對比是什麼，請參見 [指定對比的不同方式]一節）。12\n那麼，如果類型 III 分析（但不是在 jamovi 中）通常產生的 p 值對對比的選擇非常敏感，那是否意味著類型 III 測試本質上是任意的，不值得信任？在某種程度上，這是真的，當我們轉向討論類型 II 測試時，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n在類型 II 測試中，我們將看到類型 II 分析完全避免了這種任意性，但我認為這是一個過於強烈的結論。首先，重要的是要認識到某些對比選擇總是會產生相同的答案（啊，所以這就是 jamovi 中發生的事情）。特別重要的是，如果我們的對比矩陣的列都受限於求和為零，那麼類型 III 分析將始終給出相同的答案。\n\n\n\n13.10.5 第二型平方差總和\n好的，到目前為止，我們已經看過了類型 I 和 III 測試，兩者都非常簡單。類型 I 測試是通過逐一添加條款進行的，而類型 III 測試是通過使用完整模型並檢查在刪除每個條款時會發生什麼來執行的。然而，這兩者都可能有一些局限性。類型 I 測試取決於您輸入條款的順序，而類型 III 測試則取決於您如何編碼對比。類型 II 測試描述起來稍微困難一些，但它們避免了這兩個問題，因此解釋起來稍微容易一些。\n類型 II 測試與類型 III 測試大致相似。從一個“完整”模型開始，通過從該模型中刪除特定條款來進行測試。然而，類型 II 測試是基於邊際性原則的，該原則規定如果您的模型中有任何依賴於較低階條款的較高階條款，則不應從模型中省略較低階條款。所以，例如，如果您的模型包含兩個因素的交互作用 A × B（二階條款），那麼它確實應該包含主效應 A 和 B（一階條款）。同樣，如果它包含三個因素的交互作用項 A × B × C，那麼模型還必須包括主效應 A、B 和 C 以及簡單的交互作用 A × B、A × C 和 B × C。類型 III 測試經常違反邊際性原則。例如，考慮在包含所有可能交互作用條款的三因子 ANOVA 中測試 A 的主效應。根據類型 III 測試，我們的零假設和對立假設在 Table 13.23 中。\n\n\n\n\n\nTable 13.23:  類型 III 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試 \n\nNull model:\\(outcome \\sim B + C + A*B + A*C + B*C + A*B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + A*B + A*C + B*C + A*B*C\\)\n\n\n\n\n\n注意，零假設省略了 A，但將 A × B、A × C 和 A × B × C 作為模型的一部分。根據類型 II 測試，這並不是一個好的零假設選擇。相反，如果我們希望檢驗 A 對結果無關的零假設，我們應該指定一個不依賴於 A 的任何形式（即使是交互作用）的最復雜的模型。對立假設對應於該零模型加上 A 的主效應項。這個概念更接近大多數人對 “A 的主效應” 的直觀理解，並且得出了 A 的主效應的類型 II 測試（Table 13.24）。13\n\n\n\n\n\nTable 13.24:  類型 II 測試在包含所有可能交互作用條款的三因子 ANOVA 中對主效應 A 進行測試 \n\nNull model:\\(outcome \\sim B + C + B*C\\)\n\nAlternative model:\\(outcome \\sim A + B + C + B*C\\)\n\n\n\n\n\n無論如何，僅為了讓您了解類型 II 測試是如何進行的，這是在三因子因素分析中應用的完整測試表格（Table 13.25）：\n\n\n\n\n\n\nTable 13.25:  三因子因素模型的類型 II 測試 \n\nTerm being tested isNull model is outcome ~ ...Alternative model is outcome ~ ...\n\nA\\(B + C + B*C \\)\\(A + B + C + B*C \\)\n\nB\\(A + C + A*C \\)\\(A + B + C + A*C\\)\n\nC\\(A + B + A*B \\)\\(A + B + C + A*B\\)\n\nA*B\\(A + B + C + A*C + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*C\\(A + B + C + A*B + B*C  \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nB*C\\(A + B + C + A*B + A*C \\)\\(A + B + C + A*B + A*C + B*C \\)\n\nA*B*C\\(A + B + C + A*B + A*C + B*C \\)\\(A + B + C + A*B + A*C + B*C + A*B*C \\)\n\n\n\n\n\n在我們一直在咖啡數據中使用的雙因子 ANOVA 的背景下，假設檢驗更為簡單。糖的主效應對應於比較這兩個模型的 F 檢驗（Table 13.26）。\n\n\n\n\n\nTable 13.26:  咖啡數據中糖主效應的類型 II 測試 \n\nNull model:\\(babble \\sim milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n對牛奶主效應的檢驗位於 Table 13.27。\n\n\n\n\n\nTable 13.27:  咖啡數據中牛奶主效應的類型 II 測試 \n\nNull model:\\(babble \\sim  sugar \\)\n\nAlternative model:\\(babble \\sim sugar + milk\\)\n\n\n\n\n\n最後，糖與牛奶交互作用的檢驗位於 Table 13.28。\n\n\n\n\n\nTable 13.28:  以類型 II方法分析糖與牛奶交互作用 \n\nNull model:\\(babble \\sim  sugar + milk \\)\n\nAlternative model:\\(babble \\sim sugar + milk  + sugar*milk \\)\n\n\n\n\n\n運行測試再次很簡單。只需在 jamovi ‘ANOVA’ - ‘Model’ 選項中的 ‘Sum of squares’ 選擇框中選擇 ‘Type 2’，這將給我們提供 ANOVA 表格，如 Figure 13.30 所示。\n\n\n\n\n\n\n\nFigure 13.30: 在 jamovi 中使用類型 II 平方和的 ANOVA 結果表\n\n\n\n\n類型 II 測試比類型 I 和類型 III 測試具有一些明顯的優勢。它們不依賴於指定因子的順序（與類型 I 不同），也不依賴於用於指定因子的對比（與類型 III 不同）。雖然對於最後一點意見可能會有所不同，而且這肯定取決於你想用你的數據做什麼，但我認為它們指定的假設檢驗更有可能對應於你真正關心的事物。因此，我發現解釋類型 II 測試的結果通常比解釋類型 I 或類型 III 測試的結果更容易。基於這個原因，我的初步建議是，如果您無法想出任何直接映射到研究問題的明顯模型比較，但仍想在不平衡設計中運行 ANOVA，類型 II 測試可能是比類型 I 或類型 III 更好的選擇。14\n\n\n\n13.10.6 效果量(還有非加成性平方差總和)\njamovi 在您選擇這些選項時還會提供效應大小 \\(\\eta^2\\) 和部分 \\(\\eta^2\\)，如 Figure 13.30。然而，在不平衡設計中，涉及的額外複雜性有所增加。\n如果您回顧我們對 ANOVA 的早期討論，其中一個關鍵想法是在平方和計算的背後，如果我們把所有與模型中的效應相關的 SS 項加起來，再加上殘差 SS，它們應該加起來等於總平方和。此外，\\(\\eta^2\\) 背後的整個想法是，因為您將某個 SS 項除以總 SS 值，\\(\\eta^2\\) 值可以解釋為由特定項解釋的變異比例。但在不平衡設計中，這並不那麼簡單，因為有些變異會”丟失”。\n起初這似乎有點奇怪，但原因是這樣的。當您擁有不平衡設計時，您的因子會相互關聯，因此很難分辨因子 A 的效應和因子 B 的效應。在極端情況下，假設我們進行了一個 \\(2 \\times 2\\) 設計，每個組中的參與者人數如 Table 13.29。\n\n\n\n\n\nTable 13.29:  2 x 2 非常（非常！）不平衡的因子設計中的 N 參與者 \n\nsugarno sugar\n\nmilk1000\n\nno milk0100\n\n\n\n\n\n在這裡，我們有一個非常不平衡的設計：100人有牛奶和糖，100人沒有牛奶和糖，就是這樣。有0人有牛奶沒有糖，也有0人有糖沒有牛奶。現在假設，當我們收集數據時，發現”牛奶和糖”組與”無牛奶無糖”組之間存在很大（並且具有統計顯著性）的差異。這是糖的主要效應嗎？牛奶的主要效應？還是交互作用？這是不可能知道的，因為糖的存在與牛奶的存在具有完美的關聯。現在假設設計稍微平衡一些（Table 13.30）。\n\n\n\n\n\nTable 13.30:  2 x 2 仍然非常不平衡的因子設計中的 N 參與者 \n\nsugarno sugar\n\nmilk1005\n\nno milk5100\n\n\n\n\n\n這次，從技術上講，可以區分牛奶和糖的效應，因為我們有一些人只擁有其中之一。然而，由於糖和牛奶之間的關聯仍然非常強烈，兩個小組中的觀察值非常少，因此進行區分仍然相當困難。再次，我們很可能處於這樣一種情況：我們知道預測變量（牛奶和糖）與結果（發聲）相關，但我們不知道這種關係的本質是一個或另一個預測因子的主要效應，還是交互作用。\n在不平衡設計的背景下，當糖和牛奶的效應相互關聯時，應該選擇 Type II ANOVA 進行分析，因為它更易於解釋並可以避免因變量順序或對比方式帶來的問題。無論您是使用什麼軟件，都應該在報告結果時說明您使用的是哪種類型的 ANOVA 測試，以確保分析結果的準確性。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html#本章小結",
    "href": "13-Factorial-ANOVA.html#本章小結",
    "title": "13  多因子變異數分析",
    "section": "13.11 本章小結",
    "text": "13.11 本章小結\n\n平衡且無交互作用的因子設計分析以及有交互作用因子設計分析\n因子設計變異數分析的效果量估計平均值以及信賴區間。\n檢核變異數分析的執行條件\n共變數分析 (ANCOVA)\n變異數分析就是線性模型還有各種多重比較方案\n事後檢定談到杜凱氏HSD，還有提到規劃使用事前檢定方法要思考的條件。\n不平衡的因子設計分析\n\n\n\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A second-level course. Oxford University Press.\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods. Chapman; Hall."
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方適合度檢定",
    "href": "14-Categorical-data-analysis.html#卡方適合度檢定",
    "title": "14  類別資料分析",
    "section": "14.1 卡方適合度檢定",
    "text": "14.1 卡方適合度檢定\n卡方適合度檢定是最古老的假設檢定之一。它由 Karl Pearson 在上世紀初發明 (Pearson, 1900)，稍後由 Sir Ronald Fisher (Fisher, 1922) 進行了一些修正。它檢驗名義變量的觀察頻率分佈是否符合預期的頻率分佈。例如，假設一組病人正在接受實驗性治療，並對他們的健康狀況進行評估，以了解他們的病情是否有改善、保持不變或惡化。適合度檢定可以用來確定每個類別中的數字 - 改善、無變化、惡化 - 是否與標準治療選項預期的數字相匹配。讓我們用一些心理學研究案例來更深入認識。\n\n\n14.1.1 撲克牌花色T恤銷售數據\n多年來，許多研究表明人類很難模擬隨機性。儘管我們努力以“隨機”方式行事，但我們以模式和結構進行思考，因此，當被要求“隨機做某事”時，人們實際上做的事情完全不是隨機的。因此，研究人類隨機性（或者可能是非隨機性）揭示了關於我們如何看待世界的許多深刻心理問題。考慮到這一點，讓我們思考一個非常簡單的研究。假設我要求人們想像一副洗牌的牌，然後在這副想象中的牌中“隨機”選擇一張牌。在他們選擇一張牌之後，我要求他們心理上選擇第二張牌。對於兩個選擇，我們要看的是人們選擇的花色（紅心、梅花、黑桃或方塊）。在問了，例如，\\(N = 200\\)個人之後，我想查看數據並確定人們假裝選擇的牌是否真的隨機。數據包含在 randomness.csv 文件中，當你用 jamovi 打開它並查看電子表格視圖時，你會看到三個變量。這些是：為每個參與者分配唯一標識符的 id 變量，以及表示人們選擇的紙牌花色的兩個變量 choice_1 和 choice_2。\n目前，讓我們只專注於人們做出的第一個選擇。我們將使用“探索” - “描述性”下的頻率表選項來計算我們觀察到的人們選擇每個花色的次數。我們得到了這個（Table 14.1）：\n\n\n\n\n\n\nTable 14.1:  選擇每個花色的次數 \n\nclubsdiamondsheartsspades\n\n35516450\n\n\n\n\n\n這個小頻率表非常有幫助。看著它，人們可能比選擇梅花更容易選擇紅心的暗示有點明顯，但僅僅看它並不完全明顯，這是真的還是僅僅是巧合。所以我們可能需要進行某種統計分析來找出答案，這就是我將在下一節談論的內容。\n很好。從這一點開始，我們將把這個表視為我們要分析的數據。然而，由於我將不得不用數學術語（抱歉！）談論這些數據，所以明確表示符號可能是個好主意。在數學符號中，我們將易讀的單詞“觀察”縮短為字母 \\(O\\)，並使用下標表示觀察的位置。所以我們表格中的第二個觀察用數學表示為 \\(O_2\\)。英語描述與數學符號之間的關係在 Table 14.2 中說明。\n\n\n\n\n\n\nTable 14.2:  英語描述與數學符號之間的關係 \n\nlabelindex, imath. symbolthe value\n\nclubs, \\( \\clubsuit \\)1\\( O_1 \\)35\n\ndiamonds, \\( \\diamondsuit \\)2\\( O_2 \\)51\n\nhearts, \\( \\heartsuit \\)3\\( O_3 \\)64\n\nspades, \\( \\spadesuit \\)4\\( O_4 \\)50\n\n\n\n\n\n希望這很清楚。同時值得注意的是，數學家更喜歡談論一般事物而不是具體事物，因此您還會看到符號 \\(O_i\\)，它指的是在第 i 類中的觀察數（其中 i 可能是 1，2，3 或 4）。最後，如果我們要引用所有觀察到的頻率，統計學家將所有觀察值組合成一個向量 2，我將其稱為 \\(O\\)。\n\\[O = (O_1, O_2, O_3, O_4)\\]\n同樣，這沒有什麼新鮮有趣的地方。這只是符號。如果我說 \\(O = (35, 51, 64, 50)\\)，我所做的就是描述觀察到的頻率表（即觀察到的），但我使用數學符號來表示它。\n\n\n\n\n14.1.2 虛無假設與對立假設\n正如上一節所示，我們的研究假設是“人們不會隨機選擇牌”。現在我們要做的是將其轉換為一些統計假設，然後構建對這些假設的統計檢驗。我將向您介紹的檢驗是 Pearson 的 \\(\\chi^2\\) (chi-square) 適合度檢驗，正如往往是這樣的情況，我們必須通過仔細構建零假設來開始。在這種情況下，這很容易。首先，讓我們用文字陳述零假設：\n\\[H_0: \\text{ 四種花色被選擇的概率相等}\\]\n現在，由於這是統計學，我們必須用數學方式表達相同的事情。為此，讓我們使用表示符號 \\(P_j\\) 來表示選擇第 j 種花色的真實概率。如果零假設成立，那麼四種花色中的每一種都有 25% 的概率被選中。換句話說，我們的零假設聲稱 \\(P_1 = .25\\)，\\(P_2 = .25\\)，\\(P_3 = .25\\)，最後 \\(P_4 = .25\\)。然而，正如我們可以將觀察到的頻率分組成一個向量 O 來概括整個數據集一樣，我們可以使用 P 來表示與我們零假設相對應的概率。所以，如果我讓向量 \\(P = (P_1, P_2, P_3, P_4)\\) 表示描述我們零假設的概率集合，那麼我們有：\n\\[H_0: P =(.25, .25, .25, .25)\\]\n在這個特定實例中，我們的零假設對應於一個概率向量 P，其中所有的概率彼此相等。但這不一定是這樣。例如，如果實驗任務是讓人們想像他們正在抽取一副牌，這副牌的梅花數量是其他花色的兩倍，那麼零假設將對應於像 \\(P = (.4, .2, .2, .2)\\) 這樣的東西。只要求概率都是正數，並且它們的總和為 1，那麼它就是零假設的合法選擇。然而，適合度檢驗最常用於檢驗所有類別等可能的零假設，所以我們將在我們的示例中堅持使用這個。\n那麼我們的對立假設 \\(H_1\\) 呢？我們真正感興趣的是證明所涉及的概率並不都相同（也就是說，人們的選擇不是完全隨機的）。因此，我們假設的“適合人類理解”的版本看起來像這樣：\n\\(H_0: \\text{ 四種花色被選擇的概率相等}\\) \\(H_1: \\text{ 至少有一個選擇花色的概率不是 0.25}\\)\n…以及“數學家友好”版本是：\n\n\\(H_0: P= (.25, .25, .25, .25)\\) \\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n14.1.3 適合度檢定統計程序\n在此階段，我們有我們觀察到的頻率 O 和一組與我們要測試的零假設相對應的概率 P。我們現在要做的是構造一個零假設的檢驗。與往常一樣，如果我們想要檢驗 \\(H_0\\) 和 \\(H_1\\)，我們將需要一個檢驗統計量。適合度檢驗所使用的基本技巧是構造一個檢驗統計量，用來衡量數據與零假設之間的“接近度”。如果數據與零假設為真時所期望的情況不相符，那麼它可能不是真的。好吧，如果零假設是真的，我們會看到什麼呢？或者，使用正確的術語，這些是什麼期望頻率。總共有 \\(N = 200\\) 次觀察，並且（如果零假設為真），任何一個選擇紅心的概率是 \\(P_3 = .25\\)，所以我猜我們期望 \\(200 \\times .25 = 50\\) 顆紅心，對吧？或者，更具體地說，如果我們讓 Ei 指的是“在零假設為真時，我們期望觀察到的第 i 類反應的數量”，那麼\n\\[E_i=N \\times P_i\\]\n這非常容易計算。如果有 200 個觀察可以分為四個類別，我們認為所有四個類別的可能性相等，那麼平均每個類別應該有 50 個觀察，對吧？\n那麼，我們如何將這個轉換為檢驗統計量呢？顯然，我們要做的是比較每個類別的期望觀察數量（\\(E_i\\)）與該類別的實際觀察數量（\\(O_i\\)）。基於這種比較，我們應該能夠得出一個好的檢驗統計量。首先，讓我們計算零假設期望我們找到的數量和我們實際找到的數量之間的差異，也就是，我們計算“觀察到的值減去期望值”的差分數，\\(O_i - E_i\\)。這在 Table 14.3 中說明。\n\n\n\n\n\nTable 14.3:  預期和觀察到的頻率 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)50505050\n\nobserved frequency \\( O_i\\)35516450\n\ndifference score \\( O_i-E_i\\)-151140\n\n\n\n\n\n因此，根據我們的計算，顯然人們選擇紅心的次數比零假設預測的多，而選擇梅花的次數則少。然而，稍作思考便會發現，這些原始差異並非我們所尋求的。直覺上，當零假設預測觀察次數過少（就像紅心那樣）時，它與預測觀察次數過多（就像梅花那樣）一樣糟糕。因此，對於梅花有負數，對於紅心有正數有點奇怪。解決這個問題的一個簡單方法是將所有數字平方，這樣我們現在就可以計算平方差，\\((E_i - O_i)^2\\)。就像以前一樣，我們可以手工計算（Table 14.4）。\n\n\n\n\n\nTable 14.4:  將差異得分平方 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n22511960\n\n\n\n\n\n現在我們取得了進展。現在我們有了一組數字，在零假設做出糟糕預測時（梅花和紅心），數字很大，但在做出好的預測時（方塊和黑桃），數字很小。接下來，由於我將在稍後解釋的一些技術原因，讓我們也將這些數字除以期望頻率 Ei，因此我們實際上計算的是 \\(\\frac{(E_i-O_i)^2}{E_i}\\)。由於我們例子中所有類別的 \\(E_i = 50\\)，所以這不是一個非常有趣的計算，但無論如何，讓我們這麼做（Table 14.5）。\n\n\n\n\n\nTable 14.5:  將平方差異得分除以期望頻率以提供一個’誤差’得分 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\n4.500.023.920.00\n\n\n\n\n\n實際上，我們在這裡得到的是四個不同的“誤差”得分，每個都告訴我們在嘗試用零假設預測我們的觀察頻率時，空假設所犯的“錯誤”有多大。因此，為了將其轉換為有用的檢驗統計量，我們可以做的一件事就是將這些數字加起來。結果被稱為適合度統計量，通常表示為 \\(\\chi^2\\)（卡方）或 GOF。我們可以像 Table 14.6 中那樣計算它。\n\\[\\sum( (observed - expected)^2 / expected )\\]\n這給了我們一個 8.44 的值。\n[額外的技術細節 3]\n如我們從計算中看到的，在我們的卡片數據集中，我們得到了一個 \\(\\chi^2\\) = 8.44 的值。那麼現在的問題是，這個值是否足夠大以拒絕零假設？\n\n\n\n14.1.4 適合度檢定的樣本分佈\n要確定某個 \\(\\chi^2\\) 值是否足夠大，以便拒絕零假設，我們需要確定如果零假設為真，\\(\\chi^2\\) 的抽樣分佈會是什麼。因此，在本節中我將做的就是這件事。我將詳細向您展示這個抽樣分佈的構建方式，然後在下一節中使用它構建一個假設檢驗。如果你想跳過本節的其他部分，並且願意相信抽樣分佈是具有 \\(k - 1\\) 自由度的 \\(\\chi^2\\)（卡方）分佈，你可以跳過本節的其他部分。然而，如果您想了解為什麼適合度檢驗的工作方式是這樣的，請繼續閱讀。\n好吧，讓我們假設零假設實際上是真的。如果是這樣，那麼觀察值落入第 i 類的真實概率是 \\(P_i\\)。畢竟，這幾乎就是我們的零假設的定義。讓我們思考一下這實際上意味著什麼。這有點像說“自然”通過翻轉一個加權硬幣（即，得到正面的概率是 \\(P_j\\) ）來決定觀察值是否最終落入第 i 類。因此，我們可以將觀察到的頻率 \\(O_i\\) 想像成自然界翻轉了 N 個這樣的硬幣（數據集中每個觀察值各有一個），並且其中正好 \\(O_i\\) 個硬幣是正面朝上。顯然，這是一種非常奇怪的思考實驗的方式。但是，這樣做（希望）是提醒你我們之前其實已經見過這種情景。這與 Chapter 7 中 Section 7.4 部分所提到的設置完全相同。換句話說，如果零假設是真的，那麼我們的觀察頻率是由抽樣自二項分佈生成的：\n\\[O_i \\sim Binomial(P_i,N) \\]\n現在，如果您還記得我們對 Section 8.3.3 限定定理的討論，特別是當 \\(N\\) 較大且 \\(P_i\\) 與 0 或 1 不太接近時，二項分佈看起來幾乎與正態分佈相同。換句話說，只要 \\(N^P_i\\) 足夠大。或者，換句話說，當期望頻率 Ei 足夠大時，\\(O_i\\) 的理論分佈近似為正態分佈。更好的是，如果 \\(O_i\\) 是正態分佈的，那麼 \\((O_i-E_i)/\\sqrt{(E_i)}\\) 也是正態分佈的。因為 \\(E_i\\) 是一個固定值，所以減去 Ei 並除以？ Ei 改變了正態分佈的均值和標準差，但這就是它所做的全部。好吧，現在讓我們看看我們的適合度統計量實際上是什麼。我們正在做的是取一堆近似正態分佈的東西，將它們平方，然後加起來。等等。我們之前也見過這個！正如我們在談論@sec-Other-useful-distributions時所討論的那樣，當您取一堆具有標準正態分佈（即，均值為 0 且標準差為 1）的東西，將它們平方然後加起來時，所得到的數量具有卡方分佈。所以現在我們知道了零假設預測適合度統計量的抽樣分佈是卡方分佈。很酷。\n還有一個最後的細節要談論，即自由度。如果您回想一下@sec-Other-useful-distributions，我說過如果你加起來的東西數量是 k，那麼生成的卡方分佈的自由度就是 k。然而，在本節開始時我所說的是，卡方適合度檢驗的實際自由度是 \\(k - 1\\)。這是怎麼回事呢？答案在於，我們應該考慮的是被加在一起的真正獨立的東西數量。正如我將在下一節中談論的那樣，儘管我們加起來有 k 個東西，但只有 \\(k - 1\\) 個東西是真正獨立的，所以自由度實際上只有 \\(k - 1\\)。這就是下一節的主題4。\n\n\n\n14.1.5 自由度\n\n\n\n\n\nFigure 14.1: 卡方分佈具有不同“自由度”值\n\n\n\n\n當我在@sec-Other-useful-distributions中介紹卡方分佈時，對於“自由度”的含義有點籠統。顯然，它很重要。觀察@fig-fig14-1，可以看到如果我們改變自由度，那麼卡方分佈的形狀會發生很大變化。但它究竟是什麼呢？同樣，當我介紹分佈並解釋它與正態分佈的關係時，我確實提供了一個答案：它是我要平方並相加的“正態分布變量”的數量。但是，對於大多數人來說，這有點抽象，並不十分有幫助。我們真正需要做的是嘗試根據我們的數據來理解自由度。下面就讓我們開始吧。\n自由度背後的基本概念非常簡單。您計算它的方法是將描述數據的不同“數量”加起來，然後減去這些數據必須滿足的所有“約束”。5這有點籠統，所以讓我們用我們的撲克牌數據作為一個具體例子。我們使用四個數字來描述我們的數據，分別是 \\(O1, O2, O3\\) 和 \\(O4\\)，對應於四個不同類別（紅心，梅花，方塊，黑桃）的觀察頻率。這四個數字是我們實驗的隨機結果。但是我的實驗實際上內置了一個固定的約束：樣本大小 \\(N\\)。6也就是說，如果我們知道\n有多少人選擇紅心，有多少人選擇方塊，以及有多少人選擇梅花，那麼我們就能確切地知道有多少人選擇黑桃。換句話說，雖然我們的數據是用四個數字描述的，但它們實際上只對應於 \\(4 - 1 = 3\\) 个自由度。稍微不同的思考方式是注意到我們感興趣的四個概率（同樣，對應於四個不同類別），但是這些概率必須加起來等於一，這將施加一個約束。因此，自由度是 \\(4 - 1 = 3\\)。無論您想用觀察頻率還是概率的方式來思考它，答案都是一樣的。通常，當進行涉及 \\(k\\) 個組的卡方適合度檢定時，自由度將為 \\(k - 1\\)。\n\n\n\n14.1.6 檢定虛無假設\n構建假設檢定的過程的最後一步是找出拒絕域是什麼。也就是說，哪些 \\(\\chi^2\\) 值會讓我們拒絕零假設。如我們之前所見，\\(\\chi^2\\) 的大值意味著零假設在預測我們實驗中的數據方面做得很差，而 \\(\\chi^2\\) 的小值則意味著它實際上做得相當好。因此，一個相當明智的策略是說，有一個臨界值，如果 \\(\\chi^2\\) 大於臨界值，我們拒絕零假設；但如果 \\(\\chi^2\\) 小於這個值，我們保留零假設。換句話說，用我們在 Chapter 9 中引入的語言，卡方適合度檢定總是一個單邊檢定。好的，所以我們要做的就是找出這個臨界值。這很簡單。如果我們希望檢定具有顯著性水平 \\(\\alpha = .05\\)（即，我們願意容忍 Type I 錯誤率為 \\(5%\\)），那麼我們必須選擇我們的臨界值，使得在零假設成立的情況下，\\(\\chi^2\\) 達到那麼大的概率只有 5%。這在 Figure 14.2 中得到了說明。\n\n\n\n\n\n\nFigure 14.2: \\(\\chi^2\\)（卡方）適合度檢定的假設檢定如何運作的示意圖\n\n\n\n\n啊，但我聽到你問，如何找到具有 \\(k-1\\) 自由度的卡方分布的臨界值？很多很多年前，當我第一次上心理統計課時，我們曾經在一本臨界值表的書中查找這些臨界值，就像 Figure 14.3 中那樣。看這個圖，我們可以看到具有 3 個自由度的 \\(\\chi^2\\) 分布的臨界值，且 p=0.05 是 7.815。\n\n\n\n\n\n\n\nFigure 14.3: 卡方分布的臨界值表\n\n\n\n\n因此，如果我們計算出的 \\(\\chi^2\\) 統計量大於 7.815 的臨界值，那麼我們可以拒絕零假設（記住，零假設 \\(H_0\\) 是所有四個花色都以相等的概率被選擇）。既然我們之前已經計算過了（即，\\(\\chi^2\\) = 8.44），我們可以拒絕零假設。基本上就是這樣了。現在你知道了“皮爾森卡方適合度檢定”。真幸運。\n\n\n\n14.1.7 jamovi實作\n毫不意外地，jamovi 提供了一個分析工具，可以幫你完成這些計算。讓我們使用 Randomness.omv 文件。在主要的“分析”工具欄中，選擇“頻率” - “單樣本比例檢驗” - “\\(N\\) 個結果”。然後在出現的分析視窗中將要分析的變量（從選擇 1 開始）移到“變量”框中。此外，單擊“預期計數”復選框，以便將這些數據顯示在結果表中。完成所有這些操作後，你應該會在 jamovi 中看到分析結果，如 Figure 14.4。然後不出所料，jamovi 提供了與我們上面手動計算相同的預期計數和統計數據，\\(\\chi^2\\) 值為 \\((8.44\\)，自由度為 \\(3\\)，\\(p=0.038\\)。注意，我們不再需要查找臨界 p 值閾值，因為 jamovi 給出了在 \\(3\\) 自由度下計算得出的 \\(\\chi^2\\) 的實際 p 值。\n\n\n\n\n\n\n\nFigure 14.4: jamovi 中的 \\(\\chi^2\\) 單樣本比例檢驗，表格顯示觀察到的頻率和比例以及期望的頻率和比例\n\n\n\n\n\n\n14.1.8 另一種虛無假設\n此時，你可能會想知道如果你想進行適合度檢驗，但你的零假設不是所有類別的概率都相等該怎麼辦。例如，假設有人提出了這樣的理論預測，即人們應該以 \\(60\\%\\) 的概率選擇紅色牌，以 \\(40\\%\\) 的概率選擇黑色牌（我不知道為什麼你會這樣預測），但沒有其他偏好。如果是這樣，零假設將期望選擇愛心的比例為 \\(30\\%\\)，選擇方塊的比例為 \\(30\\%\\)，選擇黑桃的比例為 \\(20\\%\\)，選擇梅花的比例為 \\(20\\%\\)。換句話說，我們期望愛心和方塊的出現次數是黑桃和梅花的 1.5 倍（\\(30\\%\\) : \\(20\\%\\) 的比例與 1.5 : 1 相同）。對我來說，這似乎是一個愚蠢的理論，但是用我們的 jamovi 分析可以很容易地測試這個明確指定的零假設。在分析視窗中（標記為“比例檢驗（N個結果）”的 Figure 14.4 中，你可以展開“預期比例”的選項。當你這樣做時，將會出現一些選項，讓你為選定的變量輸入不同的比例值，在我們的案例中，這個變量是 choice 1。將比例更改為反映新的零假設，如 Figure 14.5 所示，並觀察結果如何變化。\n\n\n\n\n\n\nFigure 14.5: 在 jamovi 中更改 \\(\\\\chi^2\\) 單樣本比例檢驗的預期比例\n\n\n\n\n預期計數現在顯示在 Table 14.6 中。\n\n\n\n\n\nTable 14.6:  不同零假設的預期計數 \n\n\\( \\clubsuit \\)\\( \\diamondsuit \\)\\( \\heartsuit \\)\\( \\spadesuit \\)\n\nexpected frequency \\( E_i\\)40606040\n\n\n\n\n\n\\(\\chi^2\\) 統計量為 4.74，自由度為 3，\\(p = 0.182\\)。現在，我們更新的假設和預期頻率與上次的結果有所不同。因此，我們的 \\(\\chi^2\\) 檢驗統計量和 p 值也有所不同。令人惱火的是，p 值為 \\(.182\\)，因此我們不能拒絕零假設（回顧 Section 9.5 提醒自己為什麼）。可悲的是，儘管零假設對應著一個非常愚蠢的理論，這些數據並沒有提供足夠的證據來反駁它。\n\n\n\n14.1.9 適合度檢定的報告寫作\n現在你知道了這個測試的運作方式，也知道如何使用神奇的jamovi計算盒來進行測試。接下來你需要知道的是如何撰寫結果。畢竟，設計和執行實驗，然後分析數據，如果不告訴別人結果是沒有意義的！所以讓我們來談談在報告分析時需要做的事情。讓我們繼續以撲克牌花色為例。如果我想將這個結果寫成一篇論文之類的東西，那麼慣常的報告方式是這樣寫的：\n\n在實驗的200名參與者中，有64人首選紅心，51人選擇方塊，50人選擇黑桃，35人選擇梅花。進行了卡方適合度檢驗以測試四種花色的選擇概率是否相同。結果顯著（\\(\\chi^2(3) = 8.44, p< .05\\)），這表明人們在選擇花色時並非完全隨機。\n\n這相當直接，希望它看起來很不起眼。儘管如此，你應該注意到這個描述中的幾點內容：\n\n描述統計數據在統計檢驗之前。也就是說，在進行檢驗之前，我告訴讀者有關數據的一些信息。通常，這是一個很好的做法。永遠記住，你的讀者對你的數據了解得遠不如你。因此，除非你妥善地向他們描述，否則統計檢驗對他們來說毫無意義，他們會感到沮喪和哭泣。\n描述告訴你正在測試的虛無假設是什麼。老實說，作者並不總是這樣做，但在存在一定歧義的情況下，或者在你不能依賴你的讀者非常熟悉你正在使用的統計工具時，這通常是一個好主意。很多時候讀者可能不知道（或記不起）你正在使用的檢驗的所有細節，所以提醒他們是一種禮貌！對於適合度檢驗來說，你通常可以依賴科學觀眾了解它的運作方式（因為它涵蓋在大多數入門統計課程中）。然而，明確陳述虛無假設（簡要地！）仍然是一個好主意，因為虛無假設可能因你使用檢驗的目的而有所不同。例如，在撲克牌的例子中，我的虛無假設是四個花色的概率相同（即，\\(P1 = P2 = P3 = P4 = 0.25\\)），但這個假設並沒有什麼特別的。我可以同樣使用適合度檢驗測試虛無假設，即\\(P_1 = 0.7\\)和\\(P2 = P3 = P4 = 0.1\\)。所以，向讀者解釋你的虛無假設是有幫助的。另外，注意到我用文字而不是數學描述虛無假設。這是完全可以接受的。你可以用數學描述它，但是因為大多數讀者發現文字比符號更容易閱讀，所以大多數作者傾向於用文字描述虛無假設（如果可以的話）。\n包括”統計塊”。在報告檢驗結果本身時，我不僅僅說結果顯著，還包括了一個“統計塊”（即括號內密集的數學部分），其中報告了所有“關鍵”統計信息。對於卡方適應度檢驗，報告的信息包括檢驗統計量（即適應度統計量為8.44）、用於檢驗的分布信息（具有3個自由度的\\(\\chi^2\\)，通常縮寫為\\(\\chi^2\\)(3)），然後是結果是否顯著（在本例中為\\(p< .05\\)）。每個檢驗所需的統計塊中的特定信息各不相同，因此每次我介紹一個新檢驗時，我都會向您展示統計塊應該是什麼樣子。7 但是，一般原則是您應該始終提供足夠的信息，以便讀者在需要時可以自己檢查測試結果。\n對結果進行解釋。除了指出結果顯著之外，我還提供了結果的解釋（即，人們沒有隨機選擇）。這對讀者也是一種友善，因為它告訴他們關於數據中發生了什麼事的一些信息。如果不包括這樣的東西，讀者很難理解發生了什麼事。8\n\n正如其他所有事物一樣，你應該首要關注的是向讀者解釋事物。永遠記住，報告結果的目的是與另一個人溝通。我無法告訴您我看過多少次報告、論文甚至科學文章的結果部分就是胡言亂語，因為作者只關注確保包含所有數字，卻忘記了與人類讀者真正交流。\n\n撒旦在統計和引用經文中同樣感到高興9 – H.G. 威爾斯"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方獨立性檢定",
    "href": "14-Categorical-data-analysis.html#卡方獨立性檢定",
    "title": "14  類別資料分析",
    "section": "14.2 卡方獨立性檢定",
    "text": "14.2 卡方獨立性檢定\n\n警衛機器人 1：停！\n警衛機器人 2：你是機器人還是人類？\n莉娜：機器人…我們是。\n弗萊：呃，對！只是兩個機器人在機器人世界裡像機器人一樣生活！呃？\n警衛機器人 1：進行測試。\n警衛機器人 2：以下哪一個是你最喜歡的？A：一只小狗，B：來自你心上人的漂亮花朵，或C：一個大的、格式正確的數據文件？\n警衛機器人 1：選擇！\n《飛出個未來》第一季第5集”Fear of a Bot Planet”台詞(1999~2003的美國情境喜劇；台灣無代理商引進播映)\n\n有一天，我在觀看一部動畫紀錄片，研究Chapek 9星球上當地人的古怪風俗。顯然，要進入他們的首府，訪客必須證明他們是機器人而不是人類。為了確定訪客是否是人類，當地人會詢問訪客是喜歡小狗、花還是大型、格式正確的數據文件。我心想：“相當聰明，但如果人類和機器人有相同的喜好呢？那可能不是一個很好的測試了，對吧？”事實上，我得到了Chapek 9市民當局用來檢查這個問題的測試數據。他們所做的非常簡單。他們找了一堆機器人和一堆人，問他們喜歡什麼。我把他們的數據保存在一個名為chapek9.omv的文件中，現在我們可以將其加載到jamovi中。除了識別個人的ID變量外，還有兩個名義文本變量，species和choice。總共有180個條目在數據集中，每個人（將機器人和人類都算作“人”）被要求做出選擇。具體而言，有93個人類和87個機器人，絕大多數人選擇了數據文件。你可以通過在’探索’-‘描述統計’按鈕下詢問jamovi的頻率表來自己檢查這一點。然而，這個總結並沒有解決我們感興趣的問題。要做到這一點，我們需要對數據進行更詳細的描述。我們想要做的是查看按物種劃分的選擇。也就是說，我們需要對數據進行交叉分類（見@sec-Tabulating-and-cross-tabulating-data）。在jamovi中，我們使用’頻率’-‘列聯表’-’獨立樣本’分析來完成這個操作，我們應該得到一個類似@tbl-tab14-7的表格。\n\n\n\n\n\n\nTable 14.7:  交叉分類數據 \n\nRobotHumanTotal\n\nPuppy131528\n\nFlower301343\n\nData4465109\n\nTotal8793180\n\n\n\n\n\n從這個表格中，我們可以很清楚地看到，絕大多數人類選擇了數據文件，而機器人在他們的選擇中則相對更均衡。撇開為什麼人類可能更喜歡選擇數據文件的問題（這確實看起來有點奇怪，承認吧），我們首要任務是確定數據集中人類選擇和機器人選擇之間的差異是否具有統計顯著性。\n\n\n14.2.1 建立獨立性的假設檢定\n我們如何分析這些數據？具體來說，由於我的研究假設是”人類和機器人回答問題的方式不同”，我該如何構建對虛無假設的檢驗，即”人類和機器人以相同的方式回答問題”？與以前一樣，我們首先建立一些符號來描述數據（Table 14.8）。\n\n\n\n\n\nTable 14.8:  描述數據的符號 \n\nRobotHumanTotal\n\nPuppy\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nFlower\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nData\\(O_{31}\\)\\(O_{32}\\)\\(R_{3}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)N\n\n\n\n\n\n在這個符號中，我們用 \\(O_{ij}\\) 表示被調查者在物種 j（機器人或人類）中選擇 i（小狗，花或數據）的計數（觀察頻率）。總觀察數通常表示為 \\(N\\)。最後，我用 \\(R_i\\) 表示行總數（例如，\\(R_1\\) 是選擇花的人的總數），用 \\(C_j\\) 表示列總數（例如，\\(C_1\\) 是機器人的總數）。10\n那麼，現在讓我們考慮一下虛無假設是什麼。如果機器人和人類對這個問題的回答方式相同，那麼“機器人說小狗”的概率與“人類說小狗”的概率相同，對其他兩種可能性也是如此。所以，如果我們用 \\(P_{ij}\\) 表示“物種 j 的成員給出回應 i 的概率”，那麼我們的虛無假設是：\n\\[\n\\begin{aligned}\nH_0 &: \\text{以下全部成立：} \\\\\n&P_{11} = P_{12}\\text{ （選擇“小狗”的概率相同），} \\\\\n&P_{21} = P_{22}\\text{ （選擇“花”的概率相同），以及} \\\\\n&P_{31} = P_{32}\\text{ （選擇“數據”的概率相同）}\n\\end{aligned}\n\\]\n事實上，由於虛無假設聲稱真實的選擇概率不取決於做出選擇的人的物種，我們可以讓 Pi 代表這個概率，例如，P1 是選擇小狗的真實概率。\n接下來，就像我們在適配度檢驗中所做的那樣，我們需要計算期望頻率。也就是說，對於每個觀察到的計數 \\(O_{ij}\\)，我們需要弄清楚虛無假設告訴我們期望什麼。我們用 \\(E_{ij}\\) 表示這個期望頻率。這次，情況有點棘手。如果物種 \\(j\\) 中有 \\(C_j\\) 人，而無論物種如何選擇選項 \\(i\\) 的真實概率為 \\(P_i\\)，那麼期望頻率只是：\n\\[E_{ij}=C_j \\times P_i\\]\n現在，這固然很好，但我們遇到了一個問題。與適配度檢驗的情況不同，虛無假設實際上並未指定 Pi 的特定值。\n這是我們必須估計（見 Chapter 8）的數據！幸運的是，這非常容易做到。如果有 28 位擇一的人選擇了花朵，那麼選擇花朵的概率的自然估計就是 \\(\\frac{28}{180}\\)，大約是 \\(0.16\\)。如果我們用數學語言來表示，我們所說的是，選擇選項 i 的概率估計只是行總數除以總樣本量：\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\n因此，我們的期望頻率可以寫成行總數和列總數的乘積（即相乘），除以總觀察次數：11\n\\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]\n\n[額外的技術細節12]\n與以前一樣，\\(X^2\\) 的大值表示虛無假設對數據的描述效果不佳，而 \\(X^2\\) 的小值則表示虛無假設對數據的解釋效果很好。因此，就像上次一樣，如果 \\(X^2\\) 太大，我們希望拒絕虛無假設。\n不出所料，這個統計量遵循 \\(\\chi^2\\) 分布。我們需要做的就是弄清楚有多少自由度，實際上這並不困難。如我之前提到的，你可以（通常）將自由度視為等於你正在分析的數據點數量，減去約束的數量。具有 r 行和 c 列的列聯表包含總共 \\(r^{c}\\) 個觀察到的頻率，所以這是觀察到的總數量。約束呢？在這裡，情況稍微複雜一些。答案始終是相同的\n\\[df=(r-1)(c-1)\\]\n但是根據實驗設計，解釋為什麼自由度具有此值的原因是不同的。為了便於說明，假設我們確實打算調查 87 台機器人和 93 人（實驗者固定列總數），但讓行總數自由變化（行總數是隨機變量）。讓我們考慮在這裡適用的約束。好吧，因為我們故意通過實驗者的行為固定了列總數，所以在這裡就有 c 個約束。但實際上，還有更多的約束。記住我們的虛無假設中有一些自由參數（即，我們不得不估計 Pi 值）？這些也很重要。在這本書中，我不會解釋為什麼，但虛無假設中的每個自由參數都像是一個額外的約束。那麼，這些參數有多少呢？好吧，由於這些概率必須加起來等於 1，所以只有 \\(r - 1\\) 個。因此，我們的自由度總數是：\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\n另一種假設，假設實驗者確定的唯一事物是總樣本量 N。也就是說，我們對見到的前 180 人進行了問卷調查，結果發現 87 人是機器人，93 人是人類。這一次，我們的推理會有所不同，但仍然會得到相同的答案。我們的虛無假設仍然有 \\(r - 1\\) 個自由參數，對應於選擇概率，但現在還有 \\(c - 1\\) 個自由參數，對應於種類概率，因為我們還必須估計隨機抽樣的人類確實是機器人的概率。13 最後，由於我們確實確定了觀察數量的總數 N，所以這是另一個約束。因此，現在我們有 rc 次觀察，並且有 \\((c-1)+(r-1)+1\\) 約束。這會給出什麼呢？\n\\[\\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r- 1)(c - 1) \\end{split}\n\\]\n真是令人驚奇。\n\n\n\n\n14.2.2 獨立性檢定實作\n好吧，既然我們知道了檢驗是如何進行的，讓我們看看如何在 jamovi 中完成它。雖然讓您長時間地經歷繁瑣的計算以便被迫學習可能很有吸引力，但我認為這是沒有意義的。在上一節中，我已經向您展示了如何針對適合度檢驗進行長時間的操作，而且由於獨立性檢驗在概念上沒有任何不同，所以您不會通過長時間的操作學到任何新的東西。因此，我將直接向您展示簡單的方法。在 jamovi 中運行檢驗（“頻率” - “列聯表” - “獨立樣本”）之後，您只需查看 jamovi 結果窗口中列聯表下方，那裡就是 \\(\\chi^2\\) 統計量。這顯示了一個 \\(\\chi^2\\) 統計值為 10.72，2 d.f.，p-value = 0.005。\n那很簡單，不是嗎？您還可以要求 jamovi 顯示預期計數 - 只需單擊“Cells”選項中的“Counts” - “Expected”複選框，預期計數將出現在列聯表中。同時，在此操作中，效果大小度量會有所幫助。我們將選擇 Cramér’s \\(V\\)，您可以在“Statistics”選項中的複選框中指定它，它會給出 Cramér’s \\(V\\) 的值為 \\(0.24\\)。參見 Figure 14.6。我們稍後會再談論這個問題。\n\n\n\n\n\n\n\nFigure 14.6: 在 jamovi 中使用 Chapek 9 數據進行獨立樣本 \\(\\chi^2\\) 檢驗\n\n\n\n\n這個輸出為我們提供了足夠的信息來寫出結果：\n\nPearson 的 \\(\\chi^2\\) 顯示了物種和選擇之間存在顯著關聯（\\(\\chi^2(2) = 10.7, p< .01\\)）。機器人似乎更傾向於說他們喜歡花，而人類更傾向於說他們喜歡數據。\n\n注意，再次，我提供了一些解釋，以幫助人類讀者理解數據發生的情況。稍後在我的討論部分，我會提供更多的上下文。舉例來說，這是我可能會在之後說的：\n\n人類似乎比機器人更喜歡原始數據文件，這有點反直覺。但在某種程度上，它是有道理的，因為 Chapek 9 上的民事當局往往在發現人類時會將其殺死並解剖。因此，最有可能的是，人類參與者並未如實回答問題，以避免可能產生不良後果。這應該被認為是一個嚴重的方法論缺陷。\n\n我想，這可以被歸類為反應效應的一個極端例子。顯然，在這種情況下，問題嚴重到研究幾乎毫無價值，作為理解人類和機器人之間的差異偏好的工具。然而，我希望這能夠說明在獲得統計顯著結果（我們拒絕零假設，轉而接受替代假設）和找到具有科學價值的東西（由於嚴重的方法論缺陷，數據對我們研究假設的興趣一無所知）之間的區別。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的校正",
    "href": "14-Categorical-data-analysis.html#卡方檢定的校正",
    "title": "14  類別資料分析",
    "section": "14.3 卡方檢定的校正",
    "text": "14.3 卡方檢定的校正\n好吧，是時候稍作偏離了。到目前為止，我對您有點不誠實。當您只有 1 個自由度時，需要對計算稍作改變。這被稱為 “連續性修正”，或者有時稱為葉氏修正。請記住我之前指出的：\\(\\chi^2\\) 檢定是基於一個近似值，具體來說，是假設當 \\(N\\) 較大時，二項分佈開始類似於正態分佈。這樣做的一個問題是，它通常並不完全奏效，尤其是當你只有 1 個自由度時（例如，當你對一個 \\(2 \\times 2\\) 的列聯表進行獨立性檢定時）。造成這一現象的主要原因是，\\(X^{2}\\) 統計量的真實抽樣分佈實際上是離散的（因為您在處理分類數據！），但 \\(\\chi^2\\) 分佈是連續的。這可能引入系統性問題。具體來說，當 N 很小且 \\(df = 1\\) 時，適合度統計量往往 “太大”，這意味著您實際上的α值比您想象的要大（或者等效地說，p值稍微太小）。\n根據我從閱讀葉氏論文14中獲得的了解，修正基本上是一個突破。它不是源於任何原則性的理論。相反，它是基於對檢定行為的觀察，並發現經過修正的版本似乎效果更好。您可以在 jamovi 的 ‘Statistics’ 選項中的復選框中指定這個修正，其中它被稱為 ‘\\(\\chi^2\\) 連續性修正’。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的效果量",
    "href": "14-Categorical-data-analysis.html#卡方檢定的效果量",
    "title": "14  類別資料分析",
    "section": "14.4 卡方檢定的效果量",
    "text": "14.4 卡方檢定的效果量\n正如我們在 Section 9.8 中早先討論的，要求研究人員報告某種效應量測的情況越來越普遍。因此，假設您已經進行了卡方檢定，結果顯示具有顯著性。所以您現在知道您的變量之間存在某種關聯（獨立性檢定）或與指定概率的某種偏差（適合度檢定）。現在，您想報告一個效應量測。也就是說，假設存在關聯或偏差，其強度如何？\n您可以選擇報告幾種不同的測量值，並使用幾種不同的工具來計算它們。我不會討論所有這些測量值，而是將重點放在最常報告的效應大小測量值上。\n默認情況下，人們最常報告的兩個度量是 \\(\\phi\\) 統計量，以及稍稍優越的版本，稱為克拉默的 \\(V\\) 。\n[額外的技術細節15]\n完成後，這似乎是一個相當受歡迎的度量標準，可能是因為它易於計算，並且給出的答案並不是完全愚蠢的。有了克拉默的 \\(V\\)，您知道這個值確實在 0（完全無關聯）和 1（完全關聯）之間變化。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#卡方檢定的適用條件",
    "href": "14-Categorical-data-analysis.html#卡方檢定的適用條件",
    "title": "14  類別資料分析",
    "section": "14.5 卡方檢定的適用條件",
    "text": "14.5 卡方檢定的適用條件\n所有統計檢驗都有一定的假設，通常檢查這些假設是否符合是一個好主意。對於本章迄今為止討論的卡方檢驗，其假設包括：\n\n期望頻率足夠大。還記得我們在上一節看到的 \\(\\chi^2\\) 抽樣分佈是如何產生的嗎？因為二項分佈非常類似於正態分佈，正如我們在 Chapter 7 中討論的那樣，這只有在觀察次數足夠多的情況下才成立。實際上，這意味著所有的期望頻率都需要相對較大。什麼是合理的大小？意見不一，但默認的假設似乎是你通常希望看到所有的期望頻率都大於大約5，儘管對於更大的表格，如果至少80%的期望頻率在5以上，並且沒有一個低於1，那可能還可以。然而，從我所能發現的資料（例如， Cochran (1954) ）來看，這些似乎是作為粗略指導原則提出的，而不是硬性規定，而且它們似乎有些保守 (Larntz, 1978)。\n數據彼此獨立。卡方檢驗的一個稍微隱藏的假設是，你必須真正相信觀察結果是獨立的。我舉個例子。假設我對在某個特定醫院出生的男嬰的比例感興趣。我在產科病房走來走去，觀察到20個女孩和只有10個男孩。看起來是相當明顯的差異，對吧？但稍後，原來我實際上走進了同一個病房10次，事實上我只看到了2個女孩和1個男孩。現在不那麼令人信服了，是吧？我的原始30個觀察結果大量地不獨立，實際上只相當於3個獨立的觀察結果。顯然這是一個極端（而且非常愚蠢）的例子，但它說明了基本問題。非獨立性會“搞砸事情”。有時它會導致你錯誤地拒絕零假設，就像愚蠢的醫院例子那樣，但它也可能朝相反的方向發展。為了給出一個稍微不那麼愚蠢的例子，讓我們考慮一下如果我對卡片實驗做了一些不同的嘗試會怎麼樣。假設我不是要求200個人嘗試想像隨機抽取一張卡片，而是要求50個人選擇4張卡片。其中一種可能是每個人都選擇一張紅心、一張梅花、一張方塊和一張黑桃（符合“代表性啟發法則”(Tversky & Kahneman, 1974)）。這是人們的高度非隨機行為，但在這種情況下，我對四種花色都會得到50的觀察頻率。對於這個例子，觀察結果非獨立性（因為你選擇的四張卡片之間將彼此關聯）實際上導致了相反的效果，即錯誤地保留了零假設。\n\n如果你碰巧處於獨立性受到違反的情況，你可以使用 McNemar 檢驗（我們將討論）或 Cochran 檢驗（我們不會討論）。同樣，如果你的預期單元計數太小，可以查看 Fisher 確切檢驗。我們現在將轉向這些主題。"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#sec-The-Fisher-test",
    "href": "14-Categorical-data-analysis.html#sec-The-Fisher-test",
    "title": "14  類別資料分析",
    "section": "14.6 費雪精確檢定",
    "text": "14.6 費雪精確檢定\n如果你的單元計數太小，但你仍然想檢驗兩個變量是否獨立的零假設，該怎麼辦？一個答案是“收集更多數據”，但這太輕浮了。有很多情況下，進行此操作要么不可行，要么不道德。如果是這樣，統計學家有一種道德責任，為科學家提供更好的檢驗方法。在這個例子中， Fisher (1922) 恰好提供了問題的正確答案。為了說明這個基本概念，假設我們正在分析一個田野實驗的數據，研究被指控為巫術的人的情感狀況，其中一些人正在被燒死。16 不幸的是，對於科學家來說（但對普通大眾來說相當幸運），實際上很難找到正在被點火的人，所以在某些情況下，單元計數非常小。salem.csv 數據的列聯表說明了這一點（Table 14.9）。\n\n\n\n\n\n\nTable 14.9:  salem.csv 數據的列聯表 \n\nhappyFALSETRUE\n\non.fireFALSE310\n\nTRUE30\n\n\n\n\n\n查看這些數據，你會很難不懷疑沒有被點火的人比正在被點火的人更可能感到快樂。然而，由於樣本量很小，卡方檢驗使得這一點很難檢驗。所以，作為一個不想被點火的人，我非常希望能得到比這更好的答案。這就是費雪精確檢定（Fisher’s exact test）(Fisher, 1922) 派上用場的地方。\n費雪精確檢定的運作方式與卡方檢定（事實上，在本書中我談論的任何其他假設檢定）有所不同，因為它沒有檢定統計量，而是“直接”計算 p 值。我將解釋該檢定在 \\(2 \\times 2\\) 列聯表中的運作基本原理。與以前一樣，讓我們使用一些符號（Table 14.10）。\n\n\n\n\n\n\nTable 14.10:  費雪精確檢定的符號 \n\nHappySadTotal\n\nSet on fire\\(O_{11}\\)\\(O_{12}\\)\\(R_{1}\\)\n\nNot set on fire\\(O_{21}\\)\\(O_{22}\\)\\(R_{2}\\)\n\nTotal\\(C_{1}\\)\\(C_{2}\\)\\(N\\)\n\n\n\n\n\n為了構建檢定，費雪將行和列總數（\\(R_1, R_2, C_1\\) 和 \\(C_2\\)）都視為已知的固定量，然後計算在給定這些總數的情況下，我們會得到我們所觀察到的頻率（\\(O_{11}, O_{12}, O_{21}\\) 和 \\(O_{22}\\)）的概率。在我們在 Chapter 7 中開發的表示法中，這是寫作：\n\\[P(O_{11}, O_{12}, O_{21}, O_{22} \\text{ | } R_1, R_2, C_1, C_2)\\] 並且如您所料，弄清楚這個概率是什麼有點困難。但事實證明，這個概率是由一個稱為超幾何分佈的分佈描述的。要計算我們的 p 值，我們必須計算觀察到這個特定表格或者一個更“極端”的表格的概率。17 在 20 世紀 20 年代，即使在最簡單的情況下，計算這個和也是令人生畏的，但如今只要表格不是太大，樣本量不是太大，這就相當容易了。概念上棘手的問題是弄清楚一個列聯表比另一個列聯表更“極端”的含義。最簡單的解決方案是說，概率最低的表格是最極端的。這將給我們 p 值。\n您可以在 jamovi 中的“列聯表”分析中的“統計”選項中的復選框中指定此檢定。當您使用 salem.csv 文件中的數據執行此操作時，費雪精確檢定統計量將顯示在結果中。我們在這里感興趣的主要是 p 值，這個例子中 p 值足夠小（p = .036），足以證明拒絕那些正在燃燒的人和沒有燃燒的人一樣快樂的零假設。參見 Figure 14.7。\n\n\n\n\n\n\n\n\nFigure 14.7: jamovi 中的費雪精確檢定分析"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#sec-The-McNemar-test",
    "href": "14-Categorical-data-analysis.html#sec-The-McNemar-test",
    "title": "14  類別資料分析",
    "section": "14.7 麥內瑪檢定",
    "text": "14.7 麥內瑪檢定\n假設您被聘請為澳大利亞通用政治黨(AGPP)工作18，您的工作的一部分是了解AGPP政治廣告的有效性。因此，您決定組織一個包含\\(N = 100\\)人的樣本，讓他們觀看AGPP的廣告。在他們看到任何內容之前，您問他們是否打算投票支持AGPP，然後在播放廣告之後再問他們一遍，看看有沒有人改變主意。顯然，如果你擅長你的工作，你還會做很多其他事情，但讓我們考慮一下這個簡單的實驗。描述數據的一種方法是通過 Table 14.11 中顯示的列聯表。\n\n\n\n\n\n\nTable 14.11:  帶有AGPP政治廣告數據的列聯表 \n\nBeforeAfterTotal\n\nYes301040\n\nNo7090160\n\nTotal100100200\n\n\n\n\n\n首先，您可能會認為這種情況適合用皮爾森\\(\\chi^2\\)檢驗獨立性（如[獨立性（或關聯性）的\\(\\chi^2\\)檢驗]）。然而，稍加思考就會發現我們遇到了一個問題。我們有100名參與者，但卻有200個觀察值。這是因為每個人在”之前”的列和”之後”的列中都給出了答案。這意味著這200個觀察值彼此之間並不獨立。如果選民A第一次說“是”，選民B說“否”，那麼您就可以預期選民A第二次比選民B更可能說“是”！由於獨立性假設的違反，這意味著通常的\\(\\chi^2\\)檢驗不會給出可靠的答案。現在，如果這是一個非常不常見的情況，我不會浪費你的時間來討論它。但它並不罕見。這是一個標準的重複測量設計，而到目前為止，我們考慮的所有測試都無法處理它。哎呀。\n問題的解決方案是麥內瑪（McNemar，1947）發表的。訣竅是先用稍微不同的方式整理數據（Table 14.12）。\n\n\n\n\n\nTable 14.12:  當你有重複測量數據時，以不同的方式整理數據 \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes5510\n\nAfter: No256590\n\nTotal3070100\n\n\n\n\n\n接下來，讓我們思考一下我們的零假設是什麼：它是“之前”測試和“之後”測試中有相同比例的人說：“是的，我會投票支持AGPP。” 由於我們已經重新整理了數據，這意味著我們現在正在測試行總數和列總數來自相同分佈的假設。因此，麥內瑪檢驗中的零假設是邊際同質性。也就是說，行總數和列總數具有相同的分佈：\\(P_a + P_b = P_a + P_c\\)，同樣地，\\(P_c + P_d = P_b + P_d\\)。注意，這意味著零假設實際上簡化為Pb = Pc。換句話說，就麥內瑪檢驗而言，只有這個表格中的對角線條目（即b和c）才是重要的！注意到這一點後，麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity)與通常的\\(\\chi^2\\)檢驗沒有什麼不同。在應用Yates修正後，我們的檢驗統計量變為：\n\\[\\chi^2=\\frac{(|b-c|-0.5)^2}{b+c}\\] 或者，恢復到我們在本章前面使用的表示法：\n\\[\\chi^2=\\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}\\] 這個統計量具有\\(\\chi^2\\)分佈（近似），自由度df = 1。然而，請記住，就像其他\\(\\chi^2\\)檢驗一樣，它只是一個近似值，因此您需要預期較大的單元格計數才能使其正常工作。\n\n\n\n14.7.1 實作麥內瑪檢定\n現在您已經了解麥內瑪檢驗的所有內容，讓我們實際運行一個。agpp.csv 文件包含了我之前討論過的原始數據。agpp 數據集包含三個變量，一個id變量標記數據集中的每個參與者（我們將在片刻之後看到這有什麼用），一個response_before 變量記錄了當他們第一次被問到這個問題時的答案，以及一個response_after變量顯示他們在第二次被問到同樣問題時給出的答案。注意每個參與者在這個數據集中只出現一次。在jamovi中，轉到 ‘Analyses’ - ‘Frequencies’ - ‘Contingency Tables’ - ‘Paired Samples’ 分析，並將response_before 放入 ‘Rows’ 框，將response_after 放入 ‘Columns’ 框。然後，您將在結果窗口中獲得一個列聯表，麥內瑪檢驗的統計數據就在它下面，參見 Figure 14.8 。\n\n\n\n\n\n\n\nFigure 14.8: jamovi中的麥內瑪檢驗輸出\n\n\n\n\n我們完成了。我們剛剛運行了一個麥內瑪檢驗，以確定人們在廣告後是否和廣告前一樣有可能投票支持AGPP。檢驗是顯著的（\\(\\chi^2(1)= 12.03, p< .001\\)），表明他們並非如此。事實上，看起來廣告產生了負面影響：人們在看過廣告後，投票支持AGPP的可能性更低。考慮到典型政治廣告的質量，這是很合理的。\n\n\n\n14.7.2 與獨立性檢定有可分別?\n讓我們回到本章的開頭，再次查看卡片數據集。如果您回憶一下，我描述的實際實驗設計涉及人們進行兩次選擇。因為我們有關於每個人第一次選擇和第二次選擇的信息，我們可以構建以下列聯表，用於將第一次選擇與第二次選擇交叉列聯（Table 14.13）。\n\n\n\n\n\n\nTable 14.13:  用Randomness.omv（卡片）數據交叉列聯第一次與第二次選擇 \n\nBefore: YesBefore: NoTotal\n\nAfter: Yes\\(a \\)\\(b \\)\\(a + b \\)\n\nAfter: No\\(c  \\)\\(d  \\)\\(c + d  \\)\n\nTotal\\(a+c  \\)\\(b+d  \\)\\(n  \\)\n\n\n\n\n\n假設我想知道第二次選擇是否取決於第一次選擇。這是獨立性檢驗有用的地方，我們試圖做的是看看這個表格的行和列之間是否存在某種關係。\n另外，假設我想知道平均而言，第二次選擇的花色頻率是否與第一次選擇不同。在這種情況下，我真正想做的是看看行總數是否不同於列總數。這就是您使用麥內瑪檢驗的時候。\n這些不同分析產生的不同統計數據顯示在 Figure 14.9 中。注意結果是不同的！這些檢驗並不相同。\n\n\n\n\n\n\n\nFigure 14.9: Randomness.omv（卡片）數據中的獨立與成對（麥內瑪）"
  },
  {
    "objectID": "14-Categorical-data-analysis.html#本章小結",
    "href": "14-Categorical-data-analysis.html#本章小結",
    "title": "14  類別資料分析",
    "section": "14.8 本章小結",
    "text": "14.8 本章小結\n本章的學習重點有：\n\n卡方適合度檢定用於你的表列資料是來自不同類別的觀察次數，虛無假設是可相互比較的已知機率。\n卡方獨立性或關聯性檢定用於你的資料是能化為列聯表的觀察次數。虛無假設是兩種變項之間沒有關聯性。\n列聯表的效果量有多種測量方法。在此介紹最常見的Cramér’s V。\n上述的卡方檢定法有兩種適用條件：期望值次數夠大，觀察值彼此獨立。如果期望值次數不夠大，可以使用費雪精確檢定；如果觀察值並非彼此獨立，可以使用麥內瑪檢定。\n\n如果想學習更多類別資料分析方法，推薦閱讀 Agresti (1996) 的專書”類別資料分析導論”。如果基礎教科書無法滿足你的需要，或者未提供解決手上問題的方法，可以參考 Agresti (2002) 的進階書藉。因為是進階書，建議先充分理解導論再來學習進階教科書。\n\n\n\n\n\n\nAgresti, A. (1996). An introduction to categorical data analysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nCochran, W. G. (1954). The \\(\\chi^2\\) test of goodness of fit. The Annals of Mathematical Statistics, 23, 315–345.\n\n\nCramer, H. (1946). Mathematical methods of statistics. Princeton University Press.\n\n\nFisher, R. A. (1922). On the interpretation of \\(\\chi^2\\) from contingency tables, and the calculation of \\(p\\). Journal of the Royal Statistical Society, 84, 87–94.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction to mathematical statistics (6th ed.). Pearson.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for chi-squared goodness-of-fit statistics. Journal of the American Statistical Association, 73, 253–263.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling. Philosophical Magazine, 50, 157–175.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles and practice of statistics in biological research (3rd ed.). Freeman.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185(4157), 1124–1131."
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-intro",
    "href": "Prelude-Part-V.html#prelude-intro",
    "title": "10  線性模型的學習取向",
    "section": "10.1 教程前言",
    "text": "10.1 教程前言\n\n本節改編自簡体中文版3\n\n大部分常見的統計檢定方法（t 檢定、相關性檢定、變異數分析（ANOVA）、卡方檢定等），本質都是線性模型的一種特例或者是非常逼近的模型。這種優雅的簡潔性意味著我們並不需要掌握太多的技巧就能學習。具體來說，所有模型的來源都是多數學生在高中時期就學過的一元一次線性模型：\\(y = a \\cdot x + b\\) 。然而，很多基礎統計課程是把各種檢定方法分開教，給學生和老師們增加了很多不必要的麻煩。在學習每一個檢定方法的基本假設時，如果不是從線性模型切入，而是每個檢定方法都死記硬背，這讓學習的複雜度倍增。因此，我認為先教線性模型，然後對線性模型的一些特殊形式更改名稱是一種優秀的教學策略，這有助於更深刻地理解假設檢定。線性模型在次數主義學派、貝氏學派和基於置換的U檢定的統計推論方法之間是互通的，對初學者而言，從線性模型開始比從認識什麼是p值、型I錯誤、貝氏因子或其它術語更為友好。\n在入門課程教授到無母數檢定的時候，可以避開自我欺騙的手段，直接告訴學生無母數檢定其實就是參數本質是次序(rank)的檢定方法。對學生來說，接受向量的概念比相信你可以神奇地放棄各種假設好得多。實際上，在統計軟體如 JASP 裡，無母數檢定的貝氏等價模型就是使用潛在次序（Latent Rank）處理資料的。次數主義學派的無母數檢定在樣本量 N>15 的時非常準確。"
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-suggestions",
    "href": "Prelude-Part-V.html#prelude-suggestions",
    "title": "線性模型的學習取向",
    "section": "教與學的建議",
    "text": "教與學的建議\n\n本節改編自簡体中文版4\n\n\n迴歸的基礎知識\n\n回憶高中學過的線性模型：\\(y = a \\cdot x + b\\)，培養學生對斜率和截距的直覺。理解到這條公式能改寫成各種變項名稱。例如 money = profit * time + starting_money，或 \\(y = \\beta_1x + \\beta_2*1\\)，去除係數可寫成 y ~ x + 1。如果學生能接受的話，可以探索如何用微分方程推道這些模型是，並了解 \\(y\\) 是如何隨著 \\(x\\) 的數值改變。\n擴展到多元迴歸模型。這部分有非常多可用的生活實例做為練習作業，讓這些概念很容易用直覺理解。加深同學如何使用這些簡潔的模型描述巨量資料集的印象。\n介紹如何將非數值型資料轉換為次序尺度，並進行各種練習。\n以三種前提假設規劃教學：資料的獨立性，殘差分佈的常態性，以及集中量數平方差的同質性（homoscedasticity）。\n參數的信賴（confidence）與可信（credible）區間。說明為何很難計算極大似然估計值（Maximum-Likelihood estimate），因此區間估計更為重要。\n對以上簡單的迴歸模型，簡要地認識 \\(R^2\\)。順便提及一下，這就是 Pearson 和 Spearman 相關係數。\n\n\n\n特殊情況 1：一個或兩個平均值（t 檢定、Wilcoxon 檢定、Mann-Whitney 檢定）\n\n單一平均值：當只有一個變項 x 的時候，迴歸模型簡化為 \\(y = b\\)。如果 \\(y\\) 不是連續變項，可以轉換為等比尺度。應用模型假設（只有一個 \\(x\\)，因此這些檢定方法不適用平方差的同質性）。順便提及一下，這些僅有截距的模型又名為單一樣本 t 檢定和 Wilcoxon 符號次序檢定。\n分組平均值：如果我們把兩個變項放在 x 軸，兩者平均值的差異就是斜率。這樣就能用如同瑞士刀的線性模型來解決！應用模型的假設條件，檢查兩組平均值的平方差是否相等，相等即符合平方差的同質性。如此就能稱這個模型為獨立 t 檢定。試著創造一些虛擬資料，做一些練習，也許還能加上 Welch 檢定，再加上次序轉換，就能變成 Mann-Whitney U 檢定。\n相依樣本：這種模型違反了獨立性假設。通過計算配對組的資料差異值，模型就會與 \\(y = b\\) 等價，儘管這些方法另有專有名詞：相依 t 檢定和 Wilcoxon 配對組檢定。\n\n\n\n特殊情況 2：三個或多個均值（方差分析（ANOVA））\n\n對類別轉化後的示性變量：類別的每一個取值範圍對應的迴歸係數，是如何通過乘以一個二元（binary）示性變量，來對每個取值範圍對應的截距來進行建模的。（How one regression coefficient for each level of a factor models an intercept for each level when multiplied by a binary indicator.）這只是我們為了使資料能用線性模型建模，而擴展了在 2.1 所做的事情而已。\n一個變量的均值：單因素方差分析（one-way ANOVA）.\n兩個變量的均值：雙因素方差分析（two-way ANOVA）.\n\n\n\n特殊情況 3：三個或多個比率（卡方檢驗）\n\n對數變換：通過對數變換，把“多元乘法”模型轉化成線性模型，從而可以對比率進行建模。關於對數線性模型和對比率的卡方檢驗的等價性，可以查閱這個非常優秀的介紹。此外，還需要介紹 (log-) odds ratio（一般翻譯為“比值比”或“優勢比”）。當“多元乘法”模型使用對數變換轉化為“加法”模型之後，我們僅加上來自 3.1 的示性變量技巧，就會在接下來發現模型等價於 3.2 和 3.3 的方差分析—-除了係數的解釋發生了變化。\n單變量的比率：擬合優度檢驗.\n雙變量的比率：列聯表.\n\n\n\n假設檢定\n\n視為模型比較的假設檢定：假設檢驗用於全模型和某個參數固定了（通常為 0，也即從模型中去除）的模型進行比較，而不是對模型進行估計。比如說，在 t 檢驗 把兩個均值之一固定為零之後，我們探究單獨一個均值（單樣本 t 檢驗）對兩個組的數據的解釋程度。如果解釋程度比較好，那麼我們更傾向於這個單均值模型，而不是雙均值模型，因為前者更為簡單。假設檢驗其實是比較多個線性模型，來獲得更多的定量描述。單參數的檢驗，假設檢驗包含的信息更少。但是，同時對多個參數（如方差分析的類別變量）進行檢驗的話，模型比較就會變得沒有價值了。\n似然比：似然比是一把瑞士軍刀，它適用於單樣本 t 檢驗到 GLMM 等情況。BIC 對模型複雜度進行懲罰。還有，加上先驗（prior）的話，你就能得到貝葉斯因子（Bayes Factor）。一個工具，就能解決所有問題。我在上文方差分析中使用了似然比檢驗。"
  },
  {
    "objectID": "Prelude-Part-V.html#Prelude-corr-linear-reg",
    "href": "Prelude-Part-V.html#Prelude-corr-linear-reg",
    "title": "線性模型的學習取向",
    "section": "相闗與線性迴歸",
    "text": "相闗與線性迴歸"
  },
  {
    "objectID": "Prelude-Part-V.html#prelude-fatorial-design",
    "href": "Prelude-Part-V.html#prelude-fatorial-design",
    "title": "線性模型的學習取向",
    "section": "多因子變異數分析",
    "text": "多因子變異數分析"
  },
  {
    "objectID": "Prelude-Part-V.html#線型模型版本示範集合",
    "href": "Prelude-Part-V.html#線型模型版本示範集合",
    "title": "線性模型的學習取向",
    "section": "線型模型版本示範集合",
    "text": "線型模型版本示範集合\n\n相闗與線性迴歸\n\n\n多因子變異數分析"
  },
  {
    "objectID": "Prelude-Part-V.html#教程前言",
    "href": "Prelude-Part-V.html#教程前言",
    "title": "線性模型的學習取向",
    "section": "教程前言",
    "text": "教程前言\n\n本節部分改編自簡体中文版教程前言\n\n大部分常見的統計檢定方法（t 檢定、相關係數檢定、變異數分析（ANOVA）、卡方檢定等），本質都是線性模型的一種特例或者是非常逼近的模型。這種優雅的簡潔性意味著我們並不需要掌握太多的技巧就能學習。具體來說，所有模型的來源都是多數學生在高中時期就學過的一元一次線性模型：\\(y = a \\cdot x + b\\) 。然而，很多基礎統計課程是把各種檢定方法分開教，給學生和老師們增加了很多不必要的麻煩。在學習每一個檢定方法的基本假設時，如果不是從線性模型切入，而是每個檢定方法都死記硬背，這讓學習的複雜度倍增。因此，我認為先教線性模型，然後對線性模型的一些特殊形式更改名稱是一種優秀的教學策略，這有助於更深刻地理解假設檢定。線性模型在次數主義學派、貝氏學派和基於置換的U檢定的統計推論方法之間是互通的，對初學者而言，從線性模型開始比從認識什麼是p值、型I錯誤、貝氏因子或其它術語更為友好。\n在入門課程教授到無母數統計方法的時候，可以避開自我欺騙的手段，直接告訴學生無母數檢定其實就是參數本質是等級(rank)的檢定方法。對學生來說，接受向量的概念，比相信你可以神奇地放棄各種母數統計方法所依賴的假設要好得多。實際上，在統計軟體如 JASP 裡，無母數檢定的貝氏等價模型就是使用潛在次序（Latent Rank）處理資料，而次數主義學派的無母數檢定方法用在樣本量 N &gt; 15 的資料非常準確。\n本書統計方法的各章節，繁體中文版保留原書的內容及單元順序，讓讀者比較傳統版與線性模型版的學習方法差別。使用本書教學的講師，可以根據個人教學經驗以及學生的回饋，採取合適的教學策略及課程規劃。有關輔助線性模型學習取向的相關資訊，教學者與學習者都可以取用Lindeløv提供的簡明整理表，英文閱讀能力不足的讀者請參考教程摘要的簡体中文版。\n根據教程摘要，各種適用無母數統計方法的資料，樣本量至少要大於10，才能得到精確逼近的估計值。因此本書涉及無母數統計方法的範例，繁體中文版提供的範例資料樣本都會是至少是\\(N = 20\\)，符合樣本量條件的無母數統計方法示範說明及來源檔案，都會集中在這個章節。譯者開設的統計課程，採用Lindeløv教程的模式，平行教授連續變項及等級資料的統計方法，因此各章的學習順序不一定按照電子書的單元順序，請同學先看過以下各主題的對應單元提示，再進入各章閱讀及學習。\n此外，許多無母數統計方法的線性模型版示範檔案，會創建計算變項或轉換變項，將原始資料轉換為等級資料。除了 章节 12 相關與線性迴歸的斯皮爾曼等級相關係數範例，其他統計方法示範資料的轉換函式如下：\nIF($source==0,0,IF($source&gt;1,1,-1))*RANK($source)"
  },
  {
    "objectID": "Prelude-Part-V.html#基礎統計方法線型模型示範說明",
    "href": "Prelude-Part-V.html#基礎統計方法線型模型示範說明",
    "title": "線性模型的學習取向",
    "section": "基礎統計方法線型模型示範說明",
    "text": "基礎統計方法線型模型示範說明\n\n相關與線性迴歸\n\n相關與線性迴歸的對應單元有 章节 12.1 , 章节 12.2 , 章节 12.3 , 章节 12.4 , 章节 12.6, 章节 12.8 , 以及 章节 12.9 。使用等級資料計算斯皮爾曼等級相關與迴歸分析，總樣本量N必須大於10，才能得到精確逼近的估計值。譯者開設的課程使用另一套教程”General Analyses for the Linear Model in Jamovi”開發的範例資料進行教學，教學錄影請見這七部影片：Part 1, Part 2, Part 3, Part 4, Part 5,Part 6, Part 7。\n\n如果同學有機會處理真正的等級資料，也可以選擇使用R或其他統計軟體如JASP進行連續變項或等級資料的迴歸分析。\n\n泛統計方法的適用條件\n所有基礎統計方法幾乎共享相同的適用條件(assumptions)，包括每個資料數值的獨立性(independence of data points)，殘差的常態性(normality of residuals)，以及變異同質性(homoscedasticity)。線性迴歸模性可看到些條件以及檢核方法，透過迴歸模型認識其他統計方法，就能自然知道要符合那些條件，統計分析才有意義。需要讀者了解的用語問題，許多中文統計書會用假設而非適用條件，用了幾次jamovi，讀者應該發現許多模組有assumption check的選項，其實是提供該分析是否符合適用條件的檢測。\n\n\n以線性模型解讀自由度\n這本用jamovi上手統計學有一個專門的小節，解釋什麼是自由度(degress of freedom)。不過這個小節在更動順序後，落在比較後面的 章节 14 。做為補充章節，剛好在此為讀者提供一點小小的說明。由於剛好重新學習線性模型與線性代數的知識，我參考翻譯本書時在閱讀的另一本中文教科書：黃志勝先生撰寫的「機器學習的統計基礎 : 深度學習背後的核心技術」，其中第4章講自由度的這一節，黃先生給的定義是「包含N個變數的線性模型中，不受限制的變數個數」(該書沒有這一句話，是我重新消化組織的)，我們能透過以下範例來逐漸認識。\n\n簡單迴歸式 \\(y = a + bx\\)。\\(y\\)與\\(x\\)都是變數，要使等號左右兩邊的數值相等，必須限制其中一個變數等於固定的數值，因此自由度\\(df = n - k = 2 - 1 = 1\\)。\n多元迴歸式 \\(y = a + b_1 x_1 +b_2 x_2\\)。這個等式有三個變數，要使等號左右兩邊的數值相等，同樣必須限制其中一個變數等於固定的數值，因此自由度\\(df = n - k = 3 - 1 = 2\\)。\n我們知道樣本平均值(\\(\\bar{X}\\))是母群平均值(\\(\\mu\\))的最佳估計值，公式是\\(\\bar{X} = \\frac{X_1 + X_2 + \\dots + X_n}{n}\\)，其中有n個變數，每個變數都是隨機值且彼此獨立，沒有任何一個變數要受到限制，因此自由度\\(df = n - k = n - 0 = n\\)。\n樣本變異數(\\(S^2\\))是母群平均值(\\(\\sigma^2\\))的最佳估計值，公式是\\(S^2 = \\frac{\\sum_{n=1}^{n}(X_i - \\bar{X})^2}{n-1}\\)。這個公式也有同樣的n個隨機且彼此獨立的變數，但是公式中的平均值\\(\\bar{X}\\)來自樣本平均值公式，所以可知\\(X_n = n\\bar{X} - X_1 - X_2 - \\dots - X_{n-1}\\)，也就是說\\(X_n\\)是受限制的變數，因此樣本變異數的自由度\\(df = n - k = n - 1\\)。\n\n\n\n\n單一樣本及相依樣本\n單一樣本及相依樣本的對應單元有 章节 11.2 , 章节 11.6 , 章节 11.7 , 章节 11.8 , 章节 11.9 , 章节 11.10 。使用等級資料進行無母數統計分析，最小樣本量必須多於14，如果是採用線性模型教程，建議不要使用 章节 11.10 的範例，將連續資料的範例轉換為等級資料即可。讀者可使用線性模型版的jamovi示範檔案：單一樣本t檢定、相依樣本t檢定，搭配課程錄影學習~ 影片1、影片2、影片3、影片4、影片5、影片6、影片7、影片8。\n\n\n比較兩組平均值\n比較兩組平均值的對應單元有 章节 11.3, 章节 11.5, 章节 11.7 , 章节 11.8 , 章节 11.9 , 章节 11.10。使用等級資料進行無母數統計分析，最小樣本量必須多於10，如果是採用線性模型教程，建議不要使用 章节 11.10 的範例，將連續資料的範例轉換為等級資料即可。\nWelch’s t檢定是無法使用jamovi展示線性模型示範的統計方法，但是近年採用獨立組設計的心理學研究，被呼籲使用Welch’s t檢定的建議越來越多，因為獨立組設計的行為科學資料經常違反變異數同質性的適用條件。在此使用R套件，展示使用線性模型輸出 章节 11.5 的範例。\nR程式碼\ndf &lt;- read_csv(\"files/Harpo.csv\")\n\nsummary(nlme::gls(grade ~ 1 + as.factor(tutor), weights = nlme::varIdent(form=~1|tutor), method=\"ML\", data=df))\n輸出結果\nGeneralized least squares fit by maximum likelihood\n  Model: grade ~ 1 + as.factor(tutor)\n\nCoefficients:\n                              Value Std.Error  t-value p-value\n(Intercept)                74.53333  2.316009 32.18179  0.0000\nas.factor(tutor)Bernadette -5.47778  2.688237 -2.03768  0.0502\n讀者可使用線性模型版的jamovi示範檔案：獨立樣本t檢定，搭配課程錄影學習 ~ 影片1、影片2\n\n\n單因子變異數分析\n章节 13 單因子變異數分析 介紹的統計方法有單因子獨立樣本變異數分析，以及單因子重覆量數變異數分析。根據線性模型教程，譯者將這兩個兩個統計方法的示範，改編為以線性迴歸模型實作的獨立樣本示範檔案，以及重覆量數示範檔案。由於Lindeløv的教程並未提供單因子重覆量數變異數的等級資料分析示範，重覆量數示範檔案只有連續變項的版本。\n依線性模型教程的順序學習的讀者與學生，可以先復習 章节 12.3 這一節的內容。譯者的課程教學會從迴歸模型的R-squared起手，帶領學生認識兩種單因子變異數分析的程序及適用條件。\n長表單(long table)與寬表單(wide table)\n相依樣本t檢定處理的分析資料，也是重覆量數設計。在這份示範檔案裡，兩次成績列在不同的變項欄位(grade_test1,grade_test2)，這種將同一個獨變項的不同層次之依變項資料，各自列在獨立變項欄位的資料格式，稱為寬表單(wide table)。電子書 章节 13.7 示範單因子重覆量數變異數分析使用的資料格式，也是寬表單。\njamovi的ANOVA模組與t檢定模組，都是用來處理寬表單資料。然而線性模型版的單因子重覆量數變異數分析示範檔案，獨變項欄位只有一個(Task)，依變項資料也集中於一個變項欄位(Score)。這種資料格式稱為長表單(long table)，是各種資料收集平台最原始的儲存格式。請讀者比較兩種資料格式的差異，思考最適合你的資料處理與分析操作，用那一種表單格式與分析模組有最佳效益。\n\n課程錄影預計於2023/5/17之後上線。\n\n\n\n多因子變異數分析\n對應 章节 14 多因子變異數分析。讀者可以使用改編自這一章原始範例，使用線性模型執行的二因子獨立樣本變異數分析與二因子獨立樣本共變數分析的jamovi檔案，自行比對與原書使用變異數分析模組製作的範例，學習如何設定與解讀分析報表與適用條件。經過前三章透過線性模型學習，此章內容應該相當容易。\n\n課程錄影預計於2023/5/24之後上線。\n\n\n\n類別資料分析\n對應 章节 10 類別資料分析，這個單元介紹四種類別資料分析方法，中文版提供使用線性模型示範檔案有卡方適合度檢定、卡方獨立性檢定、小樣本類別資料的費雪精確檢定、以及相依樣本類別資料的麥內瑪檢定。讀者開啟之前，請先確定自用的jamovi已經安裝gamlj模組，才能檢視與編輯通用線性模型(Generalized Linear Model)報表及功能選單。請者可搭配課程錄影學習 ~ (預計5/31後上線)\n透過線性模型路徑，這四種統計方法都要先建立統計假設通用線性模型(\\(H_0\\)與\\(H_1\\))，運用變異數分析或逐步迴歸比較兩種統計假設的線性模型，才是與一般統計書介紹的方法相同。然而目前jamovi的內建功能或外掛模組，都沒有比較兩套線性模型的功能設定。中文版的示範檔案只有顯示對應\\(H_1\\)的線性模型，並且統計報告裡雖然有\\(\\chi^2\\)的資訊，實際上是對數似然比檢定(log-likelihood ratio test)的結果，統計數與p值雖然接近卡方檢定，但並非真正的卡方檢定。由於似然值(likelihood)是貝氏統計方法常用的指標，譯者釋出這些範例檔案有引導讀者認識貝氏統計方法的用意。此外，jamovi 2.3.21版之後，內建Frequencies模組增加log-linear Regression功能，讓使用者能運用通用線性模型處理類別資料分析。\njamovi的內建Regression模組有三種對數迴歸的功能2 Outcomes，N outcomes，以及Ordinal Outcomes。這三種對數迴歸是一般線性模型(General linear model)，與 章节 12 到 章节 14 介紹的統計方法所採用的線性模型是一樣的。需要注意的是，一般線性模型不同於通用線性模型，兩者主要差異在於一般線性模型的依變項資料殘差分佈必須符合或逼近常態分佈，不過通用線性模型的依變項資料殘差分佈可以符合或逼近任何分析者指定的機率分佈3。有興趣了解細節的讀者，可以點選連結開啟維基百科頁面。使用通用線性模型示範的四種類別資料分析方法，請見導讀影片。\n使用通用線性模型操作卡方適合度檢定、卡方獨立性檢定、以及相依樣本類別資料的麥內瑪檢定，都要先轉換資料格式，轉換方法與過程請見導讀影片。"
  },
  {
    "objectID": "13-Factorial-ANOVA.html",
    "href": "13-Factorial-ANOVA.html",
    "title": "13  多因子變異數分析",
    "section": "",
    "text": "14 ### 第二型平方差總和"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "href": "10-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "title": "10  相關與線性迴歸",
    "section": "10.1 相關",
    "text": "10.1 相關\n這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的相關。首先，讓我們看一下列在 Table 10.1 的本章示範資料描述統計。\n\n\n10.1.1 示範資料\n\n\n\n\nTable 10.1:  相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。 \n \n  \n    變項 \n    最小值 \n    最大值 \n    平均值 \n    中位數 \n    標準差 \n    四分位數間距 \n  \n \n\n  \n    老爸的沮喪程度 \n    41.00 \n    91.00 \n    63.71 \n    62.00 \n    10.05 \n    14.00 \n  \n  \n    老爸睡眠小時數 \n    4.84 \n    9.00 \n    6.97 \n    7.03 \n    1.02 \n    1.45 \n  \n  \n    小嬰兒睡眠小時數 \n    3.25 \n    12.07 \n    8.05 \n    7.95 \n    2.07 \n    3.21 \n  \n\n\n\n\n\n\n讓我們從一個與每個新生兒父母都息息相關的主題談起：睡眠。這裡使用的資料集是虛構的，但是來自本人(原作者)的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有測量我的沮喪分數、我的睡眠習慣和我兒子的睡眠習慣持續是100天。身為一位數位時代的書呆子，我把資料保存在一個名為parenthood.csv的檔案。匯入jamovi，我們可以看到四個變項：dani.sleep，baby.sleep，dani.grump和day。請注意，當您首次打開這份檔案，jamovi可能無法正確猜測每個變項的資料類型，同學可以自行修正：dani.sleep，baby.sleep，dani.grump和day都可以被指定為連續變項，而ID是一個名義且為整數的變項。2\n接著我會看一下一些基本的描述性統計數據，並且三個我有興趣的變項視覺化，也就是 Figure 10.1 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計數據，你就要報告所有數據。如果我要以此結果撰寫報告，我會挑出那些我自己以及我的讀者最感興趣的統計數據，然後將它們放入像 Table 10.1 這樣的簡潔的表格裡。3 需要注意的是，當我將數據放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。\n\n\n\n\n\n\n\nFigure 10.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。\n\n\n\n\n\n\n10.1.2 相關的強度與方向\n我們可以繪製散佈圖，讓我們能俯瞰兩個變項之間的相關性。雖 然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較dani.sleep和dani.grump之間的關係（ fig-fig10-2 ，左）與baby.sleep和dani.grump之間的關係（ fig-fig10-2 ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我或者我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，dani.sleep和dani.grump之間的關係比baby.sleep和dani.grump之間的關係更強：左圖比右圖更加整齊。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。\n\n\n\n\n\n\nFigure 10.2: 左圖是dani.sleep(老爸睡眠小時數)與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖。\n\n\n\n\n相反地， Figure 10.3 的另外兩個散佈圖告訴我們另一個角度的故事。比較“baby.sleep 與 dani.grump”的散佈圖（左）和“baby.sleep 與 dani.sleep”的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。\n\n\n\n\n\n\nFigure 10.3: 左圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.sleep(老爸睡眠小時數)的散佈圖。\n\n\n\n\n\n\n10.1.3 相關係數\n現在我們要進一步延伸上述的概念，也就是正式認識 相關係數(correlation coefficient)。更具體地說，本節主要介紹皮爾森相關係數（Pearson’s correlation），慣例書寫符號是 \\(r\\)。在下一節，我們會用更精確符號 \\(r_{XY}\\) ，表示兩個變項 \\(X\\) 和 \\(Y\\) 之間的相關係數，值域涵蓋-1到1。當\\(r = -1\\)時，表示變項之間是完全的負相關；當\\(r = 1\\)時，表示變項之間是完全的正相關；當\\(r = 0\\)時，表示變項之間是完全沒有關係。 Figure 10.4 展示幾種不同相關性的散佈圖。\n[其他技術細節 4]\n\n\n\n\n\n\nFigure 10.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為\\(0, .33, .66, 1\\)。右欄的相關係數由上而下為\\(0, -.33, -.66, -1\\)。\n\n\n\n\n標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: \\(r = 1\\)代表著完美的正相關，\\(r = -1\\)代表著完美的負相關。稍後解讀相關係數這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。\n\n\n\n10.1.4 相關係數計算實務\n只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 Figure 10.5 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。\n\n\n\n\n\n\nFigure 10.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。\n\n\n\n\n\n\n10.1.5 解讀相關係數\n在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀\\(r = 0.4\\)的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於\\(0.95\\)的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到\\(0.9\\)，任何使用這個測驗預測的理論就會失效5。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過\\(0.3\\)，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 Table 10.2 的概略式解讀原則。\n\n\n\n\n\nTable 10.2:  解讀相關係數的粗略指南。強調粗略是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。 \n \n  \n    相關係數 \n    強度 \n    方向 \n  \n \n\n  \n    -1.0 ~ -0.9 \n    非常強 \n    負相關 \n  \n  \n    -0.9 ~ -0.7 \n    強 \n    負相關 \n  \n  \n    -0.7 to -0.4 \n    中等 \n    負相關 \n  \n  \n    -0.4 ~ -0.2 \n    弱 \n    負相關 \n  \n  \n    -0.2 ~ 0 \n    微弱 \n    負相關 \n  \n  \n    0 ~ 0.2 \n    微弱 \n    正相關 \n  \n  \n    0.2 ~ 0.4 \n    弱 \n    正相關 \n  \n  \n    0.4 ~ 0.7 \n    中等 \n    正相關 \n  \n  \n    0.7 ~ 0.9 \n    強 \n    正相關 \n  \n  \n    0.9 ~ 1.0 \n    非常強 \n    正相關 \n  \n\n\n\n\n\n\n然而，有一件點任何一位統計學教師都會不厭其煩地提醒同學，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)(Anscombe, 1973)，其中有四個資料集。每個資料集都有兩個變項， \\(X\\) 與 \\(Y\\)。四個資料集的 \\(X\\) 平均值都是 \\(9\\)， \\(Y\\) 的平均值都是 \\(7.5\\)。所有 \\(X\\) 變項的標準差幾乎相同，\\(Y\\) 變項的標準差也是一致。每種資料集的\\(X\\) 和 \\(Y\\) 相關係數均為 \\(r = 0.816\\)。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。\n也許你認為這四個資料集看起來很相似，其實上並非如此。從 Figure 10.6 的散佈圖可以發現，所有四個資料集的\\(X\\) 和 \\(Y\\) 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始數據」（見 Chapter 5 ）。\n\n\n\n\n\n\nFigure 10.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。\n\n\n\n\n\n\n10.1.6 斯皮爾曼等級相關\n皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。\n線性關係是當一個變項\\(X\\)的數值增加，也能反映另一個變項\\(Y\\)的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（\\(X\\)）準備一個科目，那麼他排名的成績應該只有0％（\\(Y\\)）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。\n我們用 Figure 10.7 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的次序關係(ordinal relationship)。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數\\(r=0.91\\)能表達的。\n\n\n\n\n\n\nFigure 10.7: 這個圖解展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數\\(r = .91\\)。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數\\(\\rho = 1\\)。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。\n\n\n\n\n那麼我們要如何解決這個問題呢？其實很容易。如果我們要評估變項之間的次序關係，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”來衡量學生的努力，而是按照他們的讀書時間長短，將這\\(10\\)名學生排序。也就是說，學生\\(2\\)花在讀書的時間最少（\\(2\\)個小時），所以他獲得了最低的排名（排名=\\(1\\)）。接下來最懶惰的是學生\\(4\\)，整個學期只讀了\\(6\\)個小時的書，所以他獲得了次低的排名（排名=\\(2\\)）。請注意，在此用“排名=\\(1\\)”來表示“低排名”。在日常言談裡，多數人使用“排名=\\(1\\)”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！\n好的，讓我們從最努力且最成功的學生開始排名。 Table 10.3 顯示從最努力且最成功的學生排名的次序值。\n\n\n\n\n\nTable 10.3:  十位學生的工作時間與得分數值次序 \n \n  \n    學生編號 \n    讀書時間序列 \n    成績序列 \n  \n \n\n  \n    學生 1 \n    10 \n    10 \n  \n  \n    學生 2 \n    1 \n    1 \n  \n  \n    學生 3 \n    5 \n    5 \n  \n  \n    學生 4 \n    8 \n    8 \n  \n  \n    學生 5 \n    9 \n    9 \n  \n  \n    學生 6 \n    6 \n    6 \n  \n  \n    學生 7 \n    7 \n    7 \n  \n  \n    學生 8 \n    3 \n    3 \n  \n  \n    學生 9 \n    4 \n    4 \n  \n  \n    學生 10 \n    2 \n    2 \n  \n\n\n\n\n\n\n有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。\n至此我們等於重新發現 斯皮爾曼等級相關(Spearman’s rank order correlation)，通常用符號 \\(\\rho\\) 表示，以區分皮爾森相關係數\\(r\\)。我們可以在jamovi的“相關矩陣”選單選擇“Spearman”，計算斯皮爾曼等級相關係數。6"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "href": "10-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "title": "10  相關與線性迴歸",
    "section": "10.3 認識線性迴歸模型",
    "text": "10.3 認識線性迴歸模型\n\n我們可以將線性迴歸模型理解為稍微複雜一點的皮爾森相關係數分析程序（請見相關這一節），稍後我們會看到，迴歸模型是用途更廣泛的統計方法。\n由於迴歸模型的基本觀念與相關係數緊密相關，以下同樣使用parenthood.csv資料集進行介紹及示範。回想一下，我們分析這個資料集的目的是，找出我(原作者Dani)為什麼總是非常沮喪的原因，而我的研究假設是我沒有得到足夠的睡眠。所以畫了一些散佈圖，檢示實際睡眠時間與隔天沮喪程度之間的關係，就像 Figure 10.9 展示的散佈圖，兩者之間的相關係數達到\\(r=-0.90\\)。但是，我想描述的變項間關係更像 Figure 10.11 (a) ，也就是有一條直線穿過資料點的中間。這條直線的統計學術語是迴歸線。請注意，由於我不是統計新手，因此畫出的迴歸線一定會穿過資料散佈區域的中間地帶，絕不會認為是像 Figure 10.11 (b) 的樣子。\n\n\n\n\n\n\nFigure 10.11: 圖a展示同 Figure 10.9 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。\n\n\n\n\n確認能解讀變項間關係的迴歸線，並不需要什麼厲害的技巧。 Figure 10.11 （b）的那條線與資料的適合度(fittedness)並不高，用來解讀資料沒有太大的意義，對吧？迴歸線能很直覺地呈現變項間的關係，若是再應用迴歸線的數學理論解讀資料，會變成非常強大的分析工具。我們複習一下高中數學，一條直線的公式可以寫成這樣的等式：\n\n\\[y=a+bx\\]\n至少幾十年前澳洲的高中數學課是這樣教的。兩個資料變項用 \\(x\\) 和 \\(y\\)代表，搭配兩個係數 \\(a\\) 和 \\(b\\) 形成變項之間的等價性。7係數 \\(a\\) 代表迴歸線的截距，係數 \\(b\\) 代表迴歸線的斜率。努力回憶一下高中曾學過的內容（很抱歉，某些讀者也許已經離開高中校園很了），記得截距被解釋為“當 \\(x=0\\) 時得到的 \\(y\\) 值”。同樣地，斜率 \\(b\\) 若為正值，代表增加 \\(x\\) 的數值一個單位， \\(y\\) 值會增加 \\(b\\) 個單位；而斜率 \\(b\\) 若為負值，則代表 \\(y\\) 值會下降而不是上升。啊，是的，我們現在全都回想起來了。現在我們的記憶已經回來，所以自然會發現可以使用完全相同的公式計算迴歸線。如果 \\(Y\\) 是預測變項（依變項），\\(X\\) 是應變項（自變項），那麼描述示範資料的迴歸線等式就會像是這樣：\n\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\n嗯，這看起來這跟曾在高中教科書看到的公式一模一樣，只是多了些花俏的下標符號，讓我們來了解這些符號的意思。首先，請注意我使用 \\(X_i\\) 和 \\(Y_i\\)，而不是 \\(X\\) 和 \\(Y\\)，這是因為有下標符號的代數通常代表實際的資料。在這個公式裡，\\(X_i\\) 代表第 i 個觀察值的預測變項的值（例如我在第 i 天紀錄的睡眠時間），而 \\(Y_i\\) 則是對應的應變項數值（例如我當天的沮喪程度）。雖然公式裡沒有明確說明，但我們假設這個公式對資料集裡的所有觀察值都通用（即 i 對應所有 觀察日數）。其次，請注意我寫的是 \\(\\hat{Y}_i\\) 而不是 \\(Y_i\\)，這是因為我們要區分實際數值 \\(Y_i\\) 與被預測數值 \\(\\hat{Y}_i\\)（也就是經由迴歸線預測的數值）。第三，我將代表係數的符號從 a 和 b 改成 \\(b_0\\) 和 \\(b_1\\)，這是統計學家喜歡呈現迴歸模型的方式。我不知道為什麼他們選擇用 b 這個字母，但這就是統計學的慣例。無論如何，\\(b_0\\) 總是代表截距，\\(b_1\\) 則是代表斜率。\n跟上來的話就很好。接著我們會注意到，無論是好的迴歸線還是壞的迴歸線，資料都是不完美地落在迴歸線。換句話說，實際數值\\(Y_i\\)不完全等於迴歸模型預測的數值\\(\\hat{Y}_i\\)。由於統計學家喜歡給一切符號冠上字母、名稱和數字，讓我們稱呼模型預測的數值與實際數值之間的差異為殘差(Residuals)，代表符號為\\(\\epsilon_i\\)。8 使用數學公式表示的話，殘差可被定義為：\n\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\n接著我們就可以寫出完整的線性迴歸模型：\n\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "11-Comparing-two-means.html#等級資料的平均值檢定11-translation-01",
    "href": "11-Comparing-two-means.html#等級資料的平均值檢定11-translation-01",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 等級資料的平均值檢定24",
    "text": "11.10 等級資料的平均值檢定24\nOkay, suppose your data turn out to be pretty substantially non-normal, but you still want to run something like a t-test? This situation occurs a lot in real life. For the AFL winning margins data, for instance, the Shapiro-Wilk test made it very clear that the normality assumption is violated. This is the situation where you want to use Wilcoxon tests.\nLike the t-test, the Wilcoxon test comes in two forms, one-sample and two-sample, and they’re used in more or less the exact same situations as the corresponding t-tests. Unlike the t-test, the Wilcoxon test doesn’t assume normality, which is nice. In fact, they don’t make any assumptions about what kind of distribution is involved. In statistical jargon, this makes them nonparametric tests. While avoiding the normality assumption is nice, there’s a drawback: the Wilcoxon test is usually less powerful than the t-test (i.e., higher Type II error rate). I won’t discuss the Wilcoxon tests in as much detail as the t-tests, but I’ll give you a brief overview.\n\n11.10.1 單一樣本的Wilcoxon檢定\nWhat about the one sample Wilcoxon檢定(Wilcoxon test) (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.4.\nWhat about the one sample Wilcoxon test (or equivalently, the paired samples Wilcoxon test)? Suppose I’m interested in finding out whether taking a statistics class has any effect on the happiness of students. My data is in the happiness.csv file. What I’ve measured here is the happiness of each student before taking the class and after taking the class, and the change score is the difference between the two. Just like we saw with the t-test, there’s no fundamental difference between doing a paired-samples test using before and after, versus doing a onesample test using the change scores. As before, the simplest way to think about the test is to construct a tabulation. The way to do it this time is to take those change scores that are positive differences, and tabulate them against all the complete sample. What you end up with is a table that looks like Table 11.4.\n\n\n\n\nTable 11.4:  Comparing observations by group for a one-sample Wilcoxon U test \n\nall differences\n\n\\(-24\\)\\(-14\\)\\(-10\\)7\\(-6\\)\\(-38\\)2\\(-35\\)\\(-30\\)5\n\næ­£å·®ç°å¼7...\\( \\checkmark \\)\\( \\checkmark \\).\\( \\checkmark \\)..\\( \\checkmark \\)\n\n2......\\( \\checkmark \\)...\n\n5......\\( \\checkmark \\)..\\( \\checkmark \\)\n\n\n\n\n\nCounting up the tick marks this time we get a test statistic of \\(W = 7\\). As before, if our test is two sided, then we reject the null hypothesis when W is very large or very small. As far as running it in jamovi goes, it’s pretty much what you’d expect. For the one-sample version, you specify the ‘Wilcoxon rank’ option under ‘Tests’ in the ‘One Sample T-Test’ analysis window. This gives you Wilcoxon \\(W = 7\\), p-value = \\(0.03711\\). As this shows, we have a significant effect. Evidently, taking a statistics class does have an effect on your happiness. Switching to a paired samples version of the test won’t give us a different answer, of course; see Figure 11.26.\n\n\n\n\n\nFigure 11.26: jamovi screen showing results for one sample and paired sample Wilcoxon nonparametric tests\n\n\n\n\n\n\n11.10.2 獨立樣本的曼－惠特尼U檢定\nI’ll start by describing the 曼－惠特尼U檢定(Mann-Whitney U test), since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nI’ll start by describing the Mann-Whitney U test, since it’s actually simpler than the one sample version. Suppose we’re looking at the scores of 10 people on some test. Since my imagination has now failed me completely, let’s pretend it’s a “test of awesomeness” and there are two groups of people, “A” and “B”. I’m curious to know which group is more awesome. The data are included in the file awesome.csv, and there are two variables apart from the usual ID variable: scores and group.\nAs long as there are no ties (i.e., people with the exact same awesomeness score) then the test that we want to do is surprisingly simple. All we have to do is construct a table that compares every observation in group A against every observation in group B. Whenever the group A datum is larger, we place a check mark in the table (Table 11.5).\n\n\n\n\nTable 11.5:  Comparing observations by group for a two-sample Mann-Whitney U test \n\ngroup B\n\n14.510.412.411.713.0\n\ngroup A6.4.....\n\n10.7.\\( \\checkmark \\)...\n\n11.9.\\( \\checkmark \\).\\( \\checkmark \\).\n\n7.3.....\n\n10.....\n\n\n\n\n\nWe then count up the number of checkmarks. This is our test statistic, W. 25 The actual sampling distribution for W is somewhat complicated, and I’ll skip the details. For our purposes, it’s sufficient to note that the interpretation of W is qualitatively the same as the interpretation of \\(t\\) or \\(z\\). That is, if we want a two-sided test then we reject the null hypothesis when W is very large or very small, but if we have a directional (i.e., one-sided) hypothesis then we only use one or the other.\nIn jamovi, if we run an ‘Independent Samples T-Test’ with scores as the dependent variable. and group as the grouping variable, and then under the options for ‘tests’ check the option for ’Mann-Whitney \\(U\\), we will get results showing that \\(U = 3\\) (i.e., the same number of checkmarks as shown above), and a p-value = \\(0.05556\\). See Figure 11.27.\n\n\n\n\n\nFigure 11.27: jamovi screen showing results for the Mann-Whitney \\(U\\) test"
  },
  {
    "objectID": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-01",
    "href": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-01",
    "title": "11  比較單一與兩組平均值",
    "section": "11.1 單一樣本z檢定2",
    "text": "11.1 單一樣本z檢定2\n在本節中，我將介紹統計學中最無用的測試之一：z 檢定。認真地說，這種測試在現實生活中幾乎從不使用。它唯一的真正用途是，在教授統計學時，它是通往 t 檢定的一個非常方便的墊腳石，而 t 檢定可能是統計學中最（過度）使用的工具。\n\n\n11.1.1 使用z檢定前的注意事項\n為了介紹 z 檢定背後的概念，讓我們舉一個簡單的例子。我的一位朋友 Zeppo 博士按曲線給他的入門統計學班級打分。假設他班級的平均分數是 \\(67.5\\)，標準差是 \\(9.5\\)。他有很多學生，其中有 20 個學生還修了心理學課程。出於好奇心，我想知道這些心理學生的成績是否傾向於與其他人獲得相同的成績（即平均分數為 \\(67.5\\)），還是他們的成績往高或往低得分？他給我發了一個 zeppo.csv 文件，我用它來查看這些學生的成績，並在 jamovi 試算表視圖中計算了 “探索” - “描述性統計” 中的平均值。3平均值為 \\(72.3\\)。\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\n嗯，也許心理學生的成績比正常情況高一些。樣本平均值 \\(\\bar{X}=72.3\\) 比假設的母體平均值 \\(\\mu=67.5\\) 要高得多，但是，另一方面，樣本大小 \\(N=20\\) 並不是很大。也許這只是純粹的巧合。\n為了回答這個問題，有助於能夠寫下我所知道的。首先，我知道樣本平均值為 \\(\\bar{X}=72.3\\)。如果我願意假設心理學生的標準差與班上其他學生的標準差相同，那麼我可以說母體標準差為 \\(\\sigma=9.5\\)。我還假設由於 Zeppo 博士是按曲線給分，心理學生的成績服從正態分布。\n接下來，要明確我希望從數據中學到什麼。在這種情況下，我的研究假設與心理學生的成績母體平均值 \\(\\mu\\) 相關，而這個值是未知的。具體而言，我想知道 \\(\\mu=67.5\\) 是否成立。鑒於這是我所知道的，我們能否設計一個假設檢定來解決我們的問題？數據以及它們被認為來自的假設分佈顯示在 Figure 11.1 中。不是非常明顯什麼是正確的答案，對嗎？為此，我們需要一些統計學知識。\n\n\n\n\n\n\nFigure 11.1: 心理學生的成績（柱形圖）應該是由理論分佈（實線）生成的。\n\n\n\n\n\n\n11.1.2 建立z檢定的假設\n構建假設檢定的第一步是明確虛無假設和對立假設是什麼。這不太難做到。我們的虛無假設 \\(H_0\\) 是，心理學生的成績母體平均值 \\(\\mu\\) 為 \\(67.5\\%\\)，而我們的對立假設是母體平均值不是 \\(67.5\\%\\)。如果我們用數學符號表示，這些假設就變成了：\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\n不過老實說，這種表示法對我們理解問題沒有太大幫助，它只是一種簡潔的寫下我們試圖從數據中學到什麼的方法。我們測試的虛無假設 \\(H_0\\) 和對立假設 \\(H_1\\) 都在 Figure 11.2 中有圖示。除了提供這些假設外，上面概述的情況還為我們提供了相當多的有用的背景知識。具體而言，有兩個特殊的信息可以添加：\n\n心理學成績服從正態分布。\n這些分數的真實標準差 \\(\\sigma\\) 已知為 9.5。\n\n暫時，我們將表現得好像這些是絕對可信的事實。在現實生活中，這種絕對可信的背景知識是不存在的，因此，如果我們想依賴這些事實，我們只能假設這些東西是真實的。但是，由於這些假設可能成立或不成立，我們可能需要檢查它們。不過，現在我們還是保持簡單。\n\n\n\n\n\n\nFigure 11.2: 單一樣本 \\(z\\) 檢定（雙側版本）所假設的虛無假設和對立假設的圖形說明。虛無假設和對立假設都假設母體分布是正態分布，並且進一步假設母體標準差已知（固定在某個值 \\(\\sigma_0\\)）。虛無假設（左）是母體平均值 \\(\\mu\\) 等於某個指定值 \\(\\mu_0\\)。對立假設（右）是母體平均值與此值不同，即 \\(\\mu \\neq \\mu_0\\)。\n\n\n\n\n下一步是找出一個好的診斷檢定統計量，這樣可以幫助我們區分 \\(H_0\\) 和 \\(H_1\\)。考慮到這些假設都涉及母體平均值 \\(\\mu\\)，你會相當有信心樣本平均值 \\(\\bar{X}\\) 是一個很有用的起點。我們可以計算樣本平均值 \\(\\bar{X}\\) 與虛無假設預測的母體平均值之間的差異。在我們的例子中，這意味著我們計算 \\(\\bar{X} - 67.5\\)。更一般地，如果讓 \\(\\mu_0\\) 指虛無假設聲稱為我們的母體平均值的值，那麼我們想要計算\n\\[\\bar{X}-\\mu_0\\]\n如果這個量等於或非常接近於0，虛無假設看起來是可接受的。如果這個量遠離0，那麼虛無假設似乎不太可能是有價值的。但是，它離0有多遠才能拒絕 H0 呢？\n為了弄清楚這一點，我們需要再狡猾一點，並且我們需要依賴我之前提到的那兩個背景知識，即原始數據服從正態分布，而且我們知道母體標準差 \\(\\sigma\\) 的值。如果虛無假設實際上是真的，真正的平均值是 \\(\\mu_0\\)，那麼這些事實一起意味著我們知道數據的完整母體分布：平均值為 \\(\\mu_0\\)，標準差為 \\(\\sigma\\) 的正態分布。4\n好的，如果這是真的，那麼我們可以關於 \\(\\bar{X}\\) 的分布說些什麼呢？嗯，正如我們之前討論過的（見 Section 8.3.3），平均數 \\(\\bar{X}\\) 的抽樣分布也是正態分布，並且具有平均值 \\(\\mu\\)。但是這個抽樣分布的標準差 \\(\\\\{se(\\bar{X})\\\\}\\)，也被稱為平均值的標準誤差，是5\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\n現在來了解這個技巧。我們可以將樣本平均數 \\(\\bar{X}\\) 轉換成標準分數（參見 Section 4.5）。這通常被寫成 z，但現在我將其稱為 \\(z_{\\bar{X}}\\)。使用這種擴展符號的原因是為了幫助您記住，我們正在計算樣本平均值的標準化版本，而不是單個觀察值的標準化版本，後者是通常指的 z 分數）。當我們這樣做時，樣本平均值的 z 分數為\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\]\n也可寫成\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n這個 z 分數就是我們的檢定統計量。使用這個作為我們的檢定統計量的好處是，像所有 z 分數一樣，它具有標準正態分布：6\n換句話說，無論原始數據所處的比例尺是什麼，z 統計量本身的解釋始終相同：它等於觀察到的樣本均值 \\(\\bar{X}\\) 與零假設所預測的母體均值 \\(\\mu_0\\) 相隔的標準誤數。更好的是，無論原始分數的母體參數是什麼，z 檢定的 5% 臨界區域始終相同，如 Figure 11.3 所示。而這意味著，在人們手動進行所有統計計算的時代，某人可以發表像 Table 11.1 這樣的表格。這反過來又意味著研究人員可以手動計算他們的 z 統計量，然後在教科書中查找臨界值。\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\n\n\nTable 11.1:  各種顯著水準的臨界值 \n\ncritical z value\n\ndesired \\(\\alpha\\) leveltwo-sided testone-sided test\n\n.11.6448541.281552\n\n.051.9599641.644854\n\n.012.5758292.326348\n\n.0013.2905273.090232\n\n\n\n\n\n\n\n11.1.3 手作z檢定\n現在，正如我之前提到的，z-test在實際應用中幾乎從不使用。這個測試在實際生活中如此罕見，以至於jamovi的基本安裝不包含內置功能。然而，這個測試是如此地簡單，以至於手動進行這個測試非常容易。讓我們回到Dr Zeppo班級的數據。在載入成績數據後，我需要做的第一件事是計算樣本均值，我已經做到了(\\(72.3\\))。我們已經有了已知的母體標準差(\\(\\sigma = 9.5\\))，零假設所指定的母體平均值(\\(\\mu_0 = 67.5\\))的值，以及樣本大小(\\(N=20\\))。\n\n\n\n\n\n\nFigure 11.3: 雙尾 z-檢定 (面板(a)) 和單尾 z-檢定 (面板(b)) 的拒絕區域\n\n\n\n\n接下來，讓我們計算（真實）標準誤（可輕鬆用計算機完成）：\n\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\n最後，我們計算我們的z分數：\n\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\n此時，我們會傳統地在臨界值表中查詢 \\(2.26\\) 的值。我們原來的假設是雙邊的（我們對心理學生在統計學上表現得比其他學生好或差沒有任何理論基礎），因此我們的假設檢驗也是雙邊的（或者是兩尾的）。從我先前顯示的小表中，我們可以看到 \\(2.26\\) 大於需要在 \\(\\alpha = .05\\) 的顯著性水平下具有顯著性的臨界值 \\(1.96\\)，但小於需要在 \\(\\alpha = .01\\) 的水平下具有顯著性的值 \\(2.58\\)。因此，我們可以得出結論，我們可以這樣寫：\n\n在心理學學生的樣本中，平均成績為 \\(73.2\\)，假定真正的人口標準差為 \\(9.5\\)，我們可以得出結論，心理學學生在統計學上的得分與班級平均分有顯著差異（\\(z = 2.26, N = 20, p<.05\\)）。\n\n\n\n\n\n11.1.4 z檢定的適用條件\n如前所述，所有統計檢驗都有其前提假設，有些假設是合理的，有些則不是。我剛剛介紹的「單樣本 z 檢定」也有三個基本的前提假設，分別為：\n\n常態性。通常所描述的z-test假設真正的母體分布是正態的。7這通常是一個相當合理的假設，如果我們感到擔憂，也是我們可以檢查的假設（請參見檢查樣本的正態性一節）。\n獨立性。測試的第二個假設是資料集中的觀察結果彼此不相關或以某種有趣的方式相關。這在統計上不是那麼容易檢查，而是有賴於良好的實驗設計。違反此假設的一個明顯（且愚蠢）的例子是將同一個觀察結果在資料文件中“複製”多次，以便您最終獲得只有一個真正觀察結果的龐大“樣本大小”。更現實的是，您必須問自己，是否真的可以想像每個觀察結果都是從您感興趣的人口中完全隨機抽樣得到的。在實踐中，此假設永遠不會被滿足，但我們會盡力設計研究來最小化相關資料的問題。\n已知標準差假設。z檢驗的第三個假設是，研究人員知道母體的真實標準差。這簡直太傻了。在沒有真正的世界數據分析問題中，你知道母體的標準差\\(\\sigma\\)，但對平均值\\(\\mu\\)卻一無所知。換句話說，這個假設永遠是錯的。\n\n鑒於假設 \\(\\alpha\\) 已知的荒謬，我們試著不使用它。這將我們帶離了 z-test 這個枯燥領域，走進了有獨角獸、仙女和小矮人的神奇王國，那就是 t-test！"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-nonparameter-rank-test",
    "href": "11-Comparing-two-means.html#sec-nonparameter-rank-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.10 平均值的無母數統計檢定",
    "text": "11.10 平均值的無母數統計檢定\n\n與此同時，當資料變得非正態時，展示一下 QQ 圖和 Shapiro-Wilk 檢定的變化可能是值得的。為此，讓我們看一下我們的澳式足球聯賽（AFL）勝利幅度資料的分佈，如果您還記得@sec-描述性統計，它看起來根本不是來自常態分佈。以下是 QQ 圖發生的情況（圖 11.25）。\n而當我們對 AFL margins 資料執行 Shapiro-Wilk 檢定時，我們得到了 Shapiro-Wilk 正態檢定統計量的值 \\(W = 0.94\\)，p值 = \\(9.481\\)x\\(10^{-07}\\)。顯然是一個顯著的效應！\n\n\n11.10.1 單一樣本的Wilcoxon檢定\n那麼，單樣本 Wilcoxon 檢定（Wilcoxon test）（或等效地，成對樣本 Wilcoxon 檢定）呢？假設我有興趣了解上統計課對學生的幸福感有什麼影響。我的資料在 happiness.csv 文件中。這裡我測量的是每個學生在上課前和上課後的幸福感，變化分數是兩者之間的差距。就像我們在 t 檢驗中看到的那樣，在進行成對樣本檢定之前和之後，與使用變化分數進行單樣本檢定之間，沒有本質區別。與以前一樣，考慮該檢定的最簡單方法是進行標記。這次要做的是，對於正向變化的變化分數，將它們與所有完整樣本一起標記。最終，您得到的表格看起來像 表 11.4。\n\n\n\n\n\n\n表 11.4: 比較單樣本 Wilcoxon U 檢定中各組觀察值\n\n\n\n\nall differences\n\n\n\n\\(-24\\)\n\\(-14\\)\n\\(-10\\)\n7\n\\(-6\\)\n\\(-38\\)\n2\n\\(-35\\)\n\\(-30\\)\n5\n\n\næ­£å·®ç•°å€¼\n7\n.\n.\n.\n\\( \\checkmark \\)\n\\( \\checkmark \\)\n.\n\\( \\checkmark \\)\n.\n.\n\\( \\checkmark \\)\n\n\n2\n.\n.\n.\n.\n.\n.\n\\( \\checkmark \\)\n.\n.\n.\n\n\n5\n.\n.\n.\n.\n.\n.\n\\( \\checkmark \\)\n.\n.\n\\( \\checkmark \\)\n\n\n\n\n\n\n\n\n這一次的探討，我們有關於Wilcoxon符號等級檢定的問題。Wilcoxon符號等級檢定是用於對兩個樣本進行比較的非參數檢定，而不需要對母群進行任何假設。本次的Wilcoxon符號等級檢定結果是\\(W=7\\)，我們有一個顯著的效果。顯然，修讀統計學對幸福感有影響。在 jamovi 中執行此測試，您可以在“一样本t-Test”分析窗口中的“Tests”下指定“Wilcoxon rank”选项，这将给出Wilcoxon \\(W=7\\)，p-value=\\(0.03711\\)的结果。轉換成配對樣本版本的測試當然不會給我們不同的答案；詳見 圖 11.26 。\n\n\n\n\n\n\n圖 11.26: 單樣本和成對樣本 Wilcoxon 非參數檢驗結果的 jamovi 示範畫面\n\n\n\n\n\n\n11.10.2 獨立樣本的曼－惠特尼U檢定\n我們先從曼－惠特尼U檢定(Mann-Whitney U test) 開始講起，因為它比單一樣本版本還要簡單。假設我們正在研究一個包含10個人在某個測試中得分的資料集。由於我的想像力已經完全失靈，所以讓我們假裝這是一個”超讚指數測試”，而有兩個不同的組別，分別為”A”和”B”。我很好奇哪一個組別更厲害。這些資料包含在awesome.csv檔案中，除了通常的ID變項之外，還有兩個變項：scores和group。\n只要沒有平手的情況發生（即，有人得到完全相同的超讚指數分數），那麼我們要進行的測試就非常簡單了。我們只需要建立一個表格，將組A中的每個觀察值與組B中的每個觀察值進行比較。當組A資料大於組B時，在表格中放置一個勾號(表 11.5)。\n\n\n\n\n\n\n表 11.5: 對於雙樣本曼－惠特尼U檢定，比較各組觀測值\n\n\n\ngroup B\n\n\n14.5\n10.4\n12.4\n11.7\n13.0\n\n\ngroup A\n6.4\n.\n.\n.\n.\n.\n\n\n10.7\n.\n\\( \\checkmark \\)\n.\n.\n.\n\n\n11.9\n.\n\\( \\checkmark \\)\n.\n\\( \\checkmark \\)\n.\n\n\n7.3\n.\n.\n.\n.\n.\n\n\n10\n.\n.\n.\n.\n.\n\n\n\n\n\n\n\n\n我們然後計算出勾勾的數量，這就是我們的檢定統計量 W23。W的抽樣分佈實際上有些複雜，我會略過細節。對於我們的目的而言，關鍵是W的解釋在質性上與t或z的解釋相同。也就是說，如果我們需要進行雙側檢定，則當W非常大或非常小時，我們會拒絕虛無假設，但如果我們有一個單向（即單側）假設，那麼我們只使用其中之一。\n在jamovi中，如果我們將成績作為因變量，組別作為分組變量運行“獨立樣本T檢驗”，然後在“測試”選項中勾選“曼-惠特尼U檢定”選項，我們將獲得顯示\\(U=3\\)（即上面顯示的檢查記號數）和p值= 0.05556的結果。參見 圖 11.27 。\n\n\n\n\n\n\n圖 11.27: jamovi螢幕顯示曼-惠特尼\\(U\\)檢定的結果"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#running-an-anova-in-jamovi",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.3 jamovi的變異數分析模組",
    "text": "12.3 jamovi的變異數分析模組\n我相當確定在讀完上一節之後，您在想什麼，特別是如果您按照我的建議，用鉛筆和紙（即在試算表中）自己完成所有這些工作。自己做 ANOVA 計算很糟糕。沿途我們需要做相當多的計算，如果每次想做 ANOVA 都要一次又一次地做這些計算，會讓人厭煩。\n\n\n12.3.1 使用jamovi完成變異數分析\n為了讓您的生活更輕鬆，jamovi 可以做 ANOVA… 哈拉！ 轉到「ANOVA」-「ANOVA」分析，將 mood.gain 變項移到「依賴變項」框中，然後將 drug 變項移到「固定因子」框中。這樣應該會得到 Figure 12.3 中所示的結果。9 注意我還勾選了 ’Effect Size’選項下的 \\(\\eta^2\\) 复选框，念作“ eta 平方”，這也顯示在結果表格上。稍後我們將回到效應大小。\n\n\n\n\n\n\n\nFigure 12.3: jamovi的結果表格，用於根據施用的藥物進行情緒增益的 ANOVA。\n\n\n\n\njamovi 的結果表格顯示了平方和值、自由度以及我們現在並不真正感興趣的其他一些數量。然而，請注意，jamovi 不使用「組間」和「組內」這兩個名稱。 相反，它嘗試分配更有意義的名稱。 在我們的特定示例中，組間方差對應於藥物對結果變項的影響，組內方差對應於“剩餘”的可變性，因此它將其稱為殘差。 如果我們將這些數字與 [A worked example] 中我手工計算的數字進行比較，可以看到它們或多或少是相同的，除了四捨五入誤差。組間平方和為 \\(SS_b = 3.45\\)，組內平方和為 \\(SS_w = 1.39\\)，各自的自由度為 \\(2\\) 和 \\(15\\)。我們還得到了 F 值和 p 值，同樣，這些數字與我們在手工計算時的數字差不多相同，只是四捨五入誤差。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-multiple-regression",
    "href": "10-Correlation-and-linear-regression.html#sec-multiple-regression",
    "title": "10  相關與線性迴歸",
    "section": "10.5 多元線性迴歸",
    "text": "10.5 多元線性迴歸\n\n譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。\n\n到目前為止，我們討論過的簡單線性迴歸模型假設只有一個您感興趣的預測變項，在這種情況下是dani.sleep。事實上，在這一點上，我們談到的每一個統計工具都假設您的分析使用一個預測變項和一個結果變項。然而，在許多（也許是大多數）研究項目中，實際上有多個您想要研究的預測變項。如果是這樣，將線性迴歸框架擴展到包含多個預測變項可能會很好。也許某種多元迴歸模型將會合適？\n多元迴歸在概念上非常簡單。我們所做的就是在我們的迴歸方程中添加更多項。假設我們對兩個變項感興趣；也許我們想要使用dani.sleep和baby.sleep來預測dani.grump變項。像以前一樣，我們用\\(Y_{i}\\)表示第i天的煩躁程度。但現在我們有兩個$ X \\(變項：第一個對應我得到的睡眠量，第二個對應我兒子得到的睡眠量。所以我們用\\)X_{i1}\\(表示第i天我睡的時間，\\)X_{i2}$表示那一天嬰兒睡的時間。如果是這樣，那麼我們可以這樣寫我們的迴歸模型：\n\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\n像以前一樣，\\(\\epsilon_i\\)是與第i個觀察值相關的殘差，\\(\\epsilon_i = Y_i - \\hat{Y}_i\\)。在這個模型中，我們現在有三個需要估計的係數：b0是截距，b1是與我的睡眠相關的係數，b2是與我兒子的睡眠相關的係數。然而，儘管需要估計的係數數量有所改變，估計工作的基本思路沒有改變：我們的估計係數\\(\\hat{b}_0\\)、\\(\\hat{b}_1\\)和\\(\\hat{b}_2\\) 是那些使殘差平方和最小化的係數。\n\n\n10.5.1 jamovi實務示範\n在jamovi中，多元迴歸與簡單迴歸無異。我們所要做的就是在jamovi中的’協變項’框中添加更多變項。例如，如果我們想要使用dani.sleep和baby.sleep作為預測變項來解釋為什麼我如此煩躁，那麼將baby.sleep移動到與dani.sleep相鄰的’協變項’框中。默認情況下，jamovi假設該模型應該包括一個截距。這次我們得到的係數顯示在 Table 10.4 中。\n\n\n\n\n\nTable 10.4:  增加預測變項迴歸係數的示 \n \n  \n    截距 \n    老爸睡眠小時數 \n    小嬰兒睡眠小時數 \n  \n \n\n  \n    125.97 \n    -8.95 \n    0.01 \n  \n\n\n\n\n\n\n與dani.sleep相關的係數相當大，表明每失去一個小時的睡眠，我會變得更加煩躁。然而，baby.sleep的係數非常小，表明我的兒子睡多少其實無關緊要。就我的煩躁程度而言，重要的是我睡多少。為了讓您對這個多元迴歸模型有所了解，?fig-fig10-14顯示了一個三維圖，繪製了所有三個變項以及迴歸模型本身。\n\n\n\n\n\n\nFigure 10.14: 這張圖展示多元迴歸模型的三維立體視覺化。模型中有兩個預測變項，分別是 “dani.sleep” 和 “baby.sleep”，而目標變項是 “dani.grump”，這三個變項構成圖中的三維空間。每筆資料都是這個空間中的一個點。就像簡單線性迴歸模型在二維空間形成一條線一樣，此多元迴歸模型在三維空間形成一個平面。當我們估計迴歸係數時，我們能做的就是找到一個盡可能靠近所有資料點的平面。\n\n\n\n\n[附加技術細節10]"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-multiple-variables-combinaion",
    "href": "10-Correlation-and-linear-regression.html#sec-multiple-variables-combinaion",
    "title": "10  相關與線性迴歸",
    "section": "10.11 決定線性模型的變項組合",
    "text": "10.11 決定線性模型的變項組合\n\n譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。\n\n剩下的一個相當重要的問題是 “模型選擇” 的問題。也就是說，如果我們有一個包含幾個變項的資料集，哪些變項應該作為預測變項，哪些不應該包括在內？換句話說，我們有一個變項選擇的問題。通常，模型選擇是一個複雜的過程，但如果我們將問題限制在選擇應該包含在模型中的變項子集上，情況會變得簡單一些。儘管如此，我不打算試圖詳細涵蓋甚至這個範疇。相反，我將談論您需要考慮的兩個廣泛原則，然後討論一個具體的工具，jamovi 提供了這個工具，可以幫助您選擇要包含在模型中的變項子集。首先，兩個原則：\n\n為您的選擇提供實質性的依據是很好的。也就是說，在很多情況下，您作為研究人員有充分的理由挑選一個較小的可能的回歸模型數量，這些模型在您的領域背景下具有合理的解釋。永遠不要低估這一點的重要性。統計學為科學過程服務，而不是反過來。\n在您的選擇依賴統計推斷的程度上，簡單性和適合度之間存在權衡。當您向模型添加更多預測變項時，模型變得更複雜。每個預測因子都添加了一個新的自由參數（即，一個新的回歸係數），每個新參數都會增加模型對於隨機變異的吸收能力。因此，適合度（例如，\\(R^2\\)）隨著您添加更多預測因子而持續上升，無論如何都是如此。如果您希望模型能夠很好地概括新的觀察結果，則需要避免加入過多的變項。\n\n後者原則通常被稱為奧卡姆剃刀，並通常用以下簡潔的說法來概括：不要在必要之外繁殖實體。在這個情境下，這意味著不要僅僅為了提高你的 R2 而將一堆大致無關的預測因子扔進去。嗯，原來的說法更好。\n無論如何，我們需要一個實際的數學標準，以便在選擇回歸模型時實現奧卡姆剃刀背後的定性原則。事實證明，有幾種可能性。我將談論的一個是赤池資訊量準則（Akaike information criterion）(Akaike, 1974)，僅僅是因為它可以作為一個選項在 jamovi 中使用。29\nAIC 值越小，模型性能越好。如果我們忽略低水平的細節，AIC 的作用就非常明顯了。左邊的項隨著模型預測變差而增加；右邊的項隨著模型複雜度的增加而增加。最佳模型是用盡量少的預測變項（低 K，右側）來擬合資料（低殘差，左側）。簡而言之，這是奧卡姆剃刀的簡單實現。\n當選中 “AIC” 複選框時，AIC 可以添加到 “Model Fit Measures” 輸出表中，評估不同模型的一種笨拙方式是查看如果從回歸模型中移除一個或多個預測因子，“AIC” 值是否更低。這是 jamovi 目前實現的唯一方法，但在其他更強大的程序中，如 R，有替代方法。這些替代方法可以自動化有選擇地移除（或添加）預測變項以找到最佳 AIC 的過程。儘管這些方法在 jamovi 中尚未實現，但我將在下面簡要介紹它們，以便您了解它們。\n\n10.11.1 逐步排除法\n在逐步排除法中，您從完整的迴歸模型開始，包括所有可能的預測因子。然後，在每個「步驟」中，我們嘗試所有可能的刪除一個變項的方法，並選擇其中最好的（就最低AIC值而言）。這將成為我們的新迴歸模型，然後我們再試驗從新模型中刪除所有可能的選項，同樣選擇具有最低AIC的選項。這個過程將持續進行，直到我們得到一個具有比刪除一個預測因子的其他可能模型更低AIC值的模型。\n\n\n10.11.2 逐步納入法\n作為替代方法，您還可以嘗試逐步納入法。這次我們從最小可能的模型作為起點，僅考慮可能添加到模型中的選項。然而，還有一個麻煩。您還需要指定您願意接受的最大可能模型。\n儘管向後和向前選擇可能導致相同的結論，但它們並不總是如此。\n\n\n10.11.3 使用警告\n自動變項選擇方法是具有誘惑力的東西，特別是當它們被捆綁在強大的統計程序中的（相對）簡單函數中時。它們為您的模型選擇提供了一定程度的客觀性，這很好。不幸的是，它們有時被用作掩蓋思考的藉口。您不再需要仔細考慮要添加到模型中的哪些預測因子以及它們可能包含的理論基礎。一切都通過AIC的魔力解決了。如果我們開始丟出像奧卡姆剃刀這樣的短語，那麼一切都被包裹在一個整潔的小包裹裡，沒有人可以反駁。\n或者，也許不是。首先，對於什麼算作合適的模型選擇標準，幾乎沒有一致的看法。當我在本科時代被教授逐步排除法時，我們使用了F檢驗來執行它，因為那是軟件所使用的默認方法。我描述了使用AIC，並且因為這是一本入門教材，所以我只描述了這種方法，但AIC絕非統計之神的話語。它是一個近似值，在某些假設下得出的，並且只有在大樣本中滿足這些假設時才能保證起作用。改變那些假設，您就會得到不同的標準，比如BIC（在jamovi中也可用）。再換一種方法，您就會得到NML標準。決定成為貝葉斯，您將基於後驗概率比進行模型選擇。然後還有一堆我沒提到的迴歸特定工具。等等。所有這些不同的方法都有優點和缺點，有些比其他方法更容易計算（AIC可能是最容易的，這可能解釋了它的受歡迎程度）。幾乎所有方法在答案是“明顯”的情況下都會產生相同的結果，但在模型選擇問題變得困難時，存在相當多的分歧。\n在實踐中，這意味著什麼？好吧，您可以花幾年時間教自己模型選擇理論，學習所有的技巧，最終決定您個人認為什麼是正確的。作為實際做過這件事的人，我不建議這樣做。您可能會在結束時更加困惑。更好的策略是表現出一點常識。如果您盯著自動向後或向前選擇過程的結果，有意義的模型接近具有最小AIC值，但被一個毫無意義的模型以微弱的優勢擊敗，那麼相信您的直覺。統計模型選擇是一個不精確的工具，正如我一開始說的，可解釋性很重要。\n\n\n10.11.4 比較迴歸模型\n與使用自動模型選擇程序的方法相反，研究人員可以明確地選擇兩個或多個迴歸模型以相互比較。您可以用幾種不同的方法做到這一點，具體取決於您要回答的研究問題。假設我們想知道我兒子睡眠的多少是否與我煩躁的程度有關，超出了我自己睡眠的影響。我們還希望確保測量的那一天對這種關係沒有影響。也就是說，我們對baby.sleep和dani.grump之間的關係感興趣，從這個角度看，dani.sleep和day是我們想控制的協變項。在這種情況下，我們想知道dani.grump ~ dani.sleep + day + baby.sleep（我將其稱為Model 2，或M2）是否比dani.grump ~ dani.sleep + day（我將其稱為Model 1，或M1）更適合這些數據。我們可以用兩種不同的方式來比較這兩個模型，一種基於像AIC這樣的模型選擇標準，另一種基於顯式假設檢定。我首先向您展示基於AIC的方法，因為它更簡單，並且自然地延續了上一節的討論。首先，我需要實際運行兩個迴歸，注意每個迴歸的AIC，然後選擇AIC值較小的模型，因為它被認為是這些數據的更好模型。實際上，不要立即這樣做。繼續閱讀，因為jamovi中有一種簡單的方法可以在一個表格中獲取不同模型的AIC值。30\n基於假設檢定框架的某種不同方法來解決這個問題。假設您有兩個迴歸模型，其中一個（Model 1）包含另一個（Model 2）的一部分預測變項。也就是說，Model 2包含Model 1中包含的所有預測變項，再加上一個或多個其他預測變項。當這種情況發生時，我們說Model 1嵌套在Model 2中，或者可能說Model 1是Model 2的子模型。無論使用哪種術語，這意味著我們可以將Model 1視為虛無假設，將Model 2視為替代假設。事實上，我們可以用相當簡單的方式構建一個F檢驗。31\n那麼，這就是我們用來比較兩個迴歸模型的假設檢定。現在，我們如何在jamovi中進行呢？答案是使用“Model Builder”選項，在“Block 1”中指定Model 1預測變項dani.sleep和day，然後將Model 2中的其他預測變項（baby.sleep）添加到“Block 2”，如@fig-fig10-25所示。這在“Model Comparisons”表格中顯示了比較Model 1和Model 2的結果，\\(F(1,96) = 0.00\\)，\\(p = 0.954\\)。由於我們的p > .05，我們保留虛無假設（M1）。這種將我們所有協變數添加到零模型中，然後將感興趣的變項添加到替代模型中，然後在假設檢定框架中比較兩個模型的迴歸方法通常被稱為分層迴歸。\n我們還可以使用此“Model Comparison”選項顯示一個表格，顯示每個模型的AIC和BIC，方便比較並確定哪個模型具有最低的值，如@fig-fig10-25所示。\n\n\n\n\n\nFigure 10.25: 使用jamovi的“Model Builder”選項進行模型比較"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-regression-hypothesis-testing",
    "href": "10-Correlation-and-linear-regression.html#sec-regression-hypothesis-testing",
    "title": "10  相關與線性迴歸",
    "section": "10.7 迴歸模型的假設檢定",
    "text": "10.7 迴歸模型的假設檢定\n至此我們已經學到什麼是迴歸模型，如何估計迴歸模型的係數，以及量化模型預測效能的方法（順便說一下，相關係數與迴歸係數就是一種效果量的估計值）。接下來學習課題的是假設檢定。我們要學習兩種不同（但相關）的假設檢定：一種是檢驗包合所有預測變項的迴歸模型是否顯著優於只有截距的模型，另一種是我們檢驗只有單一預測變項的模型，迴歸係數是否顯著不等於零。\n\n10.7.1 檢定所有預測變項的模型\n\n譯註：只有分析多元迴歸的場景，才要進行這種檢定。內容文字編修排在本書最後階段進行。\n\n好吧，假設你已經估計了你的迴歸模型。你可能會嘗試的第一個假設檢定是虛無假設，即預測變項和結果之間沒有關係，而對立假設是數據的分佈完全符合迴歸模型的預測。\n[額外的技術細節13]\n我們將在 Chapter 12 中看到更多 F 統計量，但目前只需知道我們可以將較大的 F 值解釋為虛無假設與對立假設相比表現不佳。過一會兒，我將向您展示如何用 jamovi 輕鬆進行檢驗，但首先讓我們看一下單個迴歸係數的檢驗。\n\n\n10.7.2 單一迴歸係數的檢定\n前一節介紹的 F 檢定對於檢查整個線性模型是否優於隨機截距模型很有用。如果您的迴歸模型並未在 F 檢定看到顯著結果，那麼這套迴歸模型可能不是有效解讀資料的好模型（或者是要分析的資料可能並不夠好）。然而，儘管這個檢定失敗是表示模型是否可用的明顯指標，但是通過檢定（也就是拒絕虛無假設）並不表示這個模型是真的好模型！也許同學會想知道為什麼？答案在前面 Section 10.5 多元線性迴歸 已經有討論過的。\n注意一下 Table 10.4 的數值，與 dani.sleep 變項的迴歸係數估計數值（\\(-8.95\\)）相比，baby.sleep 變項的迴歸係數估計數值非常小（\\(0.01\\)）。考慮到這兩個變項的度量尺度都是一樣的（都是以“睡眠小時數”），我發現這很有啟發性。其實我看了 Table 10.4 就有想到，要預測我的沮喪程度，真正重要的變項應該只有我自己的睡眠時間長度。我們可以使用之前學到的假設檢定方法，確認我的懷疑14。我們想檢定的虛無假設是設定迴歸係數為零（\\(b = 0\\)），與迴歸係數不是零（\\(b \\neq 0\\)）的對立假設進行檢定。也就是說：\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\n這個檢定要如何進行？好吧，如果還記得中心極限定理，同學可能會猜到迴歸係數的估計值\\(\\hat{b}\\)是一種取樣分佈，而且是以 \\(b\\) 為中心的常態分佈。這表示如果虛無假設是真的，那麼\\(\\hat{b}\\)的取樣分佈平均值為零並且標準差是未知數。假設我們可以找到迴歸係數的標準誤差估計值，\\(se(\\hat{b})\\)，那麼我們就很幸運。這正好可以用 Chapter 11 介紹的單一樣本 t 檢定處理。現在可以定義以下的 t 統計值\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\n這裡不詳細說明為什麼能這樣做的原因，但在這種狀況，自由度是 \\(df = N - K - 1\\)。令初學者厭煩的通常是，迴歸係數的標準誤估計值，\\(se(\\hat{b})\\)，並不像 Chapter 11 介紹的單一樣本 t 檢定的平均值標準誤那樣容易計算。真實的公式長得有點醜陋，看起來並不那麼平易近人。15 對於我們要真正完成的目標，只需要知道迴歸係數估計值的標準誤取決於預測變項和依變項，並且要留意有沒有違反變異數相等的適用條件 （稍後 Section 10.9 就會討論）。\n無論如何，這個t統計值可以按照 Chapter 11 介紹的檢定方法解釋結果。若是設定做雙尾檢定（也就是說，你不在乎是b > 0還是b < 0），那麼極端的t值（即遠小於零或遠大於零的值）表示你應該拒絕虛無假設。\n\n\n10.7.3 用 jamovi 執行假設檢定\n要計算以上介紹的統計量數，只需要在jamovi迴歸模組選單勾選對應的選項。要選擇的選項如同 Figure 10.15 的示範，會得到一系列有用的報表輸出。\n\n\n\n\n\nFigure 10.15: jamovi示範畫面，顯示一個多元線性迴歸分析，請留意其中勾選的選項\n\n\n\n\n在jamovi分析結果的“模型係數”表格(Model Coefficients)顯示迴歸模型的係數。此表中的每一行都是對應迴歸模型的其中一個係數。第一行是截距，後面每一行是每個預測變項的檢定結果。每一列標示各種統計訊息。第一列是\\(b\\)的實際估計值（例如，截距為\\(125.97\\)，預測變項dani.sleep 為\\(-8.95\\)）。第二列是標準誤的估計值\\(\\hat{\\sigma}_b\\)。第三和第四列是關於係數估計值的95%信賴區間的下限和上限（稍後對此有更多說明）。第五列是t統計值，值得注意的是，在這個表格中，\\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\)每次分析結果都是成立的。最後一列呈現這些檢定結果的p值。16\n模型係數表唯一沒有列出的是t檢定的自由度，不過其值始終是\\(N - K - 1\\)，並且輸出到報表標題後的“模型適合度度量”(Model Fit Measures)表格中17。從這個表格中，我們可以看到模型的表現顯著優於機會水平（\\(F(2,97) = 215.24, p< .001\\)），其實這並不奇怪：\\(R^2 = .81\\)值表示迴歸模型能解釋依變項變異量的\\(81\\%\\)（調整後的\\(R^2\\)為\\(82\\%\\) ）。然而，當我們回顧每個個別係數的t檢定時，我們有相當有力的證據表明baby.sleep變項沒有顯著效果。這個模型的主要預測效力都是來由dani.sleep變項。綜合這些結果，我們可以結論這個多元迴歸模型實際上並不是能有效解 資料的模型，最好的模型應該排除baby.sleep這個預測變項。換句話說，開始的簡單迴歸模型是更好的模型。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-regression-Model-diagnosis",
    "href": "10-Correlation-and-linear-regression.html#sec-regression-Model-diagnosis",
    "title": "10  相關與線性迴歸",
    "section": "10.10 診斷迴歸模型的適用條件",
    "text": "10.10 診斷迴歸模型的適用條件\n\n譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。\n\n本節的主要焦點是迴歸診斷，這個術語是指檢查迴歸模型假設是否得到滿足、在假設被違反時如何修正模型以及一般情況下檢查是否存在不尋常情況的技術。我將這稱為模型檢查的“藝術”，理由很充分。這並不容易，儘管有許多相當標準化的工具可以用來診斷甚至可能治愈困擾模型的問題（如果存在的話！），但在這方面真的需要運用一定程度的判斷力。在檢查這件事情或那件事情的所有細節中容易迷失，試圖記住所有不同的事物是相當耗費精力的。這會產生一個非常令人討厭的副作用，很多人在試圖學習所有工具時會感到沮喪，所以他們決定不做任何模型檢查。這有點令人擔憂！\n在本節中，我描述了一些方法，用於檢查迴歸模型是否按照預期工作。它並沒有涵蓋所有您可能做的事情，但仍然比我在實踐中看到的大多數人所做的事情要詳細得多，即使在我的初級統計課程中，我通常也不會涵蓋所有這些內容。但是，我確實認為您應該了解可供您使用的工具，所以我將在這裡嘗試介紹一部分。最後，我應該指出，本節很大程度上借鑒了 Fox & Weisberg (2011) ，即與在 R 中進行迴歸分析的“car”包相關的書籍。 “car”包以提供一些出色的迴歸診斷工具而著稱，而該書本身以極為清晰的方式談論了這些工具。我不想聽起來太過於誇大，但我確實認為即使在 R 和不是 jamovi 的情況下， Fox & Weisberg (2011) 都值得一讀。\n\n10.10.1 三種殘差\n大多數迴歸診斷都圍繞著觀察殘差，到目前為止，你可能已經對統計學形成了足夠悲觀的理論，能夠猜到，正因為我們非常關心殘差，我們可能會考慮幾種不同類型的殘差。特別地，在本節中，我們將提到以下三種殘差：“普通殘差”、“標準化殘差”和“學生化殘差”。還有第四種你會在一些圖中看到的，稱為“皮爾森殘差”。然而，對於我們在本章中討論的模型，皮爾森殘差與普通殘差相同。\n首先，我們關心的最簡單類型的殘差是普通殘差。這些就是我在本章前面一直提到的實際原始殘差。普通殘差僅僅是擬合值 \\(\\hat{Y}_i\\) 和觀察值 \\(Y_i\\) 之間的差。我一直用符號 \\(\\epsilon_i\\) 表示第 i 個普通殘差，並且我將繼續堅持使用它。考慮到這一點，我們有非常簡單的方程式\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\n這當然是我們之前看到的，除非我特別提到其他類型的殘差，否則我就是在談論這個。所以這裡沒有新的東西。我只是想重申一下。使用普通殘差的一個缺點是，它們總是在不同的尺度上，取決於結果變項是什麼以及迴歸模型有多好。也就是說，除非你決定在沒有截距項的情況下運行迴歸模型，否則普通殘差的均值將為 0，但每個迴歸的方差都不同。在很多情境下，特別是當你只對殘差的模式感興趣，而不是它們的實際值時，估計標準化殘差很方便，這些殘差經過規範化後標準差為 1。\n[額外技術細節22]\n第三種殘差是學生化殘差（也稱為 “剃刀切割殘差”），它們比標準化殘差更高級。同樣，目的是將普通殘差除以某個量，以估計殘差的某種標準化概念。23\n在繼續之前，我應該指出，即使這些殘差是幾乎所有迴歸診斷的核心，你通常也不需要自己獲得這些殘差。大多數時候，提供診斷或假設檢查的各種選項將為您處理這些計算。即使如此，知道如何實際自己獲得這些東西，以防你需要進行一些非標準操作，總是很好的。\n\n\n10.10.2 三種異常資料\n使用線性迴歸模型時，您可能會遇到一個危險，那就是您的分析可能會對一小部分”不尋常”或”異常”的觀測值過於敏感。我之前在 Section 5.2.3 的上下文中討論過這個想法，當時是在討論用 ‘探索’ - ‘描述統計’ 下的 boxplot 選項自動識別的異常值，但這次我們需要更精確。在線性迴歸的背景下，有三個概念上不同的方式可以將觀測值稱為”異常”。這三者都很有趣，但對你的分析有很不同的影響。\n第一種不尋常的觀測值是異常值。在這種情況下，異常值的定義是與迴歸模型預測的結果相差很大的觀測值。Figure 10.17 中有一個例子。在實踐中，我們通過說一個異常值是具有非常大的Studentised殘差的觀測值，\\(\\epsilon_i^*\\)，來實現這個概念。異常值很有趣：一個很大的異常值可能對應垃圾數據，例如，變項在數據集中可能被錯誤地記錄，或者可能檢測到其他缺陷。請注意，僅僅因為它是一個異常值，你不應該丟掉這個觀測值。但是，它是一個異常值，這經常是一個線索，讓你更仔細地查看該案例，並嘗試找出它為什麼如此不同。\n\n\n\n\n\nFigure 10.17: 異常值的示例。虛線繪製了不包括異常觀測值的迴歸線，以及相應的殘差（即Studentised殘差）。實線顯示了包含異常觀測值的迴歸線。異常值在結果值（y軸位置）上具有不尋常的值，但在預測變項（x軸位置）上並不不尋常，並且距離迴歸線很遠\n\n\n\n\n觀測值不尋常的第二種方式是具有高槓桿作用(leverage)，這發生在觀測值與所有其他觀測值非常不同的情況下。這不一定要對應大的殘差。如果觀測值在所有變項上的不尋常程度恰好相同，則實際上可能非常接近迴歸線。這方面的一個例子如@fig-fig10-18所示。觀測值的槓桿作用通常用帽子值表示，通常寫作\\(h_i\\)。帽子值的公式相當複雜24，但它的解釋並不複雜：\\(h_i\\)是衡量第i個觀測值“控制”迴歸線走向的程度。\n\n\n\n\n\nFigure 10.18: 高槓桿點的示例。在這種情況下，異常觀測值在預測變項（x軸）和結果變項（y軸）方面都不尋常，但這種不尋常性與其他觀測值之間存在的相關性模式高度一致。觀測值非常接近迴歸線並且不會使其變形\n\n\n\n\n一般來說，如果觀測值在預測變項方面遠離其他觀測值，它將具有較大的帽子值（作為粗略指南，高槓桿是指帽子值大於平均值的2-3倍；注意帽子值的總和被限制為等於\\(K + 1\\)）。高槓桿點也值得更詳細地查看，但除非它們也是異常值，否則它們不太可能引起擔憂。\n這讓我們來到了不尋常程度的第三個衡量指標，即觀測值的影響力(influence)。高影響力的觀測值是具有高槓桿的異常值。也就是說，它在某些方面與所有其他觀測值非常不同，並且距離迴歸線很遠。這在@fig-fig10-19中有所體現。注意與前兩個圖形的對比。異常值並未使迴歸線發生很大變化，高槓桿點也是如此。但既是異常值又具有高槓桿的情況，會對迴歸線產生很大影響。這就是為什麼我們稱這些點具有高影響力，而且它們是最令人擔憂的。我們用稱為Cook’s distance的衡量指標來度量影響力。25\n\n\n\n\n\nFigure 10.19: 高影響力點的示例。在這種情況下，異常觀測值在預測變項（x軸）上非常不尋常，並且距離迴歸線很遠。因此，即使在這種情況下，異常觀測值在結果變項（y軸）上完全正常，迴歸線也會受到很大影響\n\n\n\n\n要具有較大的Cook’s距離，觀測值必須是相當大的異常值並具有高槓桿。作為粗略指南，大於1的Cook’s距離通常被認為很大（這是我通常用作快速而簡單的規則）。\n在jamovi中，可以通過單擊’Assumption Checks’ - ’Data Summary’選項下的’Cook’s Distance’複選框來計算有關Cook’s距離的信息。當你這樣做時，對於我們在本章中作為示例使用的多元迴歸模型，你將得到如@fig-fig10-20所示的結果。\n\n\n\n\n\nFigure 10.20: jamovi輸出顯示Cook’s距離統計表格\n\n\n\n\n您可以看到，在這個例子中，平均Cook’s距離值為\\(0.01\\)，範圍從\\(0.00\\)到\\(0.11\\)，因此這與前面提到的指標相去甚遠，即大於1的Cook’s距離被認為很大。\n接下來明顯要問的問題是，如果您確實擁有很大的Cook’s距離值，您應該怎麼辦？一如既往，沒有固定不變的規則。可能首先要做的是嘗試運行迴歸，排除具有最大Cook’s距離的異常值26，看看模型性能和迴歸係數會發生什麼變化。如果它們確實有很大不同，那就該開始深入研究您的數據集和您在運行研究時無疑在抄寫的筆記。嘗試找出該點為何如此不同。如果您開始確信這個數據點嚴重扭曲了您的結果，那麼您可能會考慮將其排除在外，但除非您對於這個特定案例與其他案例有本質上的不同並因此應該單獨處理，否則這種做法是不理想的。\n\n\n\n\n\n10.10.3 檢測殘差常態性\n像我們在本書中討論過的許多統計工具一樣，迴歸模型依賴於常態性假設。在這種情況下，我們假設殘差呈常態分布。首先，我們可以通過 ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ 選項繪製一個 QQ 圖。輸出顯示在 Figure 10.21，顯示了標準化殘差作為其根據迴歸模型的理論分位數的函數的圖。\n\n\n\n\n\nFigure 10.21: 模型理論分位數與標準化殘差的分位數之間的圖，由 jamovi 生成\n\n\n\n\n我們還應該檢查擬合值和殘差本身之間的關係。我們可以使用 ‘Residuals Plots’ 選項讓 jamovi 做到這一點，該選項為每個預測變項、結果變項以及擬合值與殘差提供了散點圖，見 Figure 10.22。在這些圖中，我們要尋找的是點的分佈相對均勻，沒有明顯的聚集或圖案。觀察這些圖，沒有什麼特別令人擔憂的，因為點在整個圖中分佈得相當均勻。在圖 (b) 中可能存在一點非均勻性，但偏差不大，可能不值得擔憎。\n\n\n\n\n\nFigure 10.22: 由 jamovi 生成的殘差圖\n\n\n\n\n如果我們擔憂的話，那麼在很多情況下解決這個問題（以及其他許多問題）的方法是對一個或多個變項進行轉換。我們在 Section 6.3 中討論了變項轉換的基本知識，但我想特別提一下我之前沒有完全解釋的一個額外可能性：Box-Cox 轉換。Box-Cox 函數相當簡單，並且被廣泛使用。27\n您可以在 jamovi 的 ‘Compute’ 變數屏幕中使用 BOXCOX 函數進行計算。\n\n\n10.10.4 檢測共線性\n在本章中，我將討論的最後一種迴歸診斷方法是使用方差膨脹因子（VIF），這對於確定迴歸模型中的預測變項是否彼此相關性過高很有用。模型中每個預測變項 \\(X_k\\) 都有一個相應的方差膨脹因子。28\nVIF 的平方根具有很好的解釋性。它告訴您相應的係數 bk 的信賴區間相對於預測變項彼此完全不相關時所期望的值要寬多少。如果您只有兩個預測變項，VIF 值將始終相同，正如我們在 jamovi 的 ‘Regression’ - ‘Assumptions’ 選項中選中 ‘Collinearity’ 後可以看到的那樣。對於 dani.sleep 和 baby.sleep，VIF 均為 1.65。而由於 1.65 的平方根為 1.28，我們可以看到我們兩個預測變項之間的相關性並未造成太大問題。\n為了給出我們可能會遇到具有更大共線性問題的模型的感覺，假設我要運行一個更無趣的迴歸模型，在該模型中，我試圖預測數據收集的日期，作為數據集中所有其他變項的函數。要理解這為什麼會有問題，讓我們看一下所有四個變項的相關矩陣（Figure 10.23）。\n\n\n\n\n\nFigure 10.23: 四個資料變項之間的相關係數矩陣\n\n\n\n\n我們的預測變項之間有一些相當大的相關性！當我們運行迴歸模型並查看 VIF 值時，我們可以看到共線性對係數的不確定性造成了很大影響。首先，運行迴歸，如 Figure 10.24 所示，從 VIF 值可以看出，是的，這是一些非常好的共線性。\n\n\n\n\n\nFigure 10.24: jamovi 生成的多元迴歸共線性統計"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#迴歸模型的適合度",
    "href": "10-Correlation-and-linear-regression.html#迴歸模型的適合度",
    "title": "10  相關與線性迴歸",
    "section": "10.6 迴歸模型的適合度",
    "text": "10.6 迴歸模型的適合度\n現在我們知道如何估計線性迴歸模型的係數。問題是，我們還不知道這個迴歸模型是否有效。例如，根據一號模型，多睡一小時我(原作者)的情緒會大大改善，但這可能只是廢話。請記住，迴歸模型只是生成個人情緒的預測值 \\(\\hat{Y}_i\\)，實際的情緒量測值則是 \\(Y_i\\)。如果兩種數值非常接近，表示迴歸模型非常適合預測我的情緒變化。如果兩種數值差異很大，那麼這個模型就並不太適合用來預測我的情緒變化。\n\n\n10.6.1 \\(R^2\\)\n我們再次引用一些數學知識解釋如何評估模型的適合度。首先來認識殘差平方和\n\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\n實務上會期望殘差平方和越小越好。具體地說，殘差平方和佔總變異平方和的比例越小越好\n\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\n談到這裡，我們可以逐步計算這些數值，不過不是用手算。而是使用像是Excel或其他試算表軟體。在Excel中打開parenthood.csv這份檔案，再另存新檔為parenthood rsquared.xls就能完成計算。計算步驟第一步是計算 \\(\\hat{Y}\\) 值，按照以下步驟，我們可以得到以我的睡眠時間預測情緒的簡單迴歸模型：\n\n使用公式= 125.97 + (-8.94 * dani.sleep)創建新欄位Y.pred。\n創建新欄位(Y-Y.pred)\\^2，使用公式= (dani.grump - Y.pred)\\^2計算SS(resid)。\n在(Y-Y.pred)\\^2的最後一列使用公式sum( ( Y-Y.pred)\\^2 )計算這些值的總和。\n在dani.grump的最後一列，計算dani.grump的平均值（留意Excel的函數是’AVERAGE’而不是’mean’）。\n創建新欄位(Y - mean(Y))\\^2 )，使用公式= (dani.grump - AVERAGE(dani.grump))^2。\n在(Y - mean(Y))\\^2 )最後一列，使用= sum( (Y - mean(Y))^2 )計算總和。\n在一個空白儲存格中輸入= 1 - (SS(resid) / SS(tot) )，計算R.squared。\n\n至此我們算出 \\(R^2\\) 的數值 = 0.8161018。\\(R^2\\) 在有些教科書裡被稱呼為決定係數12，這個名稱有一個簡單的解釋：預測變項總變異解釋依變項總變異的比例。因此，在這裡得到的 \\(R^2 = .816\\) 代表預測變項（my.sleep）解釋依變項（my.grump）總變異的81.6％。\n想當然而，如果同學想計算迴歸模型的 \\(R^2\\) ，實際上不需要自己用 Excel 計算。稍後在 Section 10.7.3 用 jamovi 執行假設檢定這一節，同學會看到只要在模組選單裡勾選指定選項即可。現在，讓我們暫時擱置計算問題，我想談談 \\(R^2\\) 的另一個性質。\n\n\n\n10.6.2 迴歸與相關的關聯\n現在可以重新回顧在這一章開始，我說到迴歸與相關基本上是同一回事。在相關的小單元，我們用符號 \\(r\\) 表示皮爾森相關。那麼皮爾森相關係數 \\(r\\) 和線性迴歸的 \\(R^2\\) 有存在某種關係嗎？當然有：只要將相關係數開平方， \\(r^2\\) 與只有一個預測變項的線性迴歸 \\(R^2\\) 數值是相同的。換句話說，計算皮爾森相關與計算僅有一個預測變項的線性迴歸模型基本上是相同的。\n\n\n\n10.6.3 校正後 \\(R^2\\)\n\n譯註：只有分析多元迴歸的場景，才要了解校正後 \\(R^2\\)。\n\n在繼續到下個單元之前，我最後要指出的是，統計實務通常會報告一個稱為“校正後 \\(R^2\\) ”的計量值。計算及報告校正後 \\(R^2\\) 值的理由是，到將兩個以上預測變項添加到模型中，總是會增加（或至少不降低） \\(R^2\\) 。\n[額外技術細節13]\n校正的目的是為了處理自由度。校正後 \\(R^2\\) 的主用用途是，往模型添加更多預測變後，只有能提高模型預測能力的新變項，才會顯著增加校正後 \\(R^2\\)的數值。然而，校正後 \\(R^2\\)就無法像一開始的 \\(R^2\\) 那樣的直接解釋。據我所知，調整後的 \\(R^2\\) 沒有任何相等意義的解釋。\n那麼統計實務中應該要報告 \\(R^2\\) 還是校正後 \\(R^2\\)？這可能是因人而異。如果同學比較想解釋報告裡的數值，那麼 \\(R^2\\) 較好。如果在乎校正模型的預測偏差，那麼校正後 \\(R^2\\) 可能比較好。就我(原作者)自己而言，我更喜歡 \\(R^2\\)，因為我覺得最重要的是能夠解釋模型預測能力的計量。此外，我們將在 Section 10.7 迴歸模型的假設檢定這個小單元看到，如果想知道添加預測變項後增加的 \\(R^2\\) 是由於機遇還是因為模型預測能力真的改善了，那麼我們可以用假設檢定來做判斷。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-regression-model-fittedness",
    "href": "10-Correlation-and-linear-regression.html#sec-regression-model-fittedness",
    "title": "10  相關與線性迴歸",
    "section": "10.6 迴歸模型的適合度",
    "text": "10.6 迴歸模型的適合度\n現在我們知道如何估計線性迴歸模型的係數。問題是，我們還不知道這個迴歸模型是否有效。例如，根據一號模型，多睡一小時我(原作者)的情緒會大大改善，但這可能只是廢話。請記住，迴歸模型只是生成個人情緒的預測值 \\(\\hat{Y}_i\\)，實際的情緒量測值則是 \\(Y_i\\)。如果兩種數值非常接近，表示迴歸模型非常適合預測我的情緒變化。如果兩種數值差異很大，那麼這個模型就並不太適合用來預測我的情緒變化。\n\n\n10.6.1 \\(R^2\\)\n我們再次引用一些數學知識解釋如何評估模型的適合度。首先來認識殘差平方和\n\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\n實務上會期望殘差平方和越小越好。具體地說，殘差平方和佔總變異平方和的比例越小越好\n\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\n談到這裡，我們可以逐步計算這些數值，不過不是用手算。而是使用像是Excel或其他試算表軟體。在Excel中打開parenthood.csv這份檔案，再另存新檔為parenthood rsquared.xls就能完成計算。計算步驟第一步是計算 \\(\\hat{Y}\\) 值，按照以下步驟，我們可以得到以我的睡眠時間預測情緒的簡單迴歸模型：\n\n使用公式= 125.97 + (-8.94 * dani.sleep)創建新欄位Y.pred。\n創建新欄位(Y-Y.pred)\\^2，使用公式= (dani.grump - Y.pred)\\^2計算SS(resid)。\n在(Y-Y.pred)\\^2的最後一列使用公式sum( ( Y-Y.pred)\\^2 )計算這些值的總和。\n在dani.grump的最後一列，計算dani.grump的平均值（留意Excel的函數是’AVERAGE’而不是’mean’）。\n創建新欄位(Y - mean(Y))\\^2 )，使用公式= (dani.grump - AVERAGE(dani.grump))^2。\n在(Y - mean(Y))\\^2 )最後一列，使用= sum( (Y - mean(Y))^2 )計算總和。\n在一個空白儲存格中輸入= 1 - (SS(resid) / SS(tot) )，計算R.squared。\n\n至此我們算出 \\(R^2\\) 的數值 = 0.8161018。\\(R^2\\) 在有些教科書裡被稱呼為決定係數11，這個名稱有一個簡單的解釋：預測變項總變異解釋依變項總變異的比例。因此，在這裡得到的 \\(R^2 = .816\\) 代表預測變項（my.sleep）解釋依變項（my.grump）總變異的81.6％。\n想當然而，如果同學想計算迴歸模型的 \\(R^2\\) ，實際上不需要自己用 Excel 計算。稍後在 Section 10.7.3 用 jamovi 執行假設檢定這一節，同學會看到只要在模組選單裡勾選指定選項即可。現在，讓我們暫時擱置計算問題，我想談談 \\(R^2\\) 的另一個性質。\n\n\n\n10.6.2 迴歸與相關的關聯\n現在可以重新回顧在這一章開始，我說到迴歸與相關基本上是同一回事。在相關的小單元，我們用符號 \\(r\\) 表示皮爾森相關。那麼皮爾森相關係數 \\(r\\) 和線性迴歸的 \\(R^2\\) 有存在某種關係嗎？當然有：只要將相關係數開平方， \\(r^2\\) 與只有一個預測變項的線性迴歸 \\(R^2\\) 數值是相同的。換句話說，計算皮爾森相關與計算僅有一個預測變項的線性迴歸模型基本上是相同的。\n\n\n\n10.6.3 校正後 \\(R^2\\)\n\n譯註：只有分析多元迴歸的場景，才要了解校正後 \\(R^2\\)。\n\n在繼續到下個單元之前，我最後要指出的是，統計實務通常會報告一個稱為“校正後 \\(R^2\\) ”的計量值。計算及報告校正後 \\(R^2\\) 值的理由是，到將兩個以上預測變項添加到模型中，總是會增加（或至少不降低） \\(R^2\\) 。\n[額外技術細節12]\n校正的目的是為了處理自由度。校正後 \\(R^2\\) 的主用用途是，往模型添加更多預測變後，只有能提高模型預測能力的新變項，才會顯著增加校正後 \\(R^2\\)的數值。然而，校正後 \\(R^2\\)就無法像一開始的 \\(R^2\\) 那樣的直接解釋。據我所知，調整後的 \\(R^2\\) 沒有任何相等意義的解釋。\n那麼統計實務中應該要報告 \\(R^2\\) 還是校正後 \\(R^2\\)？這可能是因人而異。如果同學比較想解釋報告裡的數值，那麼 \\(R^2\\) 較好。如果在乎校正模型的預測偏差，那麼校正後 \\(R^2\\) 可能比較好。就我(原作者)自己而言，我更喜歡 \\(R^2\\)，因為我覺得最重要的是能夠解釋模型預測能力的計量。此外，我們將在 Section 10.7 迴歸模型的假設檢定這個小單元看到，如果想知道添加預測變項後增加的 \\(R^2\\) 是由於機遇還是因為模型預測能力真的改善了，那麼我們可以用假設檢定來做判斷。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-regression-estimations",
    "href": "10-Correlation-and-linear-regression.html#sec-regression-estimations",
    "title": "10  相關與線性迴歸",
    "section": "10.8 迴歸係數的估計值",
    "text": "10.8 迴歸係數的估計值\n在討論線性迴歸的適用條件，以及如何檢查一種模型是否滿足條件之前，這裡先簡單討論兩個與迴歸係數有關的主題。首先是如何計算迴歸係數的信賴區間。然後是如何確定哪個預測變項最重要。\n\n10.8.1 迴歸係數的信賴區間\n就像任何人口變項一樣，迴歸係數b無法從樣本資料精確地估算出來；這就是為什麼需要使用假設檢定的一部分原因。有鑑於此，信賴區間能夠呈現捕捉\\(b\\)真實數值的不確定性，是非常有用的工具。這在嘗試找出變項\\(X\\)與變項\\(Y\\)之間的關係強度的研究問題尤其有用，因為在這些研究裡，主要關注的是迴歸權重\\(b\\)(regression weight)。\n[額外技術細節18]\n在jamovi的操作界面，我們可以指定“95％信賴區間”，如 Figure 10.15 的示範，如果想要更嚴謹的話，我們可以輕鬆選擇另一種區間，例如“99％信賴區間”。\n\n\n10.8.2 標準化迴歸係數的計算方法\n有經驗的使用者可能還會計算“標準化”迴歸係數，通常報告中用\\(\\beta\\)表示。標準化係數的基本原理是：在很多情況下，每個變項的測量尺度是不一樣的。例如，如果有個迴歸模型要探討受教育程度（受教育年數）和收入作為預測變項，來預測受測者的智力測驗得分。顯然，受教育程度和收入的計量尺度是不相同的。一般人的教育年限可能只有10多年，而收入差距可能高達10,000美元（或更多）。計量單位對迴歸係數有很大影響，只有預測變項和依變項的計量單位一致時，迴歸係數才具有意義，否則比較不同預測變項的迴歸係數將會非常困難。然而，有時我們希望能比較不同變項的係數。具體來說，研究者最想找到那些預測變項與依變項之間相關性最強力的標準衡量指標，這就是為什麼有標準化迴歸係數。\n標準化迴歸係數的思路很簡單；如果在執行迴歸分析之前將所有變項轉換為z分數，標準化係數就是您會得到的係數。19這裡的想法是，將所有預測變項數值轉換為z分數，使得迴歸模型的生成的機率分佈具有可對應的比例，進而消除不同尺度的變項產生的問題。無論變項的原始尺度是什麼，\\(\\beta\\)值為1都代表增加預測變項的1個標準差，就是導致依變項的對應數值增加1個標準差。因此，如果預測變項A的\\(\\beta\\)絕對值大於預測變項B的\\(\\beta\\)，研究者至少能主張預測變項A與依變項的相關性更強。值得小心的是，對於所有變項變異基本相同的這個條件 ，其實非常依賴“1個標準差變化”，並不是什麼形式的變項都可見。\n[額外技術細節20]\n為了讓簡化分析程序，只要在jamovi模組選單裡，從”Model Coefficients”的選項中勾選”Standardized estimate”，就能計算\\(\\beta\\)，如同 Figure 10.16 的示範。\n\n\n\n\n\nFigure 10.16: 多元線性迴歸的標準化係數及其95％信賴區間\n\n\n\n\n從輸出結果可明顯看出，dani.sleep是比baby.sleep有更強預測效力的變項。然而，這可能正是一個更適合使用原始係數b而不是標準化係數\\(\\beta\\)的完美例子。畢竟，我的睡眠時間和寶寶的睡眠時間已經是同一個計量尺度。為什麼要還要將它們轉換為z分數來讓事情變得更複雜呢？"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-regression-assumptions",
    "href": "10-Correlation-and-linear-regression.html#sec-regression-assumptions",
    "title": "10  相關與線性迴歸",
    "section": "10.9 迴歸模型的適用條件",
    "text": "10.9 迴歸模型的適用條件\n線性迴歸模型必須符合幾個適用條件(assumptions)，才能解讀分析結果。在 Section 10.10 診斷迴歸模型的適用條件這個單元，我們將會學習如何檢查這些假設是否得到滿足，首先簡單說明每個假設的涵意。21\n\n線性。線性迴歸模型的最基本的適用條件是\\(X\\)和\\(Y\\)之間的關係必須是線性的！無論是簡單迴歸還是多元迴歸，我們都要假設變項之間的關係是線性的。\n獨立性：這是說變項資料的殘差彼此獨立。實際上這是條“總括一切”的適用條件，更白話地說“在殘差中找不到任何有意思的東西了”。如果還能分析出一些意料之外的資訊（例如，所有殘差都與某些未測量的變項存在明顯相關性），可能會破壞分析後的結論。\n常態性。就像驅動許多統計方法的機率模型一樣，基本的簡單或多元線性迴歸也要符合常態性。具體來說，這條是指殘差的次數分佈逼近或符合常態分佈。實際上，即使預測變項\\(X\\)和依變項\\(Y\\)的實際資料次數分佈不符合常態分佈，只要殘差\\(\\epsilon\\)的次數分佈符合常態的就可以了。 Section 10.10 診斷迴歸模型的適用條件有進一步說明及示範。\n變異相等（或稱’同質性’）。嚴格來說，符合這個條件 的迴歸模型生成的所有殘差\\(\\epsilon_i\\)，都是來自一個平均值為0的常態分佈，更重要的是，每個殘差來源的機率分佈標準差\\(\\sigma\\)都是相同的。在實務中，檢驗每個殘差都是來自同一個機率分佈是不大可能做到的事。相反地，我們真正關心的是殘差的標準差相對於所有預測值\\(\\hat{Y}\\)是相同的，特別是多元迴歸模型的每個預測變項\\(X\\)所生成的預測值是相同的。\n\n所以，要執行有效的線性迴歸分析，首先要檢查是不是符合這四個適用條件 （剛好可以縮寫成LINE）。此外，還有一些需要檢查的條件 ：\n\n沒有“不良”極端值。其實這並非必要的適用條件，但是極端值可能造成潛在的問題。就是迴歸模型雖然不會因為一兩個異常極端值，造成不符合上述任何一條適用條件(譯註~特別是線性)，但是在某些情況下會引起對模型的適當性和資料可靠性的質疑。詳細說明及示範請見 Section 10.10.2 三種異常資料。\n預測變項之間無相關性。執行多元迴歸模型分析時，我們不希望預測變項彼此間的存在高度相關。這並非迴歸模型的“必要”適用條件，但在統計實務是必需的。預測變項之間相關性過強（通常稱為“共線性”）可能會造成錯誤解讀分析結果。詳細說明及示範請見 Section 10.10.4 檢查共線性。"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-paired-t-test",
    "href": "11-Comparing-two-means.html#sec-the-paired-t-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.6 相依樣本t檢定",
    "text": "11.6 相依樣本t檢定\n不論是談論學生 t 檢驗或韋爾奇 t 檢驗，獨立樣本 t 檢驗都適用於具有兩個互相獨立樣本的情況。當參與者被隨機分配到其中一個實驗條件時，自然會出現這種情況，但這對於其他類型的研究設計提供了一個非常差的近似。特別是，在重複測量設計中，每個參與者都在兩個實驗條件下測量（對於相同的結果變量），並不適合使用獨立樣本 t 檢驗進行分析。例如，我們可能會對聆聽音樂是否降低人們的工作記憶容量感興趣。為此，我們可以在兩種情況下測量每個人的工作記憶容量：有音樂和無音樂。在這樣的實驗設計中，16每個參與者出現在 兩個 組中。這需要我們以不同的方式來解決問題，即使用配對樣本 t 檢驗。\n\n\n11.6.1 示範資料\n這次我們要使用的資料集來自Chico博士的課程。17 在她的課堂上，學生要參加兩次主要考試，一次在學期初，一次在學期後。據她所說，她開的課很難，大多數學生都覺得很有挑戰性。但她認為，通過設置困難的評估，學生會被鼓勵更加努力地學習。她的理論是，第一次考試對學生來說是一個“提醒”，當他們意識到她的課有多難時，他們會為第二次考試更加努力，取得更好的成績。她的觀點正確嗎？為了測試這個問題，讓我們將chico.csv文件導入到jamovi中。這次，jamovi在導入期間做了一個好工作，正確地分類了變量的測量水平。chico資料集包含三個變量：一個id變量，用於識別課程中的每個學生，grade_test1變量記錄第一次考試的學生成績，grade_test2變量則是第二次考試的成績。\n如果我們看一下 jamovi 的試算表，似乎這個課程很難（大多數的成績都在50%到60%之間），但從第一次測驗到第二次測驗似乎有進步的趨勢。\n如果我們快速查看一下描述性統計，在 圖 11.12 中，我們可以看到這種印象似乎得到了支持。在所有20個學生中，第一次測驗的平均成績為57%，但第二次測驗的平均成績為58%。但是，考慮到標準差分別為6.6%和6.4%，這種進步感覺起來可能只是虛假的，也可能只是隨機變異。當你看到在@fig-fig11-13a中繪製的平均值和置信區間時，這種印象得到了加強。如果我們僅僅依靠這個圖表，看看這些置信區間有多寬，我們會認為學生表現的明顯改善純粹是偶然的。\n\n\n\n\n\n\n圖 11.12: chico資料集中的兩次測驗成績資料及描述統計\n\n\n\n\n然而，這種印象是錯的。要知道原因，請看 圖 11.13 (b) 中顯示的評分1和評分2的散佈圖。在這個圖中，每個點對應於一個給定學生的兩個成績。如果他們的評分1（x座標）等於他們的評分2（y座標），那麼該點就會落在直線上。在線上方的點是第二次測試表現更好的學生。重要的是，幾乎所有的資料點都在對角線以上：幾乎所有的學生似乎都有一些提高，即使只有一點點。這表明我們應該關注每個學生在一次測試和下一次測試中所取得的進步，並將其作為我們的原始資料。為此，我們需要創建一個新變量，用於表示每個學生所取得的進步，並將其添加到 chico 資料集中。最簡單的方法是計算一個新變量，使用表達式 grade test2 - grade test1。\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n圖 11.13: 第一次測驗和第二次測驗的平均分數，以及相應的95％置信區間(a)。顯示第一次測驗和第二次測驗的個別分數的散點圖(b)。 \n\n\n我們一旦計算了這個新的改進變量，就可以繪製一個直方圖，顯示這些改進分數的分布，如@fig-fig11-14所示。當我們觀察這個直方圖時，很明顯這裡有真正的進步。絕大多數的學生在第二次測試中得分比第一次高，這反映在幾乎整個直方圖都在零以上。\n\n\n\n\n圖 11.14: 柱狀圖顯示了 Chico 博士班上每位學生的成績提升。注意到幾乎整個分布都在 0 的上方——大多數學生在第二次考試中的表現確實有所提升。\n\n\n\n\n11.6.2 深入認識相依樣本t檢定\n根據先前的探索，讓我們思考如何建立一個適當的 t 檢定。一個可能的方法是嘗試使用grade_test1和grade_test2作為感興趣的變量進行獨立樣本 t 檢定。然而，這顯然是錯誤的，因為獨立樣本 t 檢定假定兩個樣本之間沒有特定的關係。然而，由於資料中的重複測量結構，這顯然是不正確的。使用我在上一節中介紹的語言，如果我們試圖進行獨立樣本 t 檢定，我們會混淆 樣本內 差異（這是我們想要測試的）和 樣本間 變異性（這是我們不想要的）。\n解決這個問題的方法很明顯，我希望，因為我們已經在前一節中完成了所有的艱苦工作。我們不是對grade_test1和grade_test2進行獨立樣本t檢驗，而是對內部差異變量improvement進行單樣本t檢驗。稍微形式化一下，如果\\(X_{i1}\\)是第i個參與者在第一個變量上獲得的分數，\\(X_{i2}\\)是同一個人在第二個變量上獲得的分數，那麼差異分數是：\n\\[D_i=X_{i1}-X_{i2}\\]\n請注意，差異分數是變量1減去變量2，而不是反過來，因此如果我們希望改進對應到一個正值的差異，我們實際上需要將「測試2」作為「變量1」。同樣，我們會說 \\(\\mu_D = \\mu_1 - \\mu_2\\) 是此差異變量的母群平均值。因此，為了將其轉換為假設檢驗，我們的虛無假設是此平均差異為零，而對立假設是它不是\n\\[H_0:\\mu_D=0\\] \\[H_1:\\mu_D \\neq 0\\]\n這假設我們進行的是雙邊檢定。這與我們描述一樣，進行一樣的假設檢定。唯一的不同之處在於零假設所預測的特定值為0。因此，我們的 t 統計量可以用類似的方式來定義。如果我們讓 \\(\\bar{D}\\) 代表差異得分的平均值，那麼\n\\[t=\\frac{\\bar{D}}{SE(\\bar{D})}\\]\n其中 \\(\\hat{\\sigma}_D\\) 是差異得分的標準差。因為這只是一個普通的單樣本 t 檢定，沒有什麼特別的地方，所以自由度仍然是 \\(N - 1\\)。這就是全部。實際上，配對樣本 t 檢定並不是一個新的檢定，它是一個應用在兩個變項之間差異上的單樣本 t 檢定。它實際上非常簡單。它需要進行長時間的討論的唯一原因是，您需要能夠辨認何時適用配對樣本 t 檢定，以及為什麼它比獨立樣本 t 檢定更好。\n\n\n\n11.6.3 實作相依樣本t檢定\n如何在jamovi中執行配對樣本t檢定？一種可能的方法是按照我上面概述的過程進行。即，創建一個“差異”變量，然後對其進行單樣本t檢定。因為我們已經創建了一個名為improvement的變量，讓我們這麼做並看看我們得到什麼，如@fig-fig11-15所示。\n\n\n\n\n\n\n圖 11.15: 配對差異分數的單一樣本t檢定結果表。\n\n\n\n\n在@fig-fig11-15中顯示的輸出與上一次使用單樣本t檢定分析時（小單元 11.2）完全相同的格式，並確認了我們的直覺。從第一次測試到第二次測試，平均改善了1.4％，並且這與0顯著不同\\((t(19)=6.48,p&lt;.001)\\)。\n然而，假設您很懶，不想花費大量精力創建新變量。或者也許您只想保持單樣本和配對樣本測試之間的差異清晰。如果是這樣，您可以使用jamovi的“配對樣本t檢定”分析，獲得@fig-fig11-16中顯示的結果。\n\n\n\n\n\n\n圖 11.16: 相依樣本t檢定結果表。請與 圖 11.15 比較看看。\n\n\n\n\n結果跟進行單樣本t檢定時的結果相同，這當然是因為成對樣本t檢定在實質上就是對差異變項執行單樣本t檢定。"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-the-one-tail-t-test",
    "href": "11-Comparing-two-means.html#sec-the-one-tail-t-test",
    "title": "11  比較單一與兩組平均值",
    "section": "11.7 單尾檢定",
    "text": "11.7 單尾檢定\n當介紹零假設檢定理論時，我提到有一些情況適合指定單邊檢定（請參閱 小單元 9.4.3 ）。到目前為止，所有的t檢定都是雙邊檢定。例如，當我們為Zeppo博士課堂上的成績指定單一樣本t檢定時，零假設是真實均值為\\(67.5\\%\\)。對立假設是真實均值高於或低於\\(67.5\\%\\)。假設我們只想知道真實均值是否高於\\(67.5\\%\\)，並且完全沒有興趣測試真實均值是否低於\\(67.5\\%\\)。如果是這樣，我們的零假設將是真實均值為\\(67.5\\%\\)或更低，對立假設將是真實均值高於\\(67.5\\%\\)。在jamovi中，對於“單一樣本t檢定”分析，您可以在“假設”下點擊“&gt; Test Value”選項進行指定。這樣做後，您將得到@fig-fig11-17中顯示的結果。\n\n\n\n\n\n\n圖 11.17: jamovi生成的「單一樣本t檢定」結果，其中實際假設是單側的，即真實平均數大於 \\(67.5%\\)\n\n\n\n\n注意到輸出與上次看到的輸出有一些變化。最重要的是實際假設已更改，以反映不同的測試。第二件要注意的事情是儘管t統計量和自由度沒有改變，但p值已經改變。這是因為單側測試具有與雙側測試不同的拒絕區域。如果您忘記了這是為什麼以及它意味著什麼，您可以回顧一下@sec-Hypothesis-testing，特別是@sec-The-difference-between-one-sided-and-two-sided-tests。第三件要注意的是置信區間也不同：現在報告了一個“單側”的置信區間，而不是一個雙側的置信區間。在雙側置信區間中，我們試圖找到數字a和b，使得如果我們重複進行該研究多次，那麼有\\(95\\%\\)的機會均值會落在a和b之間。在單側置信區間中，我們試圖找到一個數字a，使得我們有信心有\\(95\\%\\)的機會真實均值會大於a（如果您在“假設”部分中選擇了測量1 &lt; 測量2則為小於a）。\n所有版本的t檢定都可以是單側的。對於獨立樣本t檢定，如果您只想測試A組的得分是否比B組高，但對於是否要找出B組的得分是否高於A組沒有興趣，那麼您可以進行單側檢定。假設對於Harpo博士的課程，您想知道Anastasia的學生是否比Bernadette的學生成績更好。對於此分析，在“假設”選項中指定“Group 1 &gt; Group2”。您應該可以得到如圖@fig-fig11-18所示的結果。\n\n\n\n\n\n\n圖 11.18: jamovi生成的’獨立樣本t檢定’分析，實際假設為單尾檢定，即 Anastasia 的學生的成績高於 Bernadette 的學生\n\n\n\n\n再次強調，輸出結果有可預測的變化。替代假設的定義已改變，p值已變化，現在報告的是單邊信賴區間，而不是雙邊信賴區間。\n那麼，配對樣本t檢驗呢？假設我們想要測試 Dr Zeppo 課堂上考試成績是否從第一次測試到第二次測試有所提高，並且不考慮成績下降的可能性。在 jamovi 中，您可以通過在「假設」選項下指定 grade_test2 （在 jamovi 中為「測量1」，因為我們首先將其複製到了配對變量框中）&gt; grade_test1 （在 jamovi 中為「測量2」）來實現這一點。您應該可以看到 圖 11.19 中的結果。\n\n\n\n\n\n\n\n圖 11.19: jamovi生成的’相依樣本t檢定’分析，實際假設為單尾檢定，即測驗1成績 &gt; 測驗2成績。\n\n\n\n\n這次的輸出和之前一樣，以可預測的方式改變。假設已經改變，p值也改變，並且置信區間現在是單邊的。"
  },
  {
    "objectID": "11-Comparing-two-means.html#sec-t-test-effect-size",
    "href": "11-Comparing-two-means.html#sec-t-test-effect-size",
    "title": "11  比較單一與兩組平均值",
    "section": "11.8 t檢定的效果量",
    "text": "11.8 t檢定的效果量\n以下是您將翻譯成繁體中文的部分。\n對於t檢驗，最常用的效應量測量方法是科恩的d (Cohen, 1988)。從原理上看，它是一個非常簡單的方法，但當您深入研究細節時，會有相當多的變化。科恩本人主要在獨立樣本t檢驗的上下文中對其進行了定義，特別是學生檢驗。在該背景下，定義效應量的自然方法是將均值之間的差除以標準差的估計。換句話說，我們要計算的是類似於以下公式的東西：\n\\[d=\\frac{(\\text{平均值1})-(\\text{平均值2})}{\\text{標準差}}\\]\n他在 ?tbl-tab11-3中建議了一個解釋$d$的粗略指南。\n\n\n\n\n\n表 11.3: 解釋科恩的d的（非常）粗略指南。我的個人建議是不要盲目地使用這些。d統計量本身具有自然的解釋。它將均值之間的差重新描述為將這些均值分開的標準差數。因此，通常最好考慮一下這在實際條件下意味著什麼。在某些情境下，一個「小」的效應可能具有很大的實際重要性。在其他情況下，一個「大」的效應可能並不是那麼有趣。\n\n\nd-value\nrough interpretation\n\n\nabout 0.2\n\"small\" effect\n\n\nabout 0.5\n\"moderate\" effect\n\n\nabout 0.8\n\"large\" effect\n\n\n\n\n\n\n\n\n以下是一些您將翻譯成繁體中文的單詞。\n您可能會認為這應該是非常明確的，但事實並非如此。這主要是因為科恩對他認為應該用作標準差測量的方法並未過多具體（在他的辯護中，他在書中試圖闡述一個更廣泛的觀點，而不是對微小細節吹毛求疵）。如@McGrath2006所討論，常用的有幾個不同版本，每位作者都傾向於採用略有不同的表示法。為了簡單起見（而非準確性），我將使用d來指代您從樣本中計算出的任何統計量，並使用\\(\\delta\\)來指代理論的群體效應。顯然，這確實意味著有幾個不同的東西都被稱為d。\n我的懷疑是，您需要科恩的d的唯一時刻是當您運行t檢驗時，jamovi提供了一個選項可以計算它提供的所有不同類型t檢驗的效應量。\n\n\n11.8.1 單一樣本的Cohen’s d\n最簡單的情況是與單樣本t檢驗相對應的情況。在這種情況下，這是一個樣本均值\\(\\bar{X}\\)和一個（假設的）群體均值\\(\\mu_0\\)進行比較。不僅如此，真正只有一種合理的方法來估計群體標準差。我們只需使用我們通常的估計\\(\\hat{\\sigma}\\)。因此，我們最終得出以下計算\\(d\\)的唯一方法\n\\[d=\\frac{\\bar{X}-\\mu_0}{\\hat{\\sigma}}\\]\n當我們回顧@fig-fig11-6中的結果時，效應量值是科恩的\\(d = 0.50\\)。因此，總的來說，Zeppo博士班上的心理學生取得的成績（\\(mean = 72.3\\%\\)）比您預期的水平（\\(67.5\\%\\)）高出約0.5個標準差，如果他們的表現與其他學生相同。根據科恩的粗略指南，這是一個中等效應量。\n\n\n\n11.8.2 獨立樣本的Cohen’s d\n大多數關於科恩的\\(d\\)的討論都集中在與Student獨立樣本t檢驗相似的情況上，正是在這種情境下，故事變得更加混亂，因為在這種情況下，您可能想要使用幾個不同版本的\\(d\\)。為了理解為什麼\\(d\\)有多個版本，抽點時間寫下與真實群體效應大小\\(\\delta\\)相對應的公式是有幫助的。這很簡單，\n\\[\\delta=\\frac{\\mu_1-\\mu_2}{\\sigma}\\]\n其中，和往常一樣，\\(\\mu_1\\)和\\(\\mu_2\\)分別是與第1組和第2組相對應的群體均值，\\(\\sigma\\)是標準差（兩個群體都相同）。顯然，估計\\(\\delta\\)的方法是做我們在t檢驗本身中所做的完全相同的事情，即在分子使用樣本均值，並在分母使用池化標準差估計值\n\\[d=\\frac{\\bar{X}_1-\\bar{X}_2}{\\hat{\\sigma}_p}\\]\n其中\\(\\hat{\\sigma}_p\\)是t檢驗中出現的完全相同的池化標準差度量。這是應用於Student t檢驗結果時最常用的科恩的d版本，也是jamovi中提供的版本。有時它被稱為Hedges的\\(g\\)統計量(Hedges, 1981)。\n然而，還有其他可能性，我將簡要描述。首先，您可能有理由只想用兩個組中的一個作為計算標準差的基礎。這種方法（通常稱為Glass的\\(\\triangle\\)，讀作delta）在您有充分理由將兩個組中的一個視為比另一個更純粹地反映「自然變異」時，才最有意義。例如，如果兩個組中的一個是對照組，就可能發生這種情況。其次，回憶一下，在計算池化標準差的過程中，我們通常會除以\\(N - 2\\)以糾正樣本方差的偏差。在科恩的d的一個版本中，省略了這個糾正，而是除以\\(N\\)。當您試圖在樣本中計算效應量而不是估計群體中的效應量時，這個版本主要是有意義的。最後，有一個基於@Hedges1985的版本，叫做Hedge的g，他指出在科恩的d的常規（池化）估計中存在一個小的偏差。18\n\n\n\n11.8.3 相依樣本的Cohen’s d\n最後，對於配對樣本t檢驗，我們應該怎麼做？在這種情況下，答案取決於您試圖做什麼。jamovi假設您希望根據差異分數的分佈來衡量效應量，您計算的d度量為：\n\\[d=\\frac{\\bar{D}}{\\hat{\\sigma}_D}\\]\n其中\\(\\hat{\\sigma}_D\\)是差異的標準差估計。在@fig-fig11-16中，科恩的\\(d = 1.45\\)，表示時間2的分數平均比時間1的分數高出\\(1.45\\)個標準差。\n這是jamovi「配對樣本T檢驗」分析報告的科恩\\(d\\)版本。唯一的麻煩是弄清楚這是否是您想要的度量。在您關心研究的實際後果的程度上，您通常希望根據原始變量而不是差異分數（例如，與學生間的成績變異量相比，Chico博士班上隨著時間的1%改進相當小）來衡量效應量，在這種情況下，您使用的是與Student或Welch檢驗相同的科恩d版本。在jamovi中做到這一點並不那麼簡單；基本上您必須在試算表視圖中更改資料結構，所以我在這裡不會深入討論19，但是從這個角度看，科恩的d相當不同：它是\\(0.22\\)，在原始變量的尺度上評估相當小。"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-the-scatter-plot",
    "href": "10-Correlation-and-linear-regression.html#sec-the-scatter-plot",
    "title": "10  相關與線性迴歸",
    "section": "10.2 散佈圖",
    "text": "10.2 散佈圖\n散佈圖是一種簡單但有效的視覺化工具，用於具現兩個變項之間的關係，就像相關這一節所展示的圖表。通常提到“散佈圖”這個術語時，指的是具體的視覺化結果。在散佈圖中，每個觀察值都是對應一個資料點。一個點的水平位置表示一個變項的觀察值，垂直位置表示觀察值在另一個變項的數值。在許多使用情境，我們對於變項間的因果關係並沒有清晰的看法（例如，A是否引起B，還是B引起A，還是其他變項C控制A和B）。若是這樣，x軸和y軸上代表那個變項並不重要。然而在許多情境，研究者對於那個變項最有可能是原因或結果，會有一個相當明確的想法，或者對於何者為因至少有一些懷疑。若是這樣，用x軸代表原因的自變項，用y軸代表效應的應變項是一種傳統的繪圖規範。了解這樣的規範，讓我們來看一下如何合理運用jamovi繪製散佈圖，同樣使用在相關這一節做為示範的資料集（parenthood.csv）。\n假定我的目標是繪製一個顯示我睡眠時間（dani.sleep）與隔天沮喪程度（dani.grump）兩個變項關係的散佈圖，我們有兩種不同的方法使用jamovi得到我們想要的圖。第一種方法是設定’Regression’ - ‘Correlation Matrix’選單下方的’Plot’選項，這樣可以得到如圖 Figure 10.8 的結果。請注意，jamovi會繪製一條通過資料點的直線，稍後在認識線性迴歸模型這一節進一步說明。以這種方法繪製散佈圖也能繪製’變項密度’，這個選項會添加一條密度曲線，顯示每個變項的資料分佈狀況。\n\n\n\n\n\n\nFigure 10.8: 使用jamovi相關分析模組的’Correlation Matrix’所繪製的散佈圖。\n\n\n\n\n第二種方法是使用jamovi的附加模組之一scatr，只要點擊jamovi介面右上角的那個大「\\(+\\)」，在jamovi模組庫裡找到scatr，然後點擊「install」進行安裝。安裝成功後，在「Exploration」的選單下方會多出新的「Scatterplot」選項。這種方法繪製的散佈圖和第一種方法不大一樣，如同 Figure 10.9 所顯示，但是透露的訊息是一樣的。\n\n\n\n\n\n\nFigure 10.9: 使用jamovi擴充模組’scatr’繪製的散佈圖\n\n\n\n\n\n10.2.1 更多解讀散佈圖的方法\n通常我們會需要查看多個變項之間的關係，可以在 jamovi 的 ‘Correlation Matrix’選單下方的’Plot’ 選項，勾選繪制散佈圖矩陣。只要加入另一個變項到要變項列表，例如 baby.sleep，jamovi 就會生成一個散佈圖矩陣，如同 Figure 10.10 的示範。\n\n\n\n\n\n\nFigure 10.10: jamovi繪製的散佈圖矩陣"
  },
  {
    "objectID": "10-Correlation-and-linear-regression.html#sec-the-regression-coefficients-parameters",
    "href": "10-Correlation-and-linear-regression.html#sec-the-regression-coefficients-parameters",
    "title": "10  相關與線性迴歸",
    "section": "10.4 線性迴歸模型的參數估計",
    "text": "10.4 線性迴歸模型的參數估計\n\n好的，現在讓我們重新繪製散佈圖，這次會添加一些線條顯示所有觀察值的殘差。當迴歸線的適合度(fittedness)最佳時，每個殘差數值（實心黑線的長度）看起來都非常小且接近，如同 Figure 10.12 (a) ，但是當迴歸線的適合度不夠好，每個殘差之間的差異就會非常大，可以從 Figure 10.12 (b)看到這樣的差別。嗯，也許在尋找一條最好的迴歸模型時，我們會希望得到儘可能小的殘差。是的，這確實有道理。在統計實務，我們可以說「最適合」的迴歸線是具有所有殘差最小的線。或者更好的說法是，因為統計學家似乎喜歡將所有數值都用平方(sqaured)處理，也就是說：\n\n以資料估計的迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 是殘差平方和最小得到時得到的估計值，我們可以兩者的公式展開寫成 \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) 與 \\(\\sum_i \\epsilon_i^2\\) 。\n\n\n\n\n\n\n\nFigure 10.12: 圖(a)展示各觀察值資料點與穿越資料點中心地帶的最佳迴歸線之殘差，圖(b)展示觀察值資料點與最差迴歸線的殘差。前者的殘差總和明顯小於後者。\n\n\n\n\n是的沒錯，這樣說明起來更有學問一些。而且我將這段話縮排，表示這樣說可能是正確的解答。既然這是正確解答，那麼要值得注意的是，迴歸線的係數都是估計值（請復習 Chapter 8 ，使用點估計方法猜測一個母群的參數！），這也是為什麼我要在代表係數的符號上頭加個小帽子 ^ ，區別會放在報告的是\\(\\hat{b}_0\\)和\\(\\hat{b}_1\\)，而不是 \\(b_0\\) 和 \\(b_1\\)。最後，我還要指出，由於實際上有許多方法來估計迴歸模型，這一節說明的估計方法正式名稱是普通最小平方法（Ordinary Least Squares，OLS）。\n至此，我們已經得到「最佳」迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 的具體定義。下一個問題自然是：如果最佳迴歸係數是那些符合最小化殘差平方和的係數，我們要如何算出這些數值呢？實際上，這個問題的答案比較複雜，並且無法幫助你理解迴歸的邏輯。9這一次，我放過各位同學，直接介紹 jamovi 操作方法，瑣碎的讓jamovi來處理。\n\n\n10.4.1 實作線性迴歸模型\n以下是用parenthood.csv 資料檔案執行線性迴歸分析的步驟，請打開 jamovi 的 ‘Regression’ - ‘Linear Regression’ 選單 。接著，將 dani.grump 指定為 ‘Dependent Variable’，dani.sleep 輸入到 ‘Covariates’ 對話框。報表介面將出現如 Figure 10.13 的結果，結果顯示截距 \\(\\hat{b}_0 = 125.96\\) 和斜率 \\(\\hat{b}_1 = -8.94\\)。換言之， Figure 10.11 的最適合迴歸線的公式為：\n\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\nFigure 10.13: jamovi的線性迴歸分析示範畫面。\n\n\n\n\n\n\n10.4.2 解讀線性迴歸模型參數估計\n最後要知道的是如何解釋這些係數。讓我們從 \\(\\hat{b}_1\\) 開始，也就是斜率。回想一下斜率的定義，\\(\\hat{b}_1=-8.94\\) 代表將 \\(X_i\\) 增加 1， \\(Y_i\\) 就會減少 8.94。換言之，多睡一個小時的話，我的心情就會改善，我的沮喪程度就會降低 8.94 。那麼截距呢？由於 \\(\\hat{b}_0\\) 代表「當 \\(X_i\\) 為 0 時 \\(Y_i\\) 的期望值」，這就是說如果我一夜都沒睡 (\\(X_i = 0\\))，我的沮喪程度就會瘋狂升高到不敢想像的數值 (\\(Y_i = 125.96\\))。我想我最好避免這種狀況。\n\n\n\n還有關於等級資料(Rank data)的迴歸分析，請參考線性模型的學習取向的相關與線性迴歸這一節。\n\n\n\n\n以下 Section 10.5 、 Section 10.10 以及 Section 10.11 等三個小單元，是屬於傳統高等統計課程的範圍，其他單元在多數教科書被劃分為基礎統計的範圍。不過接下來的單元裡原作者都是混合一元迴歸與多元迴歸的示範案例，譯者將在屬於多元迴歸的小單元開頭明示譯註，提供使用這本電子書學習的學生與教學的老師，根據自身的學習目標決定如何運用該節內容。"
  },
  {
    "objectID": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-02",
    "href": "11-Comparing-two-means.html#單一樣本z檢定11-translatino-02",
    "title": "11  比較單一與兩組平均值",
    "section": "11.1 單一樣本z檢定3",
    "text": "11.1 單一樣本z檢定3\n在本節中，我將介紹統計學中最無用的測試之一：z 檢定。認真地說，這種測試在現實生活中幾乎從不使用。它唯一的真正用途是，在教授統計學時，它是通往 t 檢定的一個非常方便的墊腳石，而 t 檢定可能是統計學中最（過度）使用的工具。\n\n\n11.1.1 使用z檢定前的注意事項\n為了介紹 z 檢定背後的概念，讓我們舉一個簡單的例子。我的一位朋友 Zeppo 老師按曲線給他的入門統計學班級打分。假設他班級的平均分數是 \\(67.5\\)，標準差是 \\(9.5\\)。他有很多學生，其中有 20 個學生還修了心理學課程。出於好奇心，我想知道這些心理學生的成績是否傾向於與其他人獲得相同的成績（即平均分數為 \\(67.5\\)），還是他們的成績往高或往低得分？他給我發了一個 zeppo.csv 文件，我用它來查看這些學生的成績，並在 jamovi 試算表視圖中計算了 “探索” - “描述性統計” 中的平均值。4平均值為 \\(72.3\\)。\n50 60 60 64 66 66 67 69 70 74 76 76 77 79 79 79 81 82 82 89\n嗯，也許心理學生的成績比正常情況高一些。樣本平均值 \\(\\bar{X}=72.3\\) 比假設的母群平均值 \\(\\mu=67.5\\) 要高得多，但是，另一方面，樣本大小 \\(N=20\\) 並不是很大。也許這只是純粹的巧合。\n為了回答這個問題，有助於能夠寫下我所知道的。首先，我知道樣本平均值為 \\(\\bar{X}=72.3\\)。如果我願意假設心理學生的標準差與班上其他學生的標準差相同，那麼我可以說母群標準差為 \\(\\sigma=9.5\\)。我還假設由於 Zeppo 博士是按曲線給分，心理學生的成績服從正態分布。\n接下來，要明確我希望從資料中學到什麼。在這種情況下，我的研究假設與心理學生的成績母群平均值 \\(\\mu\\) 相關，而這個值是未知的。具體而言，我想知道 \\(\\mu=67.5\\) 是否成立。鑒於這是我所知道的，我們能否設計一個假設檢定來解決我們的問題？資料以及它們被認為來自的假設分佈顯示在 圖 11.1 中。不是非常明顯什麼是正確的答案，對嗎？為此，我們需要一些統計學知識。\n\n\n\n\n\n\n圖 11.1: 心理學生的成績（柱形圖）應該是由理論分佈（實線）生成的。\n\n\n\n\n\n\n11.1.2 建立z檢定的假設\n構建假設檢定的第一步是明確虛無假設和對立假設是什麼。這不太難做到。我們的虛無假設 \\(H_0\\) 是，心理學生的成績母群平均值 \\(\\mu\\) 為 \\(67.5\\%\\)，而我們的對立假設是母群平均值不是 \\(67.5\\%\\)。如果我們用數學符號表示，這些假設就變成了：\n\\[ H_0:\\mu= 67.5 \\] \\[ H_1:\\mu \\neq 67.5 \\]\n不過老實說，這種表示法對我們理解問題沒有太大幫助，它只是一種簡潔的寫下我們試圖從資料中學到什麼的方法。我們測試的虛無假設 \\(H_0\\) 和對立假設 \\(H_1\\) 都在 圖 11.2 中有圖示。除了提供這些假設外，上面概述的情況還為我們提供了相當多的有用的背景知識。具體而言，有兩個特殊的信息可以添加：\n\n心理學成績服從正態分布。\n這些分數的真實標準差 \\(\\sigma\\) 已知為 9.5。\n\n暫時，我們將表現得好像這些是絕對可信的事實。在現實生活中，這種絕對可信的背景知識是不存在的，因此，如果我們想依賴這些事實，我們只能假設這些東西是真實的。但是，由於這些假設可能成立或不成立，我們可能需要檢查它們。不過，現在我們還是保持簡單。\n\n\n\n\n\n\n圖 11.2: 單一樣本 \\(z\\) 檢定（雙側版本）所假設的虛無假設和對立假設的圖形說明。虛無假設和對立假設都假設母群分布是正態分布，並且進一步假設母群標準差已知（固定在某個值 \\(\\sigma_0\\)）。虛無假設（左）是母群平均值 \\(\\mu\\) 等於某個指定值 \\(\\mu_0\\)。對立假設（右）是母群平均值與此值不同，即 \\(\\mu \\neq \\mu_0\\)。\n\n\n\n\n下一步是找出一個好的診斷檢定統計量，這樣可以幫助我們區分 \\(H_0\\) 和 \\(H_1\\)。考慮到這些假設都涉及母群平均值 \\(\\mu\\)，你會相當有信心樣本平均值 \\(\\bar{X}\\) 是一個很有用的起點。我們可以計算樣本平均值 \\(\\bar{X}\\) 與虛無假設預測的母群平均值之間的差異。在我們的例子中，這意味著我們計算 \\(\\bar{X} - 67.5\\)。更一般地，如果讓 \\(\\mu_0\\) 指虛無假設聲稱為我們的母群平均值的值，那麼我們想要計算\n\\[\\bar{X}-\\mu_0\\]\n如果這個量等於或非常接近於0，虛無假設看起來是可接受的。如果這個量遠離0，那麼虛無假設似乎不太可能是有價值的。但是，它離0有多遠才能拒絕 H0 呢？\n為了弄清楚這一點，我們需要再狡猾一點，並且我們需要依賴我之前提到的那兩個背景知識，即原始資料服從正態分布，而且我們知道母群標準差 \\(\\sigma\\) 的值。如果虛無假設實際上是真的，真正的平均值是 \\(\\mu_0\\)，那麼這些事實一起意味著我們知道資料的完整母群分布：平均值為 \\(\\mu_0\\)，標準差為 \\(\\sigma\\) 的正態分布。5\n好的，如果這是真的，那麼我們可以關於 \\(\\bar{X}\\) 的分布說些什麼呢？嗯，正如我們之前討論過的（見 小單元 8.3.3），平均數 \\(\\bar{X}\\) 的取樣分佈也是正態分布，並且具有平均值 \\(\\mu\\)。但是這個取樣分佈的標準差 \\(\\\\{se(\\bar{X})\\\\}\\)，也被稱為平均值的標準誤差，是6\n\\[se(\\bar{X}=\\frac{\\sigma}{\\sqrt{N}})\\]\n現在來了解這個技巧。我們可以將樣本平均數 \\(\\bar{X}\\) 轉換成標準分數（參見 小單元 4.5）。這通常被寫成 z，但現在我將其稱為 \\(z_{\\bar{X}}\\)。使用這種擴展符號的原因是為了幫助您記住，我們正在計算樣本平均值的標準化版本，而不是單個觀察值的標準化版本，後者是通常指的 z 分數）。當我們這樣做時，樣本平均值的 z 分數為\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{SE(\\bar{X})}\\]\n也可寫成\n\\[z_{\\bar{X}}=\\frac{\\bar{X}-\\mu_0}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n這個 z 分數就是我們的檢定統計量。使用這個作為我們的檢定統計量的好處是，像所有 z 分數一樣，它具有標準正態分布：7\n換句話說，無論原始資料所處的比例尺是什麼，z 統計量本身的解釋始終相同：它等於觀察到的樣本均值 \\(\\bar{X}\\) 與零假設所預測的母群均值 \\(\\mu_0\\) 相隔的標準誤數。更好的是，無論原始分數的母群參數是什麼，z 檢定的 5% 臨界區域始終相同，如 圖 11.3 所示。而這意味著，在人們手動進行所有統計計算的時代，某人可以發表像 表 11.1 這樣的表格。這反過來又意味著研究人員可以手動計算他們的 z 統計量，然後在教科書中查找臨界值。\\[z_{\\bar{X}} \\sim Normal(0,1) \\]\n\n\n\n\n\n\n表 11.1: 各種顯著水準的臨界值\n\n\n\ncritical z value\n\n\ndesired \\(\\alpha\\) level\ntwo-sided test\none-sided test\n\n\n.1\n1.644854\n1.281552\n\n\n.05\n1.959964\n1.644854\n\n\n.01\n2.575829\n2.326348\n\n\n.001\n3.290527\n3.090232\n\n\n\n\n\n\n\n\n\n\n11.1.3 手作z檢定\n現在，正如我之前提到的，z-test在實際應用中幾乎從不使用。這個測試在實際生活中如此罕見，以至於jamovi的基本安裝不包含內置功能。然而，這個測試是如此地簡單，以至於手動進行這個測試非常容易。讓我們回到Dr Zeppo班級的資料。在載入成績資料後，我需要做的第一件事是計算樣本均值，我已經做到了(\\(72.3\\))。我們已經有了已知的母群標準差(\\(\\sigma = 9.5\\))，零假設所指定的母群平均值(\\(\\mu_0 = 67.5\\))的值，以及樣本大小(\\(N=20\\))。\n\n\n\n\n\n\n圖 11.3: 雙尾 z-檢定 (面板(a)) 和單尾 z-檢定 (面板(b)) 的拒絕區域\n\n\n\n\n接下來，讓我們計算（真實）標準誤（可輕鬆用計算機完成）：\n\n\\[\n\\begin{split}\nsem.true & = \\frac{sd.true}{\\sqrt{N}} \\\\\\\\\n& = \\frac{9.5}{\\sqrt{20}} \\\\\\\\\n& = 2.124265\n\\end{split}\n\\]\n最後，我們計算我們的z分數：\n\n\\[\n\\begin{split}\nz.score & = \\frac{sample.mean - mu.null}{sem.true} \\\\\\\\\n& = \\frac{ (72.3 - 67.5)}{ 2.124265} \\\\\\\\\n& = 2.259606\n\\end{split}\n\\]\n此時，我們會傳統地在臨界值表中查詢 \\(2.26\\) 的值。我們原來的假設是雙邊的（我們對心理學生在統計學上表現得比其他學生好或差沒有任何理論基礎），因此我們的假設檢驗也是雙邊的（或者是兩尾的）。從我先前顯示的小表中，我們可以看到 \\(2.26\\) 大於需要在 \\(\\alpha = .05\\) 的顯著性水平下具有顯著性的臨界值 \\(1.96\\)，但小於需要在 \\(\\alpha = .01\\) 的水平下具有顯著性的值 \\(2.58\\)。因此，我們可以得出結論，我們可以這樣寫：\n\n在心理學學生的樣本中，平均成績為 \\(73.2\\)，假定真正的人口標準差為 \\(9.5\\)，我們可以得出結論，心理學學生在統計學上的得分與班級平均分有顯著差異（\\(z = 2.26, N = 20, p&lt;.05\\)）。\n\n\n\n\n\n11.1.4 z檢定的適用條件\n如前所述，所有統計檢驗都有其前提假設，有些假設是合理的，有些則不是。我剛剛介紹的「單樣本 z 檢定」也有三個基本的前提假設，分別為：\n\n常態性。通常所描述的z-test假設真正的母群分布是正態的。8這通常是一個相當合理的假設，如果我們感到擔憂，也是我們可以檢查的假設（請參見檢查樣本的正態性一節）。\n獨立性。測試的第二個假設是資料集中的觀察結果彼此不相關或以某種有趣的方式相關。這在統計上不是那麼容易檢查，而是有賴於良好的實驗設計。違反此假設的一個明顯（且愚蠢）的例子是將同一個觀察結果在資料文件中“複製”多次，以便您最終獲得只有一個真正觀察結果的龐大“樣本大小”。更現實的是，您必須問自己，是否真的可以想像每個觀察結果都是從您感興趣的人口中完全隨機抽樣得到的。在實踐中，此假設永遠不會被滿足，但我們會盡力設計研究來最小化相關資料的問題。\n已知標準差假設。z檢驗的第三個假設是，研究人員知道母群的真實標準差。這簡直太傻了。在沒有真正的世界資料分析問題中，你知道母群的標準差\\(\\sigma\\)，但對平均值\\(\\mu\\)卻一無所知。換句話說，這個假設永遠是錯的。\n\n鑒於假設 \\(\\alpha\\) 已知的荒謬，我們試著不使用它。這將我們帶離了 z-test 這個枯燥領域，走進了有獨角獸、仙女和小矮人的神奇王國，那就是 t-test！"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的適用條件",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的適用條件",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.6 單因子變異數分析的適用條件",
    "text": "12.6 單因子變異數分析的適用條件\n像任何統計檢驗一樣，變異數分析依賴於關於數據（特別是殘差）的一些假設。您需要了解三個關鍵假設：正態性、方差同質性和獨立性。\n[額外的技術細節 [^13-comparing-several-means-one-way-anova-12]]\n[^13-comparing-several-means-one-way-anova-12]：如果您記得回到[一個實例]，我希望您至少瀏覽了一遍，即使您沒有讀完整篇文章，我以這種方式描述了支撐ANOVA的統計模型：\\[H_0:Y_{ik}=\\mu + \\epsilon_{ik}\\] \\[H_1:Y_{ik}=\\mu_k + \\epsilon_{ik}\\]在這些等式中，\\(\\mu\\)指的是對所有組別都相同的單個總群體均值，µk是第k個組的群體均值。到目前為止，我們主要關心的是我們的數據是最好用單個總均值（零假設）來描述，還是用不同的特定組均值（替代假設）來描述。當然，這是有道理的，因為這實際上是重要的研究問題！然而，我們所有的檢驗過程都是在一個關於殘差 \\(\\epsilon_{ik}\\) 的具體假設下進行的，即：\\[\\epsilon_{ik} \\sim Normal(0,\\sigma^2)\\]如果沒有這部分，所有的數學都不能正常工作。或者，確切地說，您仍然可以進行所有計算，最終得到一個F統計量，但是您無法保證這個F統計量實際上衡量了您認為它衡量的內容，因此您可能基於F檢驗得出的任何結論都可能是錯誤的。\n那麼，我們如何檢查對殘差的假設是否準確呢？嗯，正如我上面所指出的，這個陳述中隱含了三個不同的主張，我們將分別考慮它們。\n\n方差同質性。注意到我們只有一個群體標準差的值（即，\\(\\sigma\\)），而不是讓每個組都有它自己的值（即，\\(\\sigma_k\\)）。這被稱為方差同質性（有時稱為等方差性）假設。ANOVA假定所有組的群體標準差相同。我們將在[檢查方差同質性假設]部分詳細論述這一點。\n正態性。假定殘差呈正態分布。正如我們在 Section 11.9 中看到的，我們可以通過查看QQ圖（或運行Shapiro-Wilk檢驗）來評估這一點。我將在[檢查正態性假設]部分中更多地討論這個問題。\n獨立性。獨立性假設有點棘手。它基本上的意思是，了解一個殘差對於了解任何其他殘差都沒有幫助。所有的 \\(\\epsilon_{ik}\\) 值都被假定是在不考慮或與其他任何值無關的情況下生成的。對於這一點，沒有顯而易見或簡單的檢驗方法，但有些情況是明顯違反這一假設的。例如，如果您有一個重複測量設計，每個參與者在研究中出現在多個條件下，那麼獨立性就不成立了。在這種情況下，某些觀察之間存在特殊關係，即對應於同一個人的觀察！當這種情況發生時，您需要使用類似[重複測量單因子ANOVA]的方法。\n\n\n\n12.6.1 同質性檢核\n\n要進行方差的初步檢驗，就像乘坐划艇出海，看看海面條件是否足夠平靜，讓一艘大型遊輪離港！\n– 喬治·博克斯 (Box, 1953)\n\n俗話說，殺貓有很多方法，檢驗方差同質性假設也有很多方法（不過出於某種原因，沒有人把它變成一句俗話）。在文獻中，我見過的最常用的檢驗方法是Levene檢驗(Levene, 1960)，以及與之密切相關的Brown-Forsythe檢驗(Brown & Forsythe, 1974)。\n無論您是進行標準Levene檢驗還是Brown-Forsythe檢驗，檢驗統計量（有時表示為\\(F\\)，但也有時表示為\\(W\\)），都是按照計算常規ANOVA中的F-統計量的方式，只是使用\\(Z_{ik}\\)而不是\\(Y_{ik}\\)。有了這個思路，我們可以繼續看看如何在jamovi中運行檢驗。\n[額外的技術細節[^13-comparing-several-means-one-way-anova-13]]\n[^13-comparing-several-means-one-way-anova-13]：Levene檢驗非常簡單。假設我們有結果變項\\(Y_{ik}\\)。我們所要做的就是定義一個新變項，我將其稱為\\(Z_{ik}\\)，表示與組均值的絕對偏差：\\[Z_{ik}=Y_{ik}-\\bar{Y}_{k}\\]好吧，這對我們有什麼好處呢？那麼，讓我們花一點時間來思考一下\\(Z_{ik}\\)到底是什麼以及我們要檢驗什麼。\\(Z_{ik}\\)的值是度量第\\(i\\)次觀測在第\\(k\\)個組中與其組平均值的偏差程度。我們的零假設是所有組的方差都相同，即所有組平均值的總偏差相同！因此，Levene檢驗中的零假設是所有組的\\(Z\\)的母體平均值相同。嗯。那麼我們現在需要的是一個統計檢驗來檢驗所有組均值相同的零假設。我們在哪裡見過這個檢驗？哦對了，這就是ANOVA，所以Levene檢驗所做的就是對新變項\\(Z_{ik}\\)進行ANOVA。Brown-Forsythe檢驗呢？它有做什麼特別不同的事情嗎？不，與Levene檢驗唯一的不同是它以稍微不同的方式構建轉換變項Z，使用組中位數的偏差而不是組平均值的偏差。也就是說，對於Brown-Forsythe檢驗：\\[Z_{ik}=Y_{ik}-median_k(Y)\\]其中，\\(median_k(Y)\\)是第k組的中位數。\n\n\n\n12.6.2 jamovi的Levene檢定\n好的，那麼我們該如何進行Levene檢驗呢？其實很簡單 - 在ANOVA的”假設檢查”選項下，只需點擊”變異數同質性檢驗”複選框。如果我們查看 Figure 12.5 中的輸出，我們可以看到檢驗結果並無顯著差異（\\(F_{2,15} = 1.45, p = .266\\)），所以變異數同質性假設看起來沒有問題。然而，外表可能會讓人受騙！如果您的樣本量相當大，那麼即使變異數同質性假設沒有被違反到影響ANOVA的穩健性，Levene檢驗也可能顯示出顯著效應（即p < .05）。這正是George Box在上面引述中所指出的觀點。同樣地，如果您的樣本量相當小，那麼變異數同質性假設可能不被滿足，而Levene檢驗可能不顯著（即p > .05）。這意味著，在對假設是否被滿足進行任何統計檢驗的同時，您應該總是繪製每個分組/類別的均值周圍的標準差……只是為了看看它們是否看起來相當相似（即變異數同質性）或不相似。\n\n\n\n\n\n\nFigure 12.5: jamovi中單因素ANOVA的Levene檢驗輸出\n\n\n\n\n\n\n12.6.3 校正異質性的分析結果\n在我們的示例中，變異數同質性假設被證明是相當可靠的：Levene檢驗結果並無顯著差異（儘管我們還應該查看標準差的圖形），因此我們可能不需要擔心。然而，在現實生活中，我們並非總是如此幸運。當變異數同質性假設被違反時，我們該如何拯救我們的ANOVA呢？如果您回想一下我們對t檢驗的討論，我們之前遇到過這個問題。Student t檢驗假設等方差，所以解決方法是使用不需要等方差假設的Welch t檢驗。實際上， Welch (1951) 還展示了我們如何解決ANOVA的這個問題（Welch單因素檢驗）。它在jamovi中使用One-Way ANOVA分析實現。這是一種專為單因素ANOVA設計的分析方法，要在我們的示例中執行Welch單因素ANOVA，我們將按照之前的方式重新運行分析，但這次使用jamovi的ANOVA - One Way ANOVA分析命令，並選擇Welch檢驗的選項（參見 Figure 12.6 ）。為了理解這裡發生了什麼，讓我們將這些數字與我們在[最初在jamovi中運行ANOVA]時得到的數字進行比較。為了省去您回顧的麻煩，上次我們得到的是：\\(F(2, 15) = 18.611, p = .00009\\)，這也顯示為 Figure 12.6 中One-Way ANOVA的Fisher檢驗。\n\n\n\n\n\n\nFigure 12.6: Welch檢驗作為jamovi中One Way ANOVA分析的一部分\n\n\n\n\n好的，最初我們的ANOVA結果是\\(F(2, 15) = 18.6\\)，而Welch單因素檢驗給出的是\\(F(2, 9.49) = 26.32\\)。換句話說，Welch檢驗將組內自由度從15降低到了9.49，而F值從18.6上升到了26.32。\n\n\n\n12.6.4 常態性檢核\n檢驗正態性假設相對簡單。我們在 Section 11.9 中介紹了大部分你需要了解的內容。我們真正需要做的只是繪製一個QQ圖，並在可行的情況下，運行Shapiro-Wilk檢驗。QQ圖顯示在 Figure 12.7 ，對我來說看起來相當正常。如果Shapiro-Wilk檢驗不顯著（即\\(p > .05\\)），那麼這表明正態性假設沒有被違反。然而，與Levene檢驗一樣，如果樣本量很大，那麼顯著的Shapiro-Wilk檢驗實際上可能是偽陽性，也就是說，正態性假設在實質上沒有對分析造成任何問題。同樣地，非常小的樣本量可能會產生偽陰性。這就是為什麼視覺檢查QQ圖很重要。\n\n\n\n\n\n\nFigure 12.7: jamovi中One Way ANOVA分析的QQ圖\n\n\n\n\n除了檢查QQ圖中是否有偏離正態性的情況外，我們的數據的Shapiro-Wilk檢驗確實顯示出非顯著效應，p = 0.6053（見 Figure 12.6 ）。因此，這支持了QQ圖的評估；兩個檢查都沒有發現正態性被違反的跡象。\n\n\n\n12.6.5 排除非常態性的分析結果\n現在我們已經了解了如何檢查正態性，我們自然會問可以採取哪些措施來解決正態性的違反。在單因素ANOVA的背景下，最簡單的解決方案可能是轉向非參數檢驗（即不依賴於任何特定的分佈假設的檢驗）。在 Chapter 11 中，我們之前已經介紹過非參數檢驗。當你只有兩個組別時，Mann-Whitney或Wilcoxon檢驗可以提供你所需的非參數替代方法。當你有三個或更多組別時，你可以使用Kruskal-Wallis秩和檢驗(Kruskal & Wallis, 1952)。接下來我們將講解這個檢驗。\n\n\n\n12.6.6 Kruskal-Wallis檢定的邏輯\nKruskal-Wallis檢驗在某些方面與ANOVA驚人地相似。在ANOVA中，我們從\\(Y_{ik}\\)開始，對於第k個組中的第i個人，這是結果變項的值。對於Kruskal-Wallis檢驗，我們要做的是對所有的\\(Y_{ik}\\)值進行排序，並對排名數據進行分析。10\n\n\n\n12.6.7 更多分析細節\n上一節的描述說明了Kruskal-Wallis檢驗背後的邏輯。從概念上講，這是考慮測試如何工作的正確方法。[^13-comparing-several-means-one-way-anova-15]\n[^13-comparing-several-means-one-way-anova-15]：然而，從純粹的數學角度來看，這是不必要的複雜。我不會向您展示推導，但您可以使用一些代數技巧\\(^b\\)來顯示K的方程式可以是\\[K=\\frac{12}{N(N-1)}\\sum_k N_k \\bar{R}_k^2 -3(N+1)\\] 最後一個方程式有時給出了K的值。這比我在上一節中描述的版本要容易得多，但問題是對實際人類完全沒有意義。將K視為基於排名的ANOVA類比可能是最好的方式。但請記住，計算出來的檢驗統計量與我們最初用於ANOVA的統計量有很大不同。— \\(b\\)就是一些數學運算術語。\n但等等，還有更多！天啊，為什麼總是有更多呢？到目前為止，我講的故事實際上只在原始數據中沒有相同數值的情況下才成立。也就是說，如果沒有兩個觀測值具有完全相同的值。如果有相同的值，那麼我們必須引入一個校正因子來進行這些計算。在這一點上，我假設即使是最勤奮的讀者也已經不再關心（或者至少形成了繫結校正因子不需要他們立即關注的看法）。因此，我將非常快速地告訴您如何計算它，並省略為什麼以這種方式進行的繁瑣細節。假設我們為原始數據構建一個頻率表，讓fj表示具有第j個唯一值的觀測值的數量。這聽起來可能有點抽象，因此我們將從clinicaltrials.csv數據集中的mood.gain頻率表（ Table 12.11 ）給出一個具體的例子。\n\n\n\n\n\n\nTable 12.11:  數據中clinicaltrials.csv心情增益的次數表 \n\n0.10.20.30.40.50.60.80.91.11.21.31.41.71.8\n\n11211211112211\n\n\n\n\n\n觀察此表，請注意頻率表中的第三個條目值為2。由於這對應於心情增益為0.3，因此此表告訴我們有兩個人的心情增加了0.3。[^13-comparing-several-means-one-way-anova-16]\n[^13-comparing-several-means-one-way-anova-16]：更重要的是，在我上面介紹的數學表示法中，這告訴我們\\(f_3 = 2\\)。耶。那麼，現在我們知道了這一點，繫結校正因子（TCF）是：\\[TCF=1-\\frac{\\sum_j f_j^3 - f_j}{N^3 - N}\\]通過將K值除以這個數量，可以得到Kruskal-Wallis統計量的繫結校正值。這是jamovi計算的繫結校正版本。\n因此，jamovi使用繫結校正因子來計算繫結校正的Kruskall-Wallis統計量。最後，我們實際上已經完成了Kruskal-Wallis檢驗的理論。我確信你們都對我治愈了你們在意識到你們不知道如何計算Kruskal-Wallis檢驗的繫結校正因子時自然產生的存在焦慮感到非常寬慰。對吧？\n\n\n\n\n12.6.8 使用jamovi完成Kruskal-Wallis檢定\n儘管我們在努力理解Kruskal Wallis檢驗實際上做了什麼方面經歷了恐懼，但事實證明，進行該檢驗相當無痛，因為jamovi在ANOVA分析集中有一個名為「非參數」-「單因子ANOVA（Kruskall-Wallis）」的分析。大多數時候，你將擁有像clinicaltrial.csv這樣的數據集，其中包含你的結果變項mood.gain和一個分組變項drug。如果是這樣，你可以直接在jamovi中運行分析。這給我們提供了一個Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\)，如 Figure 12.8 所示。\n\n\n\n\n\n\nFigure 12.8: jamovi中的Kruskall-Wallis單因子非參數ANOVA"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#sec-oneway-repeated-measure",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#sec-oneway-repeated-measure",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.7 單因子重覆量數變異數分析",
    "text": "12.7 單因子重覆量數變異數分析\n單因子重覆量數變數分析檢驗是一種統計方法，用於檢驗三個或更多組之間的顯著差異，其中每個組都使用相同的參與者（或者每個參與者與其他實驗組的參與者密切匹配）。因此，每個實驗組中應該始終具有相等數量的分數（數據點）。這種類型的設計和分析也可以稱為「相關ANOVA」或「內部主題ANOVA」。\n重覆量數變數分析的邏輯與獨立ANOVA（有時稱為「間題」ANOVA）非常相似。您可能還記得，我們之前顯示在一個間題ANOVA總變異性可以分為組間變異性（\\(SS_b\\)）和組內變異性（\\(SS_w\\)），在將每個變異性除以相應的自由度後得到MSb和MSw（見表13.1），F比值計算為：\n\\[F=\\frac{MS_b}{MS_w}\\]\n在重覆量數變數分析中，F比值的計算方式類似，但是在獨立ANOVA中，組內變異性（\\(SS_w\\)）被用作\\(MS_w\\)的分母，而在重覆量數變數分析中，\\(SS_w\\)被劃分為兩部分。由於我們在每個組中都使用相同的受試者，因此可以從組內變異性中移除受試者間個別差異（稱為SSsubjects）的變異性。我們不會深入討論這是如何實現的，但本質上，每個受試者都成為名為受試者的因子的一個水平。然後以與任何間題因子相同的方式計算此內部受試者因子中的變異性。然後我們可以將SSsubjects從\\(SS_w\\)中減去，以提供一個較小的SSerror項：\n\\[\\text{獨立ANOVA: } SS_{error} = SS_w\\] \\[\\text{重覆量數變數分析: } SS_{error} = SS_w - SS_{subjects}\\] 這個\\(SS_{error}\\)項的變化通常會導致統計檢驗更加強大，但這確實取決於\\(SS_{error}\\)的減少是否超過了誤差項自由度的減少（因為自由度從\\((n - k)\\)11變為\\((n - 1)(k - 1)\\)（請記住，獨立ANOVA設計中的受試者更多）。\n\n\n\n12.7.1 jamovi的重覆量數變異數分析\n首先，我們需要一些數據。 Geschwind (1972) 表示，患者在中風後語言缺陷的確切性質可以用來診斷已受損的大腦特定區域。一位研究人員關心的是確定六位患有Broca失語症（中風後常見的語言缺陷）的患者所經歷的具體交流困難（ Table 12.12 ）。\n\n\n\n\n\nTable 12.12:  中風患者單詞識別作業分數 \n\nParticipantSpeechConceptualSyntax\n\n1876\n\n2786\n\n3953\n\n4545\n\n5662\n\n6874\n\n\n\n\n\n患者需要完成三個單詞識別任務。在第一個（言語生成）任務中，患者需要重複研究者大聲朗讀的單詞。在第二個（概念性）任務中，旨在測試單詞理解能力，患者需要將一系列圖片與其正確名稱匹配。在第三個（語法）任務中，旨在測試正確單詞順序的知識，要求患者對語法不正確的句子進行重新排序。每位患者都完成了所有三個任務。患者嘗試任務的順序在參與者之間進行了平衡。每個任務包括一系列10次嘗試。每位患者成功完成的嘗試次數如 Table 12.11 所示。將這些數據輸入jamovi以進行分析（或者使用捷徑加載broca.csv文件）。\n要在jamovi中執行一個單因素相關ANOVA，打開一個單因素重覆量數變數分析對話框，如 Figure 12.9 中所示，通過ANOVA - Repeated Measures ANOVA進行。\n\n\n\n\n\n\nFigure 12.9: jamovi中的重覆量數變數分析對話框\n\n\n\n\n然後：\n\n輸入一個重複測量因子名稱。這應該是您選擇的標籤，用於描述所有參與者重複的條件。例如，要描述所有參與者完成的語音、概念和語法任務，一個合適的標籤是“任務”。請注意，這個新的因子名稱代表了分析中的自變項。\n在重複測量因子文本框中添加第三個級別，因為有三個級別代表三個任務：語音、概念和語法。相應地更改級別的標籤。\n然後將每個級別變項移動到重複測量單元文本框中。\n最後，在“假設檢查”選項下，選中“球形性檢查”文本框。\n\njamovi輸出一個單因素重覆量數變數分析，如 Figure 12.10 至 Figure 12.13 所示。我們應該首先查看的是Mauchly球形性檢驗，該檢驗測試各條件之間的差異方差是否相等（意味著研究條件之間的差異得分的分佈大致相同）。在 Figure 12.10 中，Mauchly檢驗的顯著性水平為\\(p = .720\\)。如果Mauchly檢驗的結果不顯著（即p > .05，正如此分析中的情況），那麼我們有理由得出差異的方差並無顯著差異（即它們大致相等，可以假定球形性。）。\n\n\n\n\n\n\n\nFigure 12.10: 單因子重覆量數變數分析輸出 - Mauchly球形性檢驗\n\n\n\n\n如果另一方面，Mauchly檢驗顯著（p < .05），那麼我們將得出差異方差之間存在顯著差異，並且未滿足球形性要求。在這種情況下，我們應該對單因素相關ANOVA分析中獲得的F值進行修正：\n\n如果”球形性檢驗”表中的Greenhouse-Geisser值> .75，那麼您應該使用Huynh-Feldt修正\n但如果Greenhouse-Geisser值< .75，那麼您應該使用Greenhouse-Geisser修正。\n\n這兩個修正過的F值都可以在“假設檢查”選項下的球形性修正復選框中指定，修正過的F值將顯示在結果表中，如 Figure 12.11 所示。\n\n\n\n\n\n\n\nFigure 12.11: 單因素重覆量數變數分析輸出 - 內部受試者效應檢驗\n\n\n\n\n在我們的分析中，我們發現Mauchly的球形性檢驗的顯著性為p = .720（即p > 0.05）。因此，這意味著我們可以假設已滿足球形性要求，因此無需對F值進行修正。因此，我們可以使用’無’球形性修正輸出值用於重複測量”任務”：\\(F = 6.93\\)，\\(df = 2\\)，\\(p = .013\\)，我們可以得出結論，語言任務中成功完成的測試次數確實會根據任務是語音、理解還是語法為基礎而顯著不同（\\(F(2, 10) = 6.93\\)，\\(p = .013\\)）。\n在jamovi中，與獨立ANOVA相同，也可以為重覆量數變數分析指定事後檢驗。結果顯示在 Figure 12.12 。這些表明語音和語法之間存在顯著差異，但其他級別之間沒有差異。\n\n\n\n\n\n\nFigure 12.12: 重覆量數變數分析中jamovi的事後檢驗\n\n\n\n\n描述性統計（邊際均值）可以用於幫助解釋結果，在jamovi輸出中生成，如 Figure 12.13 。通過比較參與者成功完成試驗的平均次數，可以看出布洛卡失語症患者在語音產生（平均= 7.17）和語言理解（平均= 6.17）任務上表現相對較好。然而，他們在語法任務上的表現明顯較差（平均= 4.33），事後檢驗中語音和語法任務表現之間存在顯著差異。\n\n\n\n\n\n\nFigure 12.13: 單因子重覆量數變數分析輸出-描述性統計"
  },
  {
    "objectID": "12-Comparing-several-means-one-way-ANOVA.html#獨立樣本變異數分析示範資料",
    "href": "12-Comparing-several-means-one-way-ANOVA.html#獨立樣本變異數分析示範資料",
    "title": "12  比較多組平均值(單因子變異數分析)",
    "section": "12.1 獨立樣本變異數分析示範資料",
    "text": "12.1 獨立樣本變異數分析示範資料\n假想你協助執行一件臨床試驗，研究一種名為Joyzepam的新型抗憂鬱藥物效用。為了公平地測試這種新藥的效用，你要分別測試包括新藥的三種藥物。另外兩種的其中一種是安慰劑，另一種是已經上市，名為Anxifree的抗憂鬱/抗焦慮藥物。你一開始招募了18名患有中度至重度抑鬱症的參與者。由於有的參與者不只有服用藥物，也同時接受心理治療，因此所有參與者包括9個正在進行認知行為治療（CBT）的個案和9個未進行任何治療的個案。參與者以雙盲的隨機方式派給藥物，因此每一種藥物分給3位接受CBT的個案和3位無接受治療的個案。每位個案各自使用被分派的藥物3個月後，再由研究者評估每個人的情緒改善狀態，從\\(-5\\)到\\(+5\\)的範圍代表每位個案的情緒改善狀況。現在我們可以載入資料檔 clinicaltrial.csv，看看這種研究設計的內容，檔案包含三個變項，分別是藥物、治療和情緒提升分數。\n以這一章的學習目標來說，我們想了解的是藥物對情緒提升的效用。首先要做的是進行描述性統計並繪製一些圖表。在 Chapter 4 ，同學們已經學到如何使用完成描述統計，就像 Figure 12.1 顯示的圖表。\n\n\n\n\n\n\nFigure 12.1: 情緒提升的描述性統計數據，以及按給予的藥物繪製的盒形圖。\n\n\n\n\n如 Figure 12.1 所示，服用Joyzepam的參與者，情緒的改善程度大於服用Anxifree或安慰劑。Anxifree的情緒提升程度大於安慰劑，但是差距沒有像Joyzepam那麼大。在此要回答的問題是，這些藥效的差異是否“真有其事”，還是僅僅是偶然的結果？"
  },
  {
    "objectID": "Prelude-Part-V.html#footnotes",
    "href": "Prelude-Part-V.html#footnotes",
    "title": "線性模型的學習取向",
    "section": "",
    "text": "https://lindeloev.github.io/tests-as-linear/↩︎\nhttps://cosx.org/2019/09/common-tests-as-linear-models/↩︎\n這是許多貝氏統計教材會先介紹通用線性模型的主要原因，因為事前機率與事後機率的機率分佈不需要是一樣的。↩︎"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#footnotes",
    "href": "01-Why-do-we-learn-statistics.html#footnotes",
    "title": "1  為什麼要學習統計",
    "section": "",
    "text": "出自奧登在哈佛大學1946年的畢業典禮致詞中詩作『在哪張豎琴下：時代的反動手冊』。有興趣了解這段致詞的歷史背景，可參考這裡: https://www.harvardmagazine.com/2007/11/a-poets-warning.html↩︎\n其中包含多數科學家缺乏常識的說法。 ↩︎\n譯註~原文”I have a bridge in London I’d like to sell you.”是澳洲俚語，意指對方容易受騙上當，在此改寫為台灣人都知道的典故。↩︎\n譯註~ 請參考維基百科直言三段論，此處的論證格式稱為2-AOO。↩︎\n這能解釋那些網路上令我感到憤怒的訊息，其中95%的成因。↩︎\n這也許可以解釋為什麼物理學作為一門科學，比心理學略微先進。↩︎"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#footnotes",
    "href": "10-Categorical-data-analysis.html#footnotes",
    "title": "10  類別資料分析",
    "section": "",
    "text": "繁體中文報告常直接稱呼“卡方”。↩︎\n向量是由相同測量尺度的資料值所構成的序列。↩︎\n譯註~原文這裡可能寫錯↩︎\n如果說 k 是指資料的類別數目（在這個研究資料是指撲克牌花色，所以 k = 4），則 \\(\\chi^2\\) 統計值可用這個公式計算：\\[\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i-E_i)^2}{E_i}\\] 如果 \\(\\chi^2\\) 統計值很小，代表觀察次數 Oi 非常接近虛無假設預測的次數 \\(E_i\\) ，因此 \\(\\chi^2\\) 統計值要夠大，才能拒絕虛無假設。↩︎\n如果將適合度統計值的方程式重寫為 k - 1 個獨立觀察值之和，就會得到“正確的”樣本分佈，也就是自由度為 k - 1 的卡方分佈。要談這麼多數學細節就超出了這本入門書的範圍，這裡只給初學的讀者一個適合度統計值與卡方分佈關聯密切的直觀。↩︎\n我必須強調，這是一種過度簡化的解釋。雖然這種解釋適用多數情況，但是偶爾會遇到自由度並非整數的狀況。為了不要讓讀者過於困擾，遇到這樣的狀況時，只要告訴自己“自由度”其實是一種有點混亂的數學概念，初學的讀者通常只會知道簡單的解釋而非完整的解釋。以入門課程來說，簡單的解釋通常是最好的學習材料，但讀者需要知道總有一天這樣的解釋無法幫助你更進一步的學習，如果沒有先做好心理準備，有天看到了 \\(df = 3.4\\) 或類似的狀況就會感到困惑，以為你在這裡學到的是錯誤的觀念，其實是有些複的細節先不做交待。↩︎\n其實樣本量並不是永遠固定的數值。例如，實驗可能是在固定時段裡執行，參與人數取決於實際會有多少人參加。這些因素對於目前的學習目標並不重要。↩︎\n嗯，有點。如何報告統計資料的慣例在不同的學科之間有所不同。我傾向於堅持心理學的做法，因為這就是我的工作。但是，我認為向讀者提供足夠的信息，以便他們可以檢查你的結果的一般原則是相當普遍的。↩︎\n對有些人來說，這個建議可能聽起來有點奇怪，或者至少與編寫技術報告的“通常”建議相衝突。通常，學生們被告知報告的“結果”部分是用來描述資料並報告統計分析，而“討論”部分是提供解釋的。這在某種程度上是對的，但我認為人們經常過於照字面解釋它。我通常的做法是在結果部分提供對資料的快速而簡單的解釋，以便我的讀者理解資料告訴我們什麼。然後，在討論中，我嘗試講述一個關於我的結果如何與其他科學文獻相適應的更大的故事。簡而言之，不要讓“解釋屬於討論”這個建議使你的結果部分變得難以理解。讓讀者明白你的意思更重要。↩︎\n如果你一直在仔細閱讀，並且像我一樣是一個數學苛刻的人，那麼上一節中我寫卡方檢驗的方式可能會讓你有點困擾。你可能會想，用 “\\(\\chi^2(3) = 8.44\\)” 這樣寫感覺有點不對勁。畢竟，是適合度統計數等於8.44，所以我不應該寫 \\(X^2 = 8.44\\) 或者 \\(GOF = 8.44\\) 嗎？這似乎混淆了抽樣分布（即\\(\\chi^2\\)與df=3）與檢驗統計量（即\\(X^2\\)）。可能性是你認為這是錯字，因為\\(\\chi\\)和X看起來非常相似。奇怪的是，這並非如此。寫 \\(\\chi^2\\)(3)= 8.44 本質上是一種高度縮略的寫法，表示“檢驗統計量的抽樣分布為\\(\\chi^2\\)(3)，檢驗統計量的值為8.44”。從某種意義上說，這有點愚蠢。有很多不同的檢驗統計量最終具有卡方抽樣分布。我們用於適合度檢驗的\\(X^2\\)統計量只是眾多統計量中的一種（儘管是最常見的一種）。在一個明智、完全組織的世界裡，我們總是為檢驗統計量和抽樣分布分配一個單獨的名字。這樣，統計塊本身就可以告訴你研究者究竟計算了什麼。有時這確實是可能的。例如，Pearson適合度檢驗中使用的檢驗統計量寫作\\(X^2\\)，但還有一種與G-test\\(^a\\) (Sokal & Rohlf, 1994)關係密切的檢驗稱為G-test，其中檢驗統計量寫作\\(G\\)。事實上，Pearson適合度檢驗和G-test都檢驗相同的虛無假設，抽樣分布完全相同（即，具有\\(k-1\\)自由度的卡方分布）。如果我對卡片資料進行G-test而不是適合度檢驗，那麼我最終會得到一個檢驗統計量\\(G = 8.65\\)，它略有不同於我之前得到的\\(X^2 = 8.44\\)值，並產生稍微小一點的p值\\(p = .034\\)。假設約定俗成地報告檢驗統計量，然後是抽樣分布，然後是p值。如果是這樣的話，這兩種情況將產生不同的統計塊：我原來的結果將寫成\\(X^2 = 8.44\\)，\\(\\chi^2(3)\\)，\\(p = .038\\)，而使用G-test的新版本將寫成\\(G = 8.65\\)，\\(\\chi^2(3)\\)，\\(p = .034\\)。然而，使用縮略報告標準，原始結果寫作\\(\\chi^2(3) = 8.44, p =.038\\)，新結果寫作\\(\\chi^2(3) = 8.65, p = .034\\)，所以實際上不清楚我實際上運行了哪個檢驗。那麼為什麼我們不生活在一個統計塊的內容能唯一確定哪些檢驗被執行的世界呢？根本原因是生活是混亂的。我們（作為統計工具的用戶）希望它整齊、有序並且有組織。我們希望它像產品一樣被設計，但這不是生活的運作方式。統計是一門與其他任何學科一樣的智力學科，因此它是一個大規模的分布式、部分協作和部分競爭的項目，沒有人真正完全了解它。你和我作為資料分析工具的東西並非由統計之神的行為創造的。它們是由很多不同的人發明的，作為學術期刊中的論文發表，由很多其他人實施、糾正和修改，然後由其他人在教科書中向學生解釋。因此，有很多檢驗統計量甚至沒有名字，因此它們只是被賦予與相應抽樣分布相同的名字。稍後我們將看到，任何遵循\\(\\chi^2\\)分布的檢驗統計量通常被稱為“卡方統計量”，遵循\\(t\\)分布的任何統計量被稱為“t統計量”，等等。但是，正如\\(\\chi^2\\)與\\(G\\)的例子所說明的，具有相同抽樣分布的兩個不同事物仍然是不同的。因此，有時候明確表明你實際上運行的檢驗是很好的主意，特別是如果你在做一些不尋常的事情。如果你只說“卡方檢驗”，實際上並不清楚你在談論哪種檢驗。儘管如此，由於兩個最常見的卡方檢驗是適合度檢驗和獨立性檢驗，大多數具有統計培訓的讀者可能都能猜到。儘管如此，這仍然是需要注意的事情。—\\(^a\\) 讓事情變得複雜的是，G-test是一整類被稱為似然比檢驗的檢驗的特例。我在這本書中沒有介紹LRT，但它們確實是值得了解的好東西。↩︎\n技術說明。這個範例資料所設定的各列總次數是固定的（研究者手上只有87個機器人和93個人類的資料），各行總次數則是隨機的（碰巧有28個受測者選擇了小狗）。根據原作者主要引用的統計教科書所使用的專有名詞(Hogg et al., 2005)，應該將適用這類問題的假設檢定方法稱為「卡方同質性檢定」(a chi-square test of homogeneity)，「卡方獨立性檢定」應該是指每行及每列的總次數都是隨機結果的研究問題。這本書之前的版本裡，都是區分這兩種檢定這樣做的。然而事實上，這兩種檢定的程序是相同的，所以這裡將兩種檢定方法視為一種。↩︎\n就嚴格的統計操作來說，\\(E_{ij}\\) 是估計值，所以應該寫成 \\(\\hat{E_{ij}}\\)。但很少有統計教科書這樣寫，這裡也採用相同的寫法。 \\[\\hat{E}_{ij}= \\frac{R_i \\times C_j}{N}\\]↩︎\n現在已經學會如何計算期望次數，根據從適合度檢定學到的策略，定義一個檢定統計值非常容易，其實兩種方法用的統計值幾乎是相同。以具備 r 行和 c 列的列聯表來說，計算 \\(chi^2\\) 統計值的公式是 \\[\\chi^2=\\sum_{i=1}^{r}\\sum_{j=1}^{c} \\frac{(E_{ij}-O_{ij})^2}{E_{ij}}\\] 唯一的差異是獨立性檢定的統計值公式必須用兩個連加符號（\\(\\sum\\) ），表示分別要求行的總和及列的總和。↩︎\n現實中很多人都會害怕的數學推導。\\[\\begin{split} df & = \\text{(觀察值總次數) - (不可變動的資料點次數)} \\\\\\\\ & = (r \\times c) -\n((c-1) + (r - 1)+1) \\\\\\\\ & = (r- 1)(c - 1) \\end{split}\n\\]↩︎\nYates (1934) 提出了一個簡單的校正公式，將適合度統計值重新定義為：\\[\\chi^{2}=\\sum_{i}\\frac{(|E_i-O_i|-0.5)^2}{E_i}\\] 基本上，他將所有公式裡的數值都減了 0.5。↩︎\n\\(\\phi\\) 與 Cramér’s V的數學公式非常簡潔。要計算 \\(\\phi\\)，只需將的 \\(\\chi^2\\)統計值除以樣本大小，然後取平方根：\\[\\phi=\\sqrt{\\frac{\\chi^2}{N}}\\] \\(\\phi\\)的值域只在0（完全無關聯）和 1（完全關聯）之間，但是當列聯表規模大於 \\(2 \\times 2\\) ，就不適合用這個指標計算了。因為規模更大的列聯表，有可能會獲得大於1的數值，這是令人感到相當麻煩的。所以更多人喜歡使用@Cramer1946 提出的\\(V\\)來測量列聯表統計檢定的效量。調整方法非常簡單，如果有一個 r 行和 c 列的列聯表，那麼定義 \\(k = min(r, c)\\) 取兩個值中較小的那個，放到計算 \\(\\phi\\)的公式裡就變成計算Cramér’s V的公式\\[V=\\sqrt{\\frac{\\chi^2}{N(k-1)}}\\]↩︎\n這個例子來自發表在Journal of Irreproducible Results的一則笑話。↩︎\n不必懷疑，費雪精確檢定是根據費雪對 p 值的解釋，而不是尼曼的！請參考 小單元 9.5 。↩︎\n譯註~這是原作者虛構的政黨，現實中無此政黨或團體。詳見維基百科條目。↩︎"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "href": "12-Correlation-and-linear-regression.html#sec-what-is-correlation",
    "title": "12  相關與線性迴歸",
    "section": "12.1 相關",
    "text": "12.1 相關\n這一節要談如何描述資料變項之間的關係，因此會不斷提到變項之間的相關。首先，讓我們看一下列在 表 12.1 的本章示範資料描述統計。\n\n\n12.1.1 示範資料\n\n\n\n\n表 12.1: 相關分析的示範資料資訊，原作者照顧新生兒百日紀錄的描述統計。\n\n\n變項\n最小值\n最大值\n平均值\n中位數\n標準差\n四分位數間距\n\n\n\n\n老爸的沮喪程度\n41.00\n91.00\n63.71\n62.00\n10.05\n14.00\n\n\n老爸睡眠小時數\n4.84\n9.00\n6.97\n7.03\n1.02\n1.45\n\n\n小嬰兒睡眠小時數\n3.25\n12.07\n8.05\n7.95\n2.07\n3.21\n\n\n\n\n\n\n\n\n讓我們從一個與每個新生兒父母都息息相關的主題談起：睡眠。這裡使用的資料集是虛構的，但是來自本人(原作者)的真實經驗：我想知道我那剛出生的兒子的睡眠習慣對我個人的情緒有多大影響。假想我可以非常精確地評估我的沮喪分數，評分從0分（一點都不沮喪）到100分（像一個非常非常沮喪的老頭子），還有我每天都有測量我的沮喪分數、我的睡眠習慣和我兒子的睡眠習慣持續是100天。身為一位數位時代的書呆子，我把資料保存在一個名為parenthood.csv的檔案。匯入jamovi，我們可以看到四個變項：dani.sleep，baby.sleep，dani.grump和day。請注意，當您首次打開這份檔案，jamovi可能無法正確猜測每個變項的資料類型，同學可以自行修正：dani.sleep，baby.sleep，dani.grump和day都可以被指定為連續變項，而ID是一個名義且為整數的變項。2\n接著我會看一下一些基本的描述性統計數據，並且三個我有興趣的變項視覺化，也就是 圖 12.1 展示的直方圖。需要注意的是，不要因為jamovi可以一次計算幾十種不同的統計數據，你就要報告所有數據。如果我要以此結果撰寫報告，我會挑出那些我自己以及我的讀者最感興趣的統計數據，然後將它們放入像 表 12.1 這樣的簡潔的表格裡。3 需要注意的是，當我將數據放入表格時，我給了每個變項一個“高可讀性”的名稱。這是很好的做法。另外，請注意這一百天我都沒有睡飽，這不是好的習慣，不過其他帶過小孩的父母告訴我，這是很正常的事情。\n\n\n\n\n\n\n\n圖 12.1: 原作者照顧新生兒百日紀錄的三個變項直方圖。\n\n\n\n\n\n\n12.1.2 相關的強度與方向\n我們可以繪製散佈圖，讓我們能俯瞰兩個變項之間的相關性。雖 然在理想情況下，我們希望能多看到一些資訊。例如，讓我們比較dani.sleep和dani.grump之間的關係（ fig-fig10-2 ，左）與baby.sleep和dani.grump之間的關係（ fig-fig10-2 ，右）。當我們並排比較這兩份散佈圖，這兩種情況的關係很明顯是同質的：我或者我兒子的睡眠時間越長，我的情緒就越好！不過很明顯的是，dani.sleep和dani.grump之間的關係比baby.sleep和dani.grump之間的關係更強：左圖比右圖更加整齊。直覺來看，如果你想預測我的情緒，知道我兒子睡了多少個小時會有點幫助，但是知道我睡了多少個小時會更有幫助。\n\n\n\n\n\n\n圖 12.2: 左圖是dani.sleep(老爸睡眠小時數)與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖。\n\n\n\n\n相反地， 圖 12.3 的另外兩個散佈圖告訴我們另一個角度的故事。比較“baby.sleep 與 dani.grump”的散佈圖（左）和“baby.sleep 與 dani.sleep”的散佈圖（右），變項之間的整體關係強度相同，但是方向不同。也就是說，如果我的兒子睡得較長，我也會睡得更多（正相關，右圖），但是他如果睡得更多，我就不會那麼沮喪（負相關，左圖）。\n\n\n\n\n\n\n圖 12.3: 左圖是baby.sleep”小嬰兒睡眠小時數”與dani.grump(老爸的沮喪程度)的散佈圖,右圖是baby.sleep”小嬰兒睡眠小時數”與dani.sleep(老爸睡眠小時數)的散佈圖。\n\n\n\n\n\n\n12.1.3 相關係數\n現在我們要進一步延伸上述的概念，也就是正式認識 相關係數(correlation coefficient)。更具體地說，本節主要介紹皮爾森相關係數（Pearson’s correlation），慣例書寫符號是 \\(r\\)。在下一節，我們會用更精確符號 \\(r_{XY}\\) ，表示兩個變項 \\(X\\) 和 \\(Y\\) 之間的相關係數，值域涵蓋-1到1。當\\(r = -1\\)時，表示變項之間是完全的負相關；當\\(r = 1\\)時，表示變項之間是完全的正相關；當\\(r = 0\\)時，表示變項之間是完全沒有關係。 圖 12.4 展示幾種不同相關性的散佈圖。\n[其他技術細節 4]\n\n\n\n\n\n\n圖 12.4: 圖解相關係數的強度及方向。左欄的相關係數由上而下為\\(0, .33, .66, 1\\)。右欄的相關係數由上而下為\\(0, -.33, -.66, -1\\)。\n\n\n\n\n標準化共變異數不僅保留前述共變異數的所有優點，而且相關係數r的數值是有意義的: \\(r = 1\\)代表著完美的正相關，\\(r = -1\\)代表著完美的負相關。稍後解讀相關係數這一節有更詳細的討論。接著讓我們看一下如何在jamovi中計算相關係數。\n\n\n\n12.1.4 相關係數計算實務\n只要在jamovi’迴歸’模組選單，選點要計算的相關係數，就能計算所有納入變項對話框的任何兩個變項之間相關係數，如同 圖 12.5 的示範，沒有出錯的話，報表會輸出’相關係數矩陣’(Correlation Matrix)。\n\n\n\n\n\n\n圖 12.5: 使用jamovi相關分析模組計算parenthood.csv資料變項的示範畫面。\n\n\n\n\n\n\n12.1.5 解讀相關係數\n在現實世界很少會遇到相關係數為1的狀況。那麼，要如何解讀\\(r = 0.4\\)的相關性？老實說，這完全取決於你想分析這些資料的目的，以及你的研究領域對於相關係數強度的共識。我(原作者)有一位工程領域的朋友曾經對我說，任何小於\\(0.95\\)的相關係數都是沒有價值的（我覺得即使對於工程學，他的說法也有點誇張）。在心理學的分析實務，有時應該期望有如此強的相關性。 例如，使用有常模的測驗測試參與者的判斷能力，如果參與者的表現與常模資料的相關性不能達到\\(0.9\\)，任何使用這個測驗預測的理論就會失效5。然而，探討與智力分數有關的因素（例如，檢查時間，反應時間）之間的相關性，如果相關係數超過\\(0.3\\)，已經是非常好的結果。總之，解讀相關係數完全根據解讀的情境。儘管如此，剛開始接觸的同學們可以參考 表 12.2 的概略式解讀原則。\n\n\n\n\n\n表 12.2: 解讀相關係數的粗略指南。強調粗略是因為沒有真正的快速解讀指引，相關係數的真正意義取決於資料分析的問題背景。\n\n\n相關係數\n強度\n方向\n\n\n\n\n-1.0 ~ -0.9\n非常強\n負相關\n\n\n-0.9 ~ -0.7\n強\n負相關\n\n\n-0.7 to -0.4\n中等\n負相關\n\n\n-0.4 ~ -0.2\n弱\n負相關\n\n\n-0.2 ~ 0\n微弱\n負相關\n\n\n0 ~ 0.2\n微弱\n正相關\n\n\n0.2 ~ 0.4\n弱\n正相關\n\n\n0.4 ~ 0.7\n中等\n正相關\n\n\n0.7 ~ 0.9\n強\n正相關\n\n\n0.9 ~ 1.0\n非常強\n正相關\n\n\n\n\n\n\n\n\n然而，有一件點任何一位統計學教師都會不厭其煩地提醒同學，就是解讀資料變項相關係之前，一定要看散佈圖，一個相關係數可能無法充分表達你要說的意思。統計學中有個經典案例「安斯庫姆四重奏」(Anscombe’s Quartet)(Anscombe, 1973)，其中有四個資料集。每個資料集都有兩個變項， \\(X\\) 與 \\(Y\\)。四個資料集的 \\(X\\) 平均值都是 \\(9\\)， \\(Y\\) 的平均值都是 \\(7.5\\)。所有 \\(X\\) 變項的標準差幾乎相同，\\(Y\\) 變項的標準差也是一致。每種資料集的\\(X\\) 和 \\(Y\\) 相關係數均為 \\(r = 0.816\\)。同學可以打開本書示範資料庫裡的Anscombe資料檔親自驗證。\n也許你認為這四個資料集看起來很相似，其實上並非如此。從 圖 12.6 的散佈圖可以發現，所有四個資料集的\\(X\\) 和 \\(Y\\) 變項之間的關係各有千秋。這個案例給我們的教訓是，實務中很多人經常會忘記：「視覺化你的原始數據」（見 單元 5 ）。\n\n\n\n\n\n\n圖 12.6: 安斯庫姆四重奏。四份資料的相關係數都是.816，但是資料數值都不一樣。\n\n\n\n\n\n\n12.1.6 斯皮爾曼等級相關\n皮爾森相關係數的用途很多，不過也有一些缺點，尤其是這個係數只是測量兩個變項之間的線性關係強度。換句話說，係數數值是計量整體資料與一條完美直線的趨近程度。當我們想具體表達兩個變項的“關係”時，皮爾森相關係數通常是很好的選擇。但有時並非最佳選項。\n線性關係是當一個變項\\(X\\)的數值增加，也能反映另一個變項\\(Y\\)的增加。但是兩者關係不是線性的話，皮爾森相關係數就不太合適。例如，準備考試所花的時間和考試成績之間的關係，可能就是這樣的情況。如果一位同學沒有花時間（\\(X\\)）準備一個科目，那麼他排名的成績應該只有0％（\\(Y\\)）。然而，只要一點點努力就會帶來巨大的改善，像是認真上幾堂課並且做筆記就可以學到很多東西，成績排名有可能會提高到35％，而且這是假設沒有做課後復習的情況。然而，想要獲得排名90％的成績，就要比排名55％的成績付出更多努力。也就是說，當我們要分析學習時間和成績的相關係，皮爾森相關係數可能導致錯誤的解讀。\n我們用 圖 12.7 的資料舉例說明，這張散佈圖顯示10名學生在某個課程的讀書時間和考試成績之間的關係。這份虛構的資料怪異之處在於，增加讀書時間總是會提高成績。可能大幅提高，也可能略有提高，但是增加讀書時間絕不會讓成績降低。若是計算這兩個資料變項的皮爾森相關係數，得到的數值為0.91，顯示讀書時間和成績之間有強烈的關係。然而，實際這個分析結果並未充分呈現增加工作時間總是提高成績的關係。儘管我們想要主張兩者的相關性是完全的正相關，但是需要用稍微不同的“關係”來強調，也就是需要另一種方法，能夠呈現這份資料裡完全的次序關係(ordinal relationship)。也就是說，如果第一名學生的讀書時間比二名學生長，那麼我們可以預測第一名學生的成績會更好，而這不是相關係數\\(r=0.91\\)能表達的。\n\n\n\n\n\n\n圖 12.7: 這個圖解展示虛擬資料集的兩個變項”讀書時間”和”成績”之間的關係，這個資料集只有10位學生（每個點代表一個學生）。圖中的直線顯示兩個變項之間的線性關係，兩者之間有很強的皮爾森相關係數\\(r = .91\\)。不過有趣的是，兩個變項之間存在一個完美的單調函數關係。這條直線顯示，根據這份虛擬資料，增加工作時間總是會增加得分，這反映在斯皮爾曼等級相關係數\\(\\rho = 1\\)。然而，由於這個資料集很小，因此仍然存在一個問題：那一種係數是真正描述兩個變項的關係。\n\n\n\n\n那麼我們要如何解決這個問題呢？其實很容易。如果我們要評估變項之間的次序關係，只需要將資料轉換為次序尺度！所以，接著我們不再用“讀書時間”來衡量學生的努力，而是按照他們的讀書時間長短，將這\\(10\\)名學生排序。也就是說，學生\\(2\\)花在讀書的時間最少（\\(2\\)個小時），所以他獲得了最低的排名（排名=\\(1\\)）。接下來最懶惰的是學生\\(4\\)，整個學期只讀了\\(6\\)個小時的書，所以他獲得了次低的排名（排名=\\(2\\)）。請注意，在此用“排名=\\(1\\)”來表示“低排名”。在日常言談裡，多數人使用“排名=\\(1\\)”表示“最高排名”，而不是“最低排名”。因此，要注意你是用“從最小值到最大值”（即最小值做排名1）排名，還是用“從最大值到最小值”（即最大值做排名1）排名。在這種情況下，我是從最大到最小進行排名的，但是因為很容易忘記設置的方式，所以實務中必須做好紀錄！\n好的，讓我們從最努力且最成功的學生開始排名。 表 12.3 顯示從最努力且最成功的學生排名的次序值。\n\n\n\n\n\n表 12.3: 十位學生的工作時間與得分數值次序\n\n\n學生編號\n讀書時間序列\n成績序列\n\n\n\n\n學生 1\n10\n10\n\n\n學生 2\n1\n1\n\n\n學生 3\n5\n5\n\n\n學生 4\n8\n8\n\n\n學生 5\n9\n9\n\n\n學生 6\n6\n6\n\n\n學生 7\n7\n7\n\n\n學生 8\n3\n3\n\n\n學生 9\n4\n4\n\n\n學生 10\n2\n2\n\n\n\n\n\n\n\n\n有意思的是，兩個變項的排名是相同的。投入最多時間的學生得到了最好的成績，投入最少時間的學生得到了最差的成績。由於個變項的排名是相同的，只要計算皮爾森相關係數，就會得到一個完美的相關係數1.0。\n至此我們等於重新發現 斯皮爾曼等級相關(Spearman’s rank order correlation)，通常用符號 \\(\\rho\\) 表示，以區分皮爾森相關係數\\(r\\)。我們可以在jamovi的“相關矩陣”選單選擇“Spearman”，計算斯皮爾曼等級相關係數。6"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-the-scatter-plot",
    "href": "12-Correlation-and-linear-regression.html#sec-the-scatter-plot",
    "title": "12  相關與線性迴歸",
    "section": "12.2 散佈圖",
    "text": "12.2 散佈圖\n散佈圖是一種簡單但有效的視覺化工具，用於具現兩個變項之間的關係，就像相關這一節所展示的圖表。通常提到“散佈圖”這個術語時，指的是具體的視覺化結果。在散佈圖中，每個觀察值都是對應一個資料點。一個點的水平位置表示一個變項的觀察值，垂直位置表示觀察值在另一個變項的數值。在許多使用情境，我們對於變項間的因果關係並沒有清晰的看法（例如，A是否引起B，還是B引起A，還是其他變項C控制A和B）。若是這樣，x軸和y軸上代表那個變項並不重要。然而在許多情境，研究者對於那個變項最有可能是原因或結果，會有一個相當明確的想法，或者對於何者為因至少有一些懷疑。若是這樣，用x軸代表原因的自變項，用y軸代表效應的應變項是一種傳統的繪圖規範。了解這樣的規範，讓我們來看一下如何合理運用jamovi繪製散佈圖，同樣使用在相關這一節做為示範的資料集（parenthood.csv）。\n假定我的目標是繪製一個顯示我睡眠時間（dani.sleep）與隔天沮喪程度（dani.grump）兩個變項關係的散佈圖，我們有兩種不同的方法使用jamovi得到我們想要的圖。第一種方法是設定’Regression’ - ‘Correlation Matrix’選單下方的’Plot’選項，這樣可以得到如圖 圖 12.8 的結果。請注意，jamovi會繪製一條通過資料點的直線，稍後在認識線性迴歸模型這一節進一步說明。以這種方法繪製散佈圖也能繪製’變項密度’，這個選項會添加一條密度曲線，顯示每個變項的資料分佈狀況。\n\n\n\n\n\n\n圖 12.8: 使用jamovi相關分析模組的’Correlation Matrix’所繪製的散佈圖。\n\n\n\n\n第二種方法是使用jamovi的附加模組之一scatr，只要點擊jamovi介面右上角的那個大「\\(+\\)」，在jamovi模組庫裡找到scatr，然後點擊「install」進行安裝。安裝成功後，在「Exploration」的選單下方會多出新的「Scatterplot」選項。這種方法繪製的散佈圖和第一種方法不大一樣，如同 圖 12.9 所顯示，但是透露的訊息是一樣的。\n\n\n\n\n\n\n圖 12.9: 使用jamovi擴充模組’scatr’繪製的散佈圖\n\n\n\n\n\n12.2.1 更多解讀散佈圖的方法\n通常我們會需要查看多個變項之間的關係，可以在 jamovi 的 ‘Correlation Matrix’選單下方的’Plot’ 選項，勾選繪制散佈圖矩陣。只要加入另一個變項到要變項列表，例如 baby.sleep，jamovi 就會生成一個散佈圖矩陣，如同 圖 12.10 的示範。\n\n\n\n\n\n\n圖 12.10: jamovi繪製的散佈圖矩陣"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "href": "12-Correlation-and-linear-regression.html#sec-what-is-linear-model",
    "title": "12  相關與線性迴歸",
    "section": "12.3 認識線性迴歸模型",
    "text": "12.3 認識線性迴歸模型\n\n我們可以將線性迴歸模型理解為稍微複雜一點的皮爾森相關係數分析程序（請見相關這一節），稍後我們會看到，迴歸模型是用途更廣泛的統計方法。\n由於迴歸模型的基本觀念與相關係數緊密相關，以下同樣使用parenthood.csv資料集進行介紹及示範。回想一下，我們分析這個資料集的目的是，找出我(原作者Dani)為什麼總是非常沮喪的原因，而我的研究假設是我沒有得到足夠的睡眠。所以畫了一些散佈圖，檢示實際睡眠時間與隔天沮喪程度之間的關係，就像 圖 12.9 展示的散佈圖，兩者之間的相關係數達到\\(r=-0.90\\)。但是，我想描述的變項間關係更像 圖 12.11 (a) ，也就是有一條直線穿過資料點的中間。這條直線的統計學術語是迴歸線。請注意，由於我不是統計新手，因此畫出的迴歸線一定會穿過資料散佈區域的中間地帶，絕不會認為是像 圖 12.11 (b) 的樣子。\n\n\n\n\n\n\n圖 12.11: 圖a展示同 圖 12.9 的資料散佈圖，並加上穿過資料中心地帶的迴歸線。圖b的散佈圖來自同一份資料，但是迴歸線並不擬合這份資料。\n\n\n\n\n確認能解讀變項間關係的迴歸線，並不需要什麼厲害的技巧。 圖 12.11 （b）的那條線與資料的適合度(fittedness)並不高，用來解讀資料沒有太大的意義，對吧？迴歸線能很直覺地呈現變項間的關係，若是再應用迴歸線的數學理論解讀資料，會變成非常強大的分析工具。我們複習一下高中數學，一條直線的公式可以寫成這樣的等式：\n\n\\[y=a+bx\\]\n至少幾十年前澳洲的高中數學課是這樣教的。兩個資料變項用 \\(x\\) 和 \\(y\\)代表，搭配兩個係數 \\(a\\) 和 \\(b\\) 形成變項之間的等價性。7係數 \\(a\\) 代表迴歸線的截距，係數 \\(b\\) 代表迴歸線的斜率。努力回憶一下高中曾學過的內容（很抱歉，某些讀者也許已經離開高中校園很了），記得截距被解釋為“當 \\(x=0\\) 時得到的 \\(y\\) 值”。同樣地，斜率 \\(b\\) 若為正值，代表增加 \\(x\\) 的數值一個單位， \\(y\\) 值會增加 \\(b\\) 個單位；而斜率 \\(b\\) 若為負值，則代表 \\(y\\) 值會下降而不是上升。啊，是的，我們現在全都回想起來了。現在我們的記憶已經回來，所以自然會發現可以使用完全相同的公式計算迴歸線。如果 \\(Y\\) 是預測變項（依變項），\\(X\\) 是應變項（自變項），那麼描述示範資料的迴歸線等式就會像是這樣：\n\n\\[\\hat{Y}_i=b_0+b_1X_i\\]\n嗯，這看起來這跟曾在高中教科書看到的公式一模一樣，只是多了些花俏的下標符號，讓我們來了解這些符號的意思。首先，請注意我使用 \\(X_i\\) 和 \\(Y_i\\)，而不是 \\(X\\) 和 \\(Y\\)，這是因為有下標符號的代數通常代表實際的資料。在這個公式裡，\\(X_i\\) 代表第 i 個觀察值的預測變項的值（例如我在第 i 天紀錄的睡眠時間），而 \\(Y_i\\) 則是對應的應變項數值（例如我當天的沮喪程度）。雖然公式裡沒有明確說明，但我們假設這個公式對資料集裡的所有觀察值都通用（即 i 對應所有 觀察日數）。其次，請注意我寫的是 \\(\\hat{Y}_i\\) 而不是 \\(Y_i\\)，這是因為我們要區分實際數值 \\(Y_i\\) 與被預測數值 \\(\\hat{Y}_i\\)（也就是經由迴歸線預測的數值）。第三，我將代表係數的符號從 a 和 b 改成 \\(b_0\\) 和 \\(b_1\\)，這是統計學家喜歡呈現迴歸模型的方式。我不知道為什麼他們選擇用 b 這個字母，但這就是統計學的慣例。無論如何，\\(b_0\\) 總是代表截距，\\(b_1\\) 則是代表斜率。\n跟上來的話就很好。接著我們會注意到，無論是好的迴歸線還是壞的迴歸線，資料都是不完美地落在迴歸線。換句話說，實際數值\\(Y_i\\)不完全等於迴歸模型預測的數值\\(\\hat{Y}_i\\)。由於統計學家喜歡給一切符號冠上字母、名稱和數字，讓我們稱呼模型預測的數值與實際數值之間的差異為殘差(Residuals)，代表符號為\\(\\epsilon_i\\)。8 使用數學公式表示的話，殘差可被定義為：\n\n\\[\\epsilon_i=Y_i-\\hat{Y}_i\\]\n接著我們就可以寫出完整的線性迴歸模型：\n\n\\[Y_i=b_0+b_1X_i+\\epsilon_i\\]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-the-regression-coefficients-parameters",
    "href": "12-Correlation-and-linear-regression.html#sec-the-regression-coefficients-parameters",
    "title": "12  相關與線性迴歸",
    "section": "12.4 線性迴歸模型的參數估計",
    "text": "12.4 線性迴歸模型的參數估計\n\n好的，現在讓我們重新繪製散佈圖，這次會添加一些線條顯示所有觀察值的殘差。當迴歸線的適合度(fittedness)最佳時，每個殘差數值（實心黑線的長度）看起來都非常小且接近，如同 圖 12.12 (a) ，但是當迴歸線的適合度不夠好，每個殘差之間的差異就會非常大，可以從 圖 12.12 (b)看到這樣的差別。嗯，也許在尋找一條最好的迴歸模型時，我們會希望得到儘可能小的殘差。是的，這確實有道理。在統計實務，我們可以說「最適合」的迴歸線是具有所有殘差最小的線。或者更好的說法是，因為統計學家似乎喜歡將所有數值都用平方(sqaured)處理，也就是說：\n\n以資料估計的迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 是殘差平方和最小得到時得到的估計值，我們可以兩者的公式展開寫成 \\(\\sum_i (Y_i - \\hat{Y}_i)^2\\) 與 \\(\\sum_i \\epsilon_i^2\\) 。\n\n\n\n\n\n\n\n圖 12.12: 圖(a)展示各觀察值資料點與穿越資料點中心地帶的最佳迴歸線之殘差，圖(b)展示觀察值資料點與最差迴歸線的殘差。前者的殘差總和明顯小於後者。\n\n\n\n\n是的沒錯，這樣說明起來更有學問一些。而且我將這段話縮排，表示這樣說可能是正確的解答。既然這是正確解答，那麼要值得注意的是，迴歸線的係數都是估計值（請復習 單元 8 ，使用點估計方法猜測一個母群的參數！），這也是為什麼我要在代表係數的符號上頭加個小帽子 ^ ，區別會放在報告的是\\(\\hat{b}_0\\)和\\(\\hat{b}_1\\)，而不是 \\(b_0\\) 和 \\(b_1\\)。最後，我還要指出，由於實際上有許多方法來估計迴歸模型，這一節說明的估計方法正式名稱是普通最小平方法（Ordinary Least Squares，OLS）。\n至此，我們已經得到「最佳」迴歸係數 \\(\\hat{b}_0\\) 和 \\(\\hat{b}_1\\) 的具體定義。下一個問題自然是：如果最佳迴歸係數是那些符合最小化殘差平方和的係數，我們要如何算出這些數值呢？實際上，這個問題的答案比較複雜，並且無法幫助你理解迴歸的邏輯。9這一次，我放過各位同學，直接介紹 jamovi 操作方法，瑣碎的讓jamovi來處理。\n\n\n12.4.1 實作線性迴歸模型\n以下是用parenthood.csv 資料檔案執行線性迴歸分析的步驟，請打開 jamovi 的 ‘Regression’ - ‘Linear Regression’ 選單 。接著，將 dani.grump 指定為 ‘Dependent Variable’，dani.sleep 輸入到 ‘Covariates’ 對話框。報表介面將出現如 圖 12.13 的結果，結果顯示截距 \\(\\hat{b}_0 = 125.96\\) 和斜率 \\(\\hat{b}_1 = -8.94\\)。換言之， 圖 12.11 的最適合迴歸線的公式為：\n\n\\[\\hat{Y}_i=125.96+(-8.94 X_i)\\]\n\n\n\n\n\n圖 12.13: jamovi的線性迴歸分析示範畫面。\n\n\n\n\n\n\n12.4.2 解讀線性迴歸模型參數估計\n最後要知道的是如何解釋這些係數。讓我們從 \\(\\hat{b}_1\\) 開始，也就是斜率。回想一下斜率的定義，\\(\\hat{b}_1=-8.94\\) 代表將 \\(X_i\\) 增加 1， \\(Y_i\\) 就會減少 8.94。換言之，多睡一個小時的話，我的心情就會改善，我的沮喪程度就會降低 8.94 。那麼截距呢？由於 \\(\\hat{b}_0\\) 代表「當 \\(X_i\\) 為 0 時 \\(Y_i\\) 的期望值」，這就是說如果我一夜都沒睡 (\\(X_i = 0\\))，我的沮喪程度就會瘋狂升高到不敢想像的數值 (\\(Y_i = 125.96\\))。我想我最好避免這種狀況。\n\n\n\n還有關於等級資料(Rank data)的迴歸分析，請參考線性模型的學習取向的相關與線性迴歸這一節。\n\n\n\n\n以下 小單元 12.5 、 小單元 12.10 以及 小單元 12.11 等三個小單元，是屬於傳統高等統計課程的範圍，其他單元在多數教科書被劃分為基礎統計的範圍。不過接下來的單元裡原作者都是混合一元迴歸與多元迴歸的示範案例，譯者將在屬於多元迴歸的小單元開頭明示譯註，提供使用這本電子書學習的學生與教學的老師，根據自身的學習目標決定如何運用該節內容。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-multiple-regression",
    "href": "12-Correlation-and-linear-regression.html#sec-multiple-regression",
    "title": "12  相關與線性迴歸",
    "section": "12.5 多元線性迴歸",
    "text": "12.5 多元線性迴歸\n\n我們至此討論過的簡單線性迴歸模型，都是假設只有一個自變項，也就是這一章範例資料的dani.sleep。同樣地，基礎統計會學到的大部分統計方法都是假設只有一個自變項和一個應變項。然而，大多數研究項目實際要處理許多自變項。如果是這樣，學習包含多個自變項線性迴歸模型可能比較好。也許使用多元迴歸模型會更適合這一章的範例資料？\n多元迴歸的概念非常簡單，只要在簡單迴歸模型加入更多自變項。假如我們對資料中的兩個自變項都有興趣，也許可以用dani.sleep和baby.sleep預測依變項dani.grump。就像之前的說明一樣，我們用\\(Y_{i}\\)表示第i天的煩躁程度。只是現在有兩個$ X \\(變項：第一個對應我的睡眠時間，第二個對應我兒子的睡眠時間。所以我們用\\)X_{i1}\\(表示第i天我的睡眠時間，\\)X_{i2}$表示那一天我兒子的睡眠時間。那麼我們可以這樣改寫迴歸模型：\n\n\\[Y_i=b_0+b_1X_{i1}+b_2X_{i2}+\\epsilon_i\\]\n就像前一節的說，\\(\\epsilon_i\\)是與第i個觀察值相關的殘差，\\(\\epsilon_i = Y_i - \\hat{Y}_i\\)。多元迴歸模型有三個需要估計的係數：\\(b_0\\)是截距，\\(b_1\\)是代表我的睡眠時間的迴歸係數，\\(b_2\\)是代表我兒子的睡眠時間的迴歸係數。然而，儘管需要估計的迴歸係數變多，估計的基本原理還是一樣：我們要估計的係數\\(\\hat{b}_0\\)、\\(\\hat{b}_1\\)和\\(\\hat{b}_2\\) 算的都是能得到最小殘差平方和的係數。\n\n\n12.5.1 jamovi實作示範\n使用jamovi計算多元迴歸的程序與簡單迴歸完全一樣。我們要做的就是在jamovi的’自變項’對話框添加更多自變項。像是要使用dani.sleep和baby.sleep作為預測變項來解釋為什麼我如此煩躁，那麼將baby.sleep移動到與dani.sleep相鄰的’協變項’框中。默認情況下，jamovi假設該模型應該包括一個截距。這次我們得到的係數顯示在 表 12.4 中。\n\n\n\n\n\n表 12.4: 增加預測變項迴歸係數的示\n\n\n截距\n老爸睡眠小時數\n小嬰兒睡眠小時數\n\n\n\n\n125.97\n-8.95\n0.01\n\n\n\n\n\n\n\n\n與dani.sleep相關的係數相當大，表明每失去一個小時的睡眠，我會變得更加煩躁。然而，baby.sleep的係數非常小，表明我的兒子睡多少其實無關緊要。就我的煩躁程度而言，重要的是我睡多少。為了讓您對這個多元迴歸模型有所了解，?fig-fig10-14顯示了一個三維圖，繪製了所有三個變項以及迴歸模型本身。\n\n\n\n\n\n\n圖 12.14: 這張圖展示多元迴歸模型的三維立體視覺化。模型中有兩個預測變項，分別是 “dani.sleep” 和 “baby.sleep”，而目標變項是 “dani.grump”，這三個變項構成圖中的三維空間。每筆資料都是這個空間中的一個點。就像簡單線性迴歸模型在二維空間形成一條線一樣，此多元迴歸模型在三維空間形成一個平面。當我們估計迴歸係數時，我們能做的就是找到一個盡可能靠近所有資料點的平面。\n\n\n\n\n[附加技術細節10]"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-regression-model-fittedness",
    "href": "12-Correlation-and-linear-regression.html#sec-regression-model-fittedness",
    "title": "12  相關與線性迴歸",
    "section": "12.6 迴歸模型的適合度",
    "text": "12.6 迴歸模型的適合度\n現在我們知道如何估計線性迴歸模型的係數。問題是，我們還不知道這個迴歸模型是否有效。例如，根據一號模型，多睡一小時我(原作者)的情緒會大大改善，但這可能只是廢話。請記住，迴歸模型只是生成個人情緒的預測值 \\(\\hat{Y}_i\\)，實際的情緒量測值則是 \\(Y_i\\)。如果兩種數值非常接近，表示迴歸模型非常適合預測我的情緒變化。如果兩種數值差異很大，那麼這個模型就並不太適合用來預測我的情緒變化。\n\n\n12.6.1 \\(R^2\\)\n我們再次引用一些數學知識解釋如何評估模型的適合度。首先來認識殘差平方和\n\n\\[SS_{res}=\\sum_i (Y_i-\\hat{Y_i})^2\\]\n實務上會期望殘差平方和越小越好。具體地說，殘差平方和佔總變異平方和的比例越小越好\n\n\\[SS_{tot}=\\sum_i(Y_i-\\bar{Y})^2\\]\n談到這裡，我們可以逐步計算這些數值，不過不是用手算。而是使用像是Excel或其他試算表軟體。在Excel中打開parenthood.csv這份檔案，再另存新檔為parenthood rsquared.xls就能完成計算。計算步驟第一步是計算 \\(\\hat{Y}\\) 值，按照以下步驟，我們可以得到以我的睡眠時間預測情緒的簡單迴歸模型：\n\n使用公式= 125.97 + (-8.94 * dani.sleep)創建新欄位Y.pred。\n創建新欄位(Y-Y.pred)\\^2，使用公式= (dani.grump - Y.pred)\\^2計算SS(resid)。\n在(Y-Y.pred)\\^2的最後一列使用公式sum( ( Y-Y.pred)\\^2 )計算這些值的總和。\n在dani.grump的最後一列，計算dani.grump的平均值（留意Excel的函數是’AVERAGE’而不是’mean’）。\n創建新欄位(Y - mean(Y))\\^2 )，使用公式= (dani.grump - AVERAGE(dani.grump))^2。\n在(Y - mean(Y))\\^2 )最後一列，使用= sum( (Y - mean(Y))^2 )計算總和。\n在一個空白儲存格中輸入= 1 - (SS(resid) / SS(tot) )，計算R.squared。\n\n至此我們算出 \\(R^2\\) 的數值 = 0.8161018。\\(R^2\\) 在有些教科書裡被稱呼為決定係數11，這個名稱有一個簡單的解釋：預測變項總變異解釋依變項總變異的比例。因此，在這裡得到的 \\(R^2 = .816\\) 代表預測變項（my.sleep）解釋依變項（my.grump）總變異的81.6％。\n想當然而，如果同學想計算迴歸模型的 \\(R^2\\) ，實際上不需要自己用 Excel 計算。稍後在 小單元 12.7.3 用 jamovi 執行假設檢定這一節，同學會看到只要在模組選單裡勾選指定選項即可。現在，讓我們暫時擱置計算問題，我想談談 \\(R^2\\) 的另一個性質。\n\n\n\n12.6.2 迴歸與相關的關聯\n現在可以重新回顧在這一章開始，我說到迴歸與相關基本上是同一回事。在相關的小單元，我們用符號 \\(r\\) 表示皮爾森相關。那麼皮爾森相關係數 \\(r\\) 和線性迴歸的 \\(R^2\\) 有存在某種關係嗎？當然有：只要將相關係數開平方， \\(r^2\\) 與只有一個預測變項的線性迴歸 \\(R^2\\) 數值是相同的。換句話說，計算皮爾森相關與計算僅有一個預測變項的線性迴歸模型基本上是相同的。\n\n\n\n12.6.3 校正後 \\(R^2\\)\n\n譯註：只有分析多元迴歸的場景，才要了解校正後 \\(R^2\\)。\n\n在繼續到下個單元之前，我最後要指出的是，統計實務通常會報告一個稱為“校正後 \\(R^2\\) ”的計量值。計算及報告校正後 \\(R^2\\) 值的理由是，到將兩個以上預測變項添加到模型中，總是會增加（或至少不降低） \\(R^2\\) 。\n[額外技術細節12]\n校正的目的是為了處理自由度。校正後 \\(R^2\\) 的主用用途是，往模型添加更多預測變後，只有能提高模型預測能力的新變項，才會顯著增加校正後 \\(R^2\\)的數值。然而，校正後 \\(R^2\\)就無法像一開始的 \\(R^2\\) 那樣的直接解釋。據我所知，調整後的 \\(R^2\\) 沒有任何相等意義的解釋。\n那麼統計實務中應該要報告 \\(R^2\\) 還是校正後 \\(R^2\\)？這可能是因人而異。如果同學比較想解釋報告裡的數值，那麼 \\(R^2\\) 較好。如果在乎校正模型的預測偏差，那麼校正後 \\(R^2\\) 可能比較好。就我(原作者)自己而言，我更喜歡 \\(R^2\\)，因為我覺得最重要的是能夠解釋模型預測能力的計量。此外，我們將在 小單元 12.7 迴歸模型的假設檢定這個小單元看到，如果想知道添加預測變項後增加的 \\(R^2\\) 是由於機遇還是因為模型預測能力真的改善了，那麼我們可以用假設檢定來做判斷。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-regression-hypothesis-testing",
    "href": "12-Correlation-and-linear-regression.html#sec-regression-hypothesis-testing",
    "title": "12  相關與線性迴歸",
    "section": "12.7 迴歸模型的假設檢定",
    "text": "12.7 迴歸模型的假設檢定\n至此我們已經學到什麼是迴歸模型，如何估計迴歸模型的係數，以及量化模型預測效能的方法（順便說一下，相關係數與迴歸係數就是一種效果量的估計值）。接下來學習課題的是假設檢定。我們要學習兩種不同（但相關）的假設檢定：一種是檢驗包合所有預測變項的迴歸模型是否顯著優於只有截距的模型，另一種是我們檢驗只有單一預測變項的模型，迴歸係數是否顯著不等於零。\n\n12.7.1 檢定所有預測變項的模型\n\n譯註：只有分析多元迴歸的場景，才要進行這種檢定。內容文字編修排在本書最後階段進行。\n\n好吧，假設你已經估計了你的迴歸模型。你可能會嘗試的第一個假設檢定是虛無假設，即預測變項和結果之間沒有關係，而對立假設是數據的分佈完全符合迴歸模型的預測。\n[額外的技術細節13]\n我們將在 單元 13 中看到更多 F 統計量，但目前只需知道我們可以將較大的 F 值解釋為虛無假設與對立假設相比表現不佳。過一會兒，我將向您展示如何用 jamovi 輕鬆進行檢驗，但首先讓我們看一下單個迴歸係數的檢驗。\n\n\n12.7.2 單一迴歸係數的檢定\n前一節介紹的 F 檢定對於檢查整個線性模型是否優於隨機截距模型很有用。如果您的迴歸模型並未在 F 檢定看到顯著結果，那麼這套迴歸模型可能不是有效解讀資料的好模型（或者是要分析的資料可能並不夠好）。然而，儘管這個檢定失敗是表示模型是否可用的明顯指標，但是通過檢定（也就是拒絕虛無假設）並不表示這個模型是真的好模型！也許同學會想知道為什麼？答案在前面 小單元 12.5 多元線性迴歸 已經有討論過的。\n注意一下 表 12.4 的數值，與 dani.sleep 變項的迴歸係數估計數值（\\(-8.95\\)）相比，baby.sleep 變項的迴歸係數估計數值非常小（\\(0.01\\)）。考慮到這兩個變項的度量尺度都是一樣的（都是以“睡眠小時數”），我發現這很有啟發性。其實我看了 表 12.4 就有想到，要預測我的沮喪程度，真正重要的變項應該只有我自己的睡眠時間長度。我們可以使用之前學到的假設檢定方法，確認我的懷疑14。我們想檢定的虛無假設是設定迴歸係數為零（\\(b = 0\\)），與迴歸係數不是零（\\(b \\neq 0\\)）的對立假設進行檢定。也就是說：\n\\[H_0:b=0\\] \\[H_1:b \\neq 0\\]\n這個檢定要如何進行？好吧，如果還記得中心極限定理，同學可能會猜到迴歸係數的估計值\\(\\hat{b}\\)是一種取樣分佈，而且是以 \\(b\\) 為中心的常態分佈。這表示如果虛無假設是真的，那麼\\(\\hat{b}\\)的取樣分佈平均值為零並且標準差是未知數。假設我們可以找到迴歸係數的標準誤差估計值，\\(se(\\hat{b})\\)，那麼我們就很幸運。這正好可以用 單元 11 介紹的單一樣本 t 檢定處理。現在可以定義以下的 t 統計值\n\\[t=\\frac{\\hat{b}}{SE(\\hat{b})}\\]\n這裡不詳細說明為什麼能這樣做的原因，但在這種狀況，自由度是 \\(df = N - K - 1\\)。令初學者厭煩的通常是，迴歸係數的標準誤估計值，\\(se(\\hat{b})\\)，並不像 單元 11 介紹的單一樣本 t 檢定的平均值標準誤那樣容易計算。真實的公式長得有點醜陋，看起來並不那麼平易近人。15 對於我們要真正完成的目標，只需要知道迴歸係數估計值的標準誤取決於預測變項和依變項，並且要留意有沒有違反變異數相等的適用條件 （稍後 小單元 12.9 就會討論）。\n無論如何，這個t統計值可以按照 單元 11 介紹的檢定方法解釋結果。若是設定做雙尾檢定（也就是說，你不在乎是b &gt; 0還是b &lt; 0），那麼極端的t值（即遠小於零或遠大於零的值）表示你應該拒絕虛無假設。\n\n\n12.7.3 用 jamovi 執行假設檢定\n要計算以上介紹的統計量數，只需要在jamovi迴歸模組選單勾選對應的選項。要選擇的選項如同 圖 12.15 的示範，會得到一系列有用的報表輸出。\n\n\n\n\n\n圖 12.15: jamovi示範畫面，顯示一個多元線性迴歸分析，請留意其中勾選的選項\n\n\n\n\n在jamovi分析結果的“模型係數”表格(Model Coefficients)顯示迴歸模型的係數。此表中的每一行都是對應迴歸模型的其中一個係數。第一行是截距，後面每一行是每個預測變項的檢定結果。每一列標示各種統計訊息。第一列是\\(b\\)的實際估計值（例如，截距為\\(125.97\\)，預測變項dani.sleep 為\\(-8.95\\)）。第二列是標準誤的估計值\\(\\hat{\\sigma}_b\\)。第三和第四列是關於係數估計值的95%信賴區間的下限和上限（稍後對此有更多說明）。第五列是t統計值，值得注意的是，在這個表格中，\\(t=\\frac{\\hat{b}} {se({\\hat{b}})}\\)每次分析結果都是成立的。最後一列呈現這些檢定結果的p值。16\n模型係數表唯一沒有列出的是t檢定的自由度，不過其值始終是\\(N - K - 1\\)，並且輸出到報表標題後的“模型適合度度量”(Model Fit Measures)表格中17。從這個表格中，我們可以看到模型的表現顯著優於機會水平（\\(F(2,97) = 215.24, p&lt; .001\\)），其實這並不奇怪：\\(R^2 = .81\\)值表示迴歸模型能解釋依變項變異量的\\(81\\%\\)（調整後的\\(R^2\\)為\\(82\\%\\) ）。然而，當我們回顧每個個別係數的t檢定時，我們有相當有力的證據表明baby.sleep變項沒有顯著效果。這個模型的主要預測效力都是來由dani.sleep變項。綜合這些結果，我們可以結論這個多元迴歸模型實際上並不是能有效解 資料的模型，最好的模型應該排除baby.sleep這個預測變項。換句話說，開始的簡單迴歸模型是更好的模型。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-regression-estimations",
    "href": "12-Correlation-and-linear-regression.html#sec-regression-estimations",
    "title": "12  相關與線性迴歸",
    "section": "12.8 迴歸係數的估計值",
    "text": "12.8 迴歸係數的估計值\n在討論線性迴歸的適用條件，以及如何檢查一種模型是否滿足條件之前，這裡先簡單討論兩個與迴歸係數有關的主題。首先是如何計算迴歸係數的信賴區間。然後是如何確定哪個預測變項最重要。\n\n12.8.1 迴歸係數的信賴區間\n就像任何人口變項一樣，迴歸係數b無法從樣本資料精確地估算出來；這就是為什麼需要使用假設檢定的一部分原因。有鑑於此，信賴區間能夠呈現捕捉\\(b\\)真實數值的不確定性，是非常有用的工具。這在嘗試找出變項\\(X\\)與變項\\(Y\\)之間的關係強度的研究問題尤其有用，因為在這些研究裡，主要關注的是迴歸權重\\(b\\)(regression weight)。\n[額外技術細節18]\n在jamovi的操作界面，我們可以指定“95％信賴區間”，如 圖 12.15 的示範，如果想要更嚴謹的話，我們可以輕鬆選擇另一種區間，例如“99％信賴區間”。\n\n\n12.8.2 標準化迴歸係數的計算方法\n有經驗的使用者可能還會計算“標準化”迴歸係數，通常報告中用\\(\\beta\\)表示。標準化係數的基本原理是：在很多情況下，每個變項的測量尺度是不一樣的。例如，如果有個迴歸模型要探討受教育程度（受教育年數）和收入作為預測變項，來預測受測者的智力測驗得分。顯然，受教育程度和收入的計量尺度是不相同的。一般人的教育年限可能只有10多年，而收入差距可能高達10,000美元（或更多）。計量單位對迴歸係數有很大影響，只有預測變項和依變項的計量單位一致時，迴歸係數才具有意義，否則比較不同預測變項的迴歸係數將會非常困難。然而，有時我們希望能比較不同變項的係數。具體來說，研究者最想找到那些預測變項與依變項之間相關性最強力的標準衡量指標，這就是為什麼有標準化迴歸係數。\n標準化迴歸係數的思路很簡單；如果在執行迴歸分析之前將所有變項轉換為z分數，標準化係數就是您會得到的係數。19這裡的想法是，將所有預測變項數值轉換為z分數，使得迴歸模型的生成的機率分佈具有可對應的比例，進而消除不同尺度的變項產生的問題。無論變項的原始尺度是什麼，\\(\\beta\\)值為1都代表增加預測變項的1個標準差，就是導致依變項的對應數值增加1個標準差。因此，如果預測變項A的\\(\\beta\\)絕對值大於預測變項B的\\(\\beta\\)，研究者至少能主張預測變項A與依變項的相關性更強。值得小心的是，對於所有變項變異基本相同的這個條件 ，其實非常依賴“1個標準差變化”，並不是什麼形式的變項都可見。\n[額外技術細節20]\n為了讓簡化分析程序，只要在jamovi模組選單裡，從”Model Coefficients”的選項中勾選”Standardized estimate”，就能計算\\(\\beta\\)，如同 圖 12.16 的示範。\n\n\n\n\n\n圖 12.16: 多元線性迴歸的標準化係數及其95％信賴區間\n\n\n\n\n從輸出結果可明顯看出，dani.sleep是比baby.sleep有更強預測效力的變項。然而，這可能正是一個更適合使用原始係數b而不是標準化係數\\(\\beta\\)的完美例子。畢竟，我的睡眠時間和寶寶的睡眠時間已經是同一個計量尺度。為什麼要還要將它們轉換為z分數來讓事情變得更複雜呢？"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-regression-assumptions",
    "href": "12-Correlation-and-linear-regression.html#sec-regression-assumptions",
    "title": "12  相關與線性迴歸",
    "section": "12.9 迴歸模型的適用條件",
    "text": "12.9 迴歸模型的適用條件\n線性迴歸模型必須符合幾個適用條件(assumptions)，才能解讀分析結果。在 小單元 12.10 診斷迴歸模型的適用條件這個單元，我們將會學習如何檢查這些假設是否得到滿足，首先簡單說明每個假設的涵意。21\n\n線性。線性迴歸模型的最基本的適用條件是\\(X\\)和\\(Y\\)之間的關係必須是線性的！無論是簡單迴歸還是多元迴歸，我們都要假設變項之間的關係是線性的。\n獨立性：這是說變項資料的殘差彼此獨立。實際上這是條“總括一切”的適用條件，更白話地說“在殘差中找不到任何有意思的東西了”。如果還能分析出一些意料之外的資訊（例如，所有殘差都與某些未測量的變項存在明顯相關性），可能會破壞分析後的結論。\n常態性。就像驅動許多統計方法的機率模型一樣，基本的簡單或多元線性迴歸也要符合常態性。具體來說，這條是指殘差的次數分佈逼近或符合常態分佈。實際上，即使預測變項\\(X\\)和依變項\\(Y\\)的實際資料次數分佈不符合常態分佈，只要殘差\\(\\epsilon\\)的次數分佈符合常態的就可以了。 小單元 12.10 診斷迴歸模型的適用條件有進一步說明及示範。\n變異相等（或稱’同質性’）。嚴格來說，符合這個條件 的迴歸模型生成的所有殘差\\(\\epsilon_i\\)，都是來自一個平均值為0的常態分佈，更重要的是，每個殘差來源的機率分佈標準差\\(\\sigma\\)都是相同的。在實務中，檢驗每個殘差都是來自同一個機率分佈是不大可能做到的事。相反地，我們真正關心的是殘差的標準差相對於所有預測值\\(\\hat{Y}\\)是相同的，特別是多元迴歸模型的每個預測變項\\(X\\)所生成的預測值是相同的。\n\n所以，要執行有效的線性迴歸分析，首先要檢查是不是符合這四個適用條件 （剛好可以縮寫成LINE）。此外，還有一些需要檢查的條件 ：\n\n沒有“不良”極端值。其實這並非必要的適用條件，但是極端值可能造成潛在的問題。就是迴歸模型雖然不會因為一兩個異常極端值，造成不符合上述任何一條適用條件(譯註~特別是線性)，但是在某些情況下會引起對模型的適當性和資料可靠性的質疑。詳細說明及示範請見 小單元 12.10.2 三種異常資料。\n預測變項之間無相關性。執行多元迴歸模型分析時，我們不希望預測變項彼此間的存在高度相關。這並非迴歸模型的“必要”適用條件，但在統計實務是必需的。預測變項之間相關性過強（通常稱為“共線性”）可能會造成錯誤解讀分析結果。詳細說明及示範請見 小單元 12.10.4 檢查共線性。"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-regression-Model-diagnosis",
    "href": "12-Correlation-and-linear-regression.html#sec-regression-Model-diagnosis",
    "title": "12  相關與線性迴歸",
    "section": "12.10 診斷迴歸模型的適用條件",
    "text": "12.10 診斷迴歸模型的適用條件\n\n譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。\n\n本節的主要焦點是迴歸診斷，這個術語是指檢查迴歸模型假設是否得到滿足、在假設被違反時如何修正模型以及一般情況下檢查是否存在不尋常情況的技術。我將這稱為模型檢查的“藝術”，理由很充分。這並不容易，儘管有許多相當標準化的工具可以用來診斷甚至可能治愈困擾模型的問題（如果存在的話！），但在這方面真的需要運用一定程度的判斷力。在檢查這件事情或那件事情的所有細節中容易迷失，試圖記住所有不同的事物是相當耗費精力的。這會產生一個非常令人討厭的副作用，很多人在試圖學習所有工具時會感到沮喪，所以他們決定不做任何模型檢查。這有點令人擔憂！\n在本節中，我描述了一些方法，用於檢查迴歸模型是否按照預期工作。它並沒有涵蓋所有您可能做的事情，但仍然比我在實踐中看到的大多數人所做的事情要詳細得多，即使在我的初級統計課程中，我通常也不會涵蓋所有這些內容。但是，我確實認為您應該了解可供您使用的工具，所以我將在這裡嘗試介紹一部分。最後，我應該指出，本節很大程度上借鑒了 Fox & Weisberg (2011) ，即與在 R 中進行迴歸分析的“car”包相關的書籍。 “car”包以提供一些出色的迴歸診斷工具而著稱，而該書本身以極為清晰的方式談論了這些工具。我不想聽起來太過於誇大，但我確實認為即使在 R 和不是 jamovi 的情況下， Fox & Weisberg (2011) 都值得一讀。\n\n12.10.1 三種殘差\n大多數迴歸診斷都圍繞著觀察殘差，到目前為止，你可能已經對統計學形成了足夠悲觀的理論，能夠猜到，正因為我們非常關心殘差，我們可能會考慮幾種不同類型的殘差。特別地，在本節中，我們將提到以下三種殘差：“普通殘差”、“標準化殘差”和“學生化殘差”。還有第四種你會在一些圖中看到的，稱為“皮爾森殘差”。然而，對於我們在本章中討論的模型，皮爾森殘差與普通殘差相同。\n首先，我們關心的最簡單類型的殘差是普通殘差。這些就是我在本章前面一直提到的實際原始殘差。普通殘差僅僅是擬合值 \\(\\hat{Y}_i\\) 和觀察值 \\(Y_i\\) 之間的差。我一直用符號 \\(\\epsilon_i\\) 表示第 i 個普通殘差，並且我將繼續堅持使用它。考慮到這一點，我們有非常簡單的方程式\n\\[\\epsilon_i=Y_i-\\hat{Y_i}\\]\n這當然是我們之前看到的，除非我特別提到其他類型的殘差，否則我就是在談論這個。所以這裡沒有新的東西。我只是想重申一下。使用普通殘差的一個缺點是，它們總是在不同的尺度上，取決於結果變項是什麼以及迴歸模型有多好。也就是說，除非你決定在沒有截距項的情況下運行迴歸模型，否則普通殘差的均值將為 0，但每個迴歸的方差都不同。在很多情境下，特別是當你只對殘差的模式感興趣，而不是它們的實際值時，估計標準化殘差很方便，這些殘差經過規範化後標準差為 1。\n[額外技術細節22]\n第三種殘差是學生化殘差（也稱為 “剃刀切割殘差”），它們比標準化殘差更高級。同樣，目的是將普通殘差除以某個量，以估計殘差的某種標準化概念。23\n在繼續之前，我應該指出，即使這些殘差是幾乎所有迴歸診斷的核心，你通常也不需要自己獲得這些殘差。大多數時候，提供診斷或假設檢查的各種選項將為您處理這些計算。即使如此，知道如何實際自己獲得這些東西，以防你需要進行一些非標準操作，總是很好的。\n\n\n12.10.2 三種異常資料\n使用線性迴歸模型時，您可能會遇到一個危險，那就是您的分析可能會對一小部分”不尋常”或”異常”的觀測值過於敏感。我之前在 小單元 5.2.3 的上下文中討論過這個想法，當時是在討論用 ‘探索’ - ‘描述統計’ 下的 boxplot 選項自動識別的異常值，但這次我們需要更精確。在線性迴歸的背景下，有三個概念上不同的方式可以將觀測值稱為”異常”。這三者都很有趣，但對你的分析有很不同的影響。\n第一種不尋常的觀測值是異常值。在這種情況下，異常值的定義是與迴歸模型預測的結果相差很大的觀測值。圖 12.17 中有一個例子。在實踐中，我們通過說一個異常值是具有非常大的Studentised殘差的觀測值，\\(\\epsilon_i^*\\)，來實現這個概念。異常值很有趣：一個很大的異常值可能對應垃圾數據，例如，變項在數據集中可能被錯誤地記錄，或者可能檢測到其他缺陷。請注意，僅僅因為它是一個異常值，你不應該丟掉這個觀測值。但是，它是一個異常值，這經常是一個線索，讓你更仔細地查看該案例，並嘗試找出它為什麼如此不同。\n\n\n\n\n\n圖 12.17: 異常值的示例。虛線繪製了不包括異常觀測值的迴歸線，以及相應的殘差（即Studentised殘差）。實線顯示了包含異常觀測值的迴歸線。異常值在結果值（y軸位置）上具有不尋常的值，但在預測變項（x軸位置）上並不不尋常，並且距離迴歸線很遠\n\n\n\n\n觀測值不尋常的第二種方式是具有高槓桿作用(leverage)，這發生在觀測值與所有其他觀測值非常不同的情況下。這不一定要對應大的殘差。如果觀測值在所有變項上的不尋常程度恰好相同，則實際上可能非常接近迴歸線。這方面的一個例子如@fig-fig10-18所示。觀測值的槓桿作用通常用帽子值表示，通常寫作\\(h_i\\)。帽子值的公式相當複雜24，但它的解釋並不複雜：\\(h_i\\)是衡量第i個觀測值“控制”迴歸線走向的程度。\n\n\n\n\n\n圖 12.18: 高槓桿點的示例。在這種情況下，異常觀測值在預測變項（x軸）和結果變項（y軸）方面都不尋常，但這種不尋常性與其他觀測值之間存在的相關性模式高度一致。觀測值非常接近迴歸線並且不會使其變形\n\n\n\n\n一般來說，如果觀測值在預測變項方面遠離其他觀測值，它將具有較大的帽子值（作為粗略指南，高槓桿是指帽子值大於平均值的2-3倍；注意帽子值的總和被限制為等於\\(K + 1\\)）。高槓桿點也值得更詳細地查看，但除非它們也是異常值，否則它們不太可能引起擔憂。\n這讓我們來到了不尋常程度的第三個衡量指標，即觀測值的影響力(influence)。高影響力的觀測值是具有高槓桿的異常值。也就是說，它在某些方面與所有其他觀測值非常不同，並且距離迴歸線很遠。這在@fig-fig10-19中有所體現。注意與前兩個圖形的對比。異常值並未使迴歸線發生很大變化，高槓桿點也是如此。但既是異常值又具有高槓桿的情況，會對迴歸線產生很大影響。這就是為什麼我們稱這些點具有高影響力，而且它們是最令人擔憂的。我們用稱為Cook’s distance的衡量指標來度量影響力。25\n\n\n\n\n\n圖 12.19: 高影響力點的示例。在這種情況下，異常觀測值在預測變項（x軸）上非常不尋常，並且距離迴歸線很遠。因此，即使在這種情況下，異常觀測值在結果變項（y軸）上完全正常，迴歸線也會受到很大影響\n\n\n\n\n要具有較大的Cook’s距離，觀測值必須是相當大的異常值並具有高槓桿。作為粗略指南，大於1的Cook’s距離通常被認為很大（這是我通常用作快速而簡單的規則）。\n在jamovi中，可以通過單擊’Assumption Checks’ - ’Data Summary’選項下的’Cook’s Distance’複選框來計算有關Cook’s距離的信息。當你這樣做時，對於我們在本章中作為示例使用的多元迴歸模型，你將得到如@fig-fig10-20所示的結果。\n\n\n\n\n\n圖 12.20: jamovi輸出顯示Cook’s距離統計表格\n\n\n\n\n您可以看到，在這個例子中，平均Cook’s距離值為\\(0.01\\)，範圍從\\(0.00\\)到\\(0.11\\)，因此這與前面提到的指標相去甚遠，即大於1的Cook’s距離被認為很大。\n接下來明顯要問的問題是，如果您確實擁有很大的Cook’s距離值，您應該怎麼辦？一如既往，沒有固定不變的規則。可能首先要做的是嘗試運行迴歸，排除具有最大Cook’s距離的異常值26，看看模型性能和迴歸係數會發生什麼變化。如果它們確實有很大不同，那就該開始深入研究您的數據集和您在運行研究時無疑在抄寫的筆記。嘗試找出該點為何如此不同。如果您開始確信這個數據點嚴重扭曲了您的結果，那麼您可能會考慮將其排除在外，但除非您對於這個特定案例與其他案例有本質上的不同並因此應該單獨處理，否則這種做法是不理想的。\n\n\n\n\n\n12.10.3 檢測殘差常態性\n像我們在本書中討論過的許多統計工具一樣，迴歸模型依賴於常態性假設。在這種情況下，我們假設殘差呈常態分布。首先，我們可以通過 ‘Assumption Checks’ - ‘Assumption Checks’ - ‘Q-Q plot of residuals’ 選項繪製一個 QQ 圖。輸出顯示在 圖 12.21，顯示了標準化殘差作為其根據迴歸模型的理論分位數的函數的圖。\n\n\n\n\n\n圖 12.21: 模型理論分位數與標準化殘差的分位數之間的圖，由 jamovi 生成\n\n\n\n\n我們還應該檢查擬合值和殘差本身之間的關係。我們可以使用 ‘Residuals Plots’ 選項讓 jamovi 做到這一點，該選項為每個預測變項、結果變項以及擬合值與殘差提供了散點圖，見 圖 12.22。在這些圖中，我們要尋找的是點的分佈相對均勻，沒有明顯的聚集或圖案。觀察這些圖，沒有什麼特別令人擔憂的，因為點在整個圖中分佈得相當均勻。在圖 (b) 中可能存在一點非均勻性，但偏差不大，可能不值得擔憎。\n\n\n\n\n\n圖 12.22: 由 jamovi 生成的殘差圖\n\n\n\n\n如果我們擔憂的話，那麼在很多情況下解決這個問題（以及其他許多問題）的方法是對一個或多個變項進行轉換。我們在 小單元 6.3 中討論了變項轉換的基本知識，但我想特別提一下我之前沒有完全解釋的一個額外可能性：Box-Cox 轉換。Box-Cox 函數相當簡單，並且被廣泛使用。27\n您可以在 jamovi 的 ‘Compute’ 變數屏幕中使用 BOXCOX 函數進行計算。\n\n\n12.10.4 檢測共線性\n在本章中，我將討論的最後一種迴歸診斷方法是使用方差膨脹因子（VIF），這對於確定迴歸模型中的預測變項是否彼此相關性過高很有用。模型中每個預測變項 \\(X_k\\) 都有一個相應的方差膨脹因子。28\nVIF 的平方根具有很好的解釋性。它告訴您相應的係數 bk 的信賴區間相對於預測變項彼此完全不相關時所期望的值要寬多少。如果您只有兩個預測變項，VIF 值將始終相同，正如我們在 jamovi 的 ‘Regression’ - ‘Assumptions’ 選項中選中 ‘Collinearity’ 後可以看到的那樣。對於 dani.sleep 和 baby.sleep，VIF 均為 1.65。而由於 1.65 的平方根為 1.28，我們可以看到我們兩個預測變項之間的相關性並未造成太大問題。\n為了給出我們可能會遇到具有更大共線性問題的模型的感覺，假設我要運行一個更無趣的迴歸模型，在該模型中，我試圖預測數據收集的日期，作為數據集中所有其他變項的函數。要理解這為什麼會有問題，讓我們看一下所有四個變項的相關矩陣（圖 12.23）。\n\n\n\n\n\n圖 12.23: 四個資料變項之間的相關係數矩陣\n\n\n\n\n我們的預測變項之間有一些相當大的相關性！當我們運行迴歸模型並查看 VIF 值時，我們可以看到共線性對係數的不確定性造成了很大影響。首先，運行迴歸，如 圖 12.24 所示，從 VIF 值可以看出，是的，這是一些非常好的共線性。\n\n\n\n\n\n圖 12.24: jamovi 生成的多元迴歸共線性統計"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#sec-multiple-variables-combinaion",
    "href": "12-Correlation-and-linear-regression.html#sec-multiple-variables-combinaion",
    "title": "12  相關與線性迴歸",
    "section": "12.11 決定線性模型的變項組合",
    "text": "12.11 決定線性模型的變項組合\n\n譯註：這個單元全部內容都是談多元迴歸，內容文字編修排在本書最後階段進行。\n\n剩下的一個相當重要的問題是 “模型選擇” 的問題。也就是說，如果我們有一個包含幾個變項的資料集，哪些變項應該作為預測變項，哪些不應該包括在內？換句話說，我們有一個變項選擇的問題。通常，模型選擇是一個複雜的過程，但如果我們將問題限制在選擇應該包含在模型中的變項子集上，情況會變得簡單一些。儘管如此，我不打算試圖詳細涵蓋甚至這個範疇。相反，我將談論您需要考慮的兩個廣泛原則，然後討論一個具體的工具，jamovi 提供了這個工具，可以幫助您選擇要包含在模型中的變項子集。首先，兩個原則：\n\n為您的選擇提供實質性的依據是很好的。也就是說，在很多情況下，您作為研究人員有充分的理由挑選一個較小的可能的回歸模型數量，這些模型在您的領域背景下具有合理的解釋。永遠不要低估這一點的重要性。統計學為科學過程服務，而不是反過來。\n在您的選擇依賴統計推斷的程度上，簡單性和適合度之間存在權衡。當您向模型添加更多預測變項時，模型變得更複雜。每個預測因子都添加了一個新的自由參數（即，一個新的回歸係數），每個新參數都會增加模型對於隨機變異的吸收能力。因此，適合度（例如，\\(R^2\\)）隨著您添加更多預測因子而持續上升，無論如何都是如此。如果您希望模型能夠很好地概括新的觀察結果，則需要避免加入過多的變項。\n\n後者原則通常被稱為奧卡姆剃刀，並通常用以下簡潔的說法來概括：不要在必要之外繁殖實體。在這個情境下，這意味著不要僅僅為了提高你的 R2 而將一堆大致無關的預測因子扔進去。嗯，原來的說法更好。\n無論如何，我們需要一個實際的數學標準，以便在選擇回歸模型時實現奧卡姆剃刀背後的定性原則。事實證明，有幾種可能性。我將談論的一個是赤池資訊量準則（Akaike information criterion）(Akaike, 1974)，僅僅是因為它可以作為一個選項在 jamovi 中使用。29\nAIC 值越小，模型性能越好。如果我們忽略低水平的細節，AIC 的作用就非常明顯了。左邊的項隨著模型預測變差而增加；右邊的項隨著模型複雜度的增加而增加。最佳模型是用盡量少的預測變項（低 K，右側）來擬合資料（低殘差，左側）。簡而言之，這是奧卡姆剃刀的簡單實現。\n當選中 “AIC” 複選框時，AIC 可以添加到 “Model Fit Measures” 輸出表中，評估不同模型的一種笨拙方式是查看如果從回歸模型中移除一個或多個預測因子，“AIC” 值是否更低。這是 jamovi 目前實現的唯一方法，但在其他更強大的程序中，如 R，有替代方法。這些替代方法可以自動化有選擇地移除（或添加）預測變項以找到最佳 AIC 的過程。儘管這些方法在 jamovi 中尚未實現，但我將在下面簡要介紹它們，以便您了解它們。\n\n12.11.1 逐步排除法\n在逐步排除法中，您從完整的迴歸模型開始，包括所有可能的預測因子。然後，在每個「步驟」中，我們嘗試所有可能的刪除一個變項的方法，並選擇其中最好的（就最低AIC值而言）。這將成為我們的新迴歸模型，然後我們再試驗從新模型中刪除所有可能的選項，同樣選擇具有最低AIC的選項。這個過程將持續進行，直到我們得到一個具有比刪除一個預測因子的其他可能模型更低AIC值的模型。\n\n\n12.11.2 逐步納入法\n作為替代方法，您還可以嘗試逐步納入法。這次我們從最小可能的模型作為起點，僅考慮可能添加到模型中的選項。然而，還有一個麻煩。您還需要指定您願意接受的最大可能模型。\n儘管向後和向前選擇可能導致相同的結論，但它們並不總是如此。\n\n\n12.11.3 使用警告\n自動變項選擇方法是具有誘惑力的東西，特別是當它們被捆綁在強大的統計程序中的（相對）簡單函數中時。它們為您的模型選擇提供了一定程度的客觀性，這很好。不幸的是，它們有時被用作掩蓋思考的藉口。您不再需要仔細考慮要添加到模型中的哪些預測因子以及它們可能包含的理論基礎。一切都通過AIC的魔力解決了。如果我們開始丟出像奧卡姆剃刀這樣的短語，那麼一切都被包裹在一個整潔的小包裹裡，沒有人可以反駁。\n或者，也許不是。首先，對於什麼算作合適的模型選擇標準，幾乎沒有一致的看法。當我在本科時代被教授逐步排除法時，我們使用了F檢驗來執行它，因為那是軟件所使用的默認方法。我描述了使用AIC，並且因為這是一本入門教材，所以我只描述了這種方法，但AIC絕非統計之神的話語。它是一個近似值，在某些假設下得出的，並且只有在大樣本中滿足這些假設時才能保證起作用。改變那些假設，您就會得到不同的標準，比如BIC（在jamovi中也可用）。再換一種方法，您就會得到NML標準。決定成為貝葉斯，您將基於後驗概率比進行模型選擇。然後還有一堆我沒提到的迴歸特定工具。等等。所有這些不同的方法都有優點和缺點，有些比其他方法更容易計算（AIC可能是最容易的，這可能解釋了它的受歡迎程度）。幾乎所有方法在答案是“明顯”的情況下都會產生相同的結果，但在模型選擇問題變得困難時，存在相當多的分歧。\n在實踐中，這意味著什麼？好吧，您可以花幾年時間教自己模型選擇理論，學習所有的技巧，最終決定您個人認為什麼是正確的。作為實際做過這件事的人，我不建議這樣做。您可能會在結束時更加困惑。更好的策略是表現出一點常識。如果您盯著自動向後或向前選擇過程的結果，有意義的模型接近具有最小AIC值，但被一個毫無意義的模型以微弱的優勢擊敗，那麼相信您的直覺。統計模型選擇是一個不精確的工具，正如我一開始說的，可解釋性很重要。\n\n\n12.11.4 比較迴歸模型\n與使用自動模型選擇程序的方法相反，研究人員可以明確地選擇兩個或多個迴歸模型以相互比較。您可以用幾種不同的方法做到這一點，具體取決於您要回答的研究問題。假設我們想知道我兒子睡眠的多少是否與我煩躁的程度有關，超出了我自己睡眠的影響。我們還希望確保測量的那一天對這種關係沒有影響。也就是說，我們對baby.sleep和dani.grump之間的關係感興趣，從這個角度看，dani.sleep和day是我們想控制的協變項。在這種情況下，我們想知道dani.grump ~ dani.sleep + day + baby.sleep（我將其稱為Model 2，或M2）是否比dani.grump ~ dani.sleep + day（我將其稱為Model 1，或M1）更適合這些數據。我們可以用兩種不同的方式來比較這兩個模型，一種基於像AIC這樣的模型選擇標準，另一種基於顯式假設檢定。我首先向您展示基於AIC的方法，因為它更簡單，並且自然地延續了上一節的討論。首先，我需要實際運行兩個迴歸，注意每個迴歸的AIC，然後選擇AIC值較小的模型，因為它被認為是這些數據的更好模型。實際上，不要立即這樣做。繼續閱讀，因為jamovi中有一種簡單的方法可以在一個表格中獲取不同模型的AIC值。30\n基於假設檢定框架的某種不同方法來解決這個問題。假設您有兩個迴歸模型，其中一個（Model 1）包含另一個（Model 2）的一部分預測變項。也就是說，Model 2包含Model 1中包含的所有預測變項，再加上一個或多個其他預測變項。當這種情況發生時，我們說Model 1嵌套在Model 2中，或者可能說Model 1是Model 2的子模型。無論使用哪種術語，這意味著我們可以將Model 1視為虛無假設，將Model 2視為替代假設。事實上，我們可以用相當簡單的方式構建一個F檢驗。31\n那麼，這就是我們用來比較兩個迴歸模型的假設檢定。現在，我們如何在jamovi中進行呢？答案是使用“Model Builder”選項，在“Block 1”中指定Model 1預測變項dani.sleep和day，然後將Model 2中的其他預測變項（baby.sleep）添加到“Block 2”，如@fig-fig10-25所示。這在“Model Comparisons”表格中顯示了比較Model 1和Model 2的結果，\\(F(1,96) = 0.00\\)，\\(p = 0.954\\)。由於我們的p &gt; .05，我們保留虛無假設（M1）。這種將我們所有協變數添加到零模型中，然後將感興趣的變項添加到替代模型中，然後在假設檢定框架中比較兩個模型的迴歸方法通常被稱為分層迴歸。\n我們還可以使用此“Model Comparison”選項顯示一個表格，顯示每個模型的AIC和BIC，方便比較並確定哪個模型具有最低的值，如@fig-fig10-25所示。\n\n\n\n\n\n圖 12.25: 使用jamovi的“Model Builder”選項進行模型比較"
  },
  {
    "objectID": "12-Correlation-and-linear-regression.html#footnotes",
    "href": "12-Correlation-and-linear-regression.html#footnotes",
    "title": "12  相關與線性迴歸",
    "section": "",
    "text": "本章在原書為第12章，繁體中文版為了整合 基礎統計方法的線性模型學習取向，改為第10章。↩︎\n在2.0版之後的 jamovi 你也可以指定 “ID” 的專屬變項類型，但是對於分析資料的目的來說，指定 ID 的變項性質並不重要，因為我們不會將其包含在任何分析中。↩︎\n即使是表格中的資訊對某些人來說已經足夠了，但在實務上，大多數人只需要報告一個集中量數和一個變異量數就可以了。↩︎\n皮爾森相關係數的公式可以用幾種不同的方式來表示。最簡單的方式是將公式分為兩個部分。第一個部分是共變異數（covariance）(譯註：許多台灣中文教科書稱共變數)的概念。兩個變項\\(X\\)和\\(Y\\)的共變異數的數學公式是變異數公式的一般化，就是簡單描述兩個變項之間的關係，但是計算結果無法提供什麼有意義的訊息: \\[Cov(X,Y)=\\frac{1}{N-1}\\sum_{i=1}^N(X_i-\\bar{X})(Y_i-\\bar{Y})\\] 由於我們要將\\(X\\)的數值乘以\\(Y\\)的數值，再取乘積的平均值\\(^a\\)，因此共變異數的公式可以視為\\(X\\)和\\(Y\\)之間的“平均外積”。共變異數的特性之一是，如果\\(X\\)和\\(Y\\)之間完全沒有關係，那麼共變異數就是零。如果變項之間是正相關（見 圖 12.4 ），則共變異數的數值為正，如果是負相關，共變異數的數值為負。換句話說，共變異數能捕捉相關性的基本定性概念。不幸的是，共變異數的原始數值並不容易解讀，因為大小取決於\\(X\\)和\\(Y\\)所的計量單位，而且更糟糕的是，共變異數數值的單位非常奇怪。例如，如果\\(X\\)是dani.sleep變項（單位：小時）而\\(Y\\)是dani.grump變項（單位：沮喪程度），那麼共變異數的單位會是\\(hours \\times grumps\\)，我們無法解讀這是什麼意思。皮爾森相關係數則解決了這個問題，與z分數標準化原始分數的方法非常相似，將差異值除以標準差轉換為標準化分數。但是，由於共變異數來自兩個變項的數值，必須除以兩個變項的標準差才能做標準化轉換。\\(^b\\)換句話說，兩個變項\\(X\\)和\\(Y\\)的相關係數可以寫成這個公式：\\[r_{XY}=\\frac{Cov(X,Y)}{\\hat{\\sigma}_X\\hat{\\sigma}_Y}\\]—\\(^a\\) 和變異數、標準差一樣，實際上我們在計算時除以的是 \\(N-1\\) 而不是 \\(N\\)。 \\(^b\\) 這是一個過於簡化的說法，但對於我們的目的而言已足夠。↩︎\n譯註~這一句原文略談測驗信效度，但是原文說法太模糊，中文翻譯做適度改寫。↩︎\n譯註~原版電子書並未提供 表 12.3 的資料，中文版配合線性模型教程，採用另一套線上教程提供的範例檔案，請見相關與線性迴歸這一節的補充。↩︎\n也可以寫成 \\(y = mx + c\\)，其中 \\(m\\) 是斜率，\\(c\\) 是截距（常數）。↩︎\n\\(\\epsilon\\) 符號是希臘字母的 epsilon。統計報告慣例使用 \\(\\epsilon_i\\) 或 \\(e_i\\) 表示殘差。↩︎\n原作者設想大多數讀者並不想知道普通最小平方法的細節。不過讀者若是線性代數的高手（客觀地說，很多在排名前位的大學開設統計課的教師，總是會在課堂遇到厲害的學生），會很很想知道，計算迴歸係數估計值的公式是\\(\\hat{b} = (X^{'}X)^{-1}X^{'}y\\)，其中\\(\\hat{b}\\)是包含估計迴歸係數的數值向量，\\(X\\)是包含預測變項（嚴格來說，\\(X\\)是迴歸變項的矩陣，是實際資料數值加上一個全部都是1的數列，但這裡不討論這個區別），而\\(y\\)是包含依變項的數值向量。對於不是高手的讀者，這些解說並沒有太大幫助，甚至可能造成障礙。然而，由於線性迴歸有許多細節可以用線性代數的術語來表示，讀者將在本章後半部看到許多像這樣的註釋。如果讀者有能力理解其中的數學涵意，那就非常好。如果不能，可以忽略這些註釋。↩︎\n一般情況下的公式：主文中我給出的方程式顯示了當您包括兩個預測變項時，多元迴歸模型是什麼樣子。毫不奇怪，如果您想要在模型中使用多於兩個預測變項，則只需添加更多的X項和更多的b係數。換句話說，如果您在模型中有K個預測變項，那麼迴歸方程就是這樣的 \\[Y_i=b_0+(\\sum_{k=1}^{K}b_k X_{ik})+\\epsilon_i\\]↩︎\n“有些”指的是“非常少”(譯註：其實很多中文教科書都這樣稱呼)。在統計實務，多數人都用“R-squared”稱呼。↩︎\n調整後的 \\(R^2\\) 值引入了一個輕微的變化到計算中，如下所示。對於具有 \\(K\\) 個預測變項的迴歸模型，適用於包含 \\(N\\) 個觀察值的數據集，調整後的 \\(R^2\\) 為：\\[\\text{adj.}R^2=1-(\\frac{SS_{res}}{SS_{tot}} \\times \\frac{N-1}{N-K-1})\\]↩︎\n正式地，我們的「零模型」對應於相當簡單的「迴歸」模型，其中我們包括 0 預測變項並且僅包括截距項 \\(b_0\\)：\\(H_0:Y_0=b_0+\\epsilon_i\\) 如果我們的迴歸模型具有 \\(K\\) 預測變項，那麼「對立模型」使用多元迴歸模型的標準公式描述：\\[H_1:Y_i=b_0+(\\sum_{k=1}^K b_k X_{ik})+\\epsilon_i\\] 我們如何在這兩個假設之間進行檢驗呢？訣竅在於理解將總變異 \\(SStot\\) 劃分為殘差變異 SSres 和迴歸模型變異 SSmod 的可能性。我將跳過技術細節，因為我們稍後在 單元 13 中研究 ANOVA 時會研究這個問題。但是只需注意 \\(SS_{mod}=SS_{tot}-SS_{res}\\) 我們可以通過將平方和除以自由度將其轉換為平均平方。 \\[MS_{mod}=\\frac{SS_{mod}}{df_{mod}}\\] \\[MS_{res}=\\frac{SS_{res}}{df_{res}}\\] 那麼，我們有多少自由度呢？您可能會預料到，與模型相關的 df 與我們所包含的預測變項數量密切相關。實際上，\\(df_mod = K\\)。對於殘差，總自由度為 \\(df_res = N - K - 1\\)。現在我們已經有了平均平方值，我們可以像這樣計算 F 統計量 \\[F=\\frac{MS_{mod}}{MS_{res}}\\] 並且與此相關的自由度為 \\(K\\) 和 \\(N - K - 1\\)。↩︎\n譯註~這些適用條件在所有基礎統計方法幾乎都是必要的，這也是本書中文版將相關與線性迴歸調整為第一個學習的統計方法原因之一。↩︎\n給認真的高手讀者做個補充。殘差向量是\\(\\epsilon=y - X\\hat{b}\\)。對於K個預測變項加截距，估計的殘差平方差是\\(\\hat{\\sigma}^2 = \\frac{\\epsilon^{'}\\epsilon}{(N - K - 1)}\\)。係數估計值的共變異數矩陣是\\(\\hat{\\sigma}^{2}(X^{'}X)^{-1}\\)，矩 陣的主對角線是\\(se(\\hat{b})\\)，就是標準誤估計值。↩︎\n注意，儘管jamovi會自動執行多次檢定，但是沒有執行Bonferroni校正或其他類似操作（參考 單元 13 ）。這些是預設對立假設為雙側的標準單一樣本t檢定。如果想要校正多次檢定的結果，使用者需要自行操作。↩︎\n譯注~必須要勾選“Model Fit”選單裡的”F test”才能顯示自由度等資訊。↩︎\n幸運的是，迴歸權重的信賴區間可以通用的方法計算出來\\(CI(b)=\\hat{b} \\pm (t_{crit} \\times SE(\\hat{b}))\\)，其中\\(se(\\hat{b})\\)是迴歸係數的標準誤，\\(t_{crit}\\)是t分佈的對應臨界值。例如，如果我們要計算的是95％的信賴區間，則臨界值是具有\\(N - K -1\\)自由度的t分佈中\\(97.5\\)的百分位數。換句話說，這與其他統計方法使用的信賴區間的計算方法相同。↩︎\n嚴格來說，研究者需要將所有迴歸變項標準化。也就是說，在模型中與依變 項相關的每個“事物”都具有迴歸係數。對於到目前為止談過的迴歸模型，每個預測變項都恰好有一個迴歸變項，反之亦然。但是，統計實務中這並不是一般情況，稍後在 單元 14 ，我們會看到一些這方面的例子。但是，目前我們不需要太在意這種區別。(譯註~之後會提到的線性模性版範例，全都有線性迴歸模型。)↩︎\n撇開這個小單元的解釋，我們可以嘗試看看實際的計算步驟。同學們可以先將所有變項資料標準化，然後再執行迴歸分析。而且其實有一種更簡單的方法可以做到這一點，預測變項\\(X\\)和依變項\\(Y\\)的\\(\\beta\\)係數有個非常簡單的公式：\\(\\beta_X=b_X \\times \\frac{\\sigma_X}{\\sigma_Y}\\)，其中\\(\\sigma_X\\)是預測變項的標準差，\\(\\sigma_Y\\)是依變項Y的標準差。↩︎\n譯註~這些適用條件在所有基礎統計方法幾乎都是必要的，這也是本書中文版將相關與線性迴歸調整為第一個學習的統計方法原因之一。↩︎\n我們計算這些值的方法是將普通殘差除以這些殘差的（群體）標準差的估計值。由於技術原因，嘟囔囔，公式為 \\[\\epsilon_i^{'}=\\frac{\\epsilon_i}{\\hat{\\sigma}\\sqrt{1-h_i}}\\] 其中，\\(\\hat{\\sigma}\\) 在此上下文中是普通殘差的估計群體標準差，\\(h_i\\) 是第 i 個觀察值的“帽值”。我還沒有向你解釋帽值（但不用害怕\\(^c\\)，它很快就會到來），所以這個公式現在可能看起來不太合理。目前，只需將標準化殘差理解為如果我們將普通殘差轉換為 z 分數。實際上，這幾乎就是事實，只不過我們的表達稍微高級一些。+++\\(^c\\) 或者沒有希望，也可能是這樣。↩︎\n這次進行計算的公式略有不同 \\(\\epsilon _i^*=\\frac{\\epsilon_i}{\\hat{\\sigma}_{(-i)}\\sqrt{1-h_i}}\\) 注意，我們這裡的標準差估計值寫作 \\(\\hat{\\sigma}_{(-i)}\\)。這對應的是，如果你從數據集中刪除第 i 個觀察值，則將獲得的殘差標準差估計。這聽起來好像是一個很難計算的東西，因為它似乎在說你必須運行 N 個新的迴歸模型（即使是現代計算機也可能對此有些怨言，特別是如果你有一個大數據集）。幸運的是，一些非常聰明的人已經證明了這個標準差估計實際上是由以下公式給出的：\\(\\hat{\\sigma}_{(-i)}= \\hat{\\sigma}\\sqrt{\\frac{N-K-1-{\\epsilon_i^{'}}^2}{N-K-2}}\\) 難道這不是一個妙計嗎？↩︎\n再次，對於線性代數狂熱分子：定義帽矩陣為將觀測值向量\\(y\\)轉換為擬合值向量\\(\\hat{y}\\)的矩陣\\(H\\)，使得\\(\\hat{y} = Hy\\)。這個名字來自這是將帽子放在y上的矩陣。第i個觀測值的帽值是這個矩陣的第i個對角元素（所以從技術上講，我應該將其寫為\\(h_{ii}\\)而不是\\(h_i\\)）。哦，如果你在意，這是它的計算方法：\\(H = X(X^{'}X)^{1}X^{'}\\)。漂亮，不是嗎？↩︎\n\\(D_i=\\frac{{\\epsilon_i^*}^2}{K+1} \\times \\frac{h_i}{1-h_i}\\) 請注意，這是衡量觀測值的異常程度（左邊的部分）和衡量觀測值的槓桿（右邊的部分）的乘積。↩︎\n雖然目前在jamovi中進行此操作的方法不太容易，因此更強大的迴歸程序（例如R中的’car’套件）將更適合進行此更高級的分析。↩︎\n對於所有的 λ 值，除了 λ = 0，有 \\(f(x,\\lambda)=\\frac{x^{\\lambda}-1}{\\lambda}\\)。當 λ = 0 時，我們只取自然對數（即，ln（x））。↩︎\n第 k 個 VIF 的公式為：\\(VIF_k=\\frac{1}{1-R_{-k}^2}\\)，其中 \\(R^2_(-k)\\) 指的是如果您將 \\(X_k\\) 作為結果變項，並將所有其他 X 變項作為預測變項來運行迴歸，則可以得到的 R 平方值。這裡的想法是 \\(R^2_(-k)\\) 是 \\(X_k\\) 與模型中所有其他變項相關程度的非常好的衡量指標。↩︎\n在線性回歸模型的上下文中（忽略與模型無關的項！），具有 K 個預測變項和截距的模型的 AIC 為 \\(AIC=\\frac{SS_{res}}{\\hat{\\sigma}^2}+2K\\)↩︎\n在此主題上，我應該指出，經驗證據表明，BIC比AIC更好。在我看到的大多數模擬研究中，BIC在選擇正確模型方面做得更好。↩︎\n我們可以將兩個模型都應用於數據並獲得兩個模型的殘差平方和。我將它們分別表示為\\(SS_{res}^{(1)}\\)和\\(SS_{res}^{(2)}\\)。上標表示我們正在談論哪個模型。然後我們的F統計量是 \\[F= \\frac {\\frac {SS _{res}^{(1)} - SS_{res}^{(2)}} {k}}   {\\frac{SS_{res}^2} {N-p-1} }\\]，其中\\(N\\)是觀察次數，\\(p\\)是完整模型中的預測變項數量（不包括截距），\\(k\\)是兩個模型之間參數的差異。這裡的自由度是\\(k\\)和\\(N -p-1\\)。值得注意的是，將這兩個SS值之間的差異表示為它自己的平方和經常更方便。也就是說 \\[SS_\\Delta=SS_{res}^{(1)}-SS_{res}^{(2)}\\]。這樣有幫助的原因是我們可以將\\(SS_\\Delta\\)表示為兩個模型對結果變項的預測有所不同的程度。具體來說，\\[SS_\\Delta=\\sum_i{(\\hat{y}_i^{(2)}-\\hat{y}_i^{(1)})^2}\\]，其中\\(\\hat{y}_{i^{(1)}}\\)是根據模型\\(M_1\\)對\\(y_i\\)的預測值，\\(\\hat{y}_{i^{(2)}}\\)是根據模型\\(M_2\\)對\\(y_i\\)的預測值。  —  \\(^d\\)順便提一下，這個相同的F統計量可以用來檢驗比我在這裡提到的更廣泛的範圍的假設。非常簡要地說，注意嵌套模型M1對應於當我們將某些迴歸係數限制為零時的完整模型M2。在某些情況下，通過對迴歸係數施加其他類型的約束來構建子模型是有用的。例如，也許兩個不同的係數可能必須相加為零，或類似的情況。您可以為這些類型的約束構建假設檢定，但這有點更複雜，而且F的抽樣分布可能會是所謂的非中心F分布，這已經超出了本書的範疇！我想做的只是提醒您這種可能性。↩︎"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#獨立樣本變異數分析示範資料",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#獨立樣本變異數分析示範資料",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.1 獨立樣本變異數分析示範資料",
    "text": "13.1 獨立樣本變異數分析示範資料\n假想你協助執行一件臨床試驗，研究一種名為Joyzepam的新型抗憂鬱藥物效用。為了公平地測試這種新藥的效用，你要分別測試包括新藥的三種藥物。另外兩種的其中一種是安慰劑，另一種是已經上市，名為Anxifree的抗憂鬱/抗焦慮藥物。你一開始招募了18名患有中度至重度抑鬱症的參與者。由於有的參與者不只有服用藥物，也同時接受心理治療，因此所有參與者包括9個正在進行認知行為治療（CBT）的個案和9個未進行任何治療的個案。參與者以雙盲的隨機方式派給藥物，因此每一種藥物分給3位接受CBT的個案和3位無接受治療的個案。每位個案各自使用被分派的藥物3個月後，再由研究者評估每個人的情緒改善狀態，從\\(-5\\)到\\(+5\\)的範圍代表每位個案的情緒改善狀況。現在我們可以載入資料檔 clinicaltrial.csv，看看這種研究設計的內容，檔案包含三個變項，分別是藥物、治療和情緒提升分數。\n以這一章的學習目標來說，我們想了解的是藥物對情緒提升的效用。首先要做的是進行描述性統計並繪製一些圖表。在 章节 4 ，同學們已經學到如何使用完成描述統計，就像 图 13.1 顯示的圖表。\n\n\n\n\n\n\n图 13.1: 情緒提升的描述性統計數據，以及按給予的藥物繪製的盒形圖。\n\n\n\n\n如 图 13.1 所示，服用Joyzepam的參與者，情緒的改善程度大於服用Anxifree或安慰劑。Anxifree的情緒提升程度大於安慰劑，但是差距沒有像Joyzepam那麼大。在此要回答的問題是，這些藥效的差異是否“真有其事”，還是僅僅是偶然的結果？"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的適用條件",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#單因子變異數分析的適用條件",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.6 單因子變異數分析的適用條件",
    "text": "13.6 單因子變異數分析的適用條件\n像任何統計檢驗一樣，變異數分析依賴於關於數據（特別是殘差）的一些假設。您需要了解三個關鍵假設：正態性、方差同質性和獨立性。\n[額外的技術細節 [^13-comparing-several-means-one-way-anova-12]]\n[^13-comparing-several-means-one-way-anova-12]：如果您記得回到[一個實例]，我希望您至少瀏覽了一遍，即使您沒有讀完整篇文章，我以這種方式描述了支撐ANOVA的統計模型：\\[H_0:Y_{ik}=\\mu + \\epsilon_{ik}\\] \\[H_1:Y_{ik}=\\mu_k + \\epsilon_{ik}\\]在這些等式中，\\(\\mu\\)指的是對所有組別都相同的單個總群體均值，µk是第k個組的群體均值。到目前為止，我們主要關心的是我們的數據是最好用單個總均值（零假設）來描述，還是用不同的特定組均值（替代假設）來描述。當然，這是有道理的，因為這實際上是重要的研究問題！然而，我們所有的檢驗過程都是在一個關於殘差 \\(\\epsilon_{ik}\\) 的具體假設下進行的，即：\\[\\epsilon_{ik} \\sim Normal(0,\\sigma^2)\\]如果沒有這部分，所有的數學都不能正常工作。或者，確切地說，您仍然可以進行所有計算，最終得到一個F統計量，但是您無法保證這個F統計量實際上衡量了您認為它衡量的內容，因此您可能基於F檢驗得出的任何結論都可能是錯誤的。\n那麼，我們如何檢查對殘差的假設是否準確呢？嗯，正如我上面所指出的，這個陳述中隱含了三個不同的主張，我們將分別考慮它們。\n\n方差同質性。注意到我們只有一個群體標準差的值（即，\\(\\sigma\\)），而不是讓每個組都有它自己的值（即，\\(\\sigma_k\\)）。這被稱為方差同質性（有時稱為等方差性）假設。ANOVA假定所有組的群體標準差相同。我們將在[檢查方差同質性假設]部分詳細論述這一點。\n正態性。假定殘差呈正態分布。正如我們在 章节 11.9 中看到的，我們可以通過查看QQ圖（或運行Shapiro-Wilk檢驗）來評估這一點。我將在[檢查正態性假設]部分中更多地討論這個問題。\n獨立性。獨立性假設有點棘手。它基本上的意思是，了解一個殘差對於了解任何其他殘差都沒有幫助。所有的 \\(\\epsilon_{ik}\\) 值都被假定是在不考慮或與其他任何值無關的情況下生成的。對於這一點，沒有顯而易見或簡單的檢驗方法，但有些情況是明顯違反這一假設的。例如，如果您有一個重複測量設計，每個參與者在研究中出現在多個條件下，那麼獨立性就不成立了。在這種情況下，某些觀察之間存在特殊關係，即對應於同一個人的觀察！當這種情況發生時，您需要使用類似[重複測量單因子ANOVA]的方法。\n\n\n\n13.6.1 同質性檢核\n\n要進行方差的初步檢驗，就像乘坐划艇出海，看看海面條件是否足夠平靜，讓一艘大型遊輪離港！\n– 喬治·博克斯 (Box, 1953)\n\n俗話說，殺貓有很多方法，檢驗方差同質性假設也有很多方法（不過出於某種原因，沒有人把它變成一句俗話）。在文獻中，我見過的最常用的檢驗方法是Levene檢驗(Levene, 1960)，以及與之密切相關的Brown-Forsythe檢驗(Brown & Forsythe, 1974)。\n無論您是進行標準Levene檢驗還是Brown-Forsythe檢驗，檢驗統計量（有時表示為\\(F\\)，但也有時表示為\\(W\\)），都是按照計算常規ANOVA中的F-統計量的方式，只是使用\\(Z_{ik}\\)而不是\\(Y_{ik}\\)。有了這個思路，我們可以繼續看看如何在jamovi中運行檢驗。\n[額外的技術細節[^13-comparing-several-means-one-way-anova-13]]\n[^13-comparing-several-means-one-way-anova-13]：Levene檢驗非常簡單。假設我們有結果變項\\(Y_{ik}\\)。我們所要做的就是定義一個新變項，我將其稱為\\(Z_{ik}\\)，表示與組均值的絕對偏差：\\[Z_{ik}=Y_{ik}-\\bar{Y}_{k}\\]好吧，這對我們有什麼好處呢？那麼，讓我們花一點時間來思考一下\\(Z_{ik}\\)到底是什麼以及我們要檢驗什麼。\\(Z_{ik}\\)的值是度量第\\(i\\)次觀測在第\\(k\\)個組中與其組平均值的偏差程度。我們的零假設是所有組的方差都相同，即所有組平均值的總偏差相同！因此，Levene檢驗中的零假設是所有組的\\(Z\\)的母體平均值相同。嗯。那麼我們現在需要的是一個統計檢驗來檢驗所有組均值相同的零假設。我們在哪裡見過這個檢驗？哦對了，這就是ANOVA，所以Levene檢驗所做的就是對新變項\\(Z_{ik}\\)進行ANOVA。Brown-Forsythe檢驗呢？它有做什麼特別不同的事情嗎？不，與Levene檢驗唯一的不同是它以稍微不同的方式構建轉換變項Z，使用組中位數的偏差而不是組平均值的偏差。也就是說，對於Brown-Forsythe檢驗：\\[Z_{ik}=Y_{ik}-median_k(Y)\\]其中，\\(median_k(Y)\\)是第k組的中位數。\n\n\n\n13.6.2 jamovi的Levene檢定\n好的，那麼我們該如何進行Levene檢驗呢？其實很簡單 - 在ANOVA的”假設檢查”選項下，只需點擊”變異數同質性檢驗”複選框。如果我們查看 图 13.5 中的輸出，我們可以看到檢驗結果並無顯著差異（\\(F_{2,15} = 1.45, p = .266\\)），所以變異數同質性假設看起來沒有問題。然而，外表可能會讓人受騙！如果您的樣本量相當大，那麼即使變異數同質性假設沒有被違反到影響ANOVA的穩健性，Levene檢驗也可能顯示出顯著效應（即p &lt; .05）。這正是George Box在上面引述中所指出的觀點。同樣地，如果您的樣本量相當小，那麼變異數同質性假設可能不被滿足，而Levene檢驗可能不顯著（即p &gt; .05）。這意味著，在對假設是否被滿足進行任何統計檢驗的同時，您應該總是繪製每個分組/類別的均值周圍的標準差……只是為了看看它們是否看起來相當相似（即變異數同質性）或不相似。\n\n\n\n\n\n\n图 13.5: jamovi中單因素ANOVA的Levene檢驗輸出\n\n\n\n\n\n\n13.6.3 校正異質性的分析結果\n在我們的示例中，變異數同質性假設被證明是相當可靠的：Levene檢驗結果並無顯著差異（儘管我們還應該查看標準差的圖形），因此我們可能不需要擔心。然而，在現實生活中，我們並非總是如此幸運。當變異數同質性假設被違反時，我們該如何拯救我們的ANOVA呢？如果您回想一下我們對t檢驗的討論，我們之前遇到過這個問題。Student t檢驗假設等方差，所以解決方法是使用不需要等方差假設的Welch t檢驗。實際上， Welch (1951) 還展示了我們如何解決ANOVA的這個問題（Welch單因素檢驗）。它在jamovi中使用One-Way ANOVA分析實現。這是一種專為單因素ANOVA設計的分析方法，要在我們的示例中執行Welch單因素ANOVA，我們將按照之前的方式重新運行分析，但這次使用jamovi的ANOVA - One Way ANOVA分析命令，並選擇Welch檢驗的選項（參見 图 13.6 ）。為了理解這裡發生了什麼，讓我們將這些數字與我們在[最初在jamovi中運行ANOVA]時得到的數字進行比較。為了省去您回顧的麻煩，上次我們得到的是：\\(F(2, 15) = 18.611, p = .00009\\)，這也顯示為 图 13.6 中One-Way ANOVA的Fisher檢驗。\n\n\n\n\n\n\n图 13.6: Welch檢驗作為jamovi中One Way ANOVA分析的一部分\n\n\n\n\n好的，最初我們的ANOVA結果是\\(F(2, 15) = 18.6\\)，而Welch單因素檢驗給出的是\\(F(2, 9.49) = 26.32\\)。換句話說，Welch檢驗將組內自由度從15降低到了9.49，而F值從18.6上升到了26.32。\n\n\n\n13.6.4 常態性檢核\n檢驗正態性假設相對簡單。我們在 章节 11.9 中介紹了大部分你需要了解的內容。我們真正需要做的只是繪製一個QQ圖，並在可行的情況下，運行Shapiro-Wilk檢驗。QQ圖顯示在 图 13.7 ，對我來說看起來相當正常。如果Shapiro-Wilk檢驗不顯著（即\\(p &gt; .05\\)），那麼這表明正態性假設沒有被違反。然而，與Levene檢驗一樣，如果樣本量很大，那麼顯著的Shapiro-Wilk檢驗實際上可能是偽陽性，也就是說，正態性假設在實質上沒有對分析造成任何問題。同樣地，非常小的樣本量可能會產生偽陰性。這就是為什麼視覺檢查QQ圖很重要。\n\n\n\n\n\n\n图 13.7: jamovi中One Way ANOVA分析的QQ圖\n\n\n\n\n除了檢查QQ圖中是否有偏離正態性的情況外，我們的數據的Shapiro-Wilk檢驗確實顯示出非顯著效應，p = 0.6053（見 图 13.6 ）。因此，這支持了QQ圖的評估；兩個檢查都沒有發現正態性被違反的跡象。\n\n\n\n13.6.5 排除非常態性的分析結果\n現在我們已經了解了如何檢查正態性，我們自然會問可以採取哪些措施來解決正態性的違反。在單因素ANOVA的背景下，最簡單的解決方案可能是轉向非參數檢驗（即不依賴於任何特定的分佈假設的檢驗）。在 章节 11 中，我們之前已經介紹過非參數檢驗。當你只有兩個組別時，Mann-Whitney或Wilcoxon檢驗可以提供你所需的非參數替代方法。當你有三個或更多組別時，你可以使用Kruskal-Wallis秩和檢驗(Kruskal & Wallis, 1952)。接下來我們將講解這個檢驗。\n\n\n\n13.6.6 Kruskal-Wallis檢定的邏輯\nKruskal-Wallis檢驗在某些方面與ANOVA驚人地相似。在ANOVA中，我們從\\(Y_{ik}\\)開始，對於第k個組中的第i個人，這是結果變項的值。對於Kruskal-Wallis檢驗，我們要做的是對所有的\\(Y_{ik}\\)值進行排序，並對排名數據進行分析。10\n\n\n\n13.6.7 更多分析細節\n上一節的描述說明了Kruskal-Wallis檢驗背後的邏輯。從概念上講，這是考慮測試如何工作的正確方法。[^13-comparing-several-means-one-way-anova-15]\n[^13-comparing-several-means-one-way-anova-15]：然而，從純粹的數學角度來看，這是不必要的複雜。我不會向您展示推導，但您可以使用一些代數技巧\\(^b\\)來顯示K的方程式可以是\\[K=\\frac{12}{N(N-1)}\\sum_k N_k \\bar{R}_k^2 -3(N+1)\\] 最後一個方程式有時給出了K的值。這比我在上一節中描述的版本要容易得多，但問題是對實際人類完全沒有意義。將K視為基於排名的ANOVA類比可能是最好的方式。但請記住，計算出來的檢驗統計量與我們最初用於ANOVA的統計量有很大不同。— \\(b\\)就是一些數學運算術語。\n但等等，還有更多！天啊，為什麼總是有更多呢？到目前為止，我講的故事實際上只在原始數據中沒有相同數值的情況下才成立。也就是說，如果沒有兩個觀測值具有完全相同的值。如果有相同的值，那麼我們必須引入一個校正因子來進行這些計算。在這一點上，我假設即使是最勤奮的讀者也已經不再關心（或者至少形成了繫結校正因子不需要他們立即關注的看法）。因此，我將非常快速地告訴您如何計算它，並省略為什麼以這種方式進行的繁瑣細節。假設我們為原始數據構建一個頻率表，讓fj表示具有第j個唯一值的觀測值的數量。這聽起來可能有點抽象，因此我們將從clinicaltrials.csv數據集中的mood.gain頻率表（ 表格 13.11 ）給出一個具體的例子。\n\n\n\n\n\n\n表格 13.11: 數據中clinicaltrials.csv心情增益的次數表\n\n\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.8\n0.9\n1.1\n1.2\n1.3\n1.4\n1.7\n1.8\n\n\n1\n1\n2\n1\n1\n2\n1\n1\n1\n1\n2\n2\n1\n1\n\n\n\n\n\n\n\n\n觀察此表，請注意頻率表中的第三個條目值為2。由於這對應於心情增益為0.3，因此此表告訴我們有兩個人的心情增加了0.3。[^13-comparing-several-means-one-way-anova-16]\n[^13-comparing-several-means-one-way-anova-16]：更重要的是，在我上面介紹的數學表示法中，這告訴我們\\(f_3 = 2\\)。耶。那麼，現在我們知道了這一點，繫結校正因子（TCF）是：\\[TCF=1-\\frac{\\sum_j f_j^3 - f_j}{N^3 - N}\\]通過將K值除以這個數量，可以得到Kruskal-Wallis統計量的繫結校正值。這是jamovi計算的繫結校正版本。\n因此，jamovi使用繫結校正因子來計算繫結校正的Kruskall-Wallis統計量。最後，我們實際上已經完成了Kruskal-Wallis檢驗的理論。我確信你們都對我治愈了你們在意識到你們不知道如何計算Kruskal-Wallis檢驗的繫結校正因子時自然產生的存在焦慮感到非常寬慰。對吧？\n\n\n\n\n13.6.8 使用jamovi完成Kruskal-Wallis檢定\n儘管我們在努力理解Kruskal Wallis檢驗實際上做了什麼方面經歷了恐懼，但事實證明，進行該檢驗相當無痛，因為jamovi在ANOVA分析集中有一個名為「非參數」-「單因子ANOVA（Kruskall-Wallis）」的分析。大多數時候，你將擁有像clinicaltrial.csv這樣的數據集，其中包含你的結果變項mood.gain和一個分組變項drug。如果是這樣，你可以直接在jamovi中運行分析。這給我們提供了一個Kruskal-Wallis \\(\\chi^2 =12.076, df = 2, p = 0.00239\\)，如 图 13.8 所示。\n\n\n\n\n\n\n图 13.8: jamovi中的Kruskall-Wallis單因子非參數ANOVA"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#sec-oneway-repeated-measure",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#sec-oneway-repeated-measure",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "13.7 單因子重覆量數變異數分析",
    "text": "13.7 單因子重覆量數變異數分析\n單因子重覆量數變數分析檢驗是一種統計方法，用於檢驗三個或更多組之間的顯著差異，其中每個組都使用相同的參與者（或者每個參與者與其他實驗組的參與者密切匹配）。因此，每個實驗組中應該始終具有相等數量的分數（數據點）。這種類型的設計和分析也可以稱為「相關ANOVA」或「內部主題ANOVA」。\n重覆量數變數分析的邏輯與獨立ANOVA（有時稱為「間題」ANOVA）非常相似。您可能還記得，我們之前顯示在一個間題ANOVA總變異性可以分為組間變異性（\\(SS_b\\)）和組內變異性（\\(SS_w\\)），在將每個變異性除以相應的自由度後得到MSb和MSw（見表13.1），F比值計算為：\n\\[F=\\frac{MS_b}{MS_w}\\]\n在重覆量數變數分析中，F比值的計算方式類似，但是在獨立ANOVA中，組內變異性（\\(SS_w\\)）被用作\\(MS_w\\)的分母，而在重覆量數變數分析中，\\(SS_w\\)被劃分為兩部分。由於我們在每個組中都使用相同的受試者，因此可以從組內變異性中移除受試者間個別差異（稱為SSsubjects）的變異性。我們不會深入討論這是如何實現的，但本質上，每個受試者都成為名為受試者的因子的一個水平。然後以與任何間題因子相同的方式計算此內部受試者因子中的變異性。然後我們可以將SSsubjects從\\(SS_w\\)中減去，以提供一個較小的SSerror項：\n\\[\\text{獨立ANOVA: } SS_{error} = SS_w\\] \\[\\text{重覆量數變數分析: } SS_{error} = SS_w - SS_{subjects}\\] 這個\\(SS_{error}\\)項的變化通常會導致統計檢驗更加強大，但這確實取決於\\(SS_{error}\\)的減少是否超過了誤差項自由度的減少（因為自由度從\\((n - k)\\)11變為\\((n - 1)(k - 1)\\)（請記住，獨立ANOVA設計中的受試者更多）。\n\n\n\n13.7.1 jamovi的重覆量數變異數分析\n首先，我們需要一些數據。 Geschwind (1972) 表示，患者在中風後語言缺陷的確切性質可以用來診斷已受損的大腦特定區域。一位研究人員關心的是確定六位患有Broca失語症（中風後常見的語言缺陷）的患者所經歷的具體交流困難（ 表格 13.12 ）。\n\n\n\n\n\n表格 13.12: 中風患者單詞識別作業分數\n\n\nParticipant\nSpeech\nConceptual\nSyntax\n\n\n1\n8\n7\n6\n\n\n2\n7\n8\n6\n\n\n3\n9\n5\n3\n\n\n4\n5\n4\n5\n\n\n5\n6\n6\n2\n\n\n6\n8\n7\n4\n\n\n\n\n\n\n\n\n患者需要完成三個單詞識別任務。在第一個（言語生成）任務中，患者需要重複研究者大聲朗讀的單詞。在第二個（概念性）任務中，旨在測試單詞理解能力，患者需要將一系列圖片與其正確名稱匹配。在第三個（語法）任務中，旨在測試正確單詞順序的知識，要求患者對語法不正確的句子進行重新排序。每位患者都完成了所有三個任務。患者嘗試任務的順序在參與者之間進行了平衡。每個任務包括一系列10次嘗試。每位患者成功完成的嘗試次數如 表格 13.11 所示。將這些數據輸入jamovi以進行分析（或者使用捷徑加載broca.csv文件）。\n要在jamovi中執行一個單因素相關ANOVA，打開一個單因素重覆量數變數分析對話框，如 图 13.9 中所示，通過ANOVA - Repeated Measures ANOVA進行。\n\n\n\n\n\n\n图 13.9: jamovi中的重覆量數變數分析對話框\n\n\n\n\n然後：\n\n輸入一個重複測量因子名稱。這應該是您選擇的標籤，用於描述所有參與者重複的條件。例如，要描述所有參與者完成的語音、概念和語法任務，一個合適的標籤是“任務”。請注意，這個新的因子名稱代表了分析中的自變項。\n在重複測量因子文本框中添加第三個級別，因為有三個級別代表三個任務：語音、概念和語法。相應地更改級別的標籤。\n然後將每個級別變項移動到重複測量單元文本框中。\n最後，在“假設檢查”選項下，選中“球形性檢查”文本框。\n\njamovi輸出一個單因素重覆量數變數分析，如 图 13.10 至 图 13.13 所示。我們應該首先查看的是Mauchly球形性檢驗，該檢驗測試各條件之間的差異方差是否相等（意味著研究條件之間的差異得分的分佈大致相同）。在 图 13.10 中，Mauchly檢驗的顯著性水平為\\(p = .720\\)。如果Mauchly檢驗的結果不顯著（即p &gt; .05，正如此分析中的情況），那麼我們有理由得出差異的方差並無顯著差異（即它們大致相等，可以假定球形性。）。\n\n\n\n\n\n\n\n图 13.10: 單因子重覆量數變數分析輸出 - Mauchly球形性檢驗\n\n\n\n\n如果另一方面，Mauchly檢驗顯著（p &lt; .05），那麼我們將得出差異方差之間存在顯著差異，並且未滿足球形性要求。在這種情況下，我們應該對單因素相關ANOVA分析中獲得的F值進行修正：\n\n如果”球形性檢驗”表中的Greenhouse-Geisser值&gt; .75，那麼您應該使用Huynh-Feldt修正\n但如果Greenhouse-Geisser值&lt; .75，那麼您應該使用Greenhouse-Geisser修正。\n\n這兩個修正過的F值都可以在“假設檢查”選項下的球形性修正復選框中指定，修正過的F值將顯示在結果表中，如 图 13.11 所示。\n\n\n\n\n\n\n\n图 13.11: 單因素重覆量數變數分析輸出 - 內部受試者效應檢驗\n\n\n\n\n在我們的分析中，我們發現Mauchly的球形性檢驗的顯著性為p = .720（即p &gt; 0.05）。因此，這意味著我們可以假設已滿足球形性要求，因此無需對F值進行修正。因此，我們可以使用’無’球形性修正輸出值用於重複測量”任務”：\\(F = 6.93\\)，\\(df = 2\\)，\\(p = .013\\)，我們可以得出結論，語言任務中成功完成的測試次數確實會根據任務是語音、理解還是語法為基礎而顯著不同（\\(F(2, 10) = 6.93\\)，\\(p = .013\\)）。\n在jamovi中，與獨立ANOVA相同，也可以為重覆量數變數分析指定事後檢驗。結果顯示在 图 13.12 。這些表明語音和語法之間存在顯著差異，但其他級別之間沒有差異。\n\n\n\n\n\n\n图 13.12: 重覆量數變數分析中jamovi的事後檢驗\n\n\n\n\n描述性統計（邊際均值）可以用於幫助解釋結果，在jamovi輸出中生成，如 图 13.13 。通過比較參與者成功完成試驗的平均次數，可以看出布洛卡失語症患者在語音產生（平均= 7.17）和語言理解（平均= 6.17）任務上表現相對較好。然而，他們在語法任務上的表現明顯較差（平均= 4.33），事後檢驗中語音和語法任務表現之間存在顯著差異。\n\n\n\n\n\n\n图 13.13: 單因子重覆量數變數分析輸出-描述性統計"
  },
  {
    "objectID": "13-Comparing-several-means-one-way-ANOVA.html#footnotes",
    "href": "13-Comparing-several-means-one-way-ANOVA.html#footnotes",
    "title": "13  比較多組平均值(單因子變異數分析)",
    "section": "",
    "text": "當所有組的觀察值數目相同時，實驗設計被稱為“平衡”。對於本章介紹的單因子 ANOVA，平衡並不是很重要。當您開始進行更複雜的 ANOVA 時，它變得更重要。↩︎\n所以總平方和的公式與方差的公式幾乎相同 \\[SS_{tot}=\\sum_{k=1}^{G} \\sum_{i=1}^{N_k} (Y_{ik} - \\bar{Y})^2\\]↩︎\n總平方和的一個很好的特點是，我們可以將其分解為兩種不同類型的變異。首先，我們可以談論組內平方和，其中我們檢查每個個體與其所屬組的平均值之間有多大不同 \\[SS_{w}= \\sum_{k=1}^{G} \\sum_{i=1}^{N_k} (Y_{ik} - \\bar{Y}_k)^2\\]，其中 \\(\\bar{Y}_k\\) 是一個組平均值。在我們的例子中，\\(\\bar{Y}_k\\) 將是給予第 k 種藥物的那些人所經歷的平均情緒變化。所以，我們不是將個人與實驗中所有人的平均值進行比較，而是僅將他們與同一組中的人進行比較。因此，您可能會發現 \\(SS_w\\) 的值小於總平方和，因為它完全忽略了任何組之間的差異，即藥物對人們情緒的不同影響。↩︎\n為了量化這種變化的程度，我們要做的是計算組間平方和 \\[ \\begin{aligned} SS_{b} &= \\sum_{k=1}^{G} \\sum_{i=1}^{N_k} ( \\bar{Y}_{k} - \\bar{Y} )^2 \\\\ &= \\sum_{k=1}^{G} N_k ( \\bar{Y}_{k} - \\bar{Y} )^2 \\end{aligned} \\]↩︎\nSS_w 在獨立 ANOVA 中也被稱為誤差變異，即 \\(SS_{error}\\)。↩︎\n在根本上，ANOVA 是兩個不同統計模型之間的競爭，\\(H_0\\) 和 \\(H_1\\)。當我在本節開始時描述虛無假設和替代假設時，關於這些模型實際上是什麼，我有點不精確。我現在將補救這一點，儘管您可能不會因此而喜歡我。如果您回憶一下，我們的虛無假設是所有組均值彼此相同。如果是這樣，那麼考慮結果變項 \\(Y_{ik}\\) 的自然方法是將個體分數描述為單一母體均值 µ，再加上與該母體均值的偏差。這個偏差通常用 \\(\\epsilon_{ik}\\) 表示，傳統上稱為該觀察值的誤差或殘差。但要小心。就像我們在單詞“顯著”中看到的那樣，單詞“誤差”在統計學中具有與它的日常英語定義不完全相同的技術含義。在日常語言中，“誤差”暗示著某種錯誤，但在統計學中則不是這樣（至少不一定是這樣）。考慮到這一點，“殘差”這個詞比“誤差”這個詞更好。在統計學中，這兩個詞都表示“剩餘可變性”，也就是模型無法解釋的“東西”。在任何情況下，當我們將虛無假設寫成統計模型時，它看起來像這樣 \\[Y_{ik}=\\mu+\\epsilon_{ik}\\]，其中我們做出這樣的假設（稍後討論），殘差值 \\(\\epsilon_{ik}\\) 是正態分布的，平均值為 \\(0\\)，標準差 \\(\\sigma\\) 對於所有組都是相同的。使用我們在 [概率簡介] 中介紹的符號，我們將這樣的假設寫成 \\[\\epsilon_{ik} \\sim Normal(0,\\sigma^2)\\] 那麼替代假設 \\(H_1\\) 呢？虛無假設和替代假設之間的唯一區別是，我們允許每個組具有不同的母體均值。因此，如果我們讓 \\(\\mu_k\\) 表示我們實驗中第 k 個組的母體均值，那麼與 \\(H_1\\) 相對應的統計模型是 \\[Y_{ik}=\\mu_k+\\epsilon_{ik}\\]，其中，我們再次假設誤差項是正態分布的，平均值為 0，標準差為 \\(\\sigma\\)。也就是說，替代假設還假定 \\(\\epsilon \\sim Normal(0,\\sigma^2)\\) 好的，既然我們已經更詳細地描述了 \\(H_0\\) 和 \\(H_1\\) 的統計模型，那麼現在很容易說清楚平均平方值是如何衡量的，以及這對於解釋 \\(F\\) 意味著什麼。我不會用證明來煩惱你，但事實證明，組內平均平方 \\(MS_w\\) 可以看作是誤差變異數 \\(\\sigma^2\\) 的估計器。組間平均平方 \\(MS_b\\) 也是估計器，但它估計的是誤差變異數加上一個取決於組均值之間真正差異的數量。如果我們將這個數量稱為 \\(Q\\)，那麼我們可以看到 F 統計量基本上是 \\(^a\\) \\[F=\\frac{\\hat{Q}+\\hat{\\sigma}^2}{\\hat{\\sigma}^2}\\] 其中，如果虛無假設為真，那麼真實值 \\(Q = 0\\)，如果替代假設為真，則 Q &lt; 0（例如， Hays (1994) ，ch. 10）。因此，作為基本要求，\\(F\\) 值必須大於 1 才有可能拒絕虛無假設。需要注意的是，這並不意味著 F 值不可能小於 1。這意味著，如果虛無假設為真，那麼 F 比值的抽樣分布的均值為 1 [^b]，因此，我們需要看到 F 值大於 1 才能安全地拒絕虛無假設。為了更精確地說明抽樣分布，請注意，如果虛無假設為真，那麼 MSb 和 MSw 都是殘差 \\(\\epsilon_{ik}\\) 的變異數的估計量。如果這些殘差是正態分布的，那麼你可能會懷疑 \\(\\epsilon_{ik}\\) 的變異數估計是卡方分布的，因為（如 章节 7.6 中所討論的），這就是卡方分布的含義：當你對一堆正態分布的事物進行平方並將它們相加時，就會得到這樣的分布。而 F 分布（再次，根據定義）就是在兩個 \\(\\chi^2\\) 分布的事物之間取比值時得到的分布，我們就有了我們的抽樣分布。顯然，當我說這些時，我省略了很多東西，但在廣泛的意義上，這確實是我們的抽樣分布來源。  —  \\(^a\\) 如果你已經提前閱讀 章节 14 ，並查看了如何用 \\(\\alpha_k\\) 值來定義因子水平 k 上的「處理效應」（參見平衡設計，允許交互作用的因子 ANOVA 2 节），則發現 \\(Q\\) 是處理效應平方加權平均值，\\(Q = \\frac{(\\sum_{k=1}^{G}N_k \\alpha_k^2)}{(G-1)}\\)  \\(^b\\) 或者，如果我們想要非常精確，是 \\(1+\\frac{2}{df_2-2}\\)。↩︎\n或者，確切地說，像 “1899年那樣狂歡，當時我們沒有朋友，也沒有比做一些計算更好的事情可做，因為直到 1920 年左右，ANOVA 都不存在。”↩︎\n在 Excel clinicaltrial-anova.xls 中，SSb 的值與上文中顯示的值（取整誤差！）略有不同，為 \\(3.45\\)。↩︎\n與上文中的數字相比，jamovi 的結果更為準確，這是由於四捨五入誤差。↩︎\n那麼，讓R_{ik}表示給第k個組的第i個成員的排名。現在，讓我們計算\\(\\bar{R}_k\\)，即第k個組觀察值的平均排名： \\[\\bar{R}_k=\\frac{1}{N_k}\\sum_i R_{ik}\\]，讓我們也計算\\(\\bar{R}\\)，即總平均排名：\\[\\bar{R}=\\frac{1}{N}\\sum_i\\sum_k R_{ik}\\] 現在我們已經做了這些，我們可以計算與總平均排名\\(\\bar{R}\\)的平方偏差。當我們對個別分數進行這種計算時，即如果我們計算\\((R_{ik} - \\bar{R})^2\\)，那麼我們得到的是一個“非參數”的度量，用於表示第ik個觀察值與總平均排名的偏差程度。當我們計算組均值與總均值的平方偏差時，即如果我們計算\\((R_{ik} - \\bar{R})^2\\)，那麼我們得到的是一個非參數度量，用於表示該組與總平均排名的偏差程度。考慮到這一點，我們將遵循與ANOVA相同的邏輯，並定義我們的排名平方和度量，就像我們之前所做的那樣。首先，我們有我們的“總排名平方和”\\[RSS_{tot}=\\sum_k\\sum_i (R_{ik}-\\bar{R})^2\\]，我們可以像這樣定義“組間排名平方和” \\[\\begin{aligned} RSS_{b}& =\\sum{k}\\sum_{i}(\\bar{R}_{k}-\\bar{R})^2 \\\\ &= \\sum_{k} N_k (\\bar{R}_{k}-\\bar{R})^2 \\end{aligned}\\] 因此，如果虛無假設成立，且根本沒有真正的組差異，則您會期望組間排名和\\(RSS_b\\)非常小，遠小於總排名和\\(RSS_{tot}\\)。從質量上看，這與我們在構建ANOVA F-統計量時發現的非常相似，但出於技術原因，Kruskal-Wallis檢驗統計量通常表示為K，其構建方式略有不同，\\[K=(N-1) \\times \\frac{RSS_b}{RSS_{tot}}\\] 如果虛無假設成立，那麼K的抽樣分布近似為自由度為\\(G-1\\)的卡方分布（其中\\(G\\)為組的數量）。 K的值越大，數據與虛無假設的一致性就越小，因此這是一個單邊檢驗。當K足夠大時，我們拒絕\\(H_0\\)。↩︎\n（n-k）：（受試者數量-組別數量）↩︎\n就像其他章節，本章內容有許多參考來源，其中原作者參考最多的專書是 Sahai & Ageel (2000) 。這本書對初學者來說偏難，不過如果學到這裡，想知道更多變異數分析的數學原理，這本書是不錯的參考資源。↩︎"
  },
  {
    "objectID": "14-Factorial-ANOVA.html#footnotes",
    "href": "14-Factorial-ANOVA.html#footnotes",
    "title": "14  多因子變異數分析",
    "section": "",
    "text": "下標表示法的好處在於它可以很好地推廣。如果我們的實驗涉及到第三個因子，那麼我們可以添加第三個下標。原則上，表示法可以擴展到您可能想要包括的任意多個因子，但在本書中，我們很少考慮涉及兩個以上因子的分析，也絕不超過三個。↩︎\n技術上說，邊際化與普通均值並不完全相同。邊際化是一個加權平均數，在其中您需要考慮到您正在平均的不同事件的頻率。然而，在一個平衡設計中，我們所有的單元格頻率在概念上都是相等的，所以兩者是等價的。我們稍後將討論不平衡設計，當我們這樣做時，您將看到我們所有的計算都變得非常令人頭痛。但現在讓我們先忽略這一點。↩︎\n現在我們的表示法已經確定，我們可以用相對熟悉的方式計算每個因子的平方和值。對於因子 A，我們通過評估（行）邊緣均值 \\(\\bar{Y}_{1.} , \\bar{Y}_{2.}\\) 等與總體均值 \\(\\bar{Y}_{..}\\) 之間的差異來計算組間平方和。我們用與單因子方差分析相同的方法做到這一點：計算 \\(\\bar{Y}_{i.}\\) 值和 \\(\\bar{Y}_{..}\\) 值之間的平方差。具體來說，如果每組有 N 個人，那麼我們計算這個 \\[SS_A=(N \\times C)\\sum_{r=1}^R (\\bar{Y}_{r.}-\\bar{Y}_{..})^2\\] 與單因子方差分析一樣，這個公式最有趣的部分是 ^a 部分，它對應於與第 r 級相關的平方偏差。這個公式所做的就是計算因子的所有 R 個水平的這個平方偏差，加起來，然後將結果乘以 \\(N \\times C\\)。這最後一部分的原因是我們的設計中有多個細胞在因子 A 上有水平 \\(r\\)。事實上，它們有 C 個，每個可能的因子 B 水平對應一個。例如，在我們的例子中，設計中有兩個不同的細胞對應於 anxifree 藥物：一個對於沒有治療的人，另一個對於 CBT 組。不僅如此，在這些細胞中的每一個中都有 N 個觀察值。因此，如果我們想將我們的 SS 值轉換為一個在“每次觀察”的基礎上計算組間平方和的數量，我們必須乘以 \\(N \\times C\\)。因子 \\(B\\) 的公式當然也是一樣的，只是一些下標被打亂了 \\[SS_B=(N \\times R)\\sum_{c=1}^C (\\bar{Y}_{.c}-\\bar{Y}_{..})^2\\] 現在我們有了這些公式，我們可以將它們與早期部分的 jamovi 輸出進行對比。再次強調，專用的電子表格程序對於這類計算很有幫助，所以請自己動手試試。您還可以查看我在 Excel 文件clinicaltrial_factorialanova.xls中完成的版本。首先，讓我們計算與藥物主要效果相關的平方和。每個組中共有 \\(N = 3\\) 人，並且有 \\(C = 2\\) 種不同類型的治療。換句話說，共有 \\(3 \\times 2 = 6\\) 人接受了任何特定的藥物。當我們在電子表格程序中進行這些計算時，我們得到了一個與藥物主要效果相關的平方和值為 3.45。並不奇怪，這與您在我之前呈現的 ANOVA 表中查看藥物因子的 SS 值時獲得的數字相同，在 图 14.3。  我們可以對治療效果重複相同類型的計算。同樣，每個組中有 \\(N = 3\\) 人，但由於有 \\(R = 3\\) 種不同的藥物，這次我們注意到有 \\(3 \\times 3 = 9\\) 人接受了 CBT 以及另外 9 人接受了安慰劑。因此，在這種情況下，我們的計算給出了與治療主要效果相關的平方和值為 \\(0.47\\)。同樣，我們並不驚訝地看到我們的計算與 图 14.3 中的 ANOVA 輸出相同。我們可以對治療效果重複相同類型的計算。同樣，每個組中有 \\(N = 3\\) 人，但由於有 \\(R = 3\\) 種不同的藥物，這次我們注意到有 \\(3 \\times 3 = 9\\) 人接受了 CBT 以及另外 9 人接受了安慰劑。因此，在這種情況下，我們的計算給出了與治療主要效果相關的平方和值為 \\(0.47\\)。同樣，我們並不驚訝地看到我們的計算與 图 14.3 中的 ANOVA 輸出相同。  這就是您如何計算兩個主要效果的 SS 值。這些 SS 值類似於我們在 章节 13 中進行單因素方差分析時計算的組間平方和值。然而，將它們視為組間 SS 值並不是個好主意，因為我們有兩個不同的分組變量，容易混淆。然而，為了構建一個 \\(F\\) 檢驗，我們還需要計算組內平方和。為了與我們在 章节 12 中使用的術語以及 jamovi 在打印 ANOVA 表時使用的術語保持一致，我將開始將組內 SS 值稱為殘差平方和 \\(SS_R\\)。在這種情況下，我認為考慮殘差 SS 值的最簡單方法是將其視為在考慮邊際均值差異（即去除 \\(SS_A\\) 和 \\(SS_B\\) ）後結果變量中剩餘的變異。我的意思是，我們可以首先計算總平方和，我將其標記為 \\(SS_T\\)。這個公式與單因素方差分析時的公式幾乎相同。我們將每個觀察值 Yrci 與總平均數 \\(\\bar{Y}_{..}\\) 之間的差異平方，然後將它們全部相加 \\[SS_T=\\sum_{r=1}^R \\sum_{c=1}^C \\sum_{i=1}^N (Y_{rci}-\\bar{Y}_{..})^2\\] 這裡的“三重求和”看起來比實際上複雜。在前兩個求和中，我們對因子 \\(A\\) 的所有級別進行求和（即在我們的表格中的所有可能行 \\(r\\) ）並對因子 \\(B\\) 的所有級別進行求和（即所有可能列 \\(c\\) ）。每個 rc 組合對應一個單獨的組，每個組包含 \\(N\\) 人，所以我們也必須對所有這些人（即所有 \\(i\\) 值）進行求和。換句話說，我們在這裡做的只是對數據集中的所有觀察值進行求和（即所有可能的 rci 組合）。在這一點上，我們知道結果變量的總變異性 SST，並且知道這些變異性中有多少可以歸因於因子 A（\\(SS_A\\)）以及有多少可以歸因於因子 B（\\(SS_B\\)）。因此，殘差平方和被定義為無法歸因於我們兩個因子的 \\(Y\\) 變異。換句話說 \\[SS_R=SS_T-(SS_A+SS_B)\\] 當然，您可以使用一個公式直接計算殘差 SS，但我認為從概念上這樣考慮更有意義。將其稱為殘差的全部意義在於它是剩餘變異，上面的公式說明了這一點。我還應該指出，按照回歸章節中使用的術語，我們通常將 \\(SS_A + SS_B\\) 稱為“ANOVA 模型”可歸因的變異，用 SSM 表示，因此我們經常說總平方和等於模型平方和加殘差平方和。稍後在本章中，我們將看到這不僅僅是表面相似性：在底層，ANOVA 和回歸實際上是相同的事物。無論如何，檢查我們是否可以使用此公式計算 \\(SS_R\\) 並驗證我們是否可以獲得與 jamovi 在其 ANOVA 表中生成的相同答案可能是值得的。在電子表格中完成的計算非常簡單（參見 clinicaltrial_factorialanova.xls 文件）。我們可以使用上面的公式計算總 SS（獲得總 SS = 4.85 的答案），然後計算殘差 SS（= 0.92）。再一次，我們得到了相同的答案。 —  \\(^a\\) 英文翻譯：“最不繁瑣”。↩︎\n因此，將交互作用效應的概念用零假設和替代假設的條款來正式表示的方式有點困難，我猜想這本書的很多讀者可能對此並不感興趣。儘管如此，我將在此嘗試介紹基本概念。首先，我們需要更明確地說明我們的主要效果。考慮因子\\(A\\)（我們的運行示例中的藥物）的主要效果。我們最初根據兩個邊際均值\\(\\mu_r\\)相等的零假設來表述這一點。顯然，如果所有這些值彼此相等，那麼它們也必須等於總均值\\(\\mu_{..}\\)，對吧？所以我們可以做的是將因子\\(A\\)在水平\\(r\\)的效應定義為邊際均值\\(\\mu_{r.}\\)和總均值\\(\\mu_{..}\\)之間的差。讓我們用\\(\\alpha_r\\)表示這個效果，並注意到 \\[\\alpha_r=\\mu_{r.}-\\mu_{..}\\] 現在，根據定義，所有的\\(\\alpha_r\\)值必須相加為零，原因是邊際均值\\(\\mu_c\\)的平均值必須等於總平均值\\(\\mu_{..}\\)。我們可以類似地將因子B在\\(c\\)水平的效應定義為列邊際均值\\(\\mu_{.c}\\)和總均值\\(\\mu_{..}\\)之間的差： \\[\\beta_c=\\mu_{.c}-\\mu_{..}\\]同樣地，這些\\(\\beta_c\\)值必須相加為零。統計學家有時喜歡用這些\\(\\alpha_r\\)和\\(\\beta_c\\)值來談論主要效果，因為這樣他們可以明確地說明無交互作用效應意味著什麼。如果完全沒有交互作用，那麼這些\\(\\alpha_r\\)和\\(\\beta_c\\)值將完美地描述組均值\\(\\mu_{rc}\\)。具體來說，這意味著\\[\\mu_{rc}=\\mu_{..}+\\alpha_{r}+\\beta_{c}\\]也就是說，僅僅通過了解所有邊際均值，您就能完全預測到組均值。這就是我們的零假設。替代假設是\\[\\mu_{rc} \\neq \\mu_{..}+\\alpha_{r}+\\beta_{c}\\]至少對於表格中的一個組\\(rc\\)而言。然而，統計學家通常喜歡稍微不同地寫這個。他們通常會定義與組\\(rc\\)相關的特定交互作用為一個數字，笨拙地稱為\\((\\alpha \\beta)_{rc}\\)，然後他們將說替代假設是\\[\\mu_{rc}=\\mu_{..} +\\alpha_{r} +\\beta_{c} + (\\alpha \\beta )_{rc}\\]其中\\((\\alpha \\beta)_{rc}\\)對於至少一個組是非零。這種表示法看起來有點難看，但在討論如何計算平方和時它非常方便。我們應該如何計算交互項的平方和，\\(SS_{A:B}\\)？首先，有助於注意我們如何剛剛根據實際組均值與僅查看邊際均值所預期的程度的差異來定義交互作用效應。當然，所有這些公式都是指群體參數而非樣本統計數據，因此我們實際上不知道它們是什麼。但是，我們可以通過將樣本均值代替群體均值來估計它們。因此，對於因子\\(A\\)，在\\(r\\)水平的主要效果的一個好的估計方法是樣本邊際均值\\(\\bar{Y}_{rc}\\)與樣本總均值\\(\\bar{Y}_{..}\\)之間的差。也就是說，我們將使用這個作為效果的估計\\[\\hat{\\alpha}_r = \\bar{Y}_{r.}-\\bar{Y}_{..}\\]同樣地，我們可以將因子\\(B\\)在\\(c\\)水平的主要效果的估計定義為以下\\[\\hat{\\beta}_{c}=\\bar{Y}_{.c}-\\bar{Y}_{..}\\]現在，如果您回顧一下我用來描述兩個主要效果的\\(SS\\)值的公式，您會注意到這些效果值正是我們正方形和求和的量！那麼，對於交互項來說，這個類比是什麼呢？通過首先重新排列替代假設下組均值\\(\\mu_{rc}\\)的公式，我們可以找到這個答案，以便我們得到這個\\[\\begin{aligned} (\\alpha \\beta)_{rc} & = \\mu_{rc} - \\mu_{..} - \\alpha_r - \\beta_c \\\\ & = \\mu_{rc} - \\mu_{..} - (\\mu_{r.}-\\mu_{..})-(\\mu_{.c}-\\mu_{..}) \\\\ & = \\mu_{rc} - \\mu_{r.} - \\mu_{.c} +\\mu_{..} \\end{aligned}\\]因此，再次將樣本統計數據代替群體均值，我們將得到以下作為組\\(rc\\)的交互作用效應的估計，即\\[(\\hat{\\alpha \\beta})_{rc}=\\bar{Y}_{rc}-\\bar{Y}_{r.}-\\bar{Y}_{.c}+\\bar{Y}_{..}\\]現在我們所要做的就是將所有這些估計加起來，涵蓋因子\\(A\\)的所有\\(R\\)水平和因子\\(B\\)的所有\\(C\\)水平，然後我們得到以下公式，與整個交互作用相關的平方和\\[SS_{A:B}=N \\sum_{r=1}^R \\sum_{c=1}^C (\\bar{Y}_{rc}-\\bar{Y}_{r.}-\\bar{Y}_{.c}+\\bar{Y}_{..})^2\\]這裡，我們將其乘以N，因為每個組中都有N個觀察值，我們希望我們的\\(SS\\)值反映交互作用解釋的觀察值變異，而不是組之間的變異。現在我們有了計算\\(SS_{A:B}\\)的公式，重要的是要認識到交互項是模型的一部分（當然），所以與模型相關的總平方和，SSM，現在等於三個相關的SS值之和，\\(SS_A + SS_B + SS_{A:B}\\)。剩餘平方和SSR仍然被定義為剩餘變異，即\\(SS_T - SS_M\\)，但現在我們有了交互項，這變成了\\[SS_R=SS_T-(SS_A+SS_B+SS_{A:B})\\] 因此，剩餘平方和\\(SS_R\\)將小於我們原先沒有交互作用的ANOVA。↩︎\n當我們稍早前在jamovi中查看主效應分析時，您可能已經發現了這一點。為了本書中的解釋，我從之前的模型中刪除了交互組件，以保持清晰簡單。↩︎\n這一章似乎創下了R字母可以代表的不同事物數量的新紀錄。到目前為止，我們有R指的是軟件包、我們的平均數表中的行數、模型中的殘差，以及現在的回歸中的相關係數。抱歉。顯然我們的字母表中字母不夠多。但是，我已經努力地澄清了在每種情況下R指的是哪一個東西。↩︎\n您可能會問，治療對比和簡單對比之間有什麼區別？以性別主效應為例，其中 \\(m=0\\) 和 \\(f=1\\)。治療對比相應的係數將衡量女性和男性之間的平均值差異，截距將是男性的平均值。然而，對於簡單對比，即 \\(m=-1\\) 和 \\(f=1\\)，截距是平均值的平均數，主效應是每個組平均值與截距的差異。↩︎\n如果，例如，你實際上想知道 A 組與 B 組和 C 組的平均值之間是否存在顯著差異，那麼你需要使用另一種工具（例如，更保守的 Scheffe’s 方法，這超出了本書的範疇）。然而，在大多數情況下，您可能對成對組別的差異感興趣，所以 Tukey 的 HSD 是一個相當有用的工具。↩︎\n這種標準偏差的差異可能會（並且應該）讓您懷疑我們是否違反了變異數同質性假設。我將使用 Levene 檢驗選項將其作為讀者的練習來仔細檢查這一點。↩︎\n實際上，這有點誇張。除了我在本書中討論的那些方面之外，ANOVA 還可以有其他變化。例如，我完全忽略了固定效應模型（其中因子的水平是由實驗者或世界“固定”的）和隨機效應模型（其中水平是來自可能水平的更大群體的隨機樣本）之間的區別（本書僅涵蓋固定效應模型）。不要犯誤認為這本書或任何其他書籍將告訴您關於統計的“您需要知道的一切”，正如一本書可能告訴您關於心理學、物理學或哲學的一切。生活太複雜了，以至於這永遠不會是真的。然而，這並非絕望的原因。大多數研究者只憑本書所涵蓋的基本 ANOVA 工作知識就能應對。我只是希望您記住，這本書只是一個非常長的故事的開始，而不是整個故事。↩︎\n或者，至少很少有趣。↩︎\n但是，在 jamovi 中，類型 III 和平方 ANOVA 的結果與所選對比無關，所以 jamovi 顯然在做一些不同的事情！↩︎\n當然，這確實取決於用戶指定的模型。如果原始 ANOVA 模型不包含 \\(B \\times C\\) 的交互作用項，那麼顯然它不會出現在零假設或對立假設中。但這對類型 I、II 和 III 都是如此。它們永遠不會包含您未包含的任何條款，但是在您包含的條款中，它們在構建測試方面做出了不同的選擇。↩︎\n我覺得值得一提的是，R 中的默認值是類型 I，而 SPSS 和 jamovi 中的默認值是類型 III。這兩者都不是我特別喜歡的。此外，我覺得令人沮喪的是，在心理學文獻中，幾乎沒有人報告他們運行了哪種類型的測試，更不用說變量的順序（對於類型 I）或用於類型 III 的對比了。通常他們也不報告使用了什麼軟件。我能從人們通常報告的信息中理解的唯一方法就是嘗試從輔助線索猜測他們使用了哪款軟件，並假設他們從未更改過默認設置。請不要這樣做！現在您了解了這些問題，請確保說明您使用了什麼軟件，如果您報告了不平衡數據的 ANOVA 結果，那麼請指定您運行了哪種類型的測試，指定類型 I 測試的順序信息以及類型 III 測試的對比。或者，更好的是，做出與您真正關心的事物相對應的假設檢驗，然後報告這些！↩︎"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#footnotes",
    "href": "02-A-brief-introduction-to-research-design.html#footnotes",
    "title": "2  研究設計入門",
    "section": "",
    "text": "節錄自1938年，第一屆印度統計學會主席演講。來源：http://en.wikiquote.org/wiki/Ronald Fisher↩︎\n譯註~英文版提到“測量程序”與”測量值“都是以”measurement”稱呼，中文版根據主題及脈絡以不同名稱區別。↩︎\n譯注~根據名詞解說，此處中譯為”測量程序“。↩︎\n譯注~本書的多數單元將variable翻成”變項”，除了統計理論的單元大多會翻成“變數”。因為”變項”是資料的，“變數”是數學的。↩︎\n&lt;原作者的補充&gt;其實曾有物理學大神告訴我，溫度嚴格來說不是等距尺度變項，因為只要環境溫度至少有3\\(^{\\circ}\\) ，任何物體都可以加熱。雖然在物理學來說，這並不是一個講解等距尺度好例子，不過為了方便說明，這裡暫且無視物理事實。↩︎\n譯註~原著並未提供變項符號，表內符號取自jamovi說明文件。↩︎\n譯註~原文的表2-4並未說明jamovi符號，改編是為了幫助讀者了解後續章節的操作說明，建議下載示範檔案並搭配中文版示範影片閱讀。↩︎\n譯註~中文化作者認為還有第三個理由。由於有些統計教程混淆測量尺度與間斷/連續變項的切換關係，造成學習者低估機率論在學習統計的重要性。在第7章，我們會了解間斷/連續變項的數學意義。↩︎\n譯註~到本書最新版上線的時間，還沒有一種有效的應用程式或人工智慧程序能幫助人類判斷適用分析任務的資料變項型態，請參考這份示範檔案的中文版示範影片。↩︎\n譯註~原文的題目不是這一句，中文版改成接地氣的範例。↩︎\n唉，心理學的問題從來沒有一種能簡單回答的方式！↩︎\n然而讓像我一樣的老師氣惱的是，不同行為科學領域會使用各自習慣的名稱。本書不會列出所有領域的名稱，因為知道了也沒有意義。在此只舉一例：有時你會看到“反應變項”，就是本書所稱的的“結果變項”。唉，未來讀越多報告，你會遇到各式各樣搞混你的變項名稱。(這段附註強調的是現實情況中會出現許多不同的變項名稱，可能造成概念和術語的混淆。作者列舉了“反應變項”有時也會用來指稱“結果變項”，但沒有羅列所有可能的名稱變體，因為這沒有意義。最後表達了這類術語混亂現象很常見的無奈。)↩︎\n雖然有些混淆因素通常沒有被正式測量，是因為如果測量尺度的紀錄包含其影響，那麼我們可以使用一些高深的統計技巧來處理混淆因素的效果。因為有這些用來解決混淆因素影響的統計方法，研究者經常在報告裡稱呼經過測量和處理的混淆因素為調和變項。處理調和變項是更高一層的課題，這裡只是順便提一下，因為知道這些會激勵學習動機。↩︎\n有人可能會認為，如果不夠誠實就不可能成為真正的科學家。這在某種程度上確實是真的，但並不是完全正確(請搜尋“非真正的蘇格蘭人”謬論)。事實上，有很多人被僱用來充當科學家，他們的履歷表有科學訓練經歷，但是做的事是徹頭徹尾笉欺詐。只說他們不是科學家，裝作他們不存在只是鴕鳥心態。↩︎\n很明顯，真實狀況是只有缺乏穩定心理狀態的人才會讀完整本《芬尼根的守靈夜》↩︎"
  },
  {
    "objectID": "03-Getting-started-with-jamovi.html#footnotes",
    "href": "03-Getting-started-with-jamovi.html#footnotes",
    "title": "3  與jamovi的第一次接觸",
    "section": "",
    "text": "jamovi的最早版本於2018年八月問世。這本電子書將隨最新版本配合更新。↩︎\n譯註 ~ jamovi開發團隊於2021年推出不必安裝，直接在網路瀏覽器使用的雲端測試版。雲端版與本機版的操作特點在本書後續介紹。↩︎\n雖然jamovi經常更新，但是這本書的示範內容不會有太大變化。原作者編寫時已經多次修改，之後的電子書內容更新不會差異太大。↩︎\njamovi內建四種編輯變項內數值的功能，都可以在Data面板找到對應功能按鈕，使用者可依實務目的選擇使用 ~ Setup編輯已載入的資料變項；Compute新建自訂計算變項；Transform依照已存的資料變項或計算變項，設定數值轉換函式並產生新的資料變項；Filters設定排除資料變項內部分數值的條件並設定是否啟動。↩︎\n新版jamovi有內建函式Z可計算數值資料變項的z分數，使用者不必再自訂計算變項公式。 ↩︎\n譯註~雲端版操作方法相同。↩︎\n譯註~原作者提供jamovi擴充模組”lsj-data”。本機版需要自行安裝，雲端版已經內建。↩︎\n你可以從設定面板(主畫面右上角三個小點圖示按鈕)更改遺漏值的代表記號，也就是更改”Default missings”欄內的記號。設定原則是不可讓遺漏值的記號等於資料內的有效數值，例如-9999。↩︎\n這些步驟聽起來很麻煩，不過未來的更新版還沒有完美的解決之前，這是比較有保障的做法。↩︎\n譯註~雲端版分為免費的GUEST PLAN與付費訂閱的PRIORITY PLAN。只有PRIORITY PLAN能安裝擴充模組，但是不包括\\(R_j\\)。↩︎"
  },
  {
    "objectID": "07-Introduction-to-probability.html#footnotes",
    "href": "07-Introduction-to-probability.html#footnotes",
    "title": "8  機率入門",
    "section": "",
    "text": "譯註~許多機率與統計的教科書或科普書，將frequentist翻成“頻率主義”。由於frequency在英文指涉事件發生的空間與時間，配合動詞時態，能在語境裡理解frequency是指空間或時間的事件變動。然而中文動詞無時態，英文原句的frequency如果沒有指涉時間的變化，直翻為”頻率”不但不夠精確，在許多語境還可能造成誤解。由於在行為科學的統計學應用場景，「時間」是較少被提及的因素，因此本書中文版以“次數主義”稱呼。在各單元的中文翻譯，會根據語境調整用詞。↩︎\n這當然並不是說次數主義觀點的方法論不能做假設性陳述。只是如果您想使用機率模型表達假設,可能必須將假設的內容重新定義為潛在可觀測事件的次數序列，以及序列的各種結果的相對次數。↩︎\n譯註~數學家版的貝氏定理\\[Pr(A|B) = \\frac{Pr(B|A)*Pr(A)}{Pr(B)}\\] 統計學家版的貝氏定理\\[Pr(H_i|Data) = \\frac{Pr(Data|H_i)*Pr(H_i)}{\\sum_{j=1}^n Pr(Data|H_j)*P r(H_j)}\\] 兩套公式取自旗標出版AI 必須！從做中學貝氏統計。↩︎\n請注意,這裡的“成功”是武斷的命名,並不是說出現這樣的結果是我所希望的。 如果 \\(\\theta\\) 指的是某位機車騎士因為交通事故受傷的機率,我還是會用成功機率稱呼,但這並不是說我希望有人因為交通事故受傷!↩︎\n請注意,這裡的“成功”是武斷的命名,並不是說出現這樣的結果是我所希望的。 如果 \\(\\theta\\) 指的是某位機車騎士因為交通事故受傷的機率,我還是會用成功機率稱呼,但這並不是說我希望有人因為交通事故受傷!↩︎\n如果讀者知道一點微積分,我會用一種稍微精確方式解釋。如同離散機率函數涵蓋所有事件的機率，其值為必為正數且總和為1，連續機率函數涵蓋範圍內非零的機率密度且總和也是1。要計算代表事件a與b之間所有數值的發生機率，只要以密度函數計算兩者之間機率密度所佔面積的積分\\(\\int_{a}^{b} p(x) dx\\)。若是讀者從未學過或已經忘記微積分,請不用擔心這個，本書的材料不需要懂微積分也能學習。↩︎\n如同二項分佈,在此列出常態分佈的隨機變數公式,是因為這非常重要,每個學習統計學的同學都應該至少看一眼。由于這是一本簡介教材,為了不做深入介紹,完整公式隱藏於這個脚注: \\[p(X|\\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(X-\\mu)^2}{2\\sigma^2}}\\]↩︎\n常態分佈在統計分析實務非常好用,使得就算變數實際上並不是連續時，分析人員也會使用。只要構成結果變項的選項够多(像是很多問卷常用的李克特量表),以常態分佈作為近似的資料樣本分佈是相當標準的做法。這可能讓分析實務變得簡單。↩︎\n如果讀者知道一點微積分,我會用一種稍微精確方式解釋。如同離散機率函數涵蓋所有事件的機率，其值為必為正數且總和為1，連續機率函數涵蓋範圍內非零的機率密度且總和也是1。要計算代表事件a與b之間所有數值的發生機率，只要以密度函數計算兩者之間機率密度所佔面積的積分\\(\\int_{a}^{b} p(x) dx\\)。若是讀者從未學過或已經忘記微積分,請不用擔心這個，本書的材料不需要懂微積分也能學習。↩︎"
  },
  {
    "objectID": "08-Estimating-unknown-quantities-from-a-sample.html#footnotes",
    "href": "08-Estimating-unknown-quantities-from-a-sample.html#footnotes",
    "title": "8  運用樣本估計未知量數",
    "section": "",
    "text": "譯註~原文的例子已換成較通用的例子，置換理由請見中場故事的翻譯說明。↩︎\n隨機的數學推導相當繁複，並且已經超出本書的學習範圍。本書不依靠數學公式解說隨機程序，讀者只要理解取樣程序可以不斷重覆，每次取樣結果都不一樣就是一種隨機程序。↩︎\n現實世界沒有那麼簡單。很難明確地將所有人類劃分為二元類別，如“思覺失調症患者”和“非思覺失調症患者”。但這不是一本臨床心理學教科書，請原諒我在書中各處的簡化用詞。↩︎\n技術上來說，大數法則適用於任何一種形式如同算術平均值的樣本統計量數，樣本平均值當然適用。但是，其他樣本統計量數也可以寫成像平均值的形式。例如，樣本的變異數可以改寫為一種算術平均值，因此也受到大數法則的影響。然而，樣本的最小值無法寫成任何平均值的形式，因此不受大數法則的支配。↩︎\n本書正文寫的描述都是有點簡化的。中心極限定理的應用範圍比這一節展示更普遍。像大多數介紹統計學的教科書一樣，本書只討論了一種中心極限定理成立的情況：就是從相同的分佈中取出很多獨立事件的平均值。然而，中心極限定理可適用更廣泛的情況。例如，有一種“U統計量數”，這類統計量都滿足中心極限定理的條件 ，因此在處理大樣本數的狀況，U統計量數的取樣分佈都會是常態分佈。平均值是一種符合定理的統計量數，但它不是唯一的。↩︎\n請注意，如果你真的對想研究這個問題，你必須比我這裡描述的研究方式更加謹慎。不能只比較Whyalla和Port Pirie的智力測驗分數，並假設任何差異都是由鉛中毒引起的。即使這兩個城鎮之間的唯一差異確實是不一樣的精煉廠（實際情況不只如此），也需要考慮到人們已經相信鉛污染會導致認知偏誤。 復習一下@sec-A-brief-introduction-to-research-design 談到的動機效應，Port Pirie和Whyalla兩地的樣本很 可能存在不同的需求效應。換句話說，您可能會在數據中得出虛假的群體差異，因為人們認為存在真實差異。我認為這種想法相當不可信，如果一群穿著實驗袍的研究人員帶著智力測驗來到Port Pirie，當地人也許不會在乎你的真正目的，甚至會表現強烈的抗拒感，不會配合研究者進行測試。另一群Port Pirie的居民可能會有表現良好的動機，因為他們不希望他們的家鄉看起來很糟糕。Whyalla的動機效應可能會更弱，因為人們沒有任何關於“鐵礦污染”的概念，就像他們沒有“鉛污染”的概念一樣。心理學真的不是簡單的學問。↩︎\n這裡有些細節我並未明確解釋。不偏性(unbiasedness)對於估計值來說是一個理想的特徵，但除了這個特徵之外，還有其他重要因素也要考量。然而，這方面的細節已經超出本書討論的範圍。我只是想提醒同學注意這裡存在一些尚未明說的複雜條件 。↩︎\n除以 \\(N-1\\) 可以得到一個不偏的母群變異數估計值：\\[\\hat{\\sigma}^2=\\frac{1}{N-1}\\sum_{i=1}^{N}(X_i - \\bar{X})^2\\] 以及不偏的母群標準差估計值：\\[\\hat{\\sigma}=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(X_i-\\bar{X})^2}\\] 好啦，這裡我少講了另外一些東西。 經過一些怪異且違反直覺的推導，我們會認為取平方根是沒問題的，自然認為既然 \\(\\hat{\\sigma}^2\\) 是 \\(\\sigma^2\\) 的不偏估計值，那麼\\(\\hat{\\sigma}\\) 也是\\(\\sigma\\)的不偏估計值。對吧？但奇怪的是並非如此。實際上，\\(\\hat{\\sigma}\\) 存在一種微妙而極小的偏差。 這實在是太奇怪了：\\(\\hat{\\sigma}^2\\) 是母群變異數 \\(\\sigma^2\\) 的不偏估計值，但是開根號後，\\(\\hat{\\sigma}\\) 卻是母群標準差 \\(\\sigma\\) 的有偏誤估計值。很奇怪吧？那麼為什麼 \\(\\hat{\\sigma}\\) 存在偏誤呢？用數學術語來說，原因是「非線性轉換（例如平方根）不相容於期望值」，但是對於沒有修過數理統計課程的同學來說，這聽起來就像外星語言。幸運地是，在實際情況，了不了解這一點並不重要。因為偏誤非常微小，在大多數統計實務使用 \\(\\hat{\\sigma}\\) 做為母群標準差估計值不會有太多問題。有時候試著洞察數學原理只是令人惱火罷了。↩︎\n此箴言的原文散見於許多網站及客製T恤，但是原作者無法找到原始來源。↩︎\n信賴區間的數學公式是：\\[\\mu-(1.96 \\times SEM ) \\leq \\bar{X} \\leq \\mu + (1.96 \\times SEM)\\] 其中SEM等於\\(\\frac{\\sigma}{\\sqrt{N}}\\)，而且我們有95％的信心這段數值範圍是正確的。然而，這並沒有回答我們真正感興趣的問題。以上公式告訴我們，如果我們想知道母群參數是多少，那麼應該計算所樣樣本平均值的期望值。但是，假如要用信賴區間回答這個問題，也就是想知道，計算特定樣本資料後，應該要多相信母群參數是多少，這並不難做到。使用一些高中學過的代數知識就可以巧妙地以上公式推 導如下：\\[\\bar{X}-(1.96 \\times SEM ) \\leq \\mu \\leq \\bar{X}+(1.96\\times SEM )\\] 這公式透露出這段數值範圍有95％的機率包含母群平均值\\(\\mu\\)。我們稱此範圍為95％信賴區間，簡寫為\\(CI_{95}\\)。簡而言之，只要N足夠大（足夠讓我們相信平均值的取樣分佈符合常態分佈），那麼我們可以將95％信賴區間的公式變得更精簡：\\[CI_{95}=\\bar{X} \\pm (1.96 \\times \\frac{\\sigma}{\\sqrt{N}})\\]↩︎\n譯註~重點整理1. 多數行為科學研究的樣本是理論上的簡單隨機樣本，這是為什麼合格的行為科學研究者也要了解如何設計減低或控制取樣偏誤的實驗。2. 我們常假設有興趣的母群參數逼近某種機率函數的參數，包括 \\(\\mu\\)、\\(\\sigma\\)。↩︎\n譯註~重點整理1. 大數法則與中央極限定理是 隨機樣本分佈(或稱取樣分佈)的理論基礎，讓行為科學研究者相信只要是使用隨機方法取得的資料，就是某種取樣分佈的一部分。2. 以取樣分佈估計母群參數的逼近值，是任何行為科學研究都會進行的統計實務。↩︎\n譯註~重點整理1. 取樣分佈的平均值(\\(\\hat{\\mu}\\))是直接逼近母群參數(\\(\\mu\\))的估計值，實務報告都以樣本平均值(\\(\\bar{X}\\))代表取樣分佈的平均值。2. 取樣分佈的標準差(\\(\\hat{\\sigma}\\))與逼近母群參數(\\(\\sigma\\))的不偏估計值(\\(\\hat{\\sigma} = \\frac{\\sigma}{N-1}\\))，不等於直接從資料計算樣本標準差(\\(s = \\frac{\\sigma}{N}\\))。jamovi與其他專業統計軟體的計算結果都是\\(\\hat{\\sigma}\\)。3. 取樣分佈的變異數(\\(\\hat{\\sigma^2}\\))與逼近母群參數(\\(\\sigma^2\\))的不偏估計值(\\(\\hat{\\sigma^2} = \\frac{\\sigma^2}{N-1}\\))，不等於直接從資料計算樣本變異數(\\(s = \\frac{\\sigma^2}{N}\\))。jamovi與其他專業統計軟體的計算結果都是\\(\\hat{\\sigma}\\)。↩︎\n譯註~重點整理信賴區間是次數主義學派的典型估計方法，實務報告規範要提到的百分比(例如95%)並不是指研究結果的發生機率，而是重覆實驗能得到相同結果的相對次數。↩︎"
  },
  {
    "objectID": "16-Bayesian-statistics.html#footnotes",
    "href": "16-Bayesian-statistics.html#footnotes",
    "title": "16  貝氏統計",
    "section": "",
    "text": "http://en.wikiquote.org/wiki/David_Hume.↩︎\nhttp://en.wikipedia.org/wiki/Climate_of_Adelaide↩︎\n我知道,這需要一定的信任,但讓我們繼續吧,好嗎?↩︎\n噢。我很不願意提起這一點,但一些統計學家會反對我在這裡使用“似然性”一詞。問題在於,“似然性”在次數主義統計中有非常具體的含義,而且與它在貝氏統計中的含義不完全相同。據我所知,貝葉斯最初沒有任何公認的名稱來表示似然性,所以人們使用次數主義術語成為常規做法。這本不會成為問題,除非事實證明貝葉斯使用這個詞的方式與次數主義者的方式相當不同。這裡不是再來一堂冗長的歷史課的地方,但粗略地說,當貝葉斯說“一個似然函數”時,他們通常是指表中的一行。當次數主義者說同樣的事情時,他們指的是同一個表,但對他們來說,“一個似然函數”幾乎總是指的是其中一列。這種區別在某些語境中很重要,但對我們的目的來說並不重要。↩︎\n為明確起見,“先驗”信息是預先存在的知識或信念,在我們收集或使用任何數據來改善該信息之前。↩︎\n如果我們更複雜一些,我們可以擴展示例以容納我撒謊關於雨傘的可能性。但讓我們保持簡單,可以嗎?↩︎\n您可能會注意到,這個方程式實際上是我在上一節開頭列出的同一個基本規則的重述。 如果您將等式兩邊都乘以 \\(P(d)\\),那麼您得到 \\(P(d)P(h|d)=P(d,h)\\),這是聯合機率的計算規則。 所以我在這裡實際上並沒有引入任何“新的”規則,我只是以不同的方式使用了相同的規則。↩︎\n顯然,這是一個高度簡化的故事。 真實世界中貝氏假設檢定的所有複雜性都歸結為當假設 h 是一个複雜且模糊的事物時如何計算似然性 \\(P(d\\|h)\\)。 我不打算在這本書中討論這些複雜性,但我確實想要強調,儘管這個簡單的故事在目前是正確的,但現實生活比我能夠在入門統計教科書中涵蓋的要複雜得多。↩︎\nhttp://www.imdb.com/title/tt0093779/quotes 。我應該指出,我不是第一個使用這句話來抱怨次數學派方法的人。Rich Morey和他的同事們首先有了這個想法。我厚顏無恥地偷了它,因為在這種情況下使用它是個非常棒的引文,而且我絕不會錯過任何引用《公主新娘》的機會。↩︎\nhttp://about.abc.net.au/reports-publications/appreciation-survey-summary-report-2013/↩︎\nhttp://knowyourmeme.com/memes/the-cake-is-a-lie .↩︎\n為了完全誠實,我應該承認並非所有正統統計檢驗都依賴這個愚蠢的假設。有時在臨床試驗等中會使用一些序列分析工具。這些方法建立在假設數據在到達時即進行分析的基礎上,並且這些檢驗在本章中指出的方式中沒有嚴重失誤。然而,序列分析方法的構建與“標準”版虛無假設檢定截然不同。它們沒有進入任何介紹教科書,在心理學文獻中也很少使用。我在此提出的問題對我目前介紹的每一種正統檢驗和我在文獻中看到的幾乎每一種檢驗都是有效的。↩︎\n一張漫畫解釋這裡談的問題: http://xkcd.com/1478/ .↩︎\n有些讀者可能會質疑為什麼我選擇 3:1 而不是 5:1,因為 Johnson (2013) 建議 \\(p = .05\\) 落在這個範圍內。我這樣做是為了給 p 值一個好處。如果我選擇 5:1 的貝氏因子,結果對於貝氏方法來說會看起來更好。↩︎\nhttp://www.quotationspage.com/quotes/Ambrosius Macrobius/↩︎\n好吧,我知道一些知識豐富的次數主義者會讀這個部分並開始抱怨。 看,我又不傻。 我絕對知道,如果您採用序列分析的角度,您可以在正統框架內避免這些錯誤。 我也知道您可以明確設計考慮過期中分析的研究。 所以是的,從某種意義上說,我正在攻擊正統方法的“稻草人”版本。 然而,我所攻擊的稻草人是幾乎每一個從業者都在使用的版本。 如果有朝一日序列方法在實驗心理學家中成為規範,當我不再被迫每天閱讀 20 個極其可疑的方差分析時,我保證我會重寫這一節並減少苦澀。 但是在那一天到來之前,我堅持我的說法,即默認的貝氏因子方法在面對現實世界中存在的數據分析實踐中,具有更強的穩健性。 默認的正統方法很糟糕,我們都知道。↩︎\n台灣同學可以找旗標出版社出版的〈AI必須! 從做中學貝氏統計〉做為自學材料。這本書援引John Kruschke的說明方式，介紹各種貝氏統計方法理論細節及運算方法。↩︎"
  },
  {
    "objectID": "15-Factor-Analysis.html#footnotes",
    "href": "15-Factor-Analysis.html#footnotes",
    "title": "15  因素分析",
    "section": "",
    "text": "因素負荷量的解讀方式如同標準化迴歸係數。↩︎\n斜交旋轉提供了兩個因子矩陣,一個稱為結構矩陣,一個稱為模式矩陣。 在jamovi中,結果中只顯示模式矩陣,因為這通常對解釋最有用,儘管一些專家建議两者都很有幫助。 在結構矩陣中,係數顯示變項與因素之間的關係,同時忽略該因素與所有其他因素的關係(即零階相關)。 模式矩陣係數顯示因素對變項的獨特貢獻,同時控制該變項上其他因素的影響(類似於標准化偏迴歸係數)。 在正交旋轉下,結構和模式係數是相同的。↩︎\n在因子分析中有時會報告“公因性”,這是變項中能夠被因子解描述的變異量。獨特性等於 (1 \\(\\sim\\) 公因性)↩︎\n必要時先進行某些變項的反向計分↩︎\n此外,若是研究人員對最初“假設”的因素結果相當有信心,可以9跳過EFA而直接進行CFA。要先使用EFA然後進行CFA,還是直接進行CFA,這在很大程度上取決於研究人員最初對模型準確性(就因素和變項數量來源)的判斷和信心。在量表開發的早期階段或確定潛在因素結構的時候,多數研究人員傾向使用EFA。接近完成最終版量表，或者想用新樣本檢查已建立的量表,CFA是一個不錯的選擇。↩︎"
  },
  {
    "objectID": "09-Hypothesis-testing.html#footnotes",
    "href": "09-Hypothesis-testing.html#footnotes",
    "title": "9  假設檢定",
    "section": "",
    "text": "這段話引自維根斯坦於1922年出版的專書邏輯哲學論。↩︎\n技術性註記。這一章的敘述與許多入門教材的常見說明有些微不同。經典的虛無假設檢定理論是20世紀初期，由羅納德·費雪和傑茲·尼曼 兩位統計學者分別發展出來的，但是費雪和尼曼對於理論的運作有非常不同的看法。現今大多數教材所採用的檢定理論是兩種觀點的混合。本書的解說方式較接近尼曼的風格，特別在解釋p值意義的部分。↩︎\n在此對真心相信超感官知覺（ESP）存在的朋友說聲抱歉，根據我（原作者）所知道的ESP相關文獻，我認為這種研究是不切實際的。雖然說其中一些研究的設計是嚴謹的，從心理學研究設計的角度來看，這是一個有趣的研究領域。當然，在一個自由國家，只要你願意，你可以花費時間和精力來證實我錯了，但我認為這真的不是一個發揮個人智慧的實在主題。↩︎\n這種比喻只適用像英國、美國或澳洲等採用訴訟辯護制度的國家。據原作者所知，法國的審判制度是完全不同的。↩︎\n關於描述假設檢定結果會使用的詞彙，有一些需要注意的地方。首先，建議要避免使用「證明」(prove)這個詞。統計檢定不能證明一個假設是真的或假的，因為「證明」暗示絕對的確定性，但是統計學中永遠沒有絕對的確定性。這點幾乎所有懂統計的人都會同意(譯註:本書中文版一律用「證實」)。然而，還有一些讓人混淆的用語。有些人認為，我們只可以說「拒絕虛無假設」、「未能拒絕虛無假設」或「保留虛無假設」。按照這種思路，我們不可以說「接受對立假設」或「接受虛無假設」。對我而言，這樣說法太過嚴格了。我認為這混淆了虛無假設檢定和卡爾·波普認為科學研究是證偽過程的觀點。儘管證偽主義和虛無假設檢定兩者思路有相似之處，但是不完全一樣。雖然就我個人而言，我認為可以談論接受一個假設（「接受」一個假設並不意味著它一定要是真的，特別是虛無假設所主張的情況），不過許多人會持不同意見。更重要的是，同學們應該曉得這些詞彙的特殊使用方式，這樣在寫自己的報告時就不會手足無措了。↩︎\n嚴格來說，這裡設定的顯著水準其實是 \\(\\alpha = .057\\)，這有點太寬鬆了。不過，如果是選擇 \\(39\\) 和 \\(61\\) 做為臨界值，那麼棄卻域只會涵蓋分佈的 \\(3.5%\\)。原作者覺得使用 \\(40\\) 和 \\(60\\) 作為臨界值更好討論，並願意容忍 \\(5.7%\\) 的型一錯誤率，因為這是最接近 \\(\\alpha = .05\\) 的整數了。↩︎\n原作者在網路找到的資訊都是說這是Ashley說的，但是無法找到可靠的消息來源。↩︎\n完整的數值\\(p = .000000000000000000000000136\\) 在一般人看來並不是科學的表達方式！↩︎\n注意，Test value要填入的 p 與p值是兩回事。用 jamovi 進行二項式檢定，要先設定 \\(p\\) 參數代表根據虛無假設猜測正確答案的機率，也就是 \\(\\theta\\) 值。換句話說，這個 \\(p\\) 是指根據虛無假設，參與者能答對的機率。↩︎\n請注意，ESP案例的回答正確率我習慣會用p代表，但是這個p與p值無關。在 jamovi 的二項檢定中， 參數\\(p\\)對應到根據虛無假設所作出的正確反應的機率。換句話說，它是 \\(\\theta\\) 的值。↩︎\n請注意，母群參數的真實數值 \\(\\theta\\) 不一定對應自然界中不可改變的事實。在ESP實驗的狀況， \\(\\theta\\) 只是代表參與者正確猜對隔壁房間的卡片顏色機率。所以，各種因素都可以影響母群參數。當然，我們能這樣設計實驗的前提是超感官力量確實存在！↩︎\n一個可能的例外是，研究目標是測試一種新式醫療方法的有效性，研究人員能事先指定需要檢測的重要效果量，例如在任何優於現有治療方法的額外效果量。如此做考驗力分析可以獲得一些有關新療法潛在價值的信息。↩︎\n譯註~ 譯者認為原作者應該還有另一層考量。由於本書定位是入門教科書，考驗力分析需要進一步學習各種研究方法(也就是 ＠sec-A-brief-introduction-to-research-design 的完整版)，才能充分了解其用途。如果教師想為學生介紹預先註冊(preregisteration)的觀念及做法，可考慮在統計課程介紹考驗力分析的必要性。↩︎\n在這種情況下，皮爾森卡方獨立性檢驗（參見 單元 10 ）↩︎\n讀完原作者虛構的ESP研究案例，打開示範檔案庫的”Binomial Test”，找一找虛無假設與對立假設、型一與型二錯誤有沒有呈現在jamovi介面的任何地方？有的話是如何呈現？↩︎\n如果在 單元 8 有用過distrACTION這個模組認識機率分佈，再次使用這個模組繪製雙側檢定與單側檢定的棄卻域。↩︎\n分別以雙側檢定與單側檢定的結果，使用distrACTION模組確認表格內量數之外的累積機率，確認是否與表格裡的p值一致？↩︎\n首先定義一個你期望為真的成功率或失敗率，使用distrACTION模組製作一個代表這項機率的二項分佈機率，繪製出雙側與單側考驗力的區域。↩︎"
  },
  {
    "objectID": "11-Comparing-two-means.html#footnotes",
    "href": "11-Comparing-two-means.html#footnotes",
    "title": "11  比較單一與兩組平均值",
    "section": "",
    "text": "根據原作者在私家花園做的非正式實驗發現，相對於地球上其他地區，澳大利亞的原生植物只能適應低度磷肥，如果你在當地買了一間種植大量外來種植物的房子，還想種本土植物的話，請先把兩種植物分開；對歐洲植物有營養的肥料對澳大利亞原生植物來說是毒藥。↩︎\n一開始匯入資料後， \\(X\\) 的測量尺度要更改為「連續」(Continuous)，因為jamovi 預設功能會將這些成績數值自動定義為名義尺度，這樣子做的分析會不正確。↩︎\n採用 小單元 8.5 的符號表示法，統計學家通成寫成：\\[X \\sim Normal(\\mu_0,\\sigma^2)\\]↩︎\n換句話說，如果虛無假設成立，平均值的取樣分佈可以寫成：\\[\\bar{X} \\sim Normal(\\mu_0,se(\\bar{X})) \\]↩︎\n如果讀者忘記了其中的原理，可以再參考 小單元 4.5。↩︎\n實際上，這種說法可能太過絕對。嚴格來說，z檢定僅要求樣本平均數的分佈呈常態分佈。如果母群分佈是常態的，那麼樣本平均數的分佈自然也會是常態的。然而，正如在討論中心極限定理時看到的，即便母群分佈本身不是常態的，樣本平均數的分佈還是可能呈現常態（這種情況還相當常見）。不過，考慮到假定已知母群標準差是不合理的條件，詳細探討這個條件並無太大必要！↩︎\n我覺得更嚴重的是，報告中只有數字而沒有任何解釋和詮釋。我對這樣的技術報告感到非常懷疑。這可能只是因為我自視甚高，但我經常覺得，如果一個作者沒有努力向讀者解釋和詮釋他們的分析，那麼他們自己可能不理解，或者有些懶惰。你的讀者是聰明的，但不是無限耐心的。如果你能幫助他們，就不要讓他們感到煩惱。↩︎\n技術性的註解：以與 z 檢定的假設一樣的方式，我們可以削弱 t 檢定的假設，以便只談論取樣分佈。但是，對於 t 檢定，這樣做比較困難。像以前一樣，我們可以將母群正態性的假設替換為樣本平均數的取樣分佈是常態分佈的假設。然而，請記住我們還依賴於對標準差的樣本估計，因此我們還需要 \\(\\hat{\\sigma}\\) 的取樣分佈是卡方分布的假設。這使得事情變得更加麻煩，這個版本很少在實踐中使用。幸運的是，如果母群分布是正態的，那麼這兩個假設都得到滿足。↩︎\n正是因為這個例子很簡單，原作者才會用它帶各位同學入門。↩︎\n在這個時候，幾乎總是會有一個有趣的問題出現：在這種情況下，究竟是指哪個母群？是真正參加 Harpo 博士課程的學生集合（全部33人）嗎？可能會參加這門課程的人（數量未知）嗎？還是其他什麼？我們選擇哪個對象，會有影響嗎？在介紹行為統計學的課堂上，這時幾乎總是會混混噥噥，但是因為我的學生每年都會問我這個問題，所以我會給出簡短的答案。從技術上講，是有影響的。如果您改變了“真實世界”母群的定義，那麼觀察到的平均值 \\(\\bar{X}\\) 的抽樣分佈也會發生變化。t 檢驗依賴於一個假設，即觀察值是從一個無限大的母群中隨機抽樣的，實際情況很少符合這個假設，因此 t 檢驗可能是錯誤的。然而，在實踐中，這通常不是一個大問題。即使這個假設幾乎總是錯誤的，它也不會導致測試出現很多病態行為，因此我們往往會忽略它。↩︎\n數學上，我們可以這樣寫：\\[w_1=N_1-1\\] \\[w_2=N_2-1\\] 現在我們已經對每個樣本分配了權重，我們通過取兩個變異數估計值的加權平均值，\\(\\hat{sigma}_{1}^{2}\\) 和 \\(\\hat{sigma}_{2}^{2}\\)，計算變異數的集體估計值。\\[\\hat{\\sigma}_p^2=\\frac{w_1\\hat{\\sigma}_1^2+w_2\\hat{\\sigma}_2^2}{w_1+w_2}\\] 最後，我們通過取平均數的平方根，將集體變異數估計轉換為集體標準差估計。\\[\\hat{\\sigma}_p=\\sqrt{\\frac{w_1\\hat{\\sigma}_1^2+w_2\\hat{\\sigma}_2^2}{w_1+w_2}}\\] 如果你在這個方程中心理上代入 \\((w_1 = N_1 - 1)\\) 和 \\(w_2 = N_2 - 1\\)，你會得到一個非常醜陋的公式。這個公式似乎是描述集體標準差估計的“標準”方式，但這不是我最喜歡思考集體標準差的方法。我更喜歡這樣思考。我們的資料集實際上對應於一組 N 個觀測值，這些觀測值被分成兩個組。因此，讓我們使用符號 \\(X_{ik}\\) 來表示第 k 個教程組中第 i 個學生所獲得的成績。也就是說，\\((X_{11}\\) 是 Anastasia 班上第一個學生獲得的成績，\\(X_{21}\\) 是她的第二個學生，以此類推。我們有兩個不同的群體平均值 \\(\\bar{X}_1\\) 和 \\(\\bar{X}_2\\)，我們可以使用符號 \\(\\bar{X}_k\\) “一般化地”引用它們，即第 k 個教程組的平均成績。到目前為止，一切都很好。現在，由於每個學生都屬於兩個教程中的一個，我們可以將他們的偏差描述為差異\\[X_{ik}-\\bar{X}_k\\] 那為什麼不使用這些偏差（即每個學生的成績與他們的教程平均成績之間的差異）？請記住，方差只是一堆平方偏差的平均值，所以我們來這麼做。數學上，我們可以寫成以下式子： \\[\\frac{\\sum_{ik}(X_{ik}-\\bar{X}_k)^2}{N}\\] 其中符號”\\(\\sum_{ik}\\)“是一種懶惰的方式，意思是“通過查看所有教程中的所有學生來計算總和”，因為每個”\\(_{ik}\\)“對應一個學生。\\(^a\\) 但是，正如我們在 單元 8 中看到的，通過除以 N 計算變異數會產生一個偏差的樣本方差估計值。因此，以前我們需要除以 \\(N - 1\\) 來修正這個偏差。然而，正如我當時提到的，這種偏差存在的原因是因為變異數估計依賴於樣本均值，並且如果樣本均值與母群均值不等，它可能會有系統地偏移我們對變異數的估計。但是這次，我們依賴於兩個樣本均值！這是否意味著我們有更多的偏差？是的，沒錯。這表示我們現在需要除以 \\((N-2)\\) 而不是 \\((N-1)\\)，以計算我們的共變異數估計值。因此，我們的公式是：\\[\\hat{\\sigma}_p^2=\\frac{\\sum_{ik}(X_{ik}-\\bar{X}_k)^2}{N-2}\\] 最後，如果你對它取平方根，就會得到 \\(\\hat{\\sigma}_p\\)，即共同標準差的估計值。換句話說，共同標準差的計算並不特別，與計算標準差的方式沒有太大的不同。  — \\(^a\\) 更正式的符號將在 單元 13 介紹。↩︎\n只要這兩個變項真的具有相同的標準差，我們對標準誤的估計就是 \\[SE(\\bar{X}_1-\\bar{X}_2)=\\hat{\\sigma}\\sqrt{\\frac{1}{N_1}+\\frac{1}{N_2}}\\] 因此，我們的 t 統計量為 \\[t=\\frac{\\bar{X}_1-\\bar{X}_2}{SE(\\bar{X}_1-\\bar{X}_2)}\\]↩︎\n嚴格來說，應該是「平均數的差異」應該是呈現常態分佈，但是如果兩個組別的資料都呈現常態分佈，那麼平均數的差異也會呈現常態分佈。實際上，中央極限定理告訴我們，通常，被測試的兩個樣本平均數的分佈，隨著樣本量的增大，將會接近常態分佈，而不論底層資料的分佈如何。↩︎\n嗯，我猜你可以平均蘋果和橙子，而最終得到的是美味的果汁。但是，沒有人真正認為果汁是描述原始水果的很好的方式，對吧？↩︎\n但是你仍然可以估計樣本平均值之間的標準誤差，只是它看起來不同：↩︎\n這個設計非常類似於啟發麥克馬(McNemar)檢驗的設計。這一點不足為奇。兩者都是標準的重複測量設計，涉及兩次測量。唯一的區別是這次我們的結果變量是間隔尺度(工作記憶容量)，而不是二元名義尺度變量(是或否問題)。↩︎\n此時，我們有 Harpo 博士、Chico 博士和 Zeppo 博士。猜猜看誰是 Groucho 博士，不用給獎品。↩︎\n他們通過將\\(d\\)的常數值乘以\\(\\frac{(N - 3)}{(N - 2.25)}\\)引入一個小的糾正。↩︎\n如果您有興趣，可以查看在chico2.omv文件中是如何完成的↩︎\n這是一個極大的簡化。↩︎\n要么是Kolmogorov-Smirnov檢驗，這可能比Shapiro-Wilk更具傳統性。雖然我讀到的大部分東西似乎都表明Shapiro-Wilk是更好的正態性檢驗，但Kolomogorov Smirnov是一種通用的分佈等價性檢驗，可以應對其他類型的分佈檢驗。在jamovi中，Shapiro-Wilk檢驗是首選。↩︎\n該檢驗統計量通常表示為\\(W\\)，其計算方法如下。首先，我們按照大小遞增的順序對觀測值進行排序，讓\\(\\bar{X_1}\\)成為樣本中最小的值，\\(X_2\\)成為第二小的值，依此類推。然後\\(W\\)的值由以下公式給出\\[W=\\frac{(\\sum_{i=1}^N a_iX_i)^2}{\\sum_{i=1}^N(X_i-\\bar{X})^2}\\]其中\\(\\bar{X}\\)是觀測值的平均值，\\(a_i\\)值是…咕嚕咕嚕…比較複雜，超出了入門級教材的範疇。↩︎\n實際上，有兩種不同的檢定統計量版本，它們之間相差一個常數。我描述的是jamovi計算的版本。↩︎"
  },
  {
    "objectID": "01-Why-do-we-learn-statistics.html#sec-why-do-we-learn-statistics-begin",
    "href": "01-Why-do-we-learn-statistics.html#sec-why-do-we-learn-statistics-begin",
    "title": "1  為什麼要學習統計",
    "section": "1.1 首先談談統計的心理學",
    "text": "1.1 首先談談統計的心理學\n先講一個讓許多心理系新鮮人驚呆的事實，統計學在心理學課程佔有相當的份量。另外一個不令人意外的事實，是統計學很少被上過心理學課程的同學推薦。畢竟正常來說，喜歡統計的同學都是去修統計系的課，而不是來上心理學系的課。不需要太正式的調查，一大部分正在上心理學課程的同學，想必很不高興要學這麼多統計。為了幫助同學渡過這段不適應，我想先來談談一般人對於統計學的一些常見疑問。\n\n這個問題的很大一部分與統計學的本質有關。統計學是什麼？統計學是做什麼用的？為什麼科學家如此迷戀統計？這些都是值得好好回答的問題，我們先從最後一個問題開始談吧。科學家們似乎想對每件事情進行統計分析，旁人看來近乎固執。科學家確實經常使用統計學，有時候甚至忘記向一般人解釋為什麽這樣做。這是科學家之間的一種信仰，尤其是社會科學家。任何科學家沒有完成統計之前，他的發現是不會被其他科學家信任的。大學新鮮人看了，可能會學得這群人都是瘋子，因為沒有科學家會花時間向一般人回答這個非常簡單的問題：\n\n為什麼當心理學家要會使用統計？為什麼科學家不能憑生活常識做研究？\n\n在某些方面，這是一個天真的問題，但大多數好問題都是這樣開始的。對此問題已經有許多有意思的回答2，不過我心中最好的回答是一個大家都懂的人性現實：人類無法充分信任自己的判斷。正是因為了解人類本身，很容易受到各種偏見、誘惑和軟弱而影響個人的判斷。在很多情況，統計學提供一種基本保障。使用”生活常識”評估證據意味著信任直覺，依靠言語推論以及使用人類原生的推理能力找出正確答案。但是大多數科學家認為這種方法不太可靠。\n\n其實好好地想一想，這很像是一個心理學家會研究的問題，既然我在心理學系工作，這似乎是個值得深入研究的好題目。真的有道理認為”靠常識”做出的研究是值得信賴的嗎？言語推論是用語言構建的，所有語言都帶有偏見 - 像是有些事情特別難說，並不一定是因為它們是錯的（例如，量子力學是一個很好的理論，但很難用言語解釋）。因為人類的”直覺”本能並不是用來解決科學問題，而是用來應付日常推論問題 - 由於生物演化速度比文化演化遲緩，我們應該說直覺是為了解決不同於我們的生活經驗的日常問題而設計的產物。科學研究的根本是理性推理，需要我們進行”歸納”，做出明智的猜测，超越用感官接收的證據，概括這個世界的樣貌。如果你認為有本事不受各種外界干擾進行理性推理，那麼我有份年薪百萬，請你去柬埔寨打工的機會想介紹給你3。哎呀，正如下一節我要說的，我们也無法解决不需要猜測的”演繹”問題，也就是那些不會受到預先存在的偏見所影響的問題。\n\n\n1.1.1 信念偏誤的詛咒\n大多數人類都很聰明。我們肯定比地球上其他物種要聰明得多（盡管可能很多人不這麽認為）。人類的思維能力是非常神奇的產物，我們似乎有能力思考和推理任何不可思議的事情。但是這並不意味著人類完美無缺。心理學家累積多年的研究已經表明，人類確實很難保持中立，公正地評估證據，而不會受到預期偏見的影響。有個很好的例子是邏輯推理中的信念偏差效應：如果你要求我判斷一個特定的論點是否在邏輯上是正確的（也就是說，如果前提是真實的，結論就是真實的），我常常會受到結論的可信度影響，即使我明知不該如此。以下是一段結論可信的有效邏輯論證：\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n再看這一段結論不可信的有效邏輯論證：\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n兩段論證的結構都是相同而且有效4。然而第二段的前提1有理由相信並不正確，所以有人會認為結論也不正確。其實無論前提的內容如何，論證的演繹有效性僅取決於前提與結論的結構。也就是說，有效論證不必然要包括真實的敘述。\n\n另一方面，無效的論證也能有讓人相信為真的結論，就像下一段論證：\n\n\n所有會上癮的東西是昂貴的 (前提 1)\n有些香煙是便宜的 (前提 2)\n所以有些會上癮的東西不是香煙(結論)\n\n\n最後來看以下這段無效演繹且結論不可信的論證：\n\n\n所有香煙是昂貴的 (前提 1)\n有些會上癮的東西是便宜的 (前提 2)\n所以有些香煙不是會上癮的(結論)\n\n\n假設人類真的完全能夠放下對於敘述真實性的預期偏見，僅憑邏輯有效性評估論點是否合理。那麼我們可以設計實驗，測試看看是否所有人都會認為有效論證是正確的，沒有人會說無效論證是正確的。我把這樣的假設製成表1.1 。\n\n\n\n\n表1.1 判斷人類能拋開偏見進行有效論證的假設結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n100% 認為“有效”\n100% 認為“有效”\n\n\n論證無效\n0% 認為“有效”\n0% 認為“有效”\n\n\n\n假如心理學研究結果就像表內的數值（或者只是接近這樣的數值），我們可能覺得完全靠人類的直覺就能做出結論。只要是像這樣的研究結果，科學家完全可以根據他們的常識評估結果數據，不用花時間處理那堆讓很多人看不懂什麼意思的統計分析。然而，你們已經至少修過心理學概論，對於這套實驗的真正結果應該略知一二。\nEvans et al. (1983) 做了一系列探討人類如何進行邏輯推論的經典實驗。他們發現只有結論敘述符合多數人的預期偏誤(也就是信念)，實驗結果才會接近人類能做有效推論的假設(表1.2)。\n\n\n\n表1.2 預期偏誤與論證有效性的實驗結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n92% 認為“有效”\n\n\n\n論證無效\n\n8% 認為“有效”\n\n\n\n\n雖然不夠完美，但是這樣的結果也算不錯了。不過看看另外兩個與一般人的直覺完全相反實驗情況，與表1.1的完美假設完全不同(表1.3)。\n\n\n\n表1.3 直覺判斷與論證有效性的實驗結果\n\n\n\n結論應該為真\n結論應該為假\n\n\n論證有效\n92% 認為“有效”\n46% 認為“有效”\n\n\n論證無效\n92% 認為“有效”\n8% 認為“有效”\n\n\n\n\n哎呀，這不是好解釋的結果。實驗結果顯示似乎向一般人講述一個與既有信念互相矛盾但有邏輯效力的論點時，人們很難相信這是一個強而有力的論點（只有 46% 的人會相信）。更糟的是，向一般人講述一個與既有偏見相符但沒有邏輯效力的的論點時，幾乎沒有人能看出這個論點無效（高達 92% 的人判斷錯誤！）。5\n如果仔細想想，這並不是很糟糕的結果。總體來看，一般人的表現比隨機亂猜好，大約有60％的人做出了正確的判斷（隨機亂猜應該是50％）。即使如此，如果你是一名專業的“證據鑑識人員”，有人送你一個可以提高正確決策的機率的神奇工具，比如說從 60% 到95% ，你應會欣然接受吧？幸好我們有一種工具可以做到這一點。這種工具不是魔法，而是統計學。這就是為什麼科學家喜歡使用統計的最主要原因。我們很容易「信任我們想要相信的」。所以如果我們要做到「信任資料」，就需要一些工具幫助我們控制個人偏見。而這就是統計學的用途，它能幫助我們保持推論的誠實。"
  },
  {
    "objectID": "02-A-brief-introduction-to-research-design.html#混淆因素人為反應以及各種降低效度的因素",
    "href": "02-A-brief-introduction-to-research-design.html#混淆因素人為反應以及各種降低效度的因素",
    "title": "2  研究設計入門",
    "section": "2.7 混淆因素、人為反應、以及各種降低效度的因素",
    "text": "2.7 混淆因素、人為反應、以及各種降低效度的因素\n\n如果從最全面的層次看待研究效度的問題，每個研究者最擔心的兩個大問題是混淆因素和人為反應。這兩個術語的定義如下:\n\n混淆因素(confounder):混淆因素通常是額外的、非研究需要測量的變項13，最後與預測變項和結果變項有相當程度的相關性。無法排除的混淆因素會降低研究的內在效度，因為有可能無法確定現在的研究結果是預測變項所導致，還是混淆變項所導致的。\n人為反應(artifcat):如果研究結果僅在研究測試的特殊條件才能出現，參與者表現在測量尺度的反應就是“人為的”。只要研究結果可能是人為反應，就會降低研究的外在效度，因為這代表將研究結果推廣或應用到研究者所關注的實際群體的可能性不高。\n\n總而言之，採取非實驗研究的研究者更要留意混淆因素的影響，因為非實驗研究不是具備良好控制的研究。按照之前的說明，許多條件都未受到研究者控制，所以非常有可能存在大量混淆因素在非實驗研究項目。實驗研究通常能消除混淆因素的影響，是因為研究者對研究過程的控制越好，越能防止混淆因素影響結果變項。例如，運用隨機分派，混淆因素的影響能隨機且均勻地分散於各條件組之間。\n凡事都有利與弊。當研究者首要關切的問題是人為反應而不是混淆因素時，狀況就剛好完全相反，人為反應對實驗研究造成的效度問題多於對非實驗研究。同學想要充分理解為何如此，可以多閱覽非實驗研究的研究程序，因為這些研究是在自然場域裡檢視人類行為。在真實的環境中操作研究條件 ，研究者過會無法做好實驗控制(容易受到混淆因素的影響)，但因為關注於在“室外”觀察人類心理，人為反應影響結果的可能性因此減少。或者說，當研究者將心理學研究從室外帶進實驗室(通常必須這樣做才能有良好實驗控制)，總是會遇到研究程序與想研究的內容不同的意外。\n請注意，以上只是一個粗略的研究設計指引。各種實驗研究絕對存在混淆因素，非實驗研究也可能會引導出人為反應。由於各種原因，這兩種情况都有可能會發生，無法單純歸咎於的是研究設計者或實驗執行者的錯誤。在研究實務，提前考慮到所有條件真的很不容易，就算是非常有經驗的研究者也會犯錯。\n儘管可以將可能降低效度的各種問題大致區分為混淆因素或人為反應，但兩者依然是相當模糊的概念。以下介紹一些常見的非研究條件因素。\n(以下為AI初翻，尚待校稿)\n\n2.7.1 歷史效應\n歷史效應指的是研究過程發生可能會影響測量結果的特定事件。例如，在前測和後測之間發生的事情；在測試第23位參與者和第24位參與者之間發生的事情。或者正在進行的研究是根據先前找過的研究文獻建立效度指標，但是過程中有推翻指標有效性的新文獻發表，導致研究結論不再值得信賴。可歸類為歷史效應的事件像是:\n\n某位澳大利亞研究者為了探討人們如何看待風險和不確定性，於2010年12月開始收集資料。但是需要時間招募參與者和收集資料，直到2011年2月仍在尋找參與者。不幸的是(對於在澳大利亞做研究的人來說)，2011年1月昆士蘭洪水導致數十億美元的損失和許多人死亡。造成2011年2月參與測試的參與者，對處理風險的信念與2010年12月參與測試的參與者有很大不同。這兩個時間點之中(如果有的話)，哪個反映參與者的“真實”信念？答案可能兩者都是。昆士蘭洪水確實改變了澳大利亞公眾的風險信念，儘管可能只是暫時的。這裡的關鍵是，2月參與者的“歷史背景”與12月參與者的情況截然不同。\n某位在常發生有感地震的地區工作的研究者正在測試一種新型抗焦慮藥物的心理效應。在給藥前，研究者收集參與者的自我報告和生理指標來測量焦慮。給藥之後，再實施相同的測量程序。但是，剛好在研究過程中，發生了六級地震，參與者變得更焦慮了。\n\n\n\n2.7.2 發展成熟效應\n與歷史效應一樣，發展成熟效應根本上與時間的變化有關。然而，成熟效應不是對特定事件的反應。相反，它們與人們隨時間自行變化有關。我們會變老、疲憊、厭倦等。可歸類為成熟效應的事件像是:\n\n在進行發展心理學研究時，您需要意識到兒童成長得相當迅速。因此，假設您想弄清某些教育技巧是否可以幫助3歲兒童的詞彙量。您需要知道的一件事是，僅憑自己，這個年齡的兒童的詞彙量以難以置信的速度增長(每天多個詞)。如果在設計研究時沒有考慮這種成熟效應，那麼您將無法確定您的教育技巧是否有效。\n當在實驗室中運行非常長時間的實驗(比如3小時)時，人們很可能會開始感到厭倦和疲憊，並且這種成熟效應會導致表現下降，而與實驗中發生的任何其他事情無關。\n\n\n\n2.7.3 重覆測試效應\n重覆測試效應是一種要認真看待的歷史效應。假設我想對某些心理特質(例如焦慮感)進行兩次測量。研究者要擔心的是，第一次測量是否會影響第二次測量的結果。也就是說，影響第二次測量的“事件”，就是第一次測量，因此是一種歷史效應!這種狀常見於行為科學研究。可歸類為重覆測試效應的事件像是:\n\n初次學習和複習:由於參與者在第一次測試學會了如何解答“智力測試”問題的一般規則，所以第二次的“智力測試”表現可能比第一次好。\n測試場景的熟悉度:例如，如果參與者在第一次測試感到緊張，使得表現水準下降。但是度過了第一次測試之後，他們會變得不再緊張，因為已經熟悉測試場景。\n測試導致的次要條件變化:例如，如果評量情緒狀態的問卷施測過程讓人枯燥，第二次測量的報告可能較多報告的情緒是“厭倦”，不過是因為第一次測量帶來的枯燥感。\n\n\n\n2.7.4 選擇偏誤\n選擇偏誤是一個相當廣泛的專門用語。假設有一項實驗安排兩組參與者，每組參與者會得到不同的“處置”，研究者想了解不同處置能否導致不同結果。然而，儘管研究者已經盡了最大努力，兩組之間仍有性別失衡(例如，A組為80%的女性和B組為50%的女性)。聽起來這種狀況可能不會發生，但請相信，這種狀況並不少見。這就是一個選擇偏誤的例子，被“選擇進入”兩組的參與者各有不同的特點，若是這些特點中的任何一個被證明與實驗條件有高相關(例如，實驗者安排的處置對女性較有利)，那麼研究者會遇到大麻煩。\n\n\n2.7.5 個體特質差異\n在考慮淘汰極端參與者時，區分兩種不同個體特質有時候很有幫助。第一種是同質淘汰，造成的淘汰效應對所有實驗組、處置或條件都是相同的。在以上的範例中，如果只有容易感到厭倦的參與者幾乎同時退出實驗，這就是一種同質淘汰。同質淘汰的主要影響通常是降低研究樣本的代表性，造成研究結果不具普遍性，造成研究的外在效度下降。\n第二種淘汰類型是異質淘汰，各組的淘汰效應會有所不同。這種類型更常被稱為差異淘汰，是一種由研究設計引起的選擇偏誤。假如我成功找到具備完美平衡特質和代表性的樣本人群，執行心理學史上首次“難以置信的冗長且乏味的實驗”。但是因為研究過程非常冗長和無聊，很多參與者開始退出，而且我無法阻止這種情況，因為參與者有絕對權利在任何時候，無須基於任何理由停止實驗。身為研究人員，我們有義務提醒參與者確實有這些權利。所以，假如“難以置信的冗長且乏味的實驗”有非常高的退出率。您認為參與者決定退出是隨機的可能性有多大?答案是零。幾乎可以肯定，留下來的人比退出的人更盡責、更能忍受無聊等。在某些方面，責任感與我關心的心理現象高度相關，這樣的淘汰效應會降低研究結果的效度。\n另外還有一個案例。有研究者設計了一項有兩種條件的實驗。在“治療”條件，實驗者會侮辱參與者，然後給他們一份測量順從性的問卷。在“控制”條件下，實驗者和參與者閒聊了一些無意義的小事，然後給他們問卷。先不考慮這樣的研究有什麼科學價值和符不符合研究倫理，讓我們思考其中可能會造成什麼問題。一般來說，有人當面侮辱我時，我會變得不想合作。所以，非常可能有較多治療組參與者退出研究，這種退出行為顯然不會是隨機的。最有可能退出的參與者，很可能是最不願順從研究目的的人們，由於最無責任感和最不願順從的人都退出了治療組而沒有退出控制組，造成一種混淆因素:治療組裡完成問卷的參與者，責任感和順從性已經高於對照組的參與者。簡而言之，這項研究侮辱參與者的處置並沒有讓他們更加順從，而是造成不順從的參與者退出實驗!這項實驗的內部效度完全被消滅殆盡。\n\n\n2.7.6 無回應偏誤\n無回應偏誤與選擇偏誤及差異淘汰對測量結果的影響異曲同工。首先看一則案例：有項研究向1000人發出調查，只有300人回覆。回覆的300人幾乎肯定不是隨機樣本。回覆調查的人與不回覆的人具有系統性差異。這在嘗試從回覆者的反應歸納的結論，推廣至更廣泛的群眾時會造成問題，因為現在的資料並不是隨機樣本。除了研究結果的普遍性，還有一種無回應偏誤造成更多問題。我們繼續看這則案例的更多內情：收到回覆的300份問卷裡，可能會發現並非所有人都回答了每個問題。比如說其中80人未回答其中一個問題，這會導致混淆嗎?答案同樣是「可能」。如果未回答的問題印在問卷的最後一頁，而那80份問卷的最後一頁剛好遺失了，那這樣的資料漏失可能不是什麼大問題，有可能只是最後一頁沒訂好。然而，如果那80人未回答的是問卷裡最有對抗意識或冒犯意味的個人問題，那麼這項研究幾乎肯定有混淆因素的影響。這裡所描述的是研究實務的資料漏失問題，如果漏失的資料是“隨機的”，那麼這問題還不大。但若是系統性的漏失，那麼問題可能就很難收拾了。\n\n\n2.7.7 趨向平均數的迴歸\n趨向平均數的迴歸指的是研究者根據測量出的極端值，選擇資料的任何判斷。由於資料變項本來就有自然(隨機)變異，幾乎可以肯定第二次測量的結果，會比第一次測量的結果更不極端，純粹是偶然發生。\n我們來看這一個案例。假設研究者想了解心理學教育是否對學業表現優異的年輕學生有不良影響，因此找了20位大學心理系一年級，入學前學業成績最好的學生，觀察他們在大學的表現如何。結果發現，他們入學後的學習表現比高於平均水準，但是並非名列前茅，儘管他們在高中曾經是頂尖學生。這是怎麼一回事？第一個會想到的是，這一定是心理學課程對這些學生產生了不良影響。儘管這很有可能是解釋，但是實際上更可能是“趨向平均數的迴歸”的一種現象。為了理解背後的原理，讓我們稍微思考一下在高中或大學課程要獲得最高分需要什麼條件，落是不考慮課程內容，一個班級學生人數很多的話，會聚集很多非常聰明的學生。一名學生想要獲得最高分，必須非常聰明，非常努力用功，還要有一點點運氣。考試必須恰好是特別會回答的題目，而且寫答案卷的時候，也必須避免犯愚蠢的錯誤(每個人學生時代都會有這種時候)。其中的關鍵是，雖然個人智識和努力可以從一門課轉移到另一門課，但運氣沒辦法移轉。高中時期考試運氣好的學生，大學課堂的考試運氣不一定一樣好，這就是“運氣”的真正涵義。對研究結果的影響是，當研究者根據高中成績的極端值(前20名學生)選擇研究對象，其實是選擇努力用功、智識和運氣在高中時期前端的學生。由於運氣不會轉移到第二次測量(只有技能和工作)，所以合理預期這些人在大學課程的表現會有下降。因此，他們的分數會稍有下降，趨向所有同學的平均水準。這就是”趨向平均數的迴歸”。\n令許多人感到驚奇的是，趨向平均數的迴歸是非常普遍的現象。例如，兩個身材高大的夫妻生的孩子，往往會比同齡人口平均身高還高，但是不如父母高。身材矮小的夫妻情況則相反，生的小孩往往身材矮小，但是這些孩子通常會比父母高。這種現象的作用也非常微妙。例如，曾有研究顯示，一般人從負面回饋獲得的學習效果更好，而不是從正面回饋。然而，一般教師及家長嘗試運用這種學習策略的方式是，在學生表現良好時給予正面回饋，在學生表現不好時給予負面回饋。結果會發現，給予正面回饋後，學生的表現會變差，但是給予負面回饋後，表現反而會變好。請同學注意，這個案例存在選擇偏誤！當一個人在某次試驗表現得好，評分者給的評價會偏高，所以評分者應該預期，由於趨向平均數的迴歸的影響，無論是否給予回饋，下一次試驗的表現都應該會變差。同樣地，經歷一次糟糕的試驗，個人就會有改進的趨勢，負面回饋的優勢作用是由于趨向平均數的迴歸而引起的研究結果偏差(詳見 Kahneman & Tversky (1973) 的討論)。\n\n\n2.7.8 實驗者偏誤\n實驗者偏誤有多種形式。最典型的一種狀況是，實驗者也許有好意，但是可能會不經意地向參與者用微妙的方式傳達“正確答案”或“期望表現的行為”，最終影響了實驗結果。通常是因為實驗者擁有參與者所没有的特殊知識，例如問題的正確答案，或了解參與者所處的實驗條件預期出現的行為。這種狀況的典型案例是 1907年“聰明的漢斯”(Pfungst (1911))個案研究，研究對象是一隻表面上能夠閱讀、算術和表現只有靠人類的智性能力才能完成任務的馬。 聰明的漢斯走紅之後，當時的心理學家仔細地檢查他的行為。令人驚訝地發現是，漢斯其實並不會算術，而是因為照顧他的主人了解如何算術，當主人改變行為時，馬兒學會要改變自己的行為。\n實驗者偏誤的一般解決方法是雙盲研究，這個狀況下實驗者和參與者都不知道參與者所處的條件或期望會出現的行為。雖然這是一個非常好的方法，但更重要的是要認識並非完美的解決方法，也很難充分實現。例如，做研究的大學教授經常採用的雙盲研究的方法是讓研究生來執行實驗，而且是不了解實驗細節的學生。唯一知道所有細節(例如問題的正確答案、參與者與條件的分派)的教授，並未與參與者交流，與參與者當面交流的研究生什麼也不知道。這樣安排看起來應該足夠了，除了研究生言部分在現實情况不太可能是真的。為了讓研究生能有效執行研究，他們需要由教授進行指導。而事實上，研究生當然要了解指導教授的想法，還有了解一些教授對於人類和心理學的價值信念(例如，教授認為一般人比心理學家所預期的更聰明)。有此印象的研究生擔任實驗者，幾乎不可能完全避免地知道一點點教授的想法，而且即使是一點點想法也會產生影響。想想看，假如實驗者不經意地向參與者傳遞了教授預期他們在某項實驗條件會有表現良好的想法。有興趣讀者們可以搜尋「比馬龍效應」（Pygmalion effect）：只要認為別人期望自己能表現成功，個人就會努力表現達到期望。反過來也是，只要認為別人認為自己會失敗，也會往失敗的方向表現。換句話說，實驗者的期望成為參與者自我實現的預言。\n\n\n2.7.9 需求效應與反應性\n處理實驗者偏誤所要避免的問題，是實驗者將實驗條件的認識或期望傳達給參與者，這些都有可能改變參與者的行為(Rosenthal, 1966)。即使能成功避免實驗者偏誤，也不可能阻止參與者自主意識正在參與的是心理學研究，只要知道有他人正在觀察或研究個人表現，就可能對參與者的行為產生相當大的影響。這通常被稱為反應性(reactiviy)或需求效應(demand effects)。最有代表性的案例是霍桑效應(Hawthorne effect)：由於參與者了解研究者正在專心觀察他們，因而改變了個人表現。霍桑效應的由來是一項曾在芝加哥市郊的“霍桑工廠”所進行的研究(見 Adair (1984))，這項20世紀20年代的研究觀察工廠照明對工人生產力的影響。給後來研究者的重要啟示是，工人行為的變化是因為他們知道自己是研究對象，而不是工廠照明造成的效果。\n為了更具體地說明意識到自己正在被觀察，會如何改變參與者的行為，這裡借用一下社會心理學家的觀點，來看看一般人在實驗中可能會表現出來，但在現實生活中卻不會表現的一些角色特質:\n\n積極參與者嘗試幫助研究人員，他們會試圖找出實驗者的假設並證實之。\n消極參與者與積極參與者完全相反。他們會試圖以某種方式破壞或否定研究目的或假設。\n忠實參與者異常順從研究者指示。無論真實環境應該遵守的規則是什麼，他們在實驗室裡都會完美遵照指示。\n謹慎參與者對於被測試或被觀察會感到緊張，以至於他們的行為會變得非常不自然，或過度符合社會期望。\n\n\n\n2.7.10 安慰劑效應\n安慰劑效應是任何研究都要非常小心處理的一種特殊需求效應。「安慰劑」是指僅僅因為接受治療，就會導致個人身心狀態的改善，醫學臨床試驗有許多典型例子。如果醫師給患者一種完全不會在體內產生化學反應的藥物，並告訴他們這是某種疾病的特效藥，通常會發現與未接受完整治療的人相比，這些患者的康復速度會更快。換句話說，只要人們認為自己正在接受治療，就是導致結果改善的原因，而不是藥物本身。\n然而，現代醫學界的共識是真正的安慰劑效應非常罕見，此前文獻裡被認為是安慰劑效應的大部分事實，其實是自然痊癒(有些人健康狀況會自然好轉)、趨向平均數的迴歸、還有其他混淆因素的怪奇組合。 心理學研究者會感興趣的是，至少有一些安慰劑效應的有力證據，是來自自我報告的結果，最顯著案例的是治療心因性的疼痛感 (Hróbjartsson & Gøtzsche, 2010)。\n\n\n2.7.11 情境、測量、及小群體效應\n從某些方面來說，這些專有名詞是“所有其他會降低外部效度因素”的總稱。這些名詞指的是研究者從參與者組成的次群體中選擇、執行研究的地點、時間和研究方式(包括誰收集資料)以及使用的測量工具都可能會影響結果。 具體而言，研究者會擔心的是，這些因素都可能以某種方式影響結果，使得研究結果無法推廣至更廣泛的人群、地點和其他測量方式。\n\n\n2.7.12 詐欺、欺暪與自我欺暪\n\n當一個人的薪水依賴於他不理解某件事情時，很難讓他理解這件事。\n- 厄普頓·辛克萊\n\n最後還有一個主題應該要好好討論。讀過各家教科書介紹如何評估研究效度的單元，讓我很在意為何許多作者似乎都假定全世界的研究人員都是誠實的。儘管絕大多數科學家都是誠實的，但在我的經驗裡，至少有一些不是。14 不僅如此，如同前面所提到的，專業科學家也無法避免因為信念而產生的偏見。研究者很容易欺騙自己相信錯誤的研究結果，可能導致他們做出有缺陷的研究，然後在撰寫的報告裡隱藏這些缺陷。所以讀者不僅需要留意可能性不大的公開詐欺，還需要注意研究內容裡，可能相當常見的“研究者偏見”。好幾本標準教科書都沒有討論這些問題，以下是原作者自行找出的幾項已眾所週知的問題:\n\n資料造假(Data fabrication)。有時候研究人員會捏造資料，而且會覺得是“出於善意”。例如，有研究人員覺得捏造的資料確實呈現事實，並且實際上可能是真實資料“略為清理後”的版本。在其他情況，詐欺則是故意且惡意的。一些被指控或被證明資料造假的著名案例包括:西里爾·伯特 (Cyril Burt，因素分析的發明者之一，據信他捏造了部分智力測驗資料)、安德魯·韋克菲爾德 (Andrew Wakefield、被指控捏造MMR疫苗與自閉症之間聯繫的資料)和黃禹錫 (偽造了大量幹細胞研究資料)。\n惡作劇(Hoaxes)。惡作劇與資料造假有很多相似之處，但是行為者的目的不同。惡作劇通常是為了開玩笑，其中許多案例的始作俑者最後都會自已出來坦承。惡作劇的重點通常是為了詆毀某人或某個領域，至今已經發生很多眾所周知的科學惡作劇(例如皮爾當人)，有些則是故意抹黑特定領域而發表的研究(例如索卡事件)。\n資料不實(Data misrepresentation)。雖然欺詐佔據了大部分頭條，但在我的經驗中，看到資料被歪曲的情況更為常見。我這麼說不是指報紙報導錯誤(他們確實幾乎總是這樣)，而是指出事實上資料並不總是研究人員認為它們所說的那樣。我猜，幾乎總是如此，這不是蓄意不誠實的結果，而是由於資料分析缺乏成熟。例如，回想一下我在這本書開頭討論的辛普森佯謬的示例。人們展示某種“匯總”資料非常常見，有時如果你更深入地找到原始資料，你會發現匯總資料與未匯總資料講述了不同的故事。或者，您可能會發現某些資料被隱藏了，因為它們講述了一個不方便的故事(例如，研究人員可能選擇不提及某個特定變項)。這方面有很多變體，其中許多非常難以檢測。\n“不良”研究設計(Study “misdesign”)。研究設計類的問題相當微妙。這類問題在於研究人員設計了一種研究條件有瑕疵的研究，然而這些瑕疵在論文裡都沒有報告出來。報告的資料是完全真實，並且分析方法正確，不過研究結果是由設計錯誤的研究產生。研究人員是真的想找到符合研究假設的結果，所以研究設計使他們“容易地”觀察到想看到的結果。為了幫助讀者學習其中的詐欺手段，這裡分享一個小技巧，那就是設計一套實驗，然後讓參與者很明顯地知道他們「應該」要做什麼，然後發揮“需求效應”的魔法。如果要做得更天衣無縫，只要做出符合雙盲實驗的表面功夫，但結果並不會有什麼變化，因為研究材料本身就隱藏微妙的提示，讓參與者知道研究人員希望他們做到什麼。當結果發表出來時，讀者並不會馬上察覺到詐欺。身臨其境的參與者能感受明顯的提示，但是只閱讀論文並不會那麼明顯的感受到。當然，我描述這類問題的方式讓這種研究聽起來像是欺詐，確實可能有一些研究人員會故意這樣做，但在我的經驗裡，更值得擔憂的是非故意的瑕疵設計。研究人員確實相信某些事情，所以研究進行到最後恰好出現了內在瑕疵，當研究報告寫好並發表後，這些瑕疵就神奇地消失了。\n資料探勘與後設假設檢定(Data mining & post hoc hypothesising)。很多研究者可能曾親身實踐過的另一種歪曲資料手段，就是所謂的“資料挖掘”(更廣泛討論見 Gelman & Loken (2014) ，有關統計分析程序的“歧路花園”)。正如稍後將討論的問題類型，只要研究者不斷嘗試用不同方式分析資料，最終都會找到某種“看起來像”真實結果，實際上什麼都沒有的分析結果，這種手段被稱為“資料挖掘”。由於過去資料分析需要花好幾週才能完成，以前相當罕見，但是現在個人電腦的計算能力越來越強，能執行功能非常強大的統計軟體，所以這類案例變得越來越普遍。資料探勘其實並不是“錯誤的”，但是探勘得越多，風險就越大。對同一批資料做過多探勘，但是未在報告裡完全揭露，我懷疑是很常見的錯誤。也就是說，研究人員做了目前已知的各種可行分析方法，找到了正面結果，然後假裝這是他們唯一做過的分析。更糟糕的是，有些研究者會在分析完資料後，“發明”一個假設來掩蓋資料探勘。這裡我想鄭重說明，看到分析後改變一開始的想法並不是錯誤的，根據新的“後設”假設重新做分析資料也不是錯誤；真正的錯誤是從未承認你是看了資料才重新提出假設，而且我懷疑這種狀況很常見。如果研究人員承認自己這樣做了，其他研究人員就能評估這樣的行為對研究效度的影響。如果不承認，其他人就不能。這造成資料探勘具有欺騙性，非常糟糕!\n發表偏誤與自我審查(Publication bias & self-censoring)。最後一種處處可見的偏誤就是多數研究者不會報告負面結果，而這幾乎是不可能阻止的，因為學術期刊不會接受這樣的論文。許多心理學期刊喜歡那些發現了“新事物”的論文。假如有一項實驗是探討閱讀《芬尼根的守靈夜》會不會導致讀者瘋狂，有20組研究人員做了這項實驗，其中19組發現不會，你認為哪一組的論文會被接受?顯然就是那一項報告中發現《芬尼根的守靈夜》會導致瘋狂的研究。15 這是說明發表偏誤的一個案例。由於那19篇沒有發現效應的研究論文從未被正式發表，不知道學術界水有多深的讀者永遠不會知道這些證據。更糟糕的是，大多數研究人員“內化”了這種偏誤，並會自動地審查自我的研究。因為了解負面結果不會被接受發表，就不會想去寫出報告。如同我一位同事的說法“每一項成功發表的實驗，背後有10個失敗沒有看到”。其中關鍵是，雖然沒被發表的一些研究是因為無聊的原因而失敗(例如，研究者搞砸了某些事情)，但是其他研究可能是真正的“零”效果，研究者撰寫“良好”實驗計畫時就應該要承認這可能是真實結果，但是區分「失敗的研究結果」與「效果為零的研究結果」是相當難的事情。推薦大家閱讀 Ioannidis (2005) 的論文“為什麼大多數已發表的研究結果是錯誤的”；也建議去看看 Kühberger et al. (2014) 的成果，他們提供了發表偏誤在心理學領域確實存在的統計證據。\n\n關於學術詐欺還有很多問題需要好好思考，但是知道這些案例，對於剛開始學統計的同學來說已經足夠了。這裡想指出的是一個顯而易見的事實，那就是現實世界的科學是由活生生的人進行的，只有最天真的人才會想像每個科學工作者都是誠實和公正的。活生生的科學家通常不那麼天真，由於某些不明的因素，大部分的人被灌輸用天真的觀點看待這個世界，而許多統計學教授編寫的教科書似乎加強了這種刻板印象。"
  },
  {
    "objectID": "05-Drawing-graphs.html#footnotes",
    "href": "05-Drawing-graphs.html#footnotes",
    "title": "5  繪製統計圖",
    "section": "",
    "text": "The origin of this quote is Tufte’s lovely book The Visual Display of Quantitative Information.↩︎\n譯註~此處不照原文翻譯，是希望讀者回想2.2.5 連續與間斷變項談到的測量尺度與變項型態切換關係。↩︎\n這兩筆資料是被刻意輸入以便說明示範，並非原始資料裡的數值。↩︎\n連續兩個等號 “\\(==\\)” 表示符號兩邊的數值相等。在 單元 6 有進一步說明。↩︎"
  },
  {
    "objectID": "06-Pragmatic-matters.html#footnotes",
    "href": "06-Pragmatic-matters.html#footnotes",
    "title": "6  實務課題",
    "section": "",
    "text": "The quote comes from Home is the Hangman, published in 1975.↩︎\n譯註~原文這一段偏散文體，中文翻譯結合原作者的附註，整理為完整段落。↩︎\n請初學者留意，相等運算子與一般書寫公式用的”=“是不一樣的。jamovi僅使用”=“做為 輸入提示，使用者要在”=“之後輸入要運算的判斷式或函數。如果你輸入錯誤，會在試算表介面看到滿江紅的錯誤訊息。↩︎\n譯註~原文這一段描述的是早期版本的運算輸出，最新版已不相同，因此中譯重新改寫。↩︎\n這裡以最新版jamovi測試過，提醒一下留意邏輯運算子所輸出計算結果。如果輸入的算式只有一個邏輯運算子，像是2 + 2 == 5，回傳的結果是TRUE或FALSE；但是輸入的算式有交集或聯集運算子，像是(2+2 == 4) or (2+2 == 5)，回傳的結果是0或1。請同學儲存與運用計算結果時要留意。↩︎\n數的絕對值是其與零的距離,而不管其符號是負數還是正數。↩︎\n我們必須使用‘IF’命令並將零保持為零的原因是,您不能只使用likert.centred / opinion.strength來計算likert.centred的符號,因為在數學上,零除以零無法運作。嘗試一下即可看到↩︎\n如果您已經閱讀了後面更多內容,並在重新閱讀本節,那麼一個很好的示例是,有人選擇使用 AgeCats 作為分組變項進行方差分析,而不是使用 Age 作為預測變項進行迴歸。這種做法有時有很好的理由。例如,如果 Age 和您的結果變項之間的關係高度非線性,並且您不想嘗試運行非線性迴歸!但是,除非您真的有很好的理由這樣做,否則最好不要這樣做。這往往會引入各種其他問題(例如,資料可能會違反常態分佈假設),並且您可能會失去很多統計考驗力。↩︎\n我們稍后再作 box-cox 函數的討論↩︎"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "參考資料",
    "section": "",
    "text": "Adair, G. (1984). The hawthorne effect: A reconsideration of the\nmethodological artifact. Journal of Applied Psychology,\n69, 334–345.\n\n\nAgresti, A. (1996). An introduction to categorical data\nanalysis. Wiley.\n\n\nAgresti, A. (2002). Categorical data analysis (2nd ed.). Wiley.\n\n\nAkaike, H. (1974). A new look at the statistical model identification.\nIEEE Transactions on Automatic Control, 19, 716–723.\n\n\nAnscombe, F. J. (1973). Graphs in statistical analysis. American\nStatistician, 27, 17–21.\n\n\nBickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975). Sex bias in\ngraduate admissions: Data from Berkeley. Science,\n187, 398–404.\n\n\nBox, G. E. P. (1953). Non-normality and tests on variances.\nBiometrika, 40, 318–335.\n\n\nBox, J. F. (1987). Guinness, gosset, fisher, and small samples.\nStatistical Science, 2, 45–52.\n\n\nBrown, M. B., & Forsythe, A. B. (1974). Robust tests for equality of\nvariances. Journal of the American Statistical Association,\n69, 364–367.\n\n\nCampbell, D. T., & Stanley, J. C. (1963). Experimental and\nquasi-experimental designs for research. Houghton Mifflin.\n\n\nChronbach, L. J. (1951). Coefficient alpha and the internal structure of\ntests. Psychometrika, 16(3), 297–334.\n\n\nCochran, W. G. (1954). The χ2 test of goodness of\nfit. The Annals of Mathematical Statistics, 23,\n315–345.\n\n\nCohen, J. (1988). Statistical power analysis for the behavioral\nsciences (2nd ed.). Lawrence Erlbaum.\n\n\nCramer, H. (1946). Mathematical methods of statistics.\nPrinceton University Press.\n\n\nDunn, O. J. (1961). Multiple comparisons among means. Journal of the\nAmerican Statistical Association, 56, 52–64.\n\n\nEllis, P. D. (2010). The essential guide to effect sizes:\nStatistical power, meta-analysis, and the interpretation of research\nresults. Cambridge University Press.\n\n\nEvans, J. St. B. T., Barston, J. L., & Pollard, P. (1983). On the\nconflict between logic and belief in syllogistic reasoning. Memory\nand Cognition, 11, 295–306.\n\n\nEvans, M., Hastings, N., & Peacock, B. (2011). Statistical\ndistributions (3rd ed). Wiley.\n\n\nEveritt, B. S. (1996). Making sense of statistics in psychology. A\nsecond-level course. Oxford University Press.\n\n\nFabrigar, L. R., Wegener, D. T., MacCallum, R. C., & Strahan, E. J.\n(1999). Evaluating the use of exploratory factor analysis in\npsychological research. Psychological Methods, 4,\n272–299.\n\n\nFisher, R. A. (1922a). On the interpretation of χ2 from contingency\ntables, and the calculation of p. Journal of the Royal\nStatistical Society, 84, 87–94.\n\n\nFisher, R. A. (1922b). On the mathematical foundation of theoretical\nstatistics. Philosophical Transactions of the Royal Society A,\n222, 309–368.\n\n\nFisher, R. A. (1925). Statistical methods for research workers.\nOliver & Boyd.\n\n\nFox, J., & Weisberg, S. (2011). An R companion to\napplied regression (2nd ed.). Sage.\n\n\nGelman, A., & Loken, E. (2014). The statistical\ncrisis in science. American Scientist, 102(6),\n460+. https://doi.org/10.1511/2014.111.460\n\n\nGelman, A., & Stern, H. (2006). The difference between\n“significant” and “not significant” is not\nitself statistically significant. The American Statistician,\n60, 328–331.\n\n\nGeschwind, N. (1972). Language and the brain. Scientific\nAmerican, 226(4), 76–83.\n\n\nHays, W. L. (1994). Statistics (5th ed.). Harcourt Brace.\n\n\nHedges, L. V. (1981). Distribution theory for glass’s estimator of\neffect size and related estimators. Journal of Educational\nStatistics, 6, 107–128.\n\n\nHewitt, A. K., Foxcroft, D. R., & MacDonald, J. (2004).\nMultitrait-multimethod confirmatory factor analysis of the attributional\nstyle questionnaire. Personality and Individual Differences,\n37(7), 1483–1491.\n\n\nHogg, R. V., McKean, J. V., & Craig, A. T. (2005). Introduction\nto mathematical statistics (6th ed.). Pearson.\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test\nprocedure. Scandinavian Journal of Statistics, 6,\n65–70.\n\n\nHróbjartsson, A., & Gøtzsche, P. (2010). Placebo interventions for\nall clinical conditions. Cochrane Database of Systematic\nReviews, 1. https://doi.org//10.1002/14651858.CD003974.pub3\n\n\nHsu, J. C. (1996). Multiple comparisons: Theory and methods.\nChapman; Hall.\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are\nfalse. PLoS Med, 2(8), 697–701.\n\n\nJeffreys, H. (1961). The theory of probability (3rd ed.).\nOxford.\n\n\nJohnson, V. E. (2013). Revised standards for statistical evidence.\nProceedings of the National Academy of Sciences, 48,\n19313–19317.\n\n\nKahneman, D., & Tversky, A. (1973). On the psychology of prediction.\nPsychological Review, 80, 237–251.\n\n\nKass, R. E., & Raftery, A. E. (1995). Bayes factors. Journal of\nthe American Statistical Association, 90, 773–795.\n\n\nKeynes, J. M. (1923). A tract on monetary reform. Macmillan;\nCompany.\n\n\nKruschke, J. K. (2011). Doing Bayesian data analysis: A\ntutorial with R and BUGS. Academic Press.\n\n\nKruskal, W. H., & Wallis, W. A. (1952). Use of ranks in\none-criterion variance analysis. Journal of the American Statistical\nAssociation, 47, 583–621.\n\n\nKühberger, A., Fritz, A., & Scherndl, T. (2014). Publication bias in\npsychology: A diagnosis based on the correlation between effect size and\nsample size. Public Library of Science One, 9, 1–8.\n\n\nLarntz, K. (1978). Small-sample comparisons of exact levels for\nchi-squared goodness-of-fit statistics. Journal of the American\nStatistical Association, 73, 253–263.\n\n\nLee, M. D., & Wagenmakers, E.-J. (2014). Bayesian cognitive\nmodeling: A practical course. Cambridge University Press.\n\n\nLehmann, E. L. (2011). Fisher, Neyman, and the creation\nof classical statistics. Springer.\n\n\nLevene, H. (1960). Robust tests for equality of variances. In I. O. et\nal (Ed.), Contributions to probability and statistics: Essays in\nhonor of harold hotelling (pp. 278–292). Stanford University Press.\n\n\nMeehl, P. H. (1967). Theory testing in psychology and physics: A\nmethodological paradox. Philosophy of Science, 34,\n103–115.\n\n\nPearson, K. (1900). On the criterion that a given system of deviations\nfrom the probable in the case of a correlated system of variables is\nsuch that it can be reasonably supposed to have arisen from random\nsampling. Philosophical Magazine, 50, 157–175.\n\n\nPeterson, C., & Seligman, M. (1984). Causal explanations as a risk\nfactor for depression: Theory and evidence. Psychological\nReview, 91, 347–374.\n\n\nPfungst, O. (1911). Clever hans (the horse of mr. Von osten): A\ncontribution to experimental animal and human psychology (C. L.\nRahn, Trans.). Henry Holt.\n\n\nRosenthal, R. (1966). Experimenter effects in behavioral\nresearch. Appleton.\n\n\nSahai, H., & Ageel, M. I. (2000). The analysis of variance:\nFixed, random and mixed models. Birkhauser.\n\n\nShaffer, J. P. (1995). Multiple hypothesis testing. Annual Review of\nPsychology, 46, 561–584.\n\n\nShapiro, S. S., & Wilk, M. B. (1965). An analysis of variance test\nfor normality (complete samples). Biometrika, 52,\n591–611.\n\n\nSokal, R. R., & Rohlf, F. J. (1994). Biometry: The principles\nand practice of statistics in biological research (3rd ed.).\nFreeman.\n\n\nStevens, S. S. (1946). On the theory of scales of measurement.\nScience, 103, 677–680.\n\n\nStigler, S. M. (1986). The history of statistics. Harvard\nUniversity Press.\n\n\nStudent, A. (1908). The probable error of a mean. Biometrika,\n6, 1–2.\n\n\nTversky, A., & Kahneman, D. (1974). Judgment under uncertainty:\nHeuristics and biases. Science, 185(4157), 1124–1131.\n\n\nWelch, B. L. (1947). The generalization of\n“Student’s” problem when several different\npopulation variances are involved. Biometrika, 34,\n28–35.\n\n\nWelch, B. L. (1951). On the comparison of several mean values: An\nalternative approach. Biometrika, 38, 330–336.\n\n\nWilkinson, L., Wills, D., Rope, D., Norton, A., & Dubbs, R. (2006).\nThe grammar of graphics. Springer."
  },
  {
    "objectID": "Preface.html#英文版沿革與版權說明",
    "href": "Preface.html#英文版沿革與版權說明",
    "title": "前言",
    "section": "英文版沿革與版權說明",
    "text": "英文版沿革與版權說明\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA.\n\nPreface to Version 0.75\nIn this version we have updated the figures, images and text to maintain compatibility with latest versions of jamovi (2.2); many thanks to Peter Fisk for his help with this. Also tweaked and corrected are a few sections where improvements have been suggested by readers. This has mainly included fixing typos but also in places correcting conceptual detail, for example we have updated the information on kurtosis to reflect that it isn’t really about distribution “pointiness” and instead kurtosis is about whether data distributions have thin or fat tails. Thanks to all the readers who made suggestions, either through contacting me by email, or raising an issue on github.\nDavid Foxcroft\nFebruary 9th, 2022\n\n\nPreface to Version 0.70\nThis update from version 0.65 introduces some new analyses. In the ANOVA chapters we have added sections on repeated measures ANOVA and analysis of covariance (ANCOVA). In a new chapter we have introduced Factor Analysis and related techniques. Hopefully the style of this new material is consistent with the rest of the book, though eagle-eyed readers might spot a bit more of an emphasis on conceptual and practical explanations, and a bit less algebra. I’m not sure this is a good thing, and might add the algebra in a bit later. But it reflects both my approach to understanding and teaching statistics, and also some feedback I have received from students on a course I teach. In line with this, I have also been through the rest of the book and tried to separate out some of the algebra by putting it into a box or frame. It’s not that this stuff is not important or useful, but for some students they may wish to skip over it and therefore the boxing of these parts should help some readers.\nWith this version I am very grateful to comments and feedback received from my students and colleagues, notably Wakefield Morys-Carter, and also to numerous people all over the world who have sent in small suggestions and corrections - much appreciated, and keep them coming! One pretty neat new feature is that the example data files for the book can now be loaded into jamovi as an add-on module - thanks to Jonathon Love for helping with that.\nDavid Foxcroft\nFebruary 1st, 2019"
  },
  {
    "objectID": "Preface.html#版本沿革與版權說明",
    "href": "Preface.html#版本沿革與版權說明",
    "title": "前言",
    "section": "版本沿革與版權說明",
    "text": "版本沿革與版權說明\n中文化電子書翻譯及改編自統計學教材~ Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15\n中文化電子書採用姓名標示-相同方式分享 4.0 國際開放授權釋出。任何人得以任何媒介或格式重製及散布本素材，或重混、轉換本素材、及依本素材建立新素材且為任何目的，包含商業性質之使用。只要使用者遵守授權條款規定，授權人不能撤回任何使用者使用本素材的自由。"
  },
  {
    "objectID": "Preface.html#英文版版本沿革與版權說明",
    "href": "Preface.html#英文版版本沿革與版權說明",
    "title": "前言",
    "section": "英文版版本沿革與版權說明",
    "text": "英文版版本沿革與版權說明\nThis book is an adaptation of DJ Navarro (2018). Learning statistics with R: A tutorial for psychology students and other beginners. (Version 0.6). https://learningstatisticswithr.com/.\nThe book is released under a creative commons CC BY-SA 4.0 licence. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license - CC BY-SA.\n\nPreface to Version 0.75\nIn this version we have updated the figures, images and text to maintain compatibility with latest versions of jamovi (2.2); many thanks to Peter Fisk for his help with this. Also tweaked and corrected are a few sections where improvements have been suggested by readers. This has mainly included fixing typos but also in places correcting conceptual detail, for example we have updated the information on kurtosis to reflect that it isn’t really about distribution “pointiness” and instead kurtosis is about whether data distributions have thin or fat tails. Thanks to all the readers who made suggestions, either through contacting me by email, or raising an issue on github.\nDavid Foxcroft\nFebruary 9th, 2022\n\n\nPreface to Version 0.70\nThis update from version 0.65 introduces some new analyses. In the ANOVA chapters we have added sections on repeated measures ANOVA and analysis of covariance (ANCOVA). In a new chapter we have introduced Factor Analysis and related techniques. Hopefully the style of this new material is consistent with the rest of the book, though eagle-eyed readers might spot a bit more of an emphasis on conceptual and practical explanations, and a bit less algebra. I’m not sure this is a good thing, and might add the algebra in a bit later. But it reflects both my approach to understanding and teaching statistics, and also some feedback I have received from students on a course I teach. In line with this, I have also been through the rest of the book and tried to separate out some of the algebra by putting it into a box or frame. It’s not that this stuff is not important or useful, but for some students they may wish to skip over it and therefore the boxing of these parts should help some readers.\nWith this version I am very grateful to comments and feedback received from my students and colleagues, notably Wakefield Morys-Carter, and also to numerous people all over the world who have sent in small suggestions and corrections - much appreciated, and keep them coming! One pretty neat new feature is that the example data files for the book can now be loaded into jamovi as an add-on module - thanks to Jonathon Love for helping with that.\nDavid Foxcroft\nFebruary 1st, 2019\n\n\nPreface to Version 0.65\nIn this adaptation of the excellent ‘Learning statistics with R’, by Danielle Navarro, we have replaced the statistical software used for the analyses and examples with jamovi. Although R is a powerful statistical programming language, it is not the first choice for every instructor and student at the beginning of their statistical learning. Some instructors and students tend to prefer the point-and-click style of software, and that’s where jamovi comes in. jamovi is software that aims to simplify two aspects of using R. It offers a point-and-click graphical user interface (GUI), and it also provides functions that combine the capabilities of many others, bringing a more SPSS- or SAS-like method of programming to R. Importantly, jamovi will always be free and open - that’s one of its core values - because jamovi is made by the scientific community, for the scientific community.\nWith this version I am very grateful for the help of others who have read through drafts and provided excellent suggestions and corrections, particularly Dr David Emery and Kirsty Walter.\nDavid Foxcroft\nJuly 1st, 2018\n\n\nPreface to Version 0.6\nThe book hasn’t changed much since 2015 when I released Version 0.5 – it’s probably fair to say that I’ve changed more than it has. I moved from Adelaide to Sydney in 2016 and my teaching profile at UNSW is different to what it was at Adelaide, and I haven’t really had a chance to work on it since arriving here! It’s a little strange looking back at this actually. A few quick comments…\n\nWeirdly, the book consistently misgenders me, but I suppose I have only myself to blame for that one :-) There’s now a brief footnote on page 12 that mentions this issue; in real life I’ve been working through a gender affirmation process for the last two years and mostly go by she/her pronouns. I am, however, just as lazy as I ever was so I haven’t bothered updating the text in the book.\nFor Version 0.6 I haven’t changed much I’ve made a few minor changes when people have pointed out typos or other errors. In particular it’s worth noting the issue associated with the etaSquared function in the lsr package (which isn’t really being maintained any more) in Section 14.4. The function works fine for the simple examples in the book, but there are definitely bugs in there that I haven’t found time to check! So please take care with that one.\nThe biggest change really is the licensing! I’ve released it under a Creative Commons licence (CC BY-SA 4.0, specifically), and placed all the source files to the associated GitHub repository, if anyone wants to adapt it.\n\nMaybe someone would like to write a version that makes use of the tidyverse… I hear that’s become rather important to R these days :-)\nBest,\nDanielle Navarro\n\n\nPreface to Version 0.5\nAnother year, another update. This time around, the update has focused almost entirely on the theory sections of the book. Chapters 9, 10 and 11 have been rewritten, hopefully for the better. Along the same lines, Chapter 17 is entirely new, and focuses on Bayesian statistics. I think the changes have improved the book a great deal. I’ve always felt uncomfortable about the fact that all the inferential statistics in the book are presented from an orthodox perspective, even though I almost always present Bayesian data analyses in my own work. Now that I’ve managed to squeeze Bayesian methods into the book somewhere, I’m starting to feel better about the book as a whole. I wanted to get a few other things done in this update, but as usual I’m running into teaching deadlines, so the update has to go out the way it is!\nDanielle Navarro\nFebruary 16, 2015\n\n\nPreface to Version 0.4\nA year has gone by since I wrote the last preface. The book has changed in a few important ways: Chapters 3 and 4 do a better job of documenting some of the time saving features of Rstudio, Chapters 12 and 13 now make use of new functions in the lsr package for running chi-square tests and t tests, and the discussion of correlations has been adapted to refer to the new functions in the lsr package. The soft copy of 0.4 now has better internal referencing (i.e., actual hyperlinks between sections), though that was introduced in 0.3.1. There’s a few tweaks here and there, and many typo corrections (thank you to everyone who pointed out typos!), but overall 0.4 isn’t massively different from 0.3.\nI wish I’d had more time over the last 12 months to add more content. The absence of any discussion of repeated measures ANOVA and mixed models more generally really does annoy me. My excuse for this lack of progress is that my second child was born at the start of 2013, and so I spent most of last year just trying to keep my head above water. As a consequence, unpaid side projects like this book got sidelined in favour of things that actually pay my salary! Things are a little calmer now, so with any luck version 0.5 will be a bigger step forward.\nOne thing that has surprised me is the number of downloads the book gets. I finally got some basic tracking information from the website a couple of months ago, and (after excluding obvious robots) the book has been averaging about 90 downloads per day. That’s encouraging: there’s at least a few people who find the book useful!\nDanielle Navarro\nFebruary 4, 2014\n\n\nPreface to Version 0.3\nThere’s a part of me that really doesn’t want to publish this book. It’s not finished.\nAnd when I say that, I mean it. The referencing is spotty at best, the chapter summaries are just lists of section titles, there’s no index, there are no exercises for the reader, the organisation is suboptimal, and the coverage of topics is just not comprehensive enough for my liking. Additionally, there are sections with content that I’m not happy with, figures that really need to be redrawn, and I’ve had almost no time to hunt down inconsistencies, typos, or errors. In other words, this book is not finished. If I didn’t have a looming teaching deadline and a baby due in a few weeks, I really wouldn’t be making this available at all.\nWhat this means is that if you are an academic looking for teaching materials, a Ph.D. student looking to learn R, or just a member of the general public interested in statistics, I would advise you to be cautious. What you’re looking at is a first draft, and it may not serve your purposes. If we were living in the days when publishing was expensive and the internet wasn’t around, I would never consider releasing a book in this form. The thought of someone shelling out $80 for this (which is what a commercial publisher told me it would retail for when they offered to distribute it) makes me feel more than a little uncomfortable. However, it’s the 21st century, so I can post the pdf on my website for free, and I can distribute hard copies via a print-on-demand service for less than half what a textbook publisher would charge. And so my guilt is assuaged, and I’m willing to share! With that in mind, you can obtain free soft copies and cheap hard copies online, from the following webpages:\nSoft copy: [www.compcogscisydney.com/learning-statistics-with-r.html](https://www.compcogscisydney.com/learning-statistics-with-r\nHard copy: www.lulu.com/content/13570633\n[Ed: these links are defunct, try this instead: learningstatisticswithr.com]\nEven so, the warning still stands: what you are looking at is Version 0.3 of a work in progress. If and when it hits Version 1.0, I would be willing to stand behind the work and say, yes, this is a textbook that I would encourage other people to use. At that point, I’ll probably start shamelessly flogging the thing on the internet and generally acting like a tool. But until that day comes, I’d like it to be made clear that I’m really ambivalent about the work as it stands.\nAll of the above being said, there is one group of people that I can enthusiastically endorse this book to: the psychology students taking our undergraduate research methods classes (DRIP and DRIP:A) in 2013. For you, this book is ideal, because it was written to accompany your stats lectures. If a problem arises due to a shortcoming of these notes, I can and will adapt content on the fly to fix that problem. Effectively, you’ve got a textbook written specifically for your classes, distributed for free (electronic copy) or at near-cost prices (hard copy). Better yet, the notes have been tested: Version 0.1 of these notes was used in the 2011 class, Version 0.2 was used in the 2012 class, and now you’re looking at the new and improved Version 0.3. I’m not saying these notes are titanium plated awesomeness on a stick – though if you wanted to say so on the student evaluation forms, then you’re totally welcome to – because they’re not. But I am saying that they’ve been tried out in previous years and they seem to work okay. Besides, there’s a group of us around to troubleshoot if any problems come up, and you can guarantee that at least one of your lecturers has read the whole thing cover to cover!\nOkay, with all that out of the way, I should say something about what the book aims to be. At its core, it is an introductory statistics textbook pitched primarily at psychology students. As such, it covers the standard topics that you’d expect of such a book: study design, descriptive statistics, the theory of hypothesis testing, t-tests, χ 2 tests, ANOVA and regression. However, there are also several chapters devoted to the R statistical package, including a chapter on data manipulation and another one on scripts and programming. Moreover, when you look at the content presented in the book, you’ll notice a lot of topics that are traditionally swept under the carpet when teaching statistics to psychology students. The Bayesian/frequentist divide is openly disussed in the probability chapter, and the disagreement between Neyman and Fisher about hypothesis testing makes an appearance. The difference between probability and density is discussed. A detailed treatment of Type I, II and III sums of squares for unbalanced factorial ANOVA is provided. And if you have a look in the Epilogue, it should be clear that my intention is to add a lot more advanced content.\nMy reasons for pursuing this approach are pretty simple: the students can handle it, and they even seem to enjoy it. Over the last few years I’ve been pleasantly surprised at just how little difficulty I’ve had in getting undergraduate psych students to learn R. It’s certainly not easy for them, and I’ve found I need to be a little charitable in setting marking standards, but they do eventually get there. Similarly, they don’t seem to have a lot of problems tolerating ambiguity and complexity in presentation of statistical ideas, as long as they are assured that the assessment standards will be set in a fashion that is appropriate for them. So if the students can handle it, why not teach it? The potential gains are pretty enticing. If they learn R, the students get access to CRAN, which is perhaps the largest and most comprehensive library of statistical tools in existence. And if they learn about probability theory in detail, it’s easier for them to switch from orthodox null hypothesis testing to Bayesian methods if they want to. Better yet, they learn data analysis skills that they can take to an employer without being dependent on expensive and proprietary software.\nSadly, this book isn’t the silver bullet that makes all this possible. It’s a work in progress, and maybe when it is finished it will be a useful tool. One among many, I would think. There are a number of other books that try to provide a basic introduction to statistics using R, and I’m not arrogant enough to believe that mine is better. Still, I rather like the book, and maybe other people will find it useful, incomplete though it is.\nDanielle Navarro\nJanuary 13, 2013"
  },
  {
    "objectID": "Preface.html#中文化版本沿革與版權說明",
    "href": "Preface.html#中文化版本沿革與版權說明",
    "title": "前言",
    "section": "中文化版本沿革與版權說明",
    "text": "中文化版本沿革與版權說明\n中文化電子書翻譯及改編自統計學教材~ Navarro DJ and Foxcroft DR (2022). learning statistics with jamovi: a tutorial for psychology students and other beginners. (Version 0.75). DOI: 10.24384/hgc3-7p15\n中文化電子書採用姓名標示-相同方式分享 4.0 國際開放授權釋出。任何人得以任何媒介或格式重製及散布本素材，或重混、轉換本素材、及依本素材建立新素材且為任何目的，包含商業性質之使用。只要使用者遵守授權條款規定，授權人不能撤回任何使用者使用本素材的自由。\n\n中文化編修進度表\n\nAI初翻: 已全部完成。\n第一次編修: 依各單元完成次單元估計。\n第二次編修: 歡迎使用者透過各單元網頁右方編輯本頁面或問報問題的留言連結，協助編修及提出指正。\n\n\n\n\n單元\nAI初翻\n第一次編修\n第二次編修\n\n\n\n\n單元 1\n100%\n100%\n0%\n\n\n單元 2\n100%\n100%\n0%\n\n\n單元3\n100%\n100%\n0%\n\n\n單元 4\n100%\n100%\n0%\n\n\n單元 5\n100%\n100%\n0%\n\n\n單元 6\n100%\n100%\n0%\n\n\n中場故事\n100%\n100%\n0%\n\n\n單元 8\n100%\n100%\n0%\n\n\n單元 8\n100%\n100%\n0%\n\n\n單元 9\n100%\n100%\n0.5%\n\n\n單元 10\n100%\n79%\n0%\n\n\n單元 11\n100%\n0%\n0%\n\n\n單元 12\n100%\n0%\n0%\n\n\n單元 13\n100%\n0%\n0%\n\n\n單元 14\n100%\n0%\n0%\n\n\n單元 15\n100%\n75%\n0%\n\n\n單元 16\n100%\n0%\n0%\n\n\n後記\n100%\n5%\n0%"
  },
  {
    "objectID": "Preface.html#中文化編修進度表",
    "href": "Preface.html#中文化編修進度表",
    "title": "前言",
    "section": "中文化編修進度表",
    "text": "中文化編修進度表\n\n\n\n單元\nAI初翻\n第一次編修\n第二次編修\n\n\n\n\n單元1\n100%\n\n\n\n\n單元2\n100%\n\n\n\n\n單元3\n100%\n\n\n\n\n單元4\n100%\n\n\n\n\n單元5\n100%\n\n\n\n\n單元6\n100%\n\n\n\n\n中途單元\n100%\n\n\n\n\n單元7\n100%\n\n\n\n\n單元8\n100%\n\n\n\n\n單元9\n100%\n\n\n\n\n單元10\n100%\n\n\n\n\n單元11\n100%\n\n\n\n\n單元12\n100%\n\n\n\n\n單元13\n100%\n\n\n\n\n單元14\n100%\n\n\n\n\n單元15\n100%\n\n\n\n\n單元16\n100%"
  },
  {
    "objectID": "Epilogue.html#未被發現的統計學",
    "href": "Epilogue.html#未被發現的統計學",
    "title": "後記",
    "section": "未被發現的統計學",
    "text": "未被發現的統計學\n首先,我將談論一些我希望擠進本書版本中的內容,這樣您就可以對世界上還有什么其他統計思想有所了解。即使這本書快要最終完成,我也認為這一點很重要。學生們經常沒有意識到,他們的入門統計課只是個入門。如果您想走向廣闊的世界並進行真正的數據分析,您必須學會大量擴展本科課程內容的新工具。不要假定某些事情無法完成,只因為本科並未涵蓋。也不要假定某些事情就是正確的,只因為它在本科課上被涵蓋了。為了防止您陷入這個陷阱,我認為有必要概述一些其他的思路。\n\n涵蓋主題中的遺漏\n即使在我在書中涵蓋的主題中,也有很多遺漏我希望能在未來的版本中補充。僅就純統計學而言(而不是與 jamovi 相關的內容),以下是一份代表性但並不詳盡的清單,我希望能在某個時候擴展這些內容:\n\n其他類型的相關性。在 單元 12 中,我談到了兩種相關性:皮爾遜和斯皮爾曼。當您有兩個連續變量並希望評估它們之間的關係時,這兩種相關性評估方法都是適用的。如果您的變量都是名義尺度的,那麼情況又如何呢? 或者一個是名義尺度,另一個是連續的呢? 實際上,在這些情況下也存在計算相關性的方法(例如 polychoric 相關),將它們包括進來會很好。\n更多關於效應量的詳細信息。 總的來說,我認為全文中對效應量的處理有點過于簡單。 在幾乎每一種情況下,我都傾向於僅選擇一種效應量測度(通常是最流行的),並描述它。 然而,對於幾乎所有的測試和模型,都存在多種思考效應量的方法,我希望在未來能更詳細地討論這一點。\n處理被違反的假設。 在本書的若干部分中,我談到了在發現您的測試(或模型)的假設被違反時可以採取的一些措施,但我認為在這方面我應該說得更多。特別是,我認為討論更詳細的變量轉換以解決問題的內容會很好。我在 單元 6 中稍微談到了這一點,但我認為討論還不夠詳細。\n回歸的交互項。在 單元 14 中,我談到過方差分析中可以有交互項,我也指出方差分析可以被解釋為某種線性回歸模型。 然而,在 單元 12 中談及回歸時,我完全沒有提及交互。 然而,沒有什么能阻止您在回歸模型中包含交互項。 當您在談論兩個連續預測變量的交互時,弄清“交互”實際上意味著什么會稍微複雜一些,而且可以有多種方法。儘管如此,我本想稍微談論一下這個話題。\n計劃比較法。 正如我在 單元 14 中所提到的,在進行方差分析時,使用像 Tukey HSD 這樣的事後校正並不總是合適的,特別是當您事前就有非常明確(和有限)的比較關注點時。我希望將來能更多地談論這一點。\n多重比較方法。 即使在討論事後檢驗和多重比較的背景下,我也希望能更詳細地討論這些方法,並談論除了我提到的幾種選擇之外還存在哪些方法。\n\n\n\n書中遺漏的統計模型\n統計學是一個巨大的領域。我在這本書中描述的核心工具(卡方檢驗、t檢驗、回歸和方差分析)是廣泛使用的基本數據分析工具,它們構成了大多數入門統計書的核心。然而,還有很多其他工具。有如此多的數據分析情境這些工具無法涵蓋,給您一種感覺,這其中還有很多值得了解的,例如:\n\n非線性回歸。在 單元 12 中討論回歸時,我們看到回歸假定預測變量與結果變量之間的關係是線性的。另一方面,當我們在 單元 4 中討論了更簡單的相關性問題時,我們看到確實存在能夠評估變量之間非線性關係的工具(例如斯皮爾曼相關)。統計中有許多工具可以用於進行非線性回歸。例如,一些非線性回歸模型假定預測變量與結果變量之間的關係是單調的(例如等分回歸),而其他則假定它是平滑但不一定是單調的(例如 Lowess 回歸)另一些則假定關係的形式是已知的非線性形式(例如多項式回歸)。\n邏輯回歸。當結果變量是二元的但預測變量是連續的時,回歸的另一種變體。例如,假設您正在研究社交媒體,並希望了解是否可以根據收入、年齡等變量預測某人是否在 Twitter 上。這基本上是一種回歸模型,但您無法使用常規的線性回歸,因為結果變量是二元的(您是否在 Twitter 上)。由於結果變量是二元的,殘差不可能正常分佈。統計學家可以將這種情況應用許多工具,其中最突出的是邏輯回歸。\n廣義線性模型(GLM):GLM 實際上是包含邏輯回歸、線性回歸、(某些)非線性回歸、方差分析和許多其他模型的模型家族。 GLM 中的基本思想與支撐線性模型的思想基本相同,但是它允許您的數據可能不是正常分佈的,並允許預測變量與結果變量之間的非線性關係。有很多非常方便的分析屬於 GLM,所以了解它非常有用。\n存活分析。在 單元 2 中,我談到了“差異減員”,即人們以非隨機方式退出研究的趨勢。當時,我是將其作為一種潛在的方法論問題而談論的,但在很多情況下,差異減員實際上就是您感興趣的事情。例如,假設您有興趣了解人們在一次遊戲中玩不同類型的遊戲的時間有多長。人們會否傾向於連續玩實時戰略遊戲的時間長於第一人稱射擊遊戲?您可能會這樣設計您的研究。人們進入實驗室,他們可以玩盡可能長或短的時間。 一旦他們玩完了,您就記錄他們玩的時間。 然而,由于伦理限制,假設您不能讓他們玩超過兩小時。 很多人在兩小時限制之前就會停止遊戲,所以您會正確知道他們玩了多久。 但有些人會遇到兩小時的限制,所以如果允許研究繼續進行,您不知道他們會玩多久。 因此,您的數據會受到系統性刪減:您遺漏了所有非常長的時間。 您如何明智地分析這些數據? 這就是存活分析要解決的問題。 它是專門設計來處理這種情況的,當研究結束時,您會系統性地遺漏某些“一邊”的數據。 它在健康研究中應用非常廣泛,並且在這種情況下,通常被字面意義上用於分析存活。例如,您可能正在跟踪某種癌症的病人,其中一些人接受了治療 A,其他人接受了治療 B,但您只有資金跟踪他們 5 年。 在研究結束時,一些人還活著,其他人已經死了。 在這種情況下,存活分析可用於確定治療的有效性,並告知您他們隨時間面臨的死亡風險。\n混合模型:重複測量方差分析通常用於觀察值聚集在實驗單位中的情況。一個很好的例子是當您在多個時間點跟蹤個人時。假設您正在跟踪兩個人的快樂情緒隨時間的變化。亞倫的快樂指數最初為 10 分,然後下降到 8 分,然後下降到 6 分。貝琳達的快樂指數最初為 6 分,然後上升到 8 分,然后上升到 10 分。這兩個人的整體快樂水平相同(三個時間點的平均值均為 8 分),因此重複測量方差分析會以相同方式對待亞倫和貝琳達。但這明顯是錯誤的。亞倫的快樂正在降低,而貝琳達的快樂正在增加。 如果您想要優化分析實驗數據,其中人們可以隨時間改變,那麼您需要比重複測量方差分析更強大的工具。 人們用來解決這個問題的工具稱為“混合”模型,因為它們旨在了解個體實驗單位的信息(例如個人的快樂隨時間變化),以及總體影響(例如金錢對快樂隨時間推移的影響)。 重複測量方差分析也許是最簡單的混合模型例子,但使用混合模型可以做很多重複測量方差分析無法做到的事情。\n多維尺度。因子分析是“無監督學習”模型的一個例子。這意味著,與我提到的大多數“有監督學習”工具不同,您無法將變量分為預測變量和結果變量。回歸是有監督學習,而因子分析是無監督學習。這並不是唯一一種無監督學習模型。例如,在因子分析中,研究人員關心變量之間的相關分析。然而,在很多情況下,您實際上有興趣分析對象、項目或人之間的相似性或差異。在這種情況下,您可以使用多種工具,最知名的就是多維尺度法(MDS)。 在 MDS 中,思路是為您的項目找到一種“幾何”表示。每個項目被“繪製”為某個空間中的一點,兩點之間的距離是這兩個項目差異的一種測量。\n聚類:無監督學習模型的另一個例子是聚類(也稱為分類),在這種情況下,您希望將所有項目組織成有意義的組,使得相似的項目被分配到同一組中。大量聚類屬於無監督類型,這意味著您不知道組是什麼,您只能猜測。還有其他“有監督聚類”情況,您需要基於其他變量預測組成員資格,並且這些組成員資格實際上是可觀察到的。邏輯回歸是以這種方式工作的工具的一個很好的例子。然而,當您實際上不知道組成員資格時,您必須使用不同的工具(例如 k 均值聚類)。甚至還有一些情況下您想要執行所謂的“半監督聚類”,在這種情況下,您知道某些項目的組成員資格但並非所有。您可以想像,聚類是一個相當大的主題,也是一項相當有用的技能。\n\n當然,即使這個列表也不完整。我沒有提到時間序列分析、項目反應理論、市場籃分析、分類與回歸樹或其他大量主題。然而,我上面給出的列表基本上是我對這本書的期望清單。當然,它會使書的長度增加一倍,但這意味著範圍已經廣泛到足以涵蓋心理學應用研究者需要使用的大多數內容。\n\n\n別的推論方式\n本書的另一個不完整之處在於它比較嚴重地專注於如何進行推論統計的一種非常狹隘且過時的觀點。在 單元 8 中,我稍微談到了無偏估計、抽樣分布等概念。 在 單元 9 中,我更詳細地談到了空假設顯著性檢驗和 p 值的理論。這些想法可以追溯到 20 世紀初,而我在書中談到的工具高度依賴那個時代的理論思想。我覺得有義務堅持這些主題,因為科學中絕大多數數據分析也依賴這些思想。然而,統計理論的研究並不仅限於這些主題,儘管由於其實際重要性,每個人都應該了解它們,但在許多方面,這些思想並不能代表當代數據分析的最佳實踐。我特別高興的是,我已經能夠有所超越。單元 16 現在以合理的細節呈現了貝葉斯觀點,但整體而言,書中仍然相當偏向次數主義正統思想。此外,還有許多其他值得一提的推論方法:\n\n自助法:在介紹每個假設檢驗時,我都有強烈傾向於簡單地做出類似“BLAH 的抽樣分布是 t 分布”之類的斷言。在某些情況下,我實際上嘗試證明這一斷言。例如,在 單元 10 中談到 \\(chi^2\\)檢驗時,我引用了正態分布與 \\(chi^2\\) 分布之間的已知關係(參見 單元 7 ),以解釋我們如何最終假設適配度統計量的抽樣分布是 \\(chi^2\\)。 然而,也有很多這些抽樣分布是,嗯,錯誤的。 \\(chi^2\\) 檢驗就是一個很好的例子。它基於對您的数据的分布的假設,而該假設被知道在小樣本量下是錯誤的! 在 20 世紀初,面對這種情況您幾乎無能為力。統計學家得出了數學結果,即“在有關數據的 BLAH 假設下,抽樣分布大致為 BLAH”,這已經是最好的了。在很多情况下,他們甚至沒有這個。有很多數據分析情況沒有人找到所需的抽樣分布的數學解。 所以,直到 20 世紀后期,相應的檢驗要么不存在要么無法工作。 然而,計算機現在已經改變了這一切。 您現在可以使用各種高端技巧和一些不那么高端的技巧來解決這個問題。 最簡單的方法是自助法,最簡單的形式非常簡單。 您要做的就是模擬實驗結果很多很多次,同時假定 (a) 空假設為真和 (b) 未知的人群分布實際上看起來非常類似於您的原始數據。 換句話說,與其假設數據(例如)是正態分布的,不如假设数据实际上和您的样本一样,然后使用计算机模拟检验统计量的抽样分布,如果该假设成立的话。尽管依赖一些有点可疑的假设(即人群分布与样本相同!),但自助法是一个在大量数据分析问题上实际应用中奇迹般有效的快速简单的方法。\n交叉驗證:在我的統計課上偶爾會出現一個問題,通常是學生試圖挑釁時提出的,那就是“我們為什么要關心推論統計?為什么不只描述樣本呢?” 這個問題的答案通常如下:“因為作為科學家,我們的真正興趣不在於我們_過去_已經觀察到的特定樣本,我們想要對未來可能觀察到的數據做出預測”。 統計推論中很多問題的產生都是因為我們總是認為未來會與過去類似但有些不同。 或者,更一般地說,新數據不會和舊數據完全相同。 在很多情況下,我們嘗試推導出數學規則,這些規則可以幫助我們得出對於新數據最有可能正確的推論,而不是選擇最能描述舊數據的語句。例如,給定模型 A 和 B 以及今天您收集的數據集 X,嘗試選擇明天您要收集的新數據集 Y 的最佳模型。有時模擬這個過程很方便,這就是交叉驗證要做的事情。 您要做的就是將數據集劃分為兩個子集 X1 和 X2。 使用子集 X1 訓練模型(例如,估計回歸系數),但 then 在另一個子集 X2 上評估模型性能。 這為您提供了模型從舊數據延伸到新數據的能力的一種測量,這通常是比只使用完整數據集 X 擬合模型所得到的模型好壞測量更好的測量。\n穩健統計:生活很複雜,沒有什么能按應有的方式運作。對於統計來說也是如此,在嘗試分析數據時,我們經常會遇到各種問題,數據的混亂程度比應該的要高。 變量本應正常分布但實際上並非正常分布,關係本應線性但實際上並非線性,您的數據集中的一些觀察結果幾乎肯定是错误的(即,并未測量應測量的內容)。 在本書的大部分統計理論中都忽略了所有这种混乱。 然而,忽略问题并不总是能解决问题。有時,忽略混亂确实可以,因为某些类型的统计工具是“穩健的”,即使数据并不满足您的理论假设,它们仍能很好地工作。 其他类型的统计工具则不是稳健的,即使偏离理论假设很小也会导致它们失效。 稳健统计是统计学的一个分支,它研究这个问题,并谈论统计量的“破坏点”。 也就是说,您的数据必须混亂到什么程度统计量才不再可信?我在一些地方提到了这一点。均值不是變量中心趨勢的稳健估计量,但中位數是稳健的。例如,假设我告訴您我五个最好朋友的年齡分別為 34、39、31、43 和 4003 歲。您認為他們的平均年齡是多少?也就是說,這里的真實人群均值是多少?如果您使用樣本均值作為人群均值的估計量,那麼您得到的答案是 830 歲。 如果您使用樣本中位數作為人群均值的估計量,那麼您得到的答案是 39 歲。 請注意,即使在第二種情況下您在“技術上”做錯了事(使用中位數估計均值!),但您實際獲得了更好的答案。這里的问题是其中一個觀察結果顯然、明顯地是錯的。 我沒有一個 4003 歲的朋友。這可能是打字錯誤,我可能是想打 43。 但是如果我敲錯了,敲成了 53 而不是 43 呢? 您能肯定這是錯字還是不是? 有時數據中的錯誤很隱蔽,所以您無法通過目測樣本來檢測它們,但它們仍然會污染您的数据,並且仍然會影響您的結論。 稳健统计关注的是,即使面临您不知道的污染,您如何能够进行安全推論。 這是相當酷的東西。\n\n\n\n雜項主題\n\n假設您正在進行一項調查,並對運動和體重感興趣。 您向四個人發送了數據。 亞當說他經常運動,並且沒有超重。 布莉歐妮說她經常運動,並且沒有超重。 卡羅爾說她不運動,並超重。 蒂姆說他不運動,並拒絕回答他的體重問題。 艾琳沒有返回調查。 您現在有一個遺失數據的問題。有一整個調查遺失了,另一個遺失了一個問題,您對此該怎么辦? 忽略缺失數據通常不是一種安全的事情。 讓我們考慮蒂姆的調查。 首先,請注意,根據他的其他回答,他似乎與卡羅爾(我們都不運動)更相似,而不是與亞當或布莉歐妮更相似。 所以,如果您被迫猜測他的體重,您會猜測他比他們更接近她。 也許您會做一些校正,考慮到亞當和蒂姆是男性,而布莉歐妮和卡羅爾是女性。 這種類型猜測的統計名稱是“插補”。 安全地進行插補是很困難的,但它很重要,特別是當遺失的数据以有系統的方式遺失時。 由于社會壓力迫使超重的人常感到自己的體重很差(通常是由于公共衛生運動的緣故),所以我們實際上有理由懷疑不回應的人比回應調查的人更有可能超重。 給蒂姆插補體重意味着如果我們忽略蒂姆,樣本中超重人數將從 3 分之 1 增加到 4 分之 2(如果我們給蒂姆插補體重)。 顯然,這很重要。但是明智地這樣做比它聽起來更複雜。 早些時候,我建議您应将蒂姆视为卡羅爾,因为他們對運動問題給出了相同的答案。 但這并不完全正确。 他們之间存在有系統的差异。 她回答了問題,蒂姆沒有。 鑑于超重人群面临的社会壓力,蒂姆不是_比_卡羅爾更超重嗎? 當然,這仍然忽略了一個事實,即將_单个_體重插補給蒂姆是不明智的,就好像您實際上知道他的體重一樣。 相反,您需要做的是插補一系列合理的猜測(稱為多重插補),以捕捉您對蒂姆體重的不確定性大於對卡羅爾體重的不確定性这一事实。 我們還沒有開始討論艾琳沒有發回調查所帶來的問題。 您可以想像,處理遺失數據日益成為一個重要話題。 事實上,有人告訴我,如果未遵循某種明智的多重插補計劃,某些領域的很多期刊將不會接受具有遺失數據的研究。\n考驗力分析:在@sec-Hypothesis-testing中,我討論了考驗力的概念(即,如果效應實際存在,您有多大可能性能檢測到它)並提到了功效分析,這是一組用於評估您的研究功效的有用工具。功效分析在規劃研究(例如,確定您可能需要的樣本量有多大)很有用,但在分析您已經收集的數據時也起著有用的作用。例如,假設您得到了顯著結果,並且有效應量的估計。您可以使用這些信息來估計您的研究實際具有多大的功效。這還是有點有用的,特別是如果您的效應量不大。例如,假設您在 \\(p&lt; .05\\) 水平上拒絕了空假設,但您使用功效分析得出您的估計功效只有 .08。顯著結果意味著,如果空假設實際上為真,得到這樣的数据的機率為 5%。 但是低功效意味著,即使空假設為假,效應量實際上和它看起來一樣小,得到您得到的數據的機率也只有 8%。這表明您需要相當謹慎,因為運氣似乎在您的結果中起了很大作用,不管是哪一種方式!\n使用理論驅動模型進行數據分析。在本書的一些地方,我提到了反應時間(RT)數據,您記錄某人完成某件事所需的時間(例如,做出簡單決定)。我提到 RT 數據幾乎總是非正態的,並且正偏。此外,還有所謂的速度/精度權衡:如果您嘗試過快做出決定(RT 較低),那麼您可能會做出較差的決定(精度較低)。因此,如果您測量了參與者決策的精度和他們的 RT,您可能會發現速度和精度之間存在關係。當然還有更多內容,因為與反應速度無關,一些人的決策優於其他人。此外,速度取決于認知過程(即思考花費的時間)和生理過程(例如,您能夠移動肌肉的速度有多快)。 分析這些數據的過程聽起來會很複雜。的確,當您深入研究心理學文獻時,您會發現已經存在數學模型(稱為“序列採樣模型”),這些模型描述了人們如何做出簡單的決定,並且這些模型考慮到了我上面提到的許多因素。 您在標準統計教科書中找不到任何這些理論驅動的模型。 標准統計教科書描述了標准工具,這些工具可以有意義地應用于許多不同的學科,不僅僅是心理學。 方差分析就是一個標准工具的例子,它適用于心理學和藥理學。 順序採樣模型不是,它們或多或少是專門針對心理學的。 這並没有使它們成為更加強大的工具。 事實上,如果您正在分析人們必須快速做出選擇的數據,您應該真正使用序列採樣模型來分析數據。 使用方差分析或回歸或任何其他工具的效果都不會那麼好,因為支撐它們的理論假設与您的数据不太匹配。 相反,序列採樣模型是明確設計來分析這種特定類型數據的,它們的理論假設與數據非常吻合。"
  },
  {
    "objectID": "Epilogue.html#學習基礎知識以及在-jamovi-中學習",
    "href": "Epilogue.html#學習基礎知識以及在-jamovi-中學習",
    "title": "後記",
    "section": "學習基礎知識以及在 jamovi 中學習",
    "text": "學習基礎知識以及在 jamovi 中學習\n好吧,這是一個很長的列表。 即使只是列出一些內容也是大大不完整的。 統計學中真的有很多我在這本書中没有涵蓋的大思想。 當您讀完一本幾乎 500 頁的教科書后被告知這只是開始,這可能看起來相當沮喪,特別是當您開始懷疑所學的一半東西都是錯的時。例如,有很多人會強烈主張不要使用經典的方差分析模型,然而我卻花了整整兩章的篇幅討論它! 標準方差分析可以從貝葉斯角度、穩健統計角度甚至“它就是錯誤”的角度來攻擊。我花了如此多時間討論機率論基礎。我更詳細地討論了估計和假設檢驗的理論,而不僅僅是我需要的程度。我為什麼要這麼做呢?回頭看,你可能會問,我是否真的需要花那麼多時間談論機率分布是什麼,甚至為什麼會有機率密度部分。如果這本書的目標是教你如何運行 t 檢驗或方差分析,這真的有必要嗎?這難道不是對每一個人時間的巨大浪費嗎???\n我希望您會同意答案是否定的。入門統計的目標不是教授方差分析。它的目標不是教授 t 檢驗、迴歸、直方圖或 p 值。目標是讓您走上成為熟練數據分析師的道路。為了使您成為熟練的數據分析師,您不僅需要掌握方差分析、t 檢驗、迴歸和直方圖等基礎知識。您需要正確地思考數據。您需要能夠學習我在上一節中談到的更高級的統計模型,並理解它們所基於的理論。您需要能夠使用讓您能使用這些高級工具的軟體。在我看來,這就是我在基礎知識上花費額外時間的回報。如果您理解機率論,從次數主義分析切換到貝氏分析對您來說會容易得多。\n簡而言之,我認為透過這種方式學習統計的最大收益在於可擴展性。對於一本只涵蓋數據分析的基礎知識來說,這本書在學習概率論等方面花費了大量精力。這本書強迫您學習的內容遠不止所涵蓋的特定分析。因此,如果您的目標是以最短的時間學習如何執行方差分析,那麼這本書並不是一個好選擇。然而正如我所說,我不認為這是您的目標。我認為您想要學習如何進行數據分析。如果這真的是您的目標,您會想要確保您在入門統計課上學到的技能能夠自然、順暢地擴展到更複雜的實際世界數據分析中所需的模型。您要確保您學習使用真正的數據分析師使用的相同工具,以便您可以學會他們在做什麼。所以,是的,您現在是一個初學者(或者當您開始這本書時是初學者),但是這並不意味著您應該得到一個簡單化的故事,一個我不告訴您概率密度是什麼的故事,或者一個不告訴您失衡設計的主成分方差分析有多麽可怕的故事。且並不意味著您應該得到玩具而不是適當的數據分析工具。初學者并不愚笨,他們只是缺乏知識。您需要的不是從真實世界的複雜數據分析中隱藏複雜性。您需要的是讓您能夠在當它們在現實世界中不可避免地突襲您時處理這些複雜性的技能和工具。\n我希望這本書,或者這本書最終會轉變成的完整版本,能夠在這方面提供幫助。\n作者註:我以前提過,但我將快速再次提及。這本書的參考文獻列表非常不完整。請不要認為這些是我所依據的唯一來源。最終版本的這本書將包含更多引用。如果您在這本書中看到任何看起來很聰明的內容似乎沒有引用,我絕對保證這些想法是其他人的。這是一本入門教材:沒有一個想法是原創的。我將為所有的錯誤負責,但我不能因為任何好的東西而得到讚賞。這本書中所有聰明的地方都來自于其他人,他們都應該得到適當的歸屬以表彰他們優秀的工作。我只是還沒有機會這樣做。"
  },
  {
    "objectID": "Epilogue.html#尚未提到的統計學課題",
    "href": "Epilogue.html#尚未提到的統計學課題",
    "title": "後記",
    "section": "尚未提到的統計學課題",
    "text": "尚未提到的統計學課題\n首先,我將談論一些我希望擠進本書版本中的內容,這樣您就可以對世界上還有什么其他統計思想有所了解。即使這本書快要最終完成,我也認為這一點很重要。學生們經常沒有意識到,他們的入門統計課只是個入門。如果您想走向廣闊的世界並進行真正的數據分析,您必須學會大量擴展本科課程內容的新工具。不要假定某些事情無法完成,只因為本科並未涵蓋。也不要假定某些事情就是正確的,只因為它在本科課上被涵蓋了。為了防止您陷入這個陷阱,我認為有必要概述一些其他的思路。\n\n各主題單元的遺珠\n即使在我在書中涵蓋的主題中,也有很多遺漏我希望能在未來的版本中補充。僅就純統計學而言(而不是與 jamovi 相關的內容),以下是一份代表性但並不詳盡的清單,我希望能在某個時候擴展這些內容:\n\n其他類型的相關性。在 單元 12 中,我談到了兩種相關性:皮爾遜和斯皮爾曼。當您有兩個連續變量並希望評估它們之間的關係時,這兩種相關性評估方法都是適用的。如果您的變量都是名義尺度的,那麼情況又如何呢? 或者一個是名義尺度,另一個是連續的呢? 實際上,在這些情況下也存在計算相關性的方法(例如 polychoric 相關),將它們包括進來會很好。\n更多關於效應量的詳細信息。 總的來說,我認為全文中對效應量的處理有點過于簡單。 在幾乎每一種情況下,我都傾向於僅選擇一種效應量測度(通常是最流行的),並描述它。 然而,對於幾乎所有的測試和模型,都存在多種思考效應量的方法,我希望在未來能更詳細地討論這一點。\n處理被違反的假設。 在本書的若干部分中,我談到了在發現您的測試(或模型)的假設被違反時可以採取的一些措施,但我認為在這方面我應該說得更多。特別是,我認為討論更詳細的變量轉換以解決問題的內容會很好。我在 單元 6 中稍微談到了這一點,但我認為討論還不夠詳細。\n回歸的交互項。在 單元 14 中,我談到過方差分析中可以有交互項,我也指出方差分析可以被解釋為某種線性回歸模型。 然而,在 單元 12 中談及回歸時,我完全沒有提及交互。 然而,沒有什么能阻止您在回歸模型中包含交互項。 當您在談論兩個連續預測變量的交互時,弄清“交互”實際上意味著什么會稍微複雜一些,而且可以有多種方法。儘管如此,我本想稍微談論一下這個話題。\n計劃比較法。 正如我在 單元 14 中所提到的,在進行方差分析時,使用像 Tukey HSD 這樣的事後校正並不總是合適的,特別是當您事前就有非常明確(和有限)的比較關注點時。我希望將來能更多地談論這一點。\n多重比較方法。 即使在討論事後檢驗和多重比較的背景下,我也希望能更詳細地討論這些方法,並談論除了我提到的幾種選擇之外還存在哪些方法。\n\n\n\n尚未提到的統計模型\n統計學是一個巨大的領域。我在這本書中描述的核心工具(卡方檢驗、t檢驗、回歸和方差分析)是廣泛使用的基本數據分析工具,它們構成了大多數入門統計書的核心。然而,還有很多其他工具。有如此多的數據分析情境這些工具無法涵蓋,給您一種感覺,這其中還有很多值得了解的,例如:\n\n非線性回歸。在 單元 12 中討論回歸時,我們看到回歸假定預測變量與結果變量之間的關係是線性的。另一方面,當我們在 單元 4 中討論了更簡單的相關性問題時,我們看到確實存在能夠評估變量之間非線性關係的工具(例如斯皮爾曼相關)。統計中有許多工具可以用於進行非線性回歸。例如,一些非線性回歸模型假定預測變量與結果變量之間的關係是單調的(例如等分回歸),而其他則假定它是平滑但不一定是單調的(例如 Lowess 回歸)另一些則假定關係的形式是已知的非線性形式(例如多項式回歸)。\n邏輯回歸。當結果變量是二元的但預測變量是連續的時,回歸的另一種變體。例如,假設您正在研究社交媒體,並希望了解是否可以根據收入、年齡等變量預測某人是否在 Twitter 上。這基本上是一種回歸模型,但您無法使用常規的線性回歸,因為結果變量是二元的(您是否在 Twitter 上)。由於結果變量是二元的,殘差不可能正常分佈。統計學家可以將這種情況應用許多工具,其中最突出的是邏輯回歸。\n廣義線性模型(GLM):GLM 實際上是包含邏輯回歸、線性回歸、(某些)非線性回歸、方差分析和許多其他模型的模型家族。 GLM 中的基本思想與支撐線性模型的思想基本相同,但是它允許您的數據可能不是正常分佈的,並允許預測變量與結果變量之間的非線性關係。有很多非常方便的分析屬於 GLM,所以了解它非常有用。\n存活分析。在 單元 2 中,我談到了“差異減員”,即人們以非隨機方式退出研究的趨勢。當時,我是將其作為一種潛在的方法論問題而談論的,但在很多情況下,差異減員實際上就是您感興趣的事情。例如,假設您有興趣了解人們在一次遊戲中玩不同類型的遊戲的時間有多長。人們會否傾向於連續玩實時戰略遊戲的時間長於第一人稱射擊遊戲?您可能會這樣設計您的研究。人們進入實驗室,他們可以玩盡可能長或短的時間。 一旦他們玩完了,您就記錄他們玩的時間。 然而,由于伦理限制,假設您不能讓他們玩超過兩小時。 很多人在兩小時限制之前就會停止遊戲,所以您會正確知道他們玩了多久。 但有些人會遇到兩小時的限制,所以如果允許研究繼續進行,您不知道他們會玩多久。 因此,您的數據會受到系統性刪減:您遺漏了所有非常長的時間。 您如何明智地分析這些數據? 這就是存活分析要解決的問題。 它是專門設計來處理這種情況的,當研究結束時,您會系統性地遺漏某些“一邊”的數據。 它在健康研究中應用非常廣泛,並且在這種情況下,通常被字面意義上用於分析存活。例如,您可能正在跟踪某種癌症的病人,其中一些人接受了治療 A,其他人接受了治療 B,但您只有資金跟踪他們 5 年。 在研究結束時,一些人還活著,其他人已經死了。 在這種情況下,存活分析可用於確定治療的有效性,並告知您他們隨時間面臨的死亡風險。\n混合模型:重複測量方差分析通常用於觀察值聚集在實驗單位中的情況。一個很好的例子是當您在多個時間點跟蹤個人時。假設您正在跟踪兩個人的快樂情緒隨時間的變化。亞倫的快樂指數最初為 10 分,然後下降到 8 分,然後下降到 6 分。貝琳達的快樂指數最初為 6 分,然後上升到 8 分,然后上升到 10 分。這兩個人的整體快樂水平相同(三個時間點的平均值均為 8 分),因此重複測量方差分析會以相同方式對待亞倫和貝琳達。但這明顯是錯誤的。亞倫的快樂正在降低,而貝琳達的快樂正在增加。 如果您想要優化分析實驗數據,其中人們可以隨時間改變,那麼您需要比重複測量方差分析更強大的工具。 人們用來解決這個問題的工具稱為“混合”模型,因為它們旨在了解個體實驗單位的信息(例如個人的快樂隨時間變化),以及總體影響(例如金錢對快樂隨時間推移的影響)。 重複測量方差分析也許是最簡單的混合模型例子,但使用混合模型可以做很多重複測量方差分析無法做到的事情。\n多維尺度。因子分析是“無監督學習”模型的一個例子。這意味著,與我提到的大多數“有監督學習”工具不同,您無法將變量分為預測變量和結果變量。回歸是有監督學習,而因子分析是無監督學習。這並不是唯一一種無監督學習模型。例如,在因子分析中,研究人員關心變量之間的相關分析。然而,在很多情況下,您實際上有興趣分析對象、項目或人之間的相似性或差異。在這種情況下,您可以使用多種工具,最知名的就是多維尺度法(MDS)。 在 MDS 中,思路是為您的項目找到一種“幾何”表示。每個項目被“繪製”為某個空間中的一點,兩點之間的距離是這兩個項目差異的一種測量。\n聚類:無監督學習模型的另一個例子是聚類(也稱為分類),在這種情況下,您希望將所有項目組織成有意義的組,使得相似的項目被分配到同一組中。大量聚類屬於無監督類型,這意味著您不知道組是什麼,您只能猜測。還有其他“有監督聚類”情況,您需要基於其他變量預測組成員資格,並且這些組成員資格實際上是可觀察到的。邏輯回歸是以這種方式工作的工具的一個很好的例子。然而,當您實際上不知道組成員資格時,您必須使用不同的工具(例如 k 均值聚類)。甚至還有一些情況下您想要執行所謂的“半監督聚類”,在這種情況下,您知道某些項目的組成員資格但並非所有。您可以想像,聚類是一個相當大的主題,也是一項相當有用的技能。\n\n當然,即使這個列表也不完整。我沒有提到時間序列分析、項目反應理論、市場籃分析、分類與回歸樹或其他大量主題。然而,我上面給出的列表基本上是我對這本書的期望清單。當然,它會使書的長度增加一倍,但這意味著範圍已經廣泛到足以涵蓋心理學應用研究者需要使用的大多數內容。\n\n\n其他統計推論方法\n本書的另一個不完整之處在於它比較嚴重地專注於如何進行推論統計的一種非常狹隘且過時的觀點。在 單元 8 中,我稍微談到了無偏估計、抽樣分布等概念。 在 單元 9 中,我更詳細地談到了空假設顯著性檢驗和 p 值的理論。這些想法可以追溯到 20 世紀初,而我在書中談到的工具高度依賴那個時代的理論思想。我覺得有義務堅持這些主題,因為科學中絕大多數數據分析也依賴這些思想。然而,統計理論的研究並不仅限於這些主題,儘管由於其實際重要性,每個人都應該了解它們,但在許多方面,這些思想並不能代表當代數據分析的最佳實踐。我特別高興的是,我已經能夠有所超越。 單元 16 現在以合理的細節呈現了貝氏觀點,但整體而言,書中仍然相當偏向次數主義正統思想。此外,還有許多其他值得一提的推論方法:\n\n自助法:在介紹每個假設檢驗時,我都有強烈傾向於簡單地做出類似“BLAH 的抽樣分布是 t 分布”之類的斷言。在某些情況下,我實際上嘗試證明這一斷言。例如,在 單元 10 中談到 \\(chi^2\\)檢驗時,我引用了正態分布與 \\(chi^2\\) 分布之間的已知關係(參見 單元 8 ),以解釋我們如何最終假設適配度統計量的抽樣分布是 \\(chi^2\\)。 然而,也有很多這些抽樣分布是,嗯,錯誤的。 \\(chi^2\\) 檢驗就是一個很好的例子。它基於對您的数据的分布的假設,而該假設被知道在小樣本量下是錯誤的! 在 20 世紀初,面對這種情況您幾乎無能為力。統計學家得出了數學結果,即“在有關數據的 BLAH 假設下,抽樣分布大致為 BLAH”,這已經是最好的了。在很多情况下,他們甚至沒有這個。有很多數據分析情況沒有人找到所需的抽樣分布的數學解。 所以,直到 20 世紀后期,相應的檢驗要么不存在要么無法工作。 然而,計算機現在已經改變了這一切。 您現在可以使用各種高端技巧和一些不那么高端的技巧來解決這個問題。 最簡單的方法是自助法,最簡單的形式非常簡單。 您要做的就是模擬實驗結果很多很多次,同時假定 (a) 空假設為真和 (b) 未知的人群分布實際上看起來非常類似於您的原始數據。 換句話說,與其假設數據(例如)是正態分布的,不如假设数据实际上和您的样本一样,然后使用计算机模拟检验统计量的抽样分布,如果该假设成立的话。尽管依赖一些有点可疑的假设(即人群分布与样本相同!),但自助法是一个在大量数据分析问题上实际应用中奇迹般有效的快速简单的方法。\n交叉驗證:在我的統計課上偶爾會出現一個問題,通常是學生試圖挑釁時提出的,那就是“我們為什么要關心推論統計?為什么不只描述樣本呢?” 這個問題的答案通常如下:“因為作為科學家,我們的真正興趣不在於我們_過去_已經觀察到的特定樣本,我們想要對未來可能觀察到的數據做出預測”。 統計推論中很多問題的產生都是因為我們總是認為未來會與過去類似但有些不同。 或者,更一般地說,新數據不會和舊數據完全相同。 在很多情況下,我們嘗試推導出數學規則,這些規則可以幫助我們得出對於新數據最有可能正確的推論,而不是選擇最能描述舊數據的語句。例如,給定模型 A 和 B 以及今天您收集的數據集 X,嘗試選擇明天您要收集的新數據集 Y 的最佳模型。有時模擬這個過程很方便,這就是交叉驗證要做的事情。 您要做的就是將數據集劃分為兩個子集 X1 和 X2。 使用子集 X1 訓練模型(例如,估計回歸系數),但 then 在另一個子集 X2 上評估模型性能。 這為您提供了模型從舊數據延伸到新數據的能力的一種測量,這通常是比只使用完整數據集 X 擬合模型所得到的模型好壞測量更好的測量。\n穩健統計:生活很複雜,沒有什么能按應有的方式運作。對於統計來說也是如此,在嘗試分析數據時,我們經常會遇到各種問題,數據的混亂程度比應該的要高。 變量本應正常分布但實際上並非正常分布,關係本應線性但實際上並非線性,您的數據集中的一些觀察結果幾乎肯定是错误的(即,并未測量應測量的內容)。 在本書的大部分統計理論中都忽略了所有这种混乱。 然而,忽略问题并不总是能解决问题。有時,忽略混亂确实可以,因为某些类型的统计工具是“穩健的”,即使数据并不满足您的理论假设,它们仍能很好地工作。 其他类型的统计工具则不是稳健的,即使偏离理论假设很小也会导致它们失效。 稳健统计是统计学的一个分支,它研究这个问题,并谈论统计量的“破坏点”。 也就是说,您的数据必须混亂到什么程度统计量才不再可信?我在一些地方提到了这一点。均值不是變量中心趨勢的稳健估计量,但中位數是稳健的。例如,假设我告訴您我五个最好朋友的年齡分別為 34、39、31、43 和 4003 歲。您認為他們的平均年齡是多少?也就是說,這里的真實人群均值是多少?如果您使用樣本均值作為人群均值的估計量,那麼您得到的答案是 830 歲。 如果您使用樣本中位數作為人群均值的估計量,那麼您得到的答案是 39 歲。 請注意,即使在第二種情況下您在“技術上”做錯了事(使用中位數估計均值!),但您實際獲得了更好的答案。這里的问题是其中一個觀察結果顯然、明顯地是錯的。 我沒有一個 4003 歲的朋友。這可能是打字錯誤,我可能是想打 43。 但是如果我敲錯了,敲成了 53 而不是 43 呢? 您能肯定這是錯字還是不是? 有時數據中的錯誤很隱蔽,所以您無法通過目測樣本來檢測它們,但它們仍然會污染您的数据,並且仍然會影響您的結論。 稳健统计关注的是,即使面临您不知道的污染,您如何能够进行安全推論。 這是相當酷的東西。\n\n\n\n難以歸類的重要雜項\n\n假設您正在進行一項調查,並對運動和體重感興趣。 您向四個人發送了數據。 亞當說他經常運動,並且沒有超重。 布莉歐妮說她經常運動,並且沒有超重。 卡羅爾說她不運動,並超重。 蒂姆說他不運動,並拒絕回答他的體重問題。 艾琳沒有返回調查。 您現在有一個遺失數據的問題。有一整個調查遺失了,另一個遺失了一個問題,您對此該怎么辦? 忽略缺失數據通常不是一種安全的事情。 讓我們考慮蒂姆的調查。 首先,請注意,根據他的其他回答,他似乎與卡羅爾(我們都不運動)更相似,而不是與亞當或布莉歐妮更相似。 所以,如果您被迫猜測他的體重,您會猜測他比他們更接近她。 也許您會做一些校正,考慮到亞當和蒂姆是男性,而布莉歐妮和卡羅爾是女性。 這種類型猜測的統計名稱是“插補”。 安全地進行插補是很困難的,但它很重要,特別是當遺失的数据以有系統的方式遺失時。 由于社會壓力迫使超重的人常感到自己的體重很差(通常是由于公共衛生運動的緣故),所以我們實際上有理由懷疑不回應的人比回應調查的人更有可能超重。 給蒂姆插補體重意味着如果我們忽略蒂姆,樣本中超重人數將從 3 分之 1 增加到 4 分之 2(如果我們給蒂姆插補體重)。 顯然,這很重要。但是明智地這樣做比它聽起來更複雜。 早些時候,我建議您应将蒂姆视为卡羅爾,因为他們對運動問題給出了相同的答案。 但這并不完全正确。 他們之间存在有系統的差异。 她回答了問題,蒂姆沒有。 鑑于超重人群面临的社会壓力,蒂姆不是_比_卡羅爾更超重嗎? 當然,這仍然忽略了一個事實,即將_单个_體重插補給蒂姆是不明智的,就好像您實際上知道他的體重一樣。 相反,您需要做的是插補一系列合理的猜測(稱為多重插補),以捕捉您對蒂姆體重的不確定性大於對卡羅爾體重的不確定性这一事实。 我們還沒有開始討論艾琳沒有發回調查所帶來的問題。 您可以想像,處理遺失數據日益成為一個重要話題。 事實上,有人告訴我,如果未遵循某種明智的多重插補計劃,某些領域的很多期刊將不會接受具有遺失數據的研究。\n考驗力分析:在@sec-Hypothesis-testing中,我討論了考驗力的概念(即,如果效應實際存在,您有多大可能性能檢測到它)並提到了功效分析,這是一組用於評估您的研究功效的有用工具。功效分析在規劃研究(例如,確定您可能需要的樣本量有多大)很有用,但在分析您已經收集的數據時也起著有用的作用。例如,假設您得到了顯著結果,並且有效應量的估計。您可以使用這些信息來估計您的研究實際具有多大的功效。這還是有點有用的,特別是如果您的效應量不大。例如,假設您在 \\(p&lt; .05\\) 水平上拒絕了空假設,但您使用功效分析得出您的估計功效只有 .08。顯著結果意味著,如果空假設實際上為真,得到這樣的数据的機率為 5%。 但是低功效意味著,即使空假設為假,效應量實際上和它看起來一樣小,得到您得到的數據的機率也只有 8%。這表明您需要相當謹慎,因為運氣似乎在您的結果中起了很大作用,不管是哪一種方式!\n使用理論驅動模型進行數據分析。在本書的一些地方,我提到了反應時間(RT)數據,您記錄某人完成某件事所需的時間(例如,做出簡單決定)。我提到 RT 數據幾乎總是非正態的,並且正偏。此外,還有所謂的速度/精度權衡:如果您嘗試過快做出決定(RT 較低),那麼您可能會做出較差的決定(精度較低)。因此,如果您測量了參與者決策的精度和他們的 RT,您可能會發現速度和精度之間存在關係。當然還有更多內容,因為與反應速度無關,一些人的決策優於其他人。此外,速度取決于認知過程(即思考花費的時間)和生理過程(例如,您能夠移動肌肉的速度有多快)。 分析這些數據的過程聽起來會很複雜。的確,當您深入研究心理學文獻時,您會發現已經存在數學模型(稱為“序列採樣模型”),這些模型描述了人們如何做出簡單的決定,並且這些模型考慮到了我上面提到的許多因素。 您在標準統計教科書中找不到任何這些理論驅動的模型。 標准統計教科書描述了標准工具,這些工具可以有意義地應用于許多不同的學科,不僅僅是心理學。 方差分析就是一個標准工具的例子,它適用于心理學和藥理學。 順序採樣模型不是,它們或多或少是專門針對心理學的。 這並没有使它們成為更加強大的工具。 事實上,如果您正在分析人們必須快速做出選擇的數據,您應該真正使用序列採樣模型來分析數據。 使用方差分析或回歸或任何其他工具的效果都不會那麼好,因為支撐它們的理論假設与您的数据不太匹配。 相反,序列採樣模型是明確設計來分析這種特定類型數據的,它們的理論假設與數據非常吻合。"
  },
  {
    "objectID": "Epilogue.html#運用-jamovi-中學習基礎知識",
    "href": "Epilogue.html#運用-jamovi-中學習基礎知識",
    "title": "後記",
    "section": "運用 jamovi 中學習基礎知識",
    "text": "運用 jamovi 中學習基礎知識\n好吧,這是一個很長的列表。 即使只是列出一些內容也是大大不完整的。 統計學中真的有很多我在這本書中没有涵蓋的大思想。 當您讀完一本幾乎 500 頁的教科書后被告知這只是開始,這可能看起來相當沮喪,特別是當您開始懷疑所學的一半東西都是錯的時。例如,有很多人會強烈主張不要使用經典的方差分析模型,然而我卻花了整整兩章的篇幅討論它! 標準方差分析可以從貝葉斯角度、穩健統計角度甚至“它就是錯誤”的角度來攻擊。我花了如此多時間討論機率論基礎。我更詳細地討論了估計和假設檢驗的理論,而不僅僅是我需要的程度。我為什麼要這麼做呢?回頭看,你可能會問,我是否真的需要花那麼多時間談論機率分布是什麼,甚至為什麼會有機率密度部分。如果這本書的目標是教你如何運行 t 檢驗或方差分析,這真的有必要嗎?這難道不是對每一個人時間的巨大浪費嗎???\n我希望您會同意答案是否定的。入門統計的目標不是教授方差分析。它的目標不是教授 t 檢驗、迴歸、直方圖或 p 值。目標是讓您走上成為熟練數據分析師的道路。為了使您成為熟練的數據分析師,您不僅需要掌握方差分析、t 檢驗、迴歸和直方圖等基礎知識。您需要正確地思考數據。您需要能夠學習我在上一節中談到的更高級的統計模型,並理解它們所基於的理論。您需要能夠使用讓您能使用這些高級工具的軟體。在我看來,這就是我在基礎知識上花費額外時間的回報。如果您理解機率論,從次數主義分析切換到貝氏分析對您來說會容易得多。\n簡而言之,我認為透過這種方式學習統計的最大收益在於可擴展性。對於一本只涵蓋數據分析的基礎知識來說,這本書在學習概率論等方面花費了大量精力。這本書強迫您學習的內容遠不止所涵蓋的特定分析。因此,如果您的目標是以最短的時間學習如何執行方差分析,那麼這本書並不是一個好選擇。然而正如我所說,我不認為這是您的目標。我認為您想要學習如何進行數據分析。如果這真的是您的目標,您會想要確保您在入門統計課上學到的技能能夠自然、順暢地擴展到更複雜的實際世界數據分析中所需的模型。您要確保您學習使用真正的數據分析師使用的相同工具,以便您可以學會他們在做什麼。所以,是的,您現在是一個初學者(或者當您開始這本書時是初學者),但是這並不意味著您應該得到一個簡單化的故事,一個我不告訴您概率密度是什麼的故事,或者一個不告訴您失衡設計的主成分方差分析有多麽可怕的故事。且並不意味著您應該得到玩具而不是適當的數據分析工具。初學者并不愚笨,他們只是缺乏知識。您需要的不是從真實世界的複雜數據分析中隱藏複雜性。您需要的是讓您能夠在當它們在現實世界中不可避免地突襲您時處理這些複雜性的技能和工具。\n我希望這本書,或者這本書最終會轉變成的完整版本,能夠在這方面提供幫助。\n作者註:我以前提過,但我將快速再次提及。這本書的參考文獻列表非常不完整。請不要認為這些是我所依據的唯一來源。最終版本的這本書將包含更多引用。如果您在這本書中看到任何看起來很聰明的內容似乎沒有引用,我絕對保證這些想法是其他人的。這是一本入門教材:沒有一個想法是原創的。我將為所有的錯誤負責,但我不能因為任何好的東西而得到讚賞。這本書中所有聰明的地方都來自于其他人,他們都應該得到適當的歸屬以表彰他們優秀的工作。我只是還沒有機會這樣做。"
  },
  {
    "objectID": "Prelude-Part-IV.html#footnotes",
    "href": "Prelude-Part-IV.html#footnotes",
    "title": "中場故事",
    "section": "",
    "text": "譯註~譯者提供的簡化版故事。某位設計師為展覽製作一個能裝10,000顆球的巨大陶瓷瓶子，展覽告示宣稱瓶子裡有5,000顆白球和5,000顆彩色球，歡迎現場觀眾任意取球測試，看看告示是否為真。有兩位觀眾W君與C君彼此約定測試方法(取出後是否放回，由讀者自行設定)，每次取出三顆球，逐次評估告示的真實性。這天他們連續測試四次，每次取出的都是三顆白球。W君認為告示是錯的，C君認為還要繼續測試。你認為誰的主張是合理的？↩︎\n譯註~同學們也許在一些地方看過”hypothesis”與”assumption”都被翻成「假設」，但是在英文這兩個詞是有區別的。“hypothesis”是指根據某種科學理論，設定實驗結果可能是什麼樣子。“assumption”是指研究運用的收集與分析資料的技術，必須符合什麼條件才會有效。因此”hypothesis”翻成「假設」是合理的，“assumption”應該翻成「預設條件」。本書的用詞將依此原則翻譯。↩︎"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-chi2-goodenss-of-fit",
    "href": "10-Categorical-data-analysis.html#sec-chi2-goodenss-of-fit",
    "title": "10  類別資料分析",
    "section": "10.1 卡方適合度檢定",
    "text": "10.1 卡方適合度檢定\n卡方適合度檢定(goodness-of-fit test)是統計學者們最早開發出來的假設檢定方法之一。主要發明者是20世紀初的統計學者卡爾．皮爾森( Karl Pearson , 皮爾森相關係數以他的名字命名) (Pearson, 1900)，之後羅納德·費雪爵士(Sir Ronald Fisher) (Fisher, 1922) 做了一些改良，這個方法是檢定名義變項的觀察次數分佈是否符合預期次數分佈。像是有一組病人接受了實驗性治療，根據治療後的健康狀況分類，醫師紀錄病情是否有改善、保持不變或惡化，接著使用適合度檢定可以確定每個類別的人次，是否匹配標準治療後的預期人次。以下用一些心理學研究案例學習如何使用卡方適合度檢定。\n\n\n10.1.1 撲克牌花色隨機選擇資料\n過去幾十年許多人類模擬隨機現象的研究顯示，這是人類很難學成的能力。雖然每個人或多或少會試表“隨機行動”，依照模式和結構進行思考仍然是一般人難以擺脫的習慣，就算是被要求“隨機做某件事情”，實際上完全無法隨機行事，所以這類研究反過來揭露了許多人類的非隨機行為，其中透露我們如何看待世界的深刻心理問題。這一節的範例啟發自隨機行為研究，以此虛構一個非常簡單的研究。假如研究人員要求參與者想像一副已經洗好的撲克牌，然後從這副牌裡“隨機”選出一張牌。參與者選好想像中的牌之後，再選出第二張牌。參與者選擇好之後，再請他們說出選擇的花色（紅心、梅花、黑桃或方塊）。假如這個研究最後收集了 \\(N = 200\\) 個人選出的牌，研究人員想分析資料，確認一般人假裝選擇的撲克牌是否真的隨機。這份資料存在lsj檔案庫的 Randomness ，當讀者從 jamovi 的lsj檔案庫開啟這份資料的試算表，會看到三個變項：為每個參與者分配辨識碼id ，以及紀錄每個人先後選出的兩種撲克牌花色 choice_1 和 choice_2。\n現在先看參與者第一次選擇的花色。開啟“Explore” - “Descriptives”的設定視窗，點選Frequency table選項計算每個花色被參與者們選擇的次數。得到的結果就如同 表 10.1 ：\n\n\n\n\n\n\n表 10.1: 的第一次選擇撲克牌花色的次數紀錄\n\n\nclubs\ndiamonds\nhearts\nspades\n\n\n35\n51\n64\n50\n\n\n\n\n\n\n\n\n這份小小的次數表非常有用。其中數字似乎暗示，參與者們可能偏好選擇紅心而且比較不想選擇梅花。只看表面數字並不能判斷這樣的差距是不是巧合，因此需要進行統計分析來找出答案，這就是下一節要學習的功課。\n沒問題的話，接下來要分析 表 10.1 的資料囉。然後，這裡開始不得不用些數學符號討論這些資料，所以最好先認識一下每個符號的意義。首先是至目前為止一直提到“觀察值”，將用大寫字母\\(O\\)，而字母的下標表示觀察值在表格裡的位置。像是 表 10.1 的第二個觀察值可寫為\\(O_2\\)。表 10.2 說明每個花色的報告次數與代表符號之間的對應 。\n\n\n\n\n\n\n表 10.2: 英語描述與數學符號之間的關係\n\n\nlabel\nindex, i\nmath. symbol\nthe value\n\n\nclubs, \\( \\clubsuit \\)\n1\n\\( O_1 \\)\n35\n\n\ndiamonds, \\( \\diamondsuit \\)\n2\n\\( O_2 \\)\n51\n\n\nhearts, \\( \\heartsuit \\)\n3\n\\( O_3 \\)\n64\n\n\nspades, \\( \\spadesuit \\)\n4\n\\( O_4 \\)\n50\n\n\n\n\n\n\n\n\n希望這樣整理能讓讀者搞清楚。同時提醒一下，數學家更喜歡用符號討論而不是直接談論具體事項，因此接著會一直看到\\(O_i\\)之類的符號，這是指在第 i 類別的觀察次數（其中 i 可能是 1，2，3 或 4）。最後，如果我們要在報告裡提及所有觀察次數，統計學家習慣將所有觀察值構成一個向量 2，本書以 \\(O\\) 之類的大寫字母表示。\n\\[O = (O_1, O_2, O_3, O_4)\\]\n同樣的，這裡沒有什麼新奇有趣之處，一切只是符號。如果說 \\(O = (35, 51, 64, 50)\\)，就只是將描述觀察值的次數表，改用數學符號來表示而已。\n\n\n\n\n10.1.2 虛無假設與對立假設\n正如一開始的範例說明，研究者的假設是“一般人不會隨機選出想像中的撲克牌”。接著要做的是將概念中的假設，轉換為相互對立的統計假設，然後決定測試這些統計假設的統計方法。這個範例要使用的統計方法就是需要使用這一節要學習皮爾森 \\(\\chi^2\\) 適合度檢定，規劃適合度檢定的第一步是設定虛無假設，撲克牌範例的虛無假設是很簡單地。我們先用文字說明虛無假設：\n\\[H_0: \\text{ 四種花色被選擇的機率相等}\\]\n修習統計學課程的學生要學會的功課之一，就是用數學符號表達虛無假設，這裡使用 \\(P_j\\) 表示第j種花色被參與者 選擇的真實機率。如果研究結果符合虛無假設，那麼任何一種花色都有 25% 的機率被選中。換句話說，以上的虛無假設說明用數學符號表達的方式是\\(P_1 = .25\\)，\\(P_2 = .25\\)，\\(P_3 = .25\\)， \\(P_4 = .25\\)。因為研究人員習慣用向量符號涵括同一個變項的資料，在此用 \\(P\\) 表示虛無假設涵括的所有機率事件。也就是向量 \\(P = (P_1, P_2, P_3, P_4)\\) 表示虛無假設的機率事件集合，這麼一來可將虛無假設寫成：\n\\[H_0: P =(.25, .25, .25, .25)\\]\n對應虛無假設的向量 \\(P\\) ，涵括的所有事件發生機率剛好相等，不過真實的研究條件不一定會是這樣。如同這個範例的實驗任務是讓參與者從想像中的撲克牌組抽一張牌，若是這副牌的梅花數量是其他花色的兩倍，那麼虛無假設就要寫成\\(P = (.4, .2, .2, .2)\\)。只要機率值都是正數，且總和為 1，就能構成合法的虛無假設。因為許多使用適合度檢定的場景，是用來處理所有類別事件發生機率相等的虛無假設，以下討論繼續使用四種花色被挑選機率相等的虛無假設。\n那麼對立假設 \\(H_1\\)是什麼呢？這道統計假設表示研究者感興趣的研究結果，也就是認為參與者的選擇不是完全隨機，因此四種花色的發生機率並不相等。所以兩則統計假設的白話版本是：\n\\(H_0: \\text{ 四種花色被選擇的機率相等}\\) \\(H_1: \\text{ 至少有一個選擇花色的機率不是 0.25}\\)\n…或者可寫成“數學家偏好的”版本：\n\n\\(H_0: P= (.25, .25, .25, .25)\\) \\(H_1: P \\neq (.25, .25, .25, .25)\\)\n\n\n10.1.3 適合度檢定統計程序\n到了這一步，我們手上有一組觀察次數 \\(O\\) 以及一組對應虛無假設的機率 \\(P\\)，接著就是規劃虛無假設的檢定程序。如同 單元 9 的說明，要檢測 \\(H_0\\) 和 \\(H_1\\)，就需要計算檢測統計值，構成適合度檢定的統計值是衡量資料與虛無假設之間的“接近程度”。如果資料不相符虛無假設的期望機率，那麼虛無假設呈現的情況可能不是真的。那麼，如果虛無假設是真的，檢測結果會是什麼樣子呢？或者專有名詞來問，期望次數是什麼？總共有 \\(N = 200\\) 次觀察，若是虛無假設為真，任何一位參與者選擇紅心的機率是 \\(P_3 = .25\\)，所以紅心的期望次數是 \\(200 \\times .25 = 50\\) ，對吧？更具體的方式是用\\(E_i\\)代表“虛無假設為真時，研究人員期望觀察到第 i 類反應的次數”，也就是這個數學公式：\n\\[E_i=N \\times P_i\\]\n這個式子不難用手計算。就像現在的200 個觀察值是分為四個類別，研究人員認為參與者會選擇任何一個類別的可能性相等，那麼每個類別應該都有50人次，對吧？\n接著要如何將觀察次數和期望次數轉換為檢測統計值呢？只要比較每個類別的期望觀察次數（\\(E_i\\)）與各自的實際觀察次數（\\(O_i\\)），就能得出一個有用的檢測統計值。首先要計算虛無假設的期望次數與實際總計出的次數之間差異也就是計算“觀察次數減去期望次數”的差異值，\\(O_i - E_i\\)。詳細計算請見 表 10.3 。\n\n\n\n\n\n表 10.3: 各種花色的期望次數(expected frequencu)、實際次數(observed frequency)和差異分數(difference score)\n\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\nexpected frequency \\( E_i\\)\n50\n50\n50\n50\n\n\nobserved frequency \\( O_i\\)\n35\n51\n64\n50\n\n\ndifference score \\( O_i-E_i\\)\n-15\n1\n14\n0\n\n\n\n\n\n\n\n\n根據計算結果，選擇紅心的人數顯然比虛無假設預測的多，選擇梅花的人較少。不過稍微想一下就會發現，最後一列的差異值有負的數值！這裡的奇怪之處在於虛無假設所做的預測少於觀察次數(像示範資料的紅心)，和預測多於觀察次數(像示範資料的梅花)一樣糟糕。最簡單的解決方法是將所有數字平方，如此就可以計算平方差，\\((E_i - O_i)^2\\)。這樣就能算出如 表 10.4 的數值。\n\n\n\n\n\n表 10.4: 四種花色差異次數平方\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\n225\n1\n196\n0\n\n\n\n\n\n\n\n\n現在的算出來的這組數字有明顯的特色，與虛無假設預測差異太大的類別（梅花和紅心），數字都很大；預測接近的類別（方塊和黑桃），數字都很小。為了解釋稍後的步驟，我們要將這些平方差除以期望次數 \\(E_i\\) ，也就是\\(\\frac{(E_i-O_i)^2}{E_i}\\)。因為這個例子裡所有類別的 \\(E_i = 50\\)，用手一個一個算還挺無聊的，不過算完後會得到 表 10.5 的結果。\n\n\n\n\n\n表 10.5: 四種花色’誤差’分數~差異平方除以期望次數\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\n4.50\n0.02\n3.92\n0.00\n\n\n\n\n\n\n\n\n這裡我們算出了四種’誤差’分數，每個分數代表用虛無假設預測實際次數，所造成的“錯誤”程度。為了轉換為檢測統計值，接下來要做的事就是將這些數字加起來。最後加起來的值就是適合度，通常在報告裡寫成\\(\\chi^2\\)（卡方）或 適合值(GOF)。所有步驟總結如以下公式：3\n\\[\\sum( (觀察次數 - 期望次數)^2 / 期望次數 )\\]\n最後得到適合度的卡方統計值 8.44。\n[額外的技術細節 4]\n根據以上的計算，這項研究資料分析結果得到了 \\(\\chi^2\\) = 8.44。那麼現在的問題就是，這個值是否大到足夠拒絕虛無假設？\n\n\n\n10.1.4 適合度檢定的樣本分佈\n要確定某個 \\(\\chi^2\\) 統計值是否大到能充分拒絕虛無假設，接著要確定如果虛無假設為真，\\(\\chi^2\\) 的樣本分佈會是什麼，在這一節中要學習的，就是如何規劃符合適合度檢定的樣本分佈，然後下一節學習如何使用這樣的樣本分佈構建假設檢程序。如果讀者願意相信樣本分佈是具有 \\(k - 1\\) 自由度的 \\(\\chi^2\\)（卡方）分佈，其實可以跳過本節。然而，如果讀者需要了解為什麼適合度檢定程序要如此安排，請繼續閱讀下去。\n先假設虛無假設的說法是真的，那麼觀察值落入第 i 類的真實機率就是 \\(P_i\\)，這麼一來就符合對應虛無假設的向量，那這樣代表什麼意思呢？這像是擲出一枚特製的硬幣，觀察擲出那一面以決定落入類別i，而擲出那一面的的機率是 \\(P_j\\)。所以觀察到某一類別的實際次數 \\(O_i\\)，就像是擲出這枚特製硬幣N次，每次紀錄一個觀察值，最後有 \\(O_i\\) 次紀錄到硬幣的那一面。雖然這樣的實驗很奇怪，這樣的說明只是希望讀者了解，我們已經在前面的單元見識類似的場景，像是 單元 8 的 小單元 8.4 所示範的實驗設定。換句話說，如果虛無假設是真的，那麼多次實驗的觀察次數會符合二項分佈所構成的機率分佈：\n\\[O_i \\sim Binomial(P_i,N) \\]\n若是讀者還記得 小單元 8.3.3 提到的中央極限定理，當 \\(N\\) 較大且 \\(P_i\\) 剛好在0與1中間時，二項分佈看起來幾乎與常態分佈一模一樣。換句話說，只要 \\(N^P_i\\) 足夠大，或者說，當期望次數 \\(E_i\\) 足夠大時，理論上 \\(O_i\\) 的機率分佈接近常態分佈。更棒的是，如果 \\(O_i\\) 是常態分佈的，那麼 \\((O_i-E_i)/\\sqrt{E_i}\\) 也會接近常態分佈。因為任何一個 \\(E_i\\) 數值是固定的，所以實際觀察次數減去期望次數並除以期望值的開根號，相當於改變了常態分佈的平均值和標準差。為何這樣的統計值公式能計算適合度呢？因為這個公式將所有觀察值平方後加總，形成的樣本統計值可用近似的常態分佈計算可能發生的機率。等等，這就像 小單元 8.6 提到的，當我們手上有很多符合標準常態分佈的資料（平均值為 0 且標準差為 1），將所有數值平方然後加起來時，所得到的樣本數值符合卡方分佈。所以現在我們知道虛無假設所預測適合度統計值的樣本分佈符合卡方分佈。太棒了。\n最後還要提一個細節，就是自由度。 小單元 8.6 曾提到把k個數值加總起來，所構成的卡方分佈的自由度就是 k。但是卡方適合度檢定的實際自由度是 \\(k - 1\\)，倒底是怎麼一回事呢？因為真正能加總的數值必須是獨立的。下一節我們將討論，為何全部有k個觀察值，卻只有 \\(k - 1\\) 個是真正獨立的，所以自由度實際上只有 \\(k - 1\\)。這就是下一節的主題5。\n\n\n\n10.1.5 自由度\n\n\n\n\n\n圖 10.1: 不同“自由度”的卡方分佈\n\n\n\n\n在 小單元 8.6 我初次向讀者介紹卡方分佈，只有稍微講一下什麼是自由度，強調這是很重要的概念。請看一下 圖 10.1 不同的卡方分佈，可知只要自由度改變了，那麼形狀劇烈改變。那麼自由度究竟是什麼呢？其實在解釋自由度與常態態分佈的關係時，就提到了一種解釋：自由度是可平方並可加成的常態分佈變項數量。當然，這樣的解釋相當抽象，對多數讀者沒有幫助。接著讓我們透過真實的資料，來嘗試理解自由度吧。\n自由度的基本概念其實相當簡單，計算方法是將先算出資料的“總數”，然後減去“不可變動”的資料數目。6這樣說有點籠統，所以這裡用想像實驗的撲克牌資料再解釋一下。首先有\\(O1, O2, O3, O4\\)四個分別代表選出一種花色（紅心，梅花，方塊，黑桃）觀察次數的數字。這四個數字是經過實驗得到的隨機結果，其實還有一個不可變動的樣本量 \\(N\\)。7也就是說，如果我們知道有多少人選擇紅心，多少人選擇方塊，多少人選擇梅花，那麼就能確實知道還有多少人選擇黑桃。換句話說，雖然總體資料量是四個，實際上只有 \\(4 - 1 = 3\\) 個自由度。另一種角度的思理解方式是，我們想要估算四種類別的機率，但是全部類別的機率加起來必須等於一，那麼其中一個類別的機率不可變動。因此自由度是 \\(4 - 1 = 3\\)。無論您想用觀察次數的角度，還是機率的角度理解自由度，答案都是一樣的。通常要執行 \\(k\\) 組的卡方適合度檢定時，自由度都是 \\(k - 1\\)。\n\n\n\n10.1.6 檢定虛無假設\n一個完整的假設檢定程序，最後要規劃的是棄卻域，也就是決定那些 \\(\\chi^2\\) 值能拒絕虛無假設。我們已經知道越大的\\(\\chi^2\\)數值代表虛無假設的預測與真正的實驗結果差異越大，\\(\\chi^2\\)數值越小則代表虛無假設的預測越接近真正的實驗結果。所以明智的策略要設定一個臨界值，如果 \\(\\chi^2\\) 數值大於臨界值，就拒絕虛無假設；如果 \\(\\chi^2\\) 小於臨界值，就保留虛無假設。用 單元 9 介紹過的詞彙，卡方適合度檢定是一種單側檢定。好了，現在要做的就是訂下臨界值是多少。方法很簡單，如果研究人員願意容忍犯下型一錯誤的機率最多\\(5%\\)，可以顯著水準\\(\\alpha = .05\\)決定臨界值，如此一來，在虛無假設預測為真的前提下，能得到與臨界值一樣大的 \\(\\chi^2\\) 數值之機率應該只有\\(5%\\)。如同 圖 10.2 的圖解。\n\n\n\n\n\n\n圖 10.2: \\(\\chi^2\\)（卡方）適合度檢定的假設檢定如何運作的示意圖\n\n\n\n\n上到這裡，有些同學會問，要如何找到自由度是 \\(k-1\\) 的卡方分佈臨界值？如果是多年前的用實體教科書上統計課，都可以在書的附錄裡看到像 圖 10.3 的表格，用來查找臨界值。在這個表中能找到自由度是3的卡方分佈臨界值，\\(\\alpha = 0.05\\)對應的數值是 7.815。\n\n\n\n\n\n\n\n圖 10.3: 查找卡方分佈臨界值的表格\n\n\n\n\n所以，若是真正算出的 \\(\\chi^2\\) 統計值大於 7.815，就可以拒絕虛無假設（預測每一種花色被選擇的機率相等）。即然已經算出統計值是8.44，那麼可以拒絕虛無假設。至此我們已經走完“皮爾森卡方適合度檢定”的過程，真不錯。\n\n\n\n10.1.7 jamovi實作\n毫不意外地，jamovi 提供了一個分析工具，可以幫你完成這些計算。讓我們使用 Randomness.omv 文件。在主要的“分析”工具欄中，選擇“頻率” - “單樣本比例檢驗” - “\\(N\\) 個結果”。然後在出現的分析視窗中將要分析的變項（從選擇 1 開始）移到“變項”框中。此外，單擊“預期計數”復選框，以便將這些資料顯示在結果表中。完成所有這些操作後，你應該會在 jamovi 中看到分析結果，如 圖 10.4。然後不出所料，jamovi 提供了與我們上面手動計算相同的預期計數和統計資料，\\(\\chi^2\\) 值為 \\((8.44\\)，自由度為 \\(3\\)，\\(p=0.038\\)。注意，我們不再需要查找臨界 p 值閾值，因為 jamovi 給出了在 \\(3\\) 自由度下計算得出的 \\(\\chi^2\\) 的實際 p 值。\n\n\n\n\n\n\n\n圖 10.4: jamovi 中的 \\(\\chi^2\\) 單樣本比例檢驗，表格顯示觀察到的頻率和比例以及期望的頻率和比例\n\n\n\n\n\n\n10.1.8 另一種虛無假設\n此時，你可能會想知道如果你想進行適合度檢驗，但你的虛無假設不是所有類別的機率都相等該怎麼辦。例如，假設有人提出了這樣的理論預測，即人們應該以 \\(60\\%\\) 的機率選擇紅色牌，以 \\(40\\%\\) 的機率選擇黑色牌（我不知道為什麼你會這樣預測），但沒有其他偏好。如果是這樣，虛無假設將期望選擇愛心的比例為 \\(30\\%\\)，選擇方塊的比例為 \\(30\\%\\)，選擇黑桃的比例為 \\(20\\%\\)，選擇梅花的比例為 \\(20\\%\\)。換句話說，我們期望愛心和方塊的出現次數是黑桃和梅花的 1.5 倍（\\(30\\%\\) : \\(20\\%\\) 的比例與 1.5 : 1 相同）。對我來說，這似乎是一個愚蠢的理論，但是用我們的 jamovi 分析可以很容易地測試這個明確指定的虛無假設。在分析視窗中（標記為“比例檢驗（N個結果）”的 圖 10.4 中，你可以展開“預期比例”的選項。當你這樣做時，將會出現一些選項，讓你為選定的變項輸入不同的比例值，在我們的案例中，這個變項是 choice 1。將比例更改為反映新的虛無假設，如 圖 10.5 所示，並觀察結果如何變化。\n\n\n\n\n\n\n圖 10.5: 在 jamovi 中更改 \\(\\\\chi^2\\) 單樣本比例檢驗的預期比例\n\n\n\n\n預期計數現在顯示在 表 10.6 中。\n\n\n\n\n\n表 10.6: 不同虛無假設的預期計數\n\n\n\n\\( \\clubsuit \\)\n\\( \\diamondsuit \\)\n\\( \\heartsuit \\)\n\\( \\spadesuit \\)\n\n\nexpected frequency \\( E_i\\)\n40\n60\n60\n40\n\n\n\n\n\n\n\n\n\\(\\chi^2\\) 統計量為 4.74，自由度為 3，\\(p = 0.182\\)。現在，我們更新的假設和預期頻率與上次的結果有所不同。因此，我們的 \\(\\chi^2\\) 檢驗統計量和 p 值也有所不同。令人惱火的是，p 值為 \\(.182\\)，因此我們不能拒絕虛無假設（回顧 小單元 9.5 提醒自己為什麼）。可悲的是，儘管虛無假設對應著一個非常愚蠢的理論，這些資料並沒有提供足夠的證據來反駁它。\n\n\n\n10.1.9 適合度檢定的報告寫作\n現在你知道了這個測試的運作方式，也知道如何使用神奇的jamovi計算盒來進行測試。接下來你需要知道的是如何撰寫結果。畢竟，設計和執行實驗，然後分析資料，如果不告訴別人結果是沒有意義的！所以讓我們來談談在報告分析時需要做的事情。讓我們繼續以撲克牌花色為例。如果我想將這個結果寫成一篇論文之類的東西，那麼慣常的報告方式是這樣寫的：\n\n在實驗的200名參與者中，有64人首選紅心，51人選擇方塊，50人選擇黑桃，35人選擇梅花。進行了卡方適合度檢驗以測試四種花色的選擇機率是否相同。結果顯著（\\(\\chi^2(3) = 8.44, p&lt; .05\\)），這表明人們在選擇花色時並非完全隨機。\n\n這相當直接，希望它看起來很不起眼。儘管如此，你應該注意到這個描述中的幾點內容：\n\n描述統計資料在統計檢驗之前。也就是說，在進行檢驗之前，我告訴讀者有關資料的一些信息。通常，這是一個很好的做法。永遠記住，你的讀者對你的資料了解得遠不如你。因此，除非你妥善地向他們描述，否則統計檢驗對他們來說毫無意義，他們會感到沮喪和哭泣。\n描述告訴你正在測試的虛無假設是什麼。老實說，作者並不總是這樣做，但在存在一定歧義的情況下，或者在你不能依賴你的讀者非常熟悉你正在使用的統計工具時，這通常是一個好主意。很多時候讀者可能不知道（或記不起）你正在使用的檢驗的所有細節，所以提醒他們是一種禮貌！對於適合度檢驗來說，你通常可以依賴科學觀眾了解它的運作方式（因為它涵蓋在大多數入門統計課程中）。然而，明確陳述虛無假設（簡要地！）仍然是一個好主意，因為虛無假設可能因你使用檢驗的目的而有所不同。例如，在撲克牌的例子中，我的虛無假設是四個花色的機率相同（即，\\(P1 = P2 = P3 = P4 = 0.25\\)），但這個假設並沒有什麼特別的。我可以同樣使用適合度檢驗測試虛無假設，即\\(P_1 = 0.7\\)和\\(P2 = P3 = P4 = 0.1\\)。所以，向讀者解釋你的虛無假設是有幫助的。另外，注意到我用文字而不是數學描述虛無假設。這是完全可以接受的。你可以用數學描述它，但是因為大多數讀者發現文字比符號更容易閱讀，所以大多數作者傾向於用文字描述虛無假設（如果可以的話）。\n包括”統計塊”。在報告檢驗結果本身時，我不僅僅說結果顯著，還包括了一個“統計塊”（即括號內密集的數學部分），其中報告了所有“關鍵”統計信息。對於卡方適應度檢驗，報告的信息包括檢驗統計量（即適應度統計量為8.44）、用於檢驗的分布信息（具有3個自由度的\\(\\chi^2\\)，通常縮寫為\\(\\chi^2\\)(3)），然後是結果是否顯著（在本例中為\\(p&lt; .05\\)）。每個檢驗所需的統計塊中的特定信息各不相同，因此每次我介紹一個新檢驗時，我都會向您展示統計塊應該是什麼樣子。8 但是，一般原則是您應該始終提供足夠的信息，以便讀者在需要時可以自己檢查測試結果。\n對結果進行解釋。除了指出結果顯著之外，我還提供了結果的解釋（即，人們沒有隨機選擇）。這對讀者也是一種友善，因為它告訴他們關於資料中發生了什麼事的一些信息。如果不包括這樣的東西，讀者很難理解發生了什麼事。9\n\n正如其他所有事物一樣，你應該首要關注的是向讀者解釋事物。永遠記住，報告結果的目的是與另一個人溝通。我無法告訴您我看過多少次報告、論文甚至科學文章的結果部分就是胡言亂語，因為作者只關注確保包含所有數字，卻忘記了與人類讀者真正交流。\n\n撒旦在統計和引用經文中同樣感到高興10 – H.G. 威爾斯"
  },
  {
    "objectID": "10-Categorical-data-analysis.html#sec-chi2-independence",
    "href": "10-Categorical-data-analysis.html#sec-chi2-independence",
    "title": "10  類別資料分析",
    "section": "10.2 卡方獨立性檢定",
    "text": "10.2 卡方獨立性檢定\n\n守衛機器人 1：停！ 守衛機器人 2：你們是機器人還是人類？ 莉娜：我們是…機器人。 弗萊：呃，對！就是在機器人世界裡像機器人一樣生活的兩個機器人！呃？\n守衛機器人 1：測試！ 守衛機器人 2：你最喜歡哪一個東西？A：一隻小狗，B：來自心上人的漂亮鮮花，還是C：以正確格式儲存的大量資料檔案？ 守衛機器人 1：選擇！ 《飛出個未來》第一季第5集”Fear of a Bot Planet”台詞(1999~2003於美國福斯電視網播映的喜劇動畫片；台灣無代理商引進播映)\n\n某天我看了動畫片《飛出個未來》某一集，描述名叫Chapek 9的外星球原住民古怪風俗，能進入星球首都的訪客必須是機器器，絕不能是人類。守衛為了確認訪客是不是人類，會詢問訪客是喜歡小狗、鮮花還是格式正確的大量資料檔案。我心想：“這是相當聰明的問題，但是如果人類和機器人有相同的喜好呢？那就不是一個好的測試問題了吧？”其實，我偶然間取得了Chapek 9首都市政府用來檢查這個問題有沒有效的測試資料。他們做的測試非常簡單，就是找來一群機器人和一群人類，問他們喜歡什麼。所有資料都儲存在chapek9.omv這個檔案裡，有安裝本書資料庫模組的話，可以直接從jamovi資料庫匯入這個檔案(Chapek 9)。除了識別參與者的變項ID，還有兩個名義尺度變項，species和choice，這份資料檔案一共有180筆參與者的反應。全部資料包括93個人類和87個機器人，絕大多數參與者選擇了資料檔案。只要從’Exploration’ - ‘Descriptives’開啟描述統計設定視窗，建立次數表(frequency table)就能確認。不過，只有描述統計還無法檢測這樣的問題能不能有效區別機器人和人類，我們需要對資料進行更詳細的描述。我們要按照種族區分的各種選擇的次數，也就是建立這份資料的列聯表（cross tabulation，見 小單元 6.1 的介紹）。啟動’Frequencies’ - ‘Contingency Tables’ - ’Independent Samples’的設定視窗就能建立列聯表，成果會如同 表 10.7 。\n\n\n\n\n\n\n表 10.7: Chapek 9資料列聯表\n\n\n\nRobot\nHuman\nTotal\n\n\nPuppy\n13\n15\n28\n\n\nFlower\n30\n13\n43\n\n\nData\n44\n65\n109\n\n\nTotal\n87\n93\n180\n\n\n\n\n\n\n\n\n這份列聯表清楚展現絕大多數人類參與者回答喜歡資料檔案，而機器人參與者對每一項的回答相對平衡。先不管為什麼人類可能更喜歡資料檔案（這確實看起來有點奇怪，承認吧），我們現在的目標是確定這份資料的人類和機器人的選擇差異有沒有統計顯著性。\n\n\n10.2.1 獨立性假設檢定程序\n要如何分析這樣的資料？由於一開始的研究假設是”人類和機器人回答問題的方式並不一樣”，假設檢定程序要測試的虛無假設應該設定為”人類和機器人回答問題的方式是一樣的”？與適合度檢定一樣，首先要定義一些描述資料的符號（表 10.8）。\n\n\n\n\n\n表 10.8: 獨立性假設檢定範例各項資料的符號表\n\n\n\nRobot\nHuman\nTotal\n\n\nPuppy\n\\(O_{11}\\)\n\\(O_{12}\\)\n\\(R_{1}\\)\n\n\nFlower\n\\(O_{21}\\)\n\\(O_{22}\\)\n\\(R_{2}\\)\n\n\nData\n\\(O_{31}\\)\n\\(O_{32}\\)\n\\(R_{3}\\)\n\n\nTotal\n\\(C_{1}\\)\n\\(C_{2}\\)\nN\n\n\n\n\n\n\n\n\n根據符號表，每個 \\(O_{ij}\\) 代表其中一個種族的受測者j（機器人或人類）所做的選擇 i（小狗，鮮花或資料）之總計次數（觀察次數）。總計次數通常用\\(N\\)表示。然後，\\(R_i\\) 表示各項選擇的總人數，像是\\(R_1\\) 代表選擇鮮花的受測者人數，\\(C_j\\) 表示各種族受測者人數，像是\\(C_1\\) 代表機器人的總數。11\n接著來想想虛無假設的設定。如果機器人和人類對這個問題的回答是一樣的，也就是“機器人選小狗”的機率與“人類選小狗”的機率相同，其他兩個選項的機率也是如此。所以用符號\\(P_{ij}\\) 表示種族j的受測者回答選項i的機率，因此虛無假設就是：\n\\[\n\\begin{aligned}\nH_0 &: \\text{實驗結果符合以下三項：} \\\\\n&P_{11} = P_{12}\\text{ （選擇“小狗”的機率相同），} \\\\\n&P_{21} = P_{22}\\text{ （選擇“鮮花”的機率相同），還有} \\\\\n&P_{31} = P_{32}\\text{ （選擇“資料”的機率相同）}\n\\end{aligned}\n\\]\n其實，因為虛無假設所設定的真實機率不必限定受測者的種族，符號可以再簡化用\\(P_i\\)代表做某個選擇的機率，例如\\(P_1\\)代表選擇小狗的真實機率。\n接下來的程序就和適合度檢定一樣，就是計算期望次數。對應每個觀察次數\\(O_{ij}\\)，需要先搞清楚虛無假設預測每個觀察次數是多少，因此用\\(E_{ij}\\) 表示每個期望次數。這個問題的狀況有點棘手，如果種族 \\(j\\) 有 \\(C_j\\) 人，無論是那個種族的受測者做出什麼選項\\(i\\)的真實機率是\\(P_i\\)，那麼期望次數就是：\n\\[E_{ij}=C_j \\times P_i\\]\n到這一步還算順利，但是遇到了一個問題。與適合度檢定程序不同的是，這裡的虛無假設實際上並未指定\\(P_i\\)的數值。\n用資料估計未知量數是必須的步驟(需要複習的話請回 單元 8 )！幸運的是，這不難做到。若是180位參與者裡有28 位選擇了鮮花，那麼選擇鮮花的機率很自然的估計值就是 \\(\\frac{28}{180}\\)，大約是 \\(0.16\\)。若是用數學式呈現估計選擇i的機率，就是用每行總次數除以總樣本次數：\n\\[\\hat{P}_{i}= \\frac{R_i}{N}\\]\n因此，期望次數可以改寫為各行次數與各列次數的乘積，再除以總觀察次數：12\n\n[額外的技術細節13]\n與前一個檢定程序一樣，\\(\\chi^2\\) 的數值越大，表示虛無假設對資料的解釋越差，而 \\(\\chi^2\\) 的數值越大，表示虛無假設對資料的解釋越好。所以如同前一種檢定程序，如果 \\(\\chi^2\\) 數值太大，就有可能拒絕虛無假設。\n不出聰明的讀者所料，這個檢定統計值遵循 \\(\\chi^2\\) 分佈。現在要做的就是弄清楚有多少自由度，實際上這並不難知道。如同之前提到的，研究人員通常可以將自由度當成正在分析的資料點總數量，減去不可變動的資料點數量。具備 r 行和 c 列的列聯表總共有 \\(r^{c}\\) 個觀察次數，所以這是觀察次數的全部數量。那不可變動的有多少呢？這裡的狀況稍微複雜一些，但是答案始終是相同的\n\\[df=(r-1)(c-1)\\]\n不過要解釋為什麼自由度是這樣算，需要考慮實驗設計。為了方便說明，假如真實資料真的有 87 台機器人和 93 位人類，不過因為選擇是隨機的，讓每行的總次數自由變化，就可以考慮這裡有多少不可變動的資料點。由於題目情境已經限制了每列的總次數，所以有c 個不可變動的資料點。其實還有更多不可變動的資料點，記得虛無假設提到了需要估計的參數\\(P_i\\)，本書雖然不會解釋為什麼這些參數是需要考慮的，現在讀者只要知道虛無假設所列出的參數都是不可變動，如此一來，問題就簡化成這種參數有多少呢？其實很簡單，因為所有機率必須加起來等於 1，所以只有 \\(r - 1\\) 個。因此，卡方獨立性檢定的自由度是：\n\\[ \\begin{split} df & = \\text{(number of\nobservations) - (number of constraints)} \\\\\\\\ & = (r \\times c) - (c +\n(r - 1)) \\\\\\\\ & = rc - c - r + 1 \\\\\\\\ & = (r - 1)(c - 1) \\end{split}\\]\n另一種解釋方式是，假如研究人員唯一確定的數值只有總樣本量 \\(N\\)。也就是說，研究人員對180位受測者進行問卷調查，結果發現 87 位是機器人，93 位是人類。現在的推論方式會不大相同，但仍然會得到相同的答案。虛無假設仍然有 \\(r - 1\\) 個待估計的參數，對應每個回答項目被選擇的機率值，現在要加上\\(c - 1\\) 個待估計的參數，對應受測者被確認是機器人的機率值。14還有，我們確定觀察值的總次數 \\(N\\)，這是另一個不可變動的參數。所以一共有\\(rc\\)項資料點，其中有 \\((c-1)+(r-1)+1\\) 個不可變動的資料點。那正確答案是多少呢？\n這真是太神奇了。\n\n\n\n\n10.2.2 獨立性檢定實作\n好吧，既然我們知道了檢驗是如何進行的，讓我們看看如何在 jamovi 中完成它。雖然讓您長時間地經歷繁瑣的計算以便被迫學習可能很有吸引力，但我認為這是沒有意義的。在上一節中，我已經向您展示了如何針對適合度檢驗進行長時間的操作，而且由於獨立性檢驗在概念上沒有任何不同，所以您不會通過長時間的操作學到任何新的東西。因此，我將直接向您展示簡單的方法。在 jamovi 中運行檢驗（“頻率” - “列聯表” - “獨立樣本”）之後，您只需查看 jamovi 結果窗口中列聯表下方，那裡就是 \\(\\chi^2\\) 統計量。這顯示了一個 \\(\\chi^2\\) 統計值為 10.72，2 d.f.，p-value = 0.005。\n那很簡單，不是嗎？您還可以要求 jamovi 顯示預期計數 - 只需單擊“Cells”選項中的“Counts” - “Expected”複選框，預期計數將出現在列聯表中。同時，在此操作中，效果大小度量會有所幫助。我們將選擇 Cramér’s \\(V\\)，您可以在“Statistics”選項中的複選框中指定它，它會給出 Cramér’s \\(V\\) 的值為 \\(0.24\\)。參見 圖 10.6。我們稍後會再談論這個問題。\n\n\n\n\n\n\n\n圖 10.6: 在 jamovi 中使用 Chapek 9 資料進行獨立樣本 \\(\\chi^2\\) 檢驗\n\n\n\n\n這個輸出為我們提供了足夠的信息來寫出結果：\n\nPearson 的 \\(\\chi^2\\) 顯示了物種和選擇之間存在顯著關聯（\\(\\chi^2(2) = 10.7, p&lt; .01\\)）。機器人似乎更傾向於說他們喜歡花，而人類更傾向於說他們喜歡資料。\n\n注意，再次，我提供了一些解釋，以幫助人類讀者理解資料發生的情況。稍後在我的討論部分，我會提供更多的上下文。舉例來說，這是我可能會在之後說的：\n\n人類似乎比機器人更喜歡原始資料文件，這有點反直覺。但在某種程度上，它是有道理的，因為 Chapek 9 上的民事當局往往在發現人類時會將其殺死並解剖。因此，最有可能的是，人類參與者並未如實回答問題，以避免可能產生不良後果。這應該被認為是一個嚴重的方法論缺陷。\n\n我想，這可以被歸類為反應效應的一個極端例子。顯然，在這種情況下，問題嚴重到研究幾乎毫無價值，作為理解人類和機器人之間的差異偏好的工具。然而，我希望這能夠說明在獲得統計顯著結果（我們拒絕虛無假設，轉而接受替代假設）和找到具有科學價值的東西（由於嚴重的方法論缺陷，資料對我們研究假設的興趣一無所知）之間的區別。"
  },
  {
    "objectID": "04-Descriptive-statistics.html#footnotes",
    "href": "04-Descriptive-statistics.html#footnotes",
    "title": "4  描述統計",
    "section": "",
    "text": "不知道澳洲足球聯盟的同學不必擔心，即使不了解AFL規則，也不會影響使用這份資料學習描述統計。↩︎\n建議初學的同學在紙上寫下幾種平均值的公式，才能加深印象。平均值的數學記號通常是 \\(\\bar{X}\\) ，公式則寫成 \\[\\bar{X}=\\frac{X_1 + X_2 ... + X_{N-1} + X_{N}}{N}\\] 這樣寫雖然正確，但是太過冗長，所以通常會用連加符號 \\(\\sum\\) \\(^a\\) 縮減公式。像是這個例子要連加五筆資料，寫成公式 \\(X_1 + X_2 + X_3 + X_4 + X_5\\) 看起來很長，使用連加符號就能簡化為 \\[\\sum_{i=1}^{5} X_i\\] 。字面意思是”以i代表1到5，將每個 \\(X_i\\) 的數值加起來”。不過這和”連加前五筆數值“的意思還是不大一樣。雖然我們可以將這個公式擴展為適要任何狀況： \\[\\bar{X}=\\frac{1}{N}\\sum_{i=1}^{N}X_i\\] 但是可能無法幫助數感不夠的同學了解平均值的概念。其實只要寫平均值的概念只要用幾個字表達就夠清楚：將一筆資料所有數值加起來再除以資料的數值個數。但是這本書的教學目的不光是要解釋概念，也要使用這本書的同學了解在本書以及jamovi的介面隨處可見的各種數學記號是什麼意思。請留意平均值公式裡的每個記號，之後每一章都會不斷遇到。這些記號不論出現在那裡，意思都是一樣的：\\(\\bar{X}\\) 代表平值， \\(\\sum\\) 代表連加一筆數值， \\(X_i\\) 代表第i個數值， \\(N\\) 代表一筆資料的數值個數。 —\\(^a\\) 選用記號的理由 \\(\\sum\\) 代表數值連加並不是沒有理由的，其實這是希臘字母 \\(\\sigma\\) 的大寫，相當於英文字母的大寫 \\(S\\) 。也有另一個大寫希臘字母 \\(\\prod\\) 代表數值連乘，這個字母的小寫就是代表圓周率的 \\(\\pi\\) ，相當於英文字母的 \\(P\\) 。↩︎\nwww.abc.net.au/news/stories/2010/09/24/3021480.htm↩︎\n譯註~這段計算步驟可參考譯者準備的jamovi檔案，下載後以jamovi開啟，觀看計算變項的說明。↩︎\n儘管平均絕對差的手算演練告一段落，還有一些事情要交待。我建議演練過程也要將公式寫出來，這樣子計算量數的數學公式意思才能記在心中。尤其是“平均絕對差”(mean absolute deviation)與”中位數絕對差“(median absolute deviation)的英文縮寫都是MAD，讀文獻時不注意的話會混淆。原作者建議“平均絕對差”的英文縮寫應該寫成AAD(average absolute deviation)，才能避免寫作與閱讀的問題。這個範例的公式與計算結果是： \\[AAD(X) =\\frac{1}{N} \\sum_{i=1}^{N} \\mid X_i - \\bar{X} \\mid = 15.52\\]↩︎\n有關變異數的眾多細節，在此特別要提一點，就是變異數是可加成的。假如現在有兩個變項 \\(X\\) 與 \\(Y\\)，各自的變異數是 \\(Var(X)\\) 與 \\(Var(Y)\\)。接著我們定義一個新變項 \\(Z\\) 是\\(X\\) 與 \\(Y\\)的加成，也就是 \\(Z = X + Y\\)，變項 \\(Z\\) 的變異數就等於 \\(Var(X) + Var(Y)\\)。這個特性在之後推論統計方法的學習非常有幫助，而且其他變異量數沒有相同的特性。↩︎\n變異數的完整公式如下： \\[VAR(X) =\\frac{1}{N} \\sum_{i=1}^{N} ( X_i - \\bar{X} )^2\n\\] 基本上和平均絕對差幾乎一樣，除了每筆資料的離均差異值是以平方計算。這是為何變異數有時候又被稱為“平均平方差”。↩︎\n譯註~這段計算步驟可參考譯者準備的jamovi檔案，下載後以jamovi開啟，觀看計算變項的說明。↩︎\n也許還有第三種計算變異數的方法。↩︎\njamovi計算變異數公式的是： \\[\\frac{1}{N-1} \\sum_{i=1}^{N} ( X_i - \\bar{X} )^2\\]↩︎\n由於標準差是變異數的開根號，計算公式就是 \\[s=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} ( X_i - \\bar{X} )^2 }\\] 在jamovi的描述統計選單裡，勾選’Variance’下方的’Std. deviation’，就能在報表介面看到176場得分資料的標準差 \\(26.07\\)。↩︎\n到了 單元 8 我們會再來討論這個問題。請先記得jamovi計算的樣本標準差，數學記號寫作 \\(\\hat{\\sigma}\\) ，公式是 \\[\\hat{\\sigma}=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N} ( X_i - \\bar{X} )^2}\\]↩︎\n偏態指數的常用公式是 \\[skewness(X)=\\frac{1}{N \\hat{\\sigma}^3} \\sum_{i=1}^{N} ( X_i - \\bar{X})^3\\] 公式裡的 N 代表資料個數, \\(\\bar{X}\\) 是樣本平均值， \\(\\hat{\\sigma}\\) 是樣本標準差 (分母是”\\(N - 1\\)” 的那一種)↩︎\n峰度的計算公式很接近變異數與偏態，除了變異數是計算平方，偏態計算立方，峰度的公式要計算四次方： \\(^b\\) \\[kurtosis(X)=\\frac{1}{N \\hat{\\sigma}^4} \\sum_{i=1}^{N} ( X_i - \\bar{X} )^4 - 3\\] 當然，大多數同學應該沒有太大興趣知道。 — \\(^b\\) “-3”是校正用常數，確保常峰態的數值為0。在公式裡似乎畫蛇添足，其實背後有相當充分的數學原則。↩︎\n如果計算得到的數值太小或太大，jamovi會自動切換成以”指數模式”顯示。像是6.51e-4是代表簡化小數點後四位之前的0，所以這個數值的真面目是0.000651。如果是6.51e-4，代表簡化小數點前4位的0，所以這個數值的真面目是65,100.00。這種方式可以節省報表空間，想像6.51e-16的真實數值要寫多少0。↩︎\nz分數真正的數學公式是 \\[z_i =\\frac{X_i - \\bar{X}}{\\hat{\\sigma}}\\]↩︎\n任何統計學講師都會不厭其煩地提醒：變項A的標準差與變項B的標準差是不一樣的單位。同學可以運用常識想清楚z分數所代表的意義。↩︎"
  }
]