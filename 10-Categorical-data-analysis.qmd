# 類別資料分析 {#sec-Categorical-data-analysis}

```{r}
#| include: FALSE
source("header.R")
```

<!---

> **譯者註** 20230417初步以ChatGPT-4完成翻譯，內容待編修。
--->

> **譯者註** 20240210完成除了jamvoi實作，本單元各種檢定方法的原理及公式說明第一次校對。

> [導讀簡報](slides10.html){target="_blank"}


至此已經了解假設檢定的基本理論，是時候開始學習心理學研究常用的統計檢定方法。那麼應該從那種方法開始學習呢？並非所有教材作者都有共識，我(原作者)建議從“卡方檢定”（英文發音為“chi-square”[^10-categorical-data-analysis-1]）和“t 檢定”（@sec-Comparing-two-means）開始。這兩種方法在科學研究實務經常被使用，雖然解析資料的威力不如後面單元介紹“迴歸分析”和“變異數分析”強大，對初學者來說相對容易理解。

[^10-categorical-data-analysis-1]: 繁體中文報告常直接稱呼“卡方”。

“類別資料”（Categorical data）只是“名義尺度資料”（nominal scale data）的另一種稱呼。這不是本書尚未介紹的東西，只是在資料分析的場合，我不明白為何許多研究者習慣稱呼“類別資料”，而不是“名義尺度資料”。無論如何，**類別資料分析**是指處理名義尺度資料時，可以使用的一套統計方法。不過，專用於分析類別資料的方法非常多種，本單元僅介紹幾種較常用的方法。

<!--- Now that we've covered the basic theory behind hypothesis testing it's time to start looking at specific tests that are commonly used in psychology. So where should we start? Not every textbook agrees on where to start, but I'm going to start with "$\chi^2$ tests" (this chapter, pronounced "chi-square"[^10-categorical-data-analysis-1] and "t-tests" in @sec-Comparing-two-means). Both of these tools are very frequently used in scientific practice, and whilst they're not as powerful as "regression" and "analysis of variance" which we cover in later chapters, they're much easier to understand.

[^10-categorical-data-analysis-1]: Also sometimes referred to as "chi-squared".

The term "categorical data" is just another name for "nominal scale data". It's nothing that we haven't already discussed, it's just that in the context of data analysis people tend to use the term "categorical data" rather than "nominal scale data". I don't know why. In any case, **categorical data analysis** refers to a collection of tools that you can use when your data are nominal scale. However, there are a lot of different tools that can be used for categorical data analysis, and this chapter covers only a few of the more common ones. --->

## 卡方適合度檢定 {#sec-chi2-goodenss-of-fit}

卡方適合度檢定(goodness-of-fit test)是統計學者們最早開發出來的假設檢定方法之一。主要發明者是20世紀初的統計學者卡爾．皮爾森( Karl Pearson , 皮爾森相關係數以他的名字命名)  [@Pearson1900]，之後羅納德·費雪爵士(Sir Ronald Fisher) [@Fisher1922a] 做了一些改良，這個方法是檢定名義變項的觀察次數分佈是否符合預期次數分佈。像是有一組病人接受了實驗性治療，根據治療後的健康狀況分類，醫師紀錄病情是否有改善、保持不變或惡化，接著使用適合度檢定可以確定每個類別的人次，是否匹配標準治療後的預期人次。以下用一些心理學研究案例學習如何使用卡方適合度檢定。

<!--- The $\chi^2$ goodness-of-fit test is one of the oldest hypothesis tests around. It was invented by Karl Pearson around the turn of the century [@Pearson1900], with some corrections made later by Sir Ronald Fisher [@Fisher1922a]. It tests whether an observed frequency distribution of a nominal variable matches an expected frequency distribution. For example, suppose a group of patients has been undergoing an experimental treatment and have had their health assessed to see whether their condition has improved, stayed the same or worsened. A goodness-of-fit test could be used to determine whether the numbers in each category - improved, no change, worsened - match the numbers that would be expected given the standard treatment option. Let's think about this some more, with some psychology. --->

### 撲克牌花色隨機選擇資料

過去幾十年許多人類模擬隨機現象的研究顯示，這是人類很難學成的能力。雖然每個人或多或少會試表“隨機行動”，依照模式和結構進行思考仍然是一般人難以擺脫的習慣，就算是被要求“隨機做某件事情”，實際上完全無法隨機行事，所以這類研究反過來揭露了許多人類的非隨機行為，其中透露我們如何看待世界的深刻心理問題。這一節的範例啟發自隨機行為研究，以此虛構一個非常簡單的研究。假如研究人員要求參與者想像一副已經洗好的撲克牌，然後從這副牌裡“隨機”選出一張牌。參與者選好想像中的牌之後，再選出第二張牌。參與者選擇好之後，再請他們說出選擇的花色（紅心、梅花、黑桃或方塊）。假如這個研究最後收集了 $N = 200$ 個人選出的牌，研究人員想分析資料，確認一般人假裝選擇的撲克牌是否真的隨機。這份資料存在lsj檔案庫的 *Randomness* ，當讀者從 jamovi 的lsj檔案庫開啟這份資料的試算表，會看到三個變項：為每個參與者分配辨識碼`id` ，以及紀錄每個人先後選出的兩種撲克牌花色 `choice_1` 和 `choice_2`。


現在先看參與者第一次選擇的花色。開啟“Explore” - “Descriptives”的設定視窗，點選`Frequency table`選項計算每個花色被參與者們選擇的次數。得到的結果就如同 @tbl-tab14-1 ：



<!--- Over the years there have been many studies showing that humans find it difficult to simulate randomness. Try as we might to "act" random, we think in terms of patterns and structure and so, when asked to "do something at random", what people actually do is anything but random. As a consequence, the study of human randomness (or non-randomness, as the case may be) opens up a lot of deep psychological questions about how we think about the world. With this in mind, let's consider a very simple study. Suppose I asked people to imagine a shuffled deck of cards, and mentally pick one card from this imaginary deck "at random". After they've chosen one card I ask them to mentally select a second one. For both choices what we're going to look at is the suit (hearts, clubs, spades or diamonds) that people chose. After asking, say, $N = 200$ people to do this, I'd like to look at the data and figure out whether or not the cards that people pretended to select were really random. The data are contained in the *randomness.csv* file in which, when you open it up in jamovi and take a look at the spreadsheet view, you will see three variables. These are: an id variable that assigns a unique identifier to each participant, and the two variables choice_1 and choice_2 that indicate the card suits that people chose.

For the moment, let's just focus on the first choice that people made. We'll use the Frequency tables option under 'Exploration' - 'Descriptives' to count the number of times that we observed people choosing each suit. This is what we get (@tbl-tab14-1): --->

```{r}
#| label: tbl-tab14-1
#| tbl-cap: 的第一次選擇撲克牌花色的次數紀錄
#Number of times each suit was chosen
huxtabs[[10]][[1]]
```


這份小小的次數表非常有用。其中數字似乎暗示，參與者們可能偏好選擇紅心而且比較不想選擇梅花。只看表面數字並不能判斷這樣的差距是不是巧合，因此需要進行統計分析來找出答案，這就是下一節要學習的功課。

沒問題的話，接下來要分析 @tbl-tab14-1 的資料囉。然後，這裡開始不得不用些數學符號討論這些資料，所以最好先認識一下每個符號的意義。首先是至目前為止一直提到“觀察值”，將用大寫字母$O$，而字母的下標表示觀察值在表格裡的位置。像是 @tbl-tab14-1 的第二個觀察值可寫為$O_2$。@tbl-tab14-2  說明每個花色的報告次數與代表符號之間的對應 。



<!--- That little frequency table is quite helpful. Looking at it, there's a bit of a hint that people might be more likely to select hearts than clubs, but it's not completely obvious just from looking at it whether that's really true, or if this is just due to chance. So we'll probably have to do some kind of statistical analysis to find out, which is what I'm going to talk about in the next section.

Excellent. From this point on, we'll treat this table as the data that we're looking to analyse. However, since I'm going to have to talk about this data in mathematical terms (sorry!) it might be a good idea to be clear about what the notation is. In mathematical notation, we shorten the human-readable word "observed" to the letter $O$, and we use subscripts to denote the position of the observation. So the second observation in our table is written as $O_2$ in maths. The relationship between the English descriptions and the mathematical symbols are illustrated in @tbl-tab14-2. --->

```{r}
#| label: tbl-tab14-2
#| tbl-cap: 英語描述與數學符號之間的關係
#Relationship between English descriptions and mathematical symbols
huxtabs[[10]][[2]]
```

希望這樣整理能讓讀者搞清楚。同時提醒一下，數學家更喜歡用符號討論而不是直接談論具體事項，因此接著會一直看到$O_i$之類的符號，這是指在第 i 類別的觀察次數（其中 i 可能是 1，2，3 或 4）。最後，如果我們要在報告裡提及所有觀察次數，統計學家習慣將所有觀察值構成一個向量 [^10-categorical-data-analysis-2]，本書以 $O$ 之類的大寫字母表示。

[^10-categorical-data-analysis-2]: 向量是由相同測量尺度的資料值所構成的序列。

$$O = (O_1, O_2, O_3, O_4)$$

同樣的，這裡沒有什麼新奇有趣之處，一切只是符號。如果說 $O = (35, 51, 64, 50)$，就只是將描述觀察值的次數表，改用數學符號來表示而已。

<!--- Hopefully that's pretty clear. It's also worth noting that mathematicians prefer to talk about general rather than specific things, so you'll also see the notation $O_i$, which refers to the number of observations that fall within the i-th category (where i could be 1, 2, 3 or 4). Finally, if we want to refer to the set of all observed frequencies, statisticians group all observed values into a vector [^10-categorical-data-analysis-2], which I'll refer to as $O$.

[^10-categorical-data-analysis-2]: A vector is a sequence of data elements of the same basic type.

$$O = (O_1, O_2, O_3, O_4)$$

Again, this is nothing new or interesting. It's just notation. If I say that $O = (35, 51, 64, 50)$ all I'm doing is describing the table of observed frequencies (i.e., observed), but I'm referring to it using mathematical notation. --->

### 虛無假設與對立假設

正如一開始的範例說明，研究者的假設是“一般人不會隨機選出想像中的撲克牌”。接著要做的是將概念中的假設，轉換為相互對立的統計假設，然後決定測試這些統計假設的統計方法。這個範例要使用的統計方法就是需要使用這一節要學習**皮爾森 $\chi^2$ **適合度檢定，規劃適合度檢定的第一步是設定虛無假設，撲克牌範例的虛無假設是很簡單地。我們先用文字說明虛無假設：

$$H_0: \text{ 四種花色被選擇的機率相等}$$

修習統計學課程的學生要學會的功課之一，就是用數學符號表達虛無假設，這裡使用 $P_j$ 表示第j種花色被參與者 選擇的真實機率。如果研究結果符合虛無假設，那麼任何一種花色都有 25% 的機率被選中。換句話說，以上的虛無假設說明用數學符號表達的方式是$P_1 = .25$，$P_2 = .25$，$P_3 = .25$， $P_4 = .25$。因為研究人員習慣用向量符號涵括同一個變項的資料，在此用 $P$ 表示虛無假設涵括的所有機率事件。也就是向量 $P = (P_1, P_2, P_3, P_4)$ 表示虛無假設的機率事件集合，這麼一來可將虛無假設寫成：

$$H_0: P =(.25, .25, .25, .25)$$

對應虛無假設的向量 $P$ ，涵括的所有事件發生機率剛好相等，不過真實的研究條件不一定會是這樣。如同這個範例的實驗任務是讓參與者從想像中的撲克牌組抽一張牌，若是這副牌的梅花數量是其他花色的兩倍，那麼虛無假設就要寫成$P = (.4, .2, .2, .2)$。只要機率值都是正數，且總和為 1，就能構成合法的虛無假設。因為許多使用適合度檢定的場景，是用來處理所有類別事件發生機率相等的虛無假設，以下討論繼續使用四種花色被挑選機率相等的虛無假設。

那麼對立假設 $H_1$是什麼呢？這道統計假設表示研究者感興趣的研究結果，也就是認為參與者的選擇不是完全隨機，因此四種花色的發生機率並不相等。所以兩則統計假設的白話版本是：

$H_0: \text{ 四種花色被選擇的機率相等}$
$H_1: \text{ 至少有一個選擇花色的機率不是 0.25}$

...或者可寫成“數學家偏好的”版本：

<!--- As the last section indicated, our research hypothesis is that "people don't choose cards randomly". What we're going to want to do now is translate this into some statistical hypotheses and then construct a statistical test of those hypotheses. The test that I'm going to describe to you is **Pearson's** $\chi^2$ (chi-square) goodness-of-fit test, and as is so often the case we have to begin by carefully constructing our null hypothesis. In this case, it's pretty easy. First, let's state the null hypothesis in words:

$$H_0: \text{ All four suits are chosen with equal probability}$$

Now, because this is statistics, we have to be able to say the same thing in a mathematical way. To do this, let's use the notation $P_j$ to refer to the true probability that the j-th suit is chosen. If the null hypothesis is true, then each of the four suits has a 25% chance of being selected. In other words, our null hypothesis claims that $P_1 = .25$, $P_2 = .25$, $P3 = .25$ and finally that $P_4 = .25$ . However, in the same way that we can group our observed frequencies into a vector O that summarises the entire data set, we can use P to refer to the probabilities that correspond to our null hypothesis. So if I let the vector $P = (P_1, P_2, P_3, P_4)$ refer to the collection of probabilities that describe our null hypothesis, then we have:

$$H_0: P =(.25, .25, .25, .25)$$

In this particular instance, our null hypothesis corresponds to a vector of probabilities P in which all of the probabilities are equal to one another. But this doesn't have to be the case. For instance, if the experimental task was for people to imagine they were drawing from a deck that had twice as many clubs as any other suit, then the null hypothesis would correspond to something like $P = (.4, .2, .2, .2)$. As long as the probabilities are all positive numbers, and they all sum to 1, then it's a perfectly legitimate choice for the null hypothesis. However, the most common use of the goodness-of-fit test is to test a null hypothesis that all of the categories are equally likely, so we'll stick to that for our example.

What about our alternative hypothesis, $H_1$? All we're really interested in is demonstrating that the probabilities involved aren't all identical (that is, people's choices weren't completely random). As a consequence, the "human friendly" versions of our hypotheses look like this:

$H_0: \text{ All four suits are chosen with equal probability}$
$H_1: \text{ At least one of the suit-choice probabilities isn’t 0.25}$

...and the "mathematician friendly" version is: --->

$H_0: P= (.25, .25, .25, .25)$
$H_1: P \neq (.25, .25, .25, .25)$

### 適合度檢定統計程序

到了這一步，我們手上有一組觀察次數 $O$ 以及一組對應虛無假設的機率 $P$，接著就是規劃虛無假設的檢定程序。如同 @sec-Hypothesis-testing 的說明，要檢測 $H_0$ 和 $H_1$，就需要計算檢測統計值，構成適合度檢定的統計值是衡量資料與虛無假設之間的“接近程度”。如果資料不相符虛無假設的期望機率，那麼虛無假設呈現的情況可能不是真的。那麼，如果虛無假設是真的，檢測結果會是什麼樣子呢？或者專有名詞來問，**期望次數**是什麼？總共有 $N = 200$ 次觀察，若是虛無假設為真，任何一位參與者選擇紅心的機率是 $P_3 = .25$，所以紅心的期望次數是 $200 \times .25 = 50$ ，對吧？更具體的方式是用$E_i$代表“虛無假設為真時，研究人員期望觀察到第 i 類反應的次數”，也就是這個數學公式：


$$E_i=N \times P_i$$

這個式子不難用手計算。就像現在的200 個觀察值是分為四個類別，研究人員認為參與者會選擇任何一個類別的可能性相等，那麼每個類別應該都有50人次，對吧？

接著要如何將觀察次數和期望次數轉換為檢測統計值呢？只要比較每個類別的期望觀察次數（$E_i$）與各自的實際觀察次數（$O_i$），就能得出一個有用的檢測統計值。首先要計算虛無假設的期望次數與實際總計出的次數之間差異也就是計算“觀察次數減去期望次數”的差異值，$O_i - E_i$。詳細計算請見 @tbl-tab14-3 。




<!--- At this point, we have our observed frequencies O and a collection of probabilities P corresponding to the null hypothesis that we want to test. What we now want to do is construct a test of the null hypothesis. As always, if we want to test $H_0$ against $H_1$, we're going to need a test statistic. The basic trick that a goodness-of-fit test uses is to construct a test statistic that measures how "close" the data are to the null hypothesis. If the data don't resemble what you'd "expect" to see if the null hypothesis were true, then it probably isn't true. Okay, if the null hypothesis were true, what would we expect to see? Or, to use the correct terminology, what are the **expected frequencies**. There are $N = 200$ observations, and (if the null is true) the probability of any one of them choosing a heart is $P_3 = .25$, so I guess we're expecting $200 \times .25 = 50$ hearts, right? Or, more specifically, if we let Ei refer to "the number of category i responses that we're expecting if the null is true", then

$$E_i=N \times P_i$$

This is pretty easy to calculate.If there are 200 observations that can fall into four categories, and we think that all four categories are equally likely, then on average we'd expect to see 50 observations in each category, right?

Now, how do we translate this into a test statistic? Clearly, what we want to do is compare the expected number of observations in each category ($E_i$) with the observed number of observations in that category ($O_i$). And on the basis of this comparison we ought to be able to come up with a good test statistic. To start with, let's calculate the difference between what the null hypothesis expected us to find and what we actually did find. That is, we calculate the "observed minus expected" difference score, $O_i - E_i$ . This is illustrated in @tbl-tab14-3. --->

```{r}
#| label: tbl-tab14-3
#| tbl-cap: 各種花色的期望次數(expected frequencu)、實際次數(observed frequency)和差異分數(difference score)
#Expected and observed frequencies
huxtabs[[10]][[3]]
```

根據計算結果，選擇紅心的人數顯然比虛無假設預測的多，選擇梅花的人較少。不過稍微想一下就會發現，最後一列的差異值有負的數值！這裡的奇怪之處在於虛無假設所做的預測少於觀察次數(像示範資料的紅心)，和預測多於觀察次數(像示範資料的梅花)一樣糟糕。最簡單的解決方法是將所有數字平方，如此就可以計算平方差，$(E_i - O_i)^2$。這樣就能算出如 @tbl-tab14-4 的數值。


<!--- So, based on our calculations, it's clear that people chose more hearts and fewer clubs than the null hypothesis predicted. However, a moment's thought suggests that these raw differences aren't quite what we're looking for. Intuitively, it feels like it's just as bad when the null hypothesis predicts too few observations (which is what happened with hearts) as it is when it predicts too many (which is what happened with clubs). So it's a bit weird that we have a negative number for clubs and a positive number for hearts. One easy way to fix this is to square everything, so that we now calculate the squared differences, $(E_i - O_i)^2$ . As before, we can do this by hand (@tbl-tab14-4). --->

```{r}
#| label: tbl-tab14-4
#| tbl-cap: 四種花色差異次數平方
#Squaring the difference scores
huxtabs[[10]][[4]]
```

現在的算出來的這組數字有明顯的特色，與虛無假設預測差異太大的類別（梅花和紅心），數字都很大；預測接近的類別（方塊和黑桃），數字都很小。為了解釋稍後的步驟，我們要將這些平方差除以期望次數 $E_i$ ，也就是$\frac{(E_i-O_i)^2}{E_i}$。因為這個例子裡所有類別的 $E_i = 50$，用手一個一個算還挺無聊的，不過算完後會得到 @tbl-tab14-5 的結果。


<!--- Now we're making progress. What we've got now is a collection of numbers that are big whenever the null hypothesis makes a bad prediction (clubs and hearts), but are small whenever it makes a good one (diamonds and spades). Next, for some technical reasons that I'll explain in a moment, let's also divide all these numbers by the expected frequency Ei , so we're actually calculating $\frac{(E_i-O_i)^2}{E_i}$ . Since $E_i = 50$ for all categories in our example, it's not a very interesting calculation, but let's do it anyway (@tbl-tab14-5). -->

```{r}
#| label: tbl-tab14-5
#| tbl-cap: 四種花色'誤差'分數~差異平方除以期望次數
#Dividing the squared difference scores by the expected frequency to provide an 'error' score
huxtabs[[10]][[5]]
```

這裡我們算出了四種'誤差'分數，每個分數代表用虛無假設預測實際次數，所造成的“錯誤”程度。為了轉換為檢測統計值，接下來要做的事就是將這些數字加起來。最後加起來的值就是**適合度**，通常在報告裡寫成$\chi^2$（卡方）或 *適合值*(GOF)。所有步驟總結如以下公式：[^10-translation-1]

[^10-translation-1]: 譯註~原文這裡可能寫錯

$$\sum( (觀察次數 - 期望次數)^2 / 期望次數 )$$

最後得到適合度的卡方統計值 8.44。

[額外的技術細節 [^10-categorical-data-analysis-3]]

[^10-categorical-data-analysis-3]: 如果說 k 是指資料的類別數目（在這個研究資料是指撲克牌花色，所以 k = 4），則 $\chi^2$ 統計值可用這個公式計算：$$\chi^2 = \sum_{i=1}^{k} \frac{(O_i-E_i)^2}{E_i}$$ 如果 $\chi^2$ 統計值很小，代表觀察次數 Oi 非常接近虛無假設預測的次數 $E_i$ ，因此 $\chi^2$ 統計值要夠大，才能拒絕虛無假設。

根據以上的計算，這項研究資料分析結果得到了 $\chi^2$ = 8.44。那麼現在的問題就是，這個值是否大到足夠拒絕虛無假設？


<!--- In effect, what we've got here are four different "error" scores, each one telling us how big a "mistake" the null hypothesis made when we tried to use it to predict our observed frequencies. So, in order to convert this into a useful test statistic, one thing we could do is just add these numbers up. The result is called the **goodness-of-fit** statistic, conventionally referred to either as $\chi^2$ (chi-square) or GOF. We can calculate it as in @tbl-tab14-6.

$$\sum( (observed - expected)^2 / expected )$$

This gives us a value of 8.44.

[Additional technical detail [^10-categorical-data-analysis-3]]

[^10-categorical-data-analysis-3]: If we let k refer to the total number of categories (i.e., k = 4 for our cards data), then the $\chi^2$ statistic is given by: $$\chi^2 = \sum_{i=1}^{k} \frac{(O_i-E_i)^2}{E_i}$$ Intuitively, it's clear that if $chi^2$ is small, then the observed data Oi are very close to what the null hypothesis predicted $E_i$, so we're going to need a large $\chi^2$ statistic in order to reject the null.

As we've seen from our calculations, in our cards data set we've got a value of $\chi^2$ = 8.44. So now the question becomes is this a big enough value to reject the null? --->

### 適合度檢定的樣本分佈

要確定某個 $\chi^2$ 統計值是否大到能充分拒絕虛無假設，接著要確定如果虛無假設為真，$\chi^2$ 的樣本分佈會是什麼，在這一節中要學習的，就是如何規劃符合適合度檢定的樣本分佈，然後下一節學習如何使用這樣的樣本分佈構建假設檢程序。如果讀者願意相信樣本分佈是具有 $k - 1$ 自由度的 $\chi^2$（卡方）分佈，其實可以跳過本節。然而，如果讀者需要了解*為什麼*適合度檢定程序要如此安排，請繼續閱讀下去。

先假設虛無假設的說法是真的，那麼觀察值落入第 i 類的真實機率就是 $P_i$，這麼一來就符合對應虛無假設的向量，那這樣代表什麼意思呢？這像是擲出一枚特製的硬幣，觀察擲出那一面以決定落入類別i，而擲出那一面的的機率是 $P_j$。所以觀察到某一類別的實際次數 $O_i$，就像是擲出這枚特製硬幣N次，每次紀錄一個觀察值，最後有 $O_i$ 次紀錄到硬幣的那一面。雖然這樣的實驗很奇怪，這樣的說明只是希望讀者了解，我們已經在前面的單元見識類似的場景，像是 @sec-Introduction-to-probability 的 @sec-The-binomial-distribution 所示範的實驗設定。換句話說，如果虛無假設是真的，那麼多次實驗的觀察次數會符合二項分佈所構成的機率分佈：

$$O_i \sim Binomial(P_i,N) $$

若是讀者還記得 @sec-The-central-limit-theorem 提到的中央極限定理，當 $N$ 較大且 $P_i$ 剛好在0與1中間時，二項分佈看起來幾乎與常態分佈一模一樣。換句話說，只要 $N^P_i$ 足夠大，或者說，當期望次數 $E_i$ 足夠大時，理論上 $O_i$ 的機率分佈接近常態分佈。更棒的是，如果 $O_i$ 是常態分佈的，那麼 $(O_i-E_i)/\sqrt{E_i}$ 也會接近常態分佈。因為任何一個 $E_i$ 數值是固定的，所以實際觀察次數減去期望次數並除以期望值的開根號，相當於改變了常態分佈的平均值和標準差。為何這樣的統計值公式能計算適合度呢？因為這個公式將所有觀察值平方後加總，形成的樣本統計值可用近似的常態分佈計算可能發生的機率。等等，這就像 @sec-Other-useful-distributions 提到的，當我們手上有很多符合標準常態分佈的資料（平均值為 0 且標準差為 1），將所有數值平方然後加起來時，所得到的樣本數值符合卡方分佈。所以現在我們知道虛無假設所預測適合度統計值的樣本分佈符合卡方分佈。太棒了。

最後還要提一個細節，就是自由度。 @sec-Other-useful-distributions 曾提到把k個數值加總起來，所構成的卡方分佈的自由度就是 k。但是卡方適合度檢定的實際自由度是 $k - 1$，倒底是怎麼一回事呢？因為真正能加總的數值必須是獨立的。下一節我們將討論，為何全部有k個觀察值，卻只有 $k - 1$ 個是真正獨立的，所以自由度實際上只有 $k - 1$。這就是下一節的主題[^10-categorical-data-analysis-4]。

[^10-categorical-data-analysis-4]: 如果將適合度統計值的方程式重寫為 k - 1 個獨立觀察值之和，就會得到“正確的”樣本分佈，也就是自由度為  k - 1 的卡方分佈。要談這麼多數學細節就超出了這本入門書的範圍，這裡只給初學的讀者一個適合度統計值與卡方分佈關聯密切的直觀。

<!--- To determine whether or not a particular value of $\chi^2$ is large enough to justify rejecting the null hypothesis, we're going to need to figure out what the sampling distribution for $\chi^2$ would be if the null hypothesis were true. So that's what I'm going to do in this section. I'll show you in a fair amount of detail how this sampling distribution is constructed, and then, in the next section, use it to build up a hypothesis test. If you want to cut to the chase and are willing to take it on faith that the sampling distribution is a $\chi^2$ (chi-square) distribution with $k - 1$ degrees of freedom, you can skip the rest of this section. However, if you want to understand *why* the goodness-of-fit test works the way it does, read on.

Okay, let's suppose that the null hypothesis is actually true. If so, then the true probability that an observation falls in the i-th category is $P_i$ . After all, that's pretty much the definition of our null hypothesis. Let's think about what this actually means. This is kind of like saying that "nature" makes the decision about whether or not the observation ends up in category i by flipping a weighted coin (i.e., one where the probability of getting a head is $P_j$ ). And therefore we can think of our observed frequency $O_i$ by imagining that nature flipped N of these coins (one for each observation in the data set), and exactly $O_i$ of them came up heads. Obviously, this is a pretty weird way to think about the experiment. But what it does (I hope) is remind you that we've actually seen this scenario before. It's exactly the same set up that gave rise to @sec-The-binomial-distribution in @sec-Introduction-to-probability. In other words, if the null hypothesis is true, then it follows that our observed frequencies were generated by sampling from a binomial distribution:

$$O_i \sim Binomial(P_i,N) $$

Now, if you remember from our discussion of @sec-The-central-limit-theorem the binomial distribution starts to look pretty much identical to the normal distribution, especially when $N$ is large and when $P_i$ isn't too close to 0 or 1. In other words as long as $N^P_i$ is large enough. Or, to put it another way, when the expected frequency Ei is large enough then the theoretical distribution of $O_i$ is approximately normal. Better yet, if $O_i$ is normally distributed, then so is $(O_i-E_i)/\sqrt{(E_i)}$ . Since $E_i$ is a fixed value, subtracting off Ei and dividing by ? Ei changes the mean and standard deviation of the normal distribution but that's all it does. Okay, so now let's have a look at what our goodness-of-fit statistic actually is. What we're doing is taking a bunch of things that are normally-distributed, squaring them, and adding them up. Wait. We've seen that before too! As we discussed in the section on @sec-Other-useful-distributions, when you take a bunch of things that have a standard normal distribution (i.e., mean 0 and standard deviation 1), square them and then add them up, the resulting quantity has a chi-square distribution. So now we know that the null hypothesis predicts that the sampling distribution of the goodness-of-fit statistic is a chi-square distribution. Cool.

There's one last detail to talk about, namely the degrees of freedom. If you remember back to @sec-Other-useful-distributions, I said that if the number of things you're adding up is k, then the degrees of freedom for the resulting chi-square distribution is k. Yet, what I said at the start of this section is that the actual degrees of freedom for the chi-square goodness-of-fit test is $k - 1$. What's up with that? The answer here is that what we're supposed to be looking at is the number of genuinely independent things that are getting added together. And, as I'll go on to talk about in the next section, even though there are k things that we're adding only $k - 1$ of them are truly independent, and so the degrees of freedom is actually only $k - 1$. That's the topic of the next section[^10-categorical-data-analysis-4].

[^10-categorical-data-analysis-4]: If you rewrite the equation for the goodness-of-fit statistic as a sum over k - 1 independent things you get the "proper" sampling distribution, which is chi-square with k - 1 degrees of freedom. It's beyond the scope of an introductory book to show the maths in that much detail. All I wanted to do is give you a sense of why the goodness-of-fit statistic is associated with the chi-square distribution. --->

### 自由度

```{r}
#| label: fig-fig14-1
#| fig-cap: 不同“自由度”的卡方分佈
#$\chi^2$ (chi-square) distributions with different values for the 'degrees of freedom'

ggplot(data = data.frame(x = seq(0, 12, 1)), aes(x)) +
  stat_function(aes(linetype = 'df = 3'), fun = dchisq, n = 1000, args = list(df = 3), col=blueshade, linewidth=1) +
  stat_function(aes(linetype = 'df = 4'), fun = dchisq, n = 1000, args = list(df = 4), col=blueshade, linewidth=1) +
  stat_function(aes(linetype = 'df = 5'), fun = dchisq, n = 1000, args = list(df = 5), col=blueshade, linewidth=1) +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  ylab("Probability Density\n") +
  xlab("\nObserved Value") +
  scale_linetype_manual(values = c('solid', 'dashed', 'dotted'), labels = c( "df = 3", "df = 4", "df = 5")) +
  theme_classic() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.title = element_blank(),
    legend.text = element_text(size=12),
    legend.key.width= unit(1, 'cm'),
    legend.position = c(0.6, 0.8))

```

在 @sec-Other-useful-distributions 我初次向讀者介紹卡方分佈，只有稍微講一下什麼是**自由度**，強調這是很重要的概念。請看一下 @fig-fig14-1 不同的卡方分佈，可知只要自由度改變了，那麼形狀劇烈改變。那麼**自由度**究竟是什麼呢？其實在解釋自由度與常態態分佈的關係時，就提到了一種解釋：自由度是可平方並可加成的*常態分佈變項數量*。當然，這樣的解釋相當抽象，對多數讀者沒有幫助。接著讓我們透過真實的資料，來嘗試理解自由度吧。

自由度的基本概念其實相當簡單，計算方法是將先算出資料的“總數”，然後減去“不可變動”的資料數目。[^10-categorical-data-analysis-5]這樣說有點籠統，所以這裡用想像實驗的撲克牌資料再解釋一下。首先有$O1, O2, O3, O4$四個分別代表選出一種花色（紅心，梅花，方塊，黑桃）觀察次數的數字。這四個數字是經過實驗得到的隨機結果，其實還有一個不可變動的樣本量 $N$。[^10-categorical-data-analysis-6]也就是說，如果我們知道有多少人選擇紅心，多少人選擇方塊，多少人選擇梅花，那麼就能確實知道還有多少人選擇黑桃。換句話說，雖然總體資料量是四個，實際上只有 $4 - 1 = 3$ 個自由度。另一種角度的思理解方式是，我們想要估算四種類別的機率，但是全部類別的機率加起來必須等於一，那麼其中一個類別的機率不可變動。因此自由度是 $4 - 1 = 3$。無論您想用觀察次數的角度，還是機率的角度理解自由度，答案都是一樣的。通常要執行 $k$ 組的卡方適合度檢定時，自由度都是 $k - 1$。


[^10-categorical-data-analysis-5]: 我必須強調，這是一種過度簡化的解釋。雖然這種解釋適用多數情況，但是偶爾會遇到自由度並非整數的狀況。為了不要讓讀者過於困擾，遇到這樣的狀況時，只要告訴自己“自由度”其實是一種有點混亂的數學概念，初學的讀者通常只會知道簡單的解釋而非完整的解釋。以入門課程來說，簡單的解釋通常是最好的學習材料，但讀者需要知道總有一天這樣的解釋無法幫助你更進一步的學習，如果沒有先做好心理準備，有天看到了 $df = 3.4$ 或類似的狀況就會感到困惑，以為你在這裡學到的是錯誤的觀念，其實是有些複的細節先不做交待。

[^10-categorical-data-analysis-6]: 其實樣本量並不是永遠固定的數值。例如，實驗可能是在固定時段裡執行，參與人數取決於實際會有多少人參加。這些因素對於目前的學習目標並不重要。

<!--- When I introduced the chi-square distribution in @sec-Other-useful-distributions, I was a bit vague about what **"degrees of freedom"** actually means. Obviously, it matters. Looking at @fig-fig14-1, you can see that if we change the degrees of freedom then the chi-square distribution changes shape quite substantially. But what exactly is it? Again, when I introduced the distribution and explained its relationship to the normal distribution, I did offer an answer: it's the number of "normally distributed variables" that I'm squaring and adding together. But, for most people, that's kind of abstract and not entirely helpful. What we really need to do is try to understand degrees of freedom in terms of our data. So here goes.

The basic idea behind degrees of freedom is quite simple. You calculate it by counting up the number of distinct "quantities" that are used to describe your data and then subtracting off all of the "constraints" that those data must satisfy.[^10-categorical-data-analysis-5] This is a bit vague, so let's use our cards data as a concrete example. We describe our data using four numbers, $O1, O2, O3$ and O4 corresponding to the observed frequencies of the four different categories (hearts, clubs, diamonds, spades). These four numbers are the random outcomes of our experiment. But my experiment actually has a fixed constraint built into it: the sample size $N$. [^10-categorical-data-analysis-6] That is, if we know

[^10-categorical-data-analysis-5]: I feel obliged to point out that this is an over-simplification. It works nicely for quite a few situations, but every now and then we'll come across degrees of freedom values that aren't whole numbers. Don't let this worry you too much; when you come across this just remind yourself that "degrees of freedom" is actually a bit of a messy concept, and that the nice simple story that I'm telling you here isn't the whole story. For an introductory class it's usually best to stick to the simple story, but I figure it's best to warn you to expect this simple story to fall apart. If I didn't give you this warning you might start getting confused when you see $df = 3.4$ or something, (incorrectly) thinking that you had misunderstood something that I've taught you rather than (correctly) realising that there's something that I haven't told you.

[^10-categorical-data-analysis-6]: In practice, the sample size isn't always fixed. For example, we might run the experiment over a fixed period of time and the number of people participating depends on how many people show up. That doesn't matter for the current purposes

how many people chose hearts, how many chose diamonds and how many chose clubs, then we'd be able to figure out exactly how many chose spades. In other words, although our data are described using four numbers, they only actually correspond to $4 - 1 = 3$ degrees of freedom. A slightly different way of thinking about it is to notice that there are four probabilities that we're interested in (again, corresponding to the four different categories), but these probabilities must sum to one, which imposes a constraint. Therefore the degrees of freedom is $4 - 1 = 3$. Regardless of whether you want to think about it in terms of the observed frequencies or in terms of the probabilities, the answer is the same. In general, when running the $\chi^2$(chi-square) goodness-of-fit test for an experiment involving $k$ groups, then the degrees of freedom will be $k - 1$. --->

### 檢定虛無假設

一個完整的假設檢定程序，最後要規劃的是棄卻域，也就是決定那些 $\chi^2$ 值能拒絕虛無假設。我們已經知道越大的$\chi^2$數值代表虛無假設的預測與真正的實驗結果差異越大，$\chi^2$數值越小則代表虛無假設的預測越接近真正的實驗結果。所以明智的策略要設定一個臨界值，如果 $\chi^2$ 數值大於臨界值，就拒絕虛無假設；如果 $\chi^2$ 小於臨界值，就保留虛無假設。用 @sec-Hypothesis-testing 介紹過的詞彙，卡方適合度檢定是一種**單側檢定**。好了，現在要做的就是訂下臨界值是多少。方法很簡單，如果研究人員願意容忍犯下型一錯誤的機率最多$5%$，可以顯著水準$\alpha = .05$決定臨界值，如此一來，在虛無假設預測為真的前提下，能得到與臨界值一樣大的 $\chi^2$ 數值之機率應該只有$5%$。如同 @fig-fig14-2 的圖解。

<!--- The final step in the process of constructing our hypothesis test is to figure out what the rejection region is. That is, what values of $\chi^2$ would lead us to reject the null hypothesis. As we saw earlier, large values of $\chi^2$ imply that the null hypothesis has done a poor job of predicting the data from our experiment, whereas small values of $\chi^2$ imply that it's actually done pretty well. Therefore, a pretty sensible strategy would be to say there is some critical value such that if $\chi^2$ is bigger than the critical value we reject the null, but if $\chi^2$ is smaller than this value we retain the null. In other words, to use the language we introduced in @sec-Hypothesis-testing the chi-square goodness-of-fit test is always a **one-sided test**. Right, so all we have to do is figure out what this critical value is. And it's pretty straightforward. If we want our test to have significance level of $\alpha = .05$ (that is, we are willing to tolerate a Type I error rate of $5%$), then we have to choose our critical value so that there is only a 5% chance that $\chi^2$ could get to be that big if the null hypothesis is true. This is illustrated in @fig-fig14-2. --->

```{r}
#| label: fig-fig14-2
#| fig-cap: $\chi^2$（卡方）適合度檢定的假設檢定如何運作的示意圖
#Illustration of how the hypothesis testing works for the $\chi^2$ (chi-square) goodness of-fit test


ggplot(data = data.frame(x = seq(0, 12, 0.01)), aes(x)) +
  stat_function(fun = dchisq, n = 1000, args = list(df = 3), linetype = "solid", col="black", linewidth=1) +
  stat_function(fun = dchisq, n = 1000, args = list(df = 3), geom = "area",
                fill = blueshade, alpha=0.8, xlim = c(7.82, 12)) +
  scale_x_continuous(breaks = seq(0, 12, 2)) +
  geom_text(x=6.5, y=.12, label="臨界值=7.82", size=5) +
  geom_segment(x = 6.5, y = .11, xend = 7.82, yend = .026,
               arrow = arrow(length = unit(0.03, "npc"))) +
  geom_text(x=10, y=.08, label="觀察資料算出\n的適合值 = 8.44", size=5) +
  geom_segment(x = 10, y = .06, xend = 8.52, yend = .006,
               arrow = arrow(length = unit(0.03, "npc"))) +
  annotate("point", x = 8.44, y = 0, colour = "black", size=3) +
  ylab("Probability Density\n") +
  xlab("\nObserved Value") +
  theme_classic() +
  theme(
    axis.title.y = element_blank(),
    axis.text.y = element_blank(),
    axis.line.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.title = element_blank(),
    legend.text = element_text(size=12),
    legend.key.width= unit(1, 'cm'),
    legend.position = c(0.6, 0.8))

```

上到這裡，有些同學會問，要如何找到自由度是 $k-1$ 的卡方分佈臨界值？如果是多年前的用實體教科書上統計課，都可以在書的附錄裡看到像 @fig-fig14-3 的表格，用來查找臨界值。在這個表中能找到自由度是3的卡方分佈臨界值，$\alpha = 0.05$對應的數值是 7.815。


<!-- Ah but, I hear you ask, how do I find the critical value of a chi-square distribution with $k-1$ degrees of freedom? Many many years ago when I first took a psychology statistics class we used to look up these critical values in a book of critical value tables, like the one in @fig-fig14-3. Looking at this Figure, we can see that the critical value for a $\chi^2$ distribution with 3 degrees of freedom, and p=0.05 is 7.815. --->

```{r}
#| label: fig-fig14-3
#| fig-cap: 查找卡方分佈臨界值的表格
#Table of critical values for the chi-square distribution
knitr::include_graphics("images/fig10-3.png")
```

所以，若是真正算出的 $\chi^2$ 統計值大於 7.815，就可以拒絕虛無假設（預測每一種花色被選擇的機率相等）。即然已經算出統計值是8.44，那麼可以拒絕虛無假設。至此我們已經走完“皮爾森卡方適合度檢定”的過程，真不錯。

<!--- So, if our calculated $\chi^2$ statistic is bigger than the critical value of $7.815$, then we can reject the null hypothesis (remember that the null hypothesis, $H_0$, is that all four suits are chosen with equal probability). Since we actually already calculated that before (i.e., $\chi^2$ = 8.44) we can reject the null hypothesis. And that's it, basically. You now know "Pearson's $\chi^2$ test for the goodness-of-fit". Lucky you. --->

### jamovi實作

毫不意外地，jamovi 提供了一個分析工具，可以幫你完成這些計算。讓我們使用 *Randomness.omv* 文件。在主要的“分析”工具欄中，選擇“頻率” - “單樣本比例檢驗” - “$N$ 個結果”。然後在出現的分析視窗中將要分析的變項（從選擇 1 開始）移到“變項”框中。此外，單擊“預期計數”復選框，以便將這些資料顯示在結果表中。完成所有這些操作後，你應該會在 jamovi 中看到分析結果，如 @fig-fig14-4。然後不出所料，jamovi 提供了與我們上面手動計算相同的預期計數和統計資料，$\chi^2$ 值為 $(8.44$，自由度為 $3$，$p=0.038$。注意，我們不再需要查找臨界 p 值閾值，因為 jamovi 給出了在 $3$ 自由度下計算得出的 $\chi^2$ 的實際 p 值。



<!--- Not surprisingly, jamovi provides an analysis that will do these calculations for you. Let's use the *Randomness.omv* file. From the main 'Analyses' toolbar select 'Frequencies' - 'One Sample Proportion Tests' - '$N$ Outcomes'. Then in the analysis window that appears move the variable you want to analyse (choice 1 across into the 'Variable' box. Also, click on the 'Expected counts' check box so that these are shown on the results table. When you have done all this, you should see the analysis results in jamovi as in @fig-fig14-4. No surprise then that jamovi provides the same expected counts and statistics that we calculated by hand above, with a $\chi^2$ value of $(8.44$ with $3$ d.f. and $p=0.038$. Note that we don't need to look up a critical p-value threshold value any more, as jamovi gives us the actual p-value of the calculated $\chi^2$ for $3$ d.f. --->

```{r}
#| label: fig-fig14-4
#| classes: .enlarge-image
#| fig-cap: jamovi 中的 $\chi^2$ 單樣本比例檢驗，表格顯示觀察到的頻率和比例以及期望的頻率和比例
#A $\chi^2$ One Sample Proportion Test in jamovi, with table showing both observed and expected frequencies and proportions
knitr::include_graphics("images/fig10-4.png")
```

### 另一種虛無假設


此時，你可能會想知道如果你想進行適合度檢驗，但你的虛無假設不是所有類別的機率都相等該怎麼辦。例如，假設有人提出了這樣的理論預測，即人們應該以 $60\%$ 的機率選擇紅色牌，以 $40\%$ 的機率選擇黑色牌（我不知道為什麼你會這樣預測），但沒有其他偏好。如果是這樣，虛無假設將期望選擇愛心的比例為 $30\%$，選擇方塊的比例為 $30\%$，選擇黑桃的比例為 $20\%$，選擇梅花的比例為 $20\%$。換句話說，我們期望愛心和方塊的出現次數是黑桃和梅花的 1.5 倍（$30\%$ : $20\%$ 的比例與 1.5 : 1 相同）。對我來說，這似乎是一個愚蠢的理論，但是用我們的 jamovi 分析可以很容易地測試這個明確指定的虛無假設。在分析視窗中（標記為“比例檢驗（N個結果）”的 @fig-fig14-4 中，你可以展開“預期比例”的選項。當你這樣做時，將會出現一些選項，讓你為選定的變項輸入不同的比例值，在我們的案例中，這個變項是 choice 1。將比例更改為反映新的虛無假設，如 @fig-fig14-5 所示，並觀察結果如何變化。

<!--- At this point you might be wondering what to do if you want to run a goodness-of-fit test but your null hypothesis is not that all categories are equally likely. For instance, let's suppose that someone had made the theoretical prediction that people should choose red cards $60\%$ of the time, and black cards $40\%$ of the time (I've no idea why you'd predict that), but had no other preferences. If that were the case, the null hypothesis would be to expect $30\%$ of the choices to be hearts, $30\%$ to be diamonds, $20\%$ to be spades and $20\%$ to be clubs. In other words we would expect hearts and diamonds to appear 1.5 times more often than spades and clubs (the ratio $30\%$ : $20\%$ is the same as 1.5 : 1). This seems like a silly theory to me, and it's pretty easy to test this explicitly specified null hypothesis with the data in our jamovi analysis. In the analysis window (labelled 'Proportion Test (N Outcomes)' in @fig-fig14-4 you can expand the options for 'Expected Proportions'. When you do this, there are options for entering different ratio values for the variable you have selected, in our case this is choice 1. Change the ratio to reflect the new null hypothesis, as in @fig-fig14-5, and see how the results change. --->

```{r}
#| label: fig-fig14-5
#| fig-cap: 在 jamovi 中更改 $\\chi^2$ 單樣本比例檢驗的預期比例
#Changing the expected proportions in the $\\chi^2$ One Sample Proportion Test in jamovi
knitr::include_graphics("images/fig10-5.png")
```

預期計數現在顯示在 @tbl-tab14-6 中。

<!--- The expected counts are now shown in @tbl-tab14-6. --->

```{r}
#| label: tbl-tab14-6
#| tbl-cap: 不同虛無假設的預期計數
#Expected counts for a different null hypothesis
huxtabs[[10]][[6]]
```

$\chi^2$ 統計量為 4.74，自由度為 3，$p = 0.182$。現在，我們更新的假設和預期頻率與上次的結果有所不同。因此，我們的 $\chi^2$ 檢驗統計量和 p 值也有所不同。令人惱火的是，p 值為 $.182$，因此我們不能拒絕虛無假設（回顧 @sec-The-p-value-of-a-test 提醒自己為什麼）。可悲的是，儘管虛無假設對應著一個非常愚蠢的理論，這些資料並沒有提供足夠的證據來反駁它。

<!--- and the $\chi^2$ statistic is 4.74, 3 d.f., $p = 0.182$. Now, the results of our updated hypotheses and the expected frequencies are different from what they were last time. As a consequence our $\chi^2$ test statistic is different, and our p-value is different too. Annoyingly, the p-value is $.182$, so we can't reject the null hypothesis (look back at @sec-The-p-value-of-a-test to remind yourself why). Sadly, despite the fact that the null hypothesis corresponds to a very silly theory, these data don't provide enough evidence against it. --->

### 適合度檢定的報告寫作 {#sec-How-to-report-the-results-of-a-test}

現在你知道了這個測試的運作方式，也知道如何使用神奇的jamovi計算盒來進行測試。接下來你需要知道的是如何撰寫結果。畢竟，設計和執行實驗，然後分析資料，如果不告訴別人結果是沒有意義的！所以讓我們來談談在報告分析時需要做的事情。讓我們繼續以撲克牌花色為例。如果我想將這個結果寫成一篇論文之類的東西，那麼慣常的報告方式是這樣寫的：

> 在實驗的200名參與者中，有64人首選紅心，51人選擇方塊，50人選擇黑桃，35人選擇梅花。進行了卡方適合度檢驗以測試四種花色的選擇機率是否相同。結果顯著（$\chi^2(3) = 8.44, p< .05$），這表明人們在選擇花色時並非完全隨機。

這相當直接，希望它看起來很不起眼。儘管如此，你應該注意到這個描述中的幾點內容：

- *描述統計資料在統計檢驗之前*。也就是說，在進行檢驗之前，我告訴讀者有關資料的一些信息。通常，這是一個很好的做法。永遠記住，你的讀者對你的資料了解得遠不如你。因此，除非你妥善地向他們描述，否則統計檢驗對他們來說毫無意義，他們會感到沮喪和哭泣。
- *描述告訴你正在測試的虛無假設是什麼*。老實說，作者並不總是這樣做，但在存在一定歧義的情況下，或者在你不能依賴你的讀者非常熟悉你正在使用的統計工具時，這通常是一個好主意。很多時候讀者可能不知道（或記不起）你正在使用的檢驗的所有細節，所以提醒他們是一種禮貌！對於適合度檢驗來說，你通常可以依賴科學觀眾了解它的運作方式（因為它涵蓋在大多數入門統計課程中）。然而，明確陳述虛無假設（簡要地！）仍然是一個好主意，因為虛無假設可能因你使用檢驗的目的而有所不同。例如，在撲克牌的例子中，我的虛無假設是四個花色的機率相同（即，$P1 = P2 = P3 = P4 = 0.25$），但這個假設並沒有什麼特別的。我可以同樣使用適合度檢驗測試虛無假設，即$P_1 = 0.7$和$P2 = P3 = P4 = 0.1$。所以，向讀者解釋你的虛無假設是有幫助的。另外，注意到我用文字而不是數學描述虛無假設。這是完全可以接受的。你可以用數學描述它，但是因為大多數讀者發現文字比符號更容易閱讀，所以大多數作者傾向於用文字描述虛無假設（如果可以的話）。
- *包括"統計塊"*。在報告檢驗結果本身時，我不僅僅說結果顯著，還包括了一個“統計塊”（即括號內密集的數學部分），其中報告了所有“關鍵”統計信息。對於卡方適應度檢驗，報告的信息包括檢驗統計量（即適應度統計量為8.44）、用於檢驗的分布信息（具有3個自由度的$\chi^2$，通常縮寫為$\chi^2$(3)），然後是結果是否顯著（在本例中為$p< .05$）。每個檢驗所需的統計塊中的特定信息各不相同，因此每次我介紹一個新檢驗時，我都會向您展示統計塊應該是什麼樣子。[^10-categorical-data-analysis-7] 但是，一般原則是您應該始終提供足夠的信息，以便讀者在需要時可以自己檢查測試結果。

- *對結果進行解釋*。除了指出結果顯著之外，我還提供了結果的解釋（即，人們沒有隨機選擇）。這對讀者也是一種友善，因為它告訴他們關於資料中發生了什麼事的一些信息。如果不包括這樣的東西，讀者很難理解發生了什麼事。[^10-categorical-data-analysis-8]

[^10-categorical-data-analysis-7]: 嗯，有點。如何報告統計資料的慣例在不同的學科之間有所不同。我傾向於堅持心理學的做法，因為這就是我的工作。但是，我認為向讀者提供足夠的信息，以便他們可以檢查你的結果的一般原則是相當普遍的。

[^10-categorical-data-analysis-8]: 對有些人來說，這個建議可能聽起來有點奇怪，或者至少與編寫技術報告的“通常”建議相衝突。通常，學生們被告知報告的“結果”部分是用來描述資料並報告統計分析，而“討論”部分是提供解釋的。這在某種程度上是對的，但我認為人們經常過於照字面解釋它。我通常的做法是在結果部分提供對資料的快速而簡單的解釋，以便我的讀者理解資料告訴我們什麼。然後，在討論中，我嘗試講述一個關於我的結果如何與其他科學文獻相適應的更大的故事。簡而言之，不要讓“解釋屬於討論”這個建議使你的結果部分變得難以理解。讓讀者明白你的意思更重要。

正如其他所有事物一樣，你應該首要關注的是向讀者解釋事物。永遠記住，報告結果的目的是與另一個人溝通。我無法告訴您我看過多少次報告、論文甚至科學文章的結果部分就是胡言亂語，因為作者只關注確保包含所有數字，卻忘記了與人類讀者真正交流。

> *撒旦在統計和引用經文中同樣感到高興*[^10-categorical-data-analysis-9]
> -- H.G. 威爾斯

[^10-categorical-data-analysis-9]: 如果你一直在仔細閱讀，並且像我一樣是一個數學苛刻的人，那麼上一節中我寫卡方檢驗的方式可能會讓你有點困擾。你可能會想，用 "$\chi^2(3) = 8.44$" 這樣寫感覺有點不對勁。畢竟，是適合度統計數等於8.44，所以我不應該寫 $X^2 = 8.44$ 或者 $GOF = 8.44$ 嗎？這似乎混淆了抽樣分布（即$\chi^2$與df=3）與檢驗統計量（即$X^2$）。可能性是你認為這是錯字，因為$\chi$和X看起來非常相似。奇怪的是，這並非如此。寫 $\chi^2$(3)= 8.44 本質上是一種高度縮略的寫法，表示“檢驗統計量的抽樣分布為$\chi^2$(3)，檢驗統計量的值為8.44”。從某種意義上說，這有點愚蠢。有很多不同的檢驗統計量最終具有卡方抽樣分布。我們用於適合度檢驗的$X^2$統計量只是眾多統計量中的一種（儘管是最常見的一種）。在一個明智、完全組織的世界裡，我們總是為檢驗統計量和抽樣分布分配一個單獨的名字。這樣，統計塊本身就可以告訴你研究者究竟計算了什麼。有時這確實是可能的。例如，Pearson適合度檢驗中使用的檢驗統計量寫作$X^2$，但還有一種與G-test$^a$ [@Sokal1994]關係密切的檢驗稱為G-test，其中檢驗統計量寫作$G$。事實上，Pearson適合度檢驗和G-test都檢驗相同的虛無假設，抽樣分布完全相同（即，具有$k-1$自由度的卡方分布）。如果我對卡片資料進行G-test而不是適合度檢驗，那麼我最終會得到一個檢驗統計量$G = 8.65$，它略有不同於我之前得到的$X^2 = 8.44$值，並產生稍微小一點的p值$p = .034$。假設約定俗成地報告檢驗統計量，然後是抽樣分布，然後是p值。如果是這樣的話，這兩種情況將產生不同的統計塊：我原來的結果將寫成$X^2 = 8.44$，$\chi^2(3)$，$p = .038$，而使用G-test的新版本將寫成$G = 8.65$，$\chi^2(3)$，$p = .034$。然而，使用縮略報告標準，原始結果寫作$\chi^2(3) = 8.44, p =.038$，新結果寫作$\chi^2(3) = 8.65, p = .034$，所以實際上不清楚我實際上運行了哪個檢驗。那麼為什麼我們不生活在一個統計塊的內容能唯一確定哪些檢驗被執行的世界呢？根本原因是生活是混亂的。我們（作為統計工具的用戶）希望它整齊、有序並且有組織。我們希望它像產品一樣被設計，但這不是生活的運作方式。統計是一門與其他任何學科一樣的智力學科，因此它是一個大規模的分布式、部分協作和部分競爭的項目，沒有人真正完全了解它。你和我作為資料分析工具的東西並非由統計之神的行為創造的。它們是由很多不同的人發明的，作為學術期刊中的論文發表，由很多其他人實施、糾正和修改，然後由其他人在教科書中向學生解釋。因此，有很多檢驗統計量甚至沒有名字，因此它們只是被賦予與相應抽樣分布相同的名字。稍後我們將看到，任何遵循$\chi^2$分布的檢驗統計量通常被稱為“卡方統計量”，遵循$t$分布的任何統計量被稱為“t統計量”，等等。但是，正如$\chi^2$與$G$的例子所說明的，具有相同抽樣分布的兩個不同事物仍然是不同的。因此，有時候明確表明你實際上運行的檢驗是很好的主意，特別是如果你在做一些不尋常的事情。如果你只說“卡方檢驗”，實際上並不清楚你在談論哪種檢驗。儘管如此，由於兩個最常見的卡方檢驗是適合度檢驗和獨立性檢驗，大多數具有統計培訓的讀者可能都能猜到。儘管如此，這仍然是需要注意的事情。—$^a$ 讓事情變得複雜的是，G-test是一整類被稱為似然比檢驗的檢驗的特例。我在這本書中沒有介紹LRT，但它們確實是值得了解的好東西。

<!--- So now you know how the test works, and you know how to do the test using a wonderful jamovi flavoured magic computing box. The next thing you need to know is how to write up the results. After all, there's no point in designing and running an experiment and then analysing the data if you don't tell anyone about it! So let's now talk about what you need to do when reporting your analysis. Let's stick with our card-suits example. If I wanted to write this result up for a paper or something, then the conventional way to report this would be to write something like this:

> Of the 200 participants in the experiment, 64 selected hearts for their first choice, 51 selected diamonds, 50 selected spades, and 35 selected clubs. A chi-square goodness-of-fit test was conducted to test whether the choice probabilities were identical for all four suits. The results were significant ($\chi^2(3) = 8.44, p< .05)$, suggesting that people did not select suits purely at random.

This is pretty straightforward and hopefully it seems pretty unremarkable. That said, there's a few things that you should note about this description:

- *The statistical test is preceded by the descriptive statistics*. That is, I told the reader something about what the data look like before going on to do the test. In general, this is good practice. Always remember that your reader doesn't know your data anywhere near as well as you do. So, unless you describe it to them properly, the statistical tests won't make any sense to them and they'll get frustrated and cry.
- *The description tells you what the null hypothesis being tested is*. To be honest, writers don't always do this but it's often a good idea in those situations where some ambiguity exists, or when you can't rely on your readership being intimately familiar with the statistical tools that you're using. Quite often the reader might not know (or remember) all the details of the test that your using, so it's a kind of politeness to "remind" them! As far as the goodness-of-fit test goes, you can usually rely on a scientific audience knowing how it works (since it's covered in most intro stats classes). However, it's still a good idea to be explicit about stating the null hypothesis (briefly!) because the null hypothesis can be different depending on what you're using the test for. For instance, in the cards example my null hypothesis was that all the four suit probabilities were identical (i.e., $P1 = P2 = P3 = P4 = 0.25$), but there's nothing special about that hypothesis. I could just as easily have tested the null hypothesis that $P_1 = 0.7$ and $P2 = P3 = P4 = 0.1$ using a goodness-of-fit test. So it's helpful to the reader if you explain to them what your null hypothesis was. Also, notice that I described the null hypothesis in words, not in maths. That's perfectly acceptable. You can describe it in maths if you like, but since most readers find words easier to read than symbols, most writers tend to describe the null using words if they can.
- *A "stat block" is included*. When reporting the results of the test itself, I didn't just say that the result was significant, I included a "stat block" (i.e., the dense mathematical looking part in the parentheses) which reports all the "key" statistical information. For the chi-square goodness-of-fit test, the information that gets reported is the test statistic (that the goodness-of-fit statistic was 8.44), the information about the distribution used in the test ($\chi^2$ with 3 degrees of freedom which is usually shortened to $\chi^2$(3)), and then the information about whether the result was significant (in this case $p< .05$). The particular information that needs to go into the stat block is different for every test, and so each time I introduce a new test I'll show you what the stat block should look like.[^10-categorical-data-analysis-7] However the general principle is that you should always provide enough information so that the reader could check the test results themselves if they really wanted to.
- *The results are interpreted*. In addition to indicating that the result was significant, I provided an interpretation of the result (i.e., that people didn't choose randomly). This is also a kindness to the reader, because it tells them something about what they should believe about what's going on in your data. If you don't include something like this, it's really hard for your reader to understand what's going on.[^10-categorical-data-analysis-8]

[^10-categorical-data-analysis-7]: Well, sort of. The conventions for how statistics should be reported tend to differ somewhat from discipline to discipline. I've tended to stick with how things are done in psychology, since that's what I do. But the general principle of providing enough information to the reader to allow them to check your results is pretty universal, I think.

[^10-categorical-data-analysis-8]: To some people, this advice might sound odd, or at least in conflict with the "usual" advice on how to write a technical report. Very typically, students are told that the "results" section of a report is for describing the data and reporting statistical analysis, and the "discussion" section is for providing interpretation. That's true as far as it goes, but I think people often interpret it way too literally. The way I usually approach it is to provide a quick and simple interpretation of the data in the results section, so that my reader understands what the data are telling us. Then, in the discussion, I try to tell a bigger story about how my results fit with the rest of the scientific literature. In short, don't let the "interpretation goes in the discussion" advice turn your results section into incomprehensible garbage. Being understood by your reader is much more important.

As with everything else, your overriding concern should be that you explain things to your reader. Always remember that the point of reporting your results is to communicate to another human being. I cannot tell you just how many times I've seen the results section of a report or a thesis or even a scientific article that is just gibberish, because the writer has focused solely on making sure they've included all the numbers and forgotten to actually communicate with the human reader.

> *Satan delights equally in statistics and in quoting scripture*[^10-categorical-data-analysis-9]
> -- H.G. Wells

[^10-categorical-data-analysis-9]: If you've been reading very closely, and are as much of a mathematical pedant as I am, there is one thing about the way I wrote up the chi-square test in the last section that might be bugging you a little bit. There's something that feels a bit wrong with writing "$\chi^2(3) = 8.44$", you might be thinking. After all, it's the goodness-of-fit statistic that is equal to 8.44, so shouldn't I have written $X^2 = 8.44$ or maybe $GOF = 8.44$? This seems to be conflating the sampling distribution (i.e., $\chi^2$ with df = 3) with the test statistic (i.e., $X^2$). Odds are you figured it was a typo, since $\chi$ and X look pretty similar. Oddly, it's not. Writing $\chi^2$(3)= 8.44 is essentially a highly condensed way of writing "the sampling distribution of the test statistic is $\chi^2$(3), and the value of the test statistic is 8.44" In one sense, this is kind of stupid. There are lots of different test statistics out there that turn out to have a chi-square sampling distribution. The $X^2$ statistic that we've used for our goodness-of-fit test is only one of many (albeit one of the most commonly encountered ones). In a sensible, perfectly organised world we'd always have a separate name for the test statistic and the sampling distribution. That way, the stat block itself would tell you exactly what it was that the researcher had calculated. Sometimes this happens. For instance, the test statistic used in the Pearson goodness-of-fit test is written $X^2$ , but there's a closely related test known as the G-test$^a$ [@Sokal1994], in which the test statistic is written as $G$. As it happens, the Pearson goodness-of-fit test and the G-test both test the same null hypothesis, and the sampling distribution is exactly the same (i.e., chi-square with $k - 1$ degrees of freedom). If I'd done a G-test for the cards data rather than a goodness-of-fit test, then I'd have ended up with a test statistic of $G = 8.65$, which is slightly different from the $X^2 = 8.44$ value that I got earlier and which produces a slightly smaller p-value of $p = .034$. Suppose that the convention was to report the test statistic, then the sampling distribution, and then the p-value. If that were true, then these two situations would produce different stat blocks: my original result would be written $X^2 = 8.44$, $\chi^2(3)$, $p = .038$, whereas the new version using the G-test would be written as $G = 8.65$, $\chi^2(3)$, $p = .034$. However, using the condensed reporting standard, the original result is written $\chi^2(3) = 8.44, p =.038$, and the new one is written $\chi^2(3) = 8.65, p = .034$, and so it's actually unclear which test I actually ran. So why don't we live in a world in which the contents of the stat block uniquely specifies what tests were ran? The deep reason is that life is messy. We (as users of statistical tools) want it to be nice and neat and organised. We want it to be designed, as if it were a product, but that's not how life works. Statistics is an intellectual discipline just as much as any other one, and as such it's a massively distributed, partly-collaborative and partly-competitive project that no-one really understands completely. The things that you and I use as data analysis tools weren't created by an Act of the Gods of Statistics. They were invented by lots of different people, published as papers in academic journals, implemented, corrected and modified by lots of other people, and then explained to students in textbooks by someone else. As a consequence, there's a lot of test statistics that don't even have names, and as a consequence they're just given the same name as the corresponding sampling distribution. As we'll see later, any test statistic that follows a $\chi^2$ distribution is commonly called a "chi-square statistic", anything that follows a $t$-distribution is called a "t-statistic", and so on. But, as the $\chi^2$ versus $G$ example illustrates, two different things with the same sampling distribution are still, well, different. As a consequence, it's sometimes a good idea to be clear about what the actual test was that you ran, especially if you're doing something unusual. If you just say "chi-square test" it's not actually clear what test you're talking about. Although, since the two most common chi-square tests are the goodness-of-fit test and the independence test, most readers with stats training can probably guess. Nevertheless, it's something to be aware of. -- $^a$ Complicating matters, the G-test is a special case of a whole class of tests that are known as likelihood ratio tests. I don't cover LRTs in this book, but they are quite handy things to know about. --->

## 卡方獨立性檢定 {#sec-chi2-independence}

> 守衛機器人 1：停！
> 守衛機器人 2：你們是機器人還是人類？
> 莉娜：我們是...機器人。
> 弗萊：呃，對！就是在機器人世界裡像機器人一樣生活的兩個機器人！呃？\
> 守衛機器人 1：測試！
> 守衛機器人 2：你最喜歡哪一個東西？A：一隻小狗，B：來自心上人的漂亮鮮花，還是C：以正確格式儲存的大量資料檔案？
> 守衛機器人 1：選擇！
> 《飛出個未來》第一季第5集"Fear of a Bot Planet"台詞(1999~2003於美國福斯電視網播映的喜劇動畫片；台灣無代理商引進播映)

某天我看了動畫片《飛出個未來》某一集，描述名叫*Chapek 9*的外星球原住民古怪風俗，能進入星球首都的訪客必須是機器器，絕不能是人類。守衛為了確認訪客是不是人類，會詢問訪客是喜歡小狗、鮮花還是格式正確的大量資料檔案。我心想：“這是相當聰明的問題，但是如果人類和機器人有相同的喜好呢？那就不是一個好的測試問題了吧？”其實，我偶然間取得了*Chapek 9*首都市政府用來檢查這個問題有沒有效的測試資料。他們做的測試非常簡單，就是找來一群機器人和一群人類，問他們喜歡什麼。所有資料都儲存在*chapek9.omv*這個檔案裡，有安裝本書資料庫模組的話，可以直接從jamovi資料庫匯入這個檔案(Chapek 9)。除了識別參與者的變項`ID`，還有兩個名義尺度變項，`species`和`choice`，這份資料檔案一共有180筆參與者的反應。全部資料包括93個人類和87個機器人，絕大多數參與者選擇了資料檔案。只要從'Exploration' - 'Descriptives'開啟描述統計設定視窗，建立次數表(frequency table)就能確認。不過，只有描述統計還無法檢測這樣的問題能不能有效區別機器人和人類，我們需要對資料進行更詳細的描述。我們要按照*種族*區分的各種選擇的次數，也就是建立這份資料的列聯表（cross tabulation，見 @sec-Tabulating-and-cross-tabulating-data  的介紹）。啟動'Frequencies' - 'Contingency Tables' - 'Independent Samples'的設定視窗就能建立列聯表，成果會如同 @tbl-tab14-7 。 

<!---
> GUARDBOT 1: Halt! \
> GUARDBOT 2: Be you robot or human? \
> LEELA: Robot...we be. \
> FRY: Uh, yup! Just two robots out roboting it up! Eh? \
> GUARDBOT 1: Administer the test. \
> GUARDBOT 2: Which of the following would you most prefer? A: A puppy, B: A pretty flower from your sweetie, or C: A large properly-formatted data file? \
> GUARDBOT 1: Choose! \
> *Futurama, "Fear of a Bot Planet"*

The other day I was watching an animated documentary examining the quaint customs of the natives of the planet *Chapek 9*. Apparently, in order to gain access to their capital city a visitor must prove that they're a robot, not a human. In order to determine whether or not a visitor is human, the natives ask whether the visitor prefers puppies, flowers, or large, properly formatted data files. "Pretty clever," I thought to myself "but what if humans and robots have the same preferences? That probably wouldn't be a very good test then, would it?" As it happens, I got my hands on the testing data that the civil authorities of *Chapek 9* used to check this. It turns out that what they did was very simple. They found a bunch of robots and a bunch of humans and asked them what they preferred. I saved their data in a file called *chapek9.omv*, which we can now load into jamovi. As well as the ID variable that identifies individual people, there are two nominal text variables, species and choice. In total there are 180 entries in the data set, one for each person (counting both robots and humans as "people") who was asked to make a choice. Specifically, there are 93 humans and 87 robots, and overwhelmingly the preferred choice is the data file. You can check this yourself by asking jamovi for Frequency Tables, under the 'Exploration' - 'Descriptives' button. However, this summary does not address the question we're interested in. To do that, we need a more detailed description of the data. What we want to do is look at the choices broken down *by species*. That is, we need to cross-tabulate the data (see @sec-Tabulating-and-cross-tabulating-data). In jamovi we do this using the 'Frequencies' - 'Contingency Tables' - 'Independent Samples' analysis, and we should get a table something like @tbl-tab14-7. --->

```{r}
#| label: tbl-tab14-7
#| tbl-cap: Chapek 9資料列聯表
#Cross-tabulating the data
huxtabs[[10]][[7]]
```

這份列聯表清楚展現絕大多數人類參與者回答喜歡資料檔案，而機器人參與者對每一項的回答相對平衡。先不管為什麼人類可能更喜歡資料檔案（這確實看起來有點奇怪，承認吧），我們現在的目標是確定這份資料的人類和機器人的選擇差異有沒有統計顯著性。


<!--- From this, it's quite clear that the vast majority of the humans chose the data file, whereas the robots tended to be a lot more even in their preferences. Leaving aside the question of why the humans might be more likely to choose the data file for the moment (which does seem quite odd, admittedly), our first order of business is to determine if the discrepancy between human choices and robot choices in the data set is statistically significant. --->

### 獨立性假設檢定程序  {#sec-Constructing-independence-test}

要如何分析這樣的資料？由於一開始的研究假設是"人類和機器人回答問題的方式並不一樣"，假設檢定程序要測試的虛無假設應該設定為"人類和機器人回答問題的方式是一樣的"？與適合度檢定一樣，首先要定義一些描述資料的符號（@tbl-tab14-8）。

<!--- How do we analyse this data? Specifically, since my research hypothesis is that "humans and robots answer the question in different ways", how can I construct a test of the null hypothesis that "humans and robots answer the question the same way"? As before, we begin by establishing some notation to describe the data (@tbl-tab14-8). --->

```{r}
#| label: tbl-tab14-8
#| tbl-cap: 獨立性假設檢定範例各項資料的符號表
#Notation to describe the data
huxtabs[[10]][[8]]
```

根據符號表，每個 $O_{ij}$ 代表其中一個種族的受測者`j`（機器人或人類）所做的選擇 `i`（小狗，鮮花或資料）之總計次數（觀察次數）。總計次數通常用$N$表示。然後，$R_i$ 表示各項選擇的總人數，像是$R_1$ 代表選擇鮮花的受測者人數，$C_j$ 表示各種族受測者人數，像是$C_1$ 代表機器人的總數。[^10-categorical-data-analysis-10]


[^10-categorical-data-analysis-10]: 技術說明。這個範例資料所設定的各列總次數是固定的（研究者手上只有87個機器人和93個人類的資料），各行總次數則是隨機的（碰巧有28個受測者選擇了小狗）。根據原作者主要引用的統計教科書所使用的專有名詞[@Hogg2005]，應該將適用這類問題的假設檢定方法稱為「卡方同質性檢定」(a chi-square test of homogeneity)，「卡方獨立性檢定」應該是指每行及每列的總次數都是隨機結果的研究問題。這本書之前的版本裡，都是區分這兩種檢定這樣做的。然而事實上，這兩種檢定的程序是相同的，所以這裡將兩種檢定方法視為一種。

接著來想想虛無假設的設定。如果機器人和人類對這個問題的回答是一樣的，也就是“機器人選小狗”的機率與“人類選小狗”的機率相同，其他兩個選項的機率也是如此。所以用符號$P_{ij}$ 表示種族`j`的受測者回答選項`i`的機率，因此虛無假設就是：

$$
\begin{aligned}
H_0 &: \text{實驗結果符合以下三項：} \\
&P_{11} = P_{12}\text{ （選擇“小狗”的機率相同），} \\
&P_{21} = P_{22}\text{ （選擇“鮮花”的機率相同），還有} \\
&P_{31} = P_{32}\text{ （選擇“資料”的機率相同）}
\end{aligned}
$$

其實，因為虛無假設所設定的真實機率不必限定受測者的種族，符號可以再簡化用$P_i$代表做某個選擇的機率，例如$P_1$代表選擇小狗的真實機率。

接下來的程序就和適合度檢定一樣，就是計算期望次數。對應每個觀察次數$O_{ij}$，需要先搞清楚虛無假設預測每個觀察次數是多少，因此用$E_{ij}$ 表示每個期望次數。這個問題的狀況有點棘手，如果種族 $j$ 有 $C_j$ 人，無論是那個種族的受測者做出什麼選項$i$的真實機率是$P_i$，那麼期望次數就是：

$$E_{ij}=C_j \times P_i$$

到這一步還算順利，但是遇到了一個問題。與適合度檢定程序不同的是，這裡的虛無假設實際上並未指定$P_i$的數值。

用資料估計未知量數是必須的步驟(需要複習的話請回 @sec-Estimating-unknown-quantities-from-a-sample )！幸運的是，這不難做到。若是180位參與者裡有28 位選擇了鮮花，那麼選擇鮮花的機率很自然的估計值就是 $\frac{28}{180}$，大約是 $0.16$。若是用數學式呈現估計選擇`i`的機率，就是用每行總次數除以總樣本次數：

$$\hat{P}_{i}= \frac{R_i}{N}$$

因此，期望次數可以改寫為各行次數與各列次數的乘積，再除以總觀察次數：[^10-categorical-data-analysis-11]


[^10-categorical-data-analysis-11]: 就嚴格的統計操作來說，$E_{ij}$ 是估計值，所以應該寫成 $\hat{E_{ij}}$。但很少有統計教科書這樣寫，這裡也採用相同的寫法。 $$\hat{E}_{ij}= \frac{R_i \times C_j}{N}$$

<!--- 現在我們已經了解了如何從虛無假設中計算期望頻率，接下來的步驟與適合度檢驗相似。首先，對於每個 $O_{ij}$ 和 $E_{ij}$，我們計算 $(O_{ij} - E_{ij})^2 / E_{ij}$ 的值。然後，我們將這些值相加，得到一個名為卡方的統計量（$\chi^2$）。這個統計量也遵循卡方分布，因此我們可以將其用於計算 p 值，並進行檢驗。實際上，與適合度檢驗相似，卡方獨立性檢驗的自由度是由資料中的類別數量決定的，而不是預先知道的。

總之，我們可以通過將卡方獨立性檢驗應用於類別資料來檢驗虛無假設，即人類和機器人對問題的回答方式相同。這種方法允許我們計算期望頻率、卡方統計量以及對應的 p 值，從而幫助我們判斷是否拒絕虛無假設。--->

[額外的技術細節[^10-categorical-data-analysis-12]]

[^10-categorical-data-analysis-12]: 現在已經學會如何計算期望次數，根據從適合度檢定學到的策略，定義一個檢定統計值非常容易，其實兩種方法用的統計值幾乎是相同。以具備 r 行和 c 列的列聯表來說，計算 $chi^2$ 統計值的公式是 $$\chi^2=\sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(E_{ij}-O_{ij})^2}{E_{ij}}$$ 唯一的差異是獨立性檢定的統計值公式必須用兩個連加符號（$\sum$ ），表示分別要求行的總和及列的總和。

與前一個檢定程序一樣，$\chi^2$ 的數值越大，表示虛無假設對資料的解釋越差，而 $\chi^2$ 的數值越大，表示虛無假設對資料的解釋越好。所以如同前一種檢定程序，如果 $\chi^2$ 數值太大，就有可能拒絕虛無假設。

不出聰明的讀者所料，這個檢定統計值遵循 $\chi^2$ 分佈。現在要做的就是弄清楚有多少自由度，實際上這並不難知道。如同之前提到的，研究人員通常可以將自由度當成正在分析的資料點總數量，減去不可變動的資料點數量。具備 r 行和 c 列的列聯表總共有 $r^{c}$ 個觀察次數，所以這是觀察次數的全部數量。那不可變動的有多少呢？這裡的狀況稍微複雜一些，但是答案始終是相同的

$$df=(r-1)(c-1)$$

不過要解釋為什麼自由度是這樣算，需要考慮實驗設計。為了方便說明，假如真實資料真的有 87 台機器人和 93 位人類，不過因為選擇是隨機的，讓每行的總次數自由變化，就可以考慮這裡有多少不可變動的資料點。由於題目情境已經限制了每列的總次數，所以有c 個不可變動的資料點。其實還有更多不可變動的資料點，記得虛無假設提到了需要估計的參數$P_i$，本書雖然不會解釋為什麼這些參數是需要考慮的，現在讀者只要知道虛無假設所列出的參數都是不可變動，如此一來，問題就簡化成這種參數有多少呢？其實很簡單，因為所有機率必須加起來等於 1，所以只有 $r - 1$ 個。因此，卡方獨立性檢定的自由度是：


$$ \begin{split} df & = \text{(number of
observations) - (number of constraints)} \\\\ & = (r \times c) - (c +
(r - 1)) \\\\ & = rc - c - r + 1 \\\\ & = (r - 1)(c - 1) \end{split}$$

另一種解釋方式是，假如研究人員唯一確定的數值只有總樣本量 $N$。也就是說，研究人員對180位受測者進行問卷調查，結果發現 87 位是機器人，93 位是人類。現在的推論方式會不大相同，但仍然會得到相同的答案。虛無假設仍然有 $r - 1$ 個待估計的參數，對應每個回答項目被選擇的機率值，現在要加上$c - 1$ 個待估計的參數，對應受測者被確認是機器人的機率值。[^10-categorical-data-analysis-13]還有，我們確定觀察值的總次數 $N$，這是另一個不可變動的參數。所以一共有$rc$項資料點，其中有 $(c-1)+(r-1)+1$ 個不可變動的資料點。那正確答案是多少呢？


[^10-categorical-data-analysis-13]: 現實中很多人都會害怕的數學推導。$$\begin{split} df & = \text{(觀察值總次數) - (不可變動的資料點次數)} \\\\ & = (r \times c) -
((c-1) + (r - 1)+1) \\\\ & = (r- 1)(c - 1) \end{split}
$$

這真是太神奇了。


<!--- In this notation we say that $O_{ij}$ is a count (observed frequency) of the number of respondents that are of species j (robots or human) who gave answer i (puppy, flower or data) when asked to make a choice. The total number of observations is written $N$, as usual. Finally, I've used $R_i$ to denote the row totals (e.g., $R_1$ is the total number of people who chose the flower), and $C_j$ to denote the column totals (e.g., $C_1$ is the total number of robots).[^10-categorical-data-analysis-10]

[^10-categorical-data-analysis-10]: A technical note. The way I've described the test pretends that the column totals are fixed (i.e., the researcher intended to survey 87 robots and 93 humans) and the row totals are random (i.e., it just turned out that 28 people chose the puppy). To use the terminology from my mathematical statistics textbook [@Hogg2005] I should technically refer to this situation as a chi-square test of homogeneity and reserve the term chi-square test of independence for the situation where both the row and column totals are random outcomes of the experiment. In the initial drafts of this book that's exactly what I did. However, it turns out that these two tests are identical, and so I've collapsed them together.

So now let's think about what the null hypothesis says. If robots and humans are responding in the same way to the question, it means that the probability that "a robot says puppy" is the same as the probability that "a human says puppy", and so on for the other two possibilities. So, if we use $P_{ij}$ to denote "the probability that a member of species j gives response i" then our null hypothesis is that:

$$
\begin{aligned}
H_0 &: \text{All of the following are true:} \\
&P_{11} = P_{12}\text{ (same probability of saying “puppy”),} \\
&P_{21} = P_{22}\text{ (same probability of saying “flower”), and} \\
&P_{31} = P_{32}\text{ (same probability of saying “data”).}
\end{aligned}
$$

And actually, since the null hypothesis is claiming that the true choice probabilities don't depend on the species of the person making the choice, we can let Pi refer to this probability, e.g., P1 is the true probability of choosing the puppy.

Next, in much the same way that we did with the goodness-of-fit test, what we need to do is calculate the expected frequencies. That is, for each of the observed counts $O_{ij}$ , we need to figure out what the null hypothesis would tell us to expect. Let's denote this expected frequency by $E_{ij}$. This time, it's a little bit trickier. If there are a total of $C_j$ people that belong to species $j$, and the true probability of anyone (regardless of species) choosing option $i$ is $P_i$ , then the expected frequency is just:

$$E_{ij}=C_j \times P_i$$

Now, this is all very well and good, but we have a problem. Unlike the situation we had with the goodness-of-fit test, the null hypothesis doesn't actually specify a particular value for Pi .

It's something we have to estimate (see @sec-Estimating-unknown-quantities-from-a-sample) from the data! Fortunately, this is pretty easy to do. If 28 out of 180 people selected the flowers, then a natural estimate for the probability of choosing flowers is $\frac{28}{180}$, which is approximately $.16$. If we phrase this in mathematical terms, what we're saying is that our estimate for the probability of choosing option i is just the row total divided by the total sample size:

$$\hat{P}_{i}= \frac{R_i}{N}$$

Therefore, our expected frequency can be written as the product (i.e. multiplication) of the row total and the column total, divided by the total number of observations:[^10-categorical-data-analysis-11]

[^10-categorical-data-analysis-11]: Technically, $E_{ij}$ here is an estimate, so I should probably write it $\hat{E_{ij}}$ . But since no-one else does, I won't either.

$$\hat{E}_{ij}= \frac{R_i \times C_j}{N}$$

[Additional technical detail [^10-categorical-data-analysis-12]]

[^10-categorical-data-analysis-12]: Now that we've figured out how to calculate the expected frequencies, it's straightforward to define a test statistic, following the exact same strategy that we used in the goodness-of-fit test. In fact, it's pretty much the same statistic. For a contingency table with r rows and c columns, the equation that defines our $X^2$ statistic is $$X^2=\sum_{i=1}^{r}\sum_{j=1}^{c} \frac{(E_{ij}-O_{ij})^2}{E_{ij}}$$ The only difference is that I have to include two summation signs (i.e., $\sum$ ) to indicate that we're summing over both rows and columns.

As before, large values of $X^2$ indicate that the null hypothesis provides a poor description of the data, whereas small values of $X^2$ suggest that it does a good job of accounting for the data. Therefore, just like last time, we want to reject the null hypothesis if $X^2$ is too large.

Not surprisingly, this statistic is $\chi^2$ distributed. All we need to do is figure out how many degrees of freedom are involved, which actually isn't too hard. As I mentioned before, you can (usually) think of the degrees of freedom as being equal to the number of data points that you're analysing, minus the number of constraints. A contingency table with r rows and c columns contains a total of $r^{c}$ observed frequencies, so that's the total number of observations. What about the constraints? Here, it's slightly trickier. The answer is always the same

$$df=(r-1)(c-1)$$

but the explanation for why the degrees of freedom takes this value is different depending on the experimental design. For the sake of argument, let's suppose that we had honestly intended to survey exactly 87 robots and 93 humans (column totals fixed by the experimenter), but left the row totals free to vary (row totals are random variables). Let's think about the constraints that apply here. Well, since we deliberately fixed the column totals by Act of Experimenter, we have $c$ constraints right there. But, there's actually more to it than that. Remember how our null hypothesis had some free parameters (i.e., we had to estimate the Pi values)? Those matter too. I won't explain why in this book, but every free parameter in the null hypothesis is rather like an additional constraint. So, how many of those are there? Well, since these probabilities have to sum to 1, there's only $r - 1$ of these. So our total degrees of freedom is:

$$ \begin{split} df & = \text{(number of
observations) - (number of constraints)} \\\\ & = (r \times c) - (c +
(r - 1)) \\\\ & = rc - c - r + 1 \\\\ & = (r - 1)(c - 1) \end{split}$$

Alternatively, suppose that the only thing that the experimenter fixed was the total sample size N. That is, we quizzed the first 180 people that we saw and it just turned out that 87 were robots and 93 were humans. This time around our reasoning would be slightly different, but would still lead us to the same answer. Our null hypothesis still has $r - 1$ free parameters corresponding to the choice probabilities, but it now also has $c - 1$ free parameters corresponding to the species probabilities, because we'd also have to estimate the probability that a randomly sampled person turns out to be a robot.[^10-categorical-data-analysis-13] Finally, since we did actually fix the total number of observations N, that's one more constraint. So, now we have rc observations, and $(c-1)+(r-1)+1$ constraints. What does that give?

[^10-categorical-data-analysis-13]: A problem many of us worry about in real life.

$$\begin{split} df & = \text{(number of
observations) - (number of constraints)} \\\\ & = (r \times c) -
((c-1) + (r - 1)+1) \\\\ & = (r - 1)(c - 1) \end{split}
$$ Amazing.

--->

### 獨立性檢定實作  {#sec-doing-independence-test}

好吧，既然我們知道了檢驗是如何進行的，讓我們看看如何在 jamovi 中完成它。雖然讓您長時間地經歷繁瑣的計算以便被迫學習可能很有吸引力，但我認為這是沒有意義的。在上一節中，我已經向您展示了如何針對適合度檢驗進行長時間的操作，而且由於獨立性檢驗在概念上沒有任何不同，所以您不會通過長時間的操作學到任何新的東西。因此，我將直接向您展示簡單的方法。在 jamovi 中運行檢驗（“頻率” - “列聯表” - “獨立樣本”）之後，您只需查看 jamovi 結果窗口中列聯表下方，那裡就是 $\chi^2$ 統計量。這顯示了一個 $\chi^2$ 統計值為 10.72，2 d.f.，p-value = 0.005。

那很簡單，不是嗎？您還可以要求 jamovi 顯示預期計數 - 只需單擊“Cells”選項中的“Counts” - “Expected”複選框，預期計數將出現在列聯表中。同時，在此操作中，效果大小度量會有所幫助。我們將選擇 Cramér's $V$，您可以在“Statistics”選項中的複選框中指定它，它會給出 Cramér's $V$ 的值為 $0.24$。參見 @fig-fig14-6。我們稍後會再談論這個問題。


<!--- Okay, now that we know how the test works let's have a look at how it's done in jamovi. As tempting as it is to lead you through the tedious calculations so that you're forced to learn it the long way, I figure there's no point. I already showed you how to do it the long way for the goodness-of-fit test in the last section, and since the test of independence isn't conceptually any different, you won't learn anything new by doing it the long way. So instead I'll go straight to showing you the easy way. After you have run the test in jamovi ('Frequencies' - 'Contingency Tables' - 'Independent Samples'), all you have to do is look underneath the contingency table in the jamovi results window and there is the $\chi^2$ statistic for you. This shows a $\chi^2$ statistic value of 10.72, with 2 d.f. and p-value = 0.005.

That was easy, wasn't it! You can also ask jamovi to show you the expected counts - just click on the check box for 'Counts' - 'Expected' in the 'Cells' options and the expected counts will appear in the contingency table. And whilst you are doing that, an effect size measure would be helpful. We'll choose Cramér's $V$, and you can specify this from a check box in the 'Statistics' options, and it gives a value for Cramér's $V$ of $0.24$. See @fig-fig14-6. We will talk about this some more in just a moment. --->

```{r}
#| label: fig-fig14-6
#| classes: .enlarge-image
#| fig-cap: 在 jamovi 中使用 *Chapek 9* 資料進行獨立樣本 $\chi^2$ 檢驗
#Independent samples $\chi^2$ test in jamovi using the *Chapek 9* data
knitr::include_graphics("images/fig10-6.png")
```

這個輸出為我們提供了足夠的信息來寫出結果：

> *Pearson* 的 $\chi^2$ 顯示了物種和選擇之間存在顯著關聯（$\chi^2(2) = 10.7, p< .01$）。機器人似乎更傾向於說他們喜歡花，而人類更傾向於說他們喜歡資料。

注意，再次，我提供了一些解釋，以幫助人類讀者理解資料發生的情況。稍後在我的討論部分，我會提供更多的上下文。舉例來說，這是我可能會在之後說的：

> 人類似乎比機器人更喜歡原始資料文件，這有點反直覺。但在某種程度上，它是有道理的，因為 Chapek 9 上的民事當局往往在發現人類時會將其殺死並解剖。因此，最有可能的是，人類參與者並未如實回答問題，以避免可能產生不良後果。這應該被認為是一個嚴重的方法論缺陷。

我想，這可以被歸類為反應效應的一個極端例子。顯然，在這種情況下，問題嚴重到研究幾乎毫無價值，作為理解人類和機器人之間的差異偏好的工具。然而，我希望這能夠說明在獲得統計顯著結果（我們拒絕虛無假設，轉而接受替代假設）和找到具有科學價值的東西（由於嚴重的方法論缺陷，資料對我們研究假設的興趣一無所知）之間的區別。


<!--- This output gives us enough information to write up the result:

> *Pearson's* $\chi^2$ revealed a significant association between species and choice ($\chi^2(2) = 10.7, p< .01)$. Robots appeared to be more likely to say that they prefer flowers, but the humans were more likely to say they prefer data.

Notice that, once again, I provided a little bit of interpretation to help the human reader understand what's going on with the data. Later on in my discussion section I'd provide a bit more context. To illustrate the difference, here's what I'd probably say later on:

> The fact that humans appeared to have a stronger preference for raw data files than robots is somewhat counter-intuitive. However, in context it makes some sense, as the civil authority on Chapek 9 has an unfortunate tendency to kill and dissect humans when they are identified. As such it seems most likely that the human participants did not respond honestly to the question, so as to avoid potentially undesirable consequences. This should be considered to be a substantial methodological weakness.

This could be classified as a rather extreme example of a reactivity effect, I suppose. Obviously, in this case the problem is severe enough that the study is more or less worthless as a tool for understanding the difference preferences among humans and robots. However, I hope this illustrates the difference between getting a statistically significant result (our null hypothesis is rejected in favour of the alternative), and finding something of scientific value (the data tell us nothing of interest about our research hypothesis due to a big methodological flaw). --->

## 卡方檢定的校正

好了，是時候稍微離題一下下，其實談到這裡，我隱瞞了一點細節沒有交待。自由度只有 1 的時候，需要稍微改變計算方法。這被稱為 "連續性修正(continuity correction)"，或者**葉氏修正(Yates correction)**。記得稍早提到：$\chi^2$ 檢定的統計值是一種近似值，嚴格來說是假設當 $N$的值夠大時，二項分佈會逼近常態分佈。然這依賴這條假設的問題是，現實的研究問題很少遇到這種條件，尤其是自由度只有 1 的時候，像是用$2 \times 2$規模的列聯表進行獨立性檢定。無法保證這種資料的機率分佈逼近常態分佈，原因是處理的資料是分類資料！所以$\chi^2$統計值的真實樣本分佈其實是離散的，但是$\chi^2$ 分佈是連續的。如此會導致系統性問題：當$N$很小且 $df = 1$ 時，適合度統計值往往"過大"，需要比原來預設更大的$\alpha$，或者更小的p值，才能正確拒絕虛無假設。

根據描述**葉氏修正**的論文主張，這種修正其實是一種操作方法的破解。[^10-categorical-data-analysis-14]因為修正方法不是基於任何機率或統計理論，而是檢討多數分析人員的操作行為，經過測試發現經過校正後的統計值似乎比較正確。讀者可以在 jamovi 卡方檢定模組的操作視窗， 於'Statistics'這個部分勾選 $\chi^2$ continuity correction，就能輸出校正後的統計值。

[^10-categorical-data-analysis-14]: [Yates (1934)](https://www.jstor.org/stable/2983604) 提出了一個簡單的校正公式，將適合度統計值重新定義為：$$\chi^{2}=\sum_{i}\frac{(|E_i-O_i|-0.5)^2}{E_i}$$ 基本上，他將所有公式裡的數值都減了 0.5。


<!--- Okay, time for a little bit of a digression. I've been lying to you a little bit so far. There's a tiny change that you need to make to your calculations whenever you only have 1 degree of freedom. It's called the "continuity correction", or sometimes the **Yates correction**. Remember what I pointed out earlier: the $\chi^2$ test is based on an approximation, specifically on the assumption that the binomial distribution starts to look like a normal distribution for large $N$. One problem with this is that it often doesn't quite work, especially when you've only got 1 degree of freedom (e.g., when you're doing a test of independence on a $2 \times 2$ contingency table). The main reason for this is that the true sampling distribution for the $X^{2}$ statistic is actually discrete (because you're dealing with categorical data!) but the $\chi^2$ distribution is continuous. This can introduce systematic problems. Specifically, when N is small and when $df = 1$, the goodness-of-fit statistic tends to be "too big", meaning that you actually have a bigger α value than you think (or, equivalently, the p values are a bit too small).

As far as I can tell from reading Yates' paper[^10-categorical-data-analysis-14], the correction is basically a hack. It's not derived from any principled theory. Rather, it's based on an examination of the behaviour of the test, and observing that the corrected version seems to work better. You can specify this correction in jamovi from a check box in the 'Statistics' options, where it is called '$\chi^2$ continuity correction'.

[^10-categorical-data-analysis-14]: [Yates (1934)](https://www.jstor.org/stable/2983604) suggested a simple fix, in which you redefine the goodness-of-fit statistic as: $$\chi^{2}=\sum_{i}\frac{(|E_i-O_i|-0.5)^2}{E_i}$$ Basically, he just subtracts off 0.5 everywhere. --->

## 卡方檢定的效果量

稍早在 @sec-Effect-size-sample-size-and-power 提到，現在各種要求研究人員在報告中說明效果量測量指標的規範越來越普遍。若是現在已經完成了卡方檢定，結果顯示有統計顯著性，表示我們能肯定所探討的變項之間存在某種關聯（獨立性檢定）或特定條件之間存在某種差異（適合度檢定）。那麼要如何報告其中的效應量強度，也就是變項條件之間關聯性或差異程度？

有好幾種不同的測量指標可在報告裡呈現，而且有好幾種公式和程式能做計算。這裡不會介紹所有可用的測量指標，只重點介紹最多報告使用的效果量測量指標。最常見於研究報告，以及最多統計軟體預設計算的指標是 $\phi$，以及克拉默氏$V$(Cramér's V)。


[額外的技術細節[^10-categorical-data-analysis-15]]

[^10-categorical-data-analysis-15]: $\phi$ 與 Cramér's V的數學公式非常簡潔。要計算 $\phi$，只需將的 $\chi^2$統計值除以樣本大小，然後取平方根：$$\phi=\sqrt{\frac{\chi^2}{N}}$$ $\phi$的值域只在0（完全無關聯）和 1（完全關聯）之間，但是當列聯表規模大於 $2 \times 2$ ，就不適合用這個指標計算了。因為規模更大的列聯表，有可能會獲得大於1的數值，這是令人感到相當麻煩的。所以更多人喜歡使用 @Cramer1946  提出的$V$來測量列聯表統計檢定的效量。調整方法非常簡單，如果有一個 r 行和 c 列的列聯表，那麼定義 $k = min(r, c)$ 取兩個值中較小的那個，放到計算 $\phi$的公式裡就變成計算Cramér's V的公式$$V=\sqrt{\frac{\chi^2}{N(k-1)}}$$讀者很容易就能算出兩種效果量測量值。這兩種效果量指標相當受歡迎，可能是因為容易計算，又不會讓人覺得有虛應故事的感覺。用了Cramér's ，還能確認變項之間的相關程度確實在 0（完全無關聯）和 1（完全關聯）。

<!--- As we discussed earlier in @sec-Effect-size-sample-size-and-power, it's becoming commonplace to ask researchers to report some measure of effect size. So, let's suppose that you've run your chi-square test, which turns out to be significant. So you now know that there is some association between your variables (independence test) or some deviation from the specified probabilities (goodness-of-fit test). Now you want to report a measure of effect size. That is, given that there is an association or deviation, how strong is it?

There are several different measures that you can choose to report, and several different tools that you can use to calculate them. I won't discuss all of them but will instead focus on the most commonly reported measures of effect size.

By default, the two measures that people tend to report most frequently are the $\phi$ statistic and the somewhat superior version, known as Cramér's $V$ .

[Additional technical detail [^10-categorical-data-analysis-15]]

[^10-categorical-data-analysis-15]: Mathematically, they're very simple. To calculate the $\phi$ statistic, you just divide your $X^2$ value by the sample size, and take the square root: $$\phi=\sqrt{\frac{X^2}{N}}$$ The idea here is that the $\phi$ statistic is supposed to range between 0 (no association at all) and 1 (perfect association), but it doesn't always do this when your contingency table is bigger than $2 \times 2$ , which is a total pain. For bigger tables it's actually possible to obtain $\phi > 1$, which is pretty unsatisfactory. So, to correct for this, people usually prefer to report the $V$ statistic proposed by @Cramer1946. It's a pretty simple adjustment to $\phi$. If you've got a contingency table with r rows and c columns, then define $k = min(r, c)$ to be the smaller of the two values. If so, then Cramér's $V$ statistic is $$V=\sqrt{\frac{X^2}{N(k-1)}}$$

And you're done. This seems to be a fairly popular measure, presumably because it's easy to calculate, and it gives answers that aren't completely silly. With Cramér's $V$, you know that the value really does range from 0 (no association at all) to 1 (perfect association). --->

## 卡方檢定的適用條件

所有統計檢定都有適用條件，檢查手上的資料是否符合適用條件是一個好主意。至此討論過的卡方適合度檢定及獨立性檢定，適用條件包括：

- *期望次數足夠大*。還記得 @sec-chi2-independence 提到 $\chi^2$ 統計值的機率分佈是如何產生的嗎？因為二項分佈非常逼近常態分佈，正如 @sec-Introduction-to-probability 所提到的，只有在觀察次數足夠多的狀況下才會成立，這表示所有可用卡方檢定的資料期望次數都需要大到一個合理程度。那什麼是合理程度？目前學術界意見分歧，不過最起碼的共識是列聯表裡的所有期望次數至少要大於5。對於規模較大的表格，還可以期待至少有80%的期望次數大於5，並且沒有一個項目的期望次數小於1。然而，就原作者所找到的文獻（像是 @Cochran1954 ），這些條件設定似乎只是粗略的指導原則，而非硬性規定，並且主張似乎有些保守 [@Larntz1978]。

- *資料彼此獨立*。另一個卡方檢定稍微不嚴格的適用條件 是，研究人員必須相信每一項觀察結果是彼此獨立的。例如，我偶然得知某家醫院出生的嬰兒性別比例似乎很懸殊，我私下走訪這家醫院的產房，紀錄到20名女嬰和10名男嬰，看起來性別比例差異很大對吧？其實這個紀錄是我分別在十個不同的日子去參觀，每次都只看到了2名女嬰和1名男嬰。這樣的紀錄不再那麼令人信服了，是吧？表面上的30筆觀察完全不彼此獨立，實際上只相尚於3個彼此獨立的觀察結果。這顯然是一個非常極端而且容易看出缺陷的例子，但其中指出了獨立性的基本問題。有時違反獨立性的資料，就像前述的走訪醫院的例子那樣，會導致研究人員錯誤地拒絕虛無假設，但是也可能朝相反的方向發展。為了幫助讀者認識不容易看出缺陷的狀況，這裡用 @sec-chi2-goodenss-of-fit 提到的撲克牌實驗做一些修改來說明。假如研究人員不是要求200個人各自想像隨機抽出一張撲克牌，而是要求50個人各自選出4張撲克牌，其中一種可能的結果是*每個人*都會善用“代表性捷思法”[@Tversky1974]，抽出一張紅心、一張梅花、一張方塊和一張黑桃。這是人類自主意識造成的非隨機行為，但在這種實驗狀況，研究人員能預期四種花色都會出現50次觀察次數。因為參與者的四次選擇彼此關聯，破壞每次觀察結果的獨立性，導致統計檢定結果將錯誤地保留了虛無假設。


如果讀者懷疑手上的資料違反獨立性的條件，可以使用稍後會介紹的麥內瑪檢定( McNemar Test)或者本書不會介紹的 Cochran 檢定加以確認。若是覺得期望次數太小，可以使用費雪精確檢定。這一章最後兩個部分將簡單介紹費雪精確檢定與麥內瑪檢定。

<!--- All statistical tests make assumptions, and it's usually a good idea to check that those assumptions are met. For the chi-square tests discussed so far in this chapter, the assumptions are:

-   *Expected frequencies are sufficiently large*. Remember how in the previous section we saw that the $\chi^2$ sampling distribution emerges because the binomial distribution is pretty similar to a normal distribution? Well, like we discussed in @sec-Introduction-to-probability this is only true when the number of observations is sufficiently large. What that means in practice is that all of the expected frequencies need to be reasonably big. How big is reasonably big? Opinions differ, but the default assumption seems to be that you generally would like to see all your expected frequencies larger than about 5, though for larger tables you would probably be okay if at least 80% of the the expected frequencies are above 5 and none of them are below 1. However, from what I've been able to discover (e.g., @Cochran1954) these seem to have been proposed as rough guidelines, not hard and fast rules, and they seem to be somewhat conservative [@Larntz1978].
-   *Data are independent of one another*. One somewhat hidden assumption of the chi-square test is that you have to genuinely believe that the observations are independent. Here's what I mean. Suppose I'm interested in proportion of babies born at a particular hospital that are boys. I walk around the maternity wards and observe 20 girls and only 10 boys. Seems like a pretty convincing difference, right? But later on, it turns out that I'd actually walked into the same ward 10 times and in fact I'd only seen 2 girls and 1 boy. Not as convincing, is it? My original 30 observations were massively non-independent, and were only in fact equivalent to 3 independent observations. Obviously this is an extreme (and extremely silly) example, but it illustrates the basic issue. Non-independence "stuffs things up". Sometimes it causes you to falsely reject the null, as the silly hospital example illustrates, but it can go the other way too. To give a slightly less stupid example, let's consider what would happen if I'd done the cards experiment slightly differently Instead of asking 200 people to try to imagine sampling one card at random, suppose I asked 50 people to select 4 cards. One possibility would be that *everyone* selects one heart, one club, one diamond and one spade (in keeping with the "representativeness heuristic" [@Tversky1974]. This is highly non-random behaviour from people, but in this case I would get an observed frequency of 50 for all four suits. For this example the fact that the observations are non-independent (because the four cards that you pick will be related to each other) actually leads to the opposite effect, falsely retaining the null.

If you happen to find yourself in a situation where independence is violated, it may be possible to use the McNemar test (which we'll discuss) or the Cochran test (which we won't). Similarly, if your expected cell counts are too small, check out the Fisher exact test. It is to these topics that we now turn. --->

## 費雪精確檢定 {#sec-The-Fisher-test}

若是每個觀察次數太小，但還是需要檢驗兩個變項是否獨立的虛無假設，該怎麼辦？一種辦法是“收集更多資料”，但這樣做太隨便了。許多行為科學研究要收集比計畫目標更多的資料不是不可行，就是會違反研究倫理。統計學者有必要幫助研究人員解決倫理困境，提供一種更好的檢定方法。 @Fisher1922a 提出的精確檢定剛好能解決這個況的問題。為了方便說明，我們設定正在分析一個田野實驗的資料，研究對象是被指控為巫師，被綁在火刑柱上的情緒狀態，而且有些柱子已經開始燒起來了。[^10-categorical-data-analysis-16]，雖然鄉民們會因為巫師被燒死而安心，但是研究人員很難找到正在被燒的巫師。如果真的要收集，觀察次數會非常小。*salem.csv*(lsj資料集`Salem`)儲存的 資料列聯表表現了這一點（整理結果參見 @tbl-tab14-9 ）。


[^10-categorical-data-analysis-16]: 這個例子來自發表在*Journal of Irreproducible Results*的一則笑話。


<!-- What should you do if your cell counts are too small, but you'd still like to test the null hypothesis that the two variables are independent? One answer would be "collect more data", but that's far too glib There are a lot of situations in which it would be either infeasible or unethical do that. If so, statisticians have a kind of moral obligation to provide scientists with better tests. In this instance, @Fisher1922a kindly provided the right answer to the question. To illustrate the basic idea let's suppose that we're analysing data from a field experiment looking at the emotional status of people who have been accused of Witchcraft, some of whom are currently being burned at the stake.[^10-categorical-data-analysis-16] Unfortunately for the scientist (but rather fortunately for the general populace), it's actually quite hard to find people in the process of being set on fire, so the cell counts are awfully small in some cases. A contingency table of the *salem.csv* data illustrates the point (@tbl-tab14-9).

[^10-categorical-data-analysis-16]: This example is based on a joke article published in the *Journal of Irreproducible Results* --->

```{r}
#| label: tbl-tab14-9
#| tbl-cap: 以`Salem` 原始資料整理的列聯表
#Contingency table of the *salem.csv* data
huxtabs[[10]][[9]]
```

看了列聯表，研究人員很難不懷疑還沒被火燒的人，他們感覺的情緒可能比正在被火燒的人快樂。不過，因為樣本數很小，卡方檢定很難適用。做為一個不想被處火刑的嫌犯，*非常*希望有更肯定的答案，這就是**費雪精確檢定（Fisher's exact test）**[@Fisher1922a] 能派上用場的地方。

費雪精確檢定不同於卡方檢定，還有這本書介紹的其他假設檢定方法，因為這套方法沒有檢定統計值，而是“直接”計算 p 值。以下說明費雪精確檢定用在分析 $2 \times 2$ 列聯表的基本原理，與之前的單元一樣，我們要先定義一些符號（@tbl-tab14-10）。


<!--- Looking at this data, you'd be hard pressed not to suspect that people not on fire are more likely to be happy than people on fire. However, the chi-square test makes this very hard to test because of the small sample size. So, speaking as someone who doesn't want to be set on fire, I'd *really* like to be able to get a better answer than this. This is where **費雪精確檢定(Fisher's exact test)** [@Fisher1922a] comes in very handy.

The Fisher exact test works somewhat differently to the chi-square test (or in fact any of the other hypothesis tests that I talk about in this book) insofar as it doesn't have a test statistic, but it calculates the p-value "directly". I'll explain the basics of how the test works for a $2 \times 2$ contingency table. As before, let's have some notation (@tbl-tab14-10). --->

```{r}
#| label: tbl-tab14-10
#| tbl-cap: 費雪精確檢定的符號
#Notation for the Fisher exact test
huxtabs[[10]][[10]]
```

為了設定檢定程序，費雪將各行及各列的總次數（$R_1, R_2, C_1$ 和 $C_2$）都視為已知的固定值，然後根據這些總次數不變的前提，計算研究人員會得到實際觀察次數（$O_{11}, O_{12}, O_{21}$ 和 $O_{22}$）的機率。運用我們在  @sec-Introduction-to-probability 學到的機率符號表達規則，寫成的公式就是：

$$P(O_{11}, O_{12}, O_{21}, O_{22} \text{ | } R_1, R_2, C_1, C_2)$$

大部分讀者要搞清楚這是什麼樣的機率分佈是有點困難的任務，實際地說，這個機率函數符合所謂的超幾何分佈(hypergeometric distribution)。要計算 p 值，首先要計算觀察到這個表格內的數值，或者一個數值更“極端”的表格之機率。[^10-categorical-data-analysis-17]在 20 世紀 20 年代，即使是狀況最單純的研究問題，要做到這樣的計算是相當令人害怕的工作，不過現在只要表格不是太大，樣本數不是太龐大，一般計算設備都能輕易處理。真正棘手的問題是，要弄清楚一個列聯表比另一個列聯表更“極端”，是怎樣的概念？最簡單的解決方案是，出現機率最低的表格是最極端的，我們只要知道這個表格的 p 值。

[^10-categorical-data-analysis-17]: 不必懷疑，費雪精確檢定是根據費雪對 p 值的解釋，而不是尼曼的！請參考 @sec-The-p-value-of-a-test 。

從 jamovi `Analysis`界面開啟`Frequencies`-`Independent Samples`開啟`Contingency Tables`設定視窗，其中`Statistics`子選項裡有`Fisher's excat test`。若是已經載入`Salem`資料並設定好變項，就能在報表看到費雪精確檢定的統計結果。因為這種檢定方法只有輸出p值，由於這個範例的p 值足夠小（p = .036），足以拒絕那些正在被處刑的人和還沒有被處刑的人一樣快樂的虛無假設。輸出結果請參考 @fig-fig14-7。

<!---在這個例子中，我們使用了費雪精確檢定，該檢定使我們能夠在樣本量較小的情況下仍然對資料進行檢驗。通過這種方法，我們發現了正在燃燒的人和沒有燃燒的人之間的快樂程度存在顯著差異。--->

<!--- In order to construct the test Fisher treats both the row and column totals $(R_1, R_2, C_1 \text{ and } C_2)$ as known, fixed quantities and then calculates the probability that we would have obtained the observed frequencies that we did $(O_{11}, O_{12}, O_{21} \text{ and } O_{22})$ given those totals. In the notation that we developed in @sec-Introduction-to-probability this is written:

$$P(O_{11}, O_{12}, O_{21}, O_{22}  \text{ | } R_1, R_2, C_1, C_2)$$ and as you might imagine, it's a slightly tricky exercise to figure out what this probability is. But it turns out that this probability is described by a distribution known as the hypergeometric distribution. What we have to do to calculate our p-value is calculate the probability of observing this particular table or a table that is *"more extreme"*. [^10-categorical-data-analysis-17] Back in the 1920s, computing this sum was daunting even in the simplest of situations, but these days it's pretty easy as long as the tables aren't too big and the sample size isn't too large. The conceptually tricky issue is to figure out what it means to say that one contingency table is more "extreme" than another. The easiest solution is to say that the table with the lowest probability is the most extreme. This then gives us the p-value.

[^10-categorical-data-analysis-17]: Not surprisingly, the Fisher exact test is motivated by Fisher's interpretation of a p-value, not Neyman's! See @sec-The-p-value-of-a-test.

You can specify this test in jamovi from a check box in the 'Statistics' options of the 'Contingency Tables' analysis. When you do this with the data from the *salem.csv* file, the Fisher exact test statistic is shown in the results. The main thing we're interested in here is the p-value, which in this case is small enough (p = .036) to justify rejecting the null hypothesis that people on fire are just as happy as people not on fire. See @fig-fig14-7. --->

```{r}
#| label: fig-fig14-7
#| out.width: 60%
#| fig-cap: jamovi 費雪精確檢定分析結果
#Fisher exact test analysis in jamovi
knitr::include_graphics("images/fig10-7.png")
```

## 麥內瑪檢定 {#sec-The-McNemar-test}


想像一下你是*澳大利亞全民黨*(AGPP)的基層黨工[^14-translation-01]，被上級交辦要分析AGPP投放的政治宣傳有效程度，因此你找來總共$N = 100$有投票權的公民，請他們觀看AGPP的宣傳廣告。在播放宣傳內容之前，你先問他們是否打算投票給AGPP，播放廣告之後再問他們一次，看看有沒有人會改變主意。如果你是非常專業的幕僚，還會做更多事情，現在讓我們先看一下這個簡單實驗的結果，一種描述資料的方法就是建立如同 @tbl-tab14-11 的列聯表。

[^14-translation-01]: 譯註~這是原作者虛構的政黨，現實中無此政黨或團體。詳見[維基百科條目](https://en.wikipedia.org/w/index.php?title=List_of_political_parties_in_Australia&oldid=1149812960)。

<!--- Suppose you've been hired to work for the *Australian Generic Political Party* (AGPP), and part of your job is to find out how effective the AGPP political advertisements are. So you decide to put together a sample of $N = 100$ people and ask them to watch the AGPP ads. Before they see anything, you ask them if they intend to vote for the AGPP, and then after showing the ads you ask them again to see if anyone has changed their minds. Obviously, if you're any good at your job, you'd also do a whole lot of other things too, but let's consider just this one simple experiment. One way to describe your data is via the contingency table shown in @tbl-tab14-11. --->

```{r}
#| label: tbl-tab14-11
#| tbl-cap: 測試AGPP宣傳廣告效果的列聯表
#Contingency table with data on AGPP political advertisements
huxtabs[[10]][[11]]
```

也許你可能會認為這種問題適合用皮爾森$\chi^2$獨立性檢定處理，不過仔細想一下就會發現這樣做不切實際：雖然有100名參與者，全部資料卻有200個觀察值，這是因為每個人在"看廣告之前"和"看廣告之後"都有給出回應，也就是說這200個觀察值彼此之間並不獨立。如果選民A第一次說“是”，選民B說“否”，可以預期選民A第二次比選民B更可能說“是”！。因為違反了樣本獨立性的條件，$\chi^2$檢定的結果就會非常不可靠。如果這是一個相當罕見的狀況，我不會特別編寫一個小單元來討論，但是這種狀況並不罕見。最尷尬的是，這個例子是一個標準的重複測量設計，已介紹至此的任何檢定方法都無法處理。

能解決這種問題的檢定方法來自McNemar(1947)這篇論文，訣竅是先用稍微不同的方式整理列聯表，就像 @tbl-tab14-12。



<!--- At first pass, you might think that this situation lends itself to the Pearson $\chi^2$ test of independence (as per [The $\chi^2$ test of independence (or association)]). However, a little bit of thought reveals that we've got a problem. We have 100 participants but 200 observations. This is because each person has provided us with an answer in both the before column and the after column. What this means is that the 200 observations aren't independent of each other. If voter A says "yes" the first time and voter B says "no", then you'd expect that voter A is more likely to say "yes" the second time than voter B! The consequence of this is that the usual $\chi^2$ test won't give trustworthy answers due to the violation of the independence assumption. Now, if this were a really uncommon situation, I wouldn't be bothering to waste your time talking about it. But it's not uncommon at all. This is a standard repeated measures design, and none of the tests we've considered so far can handle it. Eek.

The solution to the problem was published by McNemar (1947). The trick is to start by tabulating your data in a slightly different way (@tbl-tab14-12). --->

```{r}
#| label: tbl-tab14-12
#| tbl-cap: 資料是重複測量所得的話，加上測量條件所整理的列聯表
#Tabulate the data in a different way when you have repeated measures data
huxtabs[[10]][[12]]
```

接著重新設定這個問題的虛無假設：“看宣傳內容之前”及“看宣傳內容之後”，兩種條件的測試有相同比例的人會回應：“是的，我會投票支持AGPP。”因為列聯表已經重新整理，這代表我們假設各行總次數和各列總次數都是來自相同的樣本分佈，也就是說麥內瑪檢定的虛無假設代表樣本資料的邊際同質性(marginal homogeneity)：第一行總次數等於第一列總次數$P_a + P_b = P_a + P_c$，以及第二行總次數等於第二列總次數$P_c + P_d = P_b + P_d$。請注意，這表示虛無假設只要設定為$P_b = P_c$，所以設定麥內瑪檢定程序，重要的只有列聯表中的對角格內數值。了解這一點後，執行**麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity)**應注意的適用條件與卡方檢定完全一樣。經過葉氏校正後，統計值的公式就是：

$$\chi^2=\frac{(|b-c|-0.5)^2}{b+c}$$ 

或者用這個單元一開始使用的公式改造：

$$\chi^2=\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}$$ 

這個統計值逼近自由度為1(df = 1)的$\chi^2$分佈（近似），自由度df = 1。不過務必記得，如同正規的$\chi^2$檢定，這只是一個近似值，只有觀察次數夠大才不需要使用校正後的統計值。

<!--- Next, let's think about what our null hypothesis is: it's that the "before" test and the "after" test have the same proportion of people saying "Yes, I will vote for AGPP". Because of the way that we have rewritten the data, it means that we're now testing the hypothesis that the row totals and column totals come from the same distribution. Thus, the null hypothesis in McNemar's test is that we have "marginal homogeneity". That is, the row totals and column totals have the same distribution: $P_a + P_b = P_a + P_c$ and similarly that $P_c + P_d = P_b + P_d$. Notice that this means that the null hypothesis actually simplifies to Pb = Pc. In other words, as far as the McNemar test is concerned, it's only the off-diagonal entries in this table (i.e., b and c) that matter! After noticing this, the **麥內瑪邊際同質性檢定(McNemar test of marginal homogeneity)** is no different to a usual $\chi^2$ test. After applying the Yates correction, our test statistic becomes:

$$\chi^2=\frac{(|b-c|-0.5)^2}{b+c}$$ or, to revert to the notation that we used earlier in this chapter:

$$\chi^2=\frac{(|O_{12}-O_{21}|-0.5)^2}{O_{12}+O_{21}}$$ and this statistic has a $\chi^2$ distribution (approximately) with df = 1. However, remember that just like the other $\chi^2$ tests it's only an approximation, so you need to have reasonably large expected cell counts for it to work. --->

### 實作麥內瑪檢定 {#sec-doing-The-McNemar-test}

現在您已經了解麥內瑪檢驗的所有內容，讓我們實際運行一個。*agpp.csv* 文件包含了我之前討論過的原始資料。*agpp* 資料集包含三個變項，一個id變項標記資料集中的每個參與者（我們將在片刻之後看到這有什麼用），一個**response_before** 變項記錄了當他們第一次被問到這個問題時的答案，以及一個response_after變項顯示他們在第二次被問到同樣問題時給出的答案。注意每個參與者在這個資料集中只出現一次。在jamovi中，轉到 'Analyses' - 'Frequencies' - 'Contingency Tables' - 'Paired Samples' 分析，並將**response_before** 放入 'Rows' 框，將**response_after** 放入 'Columns' 框。然後，您將在結果窗口中獲得一個列聯表，麥內瑪檢驗的統計資料就在它下面，參見 @fig-fig14-8 。

<!--- Now that you know what the McNemar test is all about, lets actually run one. The *agpp.csv* file contains the raw data that I discussed previously. The *agpp* data set contains three variables, an id variable that labels each participant in the data set (we'll see why that's useful in a moment), a **response_before** variable that records the person's answer when they were asked the question the first time, and a response_after variable that shows the answer that they gave when asked the same question a second time. Notice that each participant appears only once in this data set. Go to the 'Analyses' - 'Frequencies' - 'Contingency Tables' - 'Paired Samples' analysis in jamovi, and move **response_before** into the 'Rows' box, and **response_after** into the 'Columns' box. You will then get a contingency table in the results window, with the statistic for the McNemar test just below it, see @fig-fig14-8. --->

```{r}
#| label: fig-fig14-8
#| classes: .enlarge-image
#| fig-cap: jamovi中的麥內瑪檢驗輸出
#McNemar test output in jamovi
knitr::include_graphics("images/fig10-8.png")
```

我們完成了。我們剛剛運行了一個麥內瑪檢驗，以確定人們在廣告後是否和廣告前一樣有可能投票支持AGPP。檢驗是顯著的（$\chi^2(1)= 12.03, p< .001$），表明他們並非如此。事實上，看起來廣告產生了負面影響：人們在看過廣告後，投票支持AGPP的可能性更低。考慮到典型政治廣告的質量，這是很合理的。

<!--- And we're done. We've just run a McNemar's test to determine if people were just as likely to vote AGPP after the ads as they were before hand. The test was significant ($\chi^2(1)= 12.03, p< .001)$, suggesting that they were not. And, in fact it looks like the ads had a negative effect: people were less likely to vote AGPP after seeing the ads. Which makes a lot of sense when you consider the quality of a typical political advertisement. --->

### 與獨立性檢定有可分別? {#sec-difference-McNemar-independence}

讓我們回到本章的開頭，再次查看卡片資料集。如果您回憶一下，我描述的實際實驗設計涉及人們進行兩次選擇。因為我們有關於每個人第一次選擇和第二次選擇的信息，我們可以構建以下列聯表，用於將第一次選擇與第二次選擇交叉列聯（@tbl-tab14-13）。


<!--- Let's go all the way back to the beginning of the chapter and look at the cards data set again. If you recall, the actual experimental design that I described involved people making two choices. Because we have information about the first choice and the second choice that everyone made, we can construct the following contingency table that cross-tabulates the first choice against the second choice (@tbl-tab14-13). --->

```{r}
#| label: tbl-tab14-13
#| tbl-cap: 用*Randomness.omv*（卡片）資料交叉列聯第一次與第二次選擇
#Cross-tabulating first against second choice with the *Randomness.omv* (cards) data
huxtabs[[10]][[13]]
```

假設我想知道第二次選擇是否取決於第一次選擇。這是獨立性檢驗有用的地方，我們試圖做的是看看這個表格的行和列之間是否存在某種關係。

另外，假設我想知道平均而言，第二次選擇的花色頻率是否與第一次選擇不同。在這種情況下，我真正想做的是看看行總數是否不同於列總數。這就是您使用麥內瑪檢驗的時候。

這些不同分析產生的不同統計資料顯示在 @fig-fig14-9 中。注意結果是不同的！這些檢驗並不相同。

<!--- Suppose I wanted to know whether the choice you make the second time is dependent on the choice you made the first time. This is where a test of independence is useful, and what we're trying to do is see if there's some relationship between the rows and columns of this table.

Alternatively, suppose I wanted to know if on average, the frequencies of suit choices were different the second time than the first time. In that situation, what I'm really trying to see is if the row totals are different from the column totals. That's when you use the McNemar test.

The different statistics produced by these different analyses are shown in @fig-fig14-9. Notice that the results are different! These aren't the same test. --->

```{r}
#| label: fig-fig14-9
#| out.width: 60%
#| fig-cap: Randomness.omv（卡片）資料中的獨立與成對（麥內瑪）
#Independent vs. Paired (McNemar) with the *Randomness.omv* (cards) data
knitr::include_graphics("images/fig10-9.png")
```

## 本章小結

本章的學習重點有：

- [卡方適合度檢定](10-Categorical-data-analysis.html#%E5%8D%A1%E6%96%B9%E9%81%A9%E5%90%88%E5%BA%A6%E6%AA%A2%E5%AE%9A)用於你的表列資料是來自不同類別的觀察次數，虛無假設是可相互比較的已知機率。
- [卡方獨立性或關聯性檢定](10-Categorical-data-analysis.html#%E5%8D%A1%E6%96%B9%E7%8D%A8%E7%AB%8B%E6%80%A7%E6%AA%A2%E5%AE%9A)用於你的資料是能化為列聯表的觀察次數。虛無假設是兩種變項之間沒有關聯性。
- 列聯表的[效果量](10-Categorical-data-analysis.html#%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A%E7%9A%84%E6%95%88%E6%9E%9C%E9%87%8F)有多種測量方法。在此介紹最常見的Cramér’s V。
- 上述的卡方檢定法有兩種[適用條件](10-Categorical-data-analysis.html#%E5%8D%A1%E6%96%B9%E6%AA%A2%E5%AE%9A%E7%9A%84%E9%81%A9%E7%94%A8%E6%A2%9D%E4%BB%B6)：期望值次數夠大，觀察值彼此獨立。如果期望值次數不夠大，可以使用[費雪精確檢定](10-Categorical-data-analysis.html#sec-The-Fisher-test)；如果觀察值並非彼此獨立，可以使用[麥內瑪檢定](10-Categorical-data-analysis.html#sec-The-McNemar-test)。

如果想學習更多類別資料分析方法，推薦閱讀 @Agresti1996 的專書"類別資料分析導論"。如果基礎教科書無法滿足你的需要，或者未提供解決手上問題的方法，可以參考 @Agresti2002 的進階書藉。因為是進階書，建議先充分理解導論再來學習進階教科書。

<!---
The key ideas discussed in this chapter are:

- [The $\chi^2$ (chi-square) goodness-of-fit test] is used when you have a table of observed frequencies of different categories, and the null hypothesis gives you a set of "known" probabilities to compare them to.
- [The $\chi^2$ test of independence (or association)] is used when you have a contingency table (cross-tabulation) of two categorical variables. The null hypothesis is that there is no relationship or association between the variables.
- [Effect size] for a contingency table can be measured in several ways. In particular we noted the Cramér's $V$ statistic.
- Both versions of the Pearson test rely on two assumptions: that the expected frequencies are sufficiently large, and that the observations are independent ([Assumptions of the test(s)]. [The Fisher exact test] can be used when the expected frequencies are small. [The McNemar test] can be used for some kinds of violations of independence.

If you're interested in learning more about categorical data analysis a good first choice would be @Agresti1996 which, as the title suggests, provides an Introduction to Categorical Data Analysis. If the introductory book isn't enough for you (or can't solve the problem you're working on) you could consider @Agresti2002, Categorical Data Analysis. The latter is a more advanced text, so it's probably not wise to jump straight from this book to that one. --->
